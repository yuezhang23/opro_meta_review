wDDvJzvvBR;"REVIEW 
Summary:
This paper presents an approach to learning spatially aware language representations. The authors propose a contrastive representation model that integrates spatial context into language representations, aiming to enhance the performance of tasks that require spatial reasoning. The model combines visual and textual data to create embeddings that are sensitive to spatial relationships. The contributions include an architecture of the proposed model, extensive experimental results demonstrating improved performance on spatial reasoning benchmarks, and an analysis of the model's ability to generalize across different spatial contexts.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper makes a strong contribution to spatial reasoning. The integration of spatial context into text-to-audio generation models is an important and underexplored area, and this work offers a novel and effective solution.
2. The experimental setup is rigorous, with well-designed experiments that effectively validate the model's performance.
3. The paper is well-written, with clear explanations of the methodology and results. 
4. The findings have significant implications for improving spatial language understanding in various applications.

Weaknesses:
1. The reliance on synthetic datasets may limit the generalizability of the findings. The authors could explore the way to train the model on in-the-wild data.
2. The current interpretation experiments (Sec. 5.4) only study a four-class classification (""left,"" ""right,"" ""up,"" ""down""), which is insufficient for real-life scenarios. For instance, spatial audio applications often require more nuanced classifications, such as distance perception (e.g., strong/weak reverb in indoor/outdoor settings), which are critical for capturing and representing spatial information. The authors should consider extending the experiement to handle a wider range of spatial attributes to enhance its applicability in diverse settings. For example, the authors should consider using prompts like ""xxx is making a sound in the distance"" and ""xxx is making a sound nearby"" to figure out if the results are different.
3. The paper could benefit from a more detailed error analysis, identifying common failure cases and understanding why the model fails in certain scenarios. This analysis would provide insights for further improvement and refinement of the model.
4. While the model performs well on tasks like retrieval and source localization, its ability to generalize to spatial text-to-audio generation remains to be seen.

Limitations:
The authors have addressed the limitations of their work by discussing the datasets used and the model's computational requirements. Additional limitations can be found in the Weaknesses section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper describes a method for learning to represent spatial audio (and text).  The proposed model is trained on synthetically spatialized audio data with corresponding text prompts.  The authors evaluate the system on audio captioning/retrieval and localization tasks, showing that the proposed model effectively represents both semantics and locality of audio.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is well written, clearly organized, and easy to follow.  The proposed method makes intuitive sense and appears to be effective.  The empirical evaluation is reasonably thorough and the choice of tasks and baseline models seem appropriate.  Spatial audio is a growing area (in the context of machine learning / event detection / representation learning), and I think the work described here does fill an unaddressed area of the literature in an interesting way.  Overall I think they did a great job here.

Weaknesses:
I don't have much to fault here, but there are a few points that I think could be expanded to improve clarity and help readers understand the contribution here.  I'll elaborate on these points in the questions section, but the high-level gloss is:

- While the spatial representation part of the work (ie FOA-derived input) is explained well, there is almost no explanation of how the spatialization was implemented.
- There is little (if any) qualitative analysis of the results, only aggregate scores reported in tables.

Limitations:
The limitations sections seems sufficient to me.

One potential caveat here is that the authors do not explicitly mention any limitations imposed by the accuracy of the spatialization process, e.g., whether it will only work well for simulated closed environments (shoebox model) or if it can accurately capture open environments.  This I think would be easy to resolve with a bit more information about the process and an extra line the limitations (if necessary).

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents ELSA (Embeddings for Language and Spatial Audio), a novel model designed to learn spatially-aware audio and text embeddings using multimodal contrastive learning. The primary aim is to address the limitations of existing audio foundation models, which lack spatial awareness, and sound event localization and detection models, which are constrained to a fixed number of classes and absolute positional descriptions. The authors spatially augment several classical open-source audio datasets in order to train ELSA.  Results show that ELSA is able to capture spatial attributes and semantic meaning of the audio.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The focus of this paper is on learning spatial audio embeddings associated with natural language description, which is a very interesting and rewarding problem for which there is a lack of models.
- These authors synthesize large amounts of non-spatial audio data under various spatial configurations, which is a valuable contribution to the field of spatial audio understanding.

Weaknesses:
- For this paper, my biggest concern is the generalizability of the model to real scenarios. 
  While the synthetic dataset is extensive, there is a risk that the model might not generalize well to real-world scenarios due to potential biases in simulated environments. To show the performance of model generalization to real scenarios, the experiments only on a small real-world dataset appear too thin. Would it be possible to test ELSA in other real scenarios, for example, in some of the tasks in the latest DCase competition, e.g. Sound Event Localization?
- For paper writing, too much important information is put in appendices, such as the structure figure of the whole model. Perhaps the layout of the writing could be adjusted to make it easier to read.
- The citation format of the article is highly problematic and needs to be standardized.

Limitations:
- As the authors mentioned, for creating a spatial audio caption dataset, using LLM to rewrite the caption might lead to hallucinations.
- Model performance in real scenarios is yet to be verified.
- ELSA looks very suitable to be used as a spatial audio encoder for a caption model to conduct spatial audio captioning, but unfortunately, the authors did not show this kind of capability in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents ELSA (EMbeddings for Language and Spatial Audio),  a spatially aware-audio and text embedding model. The training data is created by synthesizing spatial audio in ambisonic format and augmenting text captions with spatial information. A small real world data is also collected for evaluations. The model training itself largely follows standard CLIP/CLAP training by using contrastive losses. Additional losses for direction and distances are added for the spatial part. Evaluations are done on both semantic retrieval tasks and spatial tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
– The paper addresses a key part of multimodal contrastive embeddings. Sounds contain a significant amount of spatial information and humans naturally rely on directional information from sounds. Considering this it is expected that embeddings with spatial information are created. The paper is a good step in the right direction. 

– For the most part, the paper is well done. Spatial audio can present several challenges with respect to data (more so in multimodal settings, training approach). Considering the challenges around learning from spatial audio, the paper presents a good approach for learning spatially-aware language embeddings. The experiments are also reasonably good. 

– The paper is also well written and mostly clear.

-------
Score increased after rebuttal.

Weaknesses:
There are a few weaknesses which are worth addressing in the paper. 

– For table 2, I would be curious to see what CLAP on its own can achieve. It would be good to contrast this zero-shot classification on the spatial task. 

– How were the non-spatial audio-text pairs used in training (as shown in Table 3, last row) ?

– Using non-spatial audio-text seems crucial for good semantic retrieval. This is evidenced by A.6 as well where the models training on just spatial audio-text pairs do not do well on semantic retrieval task. This is a bit surprising. The CLIP loss is still present in training, the semantics are also intact in spatial audio-text pairs. Why should there be a performance drop in that case ? it would be good to provide a  good discussion and justification

– In Table A.7, the performance of the model trained on spatial Clotho and Audiocaps is better on RWD data than even on Clotho and Audiocaps itself. That is a bit surprising. We would expect that the model would be better in it’s own domain. The difference also is pretty big. 

– The discussion in Section 5.4 is a bit adhoc. I would suggest not referring to anecdotal observations. The experiments could be better designed. 

– Several of the classification experiments end up using 3-4 layers MLP. I think a more shallower model (maybe even just linear classifier) would provide a better confirmation of what information the embeddings store. Otherwise such deeper networks are able to push the numbers on their and it’s not clear how good the embeddings are. 

– Some form of clustering and distance visualization would be good. It has been incorporated in some form in Table 2, but it would be good to explicitly show how the distances between embedding represent the spatial information.  

– All the spatial mapping in terms of the language is very discrete (A.2). The range for distance, direction etc. can appear a bit arbitrary and forced. While this is perhaps a good first attempt, a more continuous form of “spatial-language” is desirable. Another thing could be a perception driven approach can also be taken where the boundaries are decided by what people generally perceive as left or right w.r.t sound direction.

Limitations:
Please add some limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
8bExkmfLCr;"REVIEW 
Summary:
The paper introduces the JOBCD (J-Orthogonal Block Coordinate Descent) algorithm, a novel method designed to tackle optimization problems under J-orthogonality constraints. JOBCD includes two variants: GS-JOBCD (Gauss-Seidel strategy) and VR-J-JOBCD (Jacobi strategy with variance reduction). Theoretical analyses establish the algorithms' complexity and convergence, while extensive experiments show JOBCD's superior performance compared to state-of-the-art methods in various applications.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The strengths of this paper are listed as follows:

Originality: This paper introduces JOBCD as a novel approach to handling J-orthogonality constraints. It offers GS-JOBCD and VR-J-JOBCD, showcasing flexibility and innovation in optimization strategies.

Quality: This paper provides comprehensive complexity and convergence analyses. Extensive experiments demonstrate superior performance on real-world and synthetic data.

Clarity: The structure is logical.

Significance: This work is relevant to various statistical learning and data science fields.

Weaknesses:
Some proofs for Section 4 are hard to follow.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes two Block Coordinate gradient descent methods(BCD) for solving J-orthogonal constrained problem. One is Gauss-Seidel type, the other one is Jocobi type as well as addressing finite sum problem using variance reduction strategies. Convergence guarantees are proved with KL conditions. Numerical experiments show the advantages of the  proposed algorithm.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed algorithm decomposes the matrix variable into row block structure, yielding a block coordinate descent algorithm with a small size subproblem. The numerical performance is very impressive.

Weaknesses:
1. This paper is based on the paper "" [51] Ganzhao Yuan. A block coordinate descent method for nonsmooth composite optimization
under orthogonality constraints. ArXiv, abs/2304.03641, 2023.""
The main difference is the constraint in this paper becomes J-orthogonality constraint. However, the framework follows almost the same as [51]. The authors should highlight the novelty of the algorithm or difficulty in the extension.

2. The authors of the reference [31] may be wrong. Besides, the UMCM algorithm in [31] solves orthogonal constrained problem. Is there any difference in implmenting in solving J-orthogonality problem? The objective value of UMCM is far from the JOBCD method. I'm curious about the reasons.

3. In numerical experiment, how do you select subset from the dataset, see line 309.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes two block coordinate descent methods for minimization of a finite-sum subject to the J-Orthogonality constraints — one based on Gauss-Seidel strategy, the other based on variance reduction and Jacobi strategy. The convergence is proved, with a global convergence rate of O(N/\epsilon) and O(\sqrt{N}/\epsilon) respectively, and a local convergence rate that depends on the desingularization in the KL-condition assumption.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The algorithms proposed are novel and might be useful in practice: the update rules involve solving a small size problem thereby is very simple, and the convergence is proved theoretically under reasonable assumptions.

Weaknesses:
The paper is relatively dense, and I find it a bit hard to keep track of all the terms introduced. For instance, the parameter theta is used in the algorithms but I’m not sure where it is introduced; in Assumption 4.8 KL function is mentioned but it’s not defined…

Limitations:
Yes, the paper discusses the assumptions of the theorems.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a block coordinate descent method for solving optimization problems with J-orthogonality constraints. Several variants of the method are introduced within this framework, and convergence results are established. Extensive numerical results are also presented to demonstrate the efficiency of the proposed methods. However, I have some concerns regarding the novelty of this paper as well as the numerical results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
It appears that optimization with J-orthogonality constraints has not been thoroughly studied in the literature. This paper proposes an efficient method for addressing this problem.

Weaknesses:
1. My major concern is that the novelty of this paper might be insufficient since the row-based approach is very similar to that in [51], even though the two papers tackle different problems.

2.  For the numerical results shown in Table 1, the proposed method fails to return a feasible solution for some instances, such as randn(10-10-5) and w1a (2470-290-145), as well as some other instances in the appendix. This is strange since the paper describes a BCD-type method, which should always return a feasible solution.

3. The information of the reference [31] might be incorrect. 

4. For the GS-JOBCD method, there are two options for choosing $Q$, whereas J-JOBCD only has one option. The authors should provide an explanation for this difference.

5. The presentation could be further improved. Here are a few examples: the formulation of $P_i$  after equation (12) could be simplified by removing the notation $\mathrm{mat}$; it is unclear if the requirement on $\underline{Q}$ in equation (4) is sufficient to guarantee convergence (probably not, since $\underline{Q} = 0$  also satisfies this condition).

Limitations:
At the beginning of the paper, the authors claim that equation (2) can imply 
$\\|\nabla f_i(X) - \nabla_i f(X^+)\\| \leq L_f \\|X - X^+\\|$, which is incorrect. Note that the converse is correct. The other assumptions in Assumptions 4.1 and 4.2 essentially assume the compactness of the iterates.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
RTaSjoxmw9;"REVIEW 
Summary:
This paper addresses the challenge of achieving outlier robustness in phase retrieval, specifically focusing on the recovery of real-valued signals from intensity measurements that have been corrupted by adversarial outliers. The contribution of this work is the development of a nearly-linear time algorithm that is nearly sample-optimal and can accurately recover the true vector despite the presence of outliers. This is done through a two-step process that involves robust spectral initialization and robust gradient descent, utilizing recent results in high-dimensional robust statistics.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
+A two-stage algorithm achieves nearly-linear time complexity consisting of an initial spectral initialization phase and a gradient descent refinement phase.

+Theoretical analysis showing the algorithm can recover the ground truth signal despite the presence of outliers, while maintaining near-optimal sample efficiency.

Weaknesses:
- The paper's theoretical results assume the corruption level $\epsilon$ as a constant in Theorem 3.1, despite it being initially introduced as a variable in Definition 1.2 to denote the extent of sample corruption. This constant treatment affects the robustness of the results by neglecting the influence of $\epsilon$ on the sample complexity. A thorough analysis that explicitly considers the variability of $\epsilon$ would significantly strengthen the theoretical foundation.
- Lack of numerical validations for the theoretical claims
- The idea and design of the two-stage robust phase retrieval algorithm is not novel, by adapting existing reweighting phase retrieval algorithms (throught different design of the reweights) to handle outliers through robust statistics. The contamination model is also from existing works, which has been recent studied in a number of different contexts; e.g., robust linear/nonlinear estimation by e.g., Diakonikolas and coauthors.
- Lack of numerical validation and comparison of the proposed algorithm with respect to existing (robust) Gaussian phase retrieval algorithms. It is difficult to judge if the proposed algorithm is of practical interest (or only of theoretical interest).
- Quite a lot statements in the paper are rather confusing, e.g., ""we propose the problem of outlier robust phase retrieval""; robust phase retrieval has been long studied in the literature; as far as I understand, the paper studies a new robust phase retrieval problem by considering also adversarial a_i's on top of existing formualtions. ii) ""It is well-known that natural nonconvex formulations of phase retrieval do not have spurious local optima."" which is not precise enough and is true under very stringent assumptions. iii) ""This is first achieved via approaches based on semidefinite programming (SDP) relaxations (see, e.g., Cand`es et al. (2015c))."" I guess the first Gaussian phase retrieval algorithm was the AltMin algorithm (Netrapalli, et al 2013. Phase retrieval using alternating minimization. Advances in Neural Information Processing Systems, 26.) which was interestingly not cited in the submission. iv) ""Similar landscape results are known for other natural nonconvex formulations of phase retrieval as well (e.g., min f(z) = P i( √ yi − |⟨ai, z⟩|)2 (Soltanolkotabi, 2019))."" The first work to study the Gaussian phase retrieval based on the magnitude-based least-squares nonconvex formulation and achieve provable guarantees is (Wang et al (2017). Solving systems of random quadratic equations via truncated amplitude flow. IEEE Transactions on Information Theory. 64(2):773-94.) Please be careful and fair in stating related results.

Limitations:
NA

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper focuses on the problem of outlier robust phase retrieval, whose goal is to recover a vector $x \in \mathbb{R}^d$ from $n$ intensity measurements $y_i = (a_i^\top x)^2$ when a small fraction of the samples are adversarially corrupted. The authors propose and study this problem, providing a nearly sample-optimal and nearly linear-time algorithm to recover the ground-truth vector $x$ in the presence of outliers.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper provides an analysis of the practically interesting problem of outlier robust phase retrieval. The algorithm and framework might have implications for various applications that are relevant to phase retrieval.

Weaknesses:
1. The analysis appears to be more incremental in nature compared to those in previous theoretical works related to phase retrieval. More precisely, the key novelty of the spectral initialization step resides in the assignment of a nonnegative weight to each sample. Regarding the gradient descent step, the problem seems to be simplified to the analysis of robust mean estimation algorithms.

2. Important references are missing. For instance, the authors ought to cite the works related to robust compressed sensing (and it would be better to discuss in more detail the disparities between the analysis for the robust gradient descent step in this work and the analysis in these relevant works), such as

- Liu, Liu, Yanyao Shen, Tianyang Li, and Constantine Caramanis. ""High dimensional robust sparse regression."" In International Conference on Artificial Intelligence and Statistics, pp. 411-421. PMLR, 2020.
- Liu, Liu, Tianyang Li, and Constantine Caramanis. ""High Dimensional Robust $ M $-Estimation: Arbitrary Corruption and Heavy Tails."" arXiv preprint arXiv:1901.08237 (2019).

3. The authors solely consider the scenario of noiseless intensity measurements and fail to take into account the noisy case.

4. The paper does not include experimental results, which could limit the confidence in the practical effectiveness of the proposed approach.

Limitations:
No experimental validations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies a classical problem called phase retrieval. The goal is to obtain unknown $d$-dimensional vector $x$ from $n$ datapoints $(a_i, \langle a_i, x \rangle^2)$. This work assumes that $a_i$ are iid Gaussian vectors, but also that a small $\varepsilon$ fraction of the data is corrupted. The authors suggest a two-stage process to identify the vector up to a small error. First, a spectral based algorithm is used to have a small constant error. Further, a robust gradient descent is used to approximate the initial guess up to a small error.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Paper is well-written and provides a good overview of the problem and of the techniques.
2. Phase retrieval is a traditional non-convex problem, which was largely studied before, and understanding how robust algorithms perform on it is important.
3. Paper uses prior technique in a simple way, and it is possible that this two-stage approach can be applied to other problems.

Weaknesses:
1. Results are limited to the Gaussian setting.
2. The method for RME that is used assumes that variance $\sigma$ is known? But in the way it is used here, $\sigma$ depends on the distance between current solution and the true vector. Authors do not comment on this issue.
3. $\tilde O, \tilde \Omega$ notation is not defined.
4.  Intuition in line 98 in my interpretation contradicts more exact version in line 214 (In the end, if I understand correctly, the crucial reason why the spectral initialization algorithm works is that the adversary cannot change the top eigendirection, but can only add new directions).

Limitations:
There are no ethical limitations of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study the phase retrieval problem for retrieving a real signal under the influence of arbitrary corruption. The corruption is allowed to be present in labels or features. They propose a two-step solution. First, they ensure that the initialization is robust to the corruption and second, they show that the gradient descent updates can be made resilient. Authors claim that their method can recover the true signal (with possibly sign mismatch) to an arbitrary precision.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The ideas presented in the paper are certainly interesting. If resilience to corruption can be achieved in individual steps, then it makes sense that it might lead to a good overall recovery.

Weaknesses:
1. The authors claim that corruption level up to some universal constant $\epsilon'$ can be handled through their method. Although, to the best of my understanding, this quantity is not characterized in the main paper. What is the maximum value for $\epsilon'$? 
2. There is no discussion on the dependency of $\epsilon'$ on $n$ or $d$.   
3. The claim of signal recovery to an arbitrary precision puzzles me. It is known that for $\epsilon$-corrupted vectors, the robust mean estimation can only be done up to $\Omega(\sqrt{\epsilon})$ error. Despite that, the authors claim signal recovery (with possibly a flipped sign) to an arbitrary precision. Can the authors comment on how this is achieved?   
4. The claim in line 213 says that $y_i$ is always greater than $0$. Why is that true when the adversary can corrupt $y_i$ arbitrarily? As far as I can tell, the algorithm does not discard negative $y_i$s.

Limitations:
Please see above.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
Cth1PyCwZt;"REVIEW 
Summary:
This paper shows the use of psychometric modeling techniques to measure the reasoning ability of LLMs on human exams. Specifically, the author(s) use Item Response Theory (IRT) to evaluate a Brazilian college-entrance exam, and demonstrate that IRT can provide a more informative evaluation of LLMs , including: the ability to distinguish human-like vs non-human-like response patterns, and to determine whether an exam can reliably measure an LLM's abilities. The empirical results suggest that traditional accuracy metrics are insufficient to assess the abilities of LLMs, and advocate for using IRT/psychometric theory to evaluate them.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Provides more comprehensive evaluation methods for LLM performance, I personally agree that accuracy metrics often do not give a complete picture of LLM ability.
2. The results section is methodological, it evaluates not only the IRT scores but how reliable they are based on several metrics (increases reliability of the evaluations)

Weaknesses:
1. The results analysis would benefit from a more detailed and clearer/deeper analysis, some statements made (eg. L293-298) are high level observations based on the results, but lack further insight into why certain LLM behaviors occur. Performing more detailed analyses into the specific subset of questions that contribute to scores could help to further understand the limitations of the LLM (L328-331 alludes to this, but very briefly).

2.  All the evaluations were done on variations of the ENEM exam dataset, showing that these psychometric method would also work on other datasets would make this approach more convincing that it will work for wider applications - I understand that there is limited time to run more experiments, so this is more so just a comment.

Limitations:
As mentioned above, as experiments are done on variations of one dataset, there are doubts about the generalizability of these methods on other datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on evaluating LLM abilities on a dataset of 8 college-entrance exams in Brazil (translated to English) measuring Item Response Theory instead of Accuracy. It highlights how such metric is useful to better understand models' performance.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I have found the work very well structured and appreciated the amount of care the authors have been given to the preparation of the dataset for the experiments (PDF processing, translation to English, use of exams designed for blind people in order to address questions based on images, etc). The experiments and results are discussed in details, with clear comparisons with human performance, discussing clear differences (e.g. in Mathematics).

Weaknesses:
While the paper is well structured, I felt it was missing a ""what now?"" message. The authors wrote a convincing argument in favour of using IRT, how do we convince now the field of ML / AI to use it more extensively? What are its limitations in comparison with accuracy-based metrics (given there are many, for instance you need information on overall human performance) and how do we overcome them?

Limitations:
I think the work should have discussed more about the specificities of ENEM - I agree with the authors that this is a relevant test-bed for this sort of evaluation, but in which ways are they specific / tailored to Brazil? Is there anything researchers should know about ENEM, which would make future testing / applications more challenging? For instance which topics are covered in Humanities or Languages, how specific are they about the country cultural context?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper initiates the empirical study of the performance of LLMs using Item Response Theory (IRT) models from a large college-entrance exam.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The question of construct validity of LLM evaluations (based on scores in exams designed for humans) is very important. This paper addresses this question in the earnest, by leveraging the primary tool developed in the education and psychometrics field, IRT. As far as I know, this is an original contribution as no previous work has used IRT in the same way to tackle the construct validity issue of LLM evaluations.

- The paper is largely well-written and the concepts (e.g. section 3) are explained clearly.

- Relative strength of empirical work. The experiments are well-designed and there is some breadth in the range of hypotheses explored, e.g. English vs. Portuguese effect on performance, Different topics of exams, response patterns in LLMs vs. humans with questions sorted by IRT difficulty. Seven different LLMs were evaluated.

- Significance. The method of this paper (i.e. using IRT in LLM evals) is an important first step to understanding what LLM evals are trying to measure. The paper already observes interesting phenomena, e.g. 
(1) the Fisher information of the math exam for the LLM test response distributions is low compared to other exams (although this is a somewhat obvious corollary of the p_i's being close to random for the LLMs performance on the math exam, the FI is a metric that points in the right direction).
(2) the joint distribution of IRT scores and CTT scores for LLMs is meaningfully different from that of the human test takers.

Weaknesses:
1. Some of the conclusions drawn by the paper appear unscientific/not well-substantiated. To me, the empirical results are subtle and require more thoughtful interpretations. Most of the interpretations of the experiments are confusing to me (i.e. I'm skeptical the conclusions follow), given the actual plots shown. For example, 

(a) What are ""outlier models"" (line 237)? We cannot see from Figure 1 that ""outlier models ... have higher accuracy and/or lower IRT scores..."" - how is this statement supported?

(b) line 223-224. The scale of IRT scores and CTT scores is not comparable. How can you conclude there is ""greater variability"" in the latter than in the IRT score? This is not scientific.

(c) line 264-265. The statement ""...questions that are easy for humans but difficult for LLMs"" is again inaccurate. The questions are relatively easier for humans but may not be ""easier"" than the other questions for humans, if easier means for humans anyway.

(d) Why is the math exam not meaningful for evaluating LLMs? Doesn't it suggest that the models are randomly guessing and therefore bad? I don't agree with this interpretation.

2. A clarity issue with the math writing. Line 154-155: This sentence ""...j has a more likely response vector than indicated by their ability"" is mathematically wrong. It is not possible to have a random draw from a Multinomial distribution that is ""more likely"" (i.e. higher probability) than the expectation vector (which is not even in the space of possible draws).

3. Experiment section writing missing some details and figures are somewhat difficult to interpret (esp Figure 1). I have several unanswered questions. How was the closed curve generated from the 30 points (of random shuffles)? The caption for Figure 1 could be more informative, e.g. was the exam answered in English or Portuguese by the LLM. If English, are the IRT models fit still valid? - I don't think so. 

4. Typo in lines 232-233, ""Natural sciences"" appears twice. and the sentence contradicts the graph.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides a fresh perspective to evaluating LLMs by arguing for a stronger emphasis on psychometric methods particularly Item Response Theory (IRT) when evaluating them on exams designed for humans, rather than the reliance on traditional metrics such as accuracy. The authors postulate that IRT provides a more comprehensive evaluation by considering not just the number of correct answers but also the difficulty of the questions and the patterns of responses. The authors utilize the Brazilian college entrance exam ENEM for their case study and compare how various LLMs fare against human test-takers. They show how psychometric methods can be leveraged to distinguish between human like and non-human like responses. Furthermore, they demonstrate how IRT can be used to assess the suitability of an exam for making meaningful measurements of an LLM's abilities in the given area.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper is very well written. It provides a comprehensive literature review and does a good job at covering the background information. The experimental analysis is sound with sufficient supporting materials. The paper makes significant novel contributions to evaluation of LLMs. The application of psychometric methods and the insights that can be mined through them when used to compare LLMs can be of significant interest to the research community. The experimental results on assessing whether an exam is a good indicator of an LLM's ability are particularly interesting and open up significant opportunities for future research.

Weaknesses:
The error analysis can be more detailed especially in areas where the results are surprising. This would better help support the conclusions.       For instance for the questions in Math and Natural Sciences wherein the models show fluctuating performance it would be useful to know what those questions aim to test. Are LLMs not able to solve the problems due to calculation errors or do these problems involve more complex multi-step reasoning or is it just linked to knowledge cutoff (e.g questions involving current events)?

Limitations:
The pre-requisite for this type of evaluation seems to be the existence of a strong IRT model which in turn requires the existence of large amount of carefully annotated human data.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
HeJ1cBAgiV;"REVIEW 
Summary:
This paper analyzes the convergence of scaffnew in the quadratic setup and achieves a linear speedup in the number of clients.  It is not attained by the original paper of Scaffnew. Additionaly, the author find an application of the federated quadratic problem -- Federated
Linear Stochastic Approximation and TD Learning.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper tries to analyze the convergence of scaffnew in the quadratic setup and achieves a linear speedup in the number of clients.  It is not attained by the original paper of Scaffnew.

Weaknesses:
The achieved speedup holds only for quadratic setup and the application of the quadratic loss function is quite limited. 
The experiments are a little simple.

Limitations:
The achieved speedup holds only for quadratic setup and the application of the quadratic loss function is quite limited.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper provides a non-asymptotic analysis of Federated Linear Stochastic Approximation. The authors provide (biased and unbiased) finite-time MSE bounds for general LSA and TD learning under the assumption that the noise is i.i.d.. For Markovian noise, only an unbiased MSE bound is provided. Most importantly is that these bounds express this error as a function of the step-size used, number of agents and number of local updates. Finally, a new algorithmic variant, namely, SCAFFLSA is introduced, reducing communication between agents while maintaining the linear speed up in the algorithm. The contributions of the paper are illustrated through numerical studies.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The reviewer is not very familiar with the federated learning literature, but as far as they are aware, most of the ideas of the paper are new. 

The reviewer was unable to look at all proofs in detail, but have not identified any issues with respect to correctness.

Weaknesses:
The organization of the paper is not great. First, it is very hard to follow the main text given the amount of symbols and equations. Secondly, contributions related to i.i.d and Markovian noise and TD learning seemed to be scrambled together. I particularly think that splitting the results into different sections (one for i.i.d. noise, one for Markovian noise and one tailored to TD learning) would improved the readability of the paper a lot.

Also, there is a missing ""c"" superscript right above equation (1) in Algorithm 1.

The paper should be revised for clarity and typos.

Limitations:
The authors identify and state which assumptions are necessary to hold for each of the results to be true, so the limitations of this work are objectively identified.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper first analyzed the performance of federated linear stochastic approximation or FedLSA algorithm. Second, it proposed a new algorithm called stochastic controlled averaging for Federated LAS or SCAFFLSA and analyzed its performance. The key idea of SCAFFLSA is to use a control variable to mitigate the client drift. The performance of the proposed algorithm was verified using experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper proposed a new analytical framework to analyze the sample and communication complexity of FedLSA. Using this approach, the paper analyzed the performance of SCAFFLSA. The paper also extended the analytical framework to Federated TD learning. 

2. The paper proposed a new algorithm, SCAFFLSA, which mitigate the client drift using a control variable. The sample and communication complexity of SCAFFLSA is significantly better than that of FedLSA and Scaffnew. The performance analysis of SCAFFLSA was also extended to Federated TD.

Weaknesses:
The experiments are relatively weak. It will be more beneficial if the authors could provide more applications of the proposed algorithm to other problems, especially for federated TD learning.

Limitations:
There was no discussion on the limitations of the proposed algorithm.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
nEvnCuviMO;"REVIEW 
Summary:
Metric distortion is a framework to evaluate the ""accuracy"" of social choice rules, by considering a worst-case candidate and voter embedding in a metric space, and by assuming that reported votes are derived from distances in the metric space. So far, votes were assumed to be a deterministic function of the distances. The paper investigates the case where they are probabilistic functions of the distances, in the asymptotic limit of a large number of voters. The key finding is that this inverts the evaluation of some voting rules, in particular Copeland and Random Dictatorship, for highly noisy voting.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The model introduced by the authors is an insightful generalization of previous work which, remarkably, provides a markedly different view on social choice rules. Given the growing importance of social choice in machine learning, as well as accounting for noisy inputs while considering embedded vector spaces, I believe that this work scores high in significance.

Additionally, the analysis is quite thorough, with matching lower/upper asymptotic bounds for Plurality upper bound for Copeland and lower/upper bounds for Random Dictatorship.

The paper is also fairly well written.

Weaknesses:
My main concern is Lemma 3. The proof seems to argue that the constraints $\forall i, | b_i - w_i | \leq b_i + w_i$ (i.e. inequality constraint on $(i, W, B)$), but the optimization problem (7) has a constraint $\max_i | w_i - b_i | \leq \min_i b_i + w_i$. It is not clear to me why these constraints would be equivalent. Note that the latter implies the former set of constraints. Thus if $\mathcal E_{\alpha}'$ was defined with all voter-wise constraints, then it would be a minimum over a smaller set, and thus $opt(\mathcal E_\alpha') \geq opt(\mathcal E_\alpha)$. Since Lemma 3's proof actually says $\frac{SC(W, d)}{SC(B, d)} \leq 1 / opt(\mathcal E'_\alpha)$, using this inequality seems to imply the actual Lemma 3. Am I reading this correctly?

It is disappointing that the upper-bound for Copeland. It would be helpful if the authors can point out where the argument gets loose.

Limitations:
The paper stated its results very clearly and factually. I have no concerns about unaddressed limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper extends the framework of metric distortion, measuring how well voting rules minimize the social cost in a given metric space, to probabilistic voting scenarios where the preferences of voters are drawn from a probability distribution defined by the relative distances between candidates and each voter's ground truth position in the metric space.

The authors base their analysis on three different axioms that the induced marginal probabilities of relative preferences must verify, namely *scale-freeness*, *independence of other candidates* and *strict monotonicity.* They define a general class of marginal probabilities that verify the three axioms, and show that it encompasses the widely used *Plackett-Luce* model.

They then provide upper and lower bounds for the distortion of the *Plurality* rule, both linear in the number of candidates and matching asymptotically when the number of voters grows to infinity.

They then provide a upper-bound for the distortion of *Copeland* rule and show that it is independent of the number of candidates in the limit of a large number of voters.

Moreover, they give upper and (non-matching) lower-bounds for the distortion of *Random Dictator.*

They finally compare their results under both the *Plackett-Luce* and *Pairwise Quantal Voting* models (the latter being inspired form Quantal Response Theory), and show that the classical bounds of the metric distortion literature are recovered in the limit of vanishing randomness (although not for Copeland rule, hinting at a loose analysis).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper, in extending the metric distortion analysis to probabilistic voting models perhaps closer to reality, is rather original and proposes a more optimistic view of metric distortion where randomized dictator is beatable in a worst case distortion sense.
The paper is fairly well written and the proofs seem correct.

It is an interesting idea, with an interesting result.

Weaknesses:
The main problem of this paper is that it is not a good match to NeurIPS. This paper would work very well at an algorithms conference like SODA or ICALP, or a CS econ conference like EC (okay, probably one tier down like WINE), or possibly even at the those AI conferences that have a history in social choice theory like AAAI or IJCAI. And I also understand that NeurIPS has accepted such papers in the past. However, is it really a good fit for NeurIPS 2024? 

- There is no mention of the proof of Lemma 1 being in Appendix A.
- In the proof of Theorem 2, $\zeta$ is hardly introduced (also not in the Appendix).
- Formatting may be improved in place: e.g. Equations 6, 7, 10, or Theorem 3.
- l.312 ""converges to 9 instead of 5"". This part is not very clear, reminding the general bound in the deterministic case would improve readability.
- Typos:
	- l.220 ""and is by solving"".
	- Equation 24 showcases $(d)$ instead of $(a)$.
	- Equation 34: $\geq$ should be $=$.
	- l.618 ""LEt"".
	- l.641 should probably be deleted (equivalent to l.642).
	- l.660 weird grammar.
	- Footnote 6: missing index $\gamma_j$.
	- l.670: missing $(\hat{g}_{MID} +\hat{g}_{OUT}  )^2$ in inequalities $(a)$ and $(b).$ Furthermore, Equation 63 is used in $(a)$ rather than in $(b)$.

Limitations:
- The existence of distributions on rankings that generate pairwise order marginals of the form described in the paper is assumed and left for future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies the problem of metric distortion in single-winner elections. The key assumption is that the voters' preferences are not exactly compatible with the metric space, but they rather agree with it with a certain probability. The authors propose several axioms that formalize the requirements for the probability distribution for it to make sense in the context of distortion. Then they provide upper bounds of distortion in the probabilitic setting for Plurality, Random Dictator and Copeland (in case of the first two rules, they provide also lower bounds).

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
This work is the first one to combine probabilistic voting with metric distortion, hence the novelty is clear. The paper is overall of good quality, the axioms proposed in their work make sense to me, and the results are sound. The research direction introduced in this paper can be continued in further follow-up papers.

Weaknesses:
The paper could have been more clearly written --- for example, the formal notation should be introduced at the beginning of Section 2   (together the model) rather than in the middle of the introduction.

Besides, I think that Axiom 2 (Independence of Other Candidates) could have been better motivated. I can imagine that it was crucial to obtain the authors' results, but it seems rather natural to me that  in the real-life scenarios that motivated the research, the presence of additional candidates can impact the voter's probability of ranking one candidate over another.

Another weakness is that the authors only consider three rules, and the analysis of only two of them is complete --- many important rules (like Borda or PluralityVeto) are not considered at all. This could raise a question whether this amount of technical contribution is enough for a top conference like NeurIPS.

Limitations:
The authors adequately addressed the limitations and there are no potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers metric distortion in probabilistic models of voting. In the metric distortion framework the voters and alternatives are embedded in a metric space, and given the ranked preferences the goal is to find an alternative with low distortion. In this setting these rankings come from a probabilistic model.

In the first part of the paper, there axioms are introduced and authors show which axiom is satisfied by which probabilisitic model. In the second half of the paper the goal is to find the distortion of Plurality and Copeland rules for a specific class of probabilistic models. The results show matching upper and lower bound of $m$ for plurality and constant upper bound for Copeland.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
Defining a model for distortion in stochastic models is a useful idea in future analysis of voting systems.
The idea of the paper is novel and it uses novel techniques in the second half.
I like the definition of the three axioms. I find them natural and easy to understand.

Weaknesses:
My main concern is about the presentation. The preliminaries section is incomplete. The definition of distortion is hard to understand for a general audience and you made it harder by just putting the formula there. You have to add a description in words and give some intuition on why this definition makes sense. 

It's not clear how the probabilistic model works and how you define distortion on it until section 3 where you define it for a specific class. You have to formally define your probabilistic approach in Section 1.1 and also define distortion in this model. Not knowing the exact definition makes following the first paragraph of section 2 really hard. Before reading the rest of the paper I didn't understand why $P$ is a function of $d$ or why the preferences may not be consistent with the distances.

I think you have to add more intuition on the probabilisitic models. For instance you mention ground-truth in the definition of Mallows model but you have to explain in more details that how this model distributes the probabilities based on the distance to this ground-truth. The same explanations are needed for PQV.

My understanding is that the analysis that you provide works for any member of $G$, but currently the only members for which we have the final bound are PL and PQV. Is that true? If so is there any other interesting member of this class?

Limitations:
-

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
4dmwvbs4Ea;"REVIEW 
Summary:
This paper studies the offline policy optimization problem, i.e. to find a policy whose value function is close to the optimal value function using offline samples.  Under the assumption of linear MDP, they proposed a gradient ascent algorithm. 

The sample complexity of the algorithm only depends on the feature coverage of the best policy and does not require coverage over any other policies.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is well written. The algorithm, theorems and lemmas presented in this paper are all very clear.

The problem studied in this paper is offline policy optimization, which has significant value to the community, both empirically and theoretically. 

The algorithm in this paper is simple and computationally tractable.

In contrast to other offline RL paper, this paper does not require that the offline data is sampled i.i.d. or to be admissible, and they can handle arbitrary offline data as long as the data has sufficient coverage to the best policy.

Weaknesses:
Compared to Zanette (2021), the algorithm idea is somehow similar. Specifically, both algorithms use the actor-critic update, and the optimization in the algorithm of this paper is similar to the pessimism estimation in Zanette (2021).

The assumption made in this paper is the linear MDP assumption, which is stronger than the assumption in Zanette (2021).

Limitations:
Yes. The authors addressed all the limitations listed in the guidelines.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a new algorithm for offline reinforcement learning in linear infinite-horizon discounted MDP, which achieves a strong sample complexity under the weakest data coverage assumption. Moreover, their algorithm is easy to implement and computationally efficient. Their algorithm design is based on a reduced version of linear programming formulation of MDP, which approximately transforms the original problem into a unconstrained saddle-point optimization problem.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The algorithm proposed in this paper has many nice properties: it is computationally efficient, easy to implement, and works under the weakest data coverage assumption.
2. The developed techniques and the observations on transforming the optimization problem is insightful.
3. The paper is also well-written, with clear proof sketch and is well-positioned among related work.

Weaknesses:
The paper is pretty notation heavy and a bit hard to follow. I would suggest including a table of notations with descriptions. The choices of notation can also be optimized. For example, I found the mixed use of $D_{\pi}$ and $D_{\theta}$ confusing, and they looks like they are dependent on the value of $\pi$ and $\theta$.

Limitations:
Nothing necessary stands out.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides an approach for solving Offline RL problems in domains where the underlying MDP problem has reward and transition models that are linearly realisable under a known feature map. The paper is well presented.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Some of the tricks employed in the approach are quite interesting. 
2. The analytical results are good.

Weaknesses:
1. This work is only for MDPs where rewards and transitions are linear. Many of the moderately interesting problems are not linearly realisable, so it is quite important that authors provided a detailed discussion on how it can be addressed. 
2. I am not entirely confident about this, so will wait for inputs from authors. The approach seems to be built based on works by Hong and Tiwari, and stabilisation trick [Neu and okolo, Jacobson et al.]. I was not sure on the key significant contributions of this paper on top of those works. 
3. For me the biggest concern is that there are no experimental results. How would such an approach work for an MDP where the transition and reward models are not linear? Also, the approach is still approximate (given the bound), so would have been important to show the real results.

Limitations:
There is no limitations mentioned.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
VaXnxQ3UKo;"REVIEW 
Summary:
The paper proposes an innovative framework named AlphaMath, aimed at enhancing the mathematical reasoning capabilities of large language models (LLMs) without relying on expensive human annotations from domain experts or GPT-4. The authors leverage Monte Carlo Tree Search (MCTS) to allow a pre-trained LLM to autonomously generate process supervision and step-level evaluation signals, integrating a value model with the LLM. At the inference time, the authors propose a step-level beam search strategy, which assists the policy model in navigating more effective reasoning paths. Experimental results demonstrate that AlphaMath achieves comparable or superior results to previous state-of-the-art methods on various datasets without process supervision.

The application of MCTS in LLMs is now somewhat popular, with several highly relevant papers, such as [1] https://arxiv.org/pdf/2406.06592, [2] https://arxiv.org/pdf/2405.00451 and [3] https://arxiv.org/pdf/2309.17179] (note that [1, 2] are published after neurips deadline, so no need to perform comparison, It's just added for reference.)

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.The proposed method effectively eliminates the necessity for human or GPT-4 annotation, resulting in significant efficiencies in both financial and temporal aspects.
2. The utilization of Monte Carlo Tree Search (MCTS) can enhance the performance of LLMs in reasoning tasks by facilitating the search for optimal paths. Additionally, training with trajectories derived from MCTS is a commendable approach.
3. The introduction of step-level beam search presents a promising strategy to reduce computational costs.

Weaknesses:
1. The paper's results are all program-aided, a fact not clearly identified in the text. Consequently, the comparison with non-program-aided LLMs may not be fair enough.
2. The benchmark only includes GPT models in the 'Proprietary Models' section. It would be beneficial to include other LLMs such as Gemini and Claude for a more robust evaluation. 
3. An image illustrating the Step-Level Beam Search (SBS) would greatly aid in explaining the algorithm.

Limitations:
1. The framework does not generate a reward model that can facilitate other LLMs, which potentially limits its applicability.
2. The framework cannot be extended to tasks with open-ended outputs, limiting its potential use cases.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose the AlphaMath method, which leverages Monte Carlo Tree Search (MCTS) to improve the mathematical reasoning capabilities of large language models (LLMs) without requiring expensive human or GPT-4 annotations for process supervision. The framework integrates a value model with the LLM to generate and evaluate reasoning steps autonomously. Experimental results demonstrate that AlphaMath achieves comparable or superior performance to baseline methods on both in-domain and out-of-domain datasets.

To AC and the authors: The detailed comments are mainly covered in the **Questions** section. Please refer to the section for details. Generally speaking, I would consider the paper as very close to the threshold. If the authors can properly address the concerns in the below section, this could be a valuable study in this area.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
MCTS is a powerful algorithm for complex planning. Mathematical reasoning is a very important but challenging area. Even the most advanced LLMs nowadays can easily make mistakes when solving a mathematical problem. There are many recent attempts using MCTS on mathematical tasks, and this paper is a good one among many. The empirical results demonstrate the effectiveness of the proposed method.

Weaknesses:
Many presentations in the paper are somewhat unclear, requiring further clarification and elaboration. There are also some minor over-claims in the introduction. There are many modifications comparing to the original AlphaGo and Zero paper but the authors typically do not provide explanations for such changes. Some tables and figures are confusing. Please see the **Questions** section for the details.

One thing I'd like to especially point out is, the title ""almost zero"" is kinda misleading. One would interpret it as using very little data, e.g., a few seed data. However, the method would still need the full training set of GSM8K/MATH.

Limitations:
The authors provide a limitations section (A.1); however, it omits several critical aspects:

1. Methods that use the final answer's correctness to infer the reward or value of intermediate steps are prone to false positive and negative issues. Specifically, when the final answer is correct, all intermediate steps are rewarded to some extent. Nevertheless, false positive cases occur when models luckily guess the correct final answer despite incorrect reasoning. Previous related studies, such as Math-Shepherd and OmegaPRM, have acknowledged this limitation. It is recommended that this paper also addresses it in the limitations section.
2. The method relies on an automatic verifier to determine the correctness of the final answer. This verifier is not always applicable to many tasks, such as open-ended text generation (e.g., translation and summarization). The authors did not clearly explain the limitations concerning the scope of tasks.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper leverages the Monte Carlo Tree Search (MCTS) to iteratively train policy and value models by automatically generating process supervision and step-level evaluation signals, eliminating the need for human-annotated process supervision data. Specifically, the method combines the inner capabilities of pre-trained LLMs with the MCTS framework to generate correct and incorrect solution paths and optimize the models based on the node evaluations along these paths. To improve inference efficiency, the paper also proposes a step-level beam search strategy that allows the value model to assist the policy model in navigating more effective reasoning paths rather than relying solely on prior probabilities while avoiding excessive time consumption. Experimental results demonstrate that even without GPT-4 or human-annotated process supervision, this paper' s method performs comparably or better than existing SOTA methods across multiple in-domain and out-domain datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The work proposes a novel and efficient method that eliminates the need for human process annotations by leveraging the MCTS framework for model self-evolution.
2. The step-level beam search strategy significantly enhances the model's reasoning capabilities while maintaining computational efficiency.

Weaknesses:
1. Although the method does not rely on human-annotated process data, it still requires actual answers as reward signals. Future work could explore fully unsupervised approaches. 
2. Although the step-level beam search improves efficiency, the MCTS method still has some limitations in terms of computational complexity, which makes its practicality indistinguishable from majority voting.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel approach leveraging the Monte Carlo Tree Search (MCTS) framework to generate process supervision and step-level evaluation signals automatically, thus enhancing the mathematical reasoning capabilities of LLMs. This method bypasses the need for costly and labor-intensive manual annotations by allowing the models to train iteratively on both policy and value aspects, with an efficient inference strategy called step-level beam search. This approach demonstrates significant improvements on various datasets, showing comparable or even superior results to existing state-of-the-art methods without relying on external process annotations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper leverages a Monte Carlo Tree Search (MCTS) framework to automatically generate both the process supervision and step-level evaluation signals necessary for training LLMs. This approach avoids reliance on human annotated data by generating necessary training signals through interactions within the MCTS environment. As a result, the model achieves comparable or even superior performance to previous state-of-the-art methods on both in-domain and out-of-domain datasets.

Weaknesses:
1. Duplicate citations, such as [6], [7]
2. Appendix Figure 9, the reason why the performance of the third round model in MATH at Level 1 and Level 3, Counting & Probability, Number Theory and other categories has declined. Whether it will continue to decline after more rounds. 
3. The method integrates CoT and Code for cross-use. How does the performance of the model change if only CoT is used?
4. Evaluate on more out of domain benchmarks, such as GSM8k_hard and GSM8k_robust and OpenLLM Leaderboard
5. How much total training data is used. Explore the effect on other base model, such as Llama2, Mistral
6. Explore the effect of different values of the hyperparameter β in Eq. (8). 
7. what is the change in loss of Policy model and Value model during training.
8. What's the accurate of the Value model predictions based on the current State and Action.
9. Insufficient introduction of related work, such as Mathematical Reasoning and MCTS.
10. Differences reasoning ability and solution generation process across different rounds for the policy model , as well as the scoring differences by the Reward Model.
11. How the ground truth label scores for training the first round of the Value Model are obtained, and the procedures for the second and third rounds.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper utilizes Monte Carlo Tree Search (MCTS) to sample high-quality process-supervised data for iterative training, effectively reducing the dependency on GPT-4 and thus lowering the associated costs. Additionally, the authors propose an inference strategy, step-level beam search, which leverages value models to enhance the efficiency and effectiveness of the tree search process.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written, with a clear introduction to MCTS and the authors' proposed method, step-level beam search (SBS).
2. Their results effectively demonstrate the superiority of SBS over MCTS.
3. The model they trained shows a significant improvement over the baseline model, with an increase in performance from 33.2 to 53.6.

Weaknesses:
1. The comparison in the experimental results appears to be rather unfair. Generally, more computation leads to higher performance. In Table 2, the authors compare their fine-tuned model using SBS with greedy decoding for other open-sourced models. In Table 3, they compare SBS with majority voting. It is possible that the improvement stems from the reward model rather than the SBS algorithm itself. The authors should also report the results of best-of-n or weighted majority voting using their value model to demonstrate the effectiveness of SBS.
2. The experimental results are not detailed enough in Tables 2 and 3. The authors should also include major@n with different n values. Perhaps they can plot figures with $B_1$ and $n$ as the x-axis to provide a more comprehensive view.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
lTUXlmjcva;"REVIEW 
Summary:
This paper proposes the affinity score, which measures the non-linearity of an activation function $\sigma(X)$ given the distribution of $X$.
The affinity score is defined based on how well the 2-Wasserstein distance $W_2(X, Y)$, where $Y=\sigma(X)$, is approximated by $W_2(N_X, N_Y)$, where $N_X$ and $N_Y$ are Gaussian approximations of the distributions of $X$ and $Y$, respectively.
Note that $W_2(N_X, N_Y)$ has a closed-form solution, and it holds that $W_2(X, Y) = W_2(N_X, N_Y)$ if the relation between $X$ and $Y$ is locally affine on the support of the given $X$.
The authors then propose to characterize a DNN model by the set of affinity scores of activation functions in the model under a given input distribution.
Experimental results suggest that the affinity scores are relatively low in transformer-based vision models, meaning that the activation functions are used in a more non-linear region compared to CNN models.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
* The proposed score presents an interesting insight in comparing the series of CNN models and transformer-based models. Experiments suggest that transformer-based models utilize the non-linearity of activation functions more efficiently, leading to the higher prediction performance.

Weaknesses:
* It is empirically shown that the proposed score has a low correlation with existing non-linearity metrics such as R^2, but it is unclear whether the existing metrics are insufficient to analyze the DNN models in the way proposed in this paper. I would like to see how the distribution in Fig. 3(C) changes when other metrics such as R^2 are used instead of the proposed $\rho_{aff}$.
* In my opinion, one would expect the nonlinearity score to behave symmetrically at $x=0$ for activation functions like ReLU, but the proposed affinity score seems to have a lower score at negative $x$, as shown in Fig.2 or Fig.6. Is there any reasonable explanation for such a behavior of the proposed score?

Limitations:
See weakness above.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study proposes empirical statistics about different DNN architectures in the hope to shed some light into why some architectures are better than others for some computer vision tasks. To do so, the study leverages common optimal transport results on DNN's internal representations, under some strong assumption about the distribution of those representations.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper proposes to consider an interesting and useful question of going to the bottom of why some architectures are better than others as measured by some restricted downstream task.

Weaknesses:
- I do not agree with the following statement `Without non-linear activation functions,
84 most of DNNs, no matter how deep, reduce to a linear function unable to learn complex patterns.` as to me, models such as transformers with linear attention and linear MLP blocks have no actual nonlinearity but are higher order polynomial of the input, i.e., are not linear. Could the authors provide clarifications on that statement or did I misunderstand something?

- I also disagree with the following `Activation functions were also early identified [29, 30, 31, 32] as a key to making even a shallow
86 network capable of approximating any function, however complex it may be, to arbitrary precision.` since again, Fourier series for example can approximate any function as well. Hence DNN nonlinearities are certainly not the key ingredient to function approximation in general

- Many formal results such as Theorem 3.3 are well known and have been established for years (even decades) but no reference is provided which is misleading to the reader.

- Fig 2. is also misleading since the ""nonlinearity"" of any activation function depends on the range of the inputs. The only case that wouldn't be true is e.g. for ones with constant second derivatives, i.e., a linear activation function.... hence again that statement is highly misleading in presenting ReLU as inherently benefiting form that property compared to others

- the statement `No other metric extracted from the activation functions of the
260 considered networks exhibits a strong consistent correlation with the non-linearity signature.` is again an overstatement as the authors only compare with a few alternatives and theorem is provided to support such a statement

- the statement `We proposed the first sound approach to measure non-linearity of activation functions in neural
270 networks` is also incorrect, see e.g. 
  - https://jmlr.org/papers/v20/18-418.html
  - https://arxiv.org/pdf/2301.09554
  - https://arxiv.org/abs/1810.09274
  all the above works have been published in peer reviewed journals/conferences

Limitations:
In addition to my concerns expressed above, the study does not provide any actionable insights or understanding on the ""why"" of different architectures performing differently beyond the proposed statistical numbers. How could one use the provided analysis to better design model architectures or for model selection? 

Also, the paper does not provide any novel theoretical results. All the major theorems and results are already widely known within the community, yet they are presented as part of the contributions. With that in mind, the paper solely leverages existing OT tools, with some underlying simplifications on the DNN's data distribution, and report computed metrics. Hence the study falls below acceptance level in my opinion and would need a major rewriting + additional novel contributions to be worth acceptance.

The writing style is also filled with unsupported claims and highly misleading statements (see the **Weaknesses** examples).

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel method for quantifying the non-linearity of activation functions in neural networks, termed the ""non-linearity signature."" Using an affinity score derived from optimal transport theory, it measures the non-linearity of individual activation functions. It defines the non-linearity signature as a comprehensive set of these scores across all functions in a deep neural network (DNN). The study compares these signatures across a range of popular DNN architectures in computer vision, revealing clear patterns in their evolution over the past decade, notably showing a trend towards decreasing non-linearity until the disruptive impact of vision transformers. It emphasizes the uniqueness of their measure, as it does not strongly correlate with other metrics across different architectures. The approach could potentially be applied to analyze the non-linearity of newer large language models (LLMs) and identify innovative neural architectures that optimize internal non-linear characteristics for enhanced performance, crucial in the era of costly experiments with large-scale model optimizations.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Novelty and importance. The paper introduces a theoretically grounded measure, the affinity score, for quantifying the non-linearity of activation functions using optimal transport theory, providing a robust framework for analysis. This is the first approach to approximately measure the non-linearity of DNNs, which is crucial for understanding their inner workings.
2. Solid theoretical and experimental validation.  The method is grounded in optimal transport theory, providing a rigorous theoretical foundation for the proposed non-linearity signature. This enhances the credibility and robustness of the findings. The experimental results demonstrate the practical utility of the non-linearity signature. It can predict DNN performance and meaningfully identify the family of approaches to which a given DNN belongs, making it a valuable tool for researchers and practitioners.
3. Clear Writing. The structure of the paper is well-organized, with a clear presentation of background knowledge, theoretical properties, experimental evaluations, and conclusions. This clarity aids in understanding the contributions and implications of the research. The paper's figures and tables are comprehensive, providing clear and precise information, and the writing maintains a coherent logical sequence.

Weaknesses:
1. The authors should discuss more activation functions. Currently, only ReLU, Tanh, and Sigmoid are included.  While these are among the most commonly used activation functions in neural networks, many other activation functions have been introduced and shown to be effective in various contexts, like GELU. Including a more comprehensive analysis of a diverse set of activation functions would enhance the robustness and applicability of their proposed method. 
2. There is currently some research on the nonlinearity of deep neural networks that should be compared and discussed.
3.  It would be beneficial to showcase examples from domains beyond computer vision. While the paper focuses on computer vision tasks, it may not address the non-linearity signature's applicability to other domains such as NLP, speech recognition, or reinforcement learning. The findings might be less generalizable if the proposed measure does not perform equally well across diverse types of tasks and data.

Limitations:
Yes, they have discussed the assumption of Theorem 3.3.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
3mzFmBPFIX;"REVIEW 
Summary:
This work presents a method for learning metriplectic systems from data. Metriplectic systems are a model which conserve energy and produce entropy, two desirable features. Their method, termed “neural metriplectic systems” (NMS), is based on a more efficient system parametrization. The authors also prove universal approximation results on non-degenerate systems, and generalization error bounds. They verify that their method outperforms other metriplectic-learning baselines, GNODE and GFINN, on two physical experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Originality: Although I am not at all an expert in this field and therefore cannot properly judge, it seems that the main theorem (Theorem 3.4) is novel and non-trivial.

Quality: The proposed method outperforms baselines in two experimental settings, verifying the expected gain from having a more efficient parametrization. The corresponding theoretical results, on universality and generalization, provide a fairly thorough picture of the method. 

Significance: Within the field of learning metriplectic systems, this paper seems to make a valuable contribution and improve on prior work.

Clarity: The paper is very well-written, and mathematically rigorous. Although the details are not accessible to someone without a background that matches the subject material rather closely, the high-level ideas about the benefits of metriplectic systems, what past work has done, and the advantages of their new method, are conveyed well.

Weaknesses:
Clarity:
1. Although well-written, the paper is not accessible to most machine learning audiences, and seemingly requires the reader to already have a physics background in phenomenological modeling, or exterior algebra.

2. Mathematical terms such as algebraic brackets, Poisson brackets, degenerate metric brackets, etc. should be defined in the beginning of the paper, or with a reference to a textbook or other paper defining them. The “exterior algebra” background is suitable for only those with a strong mathematical background already, using terms like “wedge product” and “graded algebra” without definition. (Admittedly, it would be impossible to fully explain all of these concepts in only 9 pages — perhaps a citation to a textbook would be helpful here, but in practice if the reader needs to understand the decomposition result properly to grasp the contribution, then this work may be more suitable for a venue other than a machine learning conference.)

Quality: The baselines in experiments, as well as the methods discussed in the exposition, are all metriplectic. However, it seems like other methods (e.g. which preserve energy but do not increase entropy, or even those which are not physics-informed at all), should be included too.

Significance: I am not sure how widely applicable metriplectic learning systems are, or what alternative (non-metriplectic) methods can be used for the same problems. The paper would be improved by providing more of this background/motivation.

Overall, as a non-expert, my main concern is with the suitability of this work for a machine learning conference - I defer to the AC on this point. It seems that the machine learning techniques used within NMS are fairly straightforward, while it is the parametrization in Theorem 3.4 that seemingly constitutes the crux of the method. However, the statement and proof of Theorem 3.4 would be more accessible to a physics or math audience, than an ML audience.

Limitations:
Limitations are discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new parameterization for neural metriplectic systems which explicitly incorporates structural information about the degeneracy conditions $\{ S, \cdot \} = 0, [E,\cdot ] = 0$ into the model. The model requires $\sim O(n^2)$ learnable parameters for a problem with $n$ state variables instead of some prior methods which need $O(n^3)$. Further, it also encodes this degeneracy condition in a hard constraint, leading to models which will by construction respect these desired physical conditions. The authors provide a deep learning implementation scheme for their method which involves learning $E(x), S(x)$ and using $\nabla E, \nabla S$ to construct the matrices $L, M$ needed for the bracket from observed trajectories of the physical system. The gradients $\nabla E, \nabla S$ needed for the brackets are computed with autodifferentiation. The authors show that this system is trained end-to-end on simple physical systems including a two-gas system and a thermo-elastic pendulum and can outperform existing methods on these benchmarks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper motivates the need for metriplectic systems which can be efficiently implemented and that incorporate the dynamical constraints required for these systems (energy conservation and entropy production). This captures a potentially interesting class of physical systems that could be modeled by machine learning methods like those employed in the present work. The method that they present as Algorithm 1 is straightforward and improves upon the cubic time complexity of GNODE or GFINN. The authors also provide an approximation result for their algorithm and support their claims with some experiments.

Weaknesses:
While the authors improve the scaling from cubic to quadratic in the number of state variables, the total complexity (quadratic) still scales poorly with size of the problem (number of state variables / dimensions). Further, the current experiments and comparisons were performed on small benchmarks. However, since this paper is the first to point out that the cubic scaling can be improved by reflecting constraints due to degeneracy, I think the experimental component of the contribution is not the most important.

Limitations:
The authors do mention the primary limitations of this present work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a parameter efficient parameterization for neural networks simulating metripletic systems, which are differential equations that have both an energy dissipative piece and an energy conservative piece. The method works by learning several of the required quantities (L and M, which trade off dissipation and conservation, I believe), while also using a small neural network to estimate the dissipation and conservation pieces (E(x) and S(x) ). As not all quantities in the state, x, can be observed, they use a time based diffusion model to emulate the hidden states (e.g. entropy) to develop initial conditions for these. Experiments are performed on two systems of this class, where it seems like the method performs better (probably due to having better inductive biases).


Unfortunately, due to not having a strong physics background, I feel somewhat unqualified to judge many of the technical strengths although things seem reasonable from a skim. I don’t know if I can properly assess novelty and significance as a result.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Significance:
-	Building better emulators of physical systems that are complicated is a good first step in what the authors term “phenomenonological” understanding of these systems.

-	Even quicker training time (and demonstrated both practically and theoretically) is quite helpful. I remember one of the original issues with NODE was that it took a very long time to converge.

Clarity: 
-	Overall, the paper is pretty well written, even if quite dense, and okay to follow for a non expert physicist. I was able to follow at least the ML pieces and the experiments section quite well.

-	The relevant literature is reasonably well signposted; I learned a fair bit about the state of this field by checking the references.

Novelty:

-	The approach seems to have a clear inductive bias win over the prior works GFINN and GNODE due to better parameterization of the system.

Weaknesses:
Unfortunately, the writing ends up being quite dense and technical with minimal outside applications. 

Sure, emulating these physical systems in the experiments is quite nice, but what types of applications does this lead to? This is more of a writing based thing and the paper could be refactored around one of these applications if possible.

Limitations:
n/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
34dHGTri2w;"REVIEW 
Summary:
This paper presents a novel parallel sampling method named ""Follow Hamiltonian Leader"" (FHL) designed to address sampling challenges by leveraging zeroth-order information, particularly when first-order data is unreliable or unavailable. The method incorporates a leader-guiding mechanism to enhance the efficiency and effectiveness of the sampling process. Experimental results indicate that FHL significantly improves the exploration of target distributions and outperforms traditional sampling techniques, especially in scenarios involving corrupted gradients.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Innovative combination of zeroth and first-order information.
2. The effectiveness of the method is demonstrated in multiple task scenarios.
3. Theoretical analysis and prove are sufficient.

Weaknesses:
1. Is there any quantitative experiments like evaluating FID and IS on cifar10 datasets and I think it's more compelling whether a novel sampling methods combined with generative models can be used on image datasets with more complex distributions.
2. Lack of experiment of OOD in combination with EBMs or score-based models to valid the stability during sampling with proposed method.

Limitations:
1. Limited exploration of integration with other advanced MCMC methods.
2. Lack of quantitative experiments to demonstrate the advantage of proposed sampling method compared with other methods.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an interesting parallel sampling method that leverages zeroth-order information to address challenges in sampling from probability distributions, particularly when first-order data is unreliable or unavailable. The method incorporates a leader-guiding mechanism, enhancing efficiency and effectiveness by connecting multiple sampling instances through a selected leader. The proposed method, named Follow Hamiltonian Leader (FHL), extends the Hamiltonian Monte Carlo (HMC) framework by concurrently running multiple replicas at different energy levels and combining both zeroth and first-order information from various chains. Experimental results demonstrate that FHL significantly improves the exploration of target distributions and produces higher-quality outcomes compared to traditional sampling techniques, showing resilience against corrupted gradients and excelling in scenarios characterized by instability, metastability, and pseudo-stability.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed Follow Hamiltonian Leader (FHL) method markedly improves the efficiency and effectiveness of sampling processes, significantly expediting the exploration of target distributions and producing superior quality outcomes compared to traditional sampling techniques.
- FHL demonstrates greater resilience against the detrimental impacts of corrupted gradients by incorporating zeroth-order information. This robustness makes the method particularly valuable in scenarios where first-order information is compromised, ensuring more reliable and accurate sampling.

Weaknesses:
- The proposed FHL method involves intricate modifications to the traditional Hamiltonian Monte Carlo framework, such as the leader-guiding mechanism and elastic leapfrog technique, which may increase the complexity of implementation and require significant computational resources.
- The effectiveness of the FHL method heavily relies on the appropriate selection of the leader particle. If the leader is not accurately chosen, it could lead to suboptimal sampling performance, potentially compromising the overall efficiency and accuracy of the method.
- While the paper presents experimental results to demonstrate the efficacy of the FHL method, there is a lack of in-depth theoretical analysis to rigorously establish the convergence properties and performance guarantees of the proposed approach.
- The method’s scalability to high-dimensional problems or extremely large datasets is not thoroughly addressed. The parallel sampling approach may encounter challenges in maintaining efficiency and effectiveness as the dimensionality and size of the data increase.

Limitations:
The authors have not adequately addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes to incorporate the energy $U$ into the gradient-based sampling techniques. In particular, it proposes to choose the lowest energy particle as the leader and then add an extra elastic tension between the leader and followers in the Hamiltonian Monte Carlo method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The idea is simple and clear, the toy examples are easy to understand and demonstrate the benefit of the proposed method well. In addition, the authors conduct experiments for each of the three challenging sampling scenarios identified by the authors.

Weaknesses:
It might worth including the overhead of the proposed method, how much slower the algorithm is per iteration compared to HMC for instance.

The tension coefficient $\lambda$ is critical, setting it to 0 recovers the baseline. But I did not find an ablation over the $\lambda$, is it hard to choose? From my understanding, if you set $\lambda$ pretty large it might recover something like gradient descent and the sampling will collapse.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents an interesting new approach for improving sampling methods for energy-based generative models and score-matching models. The key idea is to incorporate zeroth-order information (energy values) in addition to the typical first-order gradient information used by most sampling algorithms like Hamiltonian Monte Carlo (HMC).

The authors identify several challenging scenarios where relying solely on gradients can be problematic - cases of instability, metastability, and pseudo-stability. They argue that incorporating energy values can help mitigate issues in these situations and improve sampling efficiency and quality.

Overall, the core idea of leveraging zeroth-order information in addition to gradients is quite novel and the FHL algorithm is an elegant way to implement this for improving sampling efficiency and quality. The paper is well-motivated, the method is clearly explained, and the empirical results are compelling.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Novel idea of incorporating zeroth-order energy information into sampling algorithms like HMC, which typically only use gradients. This can help address issues like instability and metastability.

2. The Follow Hamiltonian Leader (FHL) algorithm is an elegant way to exchange both energy and gradient information across parallel sampling chains in a principled manner.

3. Thorough experimental evaluation across synthetic examples illustrating the identified challenging scenarios of instability, metastability, and pseudo-stability.

4. Promising results showing improved sampling quality over baselines for energy-based generative models on real datasets like CLEVR.

5. Clear motivation and well-explained methodology.

Weaknesses:
1. It would be better to show exploration of the sensitivity to key hyperparameters like the number of parallel sampling chains.

2. Discussion of computational cost/overhead compared to baseline sampling methods are missing in the manuscript.

Limitations:
Based on the provided paper, the authors do not appear to have explicitly discussed the limitations or potential negative societal impacts of their work. The paper is primarily focused on presenting the technical details of the proposed Follow Hamiltonian Leader (FHL) sampling algorithm and its empirical evaluation.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
cCOpatbXFU;"REVIEW 
Summary:
This paper investigates a new definition for the stochastic gradient variance in mirror descent.
Most existing analyses for stochastic mirror descent require a strongly convex distance generating function to bound the gradient variance.
This limits the their applications especially when this assumption fails.
In particular, Le Priol et al. (2021) have shown that the none of the existing convergence rates applies to Gaussian maximum likelihood.

This paper aims to fix this issue by proposing a new definition of gradient variance.
They show that the new definition is strictly stronger (more likely to hold in practice) than existing definitions, and derive convergence rates in convex setting.
The authors demonstrate an application of the new variance definition bounding the estimation error of MAP for one-dimensional Gaussian distributions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Analyzing stochastic mirror descent is hard when the distance generating function is not strongly convex.
This paper is a step towards generalizing mirror descent analyses.
In particular, I like Section 2.2 where the authors show that the proposed definition is strictly better than existing ones.

- The mirror descent analysis in this paper yields a non-asymptotic bound for the estimation error of Gaussian MAP. This seems to be a fundamental problem lacking theoretical guarantees based on Le Priol et al. (2021).
However, I am now knowledgeable enough to confirm the significance or novelty of this result in statistics.

Weaknesses:
While developing the new gradient variance definition is certainly interesting, I have the following concerns.

- The authors have shown that their gradient variance \\(\sigma_{\star, \eta}^2\\) is finite for every fixed step size \\(\eta\\).
The convergence rates in the convex setting are proved using constant step sizes, and thus the optimality gap does not vanish.
To make the optimality gap vanish, diminishing step sizes are often required, which is not covered in this paper.
Proving convergence with diminishing step sizes probably requires characterizing the average variance \\(\frac1T \sum_{t=1}^{T} \sigma_{\star, \eta_t}^2\\), which I think can be done only on a case-by-case manner depending on the specific application.

- The only case so far where this new definition shines while all other definitions fail is maximum likelihood estimation for one-dimensional Gaussian distributions.
This is very restrictive.
Is it possible to generalize this result to multivariate Gaussian distributions?
In addition, it would be great if the authors could provide other applications to further justify the necessity of this new definition.

Minor:
- Line 192: Add a period.
- Bad notation in Section 4.2: It might be confusing to use $\Sigma$ to denote the standard deviation.
Consider using a different letter like $s$ or $\tau$.

Limitations:
NA.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work revisits Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduces a new (less restrictive) definition  of variance which can generally be bounded (globally) under mild regularity assumptions. Then this paper investigates this notion in more details, and show that it  naturally leads to strong convergence guarantees for stochastic mirror descent. Finally, this paper leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance.

Problem:
In proof of Proposition 2, by the definition of $\sigma_{*,\eta}^2$, we can obtain that
$ \sigma_{*,\eta}^2 = \frac{\min_x f(x) - \min_x f_\eta(x)}{\eta}  $. 
However,  $x_* =\argmin_x f(x)$ does not equal to $x_*' = \argmin_x f_{\eta}(x)$.
This will lead to $\sigma_{*,\eta}^2 \neq \frac{1}{\eta^2} D_h(x^*, x^+）$.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This work revisits Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduces a new (less restrictive) definition  of variance which can generally be bounded (globally) under mild regularity assumptions. Then this paper investigates this notion in more details, and show that it  naturally leads to strong convergence guarantees for stochastic mirror descent. Finally, this paper leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance.

Weaknesses:
No.

Limitations:
No

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a new analysis of SMD using a newly introduced generalized variance notion. The benefit of the new analysis is demonstrated in the application to maximum a posteriori estimation of Gaussian parameters.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
After introducing a new variance notion, the paper delves into comparison with other existing notions and shows that the proposed one is the largest meaningful notion. After a careful comparison, analysis of SMD is presented using this mild assumption. This analysis substantially departs from the results known in the literature. The demonstration of the use case of this new theory in the context of statistical estimation is also clear and adds more significance to the new theory.

Weaknesses:
Major:

As explained after theorem 4.3, the guarantees are derived for a reverse KL and may not imply anything on the desired quantity $f(\theta) - f(\theta_*)$. This of course, limits the contribution in this application significantly as non-asymptotic rates were known before. 

Minor problems that I hope the authors can fix in the next revision. 

1. Is the set C compact? If not, why the minimum exists in Proposition 2.2?

2. Cannot find where $x_*$ is defined. Why does it exist? 

3. There is a small issue with indicies in equation (12) and in paragraph before. $\eta_{n} = \frac{1}{n_0+n+1}$, and the stochastic gradient should depend on the new sample $X_{n+1}$.

Update: meaningful results are obtained only for relatively strongly convex case (which is a stronger assumption than even strong convexity). In the convex case, a different (much stronger) definition is used. This becomes clear only after reading Appendix D. This limitation should be clarified in section 3.2, where convergence on some surrogate loss is shown. I will update my evalutation.

Limitations:
n/a

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new variance assumption for the analysis of stochastic mirror descent (SMD) to handle cases where standard bounded variance assumption does not hold. The authors show this new assumption can be shown to hold under some regularity assumptions. The authors use the new results to show some convergence guarantees for MLE and MAP of a Gaussian with unknown mean and variance using the connection between this problem and SMD convergence guarantees.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The topic is definitely interesting and timely. Results for stochastic optimization without bounded variance assumptions are quite interesting. As shown in the prior literature, this task is especially subtle in the Bregman case. As the authors argue in detail, this difficulty is acknowledged in previous works such as [7] and [17]. It is neat that the authors show the importance of the new results by deriving convergence bounds for MAP/MLE of a Gaussian with unknown mean and variance by using the connection between these bounds and SMD in [17] (which itself is a nice connection). This adds a nice and clear motivation. The work makes some progress towards solving open questions from [17], while as the authors clearly explain, the open questions are still not completely solved.

Weaknesses:
I find the motivation of the paper and its application to MAP bounds interesting, however I have some concerns about writing and the strength of the derived results in the context of the application in Section 4. It seems necessary for the latter point to be clarified.

- Authors write after Theorem 4.3 that the open problem from [17] is not completely resolved because  the convergence is not shown for the desired quantity. In particular, the authors describe that the guarantee is for $D_A(\theta_*, \theta^{(n)})$ instead of $D_A(\theta^{(n)}, \theta_*) = f(\theta) - f(\theta_*)$. The authors then write that two quantities can be related asymptotically but they state: ""but we might also be able to exploit this control over the course of the iterations"". Can you make this point more precise? It is not clear to me what this last part is trying to describe. Is it meant to be understood as an open question or is it possible for the authors to derive the stronger result? Since the paper mentions at many places that showing convergence guarantees for MAP is an important contribution of the paper, it is important to justify the convergence metric used in the results for justifying the contribution of the paper fully.

- It might be better to replace MLE in the abstract to MAP since Section 4 is mostly about MAP.

- Abstract states a couple of times ""strong convergence"", I suggest to remove this since ""strong convergence"" has a precise meaning in infinite-dimensional optimization and usage in the abstract is confusing because of this. Clearly this is not how the authors are using this term, but it seems authors are using this as a subjective adjective, which is not necessary. By subjective, I mean that: how can one decide what convergence result is strong and what is not?

- Assumption 1 requires all $f_\xi$ are convex. This is rather strong since the standard assumption is $\mathbb{E} f_\xi$ to be convex. Can you discuss this more? According to Prop 4.1, this holds for the main application of the paper, but it might be worth discussing why componentwise convexity is needed.

Limitations:
The limitations are discussed clearly. The authors provided explanations after Theorem 3.3 and Theorem 4.3 to describe the limitations of their result.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This submission studies stochastic mirror descent (SMD) under quite mild conditions on the mirror map and objective function. More specifically, there are a variety of SMD analysis in the literature, but virtually all of them require strong conditions on the mirror map (such as strong convexity) that do not hold in cases where we only have relative smoothness (and/or relative strong convexity) of the objective function with respect to the mirror map. The authors propose a definition of variance of SMD that is better behaved under minimal assumptions. They show how this new variance can be used to obtain general convergence results for SMD. Finally, they show how the new variance definition for SMD can show some kind of non-asymptotic convergence rates for MLE and MAP of Gaussian parameter estimation with unknown mean and covariance, making partial progress on a conjecture posed by Le Priol et at.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
This is an interesting paper that tackles a hard theoretical problem. I think it is of interest for researchers interested in mirror descent. The new definition of variance of SMD has interesting properties even under very mild assumptions, as the authors show when comparing the new definition with other definitions of SMD variance in the literature. Moreover, the results on Sec 4 already show how this is an interesting way to analyze SMD, and is likely to lead to follow-up work on the area.

So the strengths summarized in bullet points:
- Thorough comparison of new variance definition with other definitions in the literature and proof of finiteness under assumption 1. 
- General convergence theorems of SMD under mild assumptions that recover known results in the deterministic case, showing this is may be a ""natural"" variance definition for SMD and useful for our understanding of SMD.
- Partial progress towards the conjecture of Le Priol et al.

Weaknesses:
In its current form, I have one main concern with the paper:
- Despite what is written at the beginning of the paper, **Assumption 1** is NOT a blanket assumption used throughout the paper. In fact, it appears only section 2 uses assumption 1. The rest of the paper uses a weaker assumption that is never clearly stated, which makes it hard to understand when the results hold or not. 
This is likely to be a problem with presentation, but in its current form it is often not clear what are the assumption required at each point. Since the main point of the paper is to use a minimal number of assumptions, it is very important for those to be clearly stated. 

A minor weakness is the lack of an example besides MAP/MLE. I could not easily think of a concrete example where I could apply the convergence results in sec 3 or 4. If the authors have an example besides MLE or MAP (even if a bit artificial), it would be great. For example, some example with a mirror map such as $- \log x$ would be interesting, but this is a minor suggestion, since it would be nice to see a concrete example of the use of the results in Sec 2 (the results in Sec 4 require a specialized bound on the variance) 

Summary of weaknesses:
- Unclear requires assumptions for many of the results
- (Minor weakness) Lack of a concrete (even artificial) example of application of any of the theorems in Sec 3 beyond MAP/MLE (and the latter require specialized bounds on the variance).

Limitations:
Although the authors are not explicit about some of the limitations of the results on sec 3, they do discuss how to interpret some of the results and limitations from their convergence rates.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
Rllg9nOasE;"REVIEW 
Summary:
This paper introduces a novel approach using cross-modal and multi-modal models to align brain activity with naturalistic stimuli, evaluates several unimodal Transformer models, and examines the effects of removing unimodal features from multi-modal representations on brain alignment.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- (S1) The paper presents a thorough imaging data analysis, including a detailed description of the dataset, a comprehensive comparison of methods, and a clear summarization of results, which enhances the robustness and transparency of the study.

- (S2) The paper is well-written and clearly presented, making the complex methodologies and findings accessible and easy to understand for readers.

- (S3) The study provides a rigorous comparison of methods, including various method variations, which highlights the strengths and weaknesses of each approach and demonstrates the thoroughness of the analysis.

Weaknesses:
- (W1) Figure caption should be more demonstrative and detailed. For example for Figure 1's captions, ""Residual analysis"" is too vague for the reader to understand the figure. For Figure 5, if the colorbar range from -0.5 to 0.5, then it does not represetn the percentage, but the proportion instead.

Limitations:
The Limitation section is properly included in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a framework for applying brain encoding models with multimodal stimuli. They apply this to a series of video, audio, and mutlimodal models (cross-modal and jointly embedded models). They introduce a residual analysis to analyze the impact each particular feature had on the corresponding fit in the encoding model. They find that multimodal models significantly out-perform their unimodal counterparts on certain language- and vision-related regions in their fMRI dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Incorporating a new collection of models used for fits to the brain for comparison.
* Expanding to multimodal models, a relatively new space.
* Incorporation of video/speech models, allowing to capture input stimuli over time and removing problems with parsing the stimuli into individual modalities such as ImageBind or VideoMAE.
* Interesting results as seen in Figure 3 showing improvement across several brain regions with multimodal networks. Figure 2 also shows really interesting results with language and visual regions separated.

Weaknesses:
* Clarification on feature removal: I think I found the feature removal description in this paper and prior papers a bit confusing and want to ask for some clarification. I wish more space was spent on that in this paper to provide more intuition. I think some extra descriptions would be useful here. See questions.
* In general, I am quite skeptical of how well the feature removal works. For example, there is no guarantee that the features are completely removed in the residual analysis. I would actually like to see a probing analysis to actually establish that the feature is removed. 
    * Furthermore, the method of projection is rather confusing. The authors use a regression to “project” unimodal video features (referring to figure 1) into the same space as the multimodal feature space. I don’t think this is necessarily wrong but potentially unreliable without any extra metric to establish how well this works. Having some MSE score or pearson correlation (with the averaged embedding) could help understand how well the projection worked. 
    * In my opinion, I wonder why the opposite direction wasn’t taken: instead, project the video features out of the cross-modal/jointly pretrained multimodal representation. You could train a projection matrix to do so using your current vision-language data. To me, this is cleaner and easier to interpret primarily because you aren’t dependent on the quality of your visual representations to capture visual information. 
* The paper compares multimodal and unimodal models to demonstrate improvement in brain alignment. One explanation for this improvement could be an improvement in unimodal processing. For example, one interpretation of the current results is that a multimodal model such as TVLT has better visual processing than ViT-B (as an example).  Is this addressed by feature removal? I’m not sure it is. Some extra text to discuss this would be useful. Some extra discussion on model performance would also be useful. 
* Baselines
    * The paper doesn’t consider the baseline comparison with randomly initialized models. Why? I think this is a very important baseline for characterizing architectural bias. This was also done in prior works.

Limitations:
* I believe these are addressed adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The manuscript investigated the process of multi-modal information in human brains through predicting neural responses based on semantic features extracted by existing models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The problem is interesting.
2. The results show insights into brain region's roles in processing multi-modal information.

Weaknesses:
## Major
1. The method builds ridge regression based on features extracted by pretrained models. However, I am worried that the findings will be affected by choice of pretrained models. It is important to demonstrate the replication of different pretrained models.
2. For some observations in Section 6, the author only presents the observations and does not give insights based on the observations. For example,
    - What does observation i) in lines 311-312 indicate?
    - What does observation ii) in lines 313-314 indicate?
    - Why is AC an exception for observation (1) in lines 316-317?
    - For observation (2) in lines 320 -322, why is TVLT different from IB-concat, given that both of them contains multi-modal information?
3. Why does the author choose ridge regression instead of more complex machine learning models? Is it possible that more intricate interactions of features extracted by pre-trained models are not captured by a ridge regression model, potentially affecting the results? And if you choose a more complex model, the rank of alignment scores of different models could be altered.
4. I do not know if it is too hard or even impossible, but it would be better to check if the results consistent with some existing neuroscientific findings.
5. In section 6.3, why do IB-concat and TVLT act differently given that they are both multi-modal representations.

## Minor
1. There seems to be a trailing 3 in Fig.3's caption.
2. The author moves the results of some brain regions in Figure 3 to the appendix due to the page limit. Since the author refers to those regions from the main text, it would be better to still include those regions in the main text in my opinion.

Limitations:
The authors have adequately addressed most limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses an important question of how accurately multi-modal models can predict brain activity when participants are engaged in multi-modal stimuli. The key challenge is how to integrate or separate the information from different sensory modalities. This work explored two types of models, ie cross-modal and joint pretrained models. Through extensive experiments, this paper found some things that are important to unveil the brain encoding principles, which are important to the AI community.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper writing good, and research problems are well explained.
The encoding pripline is clearly illustrated.  
Experimental designs are insightful.

Weaknesses:
- My major concern is about the train-test settings. There exist `clock’ (temporal) relationship which might lead to information leakage during inference. This paper did not mention how to advoid such an issue. 

- The data collection process should be blocked to aviod inter-data correlation, espeically for joint-modal training. The three settings mentioned in the paper do not really account for the speciality of brain signals.

Limitations:
See above comments

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
rC0OM4Jm4v;"REVIEW 
Summary:
The paper proposes to employ text-to-image latent diffusion models to augment images through a controlled modification such that the resultant class is different from the source class. Such augmented images are referred to as hard negative images. Building upon SDEdit style image modification, the paper controls the extent of modification by adaptively determining the appropriate noise-scale for each image separately. The benefits of this type of augmentation have been demonstrated on few-shot and long-tailed imagenet classification tasks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The paper is very well written presenting the core idea of generating hard-negative images by modifying an image with a caption of another class. This idea is simple, intuitive and interesting. 
- Furthermore, the algorithm to determine the optimal noise-level for each image adaptively is not only simple and intuitive but also effective in eliminating dependence on hyperparameters. I feel that a connection can be made to the recent work [1] on phase-transition in diffusion models since this algorithm is attempting to find the diffusion-time when phase-transition occurs. 
- The evaluation is comprehensive considering a variety of diffusion-augmentation baselines as well as traditional augmentations.
- The paper illustrates the effectiveness of the adaptive search procedure through separate experiments with DINO-v2 and visualisations.
- In many cases, synthetic data generation with a diffusion model may be replaced by a simpler retrieval baseline [2]. However, the goal of this work is to use a diffusion model to search for and generate hard negatives, which is an interesting deviation from some of the previous synthetic data augmentation approaches. 

[1] Sclocchi, Antonio, Alessandro Favero, and Matthieu Wyart. ""A phase transition in diffusion models reveals the hierarchical nature of data."" arXiv preprint arXiv:2402.16991 (2024).

Weaknesses:
- From the various results in the paper, it seems that the Text2Image, GeNIe, and GeNIe-Ada achieve comparable performance with respect to each other on average. This seems to suggest that the majority of the gains can be attributed to the increased number of _distinct_ examples --- as compared to regular augmentations which simply apply different transformations to the same image --- for each class rather than the hard-negatives in GeNIe/GeNIe-Ada. 
- Additionally, it seems that beyond some threshold, any value of $r$ that changes the source-image to the target image yields comparable performance indicating that it may be sufficient to generate an augmentation that is similar to source-image and it need not specifically be a _hard-negative_.  It may be useful to consider some other applications where images lying in the boundary of the classifier may be informative: for example, see recent work on generating outliers [1] for OOD detection. 
- GeNIe-Ada algorithm is compute-intensive as compared to a simple Text2Image augmentation since it requires generating several augmentations for each source image before selecting one optimal augmentation that lies on the decision boundary. Given how close the text2image and genie-ada performances are in some cases, it may be possible that we could generate more augmentations using text2image in the same compute budget and improve over GeNIe. 
- (minor) GeNIe is applicable to the fine-tuning stage rather than the pretraining stage.

[1] Du, X., Sun, Y., Zhu, J. and Li, Y. Dream the impossible: Outlier imagination with diffusion models. NeurIPS 2024.

Limitations:
Yes, limitations are addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces GeNIe, a data augmentation method for training vision models using synthetic images. GeNIe generates images by combining a source category image with a target category text prompt, selecting those that feature source characteristics but belong to the target category as negative samples. Experimental results show that GeNIe improves performance in both few-shot and long-tail distribution settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
* The proposed GeNIe improves the performance in few-shot and long-tail distribution settings.
* The paper provides extensive experiments to support the claims,  including the selection of noise levels.
* The paper is well-written and easy to follow.

Weaknesses:
The key idea of GeNIe is to use image editing to combine features from two categories. Here are several questions:

* Regarding controllable image augmentation
  * Line 9 mentions that GeNIe ""retains low-level and background features from the source image."" How does GeNIe control which features are retained or changed?
  * To combine features from different categories, how about adding the attribute from the target category to the prompt? For example, a ""[dog] with [wings]"".  This method does not require carefully selection of denoise steps. 
  * Other image editing methods, such as those in [1] and [2], efficiently control image changes using prompts or user instructions.  For example, they can transform a car into a motorcycle in Figure 2, while keeping the background unchanged for more challenging negative samples. What advantages does GeNIe offer over these methods?



* GeNIe generates images ""using images from all other classes as the source image"" (line 227). Will all (source image, target prompt) pairs lead to effective image generation? Which types of pairs contribute the most to the final accuracy?

     [1] Prompt-to-Prompt Image Editing with Cross-Attention Control

     [2] InstructPix2Pix: Learning to Follow Image Editing Instructions, CVPR 2023

Limitations:
The paper has discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the idea is to generate data for data augmentation by utilizing a pre-trained diffusion model. The method employs different text prompts and an adjusted noise scheduler to generate hard negative samples for the source distribution. ""GeNIe"" creates new augmentations using diffusion by leveraging source images and contradictory target prompts. ""GeNIe-Ada"" adjusts noise levels on a per-sample basis, using the classifier as the condition boundary to select the right threshold.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The method offers infinite possibilities to separate the source from the target.
- The idea is simple, original, and convincing.
- The ablation studies and experiments demonstrate strong performance.

Weaknesses:
- The method is slow, particularly GeNIe-Ada, as it requires generating an image through multiple forward passes of a diffusion model and using a classifier to select the appropriate threshold $r$.

- The number of steps required to retain low-level features is crucial for optimizing the method's performance.

- The method relies on access to a foundational text-to-image model trained on billions of images.

Limitations:
/

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel augmentation method based on diffusion models. A latent diffusion model conditioned on a text prompt generates hard negatives, by adjusting the noise level. The hard negatives can be used as challenging augmentations. The authors demonstrate the effectiveness of their approach on long-tail and few-shot settings.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Well-written paper with clear contributions and presentation.
- Extensive experiments and evaluation.
- Interesting and useful idea.
- Code included in the supplementary.

Weaknesses:
I am generally happy with the paper, experiments, and presentation. A weakness seems to be the selection of the noise ratio r.  The authors propose an algorithm for this. However, I am concerned how sensitive it is for different datasets or classification settings. This might affect performance in other settings or in real-world scenarios. If this is true, it might degrade the overall method's usefulness.

Limitations:
The authors have added a section for limitations and a section for broader impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
NGuGVT7ar2;"REVIEW 
Summary:
Traditional large language models (LLMs) struggle with tasks requiring visual and spatial interpretation based solely on text. This study introduces a visual-augmented prompting (VAP) strategy, using an external image generation tool to iteratively create intermediate visual representations that aid reasoning. VAP's effectiveness is validated in four tasks: (1) Geometry Intersection Counting, (2) Sudoku Puzzles, (3) Time Series Prediction, and (4) the Traveling Salesman Problem.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
(1) The VAP method is simple yet effective and well-motivated, aligning with human cognitive processes. It promisingly integrates both intermediate textual and visual results to enhance the accuracy and interpretability of the reasoning process. 

(2) Results clearly show that VAP significantly outperforms all baselines across four tasks.

(3) The manuscript is well-written and easy to follow.

Weaknesses:
(1) Efficiency of Iterative Reasoning: The author does not mention the time consumption of the iterative reasoning process of VAP. Additionally, I am curious about the trade-off between the number of images that need to be drawn and model performance.

(2) More LLM models (such as LLaMA 3, GPT4) need to be incorporated for more comprehensive comparison.

(3) Effect of Different Figure Drawing Tools: I am interested in understanding how different graphic rendering tools for image drawing affect the final reasoning results.

(4) Method Scalability on Complex Geometry Problems: Figure 4 shows a significant accuracy drop for VAP when the number of shapes exceeds three. This raises concerns about the scalability of VAP in handling complex geometry problems.

(5) Although VAP performs better than the baselines, its overall performance is still poor and has a noticeable gap compared to traditional methods.

Limitations:
The author has a section that addresses most of the limitations of the VAP method. However, I suggest that if the VAP method is time-consuming in terms of the iterative figure drawing process, the author should discuss the efficiency problem in the limitations section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper targeting an intresting topi in VL research: Can VLM understand the organized prompts as LLM? The authors proposed a method called VAP(visual augmented prompt) to improve the prompting learning methods for VLM. The authors argue that human have two specialized subsystems that process verbal inforation and visual-spatial information respectively. Thus, to mimic human's decision making capability, the proposed method will synthesize images and organized a chain-of-thought in both modalities. This method comprises three steps: 1. selecting an anppropriate drawing toolkit and creating an initial image; 2. Iteratively perform reasoning on the synthesized images, and generates a paragraph of accompaning text for the generated image; 3. Finally, feed all the intermediate thoughts and images to the model as input to formulate the COT process.
This method is tested on 4 different tasks to show its general capability.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. Though COT and prompt learning is not a novel topic in LLM. How to organize and build a CoT process for VLM to effectively activate the reasoning skill for VLM. While most recent work focusing on leveraging the language side to generate the rationale process. This work focusing on how to coordinately generate both visual and language rationale. This is a quite good idea.
2. The proposed method reflect the human decision process and thus sounds quite reasonable and novel.
3. The proposed method increase the results on different tasks by a large margin, even compare to many advance prompting and CoT styles. These solid increment prove the effectiveness of this method.

Weaknesses:
1. As mentioned by the authors. The image generation process not always controllable and could generate error-pone content to mislead the thinking process. Therefore I wonder, is generating a explicit image the best practice for this process. As the generation of images could result in unwanted features, we could also keep the representation in the latent space as a prompt.
2. The method relies on a planner to plan what to draw for the whole process. However, a planner itself can be incorrect and we simply have no control on this planner and the generated plan. Is there a more formulated way to generate the plan, or do we have somewhat of tools to detect the possible problem?

Limitations:
Not applicable

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new prompting technique, vision-augmented prompting (VAP), to improve the reasoning capabilities of large language models (LLMs). Different from the mainstream chain-of-thought (CoT) frameworks that only involve textual reasoning steps, the proposed VAP framework automatically generates images from visual and spatial clues via external tools. In addition, the VAP framework feeds a chain of thought with both textual and visual context into LLMs to solve the original problems. Evaluations on four tasks (i.e., geometry, sudoku, time series prediction, travelling salesman problem) demonstrate that the proposed VAP outperforms prior CoT frameworks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed VAP framework improves the traditional chain-of-thought (CoT) prompting via augmenting visual context. Specifically, the VAP generates high-level drawing planning from the textual question, and then iteratively outputs instructions to draw the stepwise visual context and provide related textual thoughts. The visual contexts with textual context (i.e., thoughts) provide richer information than the text-only context in the CoT framework and helps the reasoning capability of the LLM (MLLM) models.

2. As the iteratively generated visual and textual contexts may contain errors and lead to wrong reasoning, the authors propose a self-alignment mechanism to check if the visual and textual contexts align with the initial high-level drawing planning. If not aligned, the iterative reasoning procedure will be restarted accordingly. This mechanism helps to improve the iterative reasoning and results in a more accurate conclusion.

3. In the evaluation section, the authors conduct a comprehensive comparison between the proposed models and multiple CoT frameworks on 4 versatile benchmarks. In addition, the authors also introduce several task-specific baselines during the comparison and it further validates the effectiveness of the proposed VAP framework. Moreover, the authors conduct human analysis on some generated drawing and observe an impressive integrity rate. These evaluations are helpful to understanding the strengths of the VAP framework.

4. The paper is well written and easy to read.

Weaknesses:
1. In the VAP framework, it chooses one of three tools (matplotlib, turtle, and dalle3), but the manuscript does not provide much details about how these tools are used
- For matplotlib and turtle, code generation is needed. However, how can the framework guarantee that the code has the right syntax to properly generate the image?
- For dalle3, how is the prompt generated for the image drawing? 
- What is the distribution of calls on these three different tools?
- In each iteration of iteratively reasoning, does the selected tool draw a complete image, or overlay the new drawing on image from previous iteration?
- Can the framework use a mixture of different tools in resolving one problem?

2. In the evaluation, the questions from all benchmarks are provided in text format. I wonder if the authors can include benchmarks with both text and image in question (e.g., VQA benchmarks). This is a fairer setting since the current baselines ignore the visual capability of the GPT4v model.

3. In ablation studies, there is one experiment that removes the planning step. I wonder if the authors can provide more details. Does it mean that a different set of iterative reasoning prompt is used?

4. Typos: Line 294: is plays -> plays / ""Self-alignment plays an important role to the task of Geometry Intersection Problems"" Shouldn't it be ""Sudoku"" since self-alignment achieves the largest improvement (25.1% -> 35.5%) on Sudoku?

Limitations:
The authors mentioned and attempted to address several limitations in the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes visual-augmented prompting (VAP) for large language models (LLMs) in reasoning tasks. Specifically, VAP translates textual questions into a sequence of self-synthesized images using API calls (Python Turtle, Matplotlib, DALL-E3). These images are then fed back to Vision-LLM (GPT-4o) in a step-by-step manner as deduction steps. The detailed VAP process includes (1) planning, (2) iterative reasoning, and (3) conclusive reasoning. Experiments on several math tasks, such as geometry intersection counting, Sudoku puzzles, time series prediction, and the traveling salesperson problem, demonstrate that VAP helps LLMs perform better than chain-of-thought (CoT) and tree-of-thought (ToT) methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper extends Chain-of-Thought (CoT) with visual prompt information. In addition to the step-by-step textual deduction in CoT, it uses drawing APIs (e.g., Python Turtle, Matplotlib, DALL-E) to synthesize pictures in the intermediate steps, helping to derive the final answer for mathematical problems. This approach is interesting and has not been explored before.

2. Experiments on math-related problems, such as geometry intersection counting, Sudoku puzzles, time series prediction, and the traveling salesperson problem, demonstrate that VAP outperforms CoT and ToT methods.

Weaknesses:
1. When the LLM generates Python API calls, there is a chance that the generated code may not run successfully (bugs in the code). What is the probability of this phenomenon occurring in the experiments?

2. Generalization: CoT and ToT are more generalized to different LLM tasks, while VAP is limited to a few geometry problems. Does it generalize to normal Visual QA tasks?

3. There is a missing reference to relevant works on tool usage ability for LLMs, such as ViperGPT [1], VisProg [2], and LLava-Plus[3].


[1]. ViperGPT: Visual Inference via Python Execution for Reasoning
[2].Visual Programming for Compositional Visual Reasoning
[3]. LLaVA-Plus: Learning to Use Tools for Creating Multimodal Agents

Limitations:
The authors discuss and address some of the limitations of VAP.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zDYXdR3ClP;"REVIEW 
Summary:
This paper introduces a universal image restoration framework UIR-LoRA based on multiple low-rank adapters. UIR-LoRA employs the pre-trained text-to-image diffusion model SD-turbo as the shared component. It utilizes a LoRA composing strategy based on the degradation similarity predicted by CLIP encoder to combine different LoRA modules. Experiments show the effectiveness of the proposed method.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed LoRA-based Universal IR method is easy to understand and follow.
2. The motivation of this paper is very clear to me.

Weaknesses:
1. UIR-LoRA adopts SD-turbo as the pre-trained backbone for image restoration. However, SD-tubo utilizes VAE with high compression rate to encode input images, resulting in severe detail distortion for image restoration. This issue has been widely discussed in recent published works [1,2]. However, the paper ignores this very important issue in the Method Section and only mentions the skip-connections for VAE in Line 223.
2. The degradation-aware router seems to be unreliable. I do not believe that the original pre-trained CLIP Text Encoder can distinguish between different degradations through degraded text representations, such as ""rain"" and ""raindrop"". Therefore, DA-CLIP fine-tunes the original CLIP. But this paper doesn't contain any discussions about this.
3. This paper does not provide complete technical details, such as how the LQ image is used as a condition for SD-turbo. Is ControlNet used, or is it directly concatenated? I do not see any information about this in the paper. 
4. Tab. 1 only reports the trainable Param for UIR-LoRA. I think it's necessary to report the overall Param of the model. In addition, the reported PSNR for DiffBIR is very low. Did the authors add skip-connections to the VAE of DiffBIR for a fair comparison?
5. The visual results in Fig. 3 seem strange. The visual results of Restormer show noticeable artifacts between patches. Do the authors test Restormer using a tiled mode? As far as I know, using a single A100 GPU (Line 251), Restormer can restore the entire image without encountering out-of-memory issues.

[1] Wang, Wenjing, et al. ""Zero-Reference Low-Light Enhancement via Physical Quadruple Priors."" In CVPR, 2024.

[2] Geng, Zigang, et al. ""Instructdiffusion: A generalist modeling interface for vision tasks."" In CVPR, 2024.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes to perform universal image restoration via multiple low-rank adaptation. The key idea is to leverage a pre-trained stable diffusion model as the shared component and transfer it to specific degradations with LoRA adaptation. A degradation-aware router is further proposed to generate weights for LoRA combination based on degradation confidence. In experiments, the authors evaluated their method on multi-degradation and mixed-degradation datasets and conducted several ablation experiments on their core components.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The idea of applying LoRA to a pre-trained SD for multi-task image restoration is promising and interesting.
- The overall presentation is easy to follow.
- The experimental results are good and the ablation studies make sense.

Weaknesses:
- ControlNet is the most popular approach to adapting SD models to other tasks. I'm curious why the authors chose LoRA? As far as I know, LoRA is often used for large language models (with billions of parameters). It would be great to provide more detailed motivation in the introduction.
- In line 123, maybe it's better to use ""concatenate"" or other operators instead of ""add"" to present the unified parameters. Here, the weight $s_k$ can be ignored.
- Can the authors use other SD models as the base model? I believe applying LoRA to a multi-step diffusion process can further illustrate its efficiency.
- In Eq. (4), $s_0 \cdot M_k$ is used in both numerator and denominator, which seems weird and confusing.
- The mixed degradation experiment is cool. It would be interesting if the authors could apply their model to real-world degraded images.
- Line 45: proposed -> propose

Limitations:
See Weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This submission proposes a transfer-learning based strategy to address challenges related to image-degradation restoration. The premise is that a pre-trained generative model can be employed as a common starting component for multiple degradation types, upon which distinct sets of trainable parameters (ie. low-rank adaptors) can be added in order to address specific-degradation restoration tasks. Mixed-degradation restoration is enabled through a top-K hyperparameter, that affords a mixture of (degradation) experts to be active. The experimental setup considers multi and mixed image restoration problems where average results are offered across image-degradation datasets and appropriate standard quantitative metrics, qualitative examples, are reported in comparison with alternative approaches.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The technique described for piping specific samples down specific low-rank adaptor chutes is relatively easy to understand and yet reportedly results in competitive restoration accuracy for investigated datasets. 

* Nascent investigations into mixed-degradation image restoration problems provide a promising seed to be followed.

* The writing is of a reasonable standard.

Weaknesses:
* The key idea of leveraging pretrained VLM features (and specifically CLIP) for the task of image restoration from multiple degradations, pre-dates the current submission [R1]. While authors clearly go to some length to highlight their alternative CLIP-based scheme, which amounts to envoking specific (pre-existing [R2]) low-rank adaptors, the core technical contributions here can be regarded as somewhat limited.  

* The phrase 'Universal Image Restoration' may not be a sufficiently accurate (or modest) description for the proposed method. The submission collates ten different image restoration tasks which, despite vague statements in the abstract, remains a 'multi-task' not a 'universal' setup. Samples for all ten degradation tasks are shared between train and test (Sec. A.1) and individual task adaptors appear to be trained independently on task-specific datasets (L188--196). Generalisation ability to previously unseen degradations is also not considered. Suggest method description requires reworking.

* The claim that multi-task learning (MTL) frameworks, designed to handle image restoration for multiple degradations, share all parameters across different degradations (L029) is incomplete and somewhat misleading. Several existing MTL works (eg. [R3,R4]) make use of both shared and task-specific parameter subsets for multiple image restoration tasks. Indeed 'which proportion of parameters should be shared and which should be task specific' can be considered a fundamental (and long standing) MTL question. The idea of benefiting from commonalities between image restoration tasks is well understood and my concern is that this casts doubt on a core premise of the submission. 


References

R1. Controlling Vision-Language Models for Multi-Task Image Restoration. ICLR 2024.

R2. LoRA: Low-rank adaptation of large language models. ICLR 2022.

R3. All in One Bad Weather Removal using Architectural Search. CVPR 2020.

R4. Pre-Trained Image Processing Transformer. CVPR 2021.

Minor:

L076: 'draining' --> 'deraining'

L099: 'mim' --> 'min'

L238: 'aspects' --> 'aspects.'

Limitations:
Half of one sentence (L293) is apportioned to discussing method limitations. See above for suggestions on components that might make for valid additions here.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes universal image restoration framework using multiple low-rank adapters that learns task specific weights from to perform multi-domain transfer learning. the proposed method leverages the pre-trained generative model weights as the shared component and adapts it task specific low-rank adapters. At each layer in the restoration pipeline the proposed method uses the degradation similarity to combine LoRA adapters outputs, this enables the proposed to handle for mixed degradation restoration.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper proposes LoRA adapters to learn task specific weights and proposes a strategy to combine the adapter outputs using degradation similarity measure
- extensive experiments are performed showing the proposed strategy works better than random and average in table 3.
- extensive experiments are performed to show the proposed methods performance against the sota methods in table 1 for mutliple degradation task.
- Extensive experiments are performaed showing impact of LoRA rank and prediction accuracy

Weaknesses:
- In table of the paper authors compared proposed method against sota on REDS and LOLBlur datasets, both these datasets have mixed degradations  of blur, jpeg compression, noise, and low light. Although these comparisons performed on mixed degradations, it would be helpful to how the proposed method performs on mixed weather conditioned images (MID6), which is comparatively challenging than REDS and LOLBlur  datasets. 
MID6: Multimodal Prompt Perceiver: Empower Adaptiveness, Generalizability and Fidelity for All-in-One Image Restoration, CVPR, 2024.

- Can authors confirm, whether network re-trained seperately for each experiment in table-1,  and table-2 separately, i.e. table-1 and table-2 trained network weights for proposed method are different.

Limitations:
- authors have addresed limitations

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a framework to improve image restoration across various degradation types using Low-Rank Adapters (LoRA). The proposed method adapts a pre-trained generative model to each degradation type. It performs a weighted sum of the output of adapted models using the estimated degradation of input images. The proposed method performs impressive results in restoration accuracy and resources.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The proposed method is interesting and reasonable.
Experimental results support this paper's contributions and the proposed method's effectiveness.

Weaknesses:
In Table 3, the 'Top-1' strategy performs almost the same as the 'All' strategy, which limits the motivation of the weighted sum of the adapted models.
Table 6 presents the restoration performance comparisons for each degradation. The proposed method underperforms previous works in significant degradation types such as blurry, low-light, raindrop, and rainy.
The average scores might mislead the evaluation performances.

Limitations:
The proposed method is simple and effective, but evaluating average scores on multiple degradations can mislead its contribution.
The proposed method achieves near-best performance by selecting a single adapted model but underperforms in many major degradation types.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
3Rtn1OMTC4;"REVIEW 
Summary:
This paper studies how to extract useful visual features from out-of-domain and action-free human videos to enhance robotic visualmotor control. Specifically, the authors argure that naively extracting spatial features via MAE is insufficient for robotics control, in contrast, jointly captureing spatial control and temporal movement will be more effective. To do so, the authors propose STP, a new self-supervised learning method, that simutaneously performs MAE on current frame to extract spatial information and predict furture frames to extract temporal motion clues. The overall motivation, idea and method are straightforward and reasonable. The authors evaluate STP on diverse benchmarks including 21 tasks spanning from simulation to real world tasks using imitation learning.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-motivated, highlighting the importance of pre-training visual features for robotic foundation models.

2. The logic in the paper is clear and easy to follow.


3. The proposed method is straightforward and simple to implement.

Weaknesses:
1. The high costs associated with evaluating real-world tasks using different random seeds make it challenging to report variances. However, assessing the impact of multiple random seeds in simulated tasks could provide more reliable statistical insights. As shown in Table 1, STP's performance improvement over baselines is marginal (STP 63.7 vs. VC-1 61.8, and STP-L/16(Post PT) 78.4 vs. MAE-L/16(Post PT) 76.7). Given the inherent stochastic nature of imitation learning and reinforcement learning, evaluations across multiple episodes and various random seeds are crucial to validate the proposed methods effectively.

2. Some previous methods also consider the temporal movements when extracting the visual features. For instance, the video-language alignment loss in R3M [1] tries to align language with correct visual transitions, which can extract semantic informations about visual movements. Voltron[2] and DecisionNCE [3] also try to extract the semantic features of the temporal movements between two frames. VIP[3] and LIV[4] use RL to extract visual features, which may also capture long-term movements via bootstrapping. Therefore, the authors could strengthen their paper by highlighting these related works, demonstrating awareness of existing methods, situating their contributions and highlighting the differences between STP and these baselines.

[1] R3M: A Universal Visual Representation for Robot Manipulation. CoRL 2023

[2] Language-Driven Representation Learning for Robotics. RSS 2023.

[3] DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning. ICML 2024.

[4] VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training. ICLR 2023.

[5] LIV: Language-Image Representations and Rewards for Robotic Control. ICML 2023

Limitations:
The authors have properly discussed the limiations in the Appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a new spatio-temporal pretraining algorithm for representation learning for robotics. The authors propose using masked autoencoding for reconstructing the current frame (for spatial reasoning) and a future frame (for temporal reasoning). The authors provide extensive experimentation across simulated and real-world settings and provide ablation studies to justify their design choices.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper addresses the important topic of including temporal dynamics in video data for pretraining robot representations.
- The paper does a good job of explaining the method and detailing the various experimental settings.
- The authors provide policy performance using both the pre-trained representations and post-pre-trained representations which helps assess both the quality of representations learned from internet data as well as the advantage of finetuning representations on the task-specific data. Overall, the proposed method has been extensively evaluated over varied settings across a variety of simulated settings.
- The authors provide an insightful ablation study to justify their design choices.

Weaknesses:
- It is unclear where the diverse image data for STP trained with Ego+I in Table 1 is obtained from. Some information about this would be helpful.
- The real-world experiments seem limited with only two real-world tasks where the MAE also performs reasonably well.
- The authors must include comparisons with prior works using MAE for spatiotemporal learning [1].

[1] Feichtenhofer, Christoph, Yanghao Li, and Kaiming He. ""Masked autoencoders as spatiotemporal learners."" Advances in neural information processing systems 35 (2022): 35946-35958.

Limitations:
The limitations have been addressed adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes STP, a visual representation learning method for robotic motor control. Trained on human videos, STP uses masked auto-encoders for spatial-temporal prediction. The spatial decoder predicts the current frame from its representation with 75% of patches masked. The temporal decoder predicts the future frame using the representations of 75%-masked current frame and the 95%-masked future frame. Experiments on various simulation and real-world tasks show the effectiveness of STP compared with baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposed method is simple yet effective, utilizing a masked spatial-temporal prediction objective to learn visual representations for robotics.
2. The paper presents extensive experimental results in both simulation and real-world settings, comparing with proper visual representation baselines.

Weaknesses:
1. Many works have considered temporal information for robot visual representation learning. This paper should mention these and highlight the differences. For example, R3M [1] uses temporal contrastive learning, while VIP [2] and V-PTR [3] use temporal difference.
2. Though STP outperforms the baselines in many benchmarks, the performance gap is not significant (Table 1). The slight performance difference may be due to hyperparameter selection and randomness, as the paper did not provide error bars over multiple seeds.

[1] R3m: A universal visual representation for robot manipulation, 2023
[2] Vip: Towards universal visual reward and representation via value-implicit pre-training, 2022
[3] Robotic Offline RL from Internet Videos via Value-Function Pre-Training, 2023

Limitations:
The authors have discussed the limitations. These cannot be addressed within the scope of this paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, we present a self-supervised pre-trained visual representation in robotic motor control, with spatiotemporal prediction with dual decoders, utilizing large-scale video data. The spatial prediction follows a standard MAE pipeline, and the temporal prediction tries to predict the future based on the current frame. The trained encoder is applied to downstream tasks and real-world robot task for better sample efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper adopts actionless human video data for representation learning, which can be easily obtained. The learned representation can be adapted to downstream robotics tasks. 

2. The experiments contain several real-world tasks, which could be more valuable for applying a pre-trained visual encoder to real-world domains that lack data.

Weaknesses:
1. The major concern is the novelty of the previous methods, considering several related papers that leverage human data and visuals pertaining to downstream tasks have been proposed [1-3].

2. The experiment only contains imitation learning experiments in downstream tasks, while the reinforcement learning framework with sub-optimal data is not considered. 

[1] Learning Manipulation by Predicting Interaction. RSS 2024

[2] Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning. https://arxiv.org/html/2402.14407

[3] Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation. https://arxiv.org/abs/2312.13139

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
nw9JmfL99s;"REVIEW 
Summary:
This paper investigates when localized receptive fields arise in supervised neural networks. Extending a recent work of Ingrosso and Goldt, the authors propose that simple single-neuron models learn localized receptive fields when trained on data with sufficiently negative excess kurtosis, while if the excess kurtosis is sufficiently positive they learn delocalized receptive fields.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The topic of this paper is of broad interest in both machine learning and neuroscience, and on the whole I think this manuscript makes a worthy contribution on top of the work of Ingrosso and Goldt. There are some weaknesses which dampen my enthusiasm (see below), but on the whole I favor acceptance.

Weaknesses:
I have two primary concerns about the results presented:

- First, Lemma 3.1's treatment of time is not sufficiently precise. Can you provide a more precise answer than simply ""early in training"" or ""before A.3 is violated""? Indeed, the logic in the paragraph beginning on Line 174 is not clear. What you show is that, in some cases (see concern below), eq. (5) generates localized RFs in a similar location to those observed in actual training. This does necessarily mean that ""the gradient flow in Eq. (5) holds sufficiently long to detect the emergence of localization in the weights,"" as you write in Lines 177-178. Moreover, you have not in fact defined what you mean by ""detect the emergence of localization;"" this must be reified. Can you see a change in participation ratio, even if only numerically, before the approximation breaks down? 

- Second, the experiments are rather limited, and rely largely on exemplars rather than systematic statistical investigation. This is important given the gap in Lemma 3.1: the authors rely on experiments to justify their claim that this approximation provides meaningful information about when localization will occur, but all they actually show is that there is resemblance in a few cases. The paper would be much stronger if the authors could also show that their claims hold statistically over many realizations of the data generation process and training procedure.

Limitations:
The authors do a largely adequate job of discussing the limitations of their work, up to the technical weaknesses noted above.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
SETTING: Olshausen & Field famously showed that requiring natural image patches to be constructed from a small number of independent elements from an overcomplete dictionary populates that dictionary with spatially localized feature detectors.  But localization also appears in DNNs trained on discriminative-learning tasks.  It is also ubiquitous in the cortex.  This raises the question of what fundamentally drives the formation of localized receptive fields.

APPROACH: The MS attempts to explain the emergence of localized RFs by reference to the statistics of the stimuli (inputs).  In particular, the authors examine the time evolution of network weights trained with gradient descent to discriminate ""natural-image-like"" stimuli--analytically for a single (ReLU) unit of neural network, and in simulation for a weighted sum of multiple such units (a two-layer NN with one output unit).

More precisely, the authors consider stimuli/inputs with circulant covariance matrices, a 1D idealization of natural images (the covariance matrices of natural images are block-circulant with circulant blocks).  Under some additional simplifying assumptions (see below), they derive an ODE for the time evolution of the weights when the network is trained to discriminate stimuli according to whether their spatial correlations are ""long"" or ""short.""  They show that this ODE depends only on the marginal distribution of a single ""pixel"" rather than the joint (and is valid until localization begins).  Investigation of this ODE (Appendix C.1) reveals that localization will occur (w large for some entries, small for the rest) when the inputs have (sufficiently) negative excess kurtosis.  The authors also demonstrate this in simulation with data distributions with varying levels of excess kurtosis, both by examining the learned receptive fields and by numerically integrating the ODE.  Finally, they extend their simulations to two-layer NNs.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The MS persuasively presents a new route to localized receptive fields.  The proofs look correct (but I did not verify every line) and the simulations confirm that they are not undermined by the various approximations employer along the way.  The problem is an important one and of long-standing interest to the field (computational neuroscience).

Weaknesses:
This reviewer struggles to see how these results can be generalized beyond this very simplified setting of, essentially, a ""network"" Y = ReLU(<w, X>) (or its cousin, the SCM).  Introducing just one more learnable layer breaks (or at least vitiates) the connection between stimulus kurtosis and localized receptive fields (as the authors show in Fig. 6).

This makes it hard to find this explanation of RF localization more compelling than the classical one in terms of efficient codes.

Limitations:
N/A

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors analytically derive the learning dynamics of (extremely) simple neural network models and characterize conditions under which units learn localized receptive fields. This builds on celebrated work in computational neuroscience on sparse coding and independent components analysis, offering a new perspective for biological findings.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is beautifully written (but see comments about figures below). It is well-motivated and accessible to a broad audience. The target audience for this kind of work may be somewhat niche, but it fits within the scope of the NeurIPS conference. The authors summarize prior work concisely and clearly. The assumptions of the analysis are stated precisely. The proofs in the appendix are well written, and the main claims of the theory are tested experimentally (though see below for some areas I am a bit unclear on).

Weaknesses:
To enable analytic tractability, the paper relies on very strong and simplifying assumptions. Figures 4, 5, 6, and the right hand side of Figure 3 are hard to see. It could be helpful to add color and show fewer lines. Also, the description of these figures is the one part of the text I had trouble following. Quantifying the outcomes somehow would be helpful for me to understand exactly what I'm supposed to be looking at in these examples. For instance, in Figure 4A, the claim is that the model learns an oscillatory filter that resembles a sinusoid. But the left panel in Fig 4A, though oscillatory, doesn't look sinusoidal?

Ultimately I believe these weaknesses can be addressed during the rebuttal phase and the strengths outweigh the weaknesses.

Limitations:
Limiting assumptions are clearly stated and discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
u5BkOgWWZW;"REVIEW 
Summary:
The authors propose a general framework to generate explanations of different text datasets, by parameterizing the data distribution with textual predicates. The textual predicates, along with their weights, can be viewed as an explanation of the data. Three different tasks are used as examples: clustering, time-series modeling, and multi-class classification. To learn the textual predicates and their associated weights, the authors first iteratively learn the predicates in the embedding space along with the weights, and then use a pre-trained language model to discretize the predicate embeddings into textual space. Experiments are conducted on three datasets that have ground-truth predicates. A qualitative experiment with user-chatgpt dialogues is also presented.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed framework nicely generalizes from the classical Bayesian text analysis methods and makes them more relevant/useful in the LLM era.

2. Comprehensive experiments are performed on four different datasets/tasks.

3. The paper is well-written and easy to follow.

Weaknesses:
1. Missing pure LLM baselines. It is fairly common nowadays that LLMs are directly used to generate explanations given the data, which can be applied to any dataset and any task. More evidence is needed to show the advantages of the proposed method compared to direct prompting. For example, for the clustering task, what if you perform clustering first, and then prompt GPT4 to directly explain each cluster? 

2. The explanations generated are mostly topic descriptions, and the datasets used for qualitative results are quite simple. Can the proposed method generate more complex descriptions for more difficult datasets? For example, for a STEM problem-solving dataset (e.g. MATH), can the proposed method explain the dataset by the theorem used/subarea of the subject/similar solving strategy? 

In summary, I like the way the proposed method combines the classical Bayesian method with LLM prompting, but I'm not totally convinced that the proposed method is significant as there is no direct comparison with LLM prompting, or evaluation on harder datasets. I'm willing to raise my score if my concerns are properly addressed.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes to use ""natural language predicates"" to explain text datasets. Authors develop a model-agnostic algorithm that optimizes continuous relaxations of predicate parameters with gradient descent and discretizes them by prompting language models. With this method, the authors can characterize the dataset with a distribution over predicates. The authors apply it to taxonomize real-world user chat dialogs and show how the text data's features can evolve across time, demonstrating the potential of the method in understanding text data.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper proposes a new method to explain text datasets. There are a few new concepts in the paper, such as ""Predicate-Conditioned Distribution"" and the corresponding optimization algorithm that involve both continuous and discrete optimization.

2. A practical application of the method is demonstrated on real-world user chat dialogs. 

3. The problem is well-formulated and backed by enough mathematical and empirical evidence.

Weaknesses:
1. The motivation of the paper is somewhat unclear. It's not well explained in the Introduction section why people want to ""Explain text datasets"". Is it for controlling the dataset quality for pretraining? Or tailor the domain of dataset for finetuning LM? Although the authors showcase the method on a dataset of real-world user-ChatGPT dialogues, the results look arbitrary and not very insightful. Only ""Taxonomizing User Applications via Clustering"" and ""Characterizing Temporal Trends via Time Series Modeling"" are still not fully demonstrating the usefulness of the method.

2. The method should be compared with more baseline methods. It's unknown what's the benefit of predicate-conditioned distribution over other methods. The authors should also explain why the ""predicate"" is the key to explain text datasets, instead of other factors such as just verbs, nouns, text representations, etc.

3. The datasets being tested are relatively small and not showing the scalability of the method.

Limitations:
Limitations are being discussed in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a a framework for a family of models parameterized by natural language predicates. This models allow for a language-based interpretation of text distribution. Such framework can easily instatiate models include clustering, time-series, and classification models. The authors develop a model-agnostic algorithm that optimizes continuous relaxations of predicate parameters with gradient descent and then discretizes them by prompting language models (LM). This approach is demonstrated to be effective in taxonomizing news dataset as well user chat dialogues while also characterizing their evolution over time.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
I find the paper to have strengths along several axes:

- Novelty: The general framework is, to the best of my knowledge a novel approach to describe text datasets in a way that is interpretable and scalable.

- Model-Agnostic Algorithm: The algorithm developed to optimize the models proposes is versatile and can be applied to various types datasets under different approaches, namely classification, clustering and time-series analysis.

- Extensive experiments and interesting ablations: The authors demonstrated the practical utility of the approach by applying it to real-world datasets (user-LLM chats), as well as news data for comparison purposes.

- The results show improvements over the baseline and when this is not the case Bills and NYT dataset for clustering in table 2, the difference is either minimal or compensated with the generality of the proposed method.

Overall the paper is well written and easy to follow. The results well presented and support the claim made in the paper. The limitation as well as weaknesses of the work are presented and discussed.

Weaknesses:
I did not find major weaknesses in this paper. 

- The main one would be in the addition computational and money costs for the proposed method compared to other classical approaches. However, in my opinion, the additional costs are counter balanced by performance and generality of the method.

- Another weakness is the use of a single text embedding model. I do not think this addition experiment is necessary for accepting this paper, but I do think that testing the robustness of this method with various text embedding would make the presented claims stronger.

- Additionally, given the experiments are averages across several seeds, I would report the variance of the results.

----

MINOR

- Please adjust the citation format, in the text there's a number-based format but there are not numbers in the references

- Very minor: then I tried to read table table 1 (pag 6), I was confused about what metric surface was. This was explained later in page 7. I would add a brief explanation of surface in the caption of the table, make a reference to the section when surface is explained or move the table closer the experiments section.

- Typo in caption of table 2: relaxiation -> relaxation

- line 277 general purpose -> general-purpose

- line 648: you refer section G.1 from section G.1, did you mean section E.3?

---

Recomendation 

Overall I consider this a strong a paper and I vote in favor of its inclusion to NeurIPS

Limitations:
Yes, in section 6, E.3 and G.1.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
SYjxhKcXoN;"REVIEW 
Summary:
This paper addresses the problem of domain generalization from the perspective of  ''learning multiple experts''. In particular, they propose to train multiple experts specialized in different domains, whose output probabilities provide professional guidance by simply regularizing the logit of the target model. The proposed logit regularization provides effects of enabling the target model to harness more information, and mining hard samples from the experts during training. Experiments on standard DG benchmarks demonstrate the effectiveness of the proposed framework.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- The overall manuscript is well-organized and easy to follow.

- The method is simple and effective. Its impressive results in different tasks demonstrate the strong applicability of this idea to potential other tasks.

- The related work section is comprehensive, especially regarding comparisons with Meta-DMoE that also tries to distill knowledge from domain experts.

- The idea of using MSE loss between logit and probability for distillation is new and well-explained, supported by both theoretical and empirical evidence.

- The free lunch idea introduced in sec. 6.1 is interesting and seems to have the potential for more general applications.

Weaknesses:
While adopting the idea of knowledge distillation (KD) and the concept of domain experts for improving DG is not new in the literature, it is appreciated that this method can be implemented in such simple form and be explained with extensive insights. Nevertheless, there are still a certain amount of claims in the paper that requires proper explanations or more justification:

- Performance reported in the original Meta-DMoE paper is higher than that in Table 1. For example, Meta-DMoE reports an average of 86.9 in PACS, surpassing the best result listed in the table. The authors should provide an explanation for this discrepancy. Moreover, I noticed that the training time comparisons between these two methods are provided, it would also be beneficial to highlight the test and training resource differences between these two methods in the related work section.

- The authors mention that their method introduces only one hyper-parameter, which is within a certain range. It would be beneficial to include an ablation study to explain why this specific range was chosen.

- It would be beneficial to evaluate this method with more real-world DG problem, such as using dataset from wilds benchmark.

- The method [1] with similar settings should be compared in Table 1. Additionally, some methods, such as MIRO, report improved performance when combined with SWAD. Including these results in Table 1 would provide a more comprehensive comparison.

- Minor issues. Formats of ”i.e.” in line 145, ”Tab.” in line 578, and ”Fig.” in line 581 are inconsistent with their usages in other places. The authors should standardize the format of similar abbreviations to maintain uniformity.

Reference: 

[1] Domain Generalization via Rationale Invariance, in ICCV 2023.

Limitations:
yes

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a simple framework called LFME for learning from multiple experts in domain generalization. LFME introduces a logarithmic regularization term to enforce similarity between the target model and expert models, allowing the target model to acquire expertise from all source domains and perform well in any testing domain. Experimental results demonstrate that LFME outperforms existing techniques not only in classification and segmentation tasks but also reveals through in-depth analysis its ability to implicitly use more information for predictions and to discover difficult samples from experts, thereby enhancing generalization capability.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper proposes a novel idea on the basis of a new knowledge distillation paradigm for DG. The motivation of using multiple experts for improving generalization is clear and reasonable. Meanwhile, I also found the paper well-presented and easy to understand.

After checking the pseudocode and the provided code, I found the implementation extremely simple. Given the impressive results computed with rigorous evaluation protocols, I believe this paper can offer valuable contributions to the literature by providing an accessible and effective solution. 

The provided deep analyses that explains why their method works are appreciated and, in my opinion, crucial for a paper, especially in the era of deep learning. Coupled with the empirical evidence in their supplementary material, I think most of claims made in their paper can be well supported.

Weaknesses:
Regarding the proposed method, although large improvements over the baseline ERM are observed, their method involves higher training costs. I noticed the authors provide the training time comparisons with different methods in their supplementary material, which shows that their LFME requires 1.5 times more training time than ERM, and it is also one of the most time-consuming methods compared. While I agree with the authors that requiring more training time is inevitable for methods designed based on KD, the authors should at least list this as a major drawback in their limitation section.

As in recent times, Vision Transformers (ViTs) have demonstrated substantial improvements in the classification and semantic segmentation tasks. Although the authors conducted experiments using different ResNet models, they should also consider adopting a ViT backbone for their experiments. This could provide valuable insights into the performance of their method when applied to more advanced architectures, potentially highlighting further benefits or limitations. Including such experiments would enhance the comprehensiveness of the study and align it with current trends in the field.

Limitations:
Authors have adequately addressed the limitations.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on improving domain generalization by utilizing multiple experts. Particularly, a simple yet effective framework is proposed whereby a target (student) model is learned from multiple expert (teacher) models through logit regularization. After learning, the target model can grasp knowledge from multiple source domains and shows advantages in handling hard samples. The proposed approach demonstrates consistent improvements across different evaluation tasks, outperforming existing SOTAs.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper overall is well written. It provides in-depth theoretical insights on its proposed logit regularization and surrogate ablation studies.

Weaknesses:
1: The contribution of the proposed approach is marginal. The performance of the proposed approach does not show much improvement compared to ERM or expert models (Table 5). 

2: The baseline models for comparison are not clearly introduced, which creates difficulties in understanding the Tables.

3: The comparison to the approaches that directly learn a foundation model using data from all source domains is not discussed. Hence, the benefits of the proposed approach, which have to use a set of expert models plus a target model, compared to the foundation model are not clear.

Limitations:
No discussions on limitations and potential negative societal impact of the work are provided. 

One limitation is that the performance of the proposed approach can be upper bounded by the performance of the expert models. This limitation again raises the question of why not directly learn from all source domains?

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The computer-vision paper introduces a strategy of learning from multiple experts (LFME), which performs knowledge distillation from models specially trained on data from different domains. In particular, the experts are trained jointly with the target model, and a specific form of logit regularization is chosen for knowledge distillation due to empirical verification. The idea is relatively straightforward to understand with strong empirical results on image classification (by Domainbed) and semantic segmentation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The experimental results come in strong when compared to many other baselines. Moreover, the study has extended results for semantic segmentation. 
2. The visualizations are helpful to illustrate the ideas and evidence. 
3. The empirical study on naive aggregation of domain experts shows that naive ensemble does not necessarily lead to better generalization. Then ablation study shows the chosen form of logit regularization outperforms other knowledge distillation alternatives.

Weaknesses:
1. The novelty is relatively limited. The proposed approach is very similar to Meta-DMoE with a few changes such as logit regularization and alternating updates, but these changes are not well justified (see below). The idea of utilizing multiple domains have also been explored by [1]. 
2. There is a lack of coherent argument to understand the proposed approach. In Section 4.1, the paper briefly discussed how the additional logit regularization enables learning more information. In addition, in Section 6.2, the proposed approach LFME is compared with Label Smoothing. However, the discussion is not very convincing. From the perspective of using information from other classes, I do not see the fundamental difference between logit regularization and Label Smoothing (LS). Both logit regularization and LS have hyperparameters to be tuned, so it might not be appropriate to claim its advantage as ""not involve hand-crafted settings” (line 187) and criticize LS has “potential improper heuristic designs” (189). 
3. Moreover, in Section 4.2, the paper justifies the advantage of the proposed approach LFME by “mining hard samples from the experts”. It is a general statement that is true for low-confidence predictions from any model, so it does not constitute a strong argument. Intuitively, the in-domain samples are easier than out-of-domain samples for each expert, and the hard samples aggregated from all experts are not specifically representative of any subpopulations. The argument will be more comprehensive if the paper can show what are these hard samples and why it matters to mine the hard samples from the domain experts (and not an ensemble of experts or some random experts). 
4. In Section 6.5, the result for in-domain evaluation is only provided for one dataset. This is not sufficient to justify a strong claim that the target model is an expert on all source domains. 
5. Code is submitted but it is unclear how the proposed approach can be combined with SWAD. Does it apply to just the target model or also the domain experts? The hyperparameter configurations to obtain the SoTA results are also unclear for reproducibility. 

[1] Yao, Huaxiu, Xinyu Yang, Xinyi Pan, Shengchao Liu, Pang Wei Koh, and Chelsea Finn. ""Leveraging domain relations for domain generalization."" *arXiv preprint arXiv:2302.02609* (2023).

Limitations:
The limitations has been discussed in Appendix A.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
NhP8MTJzT5;"REVIEW 
Summary:
The paper introduces SyntheOcc, a framework utilizing diffusion models to synthesize photorealistic images for autonomous driving simulations. The proposed method addresses limitations in the existing 2D diffusion model to generate multi-view driving videos by integrating detailed 3D geometric data.
The authors effectively employ 3D semantic multi-plane images (MPIs) for precise geometric control, enhancing the realism and utility of generated images for training perception models. The paper also proposes re-weighting strategies to address the imbalance problem between foreground, background, and object categories. The experiments prove the effectiveness of the proposed MPI encoder and the reweighting strategies.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper introduces an innovative approach by incorporating 3D semantic Multi-Plane Images (MPIs) to capture both geometric and semantic details of a scene. This approach allows for the precise modeling of 3D environments in a 2D image synthesis context, enhancing the photorealism and depth accuracy of the generated images. 
- The design of the MPI encoder is very effective in handling the input conditions with a large number of channels while maintaining spatial consistency to the latent features of diffusion UNet.
- Additionally, SyntheOcc incorporates sophisticated reweighing strategies to address class imbalance and ensure focus on critical features. These include foreground enhancement, depth-aware reweighing, and class-balanced reweighing.
- The paper outlines a comprehensive set of evaluations to demonstrate the effectiveness of the proposed method. Qualitative evaluations visually demonstrate the photorealism and environmental accuracy of the generated images compared to real scenes from the nuScenes dataset. Quantitative analyses leverage metrics such as Frechet Inception Distance (FID) to measure image quality and evaluate perception model performance, offering solid empirical evidence of the framework's effectiveness. Ablation studies further dissect the impact of various components and design choices in the proposed method. Additional robustness tests are conducted to evaluate how changes in the MPI settings (like variations in depth or semantic labeling) affect the output quality and the training effectiveness of perception models.

Weaknesses:
The contributions for reweighing strategies seem to be minor improvements over 
existing methods (Kai Chen, Enze Xie, Zhe Chen, Lanqing Hong, Zhenguo Li, and Dit-Yan 
Yeung. Integrating geometric control into text-to-image diffusion models for high-quality 
detection data generation via text prompt. arXiv preprint arXiv:2306.04607, 2023, Benjin Zhu, 
Zhengkai Jiang, Xiangxin Zhou, Zeming Li, and Gang Yu. Class-balanced grouping and sampling for point cloud 3d object detection (arXiv preprint arXiv:1908.09492), which limits the perceived novelty of the paper's contributions.

The paper focuses on scene editing capabilities, but there is a noticeable underrepresentation of object-level editing in the experiments. 

Magic drive’s data augmentation is evaluated on two perception tasks BEV segmentation and 3D object detection, with CVT (Zhou & Krahenbuhl , 2022) and BEVFusion (Liu et al., 2023a) as perception models, respectively. Hence, evaluations on same downstream tasks are encouraged for better comparisons to the state-of-the-art baseline.

The paper doesn't provide any evaluations comparing the re-weighing solution proposed by GeoDiffusion.

There are also several noticeable view inconsistencies, eg: Fig 14 - row 2 column 2-3 (clouds seem different), row 7 column 4-5 there is a mismatch in building structures, which are not discussed in the paper.

Limitations:
The authors acknowledge some key limitations in the proposed method. First, it relies heavily on existing data for generating scenes, which means it doesn’t create as much variety as it could. This limits how well it can train models to handle different driving conditions. The paper also struggles with complex scenes, like crowds, where it fails to accurately identify individual people. This is a big deal for autonomous driving, where accurate representations of the scene are crucial for predictions. The authors suggest that future improvements could include better methods for creating diverse scenes and making the model more capable of handling dynamic environments, which would help make the system more practical and effective for real-world applications.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces SytheOcc, a method that employs a diffusion model with 3D occupancy as conditions to generate street view images.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Unlike previous methods that use box conditions, this paper proposes the use of 3D occupancy, resulting in finer geometric control ability.
2. The paper further suggests the use of 3D semantic multi-plane images to represent the 3D occupancy.
3. The text and figures are well-presented, and the provided examples are very promising.

Weaknesses:
1. The main concern is the inconsistency between views and frames. Despite using Cross-View and Cross-Frame Attention and 3D occupancy as conditions, the spatial and temporal consistency results are unsatisfactory (e.g., Fig 5 (b) and the video demos). This is not the expected outcome, as incorporating 3D occupancy as a consistent world representation should result in better spatial and temporal consistency. Additionally, it would be preferable to have metric results such as FVD for the temporal experiments.
2. In Table 1, why does SytheOcc-Aug show worse results for certain categories (e.g., bicycle, moto)?
3. Table 1 lacks experiments for ControlNet-Aug or ControlNet+depth-aug.
4. Some discussions regarding 3D occupancy as a 3D geometry condition:
   4.1. The field of view (FOV) for 3D occupancy is limited as it is generally generated using lidar, which leads to inconsistency issues for high-rise buildings when considering cameras of larger FOVs.
   4.2. The current annotation of 3D occupancy has limited category coverage. It would be beneficial to explore open-vocabulary approaches.
   4.3. When using only 2D semantic masks as conditions, the paper mentions the presence of ambiguity (i.e., Fig 6 a0). Can the use of instance-level semantic masks alleviate this problem?

Limitations:
The authors have provided a comprehensive list of limitations and future work of their paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper propose a new 3D semantic multi-plane images (MPIs) based image generation pipeline, which enables finer geometric control for 3D editing, dataset generation, and long-tailed scene generation. Through extensive experiments, the work demonstrates substantial advancement in generation quality and better alignment between condition and synthesized images.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The work explores a new 3D semantic Multi-Plane Images (MPIs) as a condition, which provides better spatial alignment compared with baselines and enables 3D editing.

2. The comparison results are comprehensive and demonstrate the effectiveness the proposed method, the ablation is relatively complete to validate the MPI Encoder and the reweighting strategy.

3. The paper is well-written, and the experimental results are presented clearly.

Weaknesses:
1. The MPI encoder, which is the major contribution, is not novel for me. Although the proposed 3D MPI enables finer control than BEVGen, but the diffusion model also operates on the 2D domain and generates each view and frame separately without strict geometry constraints.

2. The importance of reweighing is tricky and hard to tune, considering many hyperparameters. Are the m and n in Eq. 6 the same for different datasets?

3. It’s hard to decide how the method works without a supplementary video, I doubt the view-consistency of the generated video across frames and views.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors propose a new controllable diffusion-based image generation method named SyntheOcc, which takes an occupancy map as input and generates camera images. SyntheOcc enables the application of scene editing and long-tail corner case generation and shows a strong capability of data augmentation for autonomous driving systems.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Compared with previous controllable image generation methods for traffic scenarios like Panacea or MagicDrive, the occupancy map contains more 3D spatial information than the BEV layout.
2. The paper is well-organized and easy to follow.
3. The extensive experimental results demonstrate the effectiveness of the proposed data generation pipeline.

Weaknesses:
1. The control signal in Panacea or MagicDrive is BEV layout, which only contains lanes and foreground objects and is more easily acquired than occupancy. However, the SyntheOcc relies on sophisticated collected occupancy.

Limitations:
The proposed SyntheOcc faces challenges in real-world application scenarios. For instance, to generate planning-level long-tail corner cases, other methods like Panacea or MagicDrive simply require editing the object's trajectory. However, SyntheOcc demands not only inputting the background occupancy but also constructing a pseudo occupancy for the foreground object. This raises the question of whether using occupancy as a control signal is an advantage or a disadvantage.

The crux of the issue lies in the complexity of this method compared to alternatives. While other approaches need some adjustments to object trajectories, this technique necessitates providing comprehensive background and foreground occupancy data. This additional overhead prompts us to ponder whether occupancy-based control offers tangible benefits or introduces unnecessary complications.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
nEnazjpwOx;"REVIEW 
Summary:
The authors propose diffusion Thompson sampling, which uses a diffusion model to leverage reward under similar actions for more efficient exploration. The authors derive efficient posterior approximations under a diffusion model prior and prove a regret bound in linear instances. To efficiently compute and sample posterior distribution, the authors provide an approximation that relies on close-form solutions for case where both the score functions of the diffusion model and the likelihood are linear. For nonlinear diffusion model, the authors approximate posteriors by a Gaussian distribution.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The proof of Theorem 4.1 requires novel techniques such as recursive total variance decomposition and refined arguments such as quantifying not only the posterior information gain for the taken action but also for every learnt latent parameter. 

The paper is well-written. The main contributions and key observations from the regret bound are nicely summarized. Experimental results for all four combinations of linear and nonlinear reward, linear and nonlinear diffusion model are provided. In experiments, the authors made a number of insightful observations, accompanied by ablation results.

Weaknesses:
The authors discussed how the number of layers L affect the regret bound. A higher L increases regret bound and a smaller L may fail to capture a more complex prior. It would improve the paper to provide a heuristics on choosing an appropriate L along with justifications for the heuristics.

Limitations:
The authors addressed limitations and societal impact at Appendix E and F.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work presents the use of Diffusion models as priors for Thompson sampling.

Namely, they propose to learn diffusion models (as replacement to other parametric priors) to accommodate more complex correlations between context, action and reward functions than with simple parametric form priors.

Given that Thompson sampling requires sampling from the posterior of the model, the authors derive a linear-Gaussian posterior approximation (under the proposed diffusion model prior).

The authors analyze the proposed algorithm for the linear-Gaussian reward case, which enables them to provide a Bayes regret bound.

Experimental results demonstrate some of the benefits of the proposed diffusion-based Thompson sampling: learning the correct latent-structure is beneficial, learning more parameters (as a function of $d$, $K$ and $L$) is a harder problem, hence incurs in higher regret.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The use of diffusion models to learn complex priors for their use within MAB problems is of interest and significant.

- The authors provide a theoretical analysis of their proposed algorithm (only for the linear-Gaussian case), for which they: 
    - use the recursive total covariance decomposition,
    - showcase the dependency over K ---induced by the hierarchical parameter learning--- and
    - demonstrate the dependency with $L$, inherent to having more parameters to learn.
    
- The theoretical analysis and the experiments showcase the benefits of learning the true hierarchical model (as specified by a diffusion model) in comparison to LinTS.

Weaknesses:
- The proposed diffusion-based algorithm does not learn the diffusion model as it sequentially interacts with the world
    - Instead, using the diffusion model as a complex prior requires offline learning, so that non-trivial prior distributions can be learned, before it can be used within Thompson sampling.
    - The cost of learning such a diffusion model is not acknowledged nor discussed.

- The proposed posterior approximation seems to be equivalent to the well known Laplace approximation, i.e., a linear-Gaussian approximation to a (non-linear and non-Gaussian) posterior. See questions below.

- The provided Bayesian regret is limited to the linear-Gaussian case, and in fact is acknowledged to be similar to ""L + 1 sequential linear bandit instances stacked upon each other"".

- The empirical evaluation is executed on synthetic experiments simulated from the assumed model prior, with $L$ latent parameters. Hence, the benefits of learning the true model are somehow expected.

Limitations:
The authors do present general limitations of their work, although the cost associated with learning a diffusion model prior is less clear.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work provides a great example of diffusion modeling on bandit action parameter for better exploration.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The work provides a comprehensive description on how to employ diffusion modeling on bandit parameters for contextual bandit problems. 

The discussion on linear and non-linear diffusion model is clean and precise for readers with background in Thompson Sampling 

The analysis part also provide comprehensive discussion on how the regret of the  proposed diffusion Thompson Sampling scales with main dimension of contextual bandit problems.

Weaknesses:
I am satisfied with current version of the paper.

Limitations:
Yes, the author clearly states the assumptions to address the limitation of the theoretical analysis.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper considers the problem of contextual bandits in large action spaces.  In this problem, the reward of an arm is a function of the context and an unknown, arm specific parameter vector. To efficiently learn good policies in such large action spaces, the paper places a structured-prior distribution on the unknown arm parameters that can effectively capture the correlations between the arms. The specific form of the prior distribution considered in the paper resembles a diffusion model. The main contribution of the paper is to provide a computationally efficient heuristic for performing Thompson sampling with this prior. Experiments on synthetic data show that the proposed technique is much better at learning optimal policies than other popular baselines such as LinTS, LinUCB.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The problem of handling large action spaces in contextual bandits seems interesting. The empirical evaluation shows promise in the proposed approach

Weaknesses:
- **Related Work:** There are several ways in which large action spaces are typically handled in contextual bandits. One popular approach that is used in practice is to associate a feature vector to each arm (this feature vector is known to the learner ahead of time), and the reward of pulling certain arm for a context is a function of both the context and arm features. In the absence of arm features, the other approach is to impose some structure on the unknown arm parameter vectors. There are several works which do this, and the current paper falls in this line of research. Some of these works assume the arms can be clustered into a small number of groups or can be embedded in a low-dimensional latent space and learn the low-dimensional features during the course of the online learning (https://arxiv.org/pdf/2010.12363, https://arxiv.org/abs/2209.03997, https://arxiv.org/pdf/1810.09401, https://proceedings.neurips.cc/paper_files/paper/2023/file/f334c3375bd3744e98a0ca8eaa2403b0-Paper-Conference.pdf). The diffusion prior used in the current work resembles the low-dimensional embedding assumption. In particular, it is assumed there is a latent vector (psi_1) from which all the arm parameter vectors are generated; this is a form of rank-1 assumption on the arm features. Unfortunately, none of these works were brought up in the paper. It would be great if the authors perform a thorough literature review and better position their work.

- **Linear Setting**: A lot of emphasis has been placed on the linear model in the paper. I understand it is used to derive the heuristic for the non-linear setting. Beyond that, I do not find the regret bounds derived in section 4 to be interesting. In the linear setting, there isn't a need to work with the complex hierarchical diffusion prior. It looks like one could totally remove the latent variables psi_{*, L}, .... psi_{*, 2} and simply place a Gaussian prior on psi_{*,1} and get an equally powerful model. This would also improve the regret bounds, by removing the L factor in the regret. Given this, I'm not sure about the utility of section 4.
  - Section 4.1 compares the regret bounds obtained in this work with other baselines. But this comparison is only meaningful under the assumption that the diffusion prior is properly specified. This raises the following question: why is this a reasonable prior to use in practice? How do various techniques compare if this prior is misspecified? (There are some experiments section 5.2 on prior misspecification, but the misspecifications considered there seem to be very minor)

- **Quality of Heuristics:** How good is the heuristic used for non-linear diffusion model? There is no discussion on this in the paper (In my opinion, this needs to be thoroughly discussed in the paper, as it is the primary novelty of the work). Some empirical evaluation comparing it with other standard estimation techniques (such as variational techniques, and other posterior estimation techniques) would have been helpful in understanding his question.

Limitations:
See my comments above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
TWfNFCOPaK;"REVIEW 
Summary:
The paper proposes the use of the Wireless Geometric Algebra Transformer (Wi-GATr) to model signal propagation. Based on the Wi-GATr network, it introduces a differentiable prediction model and a diffusion model. Compared to traditional statistical and ray-tracing methods, the proposed approach not only addresses conventional signal prediction problems but also tackles inverse problems such as receiver localization and 3D environment reconstruction. Experimental results demonstrate the effectiveness of this method. Furthermore, the authors present two large-scale wireless signal propagation datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) The perspective is interesting. This paper models wireless propagation as a probability model based on diffusion, thus offering a unified approach to address signal prediction and inverse problems such as receiver localization or 3D scene reconstruction.
2) The paper is logically structured, with a comprehensive background introduction and high readability.

Weaknesses:
1) There is a gap between the challenges and the solutions. The author asserts that wireless surrogate modelling faces challenges like data scarcity and diverse data types. However, the lack of analysis on these issues makes the proposed solutions appear abrupt. It is recommended to provide insights that lead to the proposed solutions of this paper.
2) The innovation is somewhat limited. Wi-GATr primarily extends the GATr method into a wireless setting, with equivariance being a pre-existing property of the original framework. Apart from tokenizing input data, did the paper introduce any additional advancements? It would be beneficial for the authors to highlight these aspects.
3) The experimental evaluation is not sufficiently convincing. For more details, please refer to the ""Questions*"" part.
4) There are some typographical errors in the paper. For example, lines 56 and 57 do not correspond to Figure 1. Additionally, the abstract mentions transmitter localization, but the main text describes receiver localization, among other discrepancies.

Limitations:
Sufficiently discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes the use of a transformer architecture to model electromagnetic propagation of physical systems. The approach is claimed to outperform existing methods by (i) computational efficiency (compared to raytracers) and (ii) enabling solving inverse problems. The method is evaluated on a number of benchmark tasks and the paper is accompanied by two new datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The idea of modeling electromagnetic wave propagation using transformer architectures is novel and interesting. 

The generality of the approach enables a large number of tasks in wireless communication systems that would otherwise the use of raytracers or other electromagnetic modeling software.

Large parts of the main body of the paper are well-written and easy to follow.

Weaknesses:
The main body of the paper lacks the details of the proposed pipeline and transformer architecture. In fact, all of the interesting and technical details are relegated to the appendices. 

One of the main motivations of the paper seems to be that most raytracers are too slow. However, the authors seem to ignore recent projects, such as Instant RM (https://github.com/NVlabs/instant-rm) which can compute coverage maps in a few milliseconds, depending on the desired accuracy.

It is unclear why [29] is cited as a non-differentiable raytracer although it is, to my knowledge, the only raytracer that actually is. Instant RM is also differentiable and calibration results for both tools were already demonstrated. To be honest, I have the impression that the authors tried to cover up the fact that [29] is a powerful *differentiable* raytracer that enables solving inverse problems. 

Although the authors claim that channel impulse responses can be generated, this is not demonstrated in the paper. I think that this claim should be removed unless the authors demonstrate that it is actually feasible.

The description of scene geometry recovery is incomprehensible to me.

In Figure 7, the Wi-GATr is around 20ms for inference for a tiny indoor scene. The authors should compare this against Instant RM which can probably run even faster and is differentiable.

It would be good to get confidence information (e.g., standard deviation) in Fig. 3 and Fig. 4.

Limitations:
The paper lacks a detailed comparison to the capabilities of the differential raytracer from [29]. In fact, I feel that for the individual tasks, more baselines should be included. 

Scalability to very large datasets and extremely complex scenes is unclear. 

It is unclear whether the method generalizes to electromagnetic environments that are nonreciprocal (e.g., containing certain nonreciprocal metasurfaces). 

It is unclear whether the method generalizes to scenarios in which ray-tracing is inaccurate (e.g., scenarios at low carrier frequencies).

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Authors proposed transformer based ideas on very well studied area: Wireless environment simulation.
The key idea here is to capitalize Geometric Transformers to simulate radio environments. It true that wireless (directional) signal propagation is a ray tracing approach, meaning a highly directional wireless signal (Ray) may bounce off ambient surfaces or directly reach a receiver. The proposed method inserts geometric shapes in the environment as tokens in a transformer networks. The trained transformer predicts the received power at a given point in 3D space. Transformer is trained and evaluated using two datasets: Wi3R and WiPTR that simulate indoor signal propagation environments. In comparison to the baselines, transformer architecture requires 20 times less data.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. It is interesting idea to see if transformer architectures are suitable to model radio propagation environments
2.  Propagation models consider the material of ambient surface, antenna parameters, location of transmitter and receiver

Weaknesses:
1. It is not clear if datasets are of any relevance to real world environment, since the primary challenge of any modelling problem is simulation to reality gap. Since wireless channel modelling is very well studied area, a novel contribution must take in to account such differences rather than results that show the ability of transformer architecture in modelling a wireless environment
2. There are several high fidelity radio propagation modelling software, perhaps it is important to consider datasets generated from such model, current evaluation is very limited and primitive. Fig 2 and 5 are no comparison to robust channel models that are available to wireless researcher and practitioners.
3. The modelling of radio environment is no clear, reviewer is of impression that several affects like diffraction, refraction are not considered in the datasets
4. The paper also has weakness: WiNeRT: Towards Neural Ray Tracing for Wireless Channel Modelling and Differentiable Simulations which is both papers have not considered user mobility: coherence time, coherence bandwidth
5. Current evaluation is only limited to indoor environments
6. Since the prior work has already established neural network architecture are useful modelling, to push the state of the art, it is important to show accurate modelling than yet another architecture to model wireless channel.
7. Upon inspecting table 2, the reviewer is afraid that there might be issue with results here. There is 80dBm difference is accuracy with transformers based modelling, usually such a difference is unacceptable, can author please explain the training of transformer model and why it produced such a large error. The reviewer is concerned that whether such sample point is a fair to benchmark againt

Limitations:
In reviewer's opinion authors have not sufficiently addressed all the limitations of the current work. I encourage authors to look at weakness section and update the limitations of the work in the current draft

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The motivation for this work is that modeling the propagation of electromagnetic signals is critical for designing modern communication systems. Ray tracing simulators are not suitable for inverse problems or integration as channel models in designing communication systems.

In this context, the goal of this paper is to model the interplay between the 3D environment F, transmitting and receiving antennas (each characterized by a 3D position, orientation, and specific antenna characteristics) represented by t and r, respectively, and the signal h between each transmitter and receiver. The 3D geometry F is represented by a triangular mesh, where each triangle is assigned a material type from predefined classes, modeling both the shape and materials of the environment. Once the model is learned, three tasks can be performed:
1. Prediction of the received signal p(h∣F,t,r): The model is trained for this task. At test time, the network can predict signals in unseen, novel scenes. This approach is faster, fine-tunable on real measurements. The model obtained is also differentiable. This is referred to as the forward problem.
1. Localization of the receiver p(r∣F,t,h).
1. Sensing the environment p(F∣t,r,h).

The last two tasks are referred to as inverse problems. The model introduced is an adaptation of the Geometric Algebra Transformer called Wireless (Wi-GATr), used for simulating wireless propagation in a 3D environment. The authors also cast this problem as a generative modeling task of the joint distribution p(F,t,r,h) (from which the above three tasks can be accomplished) using Denoising Diffusion Probabilistic Models and Wi-GATr.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The main contributions of this work are as follows:
1. Introducing a new tokenization method for geometric wireless communication environments and transmitter and receiver characteristics.
1. Integrating diffusion-based models with Wi-GATr to model the wireless environment as a generative model, thereby determining the joint distribution of F, t, r, and h.
1. Providing new, larger datasets for the wireless environment modeling to the research community.

Weaknesses:
As per my understanding, the main weaknesses of the work are: 
1. The novelty of the work lies in tokenizing various geometric objects encountered in the wireless communication scene. However, the same tokenization is used in the vanilla transformer, making it unclear if the new tokenization provides any benefit.
1. As the authors point out, the channel is modeled only in terms of time-averaged non-coherent received power, missing crucial information such as time and direction of arrival, which are essential for modeling wireless environments.
1. While the proposed solutions seem general, most results are presented for the single antenna case. Additionally, the dataset includes only transmitting sinusoidal waveforms, which is limiting as it does not cover larger bandwidths. The wave propagation depends on frequency, and non-linearities can occur with wider bandwidths.

Limitations:
The authors mention limitations in the Discussion section (Section 6).

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents the Wireless Geometric Algebra Transformer (Wi-GATr), a new architecture for simulating wireless signal propagation in 3D environments. This model utilizes geometric algebra to handle the geometric complexities of wireless scenes and ensures E(3) equivariance to respect the symmetries of the physical problem. The authors introduce two datasets, Wi3R and WiPTR, to benchmark their model. Wi-GATr outperforms existing baselines in terms of prediction fidelity and data efficiency, and it can solve both forward (signal prediction) and inverse (receiver localization and geometry reconstruction) problems in wireless communication.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The integration of geometric algebra for handling complex 3D geometric data and ensuring E(3) equivariance is a novel and effective approach. This addresses the core challenge of accurately modeling wireless signal propagation in diverse environments.

2. The paper provides a thorough evaluation of Wi-GATr against multiple baselines across various tasks, demonstrating superior performance in signal prediction, receiver localization, and geometry reconstruction.

3. Wi-GATr shows remarkable data efficiency, achieving high-fidelity predictions with significantly less training data compared to other models. This is particularly beneficial for scenarios where obtaining large amounts of training data is challenging.

4. The model's ability to handle both forward (predictive modeling) and inverse (localization and reconstruction) problems showcases its versatility and potential for a wide range of applications in wireless communication.

Weaknesses:
1. Limited Real-World Testing: While the model performs well on the introduced datasets, its application in real-world, dynamic environments remains underexplored. Additional experiments in more varied and complex real-world scenarios, such as urban or industrial settings, would strengthen the paper.

2. Scalability and Computational Load: The paper could provide more detailed insights into the computational requirements and scalability of Wi-GATr. Understanding the model's performance with larger datasets and more complex environments would be valuable for practical deployment.

3. Generalizability Across Frequencies: The model is tested at a specific frequency (3.5 GHz). Evaluating its performance across different frequencies and under various signal conditions would provide a more comprehensive understanding of its robustness and generalizability.


4. Detailed Case Studies: While the paper presents strong experimental results, including more detailed case studies or examples of practical applications, such as network design or optimization in real-world environments, would illustrate the model's impact and practical benefits.

Limitations:
The authors briefly discussed the limitation in Sec. 6.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
RB1F2h5YEx;"REVIEW 
Summary:
The paper addresses the challenge of selecting and designing embedding methods by proposing a unified framework that treats these methods as RL problems. This framework encompasses various embedding techniques, including VAEs, UMAP, and t-SNE, providing insights into their relationships and enabling the creation of hybrid methods. The authors illustrate how the ELBO approximation in VAEs relates to the exploration-exploitation trade-off in RL, and they demonstrate the framework's flexibility in designing novel methods and extending existing ones using RL techniques. They also present several new hybrid methods, such as a variational UMAP and a UMAP/t-SNE hybrid, which show state-of-the-art performance across multiple datasets. Finally, they offer a Python package implementing this framework for practical use.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Introducing Parseval regularization as a novel technique to address the challenges of continual reinforcement learning is a significant contribution. By maintaining orthogonal weight matrices, the method preserves optimization properties crucial for training neural networks on sequential tasks.

The paper provides robust empirical evidence of the effectiveness of Parseval regularization across a variety of RL tasks, including gridworld, CARL, and MetaWorld. This empirical validation enhances the credibility of the proposed method.

The paper includes thorough ablation studies to dissect the impact of Parseval regularization. By isolating and analyzing different components of the regularization technique, such as the regularization of row norms and angles between weight vectors, the authors provide insights into why and how the method improves training performance.

It compares Parseval regularization with alternative algorithms like layer norm and shrink-and-perturb, demonstrating its superiority in certain contexts. This comparative analysis strengthens the paper's claims about the efficacy of Parseval regularization.

The exploration of how Parseval regularization interacts with different activation functions, network widths, and initialization scales suggests its versatility and potential for application across various network architectures and RL settings.

Grounding the approach in theoretical concepts such as dynamical isometry and orthogonal initialization adds depth to the paper's theoretical framework, providing a clear rationale for the effectiveness of Parseval regularization.

Weaknesses:
Parseval regularization, while theoretically sound, may add significant complexity to the implementation of neural networks. Practitioners might find it challenging to integrate and tune the regularization parameters in practice.

 The paper primarily demonstrates the benefits of Parseval regularization on relatively contained environments like gridworld, CARL, and MetaWorld. It is unclear how well the method scales to more complex, high-dimensional tasks or real-world applications.

 Although the paper compares Parseval regularization with a few alternative algorithms, it might benefit from a broader range of baseline comparisons. Including more state-of-the-art methods could provide a more comprehensive evaluation of its effectiveness.

 Regularizing weight matrices to maintain orthogonality could introduce computational overhead. The paper does not extensively discuss the trade-offs between performance gains and the computational cost of implementing Parseval regularization.

While the empirical results are strong, the theoretical analysis might be lacking in depth. A more rigorous exploration of the underlying mechanisms and theoretical guarantees could strengthen the paper’s contributions.

The findings are primarily demonstrated in controlled experimental settings. Additional experiments in more varied and realistic environments would help confirm the generalizability and robustness of the proposed approach.

Although the paper touches on different activation functions, it might not explore a wide enough variety to fully understand how

Limitations:
The paper primarily demonstrates its methods on relatively simple environments such as gridworld, CARL, and MetaWorld. It remains unclear how well Parseval regularization scales to more complex, high-dimensional tasks or real-world applications.

Implementing Parseval regularization may introduce additional computational costs, which could be significant, especially for larger networks or more complex tasks. The paper does not thoroughly discuss these potential overheads or provide a cost-benefit analysis.

The paper compares Parseval regularization with a few alternative methods, but a more extensive comparison with a broader range of state-of-the-art techniques in continual reinforcement learning could provide a more comprehensive evaluation.

The results presented are specific to the environments and tasks used in the study. There is limited evidence to suggest that the benefits of Parseval regularization can be generalized to other types of neural networks, machine learning models, or more varied and realistic continual learning scenarios.

While the empirical results are strong, the theoretical explanation for why Parseval regularization works is not deeply explored. A more rigorous theoretical analysis could strengthen the paper’s contributions and provide better insights into the underlying mechanisms.

 The paper does not extensively address how sensitive Parseval regularization is to the choice of hyperparameters or provide guidelines for selecting these parameters in practice.

Although the paper touches on different activation functions and network widths, it does not explore a wide enough variety to fully understand how Parseval regularization interacts with different network components and architectures.

The paper does not provide extensive analysis on the long-term stability and performance of reinforcement learning agents using Parseval regularization in highly dynamic and nonstationary environments.

 There is a lack of discussion on the practical implications and potential challenges of implementing Parseval regularization in real-world applications, which may differ significantly from controlled experimental settings.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses challenges in continual reinforcement learning settings by introducing an additional term, called Parseval regularization. This regularization ensures that the weight update direction remains somewhat orthogonal to the current weight, thereby preserving beneficial optimization properties. Empirical results with ablation studies are presented on Gridworld, CARL, and MetaWorld tasks, demonstrating the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Important Topic: The paper targets a significant and timely topic in the field of continual reinforcement learning.
2. Ablation Study: The inclusion of comprehensive ablation studies provides insights into the method's components, such as the importance of regularizing weight angles versus weight norms.

Weaknesses:
I have identified several primary drawbacks of this work. Overall, while the paper presents promising results, addressing these points would provide a more comprehensive understanding of its contributions and limitations.

1. Additional Memory/Computation Cost: The introduced regularization method adds extra memory and computation costs. In continual learning, which is often required for large-scale tasks, such additional costs can be significant. It is critical for the authors to report a detailed comparison of the computational costs between their method and the baselines to fully understand the trade-offs involved, including metrics such as memory usage and training time.

2. Missing Related Works: The paper overlooks two highly relevant works:

1). “Superposition of Many Models into One” by Brian Cheung et al.: Although this work is not in the RL domain, its underlying concept of model superposition is highly similar and could be extended to RL settings. The authors should discuss the relevance of this work and provide empirical comparisons to demonstrate the advantages or limitations of their approach in comparison.

2). “Memory-efficient Reinforcement Learning with Value-based Knowledge Consolidation” by Qingfeng Lan et al.: This work addresses the continual learning problem in RL, albeit in a single-task context. However, its methods can be extended to multi-task settings. Moreover, one possible effect of Parseval regularization is to mitigate strong disturbances to the current model’s outputs, making knowledge consolidation a reasonable baseline for comparison. The authors should discuss this work and consider it in their empirical evaluations.

Limitations:
see above

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies the problems of plasticity loss in continual reinforcement learning. Parseval regularization is proposed as a solution to plasticity loss. Parseval regularization encourages the weight matrices in all layers to remain orthogonal, which ensures that useful learning properties are preserved. Empirical evaluation is performed in various RL environments, and it is shown that parseval regularization outperforms many existing methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well-written. The proposed method is evaluated in a wide range of environments, and the results are statistically significant. The proposed solution is well-justified, found to be useful in many cases, and it is easy to use.

Weaknesses:
The paper has one major problem:

Computational complexity. The computational complexity of the proposed method seems too large. It is O($l*d^3$) , where $d$ is the width of the network, and $l$ is the number of layers in the network. This is much larger than that of both forward and backward passes, which is O($l  * d^2$). Given the large complexity, what is the run time of the proposed method? The authors should report the run times of all the algorithms for at least one experiment, ideally the one presented in Figure 1. If the run-time of the proposed algorithm is too long, the authors may need to come up with a variation with smaller computational complexity. One solution could be to calculate the Parseval loss after every $d$ updates.

I will update my final rating for the paper once the computational complexity and run-time of parseval regularization are reported.

Limitations:
The authors need to discuss the high computational complexity of their method in the last section of the main paper.

---------------------------------------------
UPDATE: The authors satisfactorily address the concerns about computational complexity. I have raised my score to reflect that. This paper is a good contribution to the community and should be accepted to NeurIPS.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
ACIDDnTbSJ;"REVIEW 
Summary:
This paper proposes a method to generate feint behaviors and strategies so that the agent can obtain temporal and spatial advantages when competing with opponents. Specifically, this paper first describes the characteristics of feint behaviors at action-level and proposes a Feint behavior template generator called Palindrome-directed Generation that extracts subsets of semi-symmetrical actions from an offensive behavior and synthesizes them as a Feint behavior. Then, the paper proposes a dual-behavior model, which considers the physical constraint and effectiveness when constructing effective combinations of feint behaviors and follow-up actions. Moreover, the paper formalizes the feint implications at strategy-level and proposes rewards reflecting the temporal, spatial, and collective impacts. Finally, the paper provides an implementation scheme of feint behaviors that can be integrated into the existing MARL frameworks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper provides the formalization of feint behaviors and the corresponding palindrome-directed feint behavior template generator. The analysis of feint behaviors and their combination with follow-up actions makes sense.

2. The proposed feint behavior implementation scheme can be easily integrated into existing MARL frameworks.

3. The paper is organized well and the presentation is easy to follow.

Weaknesses:
1. This paper claims that most offensive behaviors can be decomposed into three action sequences, which are Stretch-out Sequence (Sequence 1), Reward Sequence (Sequence 2), and Retract Sequence (Sequence 3) and it proposes a feint behavior template generator based on palindrome structure. This design should be effective for many feint behaviors composed at the action level. However, more complex feint behaviors belonging to the strategy level may not consist of the above three parts and thus the palindrome structure will not work. This paper may specify more clearly the scope of feint behaviors to which the proposed scheme can be applied.

2. The discussion in the experiment section is relatively weak. For example, according to Appendix E, when applying the feint behavior implementation to MARL frameworks, the implementation trains a feint policy model to generate feint behaviors. The experiment results show that the feint behaviors can increase task returns. However, it is unclear whether the actions generated by the feint policy model are really feint behaviors. It is also possible that the feint policy model trained based on $Rew_{collective}$ manages to find certain good actions to beat opponents. This paper may have more discussion about this based on a clear definition of feint behaviors.

Limitations:
Please see the weakness section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a comprehensive formalization of feint behaviors in competitive multi-player games. The paper introduces a method for the automatic generation of feint behaviors using Palindrome-directed templates and combines them with high-reward actions in a Dual-Behavior Model. This formalization is incorporated into Multi-Agent Reinforcement Learning (MARL) frameworks. The authors conducted extensive evaluations using various MARL models in both two-player and six-player scenarios, demonstrating that their formalization significantly improves game rewards.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
* The paper introduces a new formalization of feint behaviors, in which automatic generation of Feint behaviors via Palindrome-directed templates, and combine them with intended high-reward actions in a Dual-Behavior Model
* The methodology addresses Feint implications on game strategies in terms of the temporal, spatial, and their collective impacts via the implementation in existing MARL frameworks.
* In the experiments using various MARL models in both two-player and six-player scenarios, the authors demonstrate that their formalization significantly improves game rewards.

Weaknesses:
* The paper may lack clarity and consistency in the definitions and notations of key concepts, such as the reward functions in Section 4.2.1 and the Policy Occupancy Measure for model-free policies in Section 4.2.2. Additionally, the inconsistency in describing the payoff matrix dimensions in Section 4.2.3 creates confusion about the interaction between different agents' policies.

* The paper also does not provide sufficient methodological details for calculating crucial measures, such as the Policy Occupancy Measure for model-free policies and the structure of the new policy occupancy measure.  

* The evaluation results focus primarily on gaming rewards without a thorough analysis of the types and distributions of observed Feint behaviors. A more comprehensive assessment, including the underlying reasons for focusing on gaming rewards and the detailed impact of Feint behaviors, would enhance the understanding of the study's contributions.

Limitations:
The authors have not adequately addressed the limitations and potential negative societal impact of their work. While the paper presents a novel approach to generating and integrating Feint behaviors in MARL frameworks, it lacks a detailed discussion on the limitations of the proposed methods, such as the scope of applicability and potential challenges in real-world implementations. To improve, the authors should include a section discussing these aspects, providing a balanced view of the strengths and weaknesses of their approach, and consider potential negative outcomes and mitigation strategies.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a comprehensive approach to formalizing and implementing feint behaviors in multiplayer games. It introduces a new method for the automatic generation of feint behaviors using Palindrome-directed templates and combines these with high-reward actions in a Dual-Behavior Model. The paper further explores the implications of feint behaviors on game strategies, focusing on temporal and spatial impacts, and provides a unified implementation scheme. Experimental results demonstrate significant improvements in game reward gains, diversity, and minimal time overheads when incorporating feint behaviors.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The introduction of Palindrome-directed templates for generating feint behaviors is a novel concept that adds value to the field of game strategy formalization.
* The paper offers the first detailed formalization of feint behaviors at both the action and strategy levels, addressing a gap in existing literature.
* The integration of feint behaviors into common MARL frameworks and the use of diverse experimental scenarios enhance the practical applicability of the proposed methods.
* The implementation scheme is designed to be adaptable across various MARL models, which increases its versatility and potential for widespread adoption.

Weaknesses:
* While the paper evaluates several MARL models, it would benefit from a broader comparison with more diverse baseline strategies and models to establish the relative performance gains more comprehensively, such as [1-2]. 
* The paper does not sufficiently address potential scalability issues when applying the proposed methods to larger and more complex game environments beyond the tested scenarios.
* The focus is primarily on multiplayer games, and the applicability of the proposed methods to other domains with different characteristics is not explored in depth.
* It is recommended to apply appropriate smoothing to the curves to enhance the readability of the figures. This is particularly important for Fig 4, where almost all the curves without feint are nearly overlapping.

[1] Yu, Chao, et al. ""The surprising effectiveness of ppo in cooperative multi-agent games."" Advances in Neural Information Processing Systems 35 (2022): 24611-24624.
[2] Yang, Tianpei, et al. ""ASN: action semantics network for multiagent reinforcement learning."" Autonomous Agents and Multi-Agent Systems 37.2 (2023): 45.

Limitations:
The author mentioned ""Limitation discussed in the Discussions,"" but there is no section called ""Discussion"" in the manuscript. I also couldn't find any discussion about limitations in other parts of the document.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the first comprehensive formalization of Feint behaviors in multi-player games. The authors present a novel approach to automatically generate Feint behaviors using Palindrome-directed templates and combine them with intended high-reward actions in a Dual-Behavior Model. The formalization addresses both action-level and strategy-level aspects of Feint behaviors, considering temporal, spatial, and collective impacts.

The authors provide a unified implementation scheme to incorporate Feint behaviors into common Multi-Agent Reinforcement Learning (MARL) frameworks. They evaluate their approach using multiple MARL models in a custom boxing game scenario and a strategic real-game simulation.

The results demonstrate that incorporating Feint behaviors can significantly increase game rewards and improve the diversity of multi-player games. The authors show that their method outperforms existing approaches in several scenarios, with minimal computational overhead.

While the work presents a novel and potentially impactful approach to modeling deceptive behaviors in games, it is limited by its focus on specific game scenarios and lack of theoretical analysis. Nevertheless, this paper contributes a valuable framework for enhancing the realism and strategic depth of AI agents in multi-player games.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper demonstrates significant originality by providing the first comprehensive formalization of Feint behaviors in multi-player games. The authors address a notable gap in the literature, as previous works have only touched on Feint behaviors superficially or as proof-of-concept. The formalization at both action and strategy levels shows a depth of thinking that goes beyond existing approaches.

The quality of the work is evident in the thorough development of the formalization, from the Palindrome-directed templates for generating Feint behaviors to the Dual-Behavior Model for combining them with high-reward actions. The authors have clearly put considerable effort into creating a robust framework that can be applied across different game scenarios.

In terms of clarity, the paper is generally well-structured, guiding the reader through the complexities of Feint behavior formalization in a logical manner. The use of illustrative examples, particularly in the boxing game scenario, helps to ground the abstract concepts in concrete applications.

The significance of this work lies in its potential to enhance the realism and strategic depth of AI agents in multi-player games. By incorporating Feint behaviors, the authors have shown significant improvements in game rewards and diversity, which could lead to more engaging and challenging game AI. Furthermore, the unified implementation scheme for common MARL frameworks suggests broad applicability of this approach.

Weaknesses:
While the paper presents a novel approach, there are several areas where it could be strengthened:

1. The primary experimental validation is conducted on a custom boxing game scenario. While this provides a good test case, it may not fully demonstrate the generalizability of the approach. The authors should consider including experiments from a wider range of game types (e.g., strategy games, team sports) to show the broad applicability of their formalization.

2. While the paper provides a detailed formalization, it lacks rigorous theoretical analysis or proofs of the properties of the proposed approach. For instance, the authors could provide theoretical bounds on the performance improvements or convergence guarantees for their method.

3. The paper primarily compares the performance of agents with and without Feint behaviors. However, it would be valuable to see comparisons against other existing methods for modeling deceptive or strategic behaviors in games.

4. The formalization and implementation details are quite complex, which could make it challenging for others to replicate or build upon this work. The authors could consider providing a simplified version or pseudocode of key algorithms to improve accessibility.

5. While the paper focuses on game simulations, it doesn't adequately address how this approach might be applied to or impact real-world game design or AI systems beyond simulations.

Limitations:
n/a

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
D7X9Grmd7L;"REVIEW 
Summary:
This paper proposed a new type of change detection model that supports zero-shot prediction and generalization on unseen change types and data distributions. The proposed method called AnyChange is built on the segment anything model (SAM) via our training-free adaptation method. By revealing and exploiting intra-image and inter-image semantic similarities in SAM’s latent space, the proposed AnyChange could perform change detection. The authors also designed 1 1-point query mechanism for AnyChange, leveraging SAM’s point prompt mechanism and our bitemporal latent matching for filtering desired object changes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Building a foundation model for object change detection is novel and interesting.
2. The authors provided comprehensive experimental results on 4 various datasets. Some designed baselines such as different SAM variants (such as SAM+Mask Match, SAM+CVA Match, and DINOv2+CVA) are reasonable.
3. The authors considered various settings to demonstrate the ability of the proposed method and better revealed the power of the proposed method.

Weaknesses:
1) I have one main concern about the capacity of the SAM. Due to SAM being optimized by dense masks with the high-level implicit semantics, I doubt whether SAM has the ability to detect some very minor changes if the authors are performing bitemporal latent matching in the feature space of SAM. The authors should provide more demonstrations about this. 

2) Have the authors evaluated the robustness of their proposed method to the illumination/color changes and viewpoint changes?

3) As reported in Table 3, why did not the authors report the experimental results of using 10% GT and 100% GT? From current results, the proposed method did not demonstrate the advantage over existing algorithms even though I know they used fewer annotations. The accuracy is far below the existing results, which is not acceptable. The current results only demonstrate that the proposed method has a stronger zero-shot ability but I am curious about the upper bound of the proposed method if the proposed method is conducted at the same experimental setting. Furthermore, I do not really understand the meaning of ""This confirms the potential of AnyChange as a change data engine for supervised object change detection."" in Lines 308-309.

4) The authors should create one new table to combine some results (on the S2Looking dataset) from Table 1 and Table 3 to provide a better comparison. Table 1 demonstrates the zero-shot ability of some designed baselines and Table 3 reports the results under the supervised setting. However, I also noticed that AnyChange (Oracle) only achieved 62.2, 57.6, and 67.6 for F1, Prec. and Rec., respectively on S2Looking dataset. Meanwhile, the best results on the S2Looking dataset in Table 3 are 67.9, 70.3, and 65.7. I am very doubtful about the ability of the proposed method as the author said ""Oracles obtained via supervised learning have superior precision"" (Line 248-249). This observation also aligns with my doubt about whether it is suitable to perform change detection built on SAM.

If the authors could address such main concerns, I am willing to raise my score after the rebuttal.

Some minor issues:

1) The generalization ability of the proposed method to unseen images as they claim the proposed model is a foundation model.

2) The figures should be reorganized to better illustrate the differences between the pre-event image and the post-event image.

3) The best results in Table 3 should be bold.

Limitations:
The results of the current version are not that convincing to demonstrate that the proposed AnyChange is better than existing algorithms under the supervised setting. I am also skeptical about the ability of SAM to detect the minor changes, since SAM is mainly optimized by dense masks with some implicit semantics. From my point of view, SAM should not have a strong ability to identify small changes from its training nature.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose AnyChange, a novel framework for zero-shot change detection in remote sensing imagery. This framework leverages the Segment Anything Model (SAM) and introduces a ""bitemporal latent matching"" method to identify changes between images taken at different times. AnyChange identifies changes by comparing the semantic similarities of image regions in SAM's latent space, eliminating the need for explicit training on change detection tasks. Furthermore, the model incorporates a point query mechanism that allows for interactive, object-centric change detection through user clicks. Experimental results demonstrate AnyChange's effectiveness in various change detection scenarios, highlighting its potential as a valuable tool for researchers and practitioners alike.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
* One of the first works to propose zero-shot change detection of remote sensing imagery. The authors repurpose SAM for comparing satellite imagery captured at two different timesteps.
* The authors propose bidirectional latent matching technique which compares latent embeddings of SAM mask proposals coming from the bitemporal satellite imagery. The authors empirically showed high correlation between intra-image and inter-image latent embeddings, which eventually enables detecting changes.
* Optionally, the framework supports human in the loop point query mechanism to refine mask proposals from SAM and potentially reduce false positives.
* Custom baselines are constructed from scratch and experiments on three change detection benchmark datasets show superior performance of AnyChange over the baselines.

Weaknesses:
* Several components of the paper are poorly explained. SAM uses the MAE's image encoder based on ViT. The image features extracted from such a model are downsampled due to the patching effect. How is the framework able to compute pixel level features for the mask proposals? Is there an interpolation step?
* Although the idea is interesting and novel, it seems the model can easily be fooled by small radiometric changes between the timesteps or presence of other conditions such as clouds. SAM, which is extensively trained on consumer photographs, may easily confuse **seasonal changes**, which may not be relevant for a task. The experiments presented in the paper are on benchmark datasets and may not reflect practical applicability of the proposed framework.
* How does the matching algorithm handle overlapping objects with an image? Such as a tree canopy covering a part of the road. Averaging image embeddings in such cases might lead to erroneous results.

Limitations:
Limitations are discussed in the appendix section.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors address the problem of zero-shot change detection. While some models focus on zero-shot semantic segmentation, there hasn't been much work in the area of zero-shot change detection. The lack of availability of large change detection datasets makes it non-trivial to train such models from scratch using existing methods easily. The authors propose a training-free method to adapt SAM for change detection to circumvent this. They utilize the semantic space of SAM to find regions of change. More specifically, they propose ""Bitemporal Latent Matching"". For a given image, they extract the mask embeddings for each object proposal. They use the negative cosine similarity between two mask embeddings (at the same location at different times) as a measure of confidence of change. The region proposals are then sorted by their confidence scores and selected via thresholding. Their results show that they outperform other naive baselines for zero-shot change detection.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written with only a few grammatical errors
- The proposed method neatly avoids training making the approach quite resource-efficient. 
-  While the method is focused on Change Detection in Satellite Imagery, the proposed method could have more extended implications. This paper explores the idea of utilizing intra-image and inter-image similarities in SAM's embedding space to solve Change Detection in bi-temporal satellite images. This strategy can potentially aid in solving any vision tasks that require multi-temporal inputs.

Weaknesses:
- The paper lacks important ablation studies. For example: there is no experiment that shows the value of computing the change confidence scores bidirectionally as opposed to in a single direction
- I think the example used in Figure 4 is not the best to demonstrate the efficacy of the model. It is hard to know if the model is segmenting all buildings or just the buildings that have changed (since all buildings are changed between the two images). It might have been better to show examples where the changes are more localized; for example: only a few buildings are missing in image 2 and the rest of the area is unchanged. 

Minor Grammatical Errors
- Ln 132-133. ""we known""
- Ln 195 ""t denote""

Limitations:
The authors adequately discuss the limitations and societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the AnyChange model, aimed at enabling zero-shot change detection in remote sensing imagery. The model builds upon the SAM, utilizing a training-free adaptation method called bitemporal latent matching. This method leverages semantic similarities within and between images captured at different times to enable change detection without additional training. The paper demonstrates AnyChange's performance through extensive experiments, highlighting its effectiveness in various remote sensing change detection tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The problem and approach proposed in this paper are highly relevant to practitioners in the remote sensing field. Zero-shot change detection could significantly impact the field by enabling more flexible and scalable monitoring of environmental and infrastructural changes.
- The experiments presented in Table 1 are well-designed, with reasonable baselines. The methodology appears robust, and the results in Table 4, showcasing the performance of AnyChange as a change detection engine, are particularly promising for practitioners.

Weaknesses:
- The technical contribution of bitemporal latent matching does not appear to be very high. Defining feature differences based on cosine distance between latent representations in SAM's hypersphere domain seems insufficient for a significant contribution. The novelty and uniqueness of this approach compared to existing cosine similarity-based methods, such as those in Růžička et al. (2022), are questionable. The primary difference appears to be the use of SAM’s latent representations, which may not be enough to claim substantial innovation.
  - Růžička, Vít, et al. ""RaVÆn: unsupervised change detection of extreme events using ML on-board satellites."" Scientific reports 12.1 (2022): 16939.
- The paper's technical contributions seem too narrow for NeurIPS. The foundational observations Q1 and Q2 (lines 136 and 148) are only demonstrated with electro-optical images in the satellite domain, limiting the broader applicability of the proposed method to other domains or modalities.

Limitations:
They point out important limitation as vague definition and not concrete benchmark dataset.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
SOxxGATGsl;"REVIEW 
Summary:
The paper investigates a multi-armed bandit problem where the action space is a metric space a stochastic Lipschitz rewards. The authors present algorithms that use a constant amount of memory and achieve a near-optimal regret. This improves on previous results that had heavy memory usage.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper has good presentation, and the figures in the appendices are helpfulץ The contribution itself is useful in practice.

Weaknesses:
While the result is great, the ideas presented in the paper are modifications of existing methods

Limitations:
I did not find the limitations presented as sufficient and I would like to see more discussion on the downsides of the presented algorithm and future directions of research.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the Lipschitz bandit problem with a memory constraint. There are two algorithms proposed by the authors. The Memory Bounded Uniform Discretization (MBUD) algorithm uses a fixed discretization over the metric space and implements a strategy which explores first and then commits to an exploitation phase. The second algorithm, called Memory Bounded Adaptive Discretization (MBAD) , swaps arms in and out of the memory while creating a mesh over the metric space adaptively (ala zooming). The authors prove upper bounds which match lower bounds from previous work for Lipschitz bandits without memory constraints while maintaining linear time complexity and constant space complexity. Finally, the authors perform experimental validation of the theoretical results on small 1-dimensional datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Novel problem formulation in the Lipschitz bandit setting.
2. The authors show upper bounds matching with lower bounds from prior work while maintaining a memory budget on arms.
3. The concepts introduced in the paper are well explained for the most part. I did have a little trouble reading the parts about the crosscut and generating cubes but it might just be me not being familiar with prior work in the area.

Weaknesses:
I am unclear about the novelty and contributions of the paper. The problem formulation (limited memory) is new in the Lipschitz bandit setting but it has been studied in several papers in bandits with finite arms (as the authors point out in the related works). Moreover, the proof techniques used in the paper appear standard - MBAD is based on zooming introduced by kleinberg et al., the clean event analysis is from the recent textbook of Slivkins (and their papers), MBUD is based on an explore first strategy resembling the naive Explore-then-Commit algorithm (which trivially satisfies the O(1) memory constraint). In all, I’m not sure what specific parts of the paper are being claimed as novel vs that from prior work.

The experiments in this paper are very limited - only a 1 dimensional interval with an L1 metric. To show real world applicability, it would be nice to have results in higher dimensions and also on real world datasets (since that was the original motivation).

Minor/Typo:
I think the caption for Fig 1 should clarify what is on the X and Y axes. It is obvious from context but it would be nice to have from a readability perspective.

Limitations:
The Lipschitz constant needs to be known beforehand to apply these algorithms.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes two algorithms for Lipschitz bandit problems, with improved time complexity and memory requirements.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
4: excellent

Strengths:
If the algorithms and proofs are sound, then this is an excellent contribution. Developing these sort of streaming/sketching methods for key bandit problems (such as Lipschitz bandits) is an important area of research, and many people are likely to care about the results of this paper.

Weaknesses:
The paper is sloppy to the extent that it is difficult to understand the authors' algorithm or verify their claims. To be specific, consider the following sentences, all taken from a single two-paragraph subsection (section 2.2):
1. ""Let $\\{\mathcal{X}_1, \dotsc, \mathcal{X}_N\\}[\mathcal{X}_i \subset \mathcal{X}]$ be an cover of the action space $\mathcal{X}$"" --- okay, what is the $\\{\dotsc\\}[\dotsc]$ notation?  
2. ""Let $\epsilon$ denote the maximum diameter of $\mathcal{X}_i$ for all $i \in [N]$."" --- okay, but what is a diameter? Are we in a metric space? This hasn't been specified. 
3. ""Then the arm set $S = \\{ x_i \mid x_i \in \mathcal{X}_i, i \in [N]\\}$ is an $\epsilon$-mesh.""  --- what set is this? Now, I presume that the authors mean to say that they want $S$ to be any set that contains a single element chosen arbitrarily from each of the $\mathcal{X}_i$, but that's not written, instead the authors said its __the__ set, but the right hand side does not specify any unique set. Also, the concept of an $\epsilon$-mesh has not been defined, and when its defined, it needs to be with respect to some metric. And if this description was meant to be the definition of an $\epsilon$-mesh... then that's not clear either (and the definition given by the work the authors state these definitions are from, i.e. Slivkins 2019, is _very_ clear---all the authors needed to do was copy it).
4. ""The covering dimension $d$ of the action space $\mathcal{X}$ is defined as $d=\inf_{\alpha \geq 0}\\{|\mathcal{S}| \leq \epsilon^{-\alpha}, \forall \epsilon > 0\\}$."" But the set $\mathcal{S}$ does not depend on $\epsilon$ (not even implicitly)... (the correct definition, I presume, would be to ask that $\mathcal{S}\_{\epsilon}$ is a minimal $\epsilon$-cover of $\mathcal{X}$ in some metric $D$, and then have that infimum include $\mathcal{S}_\epsilon$ and not $\mathcal{S}$.)
5. ""Define $\mathcal{Y}\_j = \\{x \in \mathcal{X} \colon 2^{-j} \leq \Delta(x) \leq 2^{1-j}, j \in \mathbb{N}\\}$, then the set $\mathcal{Y}_j$ contains all arms whose gap is between $2^{-j}$ and $2^{1-j}$."" --- but $j \in \mathbb{N}$ is within the constructions of the set on the RHS, which could be read as asking that the condition holds for all such $j$, or for some $j$, but it breaks the dependence of the right hand side on the subscript $j$ of $\mathcal{Y}_j$. Of course, the definition shouldn't have the $j \in \mathbb{N}$ inside the $\\{ \dotsc \\}$ on the right hand side of the definition. 
6. ""Consider the $\epsilon$-mesh $\mathcal{S}_j$ for space $\mathcal{Y}_j$."" --- __the__ $\epsilon$-mesh? Also, $\mathcal{S}_j$ hasn't been defined. This should say instead 'fix some $\epsilon > 0$ and let $\mathcal{S}_j$ be an $\epsilon$-mesh of $\mathcal{Y}_j$', or something like that.
7. ""[...] the zooming dimension focuses only on the set $\mathcal{Y}_j$"" --- no, the zooming dimension depends on all the sets $\mathcal{Y}_1, \mathcal{Y}_2, \dotsc$, not only a single one of those sets.

While each individual mistake or ambiguity can be resolved easily enough, verifying the authors claims would require me to rewrite everything myself, and this goes beyond what I'm willing to do (and should do...).  The whole paper is like this, and it's just not acceptable.

I would urge the authors to, in the future, have someone _not intimately familiar with the work_ proof-read the work.

Note, I put down confidence as ""5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."" --- I am indeed very familiar with the related work, but I have not checked the math/other details carefully. It's too much work to read it. I am absolutely certain, however, that this level falls short of any level of clarity that might be expected in published work.

Limitations:
.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper considers regret minimization for Lipschitz bandits with time horizont $T$ and proposes an algorithm that provably achieves nearly optimal regret while having strictly smaller (by a factor of $T$) time (of order $O(T)$) and memory complexity (of order $O(1)$). This is achieved by considering a tree-like embedding of the state space and pairwise comparison between elements of the tree. A suboptimal method with uniform discretization called MBUD has dependence on the covering dimension of the state space, while MBAD, a method with adaptive discretization, instead has dependence only on the zooming dimension.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) Achieving nearly optimal regret bounds in minimax (MBUD) and instance-specific setting (MBAD), while reducing time and memory complexity. 

2) Both proposed algorithms are non-trivial and seem to be novel and interesting on their own.

Weaknesses:
1) It is not simple to parse algorithms in their current form in a short amount of time. Although you give comprehensive descriptions in text, I believe adding illustrations or additional explanations will significantly improve clarity of your algorithms. 

2) I would appreciate a more explicit comparison with previous work - what parts of the algorithms were already reported in the literature?

Limitations:
The authors have addressed limitations adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
TI7Vy90B9j;"REVIEW 
Summary:
This paper studies last-iterate convergence rates of online learning in monotone games. The main contribution is an algorithm called Gradient Ascent with Boosting Payoff Perturbation (GABP). The GABP algorithm achieves (1) $O(\log T / T)$ last-iterate convergence with full gradient feedback, which is near-optimal; (2) and $O(1/T^{1/7})$ last-iterate convergence with noisy gradient feedback (the noise is zero-mean with bounded variance). The latter result improves prior results of $O(1/T^{1/10})$. Moreover, the GABP algorithm guarantees an individual dynamic regret of $O(\log^2 T)$ under full gradient feedback, slightly worse than the state-of-the-art bound of $O(\log T)$. This paper also contains numerical experiments on small game instances to demonstrate the effectiveness of GABP.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The problem of last-iterate convergence rates of no-regret learning algorithms in monotone games is relevant and interesting. Most existing results focus on the full gradient feedback, while only a few provide concrete convergence rates under the noisy gradient or the bandit feedback. The proposed GABP algorithm has near-optimal $O(\log T / T)$ last-iterate convergence rate under full gradient feedback. It also improves the convergence rates under noisy gradient feedback from $O(1/T^{1/10})$ to $O(1/T^{1/7})$. This is a solid contribution to learning in games, although the rate for the noisy gradient feedback setting may not be tight.

Weaknesses:
1. The proposed GABP algorithm does not achieve the optimal $O(1/T)$ last-iterate convergence rate under full gradient feedback. The $O(1/T^{1/7})$ last-iterate convergence rate is also not tight for the noisy feedback.
2. The relationship between the proposed GABP algorithm and the AOG algorithm in [1] and the intuition behind the fast last-iterate convergence rates is not clearly discussed. These two algorithms are different (as shown in Appendix F) but share similar ideas. The anchoring term in both algorithms comes from the (implicit) Halpern iteration algorithm, which can not be run directly. The difference is that GABP views each step of Halpern iteration as a fixed point problem (Line 170) and uses an inner loop of $\log (1/\epsilon)$ steps to get an $\epsilon$-approximation (this is called updating the reference strategy in the paper.); In contrast, AOG directly uses optimism to approximate the implicit update. This leads to GABP being a log factor slower than AOG in the full gradient setting. However, the approximating the fixed point approach is more robust in the noisy gradient setting due to strong monotonicity. Moreover, the potential function and the approximately non-increasing potential analysis are very similar to that used in [1]. If they are inspired by [1] then this should be acknowledged. 

[1] Doubly Optimal No-Regret Learning in Monotone Games, Cai and Zheng, ICML 2023.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel algorithmic approach to enhance the convergence of first-order methods in the context of monotone games. The authors propose a payoff perturbation technique that introduces strong convexity to players' payoff functions, which is crucial for achieving last-iterate convergence. This technique is particularly designed to handle scenarios where the gradient of the payoff functions is monotone and potentially noisy. The paper presents a method called Gradient Ascent with Boosting Payoff Perturbation (GABP), which incorporates a unique perturbation into the payoff function and maintains a periodically re-initializing anchoring strategy. The authors demonstrate that GABP offers faster last-iterate convergence rates compared to existing algorithms, even in the presence of additive noise.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper presents a unique perturbation technique that addresses the challenge of last-iterate convergence in monotone games. The proposed GABP algorithm is an innovative modification of Adaptively Perturbed Mirror Descent (APMD), offering improved convergence rates.

Quality: The theoretical development is thorough, with rigorous proofs provided for the convergence rates of GABP in both full and noisy feedback settings. The paper also includes a detailed analysis of the algorithm's performance in terms of individual regret.

Clarity: The paper is well-organized, with clear explanations of the algorithm, theoretical results, and experimental setup. The use of pseudo-code for GABP aids in understanding the algorithm's implementation.

Significance: The work contributes to the field of online learning in games, providing a solution that is particularly relevant for applications such as Generative Adversarial Networks (GANs) and large language model fine-tuning, where last-iterate convergence is desirable.

Weaknesses:
Experimental Validation: While the paper provides empirical results, the experiments could be expanded to include a broader range of game types and noise levels to further validate the robustness and generalizability of GABP.

Comparison with State-of-the-Art: The paper compares GABP with APMD and Optimistic Gradient Ascent (OGA) but could benefit from a more comprehensive comparison with other existing methods in the literature to better situate its contributions.

Practical Considerations: While the paper addresses the theoretical aspects of GABP, it could provide more insights into practical considerations, such as the implementation challenges and potential modifications needed for real-world applications.

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work focuses on last-iterate convergence of  game dynamics. A payoff perturbation technique is proposed by adding strong convexity to players' payoff functions. Despite it is a well studied technqiue in learning in repeated games with first-order methods, especially in last-iterate convergence, a novel perturbation scheme introduced in this paper allows on to provide faster last-iterate convergence compared to previous works.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper provides a relatively complete result containing last-iterate convergence rate of the proposed algorithm GABP in full feedback and noisy feedback. The faster rate of convergence is an improvement compared to existing works. Except for some weakness (will be stated later), the presentation of this paper is clear to understand. The authors have reviewed most related works to my best knowledge, so that the contributions claimed are easy to follow. Addition to theoretical works, this paper has provided experiments (sufficient in my opinion) showing the comparison of GABP and existing algorithms such as Adaptively perturbed gradient ascent and Optimistic gradient ascent.

Weaknesses:
One obvious spot that should be added to improve the presentation is the following. The game considered in this paper is motivated by real-life examples. But the authors only give one example motivating monotone games. Part of contributions of the paper is claimed to be the study of two feedback models: full feedback and noisy feedback, but there is not specific examples and applications illustrating the importance of these settings. For sure readers can always find related works even just by googling the keywords, but providing concrete application scenes where the gradient of payoff can be achieved perfectly or only partially achievable gradients can be obtained is important, especially ""noisy feedback"" can be just a model of many cases.

Limitations:
This is theory based paper, no potential negative impact will cause.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies first order methods to solve monotone games where the gradient of the payoff function is monotone in the strategy, along with additive noise. The authors introduce a payoff perturbation technique which introduces strong convexity to the to the payoff functions and thereby derive last iterate convergence rates.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall the paper is well written and the method and results are interesting.

Weaknesses:
The authors should include a table which compares their paper with others in the literature. This would make it easier for the reader to place the results in context and see where improvements are made more easily.
(for example comparison to [Yoon and Ryu, 2021, Cai and Zheng, 2023] including constants)

Limitations:
See Weaknesses and Questions.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
XxSME6GE1G;"REVIEW 
Summary:
The paper introduces TAIA, a novel method for enhancing the performance of large language models (LLMs) in data-scarce domains with domain-mismatched data. The authors identify that during fine-tuning, only attention parameters significantly contribute to downstream task performance when training and test sets' distributions do not align. The proposed TAIA method retains updates to attention parameters while discarding updates to feed-forward network parameters, effectively improving performance on out-of-distribution (OOD) tasks. Extensive experiments demonstrate TAIA's superior performance across various datasets and model configurations, showcasing its robustness and generalizability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The TAIA method is novel to selectively retain beneficial parameter updates during fine-tuning.
- The paper provides a thorough analysis of the roles of self-attention and feed-forward networks in LLMs.
- The comprehensive experiments validate the proposed method.

Weaknesses:
- The presentation could be clear. For example, figure 2 could be more clear with a better caption. Currently, the caption is not informative for me to understand the method and its effectiveness. In addition, Figure 3 also made me confused. It seems that the base model performance pretty well. If this is the case, why should we use other methods?

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes the hypotheses that under some circumstances LLMs are better trained by ft all parameters and then dropping the learnt FFNN layers. The proposed method is named TAIA from training all parameters but inferring with only Attention. 

A comparison of TAIA's the performance together with another variants like TOA (train only attention) is carried out.
More over  other ablated version using the FF components instead of the attention were also evaluated.
 Several model are considered during the study such as Llama2 LLama3 and Qwen. 

Several datasets (Alpaca , CoT-collection, ...) are used to train the models with the proposed technique and baselines.
Extensive comparison are also done with other OOD techniques such as L2, EC and self-distill, among others.
Several dimensions are taken into account such as varying data sizes. 

The results are positive, showing the new proposed techniques improves performance when models are trained OOD. 

Finally an honest limitation section with some experiments show conditions where the proposed method will not work better

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper performs several experiments varying many factors to validate there method.

Several LLMS were used in the experiments, which helps into drawing generic conclusions. 
Many datasets are considered for evaluating  2 aspects : reasoning and knowledge. 
Ablation studies are carried out with other baselines varying TAIA is proposed.
Models are compared with other OOD techniques such as Self-distillation, L2, or WEC. 
Representation analysis is carried out as well as quantitative analysis.

The discussions are very interesting with clear research questions for which corresponding experiments were executed. 
Limitations include experiments to highlight some of the circumstances when the proposed technique doesn't work well. 
Finally, the extensive appendices specify many details on experiment, additional results and detailed explanations that help the reading of the paper.

Weaknesses:
It would have nice to see what happens with even larger scale models.
In some of the experiments additional rounds or repetitions would have helped to understand the variability of the results. 
Some deeper theoretical analysis could help to explain the unexpected and satisfactory results of the proposed technique.

Limitations:
Authors do an excellent work addressing and studying their proposal limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a simple but effective strategy to improve fine-tuning of LLMs: fine-tune as normal but at inference time, only use the fine-tuned attention parameters but use the pre-trained MLP parameters. The intuition for this is that knowledge is generally stored in the MLP layers, and fine-tuning introduces harmful modifications to the pre-trained knowledge.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The results are strong, showing that TAIA improves over both vanilla fine-tuning and the baseline of only training attention, across a number of test datasets and models. It also improves over continual learning strategies, which have the same intuition of wanting to preserve aspects of the pre-trained model.

There is extensive analysis showing, among other results, that TAIA works both for LoRA and full fine-tuning, it is also better against red-teaming,

Weaknesses:
My main concern is from Figure 4a, which seems to show that the gap between Vanilla and TAIA decreases as the training dataset gets larger. This also made me realize the paper does not discuss how much training data is used in the main experiments. Figure 4 also does not disclose which model is being evaluated. I think it is important to address exactly how the results in Figure 4a compare with the CoT-Collection results in Table 1, where TAIA seems to be much better than Vanilla.

Relatedly, I would suggest discussing more about dataset size in Section 4.2.

Limitations:
Limitations are appropriate.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel inference-time intervention method that trains all model parameters but retains only the self-attention updates for inference. This approach, named ""Training All parameters but Inferring with only Attention"" (TAIA), optimizes performance across a variety of downstream and closed-book tasks, as validated by extensive experiments. The paper confirms the reproducibility of TAIA in terms of fine-tuning methods and dataset scales. TAIA also maintains the few-shot adaptation ability of base models and withstands multi-level adversarial attacks, consistently improving performance in vertical domains like healthcare even with increasing OOD data.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper introduced an innovative inference-time intervention strategy termed ""Training All parameters but Inferring with only Attention"" (TAIA), which trains all model parameters while retaining solely the refined self-attention updates for inference. 
	2. Through extensive experimental validation on a variety of datasets and with LLMs of varying parameter sizes, the authors showcased that TAIA consistently outperforms both fully fine-tuned models and base models under most conditions, achieving substantial performance enhancement.
	3. The authors substantiate the robustness and generalization capabilities of TAIA across different fine-tuning methodologies and dataset scales. TAIA also preserves the base models' few-shot adaptation prowess and defends against multi-tiered adversarial assaults, markedly elevating performance in niche domains such as healthcare, even amidst escalating OOD data.

Weaknesses:
1. The authors posit that the parameters of Feed-Forward Networks (FFNs) encapsulate a substantial amount of pre-trained knowledge. However, these parameters are prone to shifts during Supervised Fine-Tuning (SFT), which may result in suboptimal performance on Out-Of-Distribution (OOD) data. But there is a notable absence of empirical evidence to substantiate this claim and merely citing other studies is insufficient.
2. The authors train the FFN parameters during the training process but discards them during inference. Could this lead to an inconsistency between the training and inference?
3. The authors lack implementation details on how to apply TAIA to the full finetune in response to RQ1.
4. The author should include a comparative experiment, utilizing only the parameters of the FFN while keeping the attention parameters fixed during the inference process. This will highlight the validity of TAIA.
5. The method section primarily contains text without any mathematical formulation, making it challenging to follow. Including formulations on how the model learns from different types of data and how it only infers with attention during inference would improve clarity and make it easier to evaluate the method's contribution.
6. Adding some theoretical proof to support your claims would strengthen the paper.
7. The overall results indicate a serious overfitting issue, as the base models have shown competitive results, such as 53.76 (base) vs. 53.24 (TAIA). Could you provide more insights into this observation?

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
rM3FFH1mqk;"REVIEW 
Summary:
The authors propose a new algorithm for measuring the Gromov-Wasserstein (GW) distance, an metric for assessing the similarity of point clouds in different spaces. Their algorithm formulates the computation of the GW distance as a quadratic programming problem, which is then solved by semidefinite relaxation. The authors conducted experiments using several synthetic datasets and confirmed that the proposed algorithm yields solutions with smaller objective function values compared to other algorithms.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
* The proposed method is grounded in solid theoretical foundations.
* The proposed method has been experimentally verified to produce solutions with smaller objective function values.
* The paper is well-written, and its contributions are clearly described.

Weaknesses:
* The novelty is limited. It is well-known that the computation of GW can be reduced to a quadratic programming (QP) problem, and solving a non-convex QP through semi-definite relaxation is a very common approach in the field of optimization. Additionally, as the authors themselves point out, such methods have been proposed for the quadratic assignment problem (QAP), which is closely related to GW. Thus, the proposed method is merely a simple variant of these approaches, and its technical contribution is minimal.

* The computational complexity of the proposed method. The proposed method requires solving an SDP with a matrix of size 
$mn \times mn$ as variables. Although SDPs can indeed yield globally optimal solutions with relatively low computational effort, it is well-known that the computational complexity increases sharply with the size of the problem. It is thus challenging to solve an SDP with a matrix of size $mn \times mn$ as variables in practical applications. In fact, Table 1 shows that for 
$n=20$, the computation time exceeds 200 seconds, indicating difficulties in applying the method to real-world problems.

* Insufficiency of the experiments. The experiments conducted in the paper are all small-scale and limited to ten artificial datasets, which is insufficient to demonstrate the effectiveness of the proposed method.

Limitations:
The authors are aware of the significant computational complexity and discuss potential directions to address this issue. While this acknowledgment is commendable, the substantial computational complexity remains a critical drawback of this method. Unless this issue is resolved, the overall contribution of the paper must be considered limited.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper explores a semi-definite programming (SDP) based relaxation of the popular Gromov Wasserstein (GW) problem. The GW problem  is an instance of non-convex quadratic program (QP). Standard SDP relaxation of QPs has been explored in the literature. The present work leverages this SDP relaxation result. However, this standard SPD relaxation is not sufficient as the resulting minimization problem is unbounded from below. Hence, the paper tightens the relaxation via additional constraints which are motivated from the GW problem. Empirical evaluations are performed to showcase the effectiveness of the obtained solution, both in terms of quality and runtime efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed SDP relaxation based approach for GW problem is an interesting idea and has not been explored in context of GW (to the best of my knowledge). This reformulation has interesting consequences such as 
    - an approximation ratio (Eqn 4) which can be computed from the solution obtained via the proposed solution. This ratio lower bounded by 1 and is equal to 1 only if the obtained solution is globally optimal. Hence, this approach provides an optimality certificate for a non-convex problem
    - While no empirical results were shown in this regard, the proposed approach allows using a general GW cost tensor L. Existing approach such as (Peyré et al., 2016) can only employ decomposable costs (such as those obtained via L2 or KL loss). 

- The paper empirically evaluates the effectiveness of the proposed approach in terms of computational efficiency and quality (lower objective is better). While the proposed approach obtains better objective compared to current state-of-the-art GW-CG, its runtime (for problems of size n = 6,12,20) is around 500-150000 times higher than GW-CG (Table 1).  While the heuristic solver (GW-PGD) proposed in Section 5 is faster than the proposed GW-SDP, it is still at least 250 times slower than GW-CG and its objective becomes comparable to GW-CG as n increase. 

- The paper is well written, explaining the underlying concepts and the related works nicely.

Weaknesses:
- The paper provides a detailed discussion on the literature related to the quadratic assignment problem (QAP) and SDP relaxations of the QPs. The key technical contribution of the proposed work w.r.t. Zhao et al. (1998) is removing constraints related to \pi being a permutation matrix, which also allows handling the m \neq n setting (lines 125-137). 

- Very high runtime compared to GW-CG. This limits the practical utility of proposed GW-SDP or GW-PGD.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a semidefinite programming (SDP) relaxation of the Gromov-Wasserstein (GW) distance. While the GW problem is non-convex, the proposed SDP relaxation is convex and hence can be solved in polynomial time with any off-the-shelf convex solver. The authors also provide an accompanying proof of global optimality for the relaxed problem, which can be checked efficiently. The numerical experiments use an off-the-shelf solver to solve the proposed SDP relaxation and compare it with two solvers from the PythonOT package, i.e.,  Conditional Gradient (CG-GW) solver and  Sinkhorn projections solver on Matching Gaussian Distributions and Graph Community Matching. Lastly, the authors present a simple heuristic algorithm optimized for their proposed SDP relaxation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The authors propose a novel SDP relaxation of the GW problem, which has not previously been explored in the literature. The theory is compelling.
* The proposed SDP approach has several compelling advantages over existing GW solvers: the solver given by POT (which implements Frank-Wolfe) can only find local optima, and entropic GW cannot be used for general cost tensors. Meanwhile, the proposed approach is broadly applicable and to my knowledge the first tractable GW solver for which the optimality of solutions can be efficiently verified.
* The paper is generally straightfoward to follow and the proofs do not seem to have any obvious mistakes.
* The authors motivate the proposed SDP relaxation well.

Weaknesses:
* The empirical evaluation is limited in scope and done entirely with synthetic data. In particular, the authors make the claim that the SDP relaxation frequently computes globally optimal solutions, but I feel the experiments are not extensive enough to fully support such a claim. A more comprehensive experimental evaluation would be required to better understand the practical applicability and limitations of the proposed method.
* The first experiment (Gaussian matching) is not that compelling to me. In Section 4.1, the experiment only considers up to 30 samples in each distribution and Figure 2a seems to suggest the possibility that Frank-Wolfe could perform similarly to the SDP relaxation for larger number of supports while being substantially faster. This is in line with what is reported in Table 2 with the heuristic algorithm, where the gap in performance between SDP and FW diminishes for larger number of supports.
* It seems as though the estimation gap for the first point in Figure 1b is less than 1, though this should not be possible.  
* Minor suggestions: 1) The readability of the paper could be enhanced by stating where proofs can be found for each claim, where the claim is presented. For instance, Proposition 3.1 is stated without mentioning where the proof can be found, and similarly for the theorems in appendix B. 2) I assume the numbers in parenthesis in Table 1 are standard errors based on Table 2. It would be helpful to state this in the caption.

Limitations:
The main limitation of the proposed work, i.e., high dimensionality of the matrix $mn\times mn$ in SDP, is provided by the authors.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors provide SDP relaxations of the Gromov-Wasserstein distance, which turns out to provide global optimal in many cases.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
-- The SDP relaxations and proofs of their exactness are most elegant. 

-- The authors suggest that their method does not make as strong assumptions on the loss as in the case of the previous work (e.g., the Proximal Point algorithm).

Weaknesses:
Commonly used pythonOT library (https://pythonot.github.io/auto_examples/gromov/plot_gromov.html) implements the conditional Gradient algorithm (ot.gromov.gromov_wasserstein), the Proximal Point algorithm with Kullback-Leibler as proximal operator (ot.gromov.entropic_gromov_wasserstein), and the  Projected Gradient algorithm with entropic regularization (ot.gromov.entropic_gromov_wasserstein), but the authors do not compare their run-time against neither of the methods. Plausibly, this is because the run-time of the SDP solver is much higher?

Limitations:
The run-time is not discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
IGAN7RldcF;"REVIEW 
Summary:
This paper studies a model of content creation and consumption on arbitrary online user-generated content platforms (e.g., YouTube, TikTok). It focuses on a type of Cournot competition in which creators mainly modify their creation volume. The paper provides a description of this model, a theoretical analyses of the Pure Nash Equilibria in this setting, an analysis of how platform designers might use mechanism design to balance consumer and creator utility, a framing of this balancing problem as an optimization problem solvable via (approximated) gradient descent, and experiments using purely synthetic data (sampled ""users"" with Gaussian preferences) and empirical data (users with preferences from the MovieLens dataset, popular in recommender systems).

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Overall, this paper provides a strong overall contribution and number of results and insights that will be of interest to a number of different communities -- researchers interested in UGC and online communities, mechanism design, ML for social media, etc.

The clarity is high throughout. The paper begins with strong and well argued motivation, the organization is helpful, and in general the overall narrative of the paper is clear.

In terms of novelty, this paper directly builds on a previous modelling work, but is very upfront about highlighting what the main differences and additions are in terms of contribution. The experiments seem to especially build off the design of [40] (esp. in terms of the synthetic data + MovieLens combination), which might be worth mentioning if that is intentional.

Overall, the potential significance of this work seems potentially high.

Weaknesses:
Overall, I expect readers won't have any major concerns with the theoretical results or experiments (see some minor questions below in the Questions section). 

Rather, the main threat to the significance of this paper is making the case that that a Cournot-style is actually common in the UGC platforms being invoked here. Of course, even if only a few platforms really end up being well-described by the model, the contribution is still very meaningful. That said, a few specific concerns with the current draft:
- a number of specific platforms are mentioned by name: YouTube, TikTok, Netflix, Spotify, and MovieLens. 
- Only data from MovieLens is used (which is very reasonable -- it's a very popular dataset for academic work for good reason).
- However, the named platforms vary quite a bit in terms of their actual creator competition, i.e. one would expect the incentives of a platform like Netflix (which also acts a creator agent, sometimes with substantially higher budget than other creators) to differ quite a bit from TikTok

See ""Questions"" section below for some specific questions about this concern that I think are likely to be in scope of a revision.

With this critique in mind -- that certain platforms might violate the assumptions needed for the model to work well -- I think the current draft may overstate the generality of the conceptual insight.

Limitations:
I do think the current draft could do more to justify the strength of the conceptual claims and/or hold a bit more space to explicitly discuss limitations (primarily, how well requisite assumptions hold across the platforms of interest). See above (Questions).

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the problem of the tradeoff between users’ satisfaction and creators’ engagement. Authors first define the traffic competition of creators on user-generated content platforms as a Cournot Content Creation Competition (C4) and establish corresponding PNEs. Based on PNEs, this work identifies the tradeoff between users’ and creators’ engagement and proposes the offline optimization solution to achieve the maximum social welfare by adjusting the exploration level of matching. Theoretical and empirical results are provided to support the effectiveness.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	Authors theoretically model the traffic competition among content creators as a C4 game, identify the tradeoff of user and creator engagement based on their theory, and finally find the optimal platform intervention to maximize the social welfare with the optimization method. Necessary proofs are provided with details. 

2.	Based on the synthetic and real-world datasets, authors validate the phenomenon of the user-creator tradeoff (Figure 1) and the benefit of optimizing \beta (Figure 2). Authors also provide the results in Appendix with different $\lambda$ in the objective $W_\lambda$ to investigate the sensitivity of their solution when the target is changed. 

3.	The manuscript is well-organized and easy to follow.

Weaknesses:
1.	Some assumptions are too strong, including (a) basic setups: Creators are producing contents with the same frequency and the same cost (only relate to the frequency) all the time. (b) platform intervention: all users contribute one unit amount of traffic, neglecting the dominant position of active users. 
2.	Although the effectiveness is guaranteed by the theory and empirical study on small datasets, authors should also present the potential of the solution to be applied in the practical scenarios, e.g., how is the efficiency of the optimization, how to conduct the daily update of intervention strategy. 
3.	Existing works have studied the C3 game. Authors may declare their unique contribution and improvement by considering “Cournot Competition” in their theory establishment and compare with previous methods in empirical validation.

Limitations:
1.	Limited practical value. The assumption is too strong, and the experiments are constrained on small dataset with 1,000 users.
2.	Unclear distinct contributions compared with previous works.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a new game-theoretical model Cournot Content Creation Competition ($C^4$), that studies the relation between the matching strategy of user-generated content (UGC) platforms and the production willingness of the platform’s content creators. Under certain assumptions, the authors show that the game has a unique Pure Nash equilibrium, and show that increasing matching accuracy elevates user satisfaction but also decreases the overall volume of content creation. Building on this tradeoff, the authors propose an optimization approach that balances the two objectives, providing both theoretical analysis and empirical simulations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Overall well-written.
- Interesting insight on the tradeoff between user satisfaction and creator engagement shown by theoretical analysis.

Weaknesses:
- (Main) Model might be too simplistic - authors assume users consistent produce work of same topic & quality and only changes the production volume.

- (Minor) The authors associate user satisfaction as a short-term goal for the platform and creation volume as a long-term goal for the platform. The authors make an argument for this in line 188-197, although I’m still not fully convinced:
   - The main imbalance that I feel comes from the fact that when I think of *long-term* goals of a platform, it's fundamentally intertwined with the ability for platforms to attract new and keep existing users, which comes from a user standpoint and not from a ""content volume"" standpoint. I get the authors argument when they mention how content creation frequency might harm user satisfaction (line 192 “users can hardly be satisfied by their previously consumed material”). However, given individual's limited attention span, I think this only happens when the number of creators are quite limited, and that it's unclear that a decrease of production frequency from say 2 weeks -> 3 weeks will result in a significant harm to the *long term* viability of a platform causing users to drop out in the long-term.
   - In general, this seems to point to an alternative model where the content volume *comes in* the user's utility model, where users’ utility are not only determined by how they liked the recommended content (which is the utility considered in the paper) but also by the availability of content on the platform, and they might drop out of the platform when their utility falls below a certain level. From this lens, it's less clear that this is a short-term v.s. long-term issue.

- Typo: i =1 -> j=1 in line 92, third and fourth -> second and fourth in line 347

Limitations:
The authors adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zeNwOAcb4q;"REVIEW 
Summary:
In this work, the authors proposed an approach to estimate the instance-dependent transition matrix in order to reliably learn from noisy labels. The idea is to use a condition diffusion model to estimate the transition matrix by using the pretrained extracted image features as the conditions. Once the transition matrices are estimated, the classifier is learned through the corrected cross entropy loss. Experiments are presented to compare the performance of the approach with other baselines using both synthetic and real noisy datasets.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper is easy to read and notations are clearly stated

Weaknesses:
The main weakness is the lack of support and discussion in substantiating the idea. Experiments are insufficient to support the claims.

Limitations:
No limitations are discussed

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper deals with the problem of supervised learning from noisy labels, where the label noise is modeled using instance-dependent label transition probability matrix. Mainly, this work attempts to leverage conditional diffusion model in order to obtain a generative model of transition matrix conditioned on the sample features. To that end, this work first generate pseudo paired samples $( x_i, T_i )_{i=1}^N$ using existing method (VolMinNet). Secondly, a conditional diffusion model is trained that generates $T_i$ given $x_i$. Finally, the classifier is trained taking into consideration the estimated transition matrix from the diffusion model.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The problem considered is of interest to the broad ML community
2. Adequate experimental settings, baselines, and ablations are provided for numerical validation.
3. The attempt to apply diffusion model is novel.

Weaknesses:
1. The technical soundness of the proposed method is questionable. Essentially, the proposed method trains a conditional diffusion model using paired samples $(x_i, T_i)$. If we consider the true transition matrix as $T(x)$ for a sample $x$, then the idea of the proposed method is to train a conditional generative model $p( T(x) | x )$. There are several issues with this attempt and the proposed implementation:
   (a) The authors use pseudo transition matrix $T_i$ generated from a sample-independent method (VolMinNet). $T_i$ only depends upon the cluster assignment of $x_i$. The diffusion model, at best, can approximate the conditional distribution $p( T_i | x_i )$. This has no clear relation to $p(T(x) | x)$. Therefore, in principle, the transition matrix generated by the trained diffusion model cannot be better than that returned by VolMinNet.
   (b) Second, the transition matrix is modeled as a deterministic function of sample, i.e., only one $T(x)$ exists for a given $x$. Therefore, it does not make sense to learn a generative model for $p(T(x) | x)$, since it is a degenerate distribution (probability of all other matrices should be zero except the true $T(x)$). 

2. Another hint at why the proposed method should be limited by the pseudo paired sample distribution is that the diffusion model training part (which is ultimately used as transition matrix estimator) does not require available noisy labels. Hence, no extra information can be extracted about the true transition matrix $T(x)$ beyond the information captured by the pseudo paired samples $(x_i, T_i)$. 

2. It is unclear where the performance gain in empirical results is coming from. The manuscript does not provide any intuitive or theoretical explanation to justify the quality of their estimator. Moreover, no rationale for the algorithm design is provided.

Limitations:
Limitations are not adequately discussed.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on the estimation of the transition matrix with instance-dependent label noise. They used a diffusion model for this estimation. By applying a diffusion process to the transition matrix, the diffusion model is trained to generate transition matrices from a prior distribution. The instance-wise generated transition matrix is then used to train the classifier with a forward cross-entropy loss. The improvement of the method is demonstrated by experiments on benchmark and real-world datasets.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
The instance-dependent label noise scenario is a challenging task.

Weaknesses:
* The reason for generating the transition matrix using a diffusion model is unclear.
  * The instance-dependent transition matrix is the target to be estimated, but it is uncertain what role training a diffusion model to generate the transition matrix without a fixed target.
  * In addition, as mentioned by the authors, the transition matrix must be satisfied: the entries are greater than 0, the row sum is to be 1, and the diagonal entry is typically the largest. However, these considerations have not been taken into account in the construction of the diffusion process. Although a transformation method is proposed in Section 3.4, there is no discussion of how this affects the training of the diffusion model.

* Pre-trained features are fed into the diffusion network, but their impact on the diffusion process has not been analysed. This could be seen as providing additional conditional information during the diffusion process, implying that this diffusion model might be a conditional diffusion model. It would be better to discuss these consideration.

* In Algorithm 3, it appears that the diffusion model is trained in order to generate the initialized $T_i$. I wonder if the desired training is for the initialized $T_i$ to be generated perfectly as is. This could lead to a transition matrix that might not contain instance-dependent information, raising questions about the mechanism by which diffusion training introduces variance.

* The diffusion training seems to take a considerable amount of time, which needs to be analysed. If it takes a long time, the performance improvement may not be significant in comparison.

Limitations:
They mentioned the limitations only briefly in the experimental section. I have noted additional limitations that I perceive in the Weaknesses part.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
iSMTo0toDO;"REVIEW 
Summary:
The paper proposed SubgDiff which is a diffusion model used in self-supervised learning setup to enhance the molecular representation learning. It introduces motif enhancement during the diffusion process to force the model to learn more structure information.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The idea of enhancing motif information in the diffusion process is promising.
2. The paper is well-organized and easy to follow.

Weaknesses:
1. The authors directly use the baseline results from the MoleculeSDE paper in their table; however, the results for MoleculeSDE are significantly lower than those reported in the original paper.
2. The proposed method is more like a graph diffusion model than a molecular representation learning model. It is limited to representation learning within a self-supervised learning framework. It would be beneficial to explicitly state this in the abstract or introduction.
3. The paper lacks an ablation study to evaluate the contribution of each component of SubgDiff to molecular representation learning.

Limitations:
No

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
SubgDiff  is introduced to improve molecular representation learning by integrating substructural information into the diffusion model framework. It offers three key technical contributions (subgraph prediction, expectation state, and k-step same subgraph diffusion) to enhance the network's understanding of molecular substructures. Experiments were carried out on several downstream tasks, particularly molecular force predictions.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Incorporate substructural information into diffusion model

Weaknesses:
* The denoising process need better explanations.
* Diffusion models excel at generating new samples. The application of SubgDiff to molecular property prediction/classification does not show its strengths. The state-of-the-art results are missed in Section 5.1. Section 5.2 doesn't compare generation results with the state-of-the-art.
* Explain COV-R and MAT-R in section 5.2

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a new denoising diffusion probabilistic model (DDPM) named SubgDiff, designed to enhance molecular representation learning by incorporating substructural information into the diffusion process. SubgDiff introduces a mask operation that selects subgraphs for diffusion, aiming to better capture the dependencies among atoms within substructures. The method includes techniques such as subgraph prediction, expectation state diffusion, and k-step same-subgraph diffusion, which together are intended to improve the learning of molecular properties related to 3D conformation. The paper claims superior performance on various downstream tasks, particularly in molecular force predictions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	Utilizing subgraphs for diffusion is a novel and intriguing exploration. 
2.	The experiments are thorough and demonstrate the effectiveness of SubgDiff across a range of molecular prediction tasks.

Weaknesses:
1.	The subgraph prediction model is trained on highly specialized datasets, which might pose a risk of overfitting. 
2.	The model integrates multiple complex diffusion stages, which could complicate the training and debugging processes and make them difficult to optimize. Particularly, adjusting hyperparameters and verifying model stability might require additional effort.
3.	Given the complexity involved in the expectation state and k-step diffusion processes, the model may have high demands on computational resources.

Limitations:
This paper introduces a complex diffusion model that leverages subgraph structures to enhance molecular representation learning. However, more limitations should be discussed, such as overfitting, computational efficiency, and generalization across diverse molecular structures.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a diffusion-based pretraining method using subgraphs to learn enhanced molecular representations. Unlike previous methods which normally add noise to every atom, this paper proposes adding noise based on subgraphs. The method is evaluated on various downstream tasks to demonstrate its effectiveness, such as 2D and 3D property prediction tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper offers an interesting perspective on existing molecular diffusion models and proposes several approaches to address them accordingly, which is inspiring.

- The paper presents many experiments and analysis to illustrate the results, which could provide valuable insights to the community.

Weaknesses:
- Although the proposed method effectively addresses the identified limitations, it does not provide much chemical intuition for the design. The authors claim that existing methods neglect the dependency in substructures, but the proposed method does not seem to integrate or learn the inherent molecular substructure information. The decomposition does not seem to be based on significant chemical knowledge, and the interactions or relations between various substructures are not explored. The motivation is not heavily grounded in domain knowledge.

- The method is a little bit confusing. The entire training process includes many steps and three key training objectives: subgraph prediction, expectation state, and k-step same-subgraph diffusion. The paper mainly describes each component separately, but it is unclear how the three objectives are leveraged during the training. Some details are also not clear. For example, “The mask vector $s_t$ is sampled from a discrete distribution $p\_{s_t}$ (S|G)”—does $p_{s_t}$ vary for each molecule? How exactly is the distribution obtained? Also, an overall framework could help better understand the process.

- The diffusion steps number is 5000, which is quite large. Molecular property prediction datasets normally contain small molecules. Are there any specific reasons for using such large steps? What are the computational costs?

- The proposed method seems impractical due to its complexity, while the performance improvement is not very significant. It is difficult to justify the trade-off between model complexity and performance. Perhaps the authors could provide a more in-depth discussion to advocate for their method compared to other baselines, beyond just the prediction performance.

- Some ablation studies are not conducted. For example, k seems to be an important h-param. What are the effects of different k values? Also, what about the performance of applying only one training objective?

Limitations:
Discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
RMdnTnffou;"REVIEW 
Summary:
This paper presents a type of label free concept bottleneck model for ante-hoc interpretability that incorporates a hierarchichal concept representation. The concepts are represented in a two-level hierarchy with high-level concepts denoting scenes/objects and lower level concepts denoting more specific attributes at a patch-level. Experimentally, the authors show better accuracy and better concept prediction (on datasets with GT annotations) while proposing a metric based on Jaccard index to judge quality of concept prediciton.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. I really like the idea of hierarchichal representation of concepts. It is natural and intuitive and novel in the context of concept representations for interpretability. It's specific instantiation here is also reasonable.
2. Experiments are comprehensive in in terms of multiple, diverse large-scale datasets and baselines generally.
3. The presentation in general is strong and motivations are clear.

Weaknesses:
1. I have some important concerns about interpretability and soundness of label-free CBMs in general that rely on CLIP embedding similarity for concept prediction. Please see Q.1, 2 in Questions tab.

2. The only baseline I would suggest adding would be standard CBMs, specially for concept prediction accuracy (Tab. 2). 
Although I expect standard CBM to perform better given its supervised training, but it'd be interesting to see how much is the performance gap if there is any.

3. It'd be interesting to see (even if qualitatively) how the system behaves with concept intervention like the original CBMs.

Limitations:
The authors do discuss them separately and clearly (partly in main paper and partly in appendix).

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The author introduced a novel Concept Bottleneck Model (CBM) that facilitates hierarchical concept learning. Specifically, the proposed Concept Discovery Block (CDB) plays a pivotal role in uncovering concepts from preprocessed image-text similarity embeddings by employing a variational Bayesian framework to learn a binary mask. Additionally, by applying the CDB module to each patch-level image to detect low-level concepts and the entire image for high-level concept discovery, information propagation between the two levels leads to robust classification performance through sparse concept learning.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- S1: As an ante-hoc interpretable CBM, the proposed method is intuitive and requires lightweight computation, which is desirable. 
- S2: The performance of the proposed method is superior to other multimodal CBM baselines.

Weaknesses:
- W1: The class/label designated at the high level and its attributes at the low level are strong hierarchical constraints. So, the proposed method was limited to showing its applicability only in cases with a transparent hierarchical relationship between attributes and classes by specifying the pool of low-level concepts corresponding to each class. This may require burdensome human inspection to configure.
- W2: Another concern is the fixation of the interpretable threshold to all CDB modules as 0.05. The author described it as the probability value used to determine whether the specific concept is active. However, even if some image patches have the same concept, it is evident that the concept may contribute to each patch to a different extent. Therefore, dynamically adjusting or learning the threshold may perform better than a fixed threshold.

Limitations:
Please check out the Weakness section.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work introduces a novel framework that leverages recent advances in vision-language models and a Bayesian approach for coarse-to-fine concept selection. It introduces the notion of concept hierarchy, allowing high-level concepts to be characterized by lower-level attributes and exploiting granular information in image patches.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The writing is fluent and easily comprehensible.
- Propose a novel way of assessing the interpretation capacity of CF-CBMs based on the Jaccard index between ground truth concepts and learned data-driven binary indicators. 
- Extensive experiments were conducted to demonstrate that the proposed CF-CBMs outperform other state-of-the-art methods in terms of classification accuracy and interpretability.

Weaknesses:
- Over-reliance on the vision-language backbone's capability might result in poor performance for images from uncommon datasets.
- There is a lack of experiments on test-time concept interventions.

Limitations:
The author mention in the Limitations of the dependence on the vision-language backbone.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose coarse-to-fine concept selection in Concept Bottleneck Models (CBMs). They introduce a concept hierarchy that identifies low-level concepts in local patches of input images, as well as high-level concepts in the overall images.  Additionally, the authors enhance interpretability by considering sparsity in concept predictions. Their proposed model, CF-CBM, achieves high classification performance while maintaining interpretability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors propose a novel evaluation metric based on Jaccard similarity to evaluate concept predictions.

2. The proposed method enhances both classification accuracy and concept prediction accuracy by making predictions from local patches in a sparse manner.

Weaknesses:
1. While the Jaccard similarity metric effectively assesses the alignment between the model's predicted concepts and the actual concepts, it serves only as one aspect of interpretability evaluation. Notably, Table 2 indicates that the Jaccard index is quite low, raising questions about whether such a score sufficiently demonstrates the model's interpretability. Additionally, it would be helpful to clarify what factors contribute to the low Jaccard index.

2. Moreover, the concepts described in the paper appear somewhat ambiguous. I encourage the authors to refer to the Questions section for further clarification.

3. A clearer explanation is needed regarding how the authors' proposed approach enhances classification accuracy and interpretability. Specifically, it would be helpful to understand whether predicting concepts from local patches is effective, if learning class predictions aids in identifying low-level concepts, and how the application of sparsity contributes to these improvements. Additionally, an ablation study is necessary to support these claims. This also includes an explanation of why this approach can be referred to as coarse-to-fine.

Limitations:
The authors have adequately addressed limitations in the Limitations & Conclusions section. Since the proposed method uses a frozen pretrained CLIP as its backbone, the ability to discover concepts may be constrained by the limitations inherent in CLIP's training.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
nu2Sqrsnr7;"REVIEW 
Summary:
The paper attempts to train PINNs which solves acoustic wave equations. They do so by using hard-constrained PINNs which can enforce IC and BCs, and propose a collocation point sampling method (DAFS) based on the amplitude of the solution at different regions.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper considers an interesting problem in acoustics and attempt to apply the techniques from PINNs to solve them.

Weaknesses:
The paper itself feels less coherent, and seems like just an application of many existing PINN training techniques (e.g., hard constraint PINNs, collocation point sampling) into solving a certain problem, rather than providing a novel method or a coherent framework into solving a domain-specific problem.

The experimental section feels incomplete. Different point selection algorithms have not been extensively compared with, e.g., from that in Wu et. al. (2023). Furthermore, it would be interesting to see how the method can scale to more realistic acoustic problems (i.e., outside of 1D settings).

The paper itself also seems incomplete. The Appendix and the NeurIPS checklist are partially filled and have half-finished sentences.

The labels within the graphs can also be enlarged slightly to make them more readable.

Limitations:
The authors have provided limitations with selection of \tau.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The manuscript treats the one dimensional wave equation with a PINN approach and discusses the imposition of boundary and initial conditions directly into the network, as common practice in PINNs. The authors then propose a quadrature scheme based on a coarse finite difference discretization of the wave equation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The imposition of the time derivative seems to be a novel construction. Furthermore, the construction seems not to be limited to the wave equation.

Weaknesses:
The main weakness of the manuscript is the focus on the very special and simple toy problem of the one dimensional wave equation. Solving the one-dimensional wave equation with PINNs is only of academic interest and insights obtained from it for the training of PINNs might not generalize. More specifically:
- The exact imposition of the time derivative should also work for general time dependent equations. The authors should comment on this.
- The sampling strategy employing a finite difference simulation to determine regions of high sampling density is not a generalizable approach. If a finite difference solver for the equation at hand is available, a PINN solver is typically not required.
- The authors determine an optimal function $\tau$ via considering six concrete examples. There is no guarantee that this approach will generalize to different equation types and is therefore of limited practical use.
- The authors might want to discuss the theoretical literature that proves the theoretical advantage of exactly imposed boundary conditions [1, 2, 3] and more elaborate constructions of distance functions.

[1] https://proceedings.mlr.press/v190/muller22b/muller22b.pdf

[2] https://arxiv.org/abs/2311.00529

[3] https://www.sciencedirect.com/science/article/abs/pii/S0045782521006186

Limitations:
The scope of the paper is too narrow.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper explores to solve the acoustic wave equation in the context of PINNs. Hard boundary and initial conditions are enforced by employing continuous functions within the PINN ansatz to ensure that these conditions are satisfied. A Dynamic Amplitude-Focused Sampling (DAFS) method is introduced to improve the efficiency of hard-constraint PINNs under a fixed number of sampling points.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. Propose a general hard constraint imposition formula which correctly imposes all boundary conditions and initial conditions as required.

Weaknesses:
1. Only the wave equation is discussed.
2. The proposed Dynamic Amplitude-Focused Sampling (DAFS) method is trivial.
3. There are no comparisons with other methods in the experiments.
4. In the experiments, the relative errors between exact solutions and predictions are not given.
5. In the context of PINNs, it is better to give explicitly the formulation of training loss. Training details are also lacking. 
6. Instead of tuning \tau (t) manually, it is better to train \tilde{u}(x,t) and \tau (t) simutanuously.
7. Many typos and grammar errors, such as ""both and \alpha"" in line 149, ""x \in {\partial \Omega}_i"" in line 125, ""computational"" in line 46.
8. The quality of Fig.7 should be improved.

Limitations:
Only the wave equation is discussed. There are no comparisons with other methods in the experiments.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper improves the training efficiency of original physics-informed neural networks to solve the 1D wave equation threefold: first by extending ansatz to also take the first derivative into account, second by a sampling method that focuses on high-amplitude regions, and third by a framework for domain decomposition.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ The related work is well presented.
+ The evaluation of the six candidate functions for \tau in section 4.2 provides interesting insights. The authors explore an advanced selection method for \tau based on the task at hand which might be an interesting research direction.

Weaknesses:
[Originality] While considering the first derivative for the ansatz is a good addition, the contribution is only minor. 
Sampling more collocation points in the regions that might be more difficult to solve is a practical approach however the comparison and distinction to other sampling methods is missing.
Lastly if I understand the domain decomposition framework correctly, the contribution is to wrap the entire training into a loop and, based on the training process's results, increase or decrease the subdomain size. 

Evaluation results are only provided for the 1D wave equation. Further results for other differential equations are necessary to demonstrate the benefits of the proposed method.

[Clarity] 
The framework for domain decomposition is not presented clearly. While the flow chart in Figure 7 provides an overview of the method additional textual explanations in Section 4.4 are needed.
There were few to no remarks about the training regime (#training points, optimizer, learning rate…, etc.), making it more difficult to reproduce results.
Minor remarks:
-            N_pde is not introduced. It is probably the number of collocation points?
-            Most of the Figures (e.g. Fig. 1, Fig 6.) are hard to read.
-            Line 46: (…) optimal size of the computational [domain?] given (…)
-            Line 149: Both [N_pde?] and alpha (…) 
-            Line 178: (…) In general, (...) performs better in general

Limitations:
While the authors clearly state that they are interested in the 1D wave equation it would have been interesting to see their proposed methods applied to the 2D wave equation of any other differential equations what are typically used in PINN benchmarks.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
drpJ7KOr3F;"REVIEW 
Summary:
The authors introduce continual learning into MLLMs to explore the ability of pre-trained LLMs to evolve continually on multiple modalities while keeping knowledge from being forgotten. A novel PathWeave with Adapter-in-Adapter (AnA) is proposed, in which uni-modal and cross-modal adapters are seamlessly integrated to facilitate efficient modality alignment and collaboration. The authors establish a challenging benchmark, MCL, to investigate the proposed method’s performance on the new modality and all previous knowledge. The experimental results are encouraging, significantly reducing training parameter burdens.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is logical, fluent, and easy to understand.

2. The proposed method enables existing pre-trained large models to progressively expand on multiple modalities without requiring joint training on all modalities. This idea of continually learning knowledge from pre-trained models to expand the modality is novel and could inspire the further exploration of multimodal works.

3. This paper establishes a challenging MCL benchmark to explore the generalization and anti-forgetting of cross-modal continual learning of pre-trained models. It is a promising benchmark that evaluates the performance of modality expansion of pre-trained MLLMs. 

4. This paper conducted sufficient experiments on five modalities and more than 20 datasets. The performance is comparable to the joint training MLLMs while significantly reducing parameter burden. In addition, the comparison with other continual learning methods shows the state-of-the-art generalization performance. More experiments in the supplementary materials further demonstrate the effectiveness of this method.

Weaknesses:
1. Compared with fine-tuning all parameters, the performance of the adapter fine-tuning method still has room for improvement. I think some necessary discussion, analysis, or experimentation should be conducted. 

2. What are the similarities and differences between this method and VPGTrans[1]? It is a highly efficient Visual Prompt Generator Transfer across LLMs with less training data and even task improvements. I believe that a related analysis of these two works is needed.

3. Some minor issues. 1) It seems that the modality sequence in Fig. 2 is unmatched with the experiments in Table 1. 2) In Fig. 2, the upper and lower trapezoids of A1, A2, etc. of AnA are represented by the same symbol. Do they share the parameters? 3) Some training details, including loss functions and hyperparameters for each modality, are not clear.

[1] Vpgtrans: Transfer visual prompt generator across llms. NeurIPS 2023.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Due to a serious illness I have been experiencing recently, I deeply regret to inform you that I am unable to complete the review as scheduled. I kindly request the Chair to consider the opinions of the other reviewers

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
-

Weaknesses:
-

Limitations:
-

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a flexible and scalable framework, PathWeave, which enables MLLMs to allow MLLMs gradually to involve reasoning ability on diverse modalities. The introduction of the adapter-in-adapter structure effectively alleviates the heavy burdens of the joint training or data replay strategies in previous MLLM methods. Extensive experiments demonstrate that the proposed method achieves state-of-the-art performance while significantly reducing training parameters by 98.73% compared to OneLLM and X-InstructBLIP.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper proposes an interesting framework to remedy the parameter and data burdens existed in training MLLMs. The proposed adapter-in-adapter framework exists novelties in multi-modal interaction and single-modal learning. Besides, they also provide a new benchmark to support the continual training and evaluation.	
+Contribution&Results: This paper proposes a novel PathWeave, which combines the transfer learning and continual learning method to progressively expand LLMs on multiple modalities. And the paper proposes a new benchmark MCL to evaluate the model’s overall performance on learned modalities and the performance on “catastrophic forgetting”. The extensive experimental results provide sufficient experimental details and verification, which validates the effectiveness and superiority of the proposed methods.

+Inspiration: The proposed method exhibits promising experimental results for inspiring the subsequent work. Specifically, Figure 3 shows the positive effect of different modalities’ knowledge on special modal expansion, which is promising for using old parameters to incrementally learn new knowledge. 

+Presentation: The paper writing is good and the idea of paper is easy to follow. Both the figures and tables are easy to understand.

Weaknesses:
-In Tables 4 and 5, there are no experimental results for Image-Video, which should be added. Furthermore, I think that the red subscript for the Average results in Table 5 may be unnecessary.

-Tables 4 and 5 show that the final method reduces the $T_2$ performance compared to the method “w/o In-Adapter”. However, the explanation in lines 287-289 is ambiguous, which requires further analysis.

-In Table 3, the metrics of XLLM[22] are missing. Please explain the reasons.

-The authors only provided the comparison on Params and data size, it is better to provide more sufficient analysis to show the efficiency of this continual learning on modalities.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
9622QfVSAb;"REVIEW 
Summary:
The authors propose an exploration of the Implicit Multimodal Alignment (IMA) phenomenon in frozen Large Language Models (LLMs): when exposed to perceptual tokens (e.g. from image or audio features), authors show that those tokens are implicitly aligned on text tokens, even so the LLM has only been trained on text previously. The study argues this phenomenon is a fundamental mechanism into LLMs, and that it is relevant to (i) better understand task performance (e.g using that implicit alignment as proxy) and (ii) to better understand model visual faithfulness (i.e absence or presence of hallucinations). Based on their findings, the authors propose as well architecture changes such as skipping specific operations on perceptual tokens in order to decrease inference cost.

Soundness:
3: good

Presentation:
1: poor

Contribution:
4: excellent

Strengths:
- This is a very original and in-depth work to understand the inside mechanisms appearing in MLLMs. Many/Most of the current published MLLMs are based on variations a pre-trained LLM + a pre-trained vision encoder with smaller VL connector in between. The insights from this study could be applied on many of those works, especially to better improve the visual faithfulness of those models.

Weaknesses:
- This is not commonly an important weakness but for this work, it might be: the presentation of the results and the overall quality of the layout organization is hurting the understanding. All the figures are pixelated bitmap/jpeg screenshots and are barely readable. This doesn't change the pertinence of the work, but hurt importantly how well its content is conveyed. I urge the authors to revisit this. Many solutions exist today, such as TKIZ or exporting figures in vectorized PDF to better include them in the final document. Lacking of space in a limited-page paper is understandable, but not using TKIZ or vectorized figures is not. Figure 5 for instance shouldn't be as is in this work. I challenge other readers to read/decipher the labels of the x and y axes on the heatmaps. 

- In many details, the work could have be improved. See the ""Typos"" list below. As well, some claims are not backed anywhere. For instance, Figure 1 and Figure 12 both claim a 70% sparsity. Where is that introduced in the main text? Consistency is not respected as well: in some contexts, P means Prompt while in other context means Perceptual. Is Prompt == Perceptual? This is not stated in Section 2. Prompt usually refers to the text and image prompts, before preprocessing and tokenization. The formatting is off as well. Sometimes, $P$ is used, sometimes plain-text P. Same for $T$ vs. T. 

- Evaluation could be more complete for the Multi Task setup with the use of common benchmarks such as LLaVABench-in-the-Wild, MMVet, etc. As well, Hallucinations benchmarks such as MMHALBench and HallusionBench should be used. Authors quote [46], which is the very work that introduces MMHALBench.

Typos:
 - Line 90: Define k? 
 - Line 94: FC1 and FC2 should be $FC1$ and $FC2$. Same for g, LN1, LN2, SA. 
 - Line 92: What is (B)? It is not defined in eq (2). 
 - Line 101: s/We/we/ 
 - L105: Reformulate ""In the paper, we focus no LLaVA-1.5-4 as it is most similar to the ST setup, and analyse other variants in App. E.""? s/ no / on / ? 
 - L106: (e.g. Sec. 3) ? Do you mean (see Sec. 3)? Why e.g.?  
 - Section/Equation/Appendix references are not following the same format. Sec. is used in place of Sec., same for App. and Appendix, but Equation is fully written. What about adopting a consistent terminology? 
- Figure 2 is barely readable, same for Figure 3, Figure 4. Figure 5, as well it is not vectorized. Consider using TKIZ or export your figures in PDF and include those cropped PDF in your tex document. Bitmap/JPEG/PNG used here are leading to poor and pixelated results, hurting the readability. 
- Figure 1: s/Analysing/Analyzing/ 
- Figure 1: s/LMM/LLM/ (twice)
- Figure 1: s/perceputa/perceptual/ 
- Figure 1: ""Computation efficiency"", do you mean ""Computational efficiency"" or ""Computation and efficiency""? 
- Figure 1: What is the point of the right block named ""Framework""? It doesn't seem related to the work or the caption of Figure 1. 
- L136: The claim ""Fig. 2 shows a clear narrow cone effect for textual and perceptual tokens."" is not verifiable when looking at Figure 2.
- Figure 10. What is the difference between Cosine Similarity and Sim. Aren't they the same as defined in Equation 3? Why using different terminology?

Limitations:
- One important limitation is the presentation of this work. The majority of the figures are pixelated and not readable. This is often a minor inconvenience in other works, but in this paper all the results are presented through figures. Figure 5 for instance should be removed or importantly reworked. Lacking of space in a limited-page paper is understandable, but not using TKIZ or vectorized figures is not. This is hurting the overall understanding of an interesting work trying to address interesting questions. This work should not be published as is. 

- Authors claim that the alignment between Image <-> Text tokens is what helps the model performing well on VQA tasks and hallucinations tasks. Something that is not tested is in which extend this is a cause (i.e performance on vision tasks is high thanks to that alignment), or a consequence (i.e alignment is high along performance on vision tasks thanks to the vision-language training). See ""Questions"" section of this review. 

- Authors claim that an LLM can be seen as a residual stream with refinement blocks acting as steering blocks, and that this architecture is what helps the LLM to implicitly having an alignment between text and image tokens. It seems hard to support this claim without contrasting the work with another architecture. See ""Questions"" section of this review.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper explores how frozen Large Language Models generalize to multimodal inputs without the need for extensive re-training. It introduces the concept of Implicit Multimodal Alignment, which suggests that despite the distinct representations of perceptual and textual tokens, there exists a form of implicit alignment facilitated by the architecture of LLMs. The study leverages experimental setups across single-task and multitask environments to validate the IMA phenomenon and discusses its implications for computational efficiency and model performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The analysis spans various setups and modalities, providing a comprehensive look at how LLMs process multimodal inputs.
2. This study can quickly extend large semantic models to the multimodal domain, effectively reducing computational costs and enhancing the generalizability of LLMs.
3. The results are meaningful, proving the effectiveness of the method.

Weaknesses:
1. The concept of alignment within neural networks, although well-explored here, does not offer a groundbreaking methodological advance. The novelty lies more in the application context rather than in the development of new techniques or models.
2. The generalizability of the results is restricted to a subset of model architectures and sizes, potentially limiting the broader applicability of the findings.
3. The visualization experiments are not clear. Current figures are somewhat generic and do not adequately convey the unique aspects of the IMA effect.

Limitations:
1. The paper lacks implementation details.
2. The study focuses primarily on a specific range of LLMs. More domains and tasks should be included.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper conducts an in-depth study on the generalization capabilities of LLM when handling multimodal inputs without multimodal fine-tuning. It reveals the implicit multimodal alignment (IMA) effect between perceptual and textual tokens within LLMs and finds that this effect is closely related to the model architecture. The IMA effect contributes to enhanced task performance and reduced inference costs. Additionally, the paper proposes methods for model compression and reducing computational overhead, providing valuable insights for the future design and optimization of multimodal models.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
Conducted an EXTENSIVE series of experiments to validate four hypotheses, providing insightful understanding of the mechanisms on how the MLLMs perceive multimodal information.

Based on the four findings, the paper offers relevant implications that explain the effects of IMA on tasks and hallucinations.

Proposes a novel approach to model compression by retaining a subnetwork.

Experiments are comprehensive and were conducted on various large language models, including OPT, Llama, and Vicuna.

Weaknesses:
The textual descriptions are somewhat difficult to understand, with limited explanation of the figures.

Insufficient explanation of the subnet part. It is unclear whether the similar performance after 50% sparsity is due to the effectiveness of the WANDA method or the extraction of the subnet.

Limitations:
See weakness and question

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work aims to understand multi-modality representation within MLLMs. It provides some interesting findings about how LLMs generalize to non-textual tokens and what helps LLMs to generalize to multimodal tokens. Additionally, several implications are proposed based on these findings.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The authors present many interesting findings about how LLMs generalize to non-textual tokens. These findings could help in understanding MLLMs and inspire future research.
2. Based on these findings, the authors propose several implications on performance, safety and efficiency.
3. The figures are well-illustrated and effectively support the findings.

Weaknesses:
**About experiments on α-SubNet.**

--Figure 12 appears incomplete. What does ""Avg"" mean in this table? 

--The comparison between task-specific pruning approaches (Wanda) and the proposed task-agnostic approach is not comprehensive. For instance, what is the performance of Wanda sub-network pruning on COCO across other multimodal tasks? In other words, I'm curious about the generalization of the task-specific methods. Including these results would help in understanding the significance of task and modality-agnostic pruning methods.

--Additionally, the results in Table 1 of the Appendix should be incorporated into the main paper for better readability.

--Overall, I recommend reorganizing the experiments related to α-SubNet.

**Others**.

--line 1192 is unfinished.

Limitations:
As stated by the authors, the generalization of the findings, to larger and more powerful models, with different architectures, including proprietary ones remains to be seen

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
Pf7kdIjHRf;"REVIEW 
Summary:
The paper introduces HPT a large-scale transformer model pretrained on multi embodiment robotic data. The main idea is to split the policy into three parts: a dataset specific token encoder for images and proprioceptive information called Stem. A shared Trunk, that processes all latent tokens and a dataset specific action head. By splitting the architecture into dataset specific encoders and dataset agnostic trunk the model is able to better process data from different robot embodiments and scale well using diverse data from Open-X-Embodiment Dataset (OXE). The proposed architecture is tested on pretraining on OXE and adapted for sim and real robot experiments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- Simple but yet effective method that shows good scaling properties for a general multi-embodiment robot transformer. 
- Large scale pretraining study covering 50 datasets and many robot embodiments. Most diverse pretraining in the context of robotics that I am aware of. The reported results shows good potential for further exploring heterogeneous robot data sources
- Insightful scaling analysis, which do not exist in the context of robotics yet.
- Several additional ablation studies to further study the impact of future analysis with discrete action spaces more input data and first results for fleet learning experiments underline the potential of the architecture.
- The paper is well structured and easy to follow.

Weaknesses:
- No adequate baselines in the real world: A comparison of a model pretrained on full robot trajectories with action prediction against a few pretrained vision encoders that are mostly trained on robotic-free first person human videos is not a fair or useful comparison and does not provide relevant insights for the readers. At least, I expect a comparison against Octo [1] as another generalist policy pretrained on OXE. Also the experiment is missing baselines trained from scratch such as Diffusion Policy or ACT. Further the experiments in the real world only cover a single robot embodiment. 
- For a method that is motivated by better learning from heterogeneous robot data, some additional results with other robots having different action spaces would be required to showcase the advantages of the proposed method
- No adequate baselines in simulation: The experiments on robotic datasets such as Robomimic and the real world again do not contain adequate comparisons. Specifically in the Robomimic experiment there is no comparison against state-of-the-art in-domain Diffusion Policy or ACT trained from scratch. The are no insights if the proposed model pretrained on diverse data improves upon these models trained from scratch. While comparison against the same architecture with different ablations exist, comparisons against other architecture are missing. Experiments on the recently proposed SimlerEnv [2] could also be interesting to compare the performance of the method against other large-scale pretrained methods such as Octo and RT-2X.
- Inference time: While the performance of larger variants of HPT is promising, I am not convinced by the idea that an $80$ layer transformer model is a suitable backbone for a policy used in a real robot context. 
- Several points about the image-encoder are not well described and miss relevant details.

[1]: Team, Octo Model, et al. ""Octo: An open-source generalist robot policy."" _arXiv preprint arXiv:2405.12213_ (2024).

[2]: Li, Xuanlin, et al. ""Evaluating Real-World Robot Manipulation Policies in Simulation."" _arXiv preprint arXiv:2405.05941_ (2024).

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents Heterogeneous Pre-trained Transformers (HPT), a method for training robotic models that addresses the challenge of heterogeneity across different robot embodiments and tasks. HPT pre-trains a shared neural network trunk to create a universal representation, which is then fine-tuned for specific tasks, showing improved performance and scalability with diverse datasets as well as demonstrating effective generalization to new tasks in both simulation and real-world settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper shows that pre-training policies across heterogeneity can exhibit scaling behaviors of validation losses.

This paper is well-written and easy to understand.

Weaknesses:
Despite the difficulty of large-scale evaluation, there is no inevitable relationship between the loss values and the task success rates. For instance, the main challenge in some tasks lies in certain key frames, rather than the average error in lengthy trajectories.

In a sense, verifying the scaling behavior at the level of loss values is quite evident.

In terms of architectural design, this paper (stem, trunk and head) does not offer significant technical contributions.

Lacking comparisons with baselines (only some vision encoders on Sweep Leftover Task), such as RT-X and Octo.

In Figure 5, why is the loss lower without vision than without proprioceptive information? I am puzzled. Does ""no vision"" here mean that the policy has no visual information as input? Why can a reasonable loss value still be obtained without vision?

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to pretrain a policy representation across a variety of simulated and real robotics datasets such that it can be quickly adapted to new downstream problem instances (embodiment, environment, task). The key technical contribution is the design of a Transformer architecture that can consume a variety of robot data (i.e., observations and actions unique to each robot dataset) via a large sharable trunk + a number of small networks that translate dataset-specific information to this shared representation space. Experimental results indicate that the proposed architecture scales both in terms of dataset size / diversity and model capacity, and that it can be transferred to new real-world problems by finetuning on relatively few demonstrations (<=100).

**Post-rebuttal update:** I believe that my concerns have been addressed by the author rebuttal. I was already leaning towards acceptance and have now updated my score from 6 -> 7.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Problem is interesting, timely, and likely to be of relevance to the NeurIPS community.
- Paper is well written and easy to follow. I believe that there is sufficient discussion of related work for unfamiliar readers to appreciate the technical contributions.
- The proposed architecture is fairly simple and intuitive. I consider this a strength. A series of ablation experiments validate the approach.
- Limitations of the evaluation metric (validation loss) is clearly stated. I don't think anyone would disagree that this is a major limitation, but I appreciate the transparancy. Unfortunately the community does not have many viable alternatives at the moment. Transfer performance seem to correlate somewhat with the validation loss.

Weaknesses:
I'm overall quite pleased with the paper in its current form. However, there's a couple of thing that I'd like to point out:
- Since the authors are conducting transfer learning experiments in simulation (in addition the real experiments), I would have liked to see more ablations in this setting. Transfer performance seems like a way stronger signal for ablations, e.g., validating that more data does indeed lead to better transfer, not just better validation loss.
- It is not clear to me how the authors arrived at certain experimental setup choices. For example, the authors state on L282 that they use 20-100 demonstrations per task in experiments, but do not specify whether the model is transferred to each task *individually* or *jointly* (i.e., finetuning jointly on N*T demonstrations for T tasks and then evaluating vs. finetuning T models each on N demonstrations). It is also not clear to me which tasks require more demonstrations + whether this relates to the number of tasks considered in a given transfer experiment (e.g. 100 demos per task for Meta-World 5 vs. 20 demos per task for Meta-World 20 would be a reasonable comparison if the model is finetuned jointly on all tasks).

Limitations:
I believe that limitations are adequately addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an approach for training large transformer behavior models on diverse heterogenous data. This involves separate encoders for proprioception and image observations for each embodiment, as well as different action decoding heads. Experiments include validation loss comparisons.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
While training large models that can learn from diverse heterogenous datasets is an important problem of relevance to the community, the current paper has some key limitations, please see weaknesses.

Weaknesses:
After rebuttal - Concerns regarding validation loss vs success rate has been addressed with additional experiments, claims have been further strengthened with real world results

########################

1. Evaluation Metrics in the experiments

For training robot behavior models, the standard metric used in the community is the success rate of the trained model after being fine-tuned on a few demos for a new task [1,2,3]. The success rate measures the robot's ability to actually perform tasks of interest, using the large pre-trained weights. This is a more accurate metric than validation loss for how to judge if the large scale model training is making use of the diverse quantities of training data. Validation loss is not perfectly correlated with downstream robot performance. If it is difficult/cumbersome to do these in the real world, this analysis should atleast be done in simulation. The reason validation loss is a much weaker indicator in the robotics setting is because it doesn't measure how good the model is on the on-policy distribution, which can diverge wildly from the expert data distribution used for training. 

2. Design choice justification

The proposed approach has separate encoders and decoders for each observation and action modality. This seems like it could lead to less sharing of information across diverse datasets, leading to poorer transfer capabilities. Why should we not instead have a single action decoding head, as is done in [2]? What if the same image embedding backbone is used for all all observations - this would lead to the sharing of features potentially enabling better transfer. This choice of separate encoders/decoders for each modality is not sufficiently ablated and analyzed. 







[1]: RT1-Robotics Transformer for Real-World Control at Scale, Brohan et al.

[2]: Octo - Octo: An Open-Source Generalist Robot Policy, Octo Model Team

[3]: OpenVLA: An Open-Source Vision-Language-Action Model, Kim et al.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
hGhLd2ByoR;"REVIEW 
Summary:
This paper reveals novel pathologies in existing unsupervised methods aimed at discovering latent knowledge from large language model (LLM) activations. Instead of extracting knowledge, these methods tend to identify the most prominent features of the activations.

The paper theoretically demonstrates that arbitrary features (not just knowledge) can satisfy the consistency structure of a popular unsupervised knowledge-elicitation method, namely contrast-consistent search. Additionally, the authors conducted a series of experiments showing that current unsupervised methods for discovering latent knowledge are insufficient. While the paper proposes potential future solutions, it does not provide a definitive solution to the problem with existing unsupervised methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall, the paper is well-written, and its theoretical and analytical contributions may be useful. I am impressed about the extensive experiments.

Weaknesses:
More experiments on other LLMs are needed to further validate the claim.

It would be better to offer possible solutions to address the problems in existing unsupervised methods.

Limitations:
No solutions to address the problems in existing unsupervised methods.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a careful study on existing methods for discovering the latent knowledge from large language models (LLMs), especially Contrastive-Consistent Search (CCS). The authors prove that CCS might not actually discover the knowledge of LLMs, instead, it could fit any features that satisfy certain conditions. Through a series of experiments, the authors further demonstrate that CCS could be distracted by random words, irrelavant texts like the character's opinion, and remain sensitive to the choice of prompt. Finally, the authors propose some general principles for the future works about unsupervised LLM task discovery.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Overall, the paper is well written and eazy to follow. The authors made interesting obervations about existing methods on knowledge discovery of LLMs. The theoretical analysis is well supported by the experiments. Sevaral guiding principles are also proposed for the future works. I think this paper would provide good information to the research community about unsupervised knowledge discovery of LLMs.

Weaknesses:
From my experience on unsupervised learning, I'd argue that the content of this paper *would not be sufficient to refute existing methods about unsupervised knowledge discovery (CCS)*. First of all, CCS is a method built on top of features from pretrained models. It'd definitely be sensitive to the features and thus also sensitive to the prompts, because features changes from different prompts (this could also be seen from the PCA visialization). Furthermore, as an unsupervised method, it'd be expected that the method might find multiple valid solutions, where only one of the solutions corresponds to the knowledge we are looking for. Taking the experiments from Section 4.2 as an example. The constructed dataset actually has two valid labels: the sentiment of the text and the sentiment of Alice. Depending on the optimization and the implicit bias of the algorithm, it could totally happen that an unsupervised method could found both valid labeling, or could only find one of them. I believe this is a common phenomenon shared by exsiting off-the-shelf unsupervised methods (like K-Means) cause they're searching for labels without supervision. From this perspective, I'd regard that this paper provides a method to construct ""adversarial datasets"" for CCS. However, it would not be a problem for CCS in practice.

Furthermore, the authors don’t provide solutions to this issue.

Also, I believe the mathematical notation in Section 3 could be simplified.

Minor issues: typo $c(x_i^+=1), c(x_i^+)=0$ in line 102

Limitations:
The authors have mentioned that this paper is focused on current methods and might not be directly applied to future works.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the failure modes of the method called ""constraint-consistent search (CCS)"" in knowledge discovery for language models. In particular, they showed: there is no unique identification on the minimizer of CCS, as there are a class of features achieves the optimal loss; demonstrated experimentally classic unsupervised methods detect features other than knowledge; discovered features are sensitive to prompt formats.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper points out a popular method's overlooked short-comings and presents both theoretical and experimental results to support that CCS may not be able to discover the true knowledge feature: 1. the observation on CCS loss is driven by xor operator rather than the feature is clever; 2. given the vast space of feasible features, CCS method is very sensitive to prompts and thus deserves more careful examination if to use CCS in practice.

Weaknesses:
The main weakness of the paper is its lack of novelty and potential impact to the field. The paper is more an analysis work on the application of a single method [1] proposed in 2023, which given the speed of ML innovation, it is hard to see long-term benefits of this criticism.  The general principles proposed in the discussion section (Section 6) are interesting and fit more into the line of proposing desiderata for the field - though in their current status, require more rigorous work. 

[1] C. Burns, H. Ye, D. Klein, and J. Steinhardt. Discovering latent knowledge in language models without supervision. In The Eleventh International Conference on Learning Representations, 357 2023.

Limitations:
Yes.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
zBG7WogAvm;"REVIEW 
Summary:
This paper proposes a method for decision-aware Bayesian experimental design, where the design is not optimized with respect to the most accurate posterior distribution of the latent parameters but rather with respect to the expected utility gain of the actual (down-stream) decision task.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
This is an innovative paper with high practical relevance. The proposed method appears sound and the corresponding neural networks well designed to suit the goal. Despite my questions and concerns (see below), I am positive about this paper overall and eager to increase my score should my points be addressed.

Weaknesses:
- The presentation of p(y_Xi | h_t) between Eq 3 and 4 is partially unclear to me. From the definition, it seems this is not actually a distribution but a set of distributions. To me, then notation p(y_Xi | h_t) appears to be quite the abuse of notation because we cannot readily read this it as a single distribution. Can you perhaps think about a different notation that makes this easier to parse and understand? Relatedly, in Equation 4, it appears that we compute an expectation over p(y_Xi | h_t). But how do we compute an expectation over a set of distributions? I think I get what the authors do and want to imply but to me this notation doesn’t help in understanding it.
- Equation 7: It seems we approximate the predictive distribution always by a Gaussian. I mean this of course works if the true underlying function is some kind of GP, but what if the true predictive distribution is far away from Gaussian? I don’t see this choice to be discussed properly so I consider it a weakness of this paper for now.
- The discussion of training and inference time can only be found in the appendix. Specifically, training speed seems to be substantial, which of course makes sense for an amortized method. However, I don’t see any discussion for when the training actually amortizes. That is, how many BED tasks do we need to run at minimum before the total (training + “inference”) time of the new method becomes better than those of the competing methods. More generally, I think a discussion of speed should be more prominent in the paper.
- 6.1 toy example was hard for me to understand at first. Is this just a standard BO task to find the point where the unknown function is maximal?

Limitations:
The paper discusses several limitations. I am missing a discussion on the initial overhead of training, which is usually substantial in amortized methods.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper looks at the problem of designing Bayesian optimal experiments taking into account the downstream decision making. At the core is a Transformer Neural Decision Process (TNDP) architecture that is trained to amortise the experimental design process whilst simultaneously inferring the optimal downstream decision.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- Relevant and interesting topic: Downstream decision making is what ultimately matters, so taking this into account when designing experiments to collect data can result in more cost- and sample-efficient learning.  

- Motivation for the paper as well as clarity of writing are excellent. Contextualisation relative to prior work can be improved as outlined in the next section.

- The proposed Transformer Neural Decision Process (TNDP) architecture is tailored to the BED problem, is well-explained and adds some novelty to the architectures typically used in the field.

Weaknesses:
### Sections 2.2 & 3.2 and Lindley's decision-theoretic BED [1]:

My main issue with the paper is the presentation of DUG and EDUG as novel. This framework was first formulated in [1], and is very well summarised in Section 1.3 of [2]. I strongly recommend the authors read that section, and present their Section 3.2 accordingly, acknowledging they follow Lindley, 1972. The questions/comments in the next 2 bullets are a consequence of this omission of literature.

- Second paragraph of Sec 2.2: I am not sure how the predictive distribution $p(y | \xi, h_t)$ is defined. I would think it is $p(y | \xi, h_t) = \mathbb{E}_{p(\theta |h_t)} [p(y | \xi, \theta)]$. Whether or not you compute/approximate the posterior $p(\theta |h_t)$, or seek to directly approximate $p(y | \xi, h_t)$ (eg variationally), I think you should explicitly define what this quantity is. 

- I am not sure how the utility $u(y_\Xi, a)$ is defined. From a Bayesian decision-theoretic approach, the utility has to depend on the state of the world $\theta$, as well as the experiments $\xi$ you are going to perform (which I guess is implicit in $y_\Xi$). So shouldn't the ""lowest level"" utility be a function $u(y, \theta, \xi, a)$, which you then integrate over $p(\theta|h_t)$, to obtain $u(y, \xi, a) = \mathbb{E}_{p(\theta|h_t)} [u(y, \theta, \xi, a)]$, then take $\max$ wrt $a$,  and finally integrate over the predictive $p(y |\xi, h_t)$ to obtain an expected utility, which can then act a design ranking criterion, as you do in Eq 4 and (cf Eq 2 in [2]).

### Related work: 

For a field that has such rich history and renewed interest from the ML community recently, the related works section is quite short and sparse on citations. Some areas that are missing include:
- Decision-theoretic BED: as previously discussed, the general framework of utility-based BED was developed by Lindley (1972).
- BED + RL: this work touches on some aspects of RL; It might be good to discuss relations recent works in the intersection such as [5] and [6] (in addition to those mentioned)
- Decision-theoretic approaches in related fields such as Bayesian Optimisation, e.g. [7], [8]
- Finally, I'm not too familiar with this line of literature, but  more recent work around decision transformers---is there any relation between TNDP with works like [9] and [10]?

### Other:

- Line 6: ""most recent BED methods use amortised inference with a policy network"" is not quite correct in the sense that no ""real inference"" (posterior updates on the parameters $\theta$) are performed. 
- Line 179: ""to ensure the framework satisfied the permutation invariance property of sequential BED"": not all BED problems are permutation invariant. For example, designing experiments for time series models (e.g SIR in [3] and [4]), permutation invariance does not hold. This aspect has been discussed in e.g. Section 3.3 of [3].
- Assuming you do want a permutation invariant architecture (most design problems fall in that category): by conditioning on $t$ as part of the global information (GI) set, I think you actually break that invariance. This is because encoding $(\xi, y)$ at time $t$ or at time $s$ will give you different outputs. As far as I can tell from Fig2b), $D_c$ does attend to GI. Could you please explain if that's the case or I have misunderstood something?

-----
#### References

[1] Lindley, D. V. (1972). Bayesian statistics: A review. Society for industrial and applied mathematics.

[2] Chaloner, K., & Verdinelli, I. (1995). Bayesian experimental design: A review. Statistical science, 273-304.

[3] Ivanova, D. R., Foster, A., Kleinegesse, S., Gutmann, M. U., & Rainforth, T. (2021). Implicit deep adaptive design: Policy-based experimental design without likelihoods. Advances in neural information processing systems, 34, 25785-25798.

[4] Kleinegesse, S., & Gutmann, M. U. (2019, April). Efficient Bayesian experimental design for implicit models. In The 22nd International Conference on Artificial Intelligence and Statistics (pp. 476-485). PMLR.

[5] Mehta, V., Paria, B., Schneider, J., Ermon, S., & Neiswanger, W. (2021). An experimental design perspective on model-based reinforcement learning. arXiv preprint arXiv:2112.05244.

[6] Mehta, V., Char, I., Abbate, J., Conlin, R., Boyer, M., Ermon, S., ... & Neiswanger, W. (2022). Exploration via planning for information about the optimal trajectory. Advances in Neural Information Processing Systems, 35, 28761-28775.

[7] Neiswanger, W., Yu, L., Zhao, S., Meng, C., & Ermon, S. (2022). Generalizing Bayesian optimization with decision-theoretic entropies. Advances in Neural Information Processing Systems, 35, 21016-21029.

[8] Ivanova, D. R., Jennings, J., Rainforth, T., Zhang, C., & Foster, A. (2023, July). CO-BED: information-theoretic contextual optimization via Bayesian experimental design. In International Conference on Machine Learning (pp. 14445-14464). PMLR.

[9] Chen, L., Lu, K., Rajeswaran, A., Lee, K., Grover, A., Laskin, M., ... & Mordatch, I. (2021). Decision transformer: Reinforcement learning via sequence modeling. Advances in neural information processing systems, 34, 15084-15097.

[10] Zheng, Q., Zhang, A., & Grover, A. (2022, June). Online decision transformer. In international conference on machine learning (pp. 27042-27059). PMLR.

Limitations:
Some limitations of the work were outlined in the Discussion section of the paper. Regarding negative societal impact, the field of experimental design (which boils down to efficient data collection), generally warrants some discussion. 

The experiments presented in this paper mostly use synthetic data and do not have negative impact; the HPO experiment, which uses real data does not (directly) represent an application with negative impact. However, applying these methods in real-world applications, particularly if decisions directly affect humans, as in e.g. personalised medicine, could raise concerns around bias, fairness, explainability and privacy. 

I would suggest to the authors to add 1-2 sentences in their limitations section to acknowledge 1) the synthetic or semi-synthetic nature of the experiments, and 2) potential concerns that might arise when applying their method in real-world applications.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a transformer-based architecture for jointly sampling designs and decisions in Bayesian Experiment Design (BED) using a forward-looking criterion. The latter considers the improvement in maximum expected utility brought about by a new design-outcome pair, where the expectation is taken with respect to the predictive distribution of the model. The main innovation of the paper lies in the coupling between information gain and utility maximization in an amortized, transformer-based framework in the spirit of attentive neural processes. The performance of the new architecture is evaluated on a toy regression task and two more representative models, exhibiting stable performance gains over contender methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is clearly written, the ideas and formulations are stringent and well-justified, overall making it easy to follow and a pleasure to read (with the exception of Section 4.1, see below).

- The proposed architecture and training objectives are novel and seem to unlock both qualitative and quantitative improvements over existing methods. 

- The results indicate superior and stable performance of the proposed architecture on two interesting tasks, along a toy 1D GP model which seems to be a standard proof-of-concept task in the neural process (NP) literature.

Weaknesses:
- Some notational confusion can be avoided by consistently using the notation $a_{1:t}$ to denote a sequence of $t$ elements and $a_t$ to denote the $t$-th element in the sequence. Currently, $h_t$ denotes a sequence, but, e.g., $y_t$ denotes an element, and then again $\theta_{1:L}$ also represents a sequence. Also, P4L126 is an abuse of notation with slightly confusing wording, such as “the predictive posterior distribution over all possible designs”, whereas the predictive distribution(s) are over future \textit{outcomes}. This is in no way different than the posterior predictive in Bayesian (non-linear or linear) regression, where the posterior predictive is conditioned on the training data set and the set of (unlabeled) predictors available at test time. Hence, I struggle to understand the need for the convoluted abuse of notation, but I may be missing something. Also section 4.1 suddenly starts using bold font for vectors, which was not the case in the preceding sections. 

- Figure 2 is not particularly informative for the data flow, as it does not clearly communicate weight sharing, input-output operations and dependencies (left panel); the right panel comes out of the blue and is not well explained (i.e., what are the elements on the “left” and on the “top”); the description below on P6 does indeed disambiguate the idea behind the construction of the masks, but I believe it is best when figures support and enhance the text and not vice versa.

- Overall, I feel that Section 4.1 is the weakest link in the paper, and I believe the authors can think about optimizing the ratio of details dispersed between the main text and the appendix. For instance, there is no need to reiterate established transformer-based computations, but it could be helpful to explicate the construction of the masks, the representation types (e.g., vectors, sequences of vectors,...?), and the precise partitioning of the components into keys, queries, and values.

- According to my understanding, none of the contender methods in the experiments is an amortized method. Wouldn’t some of the existing amortized BED methods (e.g., as highlighted in the Related Work) make for suitable benchmarks, despite not optimizing for future decisions?

- The topic of model misspecification is never mentioned in the paper, even though the comprehensive review paper [1] states that it remains a major unsolved issue in BED and in amortized Bayesian inference more generally [2]. I believe this should also be acknowledged in the current paper and the authors can potentially think about quantifying the impact of model misspecification in a small ablation study in the final version of the manuscript.
 
I am happy to discuss these points with the authors and increase my score if they are addressed / clarified.

[1] Rainforth, T., Foster, A., Ivanova, D. R., and Bickford Smith, F. (2024). Modern Bayesian
429 experimental design. Statistical Science, 39(1):100–114.

[2] Schmitt, M., Bürkner, P. C., Köthe, U., & Radev, S. T. (2024). Detecting Model Misspecification in Amortized Bayesian Inference with Neural Networks: An Extended Investigation. arXiv preprint arXiv:2406.03154.

Limitations:
The authors openly discuss the current limitations of their approach.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tackles an important problem of designing experiments in a way that directly optimizes downstream decision-making tasks, going beyond just inferring parameters of interest. The authors make several valuable contributions:

1. They introduce the concept of Decision Utility Gain (DUG) to quantify how much an experimental design improves the expected utility of the downstream decision. 

2. They propose a novel neural architecture called the Transformer Neural Decision Process (TNDP) that amortizes both the experimental design selection and the approximation of the predictive distribution needed for decision-making. This unified amortized framework is a key innovation.

3. The authors develop a non-myopic training objective that looks beyond just the immediate decision utility to account for effects of the current design on future rewards.

4. Empirically, they demonstrate TNDP's effectiveness over traditional methods on various tasks like active learning, hyperparameter optimization, showing it can find informative designs and make accurate downstream decisions.

In summary, this work makes valuable conceptual and technical contributions to the area of Bayesian experimental design by pioneering decision-aware amortized methods. It opens up new research directions for further enhancing real-world decision-making via optimized experimental data acquisition.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents a novel problem formulation by introducing the concept of Decision Utility Gain (DUG), which shifts the focus of experimental design from reducing parameter uncertainty to directly optimizing downstream decision utility. This new perspective is a creative departure from traditional Bayesian experimental design (BED) approaches.
- The application of amortized inference techniques to decision-aware experimental design can be considered an original contribution, as it represents a new domain for these methods beyond traditional BED.
- The empirical evaluation is comprehensive, spanning diverse tasks such as active learning, hyperparameter optimization, and synthetic regression problems. The results demonstrate the consistent superiority of TNDP over traditional methods.

Weaknesses:
- The authors could provide a more rigorous analysis of the properties and characteristics of the TNDP architecture, such as its convergence behavior, sample complexity, and theoretical guarantees (if any) regarding the quality of the proposed designs and decisions.
- The experimental evaluation, while comprehensive, focuses primarily on synthetic and benchmark datasets. While these serve as important proof-of-concept demonstrations, the paper could benefit from including real-world case studies or applications to further validate the practical utility of the proposed framework.
- While the amortized nature of TNDP is highlighted as a key advantage, the paper could provide a more detailed analysis of the computational complexity and scalability of the proposed approach. This analysis could include factors such as the training time required for different problem sizes, the memory footprint, and the scalability of the attention mechanisms used in the Transformer architecture.

Limitations:
- The authors mention the use of a basic REINFORCE algorithm for training the query head, which can lead to unstable training, especially in tasks with sparse reward signals. While they suggest the use of more advanced reinforcement learning methods as a potential solution, a more detailed discussion on the specific challenges faced during training and the trade-offs involved in selecting different RL algorithms would be beneficial.
- The authors mention that their model is trained on a fixed-step length, assuming a finite horizon for the experimental design process. A discussion on the limitations of this assumption and the potential difficulties in extending their approach to infinite horizon or open-ended experimental scenarios would be valuable.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
mdK1vhgpa5;"REVIEW 
Summary:
The paper proposes a method to continually adapt a pre-trained classifier to an unlabeled stream of test data. They address the problem of continual test-time adaptation through the lens of Bayesian deep learning. Their method consists of three main components: (1) a variational warm-up strategy to turn any source model into a Bayesian Neural Network, (2) a mixing strategy between the source model and the last posterior to leverage the trade-off between adaptation and forgetting, and (3) a modified entropy term that is symmetric and incorporates data augmentations. The authors compare their method on standard CIFAR-C and Imagenet-C datasets to a set of TTA baselines. The paper also includes ablation studies on the components and an evaluation of uncertainty estimates.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper tackles one of the most relevant problems in continual domain adaptation, namely the trade-off between agile adaptation while preventing the forgetting of the source model. It does so by constituting a mixture of the source and last adapted model in a VI framework, which is novel to my knowledge. (originality)
- Further, in a setting where robustness is crucial and therefore uncertainty quantification can be helpful, the combination of Bayesian deep learning and continual test-time adaptation is interesting and insightful. (originality)
- The methodological backbone is accompanied by insightful ablation studies that highlight the significance of the different parts of the paper’s contribution. (quality)
- The paper is, in most parts, pleasant to read. The notation is clear and consistent, and the reader is well guided through the different sections. (clarity)
- The paper presents clear experimental evidence in support of the method. The experiments show an improvement in adaptation accuracy on already quite saturated datasets (up to 1.8% points on CIFAR-10-C). VCoTTA also seems to be advantageous on most corruption types. (significance)

Weaknesses:
The paper presents strong evidence in support of the proposed method. However, it is left unclear to me why the method performs so much better than previous approaches.

- The method consists of a range of specific components. However, in some cases, the specific design of the components is not clearly motivated. In particular, equations 10 and 13 lack supporting citations or explanations. Why have exactly these formulations been chosen?
- I’d like to get more clarity on the difference between this paper and the original CoTTA work, as it seems to me there are certain components in common (e.g., student-teacher approach, EMA). More precisely, could you please highlight the difference in the update equations between the two papers? My understanding is that adding the VI framework notably changes (i) the optimization objective by adding the KL term (instead of solely minimizing entropy) and (ii) the predictions by marginalizing out the model parameters. Where else does the VI framework contribute to differences?

Limitations:
The authors have listed limitations including computational efficiency and the need for access to the source data at adaptation time

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces VCoTTA, a novel variational Bayesian approach to address the Continual Test-Time Adaptation (CTTA) task, which focuses on effective domain adaptation during continuous domain shifts at test time. The authors' main contributions include a method to measure uncertainties in CTTA, addressing the issue of error accumulation due to the use of unlabeled samples. They propose transforming a pretrained deterministic model into a Bayesian Neural Network (BNN) using variational warm-up at the source stage, and employ a mean-teacher update strategy during test time. The approach updates the student model by combining priors from both source and teacher models, with the evidence lower bound formulated as the cross-entropy between student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on three datasets demonstrate the method's effectiveness in mitigating error accumulation within the CTTA framework.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper demonstrates originality through the novel Variational Continual Test-Time Adaptation (VCoTTA) approach, which creatively utilizes Bayesian Inference for Continual Test-Time Adaptation, and employs strategies like the variational warm-up and prior mixture techniques. The quality of the work is evident in its solid theoretical foundation, comprehensive methodology, and empirical validation on multiple datasets. 

The paper's clarity is apparent in its well-structured presentation, use of visual aids, and explicit statement of contributions. The significance of the research is underscored by its practical relevance to risk-sensitive applications, potential for broad applicability, and the reported improvements in predictive accuracy and uncertainty estimation under distribution shifts. By addressing critical challenges in CTTA, such as error accumulation and uncertainty estimation, and bridging Bayesian methods with test-time adaptation, the paper not only advances the current state of the art but also opens up promising avenues for future research. Overall, this work represents a valuable contribution to the field, offering both theoretical insights and practical advancements in continual learning and test-time adaptation.

Weaknesses:
Regarding the computational overhead discussed in the paper, while it is noted that online Variational Inference is employed to make the approach computationally feasible, a detailed analysis of the computational costs associated with VCoTTA is absent. Table 13 presents a comparison of time and memory costs, but the source of these values is unclear. Could you specify which dataset was used for these measurements? Also, is it possible to clarify whether the time and memory comparisons pertain to training or testing phases?

The manuscript contains several typographical and grammatical errors that need to be addressed. Specifically, brackets are missing in Equation (5) and in the sentence following Equation (13). Could these omissions be corrected to prevent misinterpretation of the mathematical expressions and enhance the clarity of the paper?

There are multiple grammatical issues that require rectification. The sentence ""MT is initially proposed in semi-supervised and unsupervised learning"" is somewhat unclear. Could this be rephrased for better coherence? Additionally, the sentence ""We use the mean entropy derived from a given *serious* data augmentation to represent the confidence of the two prior models, and mix up the two priors with a modulating factor"" appears to contain a typographical error and could be better structured. Could these issues be addressed to improve the readability and accuracy of the text?

The heading for Section 5.7 seems to not accurately reflect the content discussed within. Could this heading be revised to more accurately convey the main topics or findings of the section, thereby ensuring clarity and relevance for the reader?

The explanation of how MT operates in semi-supervised and unsupervised learning settings appears incomplete and potentially misleading. The current statement, ""where the teacher model guides the unlabeled data, helping the model generalize and improve performance with the utilization of large-scale unlabeled data,"" lacks specificity. Could you specify which model (teacher or student) benefits from this guidance and in what manner? Additionally, the phrase ""where the teacher model guides the unlabeled data"" seems incorrect. Could this be clarified or corrected to accurately reflect the operational dynamics of the MT framework?

Limitations:
The authors have adequately addressed the limitations in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a variational continual adaptation method. Where a sequence of test-time domain adaptation problems are shown to a model. More specifically a labeled dataset is given as an initial dataset to learn from, and then afterwards a sequence of unlabelled datasets with domain shifts are presented to the model.

Using traditional variational continual learning methods will result in error accumulations in the posterior over parameters. So the authors propose a scheme that regularizes against the posterior learned from an initial source dataset. 

The authors propose do away with using sequential Bayesian inference. Instead, the authors use a mix of a source prior (learned using labeled data) and a teacher prior. The teacher prior is an EMA of the previous task’s posterior learned with variational inference. The method the authors propose is named VCoTTA.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The paper is well written and the components of VCoTTA are clearly explained. 
* The authors provide an ablation to demonstrate which design choices worked well, for instance, to demonstrate that the mixture of priors worked well for test-time adaptation.

Weaknesses:
Novelty:
* In terms of novelty I’m not convinced that there are important applications of continual test-time adaptation in the form of classification tasks derived from CIFAR10 datasets. I could be wrong, but some justification in the paper is required and some more realistic benchmarks would be nice.
* Maybe I have misunderstood the variational warm-up procedure. But using the MLE estimates to initialize the BNN mean parameters was done in VCL https://arxiv.org/abs/1710.10628 . So this is not a novel idea. Furthermore, there are better ways to initialize a BNN such as using Bayesian linear regression: https://proceedings.mlr.press/v97/rossi19a/rossi19a.pdf.

Clarity
* Why does the teacher model use EMA updates instead of using the inference variational posterior?
* Why is data augmentation an important component in VCoTTA (Sec 4.2)? This is suddenly presented in the paper without justification.

Notation:
* In Section 4.2, the title is a “Mixture-of-Gaussian prior”, but Eq 11 is an addition of two priors which are Gaussians by design, so this is a “scale-mixture prior” https://arxiv.org/pdf/1505.05424, not a mixture of Gaussians (https://www.inf.ed.ac.uk/teaching/courses/mlpr/2016/notes/w9b_mixture_models.pdf)?
* Confusing notation of the source prior: it is denoted as $p_0$ (Fig 2) and $p_1$ (Eq 11), this needs to be consistent.


Empirical weaknesses:
* No standard errors in the experimental results. So difficult to see which method outperforms another.
* Uncertainty estimation is not performed with standard methods like ECE or OOD detection like https://arxiv.org/abs/2102.06571. It is unclear to me whether the Brier Score estimates uncertainties.

Limitations:
There is a good discussion on the limitations of VCoTTA.

One limitation that is not discussed is in the effectiveness of (variational) Bayesian sequential inference methods. Weight space variational inference has been shown to be very difficult to do in practice, https://arxiv.org/abs/2301.01828. So weight space variational inference (without tricks like multi-head networks and coresets) might not be the best choice when wanting to remember a source distribution.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a variational Bayesian approach to handle uncertainties in continual test-time adaptation (CTTA). The source pretrained model is made Bayesian by variational warm and a mean-teacher update strategy is used at test time. To avoid drift due to uncertainty of priors using only unlabeled data at test time, the paper proposed to update the student model by combining priors from both the source and teacher models. The evidence lower bound is formulated as the cross-entropy between the student and teacher models, along with the Kullback-Leibler (KL) divergence of the prior mixture. Experimental results on three datasets demonstrate the method’s effectiveness in mitigating error accumulation within the CTTA framework.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Novelty: Bayesian approach in Continual Learning is a principled and elegant approach to the problem which this paper is relying on. In CTTA, there are additional issues due to the uncertainty of the prior distributions using only the unlabeled data from unknown domains. This paper presents a novel solution by using adaptive mixture prior models and student-teacher update on top of an existing framework.

- Relevance: CTTA is a topic that can interest a general audience, and the  Bayesian and variational framekwork can also be of interest to many.

Weaknesses:
- While Bayesian approach is nice in principle, it can be computationally demanding and offer little benefit in practice. Most of the existing CTTA methods are computationally and memory efficient, whereas this method present an opposite end of the spectrum. While the reported results are impressive, it is unclear to me why the proposed method is superior to other SOTA methods.
- The hyperparameter selection process is not addressed in the paper, which is critical in TTA where all hyperparameters should be predetermined before data access. How are they chosen?

Limitations:
Limitations are mentioned in the paper, albeit very brief. Overall, the proposed method is more complex and demanding (such as requiring a pretrained probabilistic model or source data) for TTA applications. Perhaps the proposed approach may work even better with UDA or other CL scenarios with (partially available) target labels?

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
cgb0Tn4uHy;"REVIEW 
Summary:
This paper introduces a method that supplements the traditional estimate of a class-dependent transition matrix, which is popular in label-noise learning. Traditional transition matrix methods are less effective for instance-dependent noise. To overcome the limitation, the proposed method adds a residual term such that it can extend the projection of a class-dependent T on label predictions to fit the true one as if we have an instance-dependent T. Theoretical analyses of the algorithm confirm its convergence and generalization properties under specific assumptions. Experimental results on various synthetic and real-world noisy datasets such as CIFAR-N and Clothing1M show the performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The performance is eye-catching.
2. The method is proposed with both theoretical analyses and experimental results.

Weaknesses:
1. The intuition of the proposed residual is not clear. For example, why a sparse structure is preferable in this problem? Why do u and v enable a sparse structure? Why is a Hadamard product employed? Why not simply use a vector u?
2. The theoretical part of the main paper is heavy but the outcome is not convincing. Specifically, there is a huge gap between Eq. (17) and Theorem 3.1.
3. The assumption in Eq. (7) is too strong.

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the problem of learning with noisy labels. To handle the instance-dependent noise, the authors propose an extended model for transition matrix-based methods. Specifically, their model combines a class-dependent transition matrix with a sparse implicit regularization term. The authors provide a theoretical analysis of the proposed method. Experiments conducted on both synthetic and real-world noisy label datasets verify the effectiveness of their method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Theoretical analysis of the convergence and generalization are provided.
2. Experiments are conducted thoroughly, including experiments on synthetic and real-world datasets. The ablation study is also conducted.

Weaknesses:
1. The method proposed in this paper appears to be a straightforward combination of VolMinNet and SOP.
2. The experimental results for TMR are missing for the CIFAR-N, Clothing1M, and WebVision datasets.
3. An important baseline, CCR [1], which is the state-of-the-art among transition matrix-based methods, is absent.
4. The paper lacks an analysis of the estimation error of the transition matrix. It would be beneficial to compare the estimation errors of the transition matrix for TMR against those of other baselines.

**Reference**

[1] Cheng, De, et al. ""Class-dependent label-noise learning with cycle-consistency regularization."" *Advances in Neural Information Processing Systems* 35 (2022): 11104-11116.

Limitations:
I did not find that the authors have discussed the limitations and potential negative societal impact of their work. To improve the paper, the authors can provide a thorough analysis of the limitations of their method in an independent section. For example, scenarios where the method might not perform well can be included.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In learning from noisy labels, existing methods generally focus on class-dependent (but instance-independent) noise that can be modeled by a transition matrix $\mathbf{T}$. Some methods have also been proposed for instance-dependent noise (modeled by $\mathbf{T}(x)$). This work belongs to the latter. In particular, it proposes to implicitly model $\mathbf{T}(x)$ using an extended model based on the transition matrix $\mathbf{T}$ and a residual term $\mathbf{r}(x)$. Some theoretical properties (e.g. convergence and generalization) of the proposed algorithm (TMR) are analyzed under certain conditions. Experiments show that the proposed algorithm outperforms baselines.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
**Originality**

The paper studies the challenging problem of instance-dependent label noise, which is less addressed in the literature compared to class-dependent noise. The proposed extended model for transition matrix, which is a combination of a transition matrix with residual terms, seems novel and effective. Related work is adequately cited.

**Quality**

The experiments are quite comprehensive. The paper compares the proposed method with multiple methods (including some state-of-the-art ones) on various datasets. The experimental results show that the proposed method outperforms all those baselines. Some theoretical properties (e.g. convergence and generalization) of the proposed algorithm are also analyzed under certain conditions.

**Clarity**

The description of the proposed method is clear. The experiment section is generally clearly written and well-organized.

**Significance**

The proposed method shows significant improvements compared with various baselines. Therefore, it has the potential to be adopted by other researchers and practitioners, advancing the state of the art in learning from noisy labels.

Weaknesses:
**Originality**

- In Lines 120-125, the residual term $\mathbf{r}(x)$ is introduced. However, it is not clear to me how novel it is compared to the previous work [57,25,30,31]. The authors should elaborate on this point.
- I can see why residual term $\mathbf{r}(x)$ might be useful, but why is it modeled as in the form in Line 124? The motivation should be explained.

**Quality**

- The convergence analysis seems very restrictive to me because it requires too many assumptions (Lines 171-175, Lines 183-186, and Appendix B.2).
- The generalization analysis (Theorem 3.2) is w.r.t. the training loss (surrogate loss) under the noisy distribution $\tilde{\mathbb D}$, but the test accuracy under the clean distribution $\mathbb D$ is what people really care about. Is it possible to prove any consistency guarantees?
- Knowledge of the ground truth $R_*$ is required to derive Theorem 3.2, but we do not know $R_*$ in practice.
- Section 3 is not clearly written, and I found it hard to follow and assess its correctness (see below).

**Clarity**

Section 3 is not clearly written, and I found it hard to follow and assess its correctness. Specifically:

- In Lines 173-174, is $R_{\ast}$ assumed to be $U_{\ast} \odot U_{\ast} - V_{\ast} \odot V_{\ast}$?
- In Lines 203-205, $\mathcal F$ is a set of loss functions. What is the exact meaning of ""about the data""? Why is $R$ not considered in $\mathcal F$? Is a fixed $R$ being used here?
- In Lines 206-207, what is the definition of $\epsilon$-cover?
- In Lines 207-208, what are the mathematical definitions of the ""average losses""?
- In Lines 210-213, it seems that here $R_{\ast}$ is fixed. Yet, it does not make sense to me because $R$ should depend on the transition matrix $T$ and the distributions $\mathbb D$ and $\tilde{\mathbb D}$. What is ""ground truth"" w.r.t. here?

**Significance**

The significance of the proposed method could be further enhanced through a more rigorous theoretical analysis (see above).

Limitations:
I did not see where the authors discussed the limitations of the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In noisy label learning problem, noise is often characterized by confusion matrix. In contrast to instance-independent noise, this work considers a setting where confusion matrices could be different for different samples. Under this setting, the authors proposed to use a global confusion matrix shared by all instances and a residual term for each instance to account for the different between instance-dependent confusion matrix and the global confusion matrix. For learning, an MLE loss combined with an implicit sparsity regularizer is optimized.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The work is tackling a challenging yet important setting in noisy label learning. The proposed model is a natural and intuitive extension to instance-independent confusion matrix as it allows for a wider range of noise. The proposed algorithm (TMR) is simple to implement, and demonstrated to be effective under synthetic and real-data experiments.

Weaknesses:
- Motivation for the use of sparsity regularizer is not clear. The authors does not discuss much on why the vector $\textbf{r}$, or matrix $\textbf{R}$ in their model should be sparse. They did point out in page 3, line 117 that the difference when using the global transition matrix and the instance-dependent transition matrix should be small. However, that is not sufficient to promote sparsity, as any other $l$-p ($l>1$) norm could have promoted that goal.
- The use of implicit regularizer is also not clear. And more importantly, since the output of $\textbf{T}^T P(\textbf{Y} | X) + \textbf{r}(X)$ is a probability vector, $\textbf{r}$(X) has to satisfy certain constraints. This is not discussed nor specified anywhere in the paper. And hence it is questionable how the parameterization of $\textbf{r}(X)$ could produce valid probability vector $\textbf{T}^T P(\textbf{Y} | X) + \textbf{r}(X)$.
- The analysis might contain flaw. Equation (14) is incorrect: $\widetilde{\textbf{Y}}$ is a matrix composing of one-hot vectors while the RHS is a matrix composing of probability vectors. The two are not equal in general. This equation seems to be the key step to motivate the objective to be analyzed in (17), and also the key step in the proof of Theorem 3.1 (page 15, line 534). 
- The analysis is based on linear model which is not very realistic.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
ZwS2y21mZV;"REVIEW 
Summary:
This study investigates a Jackson-type approximation rate for single-layer Transformers with one head, and compares their ability with RNNs, another nonlinear sequence-to-sequence map.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The literature overview is concise
- The Jackson-type approximation rate for the Transformer is derived for the first time

Weaknesses:
The main theorem (Theorem 4.2) sounds trivial because the bound is a combination of the definitions of complexities $C^\alpha$ (Sobolev smoothness) and $C^\beta$ (Barron bound). The universality of Eq.8 (Theorem A.3) may sound non-trivial, but it is obtained by rewriting the Kolmogorov representation of continuous function (from Theorem A.1), which is much a stronger (but only existential) result.

It could be non-trivial if the authors could provide a similar bound in a constructive manner without using the magic argument of Kolmogorov.

Limitations:
n/a

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel concept of complexity measures to construct approximation spaces for single-layer Transformers with one attention head, providing Jackson-type approximation rate results for target spaces that possess a representation theorem.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The results in this paper are presented within a general framework using rigorous and elegant mathematical tools, offering a solid theoretical foundation for researchers interested in approximation.

- Their hypothesis of singular value decay pattern regarding the target space can be validated through the experiments detailed in Section 5. Furthermore, the hypothesis underscores the crucial role of pairwise coupling and low-rank structure.

Weaknesses:
The results presented are limited to 2-layer single-head Transformers, which restricts their applicability and insights into more common models such as multi-layer multi-head Transformers.

Limitations:
See Weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The study explores the theoretical aspects of Transformer architectures in sequence modeling, particularly focusing on approximation rates for sequence-to-sequence relationships. A representation theorem is established, introducing novel complexity measures that analyze interactions among input tokens, culminating in a Jackson-type approximation rate estimate for Transformers.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This study enhances the understanding of Transformer's approximation rate and gives concrete comparisons with traditional models like recurrent neural networks.

Weaknesses:
The paper deviates from the standard Transformer architecture by requiring a neural network layer before the attention mechanism to implement the Kolmogorov Representation Theorem, potentially inheriting the theorem's limitations.

Limitations:
The paper deviates from the standard Transformer architecture by requiring a neural network layer before the attention mechanism to implement the Kolmogorov Representation Theorem, potentially inheriting the theorem's limitations.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
ufPPf9ghzP;"REVIEW 
Summary:
This paper proposes an approximate MPE inference scheme for multiple ***discrete*** probabilistic models that guarantee efficient log-likelihood computations. The approach is based on training a carefully-designed neural network to answer these queries, building upon three key ideas:
1) self-supervised learning, in order to avoid labelling via expensive MPE oracles
2) ITSELF: iterative optimization of the NN parameters at prediction time, resulting in both anytime inference and continual learning
3) GUIDE: a teacher-student training scheme for mitigating overfitting

The approach is evaluated on multiple binary benchmarks from the TPM and PGM literature.

**update after rebuttal**

The authors effectively addressed the concerns on the paper. Overall, I think that this is solid work and I hope that the authors consider adding their explanation on generalizing the approach to continuous variables and the runtime results in the main text.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper is overall well-written
- The key contributions are sensible and (to the best of my knowledge) novel
- Extensive empirical evaluation

Weaknesses:
- The limitations of the approach are not clearly stated.
- Some experimental details are unclear
- Runtime evaluation, a crucial aspect of any approximate algorithm, is deferred to the supplementary work

Limitations:
The limitations of the approach are not clearly stated in the text.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a neural network-based approach to approximate the most probable explanations (MPE) on probabilistic models. The basic idea is to train a neural network that takes as input an encoding of an assignment to an arbitrary subset of evidence variables and outputs an assignment to the remaining query variables, with the objective to maximize the likelihood given the predicted assignment. This is feasible on probabilistic models that support tractable likelihood and gradient computations such as PGMs, PCs, and NAMs. The authors present two key strategies to enhance the basic architecture. First, they employ a teacher-student architecture (GUIDE) that trains a student network by supervised learning with the teacher network’s MPE solutions as labels. In addition, they introduce inference time optimization to further refine the pre-trained models given specific MPE queries. The proposed methods are evaluated empirically on benchmark datasets and compared against baseline approximation methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ The proposed approach can answer any-MPE without a priori fixing the query/evidence variables, addressing a major limitation of prior work using neural networks to compute MPE / marginal MAP queries. The improvements using teacher-student architecture and inference tim optimization are also interesting and effective as demonstrated by experiments.

+ The paper is overall well-written and easy to follow (minus some minor comments below).

+ The experiments are thorough, and the results convincingly show that GUIDE+ITSELF outperforms baseline approximation methods across datasets and types of probabilistic models.

Weaknesses:
- It appears that the inference time optimization is doing a lot of the heavy lifting compared to pretraining. On experiments on PCs, the simple baseline MAX, which is a popular method for PCs, seems to outperform both SSMP and GUIDE without inference time training. The same is true, albeit to a lesser degree, for MADE.

- Evaluation based on average joint likelihood p(q,e) may be biased towards the neural-network based approaches, compared to evaluating using average conditional likelihood p(q|e). Because the loss function is based on joint likelihood, it will bias the network training to perform better on evidence e with larger probability p(e) than those with smaller probability. Average joint log-likelihood could easily hide poor results on instances with low probability evidence.


- The significance of Propositions 1 & 2 is unclear. More useful in motivating the training objective might be showing that the chosen loss function upper-bounds the target loss (negative log-likelihood) that we want to minimize.

Limitations:
The current approach is currently limited to binary variables.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel neural network-based method to solve the computationally challenging task of finding the Most Probable Explanation (MPE), which is known to be NP-hard. The proposed method involves an inference-time optimization process with a self-supervised loss function to iteratively improve the solutions. It also employs a teacher-student framework that provides a better initial network, which in turn helps reduce the number of inference-time optimization steps. Experiments demonstrate the efficacy and scalability of the proposed method compared to various baselines across different datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper presents a series of innovative methods, including inference-time optimization, a teacher-student framework, and self-supervised learning, to effectively address the MPE task.
The experimental results comprehensively evaluate the performance of the proposed approach and provide strong support for the claims made in the paper.
The paper is well-organized and clearly explains the details of the different modules and techniques used in the proposed solution.

Weaknesses:
The presentation of the main experimental results could be improved. The captions of Figure 1 and Figure 2 are quite simple and do not provide enough details about the information conveyed in the figures. It would be better to include more descriptive captions that explain the meaning of the rows, columns, and color schemes used in the visualizations.

Limitations:
N.A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
rcch4UsMBi;"REVIEW 
Summary:
## Overall summary
- This paper introduces GLAN, a method for enhancing LLMs by generating synthetic instruction data using a taxonomy of human knowledge and capabilities. GLAN constructs this taxonomy by decomposing knowledge into fields and disciplines, leveraging LLMs for generating a comprehensive syllabus for each subject. 
- GLAN’s scalable and customizable framework allows for easy integration of new fields of skills, highlighting its potential for ongoing improvement and adaptation.

## My opinion of the paper
- I think this is a really interesting approach to generate data that can allow LLMs to be potentially smarter. However, I am wondering if there are newer topics, for example (within the medical area, we have the new topic called ""Covid-19"".) Since GLAN is very dependent on LLMs, the main area of concern would be ensuring that the LLMs that GLAN depends on remains updated.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
## Originality
- The approach is quite interesting. The authors made use of real life scenarios, which is to use the structure of human education systems to build the taxonomy. This approach mimics the systematic acquisition of knowledge and skills in education, providing a framework for generating instruction data.
## Clarity
- Pseudo Algorithm provided and figures are easy to understand.
## Significance
- By creating a general and scalable method for instruction tuning, GLAN has the potential to improve the performance of LLMs across a wide range of tasks and domains.

Weaknesses:
## Quality
- While the paper claims scalability, there is limited discussion on the computational resources required for generating the synthetic data at scale. Practical constraints related to computational costs and time could be a potential weakness. It was mentioned in the checklist that it is very computationally expensive to repeat experiments.

Limitations:
Indicated in the appendix (do consider placing it in main paper), but did not mention about computation cost like what was mentioned in the checklist.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces generalized isntruction tuning (GLAN), an approach for synthesizing instruction tuning data using a taxonomy-based approach. GLAN generates synthetic instruction data from pre-curated taxonomy of human knowledge and capabilities and aims to create diverse and broad-ranging instruction dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Comprehensive Coverage of Evaluation: The paper presents extensive experiments demonstrating that GLAN outperforms various popular instruction-tuned LLMs across multiple dimensions, including mathematical reasoning, coding, logical reasoning, and general instruction following.
2. Minimization of Human Involvement: The generation process significantly reduces human involvement, requiring human verification only at the taxonomy construction stage. This makes the approach scalable and less labor-intensive.
3. Customizability and Extensibility: The taxonomy-based approach allows for easy customization and extension. New fields or skills can be incorporated by simply adding new nodes to the taxonomy.

Weaknesses:
1. While the paper addresses generalization, there is a risk that the generated synthetic data might overfit to the taxonomy's structure, potentially missing out on more nuanced, real-world instructions.

Limitations:
The authors have adequately addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces GLAN, a general and scalable method for instruction tuning of Large Language Models (LLMs). GLAN employs a top-down approach to generate high-quality instruction tuning datasets. Experiments across various benchmarks demonstrate that GLAN performs comparably to other existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper focuses on the alignment of Large Language Models, which is a trendy and important topic. If the dataset is released, it will be beneficial for the community.
2. This method is easy to follow. The process is highly scalable, leveraging LLMs like GPT-4 for generating instructions on a massive scale.
GLAN allows for easy customization. New fields can be added by incorporating new nodes into the taxonomy.

Weaknesses:
The novelty is limited as similar top-down designs have been utilized in many previous works. Besides, the main experimental results in Table 1 appear mediocre compared to other methods.

Limitations:
Refer to the weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a generalized way of creating instruction data. The high-level motivation is to take inspiration from how curriculum is designed for human learning into a taxonomy of subjects and use the same to prompt an off-the-shelf LLM to create data. GLAN does not need seed examples, or pre built-taxonomy like prior work. Human verification is also performed post the building of taxonomy to weed out unimportant or inaccurate divisions. The overall process is High level taxonomy -> subjects -> syllabus -> instructions.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Overall strong Performance: Extensive experiments show GLAN's effectiveness in various tasks, outperforming or matching state-of-the-art models in several benchmarks (Table 1)
2. Figure 2 on scaling properties of GLAN: I found this figure quite interesting. It suggests a log linear scaling trend in performance as GLAN data is scaled up. This is quite promising.
3. Section 3.5 on Task-specific overfitting: Another great analysis section that discusses how GLAN does not particularly overfit to the training data. This ensures that the synthetic data remains generalizable across different domains.
4. Modularity of the pipeline: The modular nature of the GLAN pipeline allows for easy customization and extension by incorporating new nodes into the taxonomy without re-generating the entire dataset.

Weaknesses:
1. No use of actual human curriculum: The paper set the expectation right in the abstract of using/getting strongly inspired from human curriculum. I was disappointed that the method does not utilize existing human curriculum structures, potentially missing out on years of insights in developing the same. Generating synthetic data, and in this case entire taxonomies from pre-existsing models can lead to extremely large amounts of bias. I would have much rather seen the authors delegate only lower level questions to LLMs than high level abstractions, which would lead to a trickle down effect on every single node in the taxonomy. This study, in my opinion, is incomplete without using either human generated taxonomies, and/or a comparison between how different the taxonomies are.
2. Computation cost not compared: The paper does not provide a comparison of computational costs with similar methods, such as WizardLM. For instance, GLAN training required approximately 8 days using 32 A100 GPUs to generate 10 million instructions, but no direct comparisons are made to illustrate the efficiency or cost-effectiveness relative to other approaches.
3. The method is limited by the performance of GPT-3.5/4: The quality of the generated taxonomy and syllabus heavily depends on the capabilities of the underlying LLMs used in the process, namely GPT-3.5 and GPT-4. In general, GLAN does not inform how we can improve capabilities of models beyond GPT4. But also, does not consider the cost of generating 10 million instructions.
4. High variability in results (Table 2): There is significant variability in GLAN's performance across different categories, with particularly weaker results in humanities and social sciences compared to STEM fields. The authors should address this, also discuss the document proportion of each taxonomy, and potentially see if there is a correlation between the data size and performance.

Limitations:
Please see Weaknesses

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
89AUi5L1uA;"REVIEW 
Summary:
The paper introduces SOFTS (Series-cOre Fused Time Series forecaster), an efficient multivariate time series forecasting model that addresses the gap between channel independence and channel correlation in a novel way. By utilizing a centralized STAR (STar Aggregate-Redistribute) module, SOFTS creates a global core representation aggregated from all series, which is then redistributed and fused with individual series representations. This mechanism allows for efficient channel interaction while reducing dependence on the quality of each channel. The paper demonstrates SOFTS's superiority over state-of-the-art methods in terms of performance and computational complexity, and showcases the STAR module's adaptability across various forecasting models.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- SOFTS proposes a unique approach to handling channel correlations in multivariate time series forecasting by employing a centralized STAR module, which aggregates and redistributes series representations efficiently.
- The model's design and implementation are well-thought-out, and the empirical results show that SOFTS outperforms existing state-of-the-art methods.
- The paper is well-written and structured, providing clear explanations of the STAR mechanism and its integration into SOFTS.
- The proposed method has the potential to enhance forecasting accuracy and efficiency across various domains, making it a valuable addition to the field.

Weaknesses:
- Additional experiments evaluating the robustness of SOFTS under varying conditions of distribution drift could provide deeper insight into its reliability.

Limitations:
- The authors have adequately addressed the technical limitations of their work within the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an efficient MLP-based model, the Series-cOre Fused Time Series forecaster (SOFTS). SOFTS incorporates a novel STar Aggregate-Redistribute (STAR) module to aggregate all series to form a global core representation, which is then dispatched and fused with individual series representations to facilitate channel interactions. The broad applicability of the STAR module across different forecasting models is also demonstrated empirically.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written.
2. The proposed STAR module is designed as a core to aggregate and exchange information from the channels efficiently, which is a universal module and can replace the attention mechanism.
3. The experiments in Figure 6 are interesting and the results are impressive.

Weaknesses:
The paper lacks significant innovation. The stochastic pooling itself is not novel in deep neural networks. A somewhat novel facet of the proposed model is to use the pooling to extract global representations. However, the idea of aggregate-and-dispatch the interactions between variables has already been studied in TimeXer [1].


[1] Wang, Y., Wu, H., Dong, J., Liu, Y., Qiu, Y., Zhang, H., ... & Long, M. (2024). Timexer: Empowering transformers for time series forecasting with exogenous variables. arXiv preprint arXiv:2402.19072.

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes to use a global representation to capture the channel correlations for multivariate time series forecasting. Specifically, it uses stochastic pooling to get the global representation by aggregating representations of individual series and then concats the global representation and individual representations to reflect channel correlations for each series. Experiment results confirm that the proposal is much efficiency and achieves better performance than existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper proposes an efficient method to capture the channel correlations for multivariate time series forecasting.
2. Extensive experiments are conducted to confirm the effectiveness of the proposal.
3. The paper is well-written and easy to understand in general.

Weaknesses:
1. Some experimental results are unconvincing.

-It is unclear why different datasets and metrics are used for different ablation studies, e.g., the datasets used in Table 3 and 4 are different, MAE is used in Figure 4 but MSE is used in other figures.

-There is no statistical significance tests between the results of the proposal and baselines.

-The results of Lookback Window Length 720 should be given as done in other papers.

-It is better to give the training time for each methods as well.

2. Some places are not clearly described.

-It is unclear the MLP and Linear operations are channel independent or dependent. I can guess they are channel independent, but it is not clearly described in Figure 1 and Section 3.

-The sentence ""... rely too heavily on the correlation to achieve satisfactory results under distribution drift"" in the Abstract is not clearly explained. 

3. Minor mistakes or typos: Embedding module is missing in Figure1; oi ∈ Rd' should be oi ∈ RC×d' in Algorithm 1.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors present a framework for modeling correlations between channels in a multivariate time series forecasting task. This framework concatenates each channel embedding with a ‘global core embedding’ which contains information from all channels in the lookback window. The authors present experiments that demonstrate the utility of this concept, both from a performance and efficiency perspective.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Time series forecasting has been an important problem and it continues to grow with the advent of time series foundation models. To the best of my knowledge, it remains an open question for how to best enable multivariate time series forecasting, and this paper provides a conceptually reasonable approach. I believe the authors’ work is of broad interest.

Weaknesses:
In my opinion, the paper would be improved with analysis and discussion of their results.  There is little discussion beyond drawing attention to features of figures and tables, which misses an opportunity to explain why the authors believe they are observing such behavior.  I am specifically interested in a discussion between iTransformer and SOFTS, which appear to be quite similar along many dimensions that SOFTS claims to be superior.  Such a discussion will help guide potential readers through the considerations they should take into account when deciding which framework to implement on their own forecasting problems.

I also recommend adding either a few sentences or a small figure that highlights the differences between PatchTST, transformer, and SOFTS — I see some details in the text of table 4, but I think making this information more prominent would be helpful and make the paper more self-contained.

Limitations:
As is, there is no discussion of the limitations in the main text of the paper.  To address this partially, section G of the appendix could be moved to the main text.

However, the discussion of the limitations of SOFTS currently exists in a vacuum, lacking a comparison/contrast with other models mentioned in the paper, especially PatchTST and iTransformer.  The authors should characterize and highlight the characteristics of datasets and inference tasks where SOFTS outperforms existing methods by wide margin.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
Ao0FiZqrXa;"REVIEW 
Summary:
This paper presents a fast-training accelerate sampling algorithm that is based on the distillation paradigm. This algorithm performs trajectory matching under all time points from 1 to 0 (t=80-0.006). The approach avoids the huge overhead of bi-level optimization through the ``detach()'' operation in pytorch, and further improves the performance of the algorithm by correcting the settings of a series of hyperparameters: loss function ($L_1$), step-condition, analytical first step.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The results of the algorithm presented in this paper are extremely impressive, surpassing all known distillation-based accelerated sampling algorithms.

2. The experiments conducted in this paper are thorough and well-analyzed. Particularly enlightening is the analysis of the guidance scale on Stable Diffusion.

3. The paper is exceptionally well-written, offering clarity and ease of understanding, and it is inspiring in various ways. Notable examples include the introduction of the SFD algorithm and the innovative use of CFG=1 for training, as well as the adaptation of any CFG for inference on SD-v1.5.

Weaknesses:
The paper has no significant shortcomings; however, it is worth noting that while the algorithm substantially reduces training costs, it also inevitably lowers the maximum performance threshold. An open question remains: if we extend the training duration, can we achieve results comparable to those of the CTM? It appears that not all time steps contribute equally to model performance. Additionally, introducing randomness through local trajectory matching could potentially enhance generalization.

Limitations:
Yes

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a fast distillation method for diffusion models. This method simplifies the existing knowledge distillation framework and proposes Simple and Fast Distillation from a global perspective to reduce redundant time steps in training. The SFD framework can achieve good experimental results in a very short time compared to existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The method proposed in the paper is not complicated and explores many available improvement ideas, achieving good results.
2.	Compared with existing methods, the method proposed in this paper can achieve good results in an extremely short time, with significant improvements in time efficiency.

Weaknesses:
1.	Many of the techniques in the paper lack innovation and are more like technical explorations.
2.	Some of the figures in the paper are difficult to understand, and figures like fig.6 should be explained more.
3.	The selection of some hyperparameters in the paper, such as t_min, appears too random.
4.	If the performance of other training methods such as CTM under the same training time as SFD were reported, it will further demonstrate the superiority of the proposed method.

Limitations:
Yes, the authors adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this work authors propose a novel diffusion distillation method by unrolling the student model to match unrolled/generated pre-trained diffusion (teacher) model's trajectory. Authors demonstrate effectiveness and compute efficiency of approach on stable diffusion.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Paper is easy to read and follow.
Show effectiveness on stable diffusion and much faster to train for large scale stable Diffusion too.
It is interesting to see fixing model at some part improves model over all other parts.

Weaknesses:
What is effect of SFD on diversity w.r.t distilled model? It might be easy to have high quality but much lower diversity. Proposed unrolling of student model ( global trajectory optimization) is effectively multi-step training like in structured prediction and imitation learning which has shown to be prone to mode-collapse.

Justification of method is still unclear and also not sure on how sensitive is SFD to training time noise schedule/time step weighting i.e., forward diffusion process and its impact on induced trajectory. E.g. if we assume flow matching style linear trajectories, they could be more easier to distill within proposed framework.

Unrolling trajectories for distillation is also considered in previous works to account for accumulation of error like BOOT, ImagineFlash, etc. So that in isolation can't be considered as major contribution.

Also proposed method though is more effective for Stable Diffusion, there is siginificant performance drop on LSUN, ImageNet etc w.r.t progressive distillation or consistency distillation? This also further raises questions and need for additional experiments to understand under what settings this method is effective.

Limitations:
As discussed in weakness and questions generalization to other models is unclear, effect of forward process on SFD? And currently this work also lacks conclusive reasoning under what settings its useful also do not demonstrate impact on diversity, etc.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
gXWmhzeVmh;"REVIEW 
Summary:
This paper introduces the Rough Transformer, a variant of the original Transformer that allows the processing of discrete-time series as continuous-time signals through the use of multi-view signature attention. Empirical comparisons shows that Rough Transformers outperform vanilla Transformers and continuous-time models on a variety of time-series tasks and are robust to the sampling rate of the signal.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Overall the paper is well written and is easy to follow
- The idea of using signature transform within an attention mechanism is interesting

Weaknesses:
- Empirical evaluations are limited, casting doubts on the true potential of the proposed architecture
- The tasks considered are rather simple, and it is not clear whether the proposed architecture will give favorable tradeoffs between accuracy and efficiency in the more challenging tasks (see below) 
- Missing evaluations on time series forecasting tasks (only classification and regression tasks are considered)
- Missing comparisons with recent RNN models (such as https://arxiv.org/abs/2110.04744, https://arxiv.org/abs/2212.00228), Transformer models (e.g., those studied in https://arxiv.org/abs/2011.04006), State Space models (https://arxiv.org/abs/2111.00396 and the more recent variants such as Mamba: https://arxiv.org/abs/2312.00752) and other sequence models (https://arxiv.org/abs/2305.01638, https://arxiv.org/abs/2209.10655) 
- Missing ablation studies on the components of the proposed architecture, particularly the role of the global and local components in the multi-view signature, truncation level n, etc.
- Missing related work; e.g., the papers mentioned above, https://link.springer.com/article/10.1007/s40304-017-0103-z and https://arxiv.org/abs/1710.10121 for continuous-time DL models, https://arxiv.org/abs/2006.12070, https://arxiv.org/abs/2102.04877 for continuous-time RNN models, https://dl.acm.org/doi/abs/10.5555/3546258.3546305 for using path signatures to understand continuous-time RNNs

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes Rough Transformers, an attention-based model for long continuous-time signals. The model utilizes ideas from rough path theory to extract path signatures from the continuous-time signal (obtained by interpolation of the original signal). Two types of signatures are extracted: global and local. The global signature extracts long-term information from the signal whereas the local signature extracts local information. Self-attention is then used on this ""multi-view"" signature. Experiments on classification and regression tasks show improved performance over existing model architectures.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper views the problem of modeling long continuous-time signals through the lens of rough path theory. While this in itself is not novel, the combination of rough path signatures with attention is a novel combination.
- Experiments on classification and regression tasks show that RFormer improves over existing models in terms of accuracy and significantly more compute efficient. (I have some concerns and questions about the empirical analysis, please see Weaknesses)

Weaknesses:
- The technical contribution is limited. In such scenarios, the empirical analysis needs to be sufficiently strong.
- The empirical analysis has been conducted on a few toyish datasets. While the results are definitely promising, more experimental support is needed to validate the model.
    - Although the model is motivated from a continuous-time and irregularly-sampled data perspective, the actual investigation of these settings is limited. As per my understanding, all experiments under 4.1 have been conducted on regularly sampled time series (please correct me, if I am wrong). I find it surprising that simple RNN-based methods do not perform well in these settings. If this is indeed the case, some simple CNN-based method should be studied. Recent models based on state space layers (e.g., S4, Mamba) can also be explored as baselines.
   - OOM for important baselines is not really helpful to draw any conclusions. To highlight efficiency of RFormer, please conducted experiments where you increased the context length or other parameters to show where baselines run OOM and how do they perform before that.
   - More experiments are needed, particularly for the forecasting task to understand how well the model understands the dynamics. 
   - I took a brief look at the code and it looks like hyperparameter tuning was conducted for RFormer. Was such tuning also performed for the baselines? If no (which is a valid response), how did you selected the baseline parameters? How sensitive is the model to different hyperparameters? 
- The discussion on related work needs to be moved to the main text and improved. Please contrast RFormer with the related works, particularly the ones that are closely related such as NRDE and ContiFormer. Discussion on some closely related works [1, 2, 3] is missing. Ideally there should also be a comparison with at least of these methods (e.g., CRU).

[1] Schirmer, Mona, et al. ""Modeling irregular time series with continuous recurrent units."" International conference on machine learning. PMLR, 2022.   
[2] Ansari, Abdul Fatir, et al. ""Neural continuous-discrete state space models for irregularly-sampled time series."" International Conference on Machine Learning. PMLR, 2023.   
[3] Oh, YongKyung, Dongyoung Lim, and Sungil Kim. ""Stable Neural Stochastic Differential Equations in Analyzing Irregular Time Series Data."" arXiv preprint arXiv:2402.14989 (2024).    

I am happy to update my score, if my concerns are adequately addressed.

Limitations:
There is a brief discussion on limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes Rough Transformer (RFormer), an extention of the Transformer architecture towards operating on continuous-time representations of time-series data. RFormer employs a novel technique called multi-view signature attention, which performs attention on path signatures pre-computed from input data offline, thereby capturing both local and global dependencies across observations. Experiments on various real-world time-series datasets shows that RFormer enjoys superior predictive performance as well as computational efficiency compared to previous methods, while being robust to changes in sequence length and irregular sampling.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- [S1] **Good novelty.** To the best of my knowledge, incorporating rough path theory to time-series representation learning is a novel approach, and would be of great interest to the machine learning community. 

- [S2] **Great empirical performance.** Experiments on a wide variety of real-world datasets show large improvements in both accuracy and efficiency, demonstrating strong utility of proposed multi-view signatures in time-series modeling.

Weaknesses:
- [W1] **Questionable motivation of synthetic frequency classification experiments.** The first experimental section tests RFormer on two synthetic datasets with which the task is to classify input time-series based on their ground-truth frequencies. While L233-234 mentions the second setup in particular is designed towards testing the long-range reasoning capabilities of RFormer, but it is unclear whether this indeed the case. For the second synthetic dataset, in particular, how can identifying the frequency be a proxy for long-range reasoning when the frequency $\omega_0$ is used for $t < t_0$ only? The results on Figure 2 showing that methods that are ""tailor-made for long-range time series modeling"" (L254) such as Neural-CDE and Neural-RDE underperforming significantly also indicates that the designed task is not really representative of long-range reasoning.
\
\
More interesting questions to ask could be: What makes RFormer sample-efficient vs. vanilla Transformer particularly on the Sinusoidal dataset and not so much on the Long Sinusoidal dataset? What makes RFormer more robust to changes in sampling frequency compared to Neural-CDE and Neural-RDE? Table 1 of [A] shows Neural-CDE is also quite robust to dropped data, but is this characteristic not emergent for Sinusoidal and Long Sinusoidal datasets?

- [W2] **Missing analysis on interpolation methods.** By default, RFormer uses piecewise-linear paths for computing path signatures, but as mentioned in L140, it seems any continuous-time interpolation can be used. As such, it would be interesting to discuss (1) whether any other interpolation techniques can be deployed efficiently similarly to piecewise-linear paths and (2) if they lead to any boosts in predictive accuracy, but these discussions are missing in the current draft (i.e., is the currently used piecewise-linear interpolation ""pareto-optimal"" under performance-efficiency trade-offs?).

[A] Kidger et al., Neural Controlled Differential Equations for Irregular Time Series. NeurIPS 2020.

Limitations:
The authors have adequately addressed the limitations in Appendix B.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
QEmsZoQ45M;"REVIEW 
Summary:
The paper proposes two novel classes of MDPs in the function approximation setting:
* Mildly Smooth MDPs: in which the bellman optimality operator outputs smooth functions of degree $\nu$.
* Locally Linearizable MDPs: in which there exist a state-action feature mapping into $\mathbb R^d$ and a finite partition of the state-action space that give rise to a Q function approximator class, which is linear in the feature mapping inside each of the partitions (the vector defining the linear mapping is allowed to change between partitions).

An online algorithm (CINDERELLA) is presented and a proof of sublinear regret is given for the class of Locally Linearizable MDPs. Subsequently, it is shown that for any Mildly Smooth MDP, there exist a feature mapping and a partition (of size exponential in $d$) that constitute a Locally Linearizable MDP. 
As a result, a sublinear regret bound of CINDERELLA is derived for the class of Mildly Smooth MDPs: 
$\tilde O(H \nu_*^d K^{\frac{\nu + 2d}{2\nu + 2d}}$  $ + H^{\frac{2\nu + 2d}{\nu}})$, 

where $\nu_* := \lceil \nu - 1 \rceil$.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* This work proposes a new class of continuous state-action MDPs in which sublinear regret is achievable
* In particular, this new class is the currently the most general one that does not suffer from lower bounds exponential in $H$

Weaknesses:
* The regret bound has factors that are exponential in $d$, and sublinearity in $K$ is weak (as to be expected in this level of generality).
* Similar to other algorithms in this line of work, the CINDERELLA is not computationally efficient and far from being a practical algorithm.
* I am not sure what we really learn from such a work; the technical details are complicated even at the level of exposition, and the results are very weak (as to be expected in this level of generality).

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a notion of local linearization, which is then applied to smooth MDPs. It generalizes the ""Eleanor"" algorithm into ""Cinderella,"" which, by avoiding a ""cardinality of N"" term on the suboptimality with respect to the inherent Bellman error, gets sublinear regret for all classes of smooth problems.

EDIT: I appreciate the authors' detailed feedback. My score remains unchanged.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper is written very clearly, exposes all the strengths and weaknesses of its contributions, and makes connections to prior work in a thorough and honest fashion. The paper explain, in particular, why Cinderella is inadequate, and how the parameter learning is decoupled in a way to permit efficient learning. This requires additional technical subtlety in the analysis, but with it comes a bound which would be intractable otherwise.

Weaknesses:
This paper is well executed and technically solid, but I believe its main weakness is that the results are not particular surprising to an expert in the area. It seems clear that one can decompose ""smooth"" MDPs into local linear ones (this has been done, e.g. with zooming bandits and zooming MDPs), and is rather unsurprising that a careful way of handling the analysis makes these objects learnable. I don't seem to see any particular new techniques or insights, and so it is hard for me to be incredibly excited about the result. Still, the paper is executed commendably and for that I lean towards acceptance. 

There are also some limitations with presentation. Chief among them, the authors should do more to emphasize the algorithmic differences between Eleanor and Cinderalla (as they did so well with Eleanor's limitations for regret). The extent of the discussion seems limited to the sentence ""Difference with ELEANOR stays in the fact that parameters relative to different regions are learned separately"", which (a) bears elaboration, (b) is somewhat hidden in the mass of text, and (c) is not grammatically correct. I would encourate the authors to explain what techniques and ideas are needed to update the algorithm and analysis for the resultant guarantees.

Limitations:
Novelty, somewhat incremental. Moreover, this paper has the computational inefficiency challenges associated with many algorithms in the field, and, like any non-parametric style method, solves exp(dimension, horizon). Lastly, there do not seem to be any lower bounds that characterize the correct exponents.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces the concept of Locally Linearizable MDPs, a class of MDPs that generalizes existing ones like Linear MDPs and MDPs with low inherent Bellman error. In this model, the state-action space is partitioned into $N$ regions, where the Q-functions belong to a class that allows the result of the Bellman optimality operator to be well approximated by a linear function within each region, up to some ""Inherent Bellman Error."" The authors propose CINDERELLA, a no-regret (computationally inefficient) algorithm designed for this class, achieving regret bound of $Nd\sqrt{K} + \sqrt{d}\mathcal{I}K$, where $N$ is the number of partitions, $d$ is the dimension of the state-action feature space, $K$ is the number of episodes and $\mathcal{I}$ is the Inherent Bellmann Error. 

Next, they present the class of ""Mildly Smooth MDPs"". This class is similar yet slightly less general than ""Weakly Smooth MDPs,"" which assume that the output of the Bellman operator on a smooth function remains smooth. Still, Mildly Smooth MDPs are more general than ""Strongly Smooth MDPs"" and thus positioned somewhere between the two. The authors show that Mildly Smooth MDPs are Locally Linearizable for some partition and Inherent Bellmann Error and by that achieve regret of $H v^d K^{\frac{\nu+2d}{2\nu+2d}} + H^{\frac{2\nu + 2d}{\nu}}$ where $\nu$ is the smoothness parameter of the class.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-crafted and clearly positions itself within the existing literature. It employs a well-structured pedagogical approach. It first presents a straightforward solution for Locally Linearized MDPs and proceeds to point out that this naive approach's regret bounds are insufficient for deducing a non-trivial regret bound for Mildly Smooth MDPs. This leads to the introduction of their algorithm, which offers improved regret bounds and leads to their main result.
    
The authors conduct a comprehensive analysis of the regret bounds, discussing each component to assess its necessity and provide a comparison with prior work. For the most part, the paper is transparent and addresses its limitations.

Weaknesses:
The nature and definition of Mildly Smooth MDPs are not very natural, and it is hard to agree that learning becomes substantially more feasible compared to Weakly smooth MDPs. Specifically, the authors mention that in Weakly Smooth MDPs the regret bound must scale exponentially with $H$, which they consider ""statistically unfeasible"". However the proposed approach results in a regret bound that scales exponentially with $d$, the dimension of the feature space. This raises similar concerns about the practical feasibility of the setting. The authors claim that for $\nu \to \infty$ the regret becomes of order $\sqrt{K}$, but this is not clearly derived from the provided regret bounds, leaving it unclear whether the result generalizes existing results in Strongly Smooth MDPs.

Limitations:
For the most part, the paper addresses its limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper discusses the concept of local-linear MDPs which is a general representation class of MDPs that extends previous works on learnable (sublinear regret in $K$) and feasible (polynomial regret in $H$) episodic MDPs.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper considers important complexity questions associated with the continuous episodic MDP, and is very well-written. Compared to the much more well-studied tabular episodic case, the complexity of continuous episodic MDPs is much more challenging to characterize. The authors identified a class of mildly smooth MDPs and show one can achieve no-regret learning in this regime. The regret bound still has an exponential dependence in $H$ but the authors demonstrated through a continuous bandit special case this exponential dependence is unavoidable. The class of mildly smooth MDPs is larger than the previously studied classes such as kernelized MDPs and strongly smooth MDPs where no-regret learning is possible.

Weaknesses:
Even though the theoretical development is sound and complete, I think the authors could do better in further highlighting their contributions which I will use the questions below to elaborate.

Limitations:
There is no obvious limitation in this paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studied a structural property called local linearity that makes continuous MDPs learnable. In particular, local linearity means that the continuous state-action space can be partitioned into multiple regions and in each region, the Q-function is linear w.r.t. a unique (different) parameter. The paper first proposed an algorithm to learn MDPs with local linearity and proved that the regret is sublinear. Then, the paper showed that mildly smooth MDPs, where the Bellman operator is smooth, are locally linearizable. So that it demonstrated the broadness of local linearity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The studied problem is important.
2. The new concept of local linearity is interesting and indeed capture a broad class of MDPs according the paper.

Weaknesses:
1. The proposed algorithm is computationally inefficient. It needs to run over all regions while the number of regions might be exponentially large according to section 4.

Limitations:
No further limitations need to be addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
MSSRhxwZP7;"REVIEW 
Summary:
This paper explores the method for No-Reference Point Cloud Quality Assessment. The key idea is to involve the disentangled representation learning to minimize mutual information between representations of point cloud content and distortion. The authors conduct experimental performance comparisons on three public databases and compare their proposed method with 15 existing models. The proposed method achieves optimal or suboptimal results in most of the metrics. The ablation study demonstrated the necessity
of each design part.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is clearly written and provides a detailed formulation of the approach.

Weaknesses:
The weak point of the paper is its presentation. Many terms are introduced without explaining them properly, e.g. ""the tight upper bound"" or ""the masked autoencoding strategy”. Furthermore, the Fig 2 is not well-designed with content aware branch. The Fig2 splits the proposed architecture and the content-aware pretraining and masked autoencoding strategy, which does not help the reader to understand architecture with figure. Lastly, it would be highly beneficial to make the code publicly available to enhance collaborative efforts and facilitate the sharing of this work. Apart from this, there are quite a few typos and grammar mistakes that should be corrected, such as “the can be”, “masked auto-encoding / autoencoding

Limitations:
See Weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel no-reference quality assessment model tailored for point-cloud data. A disentangled representation learning strategy is leveraged to account for both content-aware information and distortion-aware information. Comprehensive experiments are conducted and the effectiveness of this proposed model is well verified.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed framework is novel and the paper is easy to understand.

Weaknesses:
1. The idea of framing the quality assessment into content-/semantic-aware and distortion-aware aspects is actually quite common in IQA/VQA on 2D visual data. It’d be better to review some related QA metrics for 2D data, and analyze the key difference on the implementation of this idea between point-cloud data and 2D data quality assessment.
2. Lack of comparison on computational complexity.

Limitations:
Please see the Weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel disentangled representation learning framework called DisPA to decouple the representation learning process of point cloud content and distortion. To sufficiently disentangle these two representations, the DisPA uses two branches to learn them and adopt different training philosophies separately. For the content-aware branch, DisPA pretrains one encoder using a proposed masked auto-encoding strategy, which partially masks the images projected from distorted point clouds and reconstructs the corresponding patches of the images projected from pristine point clouds; For the distortion-aware branch, the DisPA integrates the mini patches of the rendered multi-view images into a mini-patch map, which can focus on local distortions and ignore the global point cloud content. Furthermore, to disentangle the learned representations, the DisPA uses a trainable mutual information estimator to estimate the mutual information (actually the tight upper bound) between these two branches and further minimize it alternatively along with the training of the main network (i.e., the two encoders and regression layers). Finally, the experimental results demonstrate the superior performance of DisPA in terms of both prediction accuracy and generalizability.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ This is the first paper exploring disentangled representation learning for PCQA. The disentanglement is reasonable and even necessary because the point cloud content and distortion are differently perceived by humans.

+ The proposed methodology is well-motivated. The masked auto-encoding strategy can intuitively make the encoder capture the point cloud content information.

+ The paper is written well. The motivation of why disentangled representation learning is essential for PCQA has been introduced clearly from the observation of human vision systems.

+ The mathematical derivations are detailed and easy to follow, including the approximation of MI in Section 3 and further proof in Appendix A.

Weaknesses:
1. From my viewpoint, the DisPA can be totally used for IQA, or even more suitable, since this work does not process 3D native point cloud data but just projects point clouds into images and uses 2D networks. Have the authors tried to introduce unique attribute information that is closely related to point clouds?
2. The whole DisPA is based on projections of point clouds, so the number of viewpoints is very important to the quality score prediction. However, the authors did not discuss the impact of the number of viewpoints.
3. How much time does it take to pretrain the content-aware encoder? The authors did not discuss the pretraining details in the implementation details part.
4. Why does not the LS-PCQA follow the K-fold data splitting? The authors should explain this for the loss of a unified experiment setting.
5. What is the actual architecture of the MI estimator? And what is the relation between the MI estimator and the lightweight neural network Q_phi? The idea of alternative training of the MI estimator and the main network is easy to understand, but the components of the MI estimator need more clarification. Is it just simple MLPs?
6. The mini-patch map generation has been used in many papers [1,2,3]. However, considering this is a minor contribution and the mini-patch map is actually effective for learning disentangled representations, this limited contribution is acceptable.
7. A minor weakness: There are some small flaws (gray bounding box) with the presentation of Figure 5 when zooming in. Please ensure all figures can be presented clearly without error.

[1] Wu, Haoning, et al. ""Exploring video quality assessment on user generated contents from aesthetic and technical perspectives."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.

[2] Wu, Haoning, et al. ""Fast-vqa: Efficient end-to-end video quality assessment with fragment sampling."" European conference on computer vision. Cham: Springer Nature Switzerland, 2022.

[3] Zhang, Zicheng, et al. ""Gms-3dqa: Projection-based grid mini-patch sampling for 3d model quality assessment."" ACM Transactions on Multimedia Computing, Communications and Applications 20.6 (2024): 1-19.

Limitations:
Yes, the authors have adequately addressed the limitations and discussed the potential negative societal impact in the appendix D Limitations and Future Work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This article's motivation is interesting. It combines content-aware and distortion-aware characteristics to train a 3D quality assessment network. Additionally, it employs a MAE-based method to train a content-aware encoder, uses patches to focus the network on learning distortion, and applies an MI module to integrate both features.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors first analyze the shortcomings of existing networks, specifically that data imbalance leads to overfitting in current methods, resulting in poor processing of other content images with the same degradation. Therefore, the authors aim to use an MAE-based method to learn content-related features.
2. The proposed key MI-based regularization is effective.
3. The presented methods can obtain impressive results on multiple datasets.

Weaknesses:
1. Regarding the masked part, there is a gap between the first and second stages because the input in the first stage is a partially masked image, while the input in the second stage is the complete image.
2. I am also a bit confused about why the constraint in the first stage is a clean/reference image. This would give the masked encoder the characteristic of restoration, while the core of quality assessment is to evaluate the quality of the image. If the image features are restored, will it affect the accuracy of the image quality assessment?
3. The motivation for the MI part should provide more details and explanations, which leaves me somewhat puzzled.
4. I am not quite sure what the distortion-aware encoder has learned. Is it truly related to degradation features? This might require some verification.

Limitations:
No potential negative societal impact. Please address the questions/suggestions above.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
Hc2ZwCYgmB;"REVIEW 
Summary:
This work presents a zero-shot face-generation method based on diffusion models. The proposed method first extracts face features using Face2Vec and trains a network to map these features into the textual space (i.e., the prompt embedding space for diffusion’s text condition). The main difference from existing zero-shot face generation approaches is that, instead of adding conditions in the denoising UNet, the presented method incorporates the condition into the textual embedding. To prevent the subject information from overwhelming the generation (e.g., preserving only the subject ID while ignoring other descriptions), the authors propose a Composition Distillation Loss. This contrastive loss encourages the model to generate descriptions other than the subject information.

The main shortcoming of the paper lies in the experimental validation. There are multiple components proposed, but their effectiveness is not adequately demonstrated. Additionally, both qualitative and quantitative evidence fail to show that the proposed method outperforms SoTA methods.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
* The Compositional Distillation Loss is an interesting and intuitive approach to reducing the problem of subject information overwhelming the generation

Weaknesses:
* There is a lack of intuition behind using multi-timestep distillation. This approach can cause accumulated errors, and there is no evidence provided to support the benefits of adopting such a strategy.
* The section on dynamic model expansion is very unclear. It is not specified which part of the model is expanded or if the tokens are simply replicated. While adding Gaussian noise to replicated tokens is mentioned, there is no empirical validation of its effectiveness.
* The qualitative comparison, especially in Figure 7, does not support the claim that the proposed method has advantages over baselines like PuLID. Additionally, the benchmarking results in Table 1 show limited improvement in facial identity preservation, and the text alignment can be inferior to PuLID. Thus, the experiments are not comprehensive enough to convincingly demonstrate that the proposed approach is superior to existing methods like PuLID.

Limitations:
The authors have discussed the limitations.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes AdaFace, a face encoder that maps facial features from the image space to the text space through the AdaFace Prompt Inverter, utilizing the structure and pre-trained weights of the CLIP text encoder for initialization. During the face distillation phase, AdaFace employs random Gaussian face embeddings and multi-timestep distillation, enhancing the model's ability to capture subtle facial details through dynamic model expansion. In the composition distillation phase, AdaFace uses a comparative learning loss, aligning feature increments with orthogonal subtraction, while introducing an elastic face preservation loss to address the misalignment of facial features caused by different prompts.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well written and easy to follow
- The proposed method requires fewer training resources
- The approach to constructing contrastive pairs during the composition distillation stage sounds reasonable

Weaknesses:
- I tried the demo provided by the authors, and the ID similarity on a few test images was relatively low; it should be far from the state-of-the-art (SOTA) level of ID similarity claimed in the paper.
- The test dataset consists of celebrities, which does not guarantee whether these IDs have appeared in the training set. Furthermore, the number of test samples is too small to be convincing.
- The upper bound of ID fidelity is constrained by the frozen Face2Image model, in this paper, Arc2Face.
- The proposed improvements like Random Gaussian Face Embeddings, Orthogonal Subtraction, etc. are not effectively validated through ablation study.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes AdaFace, a method for personalizing text-to-image diffusion models for human faces. At its core, it learns a prompt inverter that maps face embeddings from a pretrained face encoder to the text embedding space of diffusion prompts. It leverages various components including face distillation, composition distillation and elastic face preserving loss to preserve subject identity while attaining good compositionality.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper designs targeted training losses and regularizations for the task at hand. The explanation of the methods is detailed. The video qualitative results show improvement over ID-Animator.

Weaknesses:
*  The quantitative metrics do not show a clear advantage of AdaFace over other existing personalization methods like PuLID. The number of qualitative examples for comparing with those methods is also limited -- just the 5 images per method in Figure 7, and not sufficient to clearly demonstrate that AdaFace outperforms existing methods. It would be helpful to show a larger number of uncurated examples comparing AdaFace and baselines to get a better comparison of their performance.
* The training of the prompt inverter involves a number of components -- such as model expansion, the inclusion of different feature types in composition distillation, orthogonal subtraction and elastic face preserving loss -- but there are no ablation studies on most of them to demonstrate their effects on the performance.

Limitations:
The authors have discussed limitations and societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes AdaFace, a test-time-tuning-free method for personalized text-to-face-image generation. Previous methods involving face features in the feature space of a face encoder, which is not flexibly composable with natural language for personalized generation. Thus, this paper proposes to map the face features into the features in the text conditioning space. Several techniques are proposed to enhance the performance, like Random Gaussian Face Embeddings, Multi-Timestep Distillation, Dynamic Model Expansion, Composition Distillation, and Elastic Face Preserving Loss. Some experiments demonstrate that the proposed method achieves good visual results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The visual results are satisfactory in general.
2. The proposed method can also be applied for personalized text-to-video generation.

Weaknesses:
1. The overall motivation is not novel enough. Finding ways to convert input images into the textual space is a fundamental goal in text-to-image personalization, which has been emphasized in the very first TextualInversion work. Even in the context of tuning-free based methods, the proposed framework is not so novel compared with ELITE [a], which also involves training a mapper from the image space into the textual space. Similar compositional distillation technique has also been explored in SuTI [b].
2. Lack of detailed studies of the proposed components, either qualitatively or quantitively. The authors propose a bag of techniques to improve the performance. Although their motivation is mentioned in the texts, there is no supportive results to illustrate how these techniques work. 
    * There is only one quantitive study in Tab. 1 regarding the compositional distillation. However, there are actually a lot of technical details in the proposed compositional distillation techniques, like the orthogonal subtraction and compositional delta loss, which lack careful experimental analysis against their alternatives. 
    * The proposed face distillation is not well supported. Can we simply train the face encoder with the simple noise prediction loss of diffusion models?
    * The analysis of the proposed Elastic Face Preserving Loss is also missing.
3. ELITE [a] mentions that using multiple token to represent an image may hurt the textual compatibility. It is necessary for the authors to provide a rationale for doing so.
4. How about the method comparing with the popular IP-Adapter (face version) [c]?
5. The overall training pipeline requires multiple stages of training, which is not so elegant.
6. The authors would like to consider merging multiple figures with similar functionalities and structures to one, like Figs. 2, 3, 4 and 5, to leave enough space for necessary experimental results.

[a] ELITE: Encoding Visual Concepts into Textual Embeddings for Customized Text-to-Image Generation, Wei et al., ICCV 2023.

[b] Subject-driven Text-to-Image Generation via Apprenticeship Learning, Chen et al., NeurIPS 2023.

[c] IP-Adapter: Text Compatible Image Prompt Adapter for Text-to-Image Diffusion Models, Ye et al..

Limitations:
The authors have discussed the limitations of the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
Ke40kfOT2E;"REVIEW 
Summary:
This paper extends the work of PIC with continuous LVs, proposes to tensorize and to use neural functional sharing to scale the model.
The results show that the proposed approach decreases the trainable parameters as well as the memory usage in training, and also reduces the running time. The approach also works well in density estimation tasks.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper extends the work of PIC for better scalability and efficiency, and thus has its novelty.

The paper is well written and not difficult to understand. e.g., the illustration of Algorithm 1 in Fig.2 (a-b) is a very nice visualisation. 

With the claimed contribution, the paper has a high potential to contribute to the community.

Weaknesses:
It is not clear to me what data set is used in the experiment of ""Scaling PICs"", and how many RVs are modelled. 

I compared the density estimation results in PIC[18] on the MNIST family and the results in this paper outperforms the ones in PIC[18].
As far as I understand, the PIC (F, C) in this paper has reduced its trainable parameters and therefore should in principle has smaller model capacity. I am wondering why the density estimation results of the proposed model outperform the original PIC? 

Similarly, it would be nice if the drop (or increase, though unlikely) of density estimation performance can be shown between PIC (F, N) and PIC (F, C) so readers will know how much they can lose/gain w.r.t. performance when applying the functional sharing.

Limitations:
The limitations are well discussed in the end of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a pipeline to build probabilistic integral circuits (PICs) more generally as directed acyclic graphs, as opposed to the tree-shaped structure they were limited to before. This significantly increases the expressiveness of the PICs, improving their representation of distributions. The authors present a procedure for turning a region graph into a PIC, into a QPC that approximates the PIC, and finally into a folded QPC, which are effectively tensorized structures that significantly speed up the inference and learning process. From the tensorized PIC the authors then make further improvements on QPC training, using neural functional sharing techniques over groupings of the input and integral units to result in greater materialization of QPCs and far fewer parameters.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
+ The proposed approaches significantly increase the expressivity and learning efficiency of PICs, as showcased by the experiments.

+ The paper is technically sound with clear arguments presented for each step.

+ The paper is overall well structured and easy to follow.

+ The authors also address the current limitations of PICs with a clear goal for future improvements.

Weaknesses:
Not a major weakness, but as the authors also point out, it is currently not possible to sample from PICs, limiting their impact as a generative model.

Limitations:
The authors acknowledge the limitation of PICs that, despite being a continuous latent variable model like many generative models, they do not support sampling. This is left as future work.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new approach for building probabilistic integral circuits (PICs), a recently introduced probabilistic model that extends probabilistic circuits (PC) with continuous functions of latent variables (in addition to discrete latent variables), with inference performed using a numerical quadrature procedure. In particular, while the previous implementation was restricted to tree-structured region graphs/variable decompositions, it is shown how to construct PICs for arbitrary region graphs. Additionally, sharing of functions across different regions is used to improve parameter efficiency. Empirical results show that the architectural improvements enable scaling to complex image datasets such as ImageNet64, outperforming comparable PCs trained using maximum likelihood or EM.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper proposes a number of innovations on top of the recently proposed PICs, such as allowing for arbitrary region graphs and functional sharing. Taken together, this constitutes a well-engineered solution that significantly improves the scalability and flexibility of PICs. 

The experiments are well-executed with strong results on a range of image datasets in comparison to comparable PCs learned directly. Many experimental details are reported, such as memory usage, time taken and parameter counts/hyperparameters, which should make the results reproducible.

The paper was a pleasure to read, with clear and consistent notation throughout explaining the method and accompanying pictorial illustrations that make the idea of the paper transparent and easy to understand.

Weaknesses:
The paper is arguably a little weak in terms of novelty as most of the new components are adaptations of existing ideas in the literature (e.g. tucker/cp layers, parameter sharing) to probabilistic integral circuits.

Limitations:
The limitations of the method are appropriately addressed in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work extends the probabilistic integral circuits (PICs) from tree-shaped region graphs (RGs) based ones to DAG-shaped ones. While constructing PICs from DAG RGs can lead to more expressive models, it comes with the concern of scalability since the circuit sizes might be increased a lot. To address this concern, this work proposes tensorized circuits to approximate the resulting PICs by a PIC to QPC reduction. This is further combined with functional sharing and materialization to achieve efficiency and scalability, which is validated by experimental results on several image datasets.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The extension from tree-shaped PICs to DAG-shaped ones is a solid contribution as it greatly improves the expressiveness of the resulting PICs and is an important step towards PICs constructed from more complex RGs for complicated tasks.
- The functional sharing and materialization techniques proposed in this work are shown to be very useful by the empirical results. From Fig 5, learning PICs consumes similar resources to PCs which are efficient models and it requires much less trainable parameters than the ones without functional sharing, making the training of large-scale PICs feasible.

Weaknesses:
I appreciate that this work aims to solve an important problem and the proposed solution is shown to be novel and efficient. Still, it's heavy in technical details and thus I don't think I fully understand the algorithmic details. Specifically,
- I'm confused by the definition of the integral unit from Line 58 to Line 62, especially by the inconsistency in the input of g_i between Line 61 and Line 62 (the g_i inside the integral). Also, the integral in Line 62 contains the function g_i while Figure 1(b) does not, which is also confusing. It might be helpful to put a numeric example there.
- This also applies to the other technical sections of this work given that this work is heavy in techniques. It would be very helpful to provide a toy example to showcase the reduction of PICs to QPCs to improve accessibility and reproducibility for readers who are not well-versed in these areas.
- I'm curious to see some theoretical analysis on the circuit sizes characterized. I understand that the bounds might be loose and not that informative. Still, it might be helpful to understand how the PIC to QPC reduction affects the circuit sizes.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
r7mj17BKzw;"REVIEW 
Summary:
This paper introduces SuperEncoder, a novel approach to Quantum State Preparation (QSP) that aims to combine the scalability of Approximate Amplitude Encoding (AAE) with the speed of traditional Amplitude Encoding (AE). SuperEncoder uses a pre-trained neural network to directly estimate the parameters of a Parameterized Quantum Circuit (PQC) for any given quantum state, eliminating the need for iterative parameter tuning during runtime. The authors explore different loss functions for training SuperEncoder, finding that state-oriented training using fidelity as a metric (L3) performs best. They evaluate SuperEncoder on synthetic datasets and downstream tasks like Quantum Machine Learning and the HHL algorithm, comparing it to AE and AAE. Results show that SuperEncoder achieves runtime similar to AE while maintaining the scalability of AAE, but with some degradation in fidelity. The impact of this fidelity loss varies across applications, being more tolerable in QML tasks than in precise algorithms like HHL.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper presents a novel approach to Quantum State Preparation with SuperEncoder, which innovatively combines the strengths of existing methods (AAE and AE). The idea of using a pre-trained neural network to directly estimate quantum circuit parameters is a nice solution to the QSP problem.

Quality: The research demonstrates high quality through its comprehensive experimental design. The authors explore different loss functions, provide detailed analysis of their landscapes, and evaluate the method on both synthetic datasets and real-world applications. The comparison with existing methods (AE and AAE) across multiple metrics (runtime, scalability, and fidelity) shows a rigorous approach to validation.

Clarity: The paper is well-structured and clearly written. Complex concepts are explained in an accessible manner, with helpful diagrams (like Figures 2 and 3) to illustrate key ideas.

Significance: SuperEncoder potentially represents a step towards more efficient QSP, which is crucial for many quantum algorithms.

Weaknesses:
1. The gradient evaluation of the loss function (e.g. Eq. 1) requires computing the derivative of the state $\rho$ with respect to model parameters. As the authors acknowledge, this could become complicated on real devices due to the enormous cost of quantum state tomography. The authors work around this by using the parameter-shift rule to compute the gradient. However, the parameter-shift rule does not scale as well as classical backpropagation with autodiff (see https://openreview.net/forum?id=HF6bnhfSqH -- I guess a citation to this work would be relevant here). This casts doubts on the whole scalability of this method.

2. Again related to scalability, the number of input neurons to the model has to be $2^n$. This again doesn't look too scalable past 20 qubits, which can already be realized experimentally.

Limitations:
Limitations have been discussed

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors propose a model, namely SuperEncoder, to solve the quantum state preparation problem. Instead of evolving the parameterized gates to generate the target quantum state, they train a model to predict the rotation parameters from the target states.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Solve the quantum state preparation problem from a new perspective.

Weaknesses:
1. Poor results. The results seem ok with four qubits but decrease way too fast when increasing the number of qubits. The proposed method is not comparable to previous methods.
2. It is actually impossible to use an ML model to predict the parameters. Since training the AAE ansatz is a non-convex optimization problem, finding the optimal parameter is indeed an NP-hard problem. There are infinitely many pairs of quantum states and parameters, and I wonder how the size of the training set would scale with the number of qubits. 
3. The training overhead is non-negligible. If we are preparing a quantum state that is beyond the simulation power of classical devices, the evaluation methods based on state fidelity would need an enormous number of quantum circuit executions, which I suspect would not be much less than training the AAE.

Limitations:
Naive ideas with poor experimental results.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper addresses the problem of Quantum State Preparation (QSP), which is critical for quantum computing but requires a circuit depth that scales exponentially with the number of qubits, making it impractical for large-scale problems. The authors propose SuperEncoder, a pre-trained classical neural network model designed to estimate the parameters of a Parameterized Quantum Circuit (PQC) for any given quantum state. This approach eliminates the need for iterative parameter tuning, making it a significant advancement towards iteration-free approximate QSP. 

Contributions

1. Introduction of SuperEncoder, which pre-trains a classical neural network to estimate PQC parameters directly, bypassing the need for iterative updates.
2.  Provides empirical evidence that SuperEncoder significantly reduces the runtime for quantum state preparation compared to traditional methods, thus enhancing the efficiency of quantum algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
See  Contributions.

Weaknesses:
1. [Scalability Issue]
The most significant drawback of this work is its poor scalability. Since the input to the SuperEncoder is $2^n$ dimensional, the number of qubits cannot be too high, such as exceeding 20 qubits. This limitation severely restricts the applicability of the SuperEncoder to larger quantum systems. Discussing potential strategies to overcome this drawback would greatly enhance the practical value of the SuperEncoder.

2. [Barren Plateau Problem]
Another major issue is that, even within a reasonable range of qubit numbers (e.g., 10-20), training the SuperEncoder is challenging due to the barren plateau problem. Consequently, the SuperEncoder is likely only suitable for situations involving fewer than 10 qubits. In these cases, the time difference between AAE and SuperEncoder is not as significant as one might expect, which greatly limits the potential impact of this work.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
NC0Bjl4uTf;"REVIEW 
Summary:
The paper presents a novel Chinese inertial generative adversarial network (CI-GAN) designed to generate high-quality training samples for Chinese writing recognition using inertial sensors. The CI-GAN integrates Chinese Glyph Encoding (CGE), Forced Optimal Transport (FOT), and Semantic Relevance Alignment (SRA) to enhance the quality and authenticity of generated inertial signals. The approach addresses the challenge of collecting diverse and extensive training data for Chinese character recognition, showing significant improvements in classifier performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper introduces innovative methods in the form of CGE, FOT, and SRA, contributing significantly to the field of inertial writing recognition. The release of a new dataset further enriches the community's resources.

Weaknesses:
1.Lack of Detailed Baseline Configuration: The paper compares CI-GAN with a traditional GAN in the appendix, but fails to provide detailed settings for the baseline method. This lack of information hinders the ability to fully understand and replicate the comparative effectiveness reported.
2.Insufficient Comparison with Other Augmentation Techniques: The study does not compare CI-GAN with other data augmentation methods, such as random perturbations. It remains unexplored whether applying random disturbances to the data could also substantially improve classifier performance.

Limitations:
The authors have discussed limitations related to the variability of writing styles and the potential impact of environmental factors on sensor data.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes CI-GAN to acquire unlimited high-quality training samples, alleviating the data scarcity in the inertial signal recognition of Chinese characters. By utilizing these generated data, the performance of recognition models is highly improved.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is easy to follow.
- The proposed methods may help disabled people.

Weaknesses:
- The pipeline lacks novelty. The employed technologies are widely used in CV and NLP, and the proposed pipeline merely reuses them for the inertial signal domain without any innovative design. Furthermore, the author fails to cite relevant studies such as [1][2] and does not discuss their differences.
[1] Wasserstein GAN (WGAN)
[2] Efficient Estimation of Word Representations in Vector Space

- The proposed CGE is simply a learnable embedding to represent Chinese characters, lacking innovative design for glyph information. The author introduces GER to enhance the orthogonality of character embeddings but does not provide an ablation study to verify its effectiveness.

- The author uses Wasserstein distance in GANs. What is the difference between this approach and WGAN [1]? Additionally, the author proposes using FFM to supervise the signal in feature spaces. These measures are also similar to some works, such as perceptual loss using VGG and identity loss using ArcFace, but the author does not cite these and discuss the difference. 

- The dataset used for training and testing is too small, which could not effectively verify the effectiveness of the proposed method.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper address an important probem in human computer interaction: making computers accessible to vision impaired people. The paper address this my collection paired data of text and imu signals. First, the paper address the issues of limited data by training a generative model, to resample/bootstrap more data and then train recognition model on both real and generated data to archive high performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
the paper addresses an important social problem, and accessibility should be focused on all groups. 

The data collected for this paper, the paired data on text and imu is very useful, hope the authors will open-source it. 

paper is well written and the figures are clear and convey the ideas.

Weaknesses:
My main concern is, that it is very unlikely that we get more than we give to the system, the generated samples are a function of real samples. 
I would like to see, a competitive baseline with good data augmentation, and maybe on a low data regime gan generated samples are better than augmentation, but this has to be shown, otherwise, I don't see the value of extra effort to train a generative model to get data augmentation.

Limitations:
I wouldn't say this is a major limitation, but on the scale axis, this problem can be solved by collecting more data. Unlike annotations like explaining an image or video, handwriting signals are more easy to collect on the long term. would be nice if the authors can address this, also please explain the issues with data augmentation.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
YYY5lzE547;"REVIEW 
Summary:
This paper studies the push-relabel algorithm with warm-starts for the max-flow problem. Despite its empirical efficiency, the standard analysis of the algorithm is known to result in a somewhat pessimistic bound of $O(n^2m)$. The current paper considers a situation where the algorithm takes a predicted pseudo-flow $\hat f$ and has shown that the running time can be bounded by $O(n^2\eta)$, where $\eta$ is a prediction error that is at most $\\| \hat f - f^* \\|\_1$ for an optimal flow $f^*$. En route to this result, the authors have also shown that push-relabel with the gap-relabeling heuristic fed with a cut-saturating pre-flow (Algorithm 1) enjoys the same error-dependent bound. Overall, the algorithm converts $\hat f$ into a cut-saturating pseudo-flow $f$ in $O(n^2\eta)$ time (Algorithm 5), updates $f$ in $O(n^2\eta)$ time so that the $t$-side ($s$-side) has no excess (deficit) nodes (Algorithm 2), and fixes excess (deficit) in the $t$-side ($s$-side) with Algorithm 1. The main technical novelty lies in the update step, which swaps excess and deficit nodes between $t$- and $s$-sides. Finally, experiments on image-segmentation datasets confirm the effectiveness of push-relabel with warm-starts, compared with cold-start push-relabel and cold- and warm-start Ford--Fulkerson.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The push-relabel is an important practical algorithm with a large gap between theory and practice. The paper gives an interesting result to fill the gap.
2. Similarly, the analysis that applies to push-relabel with the gap-relabeling heuristic is also a nice result.
3. The algorithm for swapping excess and deficit nodes between $t$- and $s$-sides is an interesting new gadget.

Weaknesses:
1. I agree that analyzing push-relabel is important due to its practicality. On the theoretical side, however, the impact of the $O(n^2\eta)$ bound is somewhat weak since Ford-Fulkerson is already shown to run in $O(m \\| \hat f - f^* \\|\_1)$ time.
2. The paper has little implication about how we should learn $\hat f$. I understand that $\\| \hat f - f^* \\|\_1$ can be used as a surrogate loss that upper bounds $\eta$ in Definition 1. However, since it is mentioned in lines 86--87 that the $\ell_1$-error is not a good measure of the prediction quality, I would like to know whether we can develop a better method for learning $\hat f$ based on the error measure in Definition 1.
3. The paper is somewhat hard to follow as it involves many pseudo-codes, most of which are in the appendix. For me, the proof of Theorem 1, which includes a brief explanation of what each algorithm does, was helpful to overview the entire algorithm with predicted pseudo-flow. It might be better to present a similar technical overview when describing Figure 1.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Authors provide the first theoretical guarantees for the Push-Relabel (PR) algorithm coupled with the gap relabeling heuristic, to address max flow problems, while using an arbitrary (pseudo-flow) initialization such as a predicted flow. The main result relates to proving a worse case complexity of $O(\eta * n^2)$ for their algorithm given a predicted pseudo-flow with error $\eta$ (cf Def 1) on the network with $n$ nodes. Finally, they validate empirically their theoretical analysis while showing the relevance of their algorithm compared to well-known competitive algorithms.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- Overall the paper is well-written.
- (Theorem 2) Authors provide the worse case complexity of their algorithm while using a cut-saturating pre-flow (arguably most intuitive init knowing maintains this structure along iterations)
- (Theorem 1) General case given any predicted pseudo flow.
- Good empirical validation of the theoretical results and comparison with FF algorithms using different warmstart strategy.

Weaknesses:
*edits after authors' rebuttal are in italic*

1. Potentially under-exploited empirical results:
   - a) *[addressed, see suggested improvements in discussions]*  visualizations corresponding to values reported to Table 2 / 5 could have been interesting to really observe the quadratic behaviour.
   - b) *[partly addressed in answer to all reviewers, paper to adjust accordingly]* Lack of analysis w.r.t the ""parameter"" eta, that relates to the error between the previously predicted flow and the new one. Potentially synthetic datasets could have been designed to control eta and compare theoretical results with empirical ones. It seems also possible on real-world dataset to compute a posteriori eta (e.g knowing all optimal flows over a sequence) along iterations to perform this comparison between theory and practice more accurately.
   - c) Even if authors explicitly state that their goal is not to claim SOTA results, it is interesting to have a benchmark to SOTA approaches for reference.

2. Limitations of theoretical results could be further discussed. In theory, FF algorithm has a better worse case complexity than the PR algorithm studied by authors, but the later is faster in practice. This either question the tightness of the bound given by authors, or that the average (not worse case) complexity is simply better for the later than the FF one, which is not theoretically studied by authors.

Limitations:
The authors have made an effort to address the limitations of their work, I encourage them to answer the questions above to ensure that all potential limitations have been covered.

No potential negative societal impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper provides the running-time complexity analysis for warm-start Push-Relabel algorithm for the fundamental max-flow/min-cut problem. In particular, they study learning-augmented version of the Push-Relabel algorithm, where the algorithm can start from a pseudo-flow with error bounded by $\eta$. The main result is that a minimum cut can be found in time $O(\eta\cdot n^2)$, which improves upon the $O(n^2 m)$ runtime of basic Ford Fulkerson.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This provides the first theoretical guarantees for warm-starting Push-Relabel with a predicted flow and demonstrates the benefits of learning-augmented version in improving the running time, where the predicted flow is close to an optimal flow. 

- The theoretical results are further demonstrated by empirical experiments using image segmentation tasks over image sequences datasets, where the predicted flow is given by the result of the previous image sequence. 

- The paper is well-written and well-structured, with clear motivations. The theoretical results deliver important insights about the practical benefits of utilizing learning-based predictions for the fundamental problems on graphs.

Weaknesses:
- The algorithm (Algorithm 2) requires a bound on the error $\eta$ of the prediction, which can be hard to estimate in practice. 
- Even though the Push-Relabel performs much faster than Ford-Fulkerson procedures in image segmentation task (Sec. 4), when just comparing the theoretical bounds, $O(\eta n^2)$ (warm-starting Push-Relabel) and $O(\eta m)$ (warm-starting Ford-Fulkerson), the run-time benefits of warm-starting Push-Relabel cannot be seen compared to that of warm-starting Ford-Fulkerson. Is this due to a loose bound on the warm-starting Push-Relabel?

Limitations:
No ethical limitations

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper propose and analyzes a warm starting scheme for the classical
push-relabel algorithm for max-flow problems. The basic approach is to convert
the warm-starting flow into a cut-saturating flow, at which point a clever
push-relabel scheme is used to ensure that the source-side of the cut has only
excess flow while the sink side of the cut has only flow deficits.  This
immediately yields a min-cut and is easily transformed into a max flow
solution.  The authors compare their approach with warm-starting methods for
the Ford-Folkerson algorithm, which they show is less efficient in practice.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well written and understandable by non-experts.

- The warm-started version of push-relabel has a very nice instance-dependent
    complexity compared standard push-relabel ($O(m n^2)$ to $O(\eta n^2)$),
    allowing for fine-grained complexity estimates.

- Warm-started push-relabel works well in practice and is much faster than
    other warm starting methods for large problem instances.

Weaknesses:
- The error $\eta$ of a pseudo-flow $\hat f$ is hard to estimate without
    running Algorithm 5, meaning the total complexity of computation is not
    known until partway through execution. Moreover, this issue is not addressed
    in the paper.

- It is difficult to compare warm-started push-relabel with the Edmonds-Karp
    selection rule for Ford-Fulkerson because their complexities they depend on
    different aspects of the problem. 

### Detailed Comments

Firstly, I want to say that this paper is significantly outside of my research
area (convex optimization) and so I cannot comment on the novelty of the ideas.
However, I think the ideas in this paper are of significant interest,
particularly for researchers focused on deriving instance-dependent
complexities. As such, I think the paper should probably be accepted.


**Computing $\eta$**:
Reading Section A.3, it seems like computing the value of $\eta$ is a
unaddressed difficulty in the main paper. I don't think it is reasonable to
consider $\eta$ to be input to the algorithm, so the doubling procedure in A.3
is required to estimate $\eta$ when running Algorithm 5. Since the complexity
of estimating $\eta$ with Algorithm 5 is no greater than that of running
Algorithm 5 with $\eta$ as in input, I think it makes sense to write the full
algorithm without knowledge of $\eta$ in appendix. Similarly, some comment
should be made about this issue in the main paper. 


**Edmonds-Karp Selection Rule**: 
You comment that Ford-Fulkerson with the Edmonds-Karp selection rule has
complexity $O(m \|f^* - \hat f\|_1)$. In comparison, warm started push-relabel
has complexity $O(\eta n^2)$. It is difficult to compare these two complexities
since $m \leq n^2$ but $\eta \leq \|f^* - \hat f\|_1$. In practice,
warm-started push-relabel seems to have much better complexity, but is it
possible to draw a rigorous comparison between the two methods? At the very
least, I think this issue should be addressed in the main paper.

Limitations:
Limitations are appropriately addressed in the paper.  The paper topic might be
considered somewhat niche to the majority of the NeurIPS audience. This is not
a weakness of the paper, however.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
nK6OnCpd3n;"REVIEW 
Summary:
The paper proposed a reward generating pipeline leveraging a text-conditioned diffusion model with a text prompt to master RL tasks described by the prompt.
The pipeline compares the difference of the generated image with/without the prompt and the original image to calculate a dense reward, which also makes sense intuitively.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- The authors cleverly use a conditional diffusion model to estimate the alignment level of an image to the text prompt with a hypothesis that  diffusion models capture naturalness and world knowledge.
- The authors expand the proposed method from image-text space to video-text space.
- As it is hard to quantitatively evaluate the performance of benchmarks, the paper conducted user studies to compare benchmarks by human volunteers.

Weaknesses:
- The proposed method requires more hyperparameter-tuning of the noise level and the overall reward function. Thus, more investigation is needed before training is conducted, such as Appendix A.
- The proposed method heavily relies on the capability of the external diffusion model to obtain world knowledge. It is a natural sin due to the complexity of the tasks the paper is solving.
- Another minor weakness is that some details of the user study are not presented, such as the variance of ✔ and ✘ and scores (instead of giving the majority and avg).

Limitations:
See weakness 1. The paper concluded most of its limitation in the main text and Appendix.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces Text-Aware Diffusion for Policy Learning (TADPoLe), a method for reinforcement learning that leverages pretrained text-conditioned diffusion models to compute dense reward signals. This approach allows agents to learn text-aligned behaviors without the need for expert demonstrations or handcrafted reward functions. The authors demonstrate the effectiveness of TADPoLe in various environments, including Humanoid, Dog, and Meta-World, achieving zero-shot policy learning with natural language inputs.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper provides a novel approach to reinforcement learning by using large-scale pretrained generative models to generate reward signals based on natural language descriptions. This method removes the need for manually crafted reward functions, which are often a bottleneck in RL tasks. The approach is well-motivated, leveraging the rich priors encoded in generative models trained on vast datasets. The experimental results show the ability of TADPoLe to learn diverse behaviors across different environments and tasks.

Weaknesses:
1. Presentation Issues: The paper's presentation is problematic. The integration of components such as the 'symlog' of the reward function and the 'noise level range' is not clearly explained. It is difficult to understand the necessity and utility of these components and how much performance improvement depends on them. Additionally, the paper lacks clarity on the source of ground-truth for rendered subsequent images and whether the method requires pre-existing well-rendered videos for each environment for TADPoLe training.

2. Alignment with Motivation: The motivation to leverage text-conditioned diffusion models for reward generation is clear; however, the demand for well-rendered videos for each environment does not fully align with the goal of making reinforcement learning more practical and scalable. Some environments donot have existing well-performed video for the agent, so will this method require for a pre-trained policy to collect the video data?

3. Experimental Insufficiency: The experiments are not comprehensive. There is a need to expand the range of tasks to provide a more thorough evaluation of the method. Additionally, the paper had better include comparisons with the Diffusion-Reward method [1] to highlight the advantages of TADPoLe. 

4. The ablation study is also insufficient. It should further explore the design of the 'symlog' component and the selection of the two weights, as these operations lack detailed justification. This analysis is crucial to understand the contribution of each part to the overall performance and to provide a clearer justification for their inclusion.

5. Overclaim. The writing of the paper is dense and lacks clarity in several key areas. The explanations of the methodology and components are not sufficiently detailed, making it challenging for readers to grasp the full picture of how TADPoLe functions and why certain design choices were made. Improving the clarity and coherence of the writing would significantly enhance the paper's readability and accessibility.

[1] Huang, T., Jiang, G., Ze, Y., & Xu, H. (2023). Diffusion Reward: Learning Rewards via Conditional Video Diffusion. arXiv preprint arXiv:2312.14134.

Limitations:
The author has addressed a limitation. It's also recommended to add the unsolvable questions to the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents Text-Aware Diffusion for Policy Learning (TADPoLe), a framework that leverages pretrained text-conditioned diffusion models to generate dense, zero-shot reward signals for policy learning in reinforcement learning tasks. The approach aims to address the limitations of manually designed reward functions by utilizing large-scale generative models to encode rich priors that guide policies in a natural and text-aligned manner. Experiments demonstrate TADPoLe’s effectiveness in learning policies for novel goals and continuous locomotion behaviors in various environments, including humanoid, dog, and robotic manipulation tasks, without ground-truth rewards or expert demonstrations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
**Strengths:**

- **Innovative Reward Generation:** TADPoLe introduces a novel approach to reward signal generation using pretrained diffusion models, reducing the need for manually crafted reward functions.
- **Zero-Shot Learning:** The framework supports zero-shot policy learning, enabling the agent to learn new tasks and behaviors from natural language descriptions without prior demonstrations.
- **Diverse Applications:** Demonstrates versatility across different environments and tasks, including humanoid and dog locomotion, and robotic manipulation in the Meta-World environment.
- **Human Evaluation:** Qualitative assessments show that the policies learned by TADPoLe are perceived as more natural and aligned with the provided text prompts by human evaluators.

Weaknesses:
**Weaknesses:**

- **Evaluation Metrics:** While the paper provides qualitative evaluations, the reliance on human judgment for assessing the naturalness and alignment of behaviors could introduce subjectivity.
- **Scalability of Text Prompts:** The approach may struggle with very complex or ambiguous text prompts, and the extent to which it can handle highly detailed or context-specific instructions is not fully explored.
- **Comparative Baselines:** The comparisons with other text-to-reward methods are limited, and it would be beneficial to include more diverse baselines to better understand the relative performance of TADPoLe.
- **Computational Overhead:** The approach involves significant computational overhead due to the use of large-scale diffusion models, which could limit its practicality in resource-constrained environments.

Limitations:
Same as the above section.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces Text-Aware Diffusion for Policy Learning (TADPoLe), which uses a large-scale pretrained text-conditioned diffusion model to provide zero-shot reward signals for training agents without expert demonstrations or manually designed reward functions. TADPoLe enables agents to learn behaviors and achieve goals specified by natural language in various tasks, demonstrating its effectiveness.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The idea is straightforward and easy to follow; the authors have adopted a clear approach to present their insights.

2. Using text-conditioned diffusion models to provide rewards is novel.

Weaknesses:
1. Although the text-aware diffusion reward is novel, both text-aware rewards and diffusion rewards have been proposed by prior works. The paper lacks an apple-to-apple comparison, making it difficult to discern the specific advantages of using diffusion models.

2. I find the experiments to be quite limited. For example, the authors only use TD-MPC as the algorithm backbone, which is just one model-based RL algorithm and does not have a significant advantage in visual RL. Additionally, the curve shown in Figure 5 is quite odd—why not place the baseline and proposed method on the same graph and include curves for other prompts? Furthermore, the comparisons in the Metaworld experiments are minimal, making the experimental results less convincing.

3. The absence of real-world experiments makes it hard to assess the paper's contribution to the community.

4. The authors use a diffusion model to generate rewards at each step, but there is no detailed analysis of the computational cost and its impact on speed.

Limitations:
The authors discuss limitations in the paper about how to control the weight of each individual word in the prompt. However, I believe that the primary limitations lie in the efficiency of the diffusion model. This paper has many aspects that require further discussion and improvement.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
egGFHCFPiU;"REVIEW 
Summary:
This work creates a hybrid LLM and classic planning algorithm, by integrating a LLM into the GraphPlan algorithm. The GraphPlan is an algorithm that solves a relaxed planning problem (forward expansion), and then traverses the created graph to find a valid plan (backtracking). Both steps are expensive. In the hybrid approach, a LLM is prompted in the forward expansion to limit the exploration of states deemed irrelevant. In the backtracking phase, the LLM is used to sort actions to explore first. Experiments with corrupted domain files show that LLMs can better handle corruption than the GraphPlan algorithm.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- A very interesting novel idea of a hybrid planning approach with a fundamental classic planning algorithm.
- The paper provides an introduction to an interesting research area of classical planning (e.g., Figure 4).

Weaknesses:
- Multiple missing experiments and discussions severely undermine the results of the paper.
    - It is not clearly motivated why experiments with corrupted pddl domain files are interesting. This was introduced quite suddenly in the *results section* (lines 261-262) without enough details and without providing motivation.
    - The paper is missing important discussion and experiments about the trade-off between the hybrid approach and the classic GP algorithm. Experiments with valid pddl domain files are not included, which could have alleviate it.  
    - The effect of hyperparameters on the results, such as the number of iterations (N) in Algorithm 1, is not discussed.
    - The failure of LLMs4PLAN-GPT3.5 compared to the phenomenal success of LLMs4Plan-GPT4 is somewhat unexpected and undermines the results of the paper.
- Multiple details are missing regarding the experimental setups. (see questions below)
- The paper's writing needs to be improved. (see suggestions below)

Limitations:
The authors did not discuss limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates how large language models (LLMs) can be integrated into established planning frameworks, specifically graph-based planning. The authors propose a novel framework called LLMs4Plan, which incorporates LLMs at two critical stages of the planning process: action selection during graph expansion and candidate action set generation during backtracking. The framework is tested across various planning domains, demonstrating improved efficiency and effectiveness in planning tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper's approach of embedding LLMs into graph-based planning is innovative and contributes to the field of automated planning.
2. The technical implementation of LLMs4Plan is well-detailed, with descriptions of how LLMs are utilized in action selection and candidate set generation.
3. The effectiveness of the proposed framework is empirically validated across ten planning domains, showcasing its practical applicability.

Weaknesses:
1. The proposed integration of LLMs into planning frameworks in LLMs4Plan may be complex and difficult to scale.
2. Comparisons with more recent LLM integrated planning baselines is limited.

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
There have been debates about the fundamental planning abilities of LLMs in planning tasks. To achieve more reliable performance, several recent works have embedded an LLM into a search framework (e.g., MCTS, BFS) and viewed LLMs as heuristics. Along this line, this work take a closer look at the roles LLMs can play in Planning Graph. It considers two tasks for LLMs: pruning actions and sorting actions (as heuristics).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written, with precise language and formalism.
- The experiment is conducted on over 10 domains, making it quite comprehensive.

Weaknesses:
1. My biggest concern with this work is that it restricts the use of LLMs to specific roles within a classical planning algorithm. There are many other roles LLMs can play in planning. For instance, see the recent LLM-modulo framework below. Instead of just filtering and ranking actions, LLMs have also been used to evaluate state values or rank plans (i.e., action sequences rather than individual actions).

    - Kambhampati, Subbarao, et al. ""Position: LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks."" ICML 2024

2. The evaluation based on the number of nodes explored is partial. We should not ignore the time cost (e.g., latency of calling LLMs) + financial cost of using commercial LLMs. It could be very likely that, although LLM+Graph Planning expands fewer nodes, it may take a longer wall-clock time to give the final outputs. I understand that the evaluation could be tricky and it remains an open question for a while. However, the authors should at least make an attempt to address this.

3. In the abstract, this statement is inaccurate: “works have been proposed to investigate the planning effectiveness of LLMs, without considering any utilization of off-the-shelf planning techniques in LLMs.” There have been quite some paper embedding LLMs in off-the-shelf planning algos

    - Zhao, Zirui, Wee Sun Lee, and David Hsu. ""Large language models as commonsense knowledge for large-scale task planning."" NeurIPS 2023.
    - Yao, Shunyu, et al. ""Tree of thoughts: Deliberate problem solving with large language models."" NeurIPS 2023.


4. While the corrupted domain model experiment looks interesting, it is unclear what messages it tries to convey. Specifically, why would one run the algo on top of a corrupted domain model when there exists approaches that can leverage LLMs to help complete the domain model before starting the search?

    - Guan, Lin, et al. ""Leveraging pre-trained large language models to construct and utilize world models for model-based task planning."" NeurIPS 2023
    - Wong, Lionel, et al. ""Learning adaptive planning representations with natural language guidance."" ICLR 2024.


5. The step of LLM-based action pruning can make the search incomplete, since an LLM may keep ignoring the required action(s) -- in other word, there is no guarantee that the LLM can produce a goal-reaching plan. I notice the authors mention this at a later section (which should be moved to earlier part) that including pruning probabilities could address the problem. I don’t fully agree with this. Can the authors give more detail on how pruning probabilities could guarantee completeness?

6. In the prompt (fig. 3), only the proposition set at the current state is provided. Did the authors consider including the running history of actions (i.e., the partial plan)? Would this affect the overall performance?

7. Line 109: typo in “Algorithm ??”

8. Several works (mentioned earlier) already show that LLMs can be useful heuristics. Can the authors elaborate on the new insights this work provides?

-----
Overall, this study provides a thorough evaluation of LLMs within the Planning Graph algorithm. I appreciate the comprehensiveness of the experiments. However, I also have concerns over the scope of this study (i.e., restricting itself to a limited set of roles). I need to discuss with other reviewers and the authors before finalizing my recommendation.

Limitations:
See the Weakness section.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to investigate integrating large language models (LLMs) into classical planning frameworks to enhance the planning effectiveness. The authors proposed a novel method named LLMs4Plan which integrates LLMs into action selection and mutual constraints solving within the graph-based planning framework. Evaluated across ten classic planning problems, this approach demonstrates improved success rates and reduced computational complexity compared to traditional methods. The study concludes that while LLMs alone are insufficient for planning, their integration into classical frameworks significantly boosts performance,.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper investigates an intriguing topic: the performance of LLMs in classical planning problems. While the impressive performance of LLMs in natural language processing and coding tasks is well-investigated, their efficacy in planning tasks remains largely unexplored. Understanding whether LLMs can replace classical planning algorithms is a significant and meaningful research question.
2. The paper conducts extensive experiments on ten classical planning problems, which enhances the credibility of its findings and conclusions. This comprehensive evaluation demonstrates the robustness of the proposed approach.
3. The paper reveals that LLMs still cannot surpass classical planning algorithms, thereby highlighting a valuable direction for future research. This insight encourages further investigation into how LLMs can be effectively integrated with traditional planning methods.

Weaknesses:
1. Although the authors point out that LLMs cannot outperform classical planning algorithms on their own and need to be integrated with classical methods to perform well, the paper lacks detailed insights on this integration. For example, specific strategies for integrating LLMs with the classic planning algorithms and the roles where LLMs excel within planning problems are not thoroughly discussed. The designed ""expandGraph"" and ""sortActions"" may not be the best practice manner. Future research directions to enhance the planning capabilities of LLMs should be more explicitly outlined.
2. The experiments are conducted in simulated planning domains, and the paper does not provide real-world applications or case studies to validate the practical utility of the approach. Including experimental results from more realistic scenarios would strengthen the paper.
3. While the method is effective for graph-based planning, its applicability to other planning frameworks or domains is not thoroughly investigated. A broader analysis could reveal the versatility of the proposed approach.
4. Typos: Algorithm ?? in Line 109.

Limitations:
See the Weaknesses part

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
YZoGmJrOS9;"REVIEW 
Summary:
This work presents an analysis of in-context learning (ICL) for a variety of hybrid architectures (composed of different blocks from preexisting large language model architectures) on different regression tasks. The experiments are built on top of a couple of prior works [1, 2] that also explored ICL in similar contexts. This paper highlights that several prior results can be reproduced, and for novel hybrid architectures which are the main focus of this work – most of them converge to optimal solutions while some others can escape suboptimal solutions or even fail to converge in the first place. The authors also propose a new metric “ICL regression score” to evaluate ICL performance in comparison to a known baseline. The modularized code for this work is publicly available for the broader scientific community.


[1] Garg S, Tsipras D, Liang PS, Valiant G. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems. 2022.
[2] Park J, Park J, Xiong Z, Lee N, Cho J, Oymak S, Lee K, Papailiopoulos D. Can mamba learn how to learn? a comparative study on in-context learning tasks. arXiv preprint arXiv:2402.04248. 2024.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- The open source codebase with simple abstractions and interfaces to facilitate reproducibility, extensions, and modifications are a welcome contribution.
- The intuitive explanation behind the ICL regression score values in Figure 2(a) are well-appreciated and helpful to follow along the results.
- The authors evaluate multiple architectures and tasks and clearly outline what components they are using from prior works to build on top of.

Weaknesses:
- The paper is not very well-motivated. Why are hybrid architectures (especially the two that are focused on) important to study? What intuitions or profound reasons drive the authors to make the experimental design choices that they did? 
- Additionally, use cases for ICL itself are not well-motivated. Are there any practical use cases that warrant such extensive evaluation? The writing is not easy to understand for a reader not very up-to-date with the ICL literature. 
- The technical novelty of the work is limited.
- The results are presented in a manner where the performance metrics are reported for the 12 architectures and 5 tasks but it is not very clear what the reader or the scientific community working on ICL should take away from the results. Are there patterns regarding why certain hybrid architecture + task combinations make ICL shine compared to the baselines and why some others do not? A lot of the interpretation of such results is left to the reader to figure out. A lack of a deeper understanding and intuition about the reasons behind the results makes it hard to see solid/impactful takeaways that others could build on top of. 
- The authors mention and describe a 6th task Vector MQAR but do not report or discuss any of its results in detail in the main text of the paper. One figure is present in the Appendix but it is not explained and it is too hard to read the text in the figure.
- Some typos:
     - Line 161: Mention the word “**Figure**” before 2a.
     - Line 164: “**Figure 2b**” instead of “Table 2b”
     - Line 185: Park et al. [14] **show** that ..
     - Line 202: “Sparse Linear ~~on~~ adopts a suboptimal..”
- References to result tables (Table 3 for lines 213, 230)  and model descriptions (Figure 1a for lines 194, 204, 201, 16 etc.) could further enhance readability and the user’s understanding.

Limitations:
In terms of negative societal impacts, could the potential misuse of natural language tasks via the hybrid architectures presented be a legitimate concern? It is certainly out of scope of this work for evaluation but could be listed as part of the broader impacts section of the checklist.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors explore a number of different attention-based architectures along with Mamba on a series of in-context regression tasks. The architectures vary in their choice of normalization, positional encodings, activations, and hybridization with Mamba. They discover some varying capacities for the different ICL tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper appears to be closely related to [Park et al.](https://arxiv.org/abs/2402.04248), and I'm uncertain what additional scientific value it adds. As such, it's originality and significance appear to be limited.

Weaknesses:
As mentioned above, the present work seems to be closely related to [Park et al.](https://arxiv.org/abs/2402.04248), with little added insight. While an attempt has been made to explore minor architectural aspects like the choice of normalization, positional encodings, and activations, for the most part the changes seem to matter little (according to Table 3). I'm unsure what the key take-away is. Is there a particular architectural configuration that works best? What concrete practices can a user apply to improve their models' performance? The authors appear to have an ambition towards answering questions like these, but do not ultimately resolve them.

The results are sometimes difficult to interpret. At times, this is simply because the plots are unreadable, with text that is too small. I'm unsure how to interpret the in-context regression score. It looks like it's often <1 across models. Does this mean they all fail to outperform the baseline? Is this score comparable across different tasks?

A main objective of the paper is to compute numerics comparing models, but only one training run was executed for each experiment. If the compute budget is very limited, a more valuable approach may be to consider a smaller subset of models (e.g. keeping either GPT-2 or Llama, but not both) and simpler task parameterizations. Doing so will enable you to sweep across many more settings and increase your experiment replications, generating more convincing numerics. 

Additional minor formatting comments:
- Line 25: consider compressing citations (e.g. with sort&compress)
- Table 1: third hline from the top intersects with text
- Consider plotting figures with point-markers for each data point, to clarify where exactly your data points fall
- For related models that vary by parameter (e.g. training iteraitons), consider using the same color but with different shadings / style
- Figure 15: text is unreadable
- Figure text overall is small and hard to read

Limitations:
The authors adequately state the limitations of their approach.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors build on the ICL work of [1] and [2], wherein networks are ""trained to in-context learn"" several tasks of varying complexity (e.g., Linear Regression, Sparse Linear Regression, Vector MQAR, etc.).  Carrying on from [2], the main contribution of the presented work is the combination of various permutations of the attention/SSM blocks from GPT-2, LLama-2, and Mamba models.  Such permutations include swapping the Layer Norm in GPT-2 attention blocks with RMSNorm, the LLama-2 SwigGLU with a Mamba Mixer, and so on; in total, 9 different hybrid combinations are considered.  For each task and specific hybrid model, the model is trained over task samples for 500k steps, then evaluated on ICL performance on that task.  In addition to reporting squared error per task, the authors also propose a new metric, called the ""ICL regression score.""  The results of specific model-task pairs are plotted and several trends are discussed.

[1] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583–30598, 2022.
[2] Jongho Park, Jaeseung Park, Zheyang Xiong, Nayoung Lee, Jaewoong Cho, Samet Oymak, Kangwook Lee, and Dimitris Papailiopoulos. Can mamba learn how to learn? a comparative study on in-context learning tasks. arXiv preprint arXiv:2402.04248, 2024.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
**Originality** - while the authors largely adapt the ""trained to in-context learn"" framework from other works, they consider new model configurations.  This can be useful to determine what permutations of model components leads to failure modes in ICL.

**Quality** - In considering 9 different model configurations for 8 different tasks, the author thoroughly consider a large number (72) of different LLM building blocks and their ICL capabilities across tasks of varying complexities.

**Significance** - The framework of [1] has grown as an interesting alternative to standard ICL, as a quick means to assess the ICL success and failure modes of different models.  Thus, the presented work can aid in flushing out this information over new LLMs and previously proposed tasks.

[1] Shivam Garg, Dimitris Tsipras, Percy S Liang, and Gregory Valiant. What can transformers learn in-context? a case study of simple function classes. Advances in Neural Information Processing Systems, 35:30583–30598, 2022.

Weaknesses:
### Clarity
There is significant room to improve the clarity of the presented work.  Currently, several important details are missing (discussed below), key concepts are not fully explained, and the current paper could benefit from an editorial pass (it is currently difficult to read).  It is also important to note that, while the authors presented the results of model + task pairs and detailed where models failed, no possible explanations and follow up ablation studies were conducted; the question of ""why"" remains for all the presented results.  E.g.,
> Specific hybrid architectures can hesitate to learn/converge for certain function classes.

Do the authors have intuition or an explanation which can be explored to explain this?  Arguably, this is one of the most important contributions to be made in a large empirical review like the presented paper, i.e., to make sense of the experiments to gain a greater intuition for why an LLM behaves in a certain way.

Wrt important missing information:
- ""We replicate the function classes Linear Regression, Sparse Linear Regression, 2-Layer MLP Regression, and Decision Tree Regression from Garg et al. [6] as they present a wide range of ""difficulty"" for sequence models. In addition, to capture the existence of some ICL ability, we also regress onto the two function classes examined in Park et al. [14]: parity function with induced sparsity (Sparse Parity) and parallel associative recall (Vector MQAR)."" <- - How training instances are produced per task? How many test samples are produced per task?  If this follows Garg et al., then each model is trained *from scratch* on 40 samples per task. Can you please clarify and state these in the main text?
- It is not clear	what the author's mean by ""zero estimator.""  Is this the zero shot prediction? Correspondingly, it is not clear exactly what the presented ICL regression score represents.
- ""To determine task-specific ICL ability, our sequence models regress onto the functions shown above [14].""  <- It would help to clearly state the paper trains the models ""from scratch"" to in-context learn, as in previous works.

Limitations:
The did well to state limitations of the presented evaluation.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents a codebase for benchmarking the in-context learning ability of language models, especially for hybrid models. In addition, several empirical results are presented to show that some model architectures fail entirely or have suboptimal performance on specific in-context learning tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The codebase for studying in-context learning ability could be useful to understand capabilities and limitations of hybrid models, which will accelerate research in this area.
* It is interesting to find that even a small change in architecture (e.g., adding RMS to GPT-2) will lead to noticeable differences on some tasks (e.g., sparse linear). It would be interesting to investigate the root reason behind that.

Weaknesses:
* I feel that it is hard to assess the contribution of this work. It seems that this work's main contribution is the implementation of the in-context learning ability benchmark codebase. While such a codebase is important and useful, I did not find what technical challenges the codebase is trying to address and the effectiveness of the codebase.
* Another contribution is the empirical findings of the relationship between architectures and per-task performance on in-context learning. However, I found the empirical results are not systematic and hard to interpret. I am unsure how these findings motivate future architecture design.

Limitations:
see above

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
wQpNG9JnPK;"REVIEW 
Summary:
This paper addresses the problem of spurious correlations caused by environments from where data are collected.
The proposed method applies a mask to input data to separate spurious and semantic features.
The masked input data are fed into a local model specialized to each environment.
Each local model is trained to induce neural collapse for OOD generalization.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- S1: Making use of neural collapse for OOD generalization is interesting.

Weaknesses:
- W1: Comparison with not only OOD generalization methods but also spurious correlation (sometimes called bias or shortcut) methods is necessary. Methods that can automatically detect and split spurious and semantic features have been developed [a-e].
- W2: Types of spurious features that the proposed method can handle need to be clarified. Can the proposed method handle spurious features in superposition, e.g., objects and textures?
- W3: The rationale behind the proposed method needs to be clarified. For instance, it is unclear why the method adds the noise to the mask when learning it.
- W4: Deeper analyses in the experiments would make the paper more interesting. For example,      
  - Whether the neural collapse is achieved by the proposed method should be confirmed in the experiment.    
  - Visualizing learned masks would produce more valuable insights.
- W5: What is described in the introduction and what is done in the proposed method seems to be different. Although L42 states that `we propose to compute the Frobenius norm (F-norm) of the difference between the feature prototypes and the standard simplex ETF`, the F-norm does not appear in the proposed method.
- W6: Writing and formatting can be improved. There are many inconsistent spellings. For example,    
  - Is ""variable features"" in L153 the same as spurious features?  
  - The meaning of ""interaction"" in L189, 192, and so on is unclear. Maybe ""training?""  
  - Such inconsistent spellings occur from Section 4.

[a] Tiwari, Rishabh, and Pradeep Shenoy. ""Overcoming simplicity bias in deep networks using a feature sieve."" ICML2023.    
[b] Bahng, Hyojin, et al. ""Learning de-biased representations with biased representations."" ICML2020.    
[c] Yang, Wanqian, et al. ""Chroma-vae: Mitigating shortcut learning with generative classifiers."" NeurIPS2022.    
[d] Liu, Evan Z., et al. ""Just train twice: Improving group robustness without training group information."" ICML2021.    
[e] Nam, Junhyun, et al. ""Learning from failure: De-biasing classifier from biased classifier."" NeurIPS2020.

Limitations:
Discussed in Section 6.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper leverages the neural collapse inspired ETF behavior to simulate different environments in datasets, and uses it for OOD classification.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper uses a phenomenon that's apparent in the standard setting, for a task that varies from the standard setting. It uses intuitive notions to tackle the task of OOD classification. The paper experiments are generally convincing.

Weaknesses:
The paper seems generally consistent and well merited. The experiments are a bit lacking, but are convincing.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The spurious correlation between image background features and their labels is a significant research problem, and the existing research suffers from the issue of difficult decoupling. In this paper, we propose a new approach to solve the spurious association problem by alternately performing environment segmentation and learning semantic masks from the perspective of neural collapse. Extensive experiments are conducted on four datasets and the results show that the proposed method significantly improves the out-of-distribution performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper explores an important and widespread problem in real-world applications with solid and extensive experiments. The writing is clear and the narrative is easy to follow, facilitating an understanding of the spurious correlations problem. The use of neural collapse is particularly innovative.

Weaknesses:
W1:  In lines 48-50, it is mentioned that IRM-based methods learn similar representations from different environments, indicating a lack of proper alignment. Could you provide a corresponding experiment to demonstrate this phenomenon?

W2: In Figure 3, the explanation of the middle module that uses logits to judge the environment is unclear. Could you please clarify the structure of the local models, the number of local models used, and the specific meaning of the logit values?

W3: Could you explain the differences between masks based on pixel-level and feature-level approaches? If using feature-level masks, what is the impact of different network-layer features on model performance?

This work addresses an important and interesting question by introducing neural collapse from an invariant perspective, which I believe can provide valuable insights to the community. However, my main concern is that the same mask is used to learn both invariant and variable feature information. What are the advantages of the mask learning mechanism proposed in this paper compared to HRM's [1] mask mechanism?

[1] Heterogeneous Risk Minimization

Limitations:
Yes, the authors have adequately described the limitations in their submission.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
XKrSB5a79F;"REVIEW 
Summary:
In this paper, the authors give improved mixing times for a Markov chain whose goal is to approximately sample from distributions whose densities are proportional to $\exp(-f(x))$, where $f$ is $L$-Lipschitz and convex, restricted to a convex set defined by the intersection of $n$ hyperplanes that lies in a Euclidean ball of radius $R$. The Markov chain itself is an approximate variant of the Dikin walk that tolerates using spectral approximations to the Hessian instead of the exact Hessian for drawing the next sample.

The main result is as follows. Given a subroutine to $1/d$-spectrally approximate the Hessian of the barrier that runs in time $C_g$ and given that the barrier itself is $\nu$-self concordant, the authors give a Markov chain whose iteration complexity to converge to a $\delta$-TV-approximate distribution to the target is $\widetilde{O}(\nu d + d L^2 R^2)\log(1/\delta)$. Each step has runtime $C_g + d^{\omega}$. 

This general framework recovers some of the best known mixing time bounds for special cases of the studied problem, including sampling from the uniform distribution over a convex body. Moreover, for the setting considered, as far as I can tell, this is the first guarantee whose iteration complexity does not depend on the number of constraints defining the polytope (this seems to get replaced by the self-concordance parameter of the barrier, which can be made to be $\widetilde{O}(d)$ using more sophisticated barriers than the log-barrier).

The ideas also transfer to giving mixing times for the problem of sampling uniformly from the constraint set of a covering SDP. Again, a key part of the contribution is that the walk can tolerate using spectral approximations to the Hessian of the barrier (in this case, the log det barrier) instead of requiring it exactly. 

The main technical insight is that there are a few properties that one needs the barrier to satisfy in order to give the $O(\nu d)$ dependence in the mixing time. They consist of a symmetry assumption, a bounded local norm condition, and that the barrier is convex under regularizing it with an identity matrix (see page 6). Under these assumptions, the authors prove that their variant of the Dikin walk satisfies the promised mixing time (the argument is executed in Appendices B, C, D).

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The problem addressed in this work is an important one to advance our understanding of the geometry of polytopes. The conditions under which mixing times can be obtained are pretty general, and as the authors show, only depend on certain natural properties of the barrier function that is being used. The results also imply significant quantitative improvements over prior work (in particular the result about uniformly sampling from polytopes with an iteration complexity that is independent of the number of constraints when $n$ is much larger than $d$).

Weaknesses:
There are a few questions I have (see below).

Besides that, a very minor comment -- in Section 2.3, the Lewis weight optimization program as written is not actually convex. But, there is a convex formulation for all $p > 0$ given in [LS19].

Limitations:
Yes

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a sampling framework from a convex body with a barrier.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The main strength of the paper lies at the novelty and the significance of the results, especially that of Theorem 1.1.

Weaknesses:
See questions below.

Limitations:
None.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper gives a Markov chain Monte Carlo algorithm for estimating the probability of a high dimensional polytope under a log-concave probability distribution. The algorithm improves the mixing time of previous results, while maintaining the best known per-iteration cost. More specifically, in conjunction with a known self-concordant barrier function, they use a cheap spectral approximation of the Hessian matrix required in an iteration, and they modify the Markov chain to converge more quickly despite the approximation.  A second result applies similar ideas to spectrahedrons, and there the improvement over past results is both the asymptotic number of iterations and the asymptotic computational complexity of each iteration.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This is an important and fundamental problem that has been researched extensively recently, and the result improves upon the best known algorithms.

Weaknesses:
The improvement is somewhat incremental, but nonetheless significant.

Limitations:
None.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

";1
KAAUvi4kpb;"REVIEW 
Summary:
This paper aims to reduce the dimensionality of brain signals using linear layers to determine the minimal dimension required to preserve most of the reconstruction quality. The study involves experiments with three existing methods: two for brain-to-image reconstruction and one for brain-to-language reconstruction.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper is easy to follow.
- The paper aims to explore the extent of information necessary for brain decoding.

Weaknesses:
1. The paper suggests that better reconstruction may be achieved with less brain signal input, yet all brain signals are utilized by the reconstruction models. While compressing signals to lower dimensions is possible, it does not guarantee that fewer signals can be used as input.
2. Only diffusion-based models are employed for decoding images. The relatively favorable results may stem from well-trained diffusion models capable of operating with minimal semantics. The discrepancy in bottleneck dimensions between image and text decoding (50 vs. 1000) could support this. In reality, diffusion models exhibit significant randomness in image generation, enabling the selection of images resembling ground truth stimuli from various generation runs. Consequently, drawing general conclusions based on the use of diffusion-based models may be challenging.
3. The performance of brain-to-image generation remains unsatisfactory. While subject1 shows promising results in some instances, the images generated using whole brain signals lack the structure and detail seen in the stimulus images. Furthermore, in additional poor cases not illustrated in this paper, the decoding results may be even more unsatisfactory. Therefore, achieving good image decoding results with fewer signals may be challenging.
4. The number of samples for different subjects is limited. Literature suggests significant performance variations in image generation among subjects, with some individuals exhibiting poor reconstruction outcomes.
5. The paper asserts that the method can be adapted to various neural recording modalities; however, differences in temporal and spatial resolution as well as signal characteristics among recorded signals pose uncertainties about BrainBits' adaptability. Notably, no experiments support this claim.
6. The second paragraph in the introduction appears somewhat contradictory.  It mentions that higher quality reconstruction may require same or less signal from the brain, while also acknowledging the scarcity of open neuroscience datasets, especially those of sufficient scale to support this type of research.
7. The brain regions depicted in Figure 5 are not clearly labeled, making it difficult for readers to fully understand.
8. The effective dimensionality in Figure 4 lacks explicit clarification.
9. Text occlusion is apparent in both Figure 3 and Figure 11, affecting the clarity.

Limitations:
There is no potential negative societal impact.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce BrainBits, an information-bottleneck pipeline that measures reconstruction performances from brain signals (fMRI datasets) as a function of bottleneck size by (linearly) projecting the data into a lower dimensional space of controlled dimensionality. The rationale is to disentangle the contributions to improved reconstruction quality seen in recent  works into (1) improvements in actual decoding (better use of neural information - the actual goal of the decoding techniques) and (2) general improvements in generative models (more powerful architectures with better priors). Indeed the authors reveal that modern improvements are cause by the latter, with recent models making very little use of neural signals. Furthermore, the BrainBits pipeline enables inspection of which brain areas are mostly relied upon. The technique has broad applicability to both vision and language decoding and different signal modalities (however, only fMRI is reported).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
*a. Originality:* The work provides a novel method to rigorously characterize decoding performances which surpasses previous metrics on critical aspect and is resilient to simply scaling decoder complexity.
*b. Quality:* The work quality is generally high, with enough experimental result to support the author's claim.
*c. Clarity:* The work is well written and properly organized.
*d. Significance:* The results presented are of high significance for the brain-decoding community. Recent years have witnessed a wealth of novel contributions, each time showcasing evermore detailed brain reconstructions, implying significant advancements in our ability to decode neural signals. This work offers a much needed warning that we need to guard against misleading metric improvements granted by more powerful generative models and provides the tools to do so. The proposed pipeline is flexible and easy to build upon, offering a valuable contribution to the community at large.

Weaknesses:
- Figure 3c is hardly readable. Why are the axes y-scale range in [0, 1] (which then requires smaller inset to actually inspect the data)? Can't the range by set according to data dynamic range?
- Regarding the analysis on brain regions (Figure 5). The authors claim that the BrainDiffuser model ""*As the bottleneck size goes up models exploit those original areas but do not meaningfully expand to new areas*"". By looking at the (small) image, however, bottleneck size 50 seems to have far fewer ""silent voxels"" (dark purple) than bottleneck 1 or 5 for example. This appears to hold also for other examples presented in Figure 11 in the Appendix (Note that Figure 11 has cut-out subplot titles that are illegible). Why do authors claim that the measured expansion is not ""meaningful""? What would a meaningful expansion look like?

Limitations:
The authors have adequately addressed the limitations of their study by presenting a dedicated section (6) with extensive discussion.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a method called BrainBits which aims at answering if the progress of the fMRI-to-Image/Text field of research comes from a better signal extraction from the brain or from other sources such as having better generative models or exploiting bad metrics. Their method introduces a bottleneck between the fmri data and various fMRI-to-Image/Text methods. Their method is thus directly applicable to basically every reconstruction method, and allows to change the dimensionality of the bottleneck and see whether one can obtain a substantial percentage of the original performance even with the bottleneck.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The questions asked by the authors is very important to the field. Much progress has been made recently, but the causes of that progress remain unclear. Everyone would love the cause to be better brain signal extraction, and thus better understanding of the brain activity by the model. By providing a way to establish whether it is the case or not, the authors tackle a crucial issue. It is a known fact that the metrics used in the field lack a way of knowing how the brain data has been exploited. 

- The results obtained by the authors are surprising: in most cases, even with a narrow bottleneck, we can obtain a substantial percentage of the performance of the original model. 

- The paper is well written and easy to follow. The experiments are clear.

Weaknesses:
- The main weakness is that the bottleneck introduced in the paper does not definitely answer how much of the brain signal has been exploited in the process. Indeed, the MLP projecting the fMRI data to the bottleneck actually learns to identify the most important features within the brain data, in order to obtain the best reconstruction. Much like a VAE, it learns to compress information as efficiently as possible in order to have the best performance possible. Thus, the compression ratio, even if it is generally very high in the authors' results, is also a result of the best effort of the MLP to identify the best features. This crucial point kind of defeats the point of the paper: isolating how much of the brain signal was extracted and used. However, given the size of the compression ratio (300 in the case of BrainDiffusers for instance), I still believe that the authors rightfully identified that not all of the brain signal is used, and that their research is on the right track. However this point could be further improved and disentangled.

Limitations:
I would add the limitation identified in the weakness part, if it still stands after the rebuttal.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a method called BrainBits that aims to assess the extent to which generative image reconstruction based on fMRI data is based on the neural data itself, versus some spurious contribution of the reconstruction model itself (e.g., a stronger prior over natural images, or overfitting to the distribution of images used in benchmarks). The method involves introducing a bottleneck of varying size, and assessing how reconstruction performances varies based on the size of the bottleneck. They apply their method to two image reconstruction tasks and one language reconstruction task. They find that performance plateaus at a surprisingly small bottleneck size, and use this finding to argue that neural stimulus reconstruction approaches are only using a fraction of the information available in the neural data, such that we should worry that recent improvements in reconstruction are due to contributions from models, rather than more effective extraction of information from the brain.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The motivation of the paper is timely, sound, and convincing: we need to interrogate the possible sources of improved stimulus reconstruction performance, and we need ways to assess the contribution of the model versus the neural data itself. The authors argue this point clearly, and I could imagine this paper playing a useful role in raising awareness of this problem in the field, and making a first stab at addressing this issue. 

On a methods level, the paper appears to be sound: including the random performance and reconstruction ceiling provides helpful context, and the analyses linking the bottleneck activations to brain topography and decodable features were illuminating. The figures were well-chosen and clearly presented. The analyses support the case that reconstruction performance plateaus at a small number of dimensions (although I think the authors draw inferences from this that aren't warranted; see weaknesses section).

Weaknesses:
While I found the motivation for the paper to be convincing, and I believe the authors effectively make the case that a relatively small number of dimensions is sufficient to achieve maximum reconstruction performance, I think the authors' argument ignores an important piece of the puzzle: what is the effective dimensionality of the actual neural activations? For the sake of argument, suppose that the dimensionality of the neural responses (over the space of stimuli sampled) is 50: then, the fact that reconstruction plateaus with a bottleneck of size 50 is not due to the method using a small proportion of the underlying information available in the neural signal, but rather due to the fact that the neural activity patterns are themselves low dimensional (e.g., due to correlations in the underlying neuronal firing patterns, or the fact that the BOLD signal in each voxel reflects the aggregate activity of many neurons). Without addressing this issue, I don't think the authors' conclusions follow from their results. 

The writing style sometimes borders on overly informal (""Although, small bottlenecks are perhaps not that
interesting given that the goal is to explain more of the brain""), though I believe this is easily fixed with further edits.

Limitations:
There are no negative societal impacts that I can think of, and the limitations section provides helpful context regarding the practical application of their method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
OwguhIAh8R;"REVIEW 
Summary:
This paper studies heterogeneous graph label distribution learning with the aim of predicting label distributions of unlabeled nodes in a heterogeneous graph. This paper elaborates the challenges for generalizing LDL into networked data, and proposes an LDL algorithm HGDL to overcome the challenges. Besides, this paper derives the PAC-Bayes error bound for HGDL and conducts experiments to show the superiority of the proposal.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper studies a new problem, i.e., label distribution learning in heterogeneous graphs. Besides, this paper proposes an end-to-end HGDL learning approach to jointly learn the optimal meta-path graph topology and node representation. The effectiveness of the proposed method is studied theoretically and empirically.

Weaknesses:
The contribution of this paper is unclear. This paper attempts to combine heterogeneous graph learning with label distribution learning. However, the paper addresses challenges about the topology, heterogeneity, and inconsistency in terms of instances, and pays little attention to the challenges of learning label distributions of instances, i.e., the proposed challenges do not arise from label distributions. Therefore, the contributions for LDL are not clear.

Limitations:
I don't believe that the paper has a potential negative social impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the problem of heterogeneous graph label distribution learning. To deal with it, this paper proposes an HGDL method that optimizes meta-path graph topology and aligns it with nodal features for consistent message-passing, backed by theoretical support. Experimental results on five datasets demonstrate the effectiveness of the proposed model.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
S1. This paper is the first to investigate the LDL problem in heterogeneous graphs, which seems interesting.

S2. HGDL effectively combines meta-path aggregation and transformer-based methods to ensure consistent node label distribution learning, validated by both theoretical analysis and empirical studies.

S3. The contributions are clearly written, with codes and datasets provided for reproducibility and practical utility.

Weaknesses:
W1. The intuitions behind the techniques lack explanation. More details can be found in Q1 and Q2.

W2. The approach section is somewhat difficult to follow and could benefit from improved presentation.

W3. It would be beneficial to include baselines for the LDL problem, such as GLDL [1]. While these methods are not specifically designed for heterogeneous graphs, it is still important to treat heterogeneous graphs as homogeneous ones and apply these methods for comparison. This will provide a more comprehensive evaluation of the proposed approach.

W4. This paper compares only three baselines, which are relatively dated for heterogeneous graph learning. For example, GCN was published in 2017, HAN in 2019, and SeHGNN in 2023. This makes it challenging to convincingly demonstrate the advantages of the proposed HGDL. Thus, it is essential to include more recent baselines, such as HINormer [2].

References:

[1] Y. Jin, R. Gao, Y. He, and X. Zhu, “Gldl: Graph label distribution learning,” in Proceedings of the 38th Annual AAAI Conference on Artificial Intelligence, 2024.

[2] Qiheng Mao, Zemin Liu, Chenghao Liu, and Jianling Sun. ""Hinormer: Representation learning on heterogeneous information networks with graph transformer"", in Proceedings of the ACM Web Conference, 2023.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel approach to Label Distribution Learning (LDL) specifically tailored for heterogeneous graphs, addressing the inherent complexities and challenges associated with this domain. By highlighting the necessity of LDL in heterogeneous settings and outlining the unique challenges involved, the paper lays a foundation for advancing research in this emerging field.
The proposed method is underpinned by a robust theoretical framework, providing a coherent rationale for its design and implementation. 
Experimental validation across five distinct datasets and evaluation using six metrics demonstrate the method's effectiveness over established baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.This paper is the first to identify the necessity of Label Distribution Learning (LDL) on heterogeneous graphs, addressing the unique challenges and complexities associated with this task. 
2.The proposed method is well-grounded in theory, providing a solid foundation for its design and implementation. The authors present a clear and thorough theoretical framework that supports the efficacy and rationale behind their approach, enhancing the credibility and robustness of the method.
3.The effectiveness of the proposed method is demonstrated through extensive experiments on five diverse datasets and six different metrics. The results consistently show that the method outperforms the baselines, indicating its superiority and practical applicability across various scenarios and evaluation criteria.

Weaknesses:
1.The paper categorizes current LDL methods into three distinct types in the related work section. However, the experimental evaluation only includes three models (GCN, HAN, and SeHGNN) with the KL-divergence loss function as baselines. This limited selection raises questions about whether the experiments sufficiently demonstrate the proposed method's superiority over the broader range of existing LDL methods.
2.The study employs γ as a hyperparameter in the proposed model but does not provide any analysis of how variations in this parameter affect the model's performance. A thorough analysis of the hyperparameter's impact would offer valuable insights into the model's sensitivity and robustness.
3.The paper does not present a detailed comparison of the computational time required for the proposed method versus the baselines. Without this information, it is unclear whether the proposed method is more efficient in terms of runtime.
4. The motivation of this paper is not very convincing. It seems that this work is done just because LDL has not been applied in heterogeneous graph. It is not clear why LDL can solve some key problems in heterogenous graph.

Limitations:
While the authors provide a complexity analysis and mention that scalability is not the main concern of this paper, they acknowledge that the current model may not scale well with larger datasets. Although a modification to improve scalability is suggested, it is left for future work, indicating that the current version might struggle with large-scale data.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper advances Label Distribution Learning (LDL) into the realm of graph domains, specifically addressing the heterogeneous graph label distribution learning (HGDL) problem. The authors highlight that graph heterogeneity, reflected in node types, node attributes, and neighborhood structures, poses significant challenges for generalizing LDL to graphs. To tackle these challenges, the authors propose a new learning framework with two key components: 
1.Proactive Graph Topology Homogenization: This component focuses on learning optimal information aggregation between meta-paths to address node heterogeneity before the embedding learning phase.
2. Topology and Content Consistency-aware Graph Transformer: This component uses an attention mechanism to learn the consistency between meta-paths and node attributes, ensuring that network topology and nodal attributes are equally emphasized during label distribution learning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The introduction of a framework that proactively addresses graph heterogeneity and incorporates consistency-aware mechanisms is a novel contribution to the field.

2.The use of KL-divergence and additional constraints in an end-to-end learning process provides a robust solution for label distribution learning on graphs.

3.The theoretical and experimental validations are thorough, and the availability of code and datasets promotes transparency and reproducibility.

Weaknesses:
1. The paper lacks an analysis of the algorithm's complexity, and the framework diagram in Figure 2 is somewhat cluttered.

2. The authors' motivation is to address the challenges posed by graph heterogeneity, but these challenges are not clearly described in the introduction.

3. For optimization formula (3), the authors need to provide a detailed explanation of the advantages of this design.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
UGwdz3kjht;"REVIEW 
Summary:
This paper makes the key observation that existing dataset distillation methods often introduce misaligned information during both the extraction and embedding stages, which leads to suboptimal performances. In response to this observation, the authors propose a method called Prioritize Alignment in Dataset Distillation (PAD), which aims to filter out misaligned information through two steps: 1). Pruning the target dataset based on sample difficulty according to the compression ratio, and 2) using only deep layers of the agent model during distillation to avoid encoding low-level, redundant information.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper synthesize a universal framework for existing data distillation methods by abstracting those methods into two steps: 1). Information extraction and 2) information embedding. Furthermore, it identifies a common theme of information misalignment in both steps. This observation enhances the understanding of current limitations as well as provides a clear direction for future research. 
2. The method presented in this paper effectively combines known conclusion from two distinct areas of research: data selection and representation learning. By leveraging established principles from both domains, the provide improvements to existing data distillation methods. More importantly, their analysis builds on developing an understanding of data distillation and the underlying mechanism: 1) small datasets require simple data 2) large datasets do not benefit from low-level information/features.

Weaknesses:
1. The proposed method’s filtering of information extraction is supported experiments shown in Figure 2. However, in practice, the method introduces two sets of hyper parameters - initial ratio and data addition epoch. The sensitivity to these hyperparameters (especially AEE shown in Table c) relative to the incremental performance gain presents a challenge, as running AEE can be complex and time-consuming (involves retraining the agent). This sensitivity and the associated tuning complexity could hinder its practical adoption in larger-scale datasets. 
2. The proposed method is adapted from the DATM framework with modifications at enhancing information alignment. However, the performance improves over DATM (shown in Table 1 and in the cross-architecture generalization results in Table 3) are not significant. This marginal improvement raises concerns about the practical value of the proposed changes, as they may not justify the added complexity.

Limitations:
Yes, the authors adequately addressed the limitations of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes to study the information misalignment problem in dataset distillation.
It proposes two basic pruning strategies: (1) learn the synthetic data with easy real samples first, and gradually change to harder samples, and (2) only match deep layers of the network during trajectory matching. The proposed method could enhance the current method in a wide range of dataset settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written.
- The motivation is reasonable.
- The idea of using a scheduler to dynamically adjust the real sample difficulty is smart.

Weaknesses:
1. The experimental observations to support the two strategies (Information Extraction and Information Embedding) involving Figure 2 and 3, is not sufficient. Experiments on a wider range of datasets and more pruning ratios could rationalize the method. And a comparison of discarding deeper-layer parameters is missing.
2. The wide existence of difficulty-aware dataset distillation could **potentially** weaken the contribution. Some discussion is appreciable:
```
[1] Prune Then Distill: Dataset Distillation with Importance Sampling
[2] Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection
[3] (DATM) Towards Lossless Dataset Distillation via Difficulty-Aligned Trajectory Matching
[4] On the Diversity and Realism of Distilled Dataset: An Efficient Dataset Distillation Paradigm
```

Limitations:
The authors have discussed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors claim that existing data distillation methods introduce misaligned information, so they propose Prioritize Alignment in Dataset Distillation (PAD). PAD prunes the target dataset and uses only deep layers of the agent model to perform the distillation, achieving state-of-the-art performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is somewhat well-written and mostly easy to follow. And the tables/figures are well-demonstrated.

2. The authors analyze the misaligned information from two perspectives and propose method. 

3. PAD achieves improvements on various benchmarks, achieving state-of-the-art performance.

Weaknesses:
The performance gains brought by the method proposed by the authors are subtle and limited, potentially attributable to other explanations. For instance, as mentioned in [1], discarding original data in certain ways, or even randomly, can yield minor performance improvements under different IPC conditions. Or tricks mentioned in [2]. 

The trend changes in Figure 2 are not pronounced, and there are even instances where the trends contradict the explanations. Could additional test ratios or test IPCs be included to validate the findings?

[1] Distill Gold from Massive Ores: Efficient Dataset Distillation via Critical Samples Selection.
[2] Data Distillation Can Be Like Vodka: Distilling More Times For Better Quality

Limitations:
Due to the limitation of computing resources, the authors only validated their method’s effectiveness on DATM, DM, and DC.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
cuO0DenqMl;"REVIEW 
Summary:
This paper proposed a new ensemble algorithm, called Wasserstein Gradient Boosting (WGBoost), which is a novel gradient boosting framework that leverages the wasserstein gradient for probabilistic prediction. Specifically, WGBoost fits a new base learner to the Wasserstein gradient of a loss functional on the space of probability distributions. Since we cannot access to the probability distribution at each iteration and we also cannot evaluate the wasserstein gradient at each iteration, the author proposed to implement it using the particle method and approximate the functional gradient by the kernel method. The algorithm returns a set of particles that approximate a target distribution for each input. The main application demonstrated is posterior regression, where WGBoost provides a distributional estimate of output-distribution parameters.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- WGBoost's ability to approximate target distributions with particles offers a robust approach to posterior regression,  capturing predictive uncertainty effectively. 
- The proposed method shows superior performance in empirical evaluations on real-world tabular datasets, both for regression and out-of-distribution detection tasks.
- The implementation is seems easy by utilizing the particle method combined with kernel approximation.

Weaknesses:
The approach seems interesting, but the paper lacks the comparison with existing work. Moreover, there is no analysis or discussion when this algorithm is useful as I will explain in the below. 
- It seems that the proposed method seems almost identical to the stein variational gradient descent (SVGD), but there is no qualitative and quantitative comparison with that. Please explain what is the fundamental difference compared to SVGD. 
- Since the proposed method is very similar to SVGD, I think the comparison with SVGD and its extended methods including [3, 5, 6, 7] (I think there are other many variants of extention in SVGD).

- It has been known that the posterior approximation quality strongly depends on the choice of kernels in SVGD [1, 2, 3, 4]. Since the proposed algorithm is almost identical to SVGD, the quality of approximation by the WGBoost is strongly affected by the choice of the kernel function. However, there is no discussion about this point.

- The numerical experiments are only conducted with respect to the final performance on benchmark dataset and I cannot understand when and what kind of problems the proposed algorithm is suitable to  approximate the posterior distribution. It is known that SVGD suffers from collapse phenomena.
- In addition to the above point, there is no discussion about the computational cost of the proposed algorithm. When I say the computational cost, I point about the computational cost at each iteration and convergence speed. How large computational cost is compared to existing method regarding the number of particles and training dataset size ? I think the proposed algorithm is based on the boosting method, so it suffers from large computational cost with respect to the training dataset size.
- As for the convergence speed, it has been known that the convergence of the SVGD is slow, which does not show the linear convergence [4], and I suspect that the proposed method suffers similar problem. However, no discussion is present about the convergence speed or no numerical comparison exists with existing method.

[1] Stein Points 

[2] Measuring Sample Quality with Kernels

[3] Kernel Stein Discrepancy Descent

[4] On the geometry of Stein variational gradient descent

[5] FUNCTION SPACE PARTICLE OPTIMIZATION FOR BAYESIAN NEURAL NETWORKS

[6] Feature Space Particle Inference for Neural Network Ensembles

[7] Repulsive Deep Ensembles are Bayesian

Limitations:
The limitation is unclear. As far as I read no formal description is presented.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces a probabilistic boosting tree algorithm called Wasserstein boosting that uses a smoothed particle gradient to provide probabilistic predictions. Experiments are performed using UCI tabular regression, and out of distribution classification.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Originality: 

-	I like the application of Wasserstein gradients and gradient flow to gradient boosting trees, don’t think I’ve really seen any paper like this before.

Quality:

-	I didn’t check especially carefully but the machinery for the algorithm seems to be well explained and correct.

Clarity:

-	The relevant machinery of Wasserstein gradients and gradient boosting is mostly well explained.

Significance:

-	Making trees more probabilistic with limited modifications to their tabular capabilities would be a quite nice advance.

Weaknesses:
Originality: 

-	Nothing really noted. Being a straightforward application of some machinery is totally fine.

Quality:

-	For a paper on trees, I find the experiments quite small scale and limited. I would have expected NLL experiments on larger scale datasets, such as the xgboost and lightgbm papers. 

-	Comparisons to (approximate) Bayesian neural networks are quite weak – many other methods such as sgmcmc, etc. tend to outperform on datasets of these scales. references: https://arxiv.org/abs/1902.03932, https://arxiv.org/abs/1907.07504, https://arxiv.org/abs/2002.03704, amongst others

-	The natural missing comparison here is to Gaussian processes and other kernel methods, which are naturally probabilistic and similarly nonparametric (like trees).

-	Another missing set of experiments is comparison to quantile regression  / pinball loss using trees, which is implemented directly in lightgbm. Quantile regression itself is also naturally nonparametric in at least some sense.

Clarity:

-	My understanding of Wasserstein particle flows is that the particles should interact in some manner during the gradient step. However, the writing of Algorithm 1 makes this quite unclear. I think that the kernel smoothing in the gradient step for the approximate flow is what makes the particles interact, but it’s overall quite unclear.

         o	The code doesn’t seem to provide any clarity here.

-	Overall, it’s quite unclear which parameters in the loss are actually being estimated. If we’re only estimating uncertainty in regression parameters (or analogously classification), then there’s straightforward two stage approaches. 

      o	For example, one can easily fit a tree predicting the mean and then modify the loss function to predict its variance, or we can modify the loss in classification problems to do something analogous.



-	The writing is extremely passive and non-specific. Suggestions below.

       o	L150: “procedure of exact or approximate” Please use the algorithm box to specifically write or point out which algorithm is used in the experiments. The current algorithm is so unspecific it’s very hard to follow.
       o	L113-115: “Although [32] … originally suggested…” rephrase to something like “Although Friedman [32]  originally proposed using a line search … , Buhlmann and Hothorn [34] recommend against the line search …”

Significance:

-	Part of the strength of trees in my experience is that they scale pretty well to large tabular datasets (e.g. n = 10 million). You also tend to need strong uncertainty quantification on these types of datasets, which is part of the reason why Bayesian neural nets became popular for a while. Yet, these large scale uncertainty quantification experiments are lacking from the paper.

      o	Bayesian neural nets: https://proceedings.mlr.press/v115/izmailov20a.html, https://proceedings.mlr.press/v130/immer21a/immer21a.pdf, 

      o	Gaussian processes: https://arxiv.org/abs/1809.11165,

Limitations:
n/a

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes to perform boosting with base learner which are fitted to the Wasserstein gradient of a loss function on the space of probability distributions, which can be useful in particular to capture uncertainty of the models.

Several variants of the algorithm are discussed (by adding a diagonal Hessian preconditioner and with different approximation of Wasserstein gradients for functionals not differentiable on discrete measures). The method is demonstrated on posterior regression tasks where the functionals are KL divergences with respect to some prior, and applied on different real datasets benchmarks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This paper is well written and proposes a new interesting boosting method to minimize loss over probability distributions.

- The paper is well written
- A new boosting algorithm guided by functionals on probability distributions
- Application on real datasets outperforming baseline methods

Weaknesses:
The paper is good overall in my opinion, but still has some weaknesses.

- Experiments focus on KL divergence functional.
- No theoretical analysis

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel gradient boosting framework called Wasserstein Gradient Boosting (WGBoost). Unlike traditional gradient boosting methods that fit base learners to the gradient of the loss function, WGBoost fits them to the Wasserstein gradient of a loss functional defined over probability distributions. This approach is particularly useful for probabilistic prediction, where the goal is to approximate the uncertainty in the model prediction. The authors provide a general formulation of WGBoost, its algorithmic implementation, and empirical evaluations on various benchmarks. 

Paper's major contributions include, the introduction of the Wasserstein gradient flow framework into gradient boosting; the development of an approximate algorithm for posterior regression using the KL divergence; and the demonstration of WGBoost's performance on regression, classification, and out-of-distribution (OOD) detection tasks. Authors also propose a second-order WGBoost algorithm built on the approximate Wasserstein gradient and Hessian of the KL divergence.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
S1: The application of Wasserstein gradient flows to gradient boosting is a novel and promising direction, providing a new perspective on ensemble learning algorithms.

S2: Solid theoretical concepts, with detailed derivations and explanations of the Wasserstein gradient and Hessian approximations.

Weaknesses:
Authors did not specify details about the hyperparameter selection for Conditional Density Estimation, Classification and OOD Detection tasks.

Misc:
There is a typo in the line no 284, root is written as room.

Limitations:
NA

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
OQUg2T4qJB;"REVIEW 
Summary:
This paper studies the causal discovery problem in mixed functional relations data, where both linear and non-linear relationships exist in the causal graph. The author presents a Jacobian score-based method (essentially a score-matching method) to identify leaf nodes and thereby recover the causal order. The experimental results demonstrate the efficiency of the proposed methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is clearly written and well-organized.

2. The setting of mixed functional relations data is interesting and may be important for real-world scenarios.

3. The author proposes a Jacobian score-based method, which is an extension of the score-matching method for non-linear Additive Noise Models (ANM).

Weaknesses:
1. The non-decreasing variance of noises assumption is too strong and restrictive. Typically, in ANM, the noise term is assumed to be mutually independent.

2. It appears that the primary difference between the score-matching method for ANM and the proposed method is the introduction of Assumption 1.

3. If I use an independent residuals-based method, it seems to work in your setting. So, what are the advantages of the proposed method? For example, is the proposed method capable of handling large-scale structures? If so, the experimental results should demonstrate this.

Limitations:
NAN

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an ordering based causal discovery method when the underlying causal model has both linear and nonlinear causal relationships. Starting with a method to iteratively find leaf nodes, this paper proposes to use parent score for better pruning. Results show that the proposed method outperforms baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Paper is written well and easy to understand.
2. Theoretical motivations are clearly explained and the proof are adequately provided.
3. Experiments are extensive and cover all theoretical aspects.

Weaknesses:
1. Results are not great on real-world datasets. 
2. Topological divergence is a popular metric for evaluating the topological order. Very few results are presented in supplementary on this metric.

Limitations:
Limitations are discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose an ordering-based causal discovery algorithm designed to handle both linear and nonlinear causal relations in an SEM. In contrast to existing methods that assume purely linear or nonlinear relations, CaPS introduces a unified criterion for topological ordering and a new ""parent score"" to quantify the average causal effect, which aids in pruning and correcting predictions. Experimental results show that CaPS outperforms some sota methods on synthetic data with mixed linear and nonlinear relations and demonstrates competitive performance on real-world data.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* CaPS provides a new approach that can handle both linear and nonlinear causal relationships, addressing a relevant gap in current causal discovery methods.
* The introduction of the parent score is interesting and provides a quantitative measure of causal strength, which improves the pruning process and prediction accuracy.
* The authors present a new criterion for distinguishing leaf nodes using the expectation of the Hessian of the data log-likelihood and provides sufficient conditions for the identifiability of the causal graph, inspired by SCORE and LiSTEN.

Weaknesses:
* All noises are assumed to be Gaussian.
* The identifiability conditions rely on assumptions such as non-decreasing variance of noises, which is hard to hold in practical scenarios.
* Some more recent methods are not compared against.

Limitations:
There are some limitations not ""explicitly"" stated such as assumptions on causal sufficiency and Gaussianity of noises.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of ordering-based causal discovery, which involves first determining the topological ordering of variables (typically by recursively identifying sub-leaf nodes) and then identifying the parent set for each variable.

Existing methods often focus on either nonlinear or linear relationships. For instance, SCORE relies on a constant score Jacobian, which fails in the absence of nonlinear relationships, whereas LISTEN employs a precision matrix, which makes no sense in nonlinear contexts.

This work proposes an ordering-based method that accommodates both linear and nonlinear relationships. Specifically, it identifies the topological ordering using the expectation (instead of the variance) of the score's Jacobian, under a sortability assumption on the exogenous noise components. Subsequently, average treatment effect estimation is extended to identify the parent sets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. The application of ordering-based causal discovery methods to models with both linear and nonlinear relationships is novel to me.

2. The theorems and mathematical details appear to be correct, though I haven't checked all the details.

3. The experimental results are comprehensive, covering various competitors, different settings, and cases where assumptions are violated (e.g., C.7).

Weaknesses:
1. **Assumptions are too strong:**
  - For linear relationships in the ANM, additional assumptions are required for identifiability. This paper adopts assumptions similar to those in LISTEN, namely, that the variances of exogenous additive noise components follow the same topological ordering of the causal DAG, akin to VAR-sortability assumptions (Reisach et al., 2021). These assumptions are overly stringent, impractical, and lack testability. More discussions regarding this can be referred to ""Structure Learning with Continuous Optimization: A Sober Look and Beyond"".
  - The authors also assume that all additive noise components are zero-mean Gaussian. It is unclear if this assumption is utilized throughout the paper or why it was mentioned if not. Also, are these assumptions testable?
  - Regarding the Gaussian assumption, if it is not used for any proof, the authors might consider assuming non-Gaussian noise. This would allow the DAG to be identifiable even with linear relationships. Then, with score matching (which still works, as in Sec4.3 in the SCORE paper) and some straightforward processing (to preserve for non-Gaussianity/residual independence), the problem might still be solvable in a much more elegant way.

2. **Insufficient motivation for ""parent score"":** Once the topological ordering of the DAG is identified, one could use conditional independence tests between variables and all preceding variables to determine each edge's existence (as in most permutation-based methods), or employ sparse regression, as suggested in the original CAM paper. The authors need to justify the necessity of proposing a ""parent score,"" which seems over-complicated with average treatment effect estimation framework. Are there any advantages (e.g., in terms of time complexity or finite sample guarantee, as in the LISTEN paper)?

3. **Lack of technical novelty:** The technical contributions mainly combine ideas from SCORE and LISTEN, making the results and derivations (e.g., from constant variances to expected value of variances) straightforward extensions of previous work. While novelty is not a primary concern for me, it is worth noting as a minor weakness.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
fHxmoekQBh;"REVIEW 
Summary:
This paper proposes MaVEn, a novel multi-granularity hybrid visual encoding framework for multimodal large language models (MLLMs). MaVEn aims to improve MLLMs' capabilities in multi-image reasoning by combining discrete and continuous visual representations. The authors design a dynamic reduction mechanism to reduce the computational overhead of long continuous sequences. Experimental results demonstrate that MaVEn significantly improves performance on both multi-image and single-image benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper proposed a novel approach to combine the strength of both discrete and continuous visual representation as well as dynamic reduction mechanism.  
In addition, the authors conduct comprehensive experiments on both multi-image and single-image benchmarks to demonstrate the effectiveness of MaVEn.

Weaknesses:
While the MaVEn proposed a novel approach to combine the advantage of discrete and continuous visual info, the system can be a bit over complex for serving / maintenance in real applications.

In addition, the paper didn't discuss the computation complexity of MaVEn and compare it with other existing models.

Limitations:
Need more discussion on the model's limitation and computational complexity

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel approach, termed MaVEn, which aims to enhance the performance of MLLMs in multi-image scenario by integrating discrete visual symbol sequences with traditional continuous representation sequences. This dual strategy is designed to bridge the semantic discrepancies between visual and textual information. The approach also incorporates a dynamic reduction mechanism for long-sequence continuous features, aiming to boost processing efficiency in scenarios involving multiple images.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) The manuscript makes a significant contribution by proposing a hybrid model that combines both discrete and continuous data representations. This is a promising approach to mitigate the issues of semantic gaps in multimodal learning. The dynamic reduction mechanism for handling long visual sequences is also an innovative solution that could have broad applications in the field.
2) The experiments conducted are robust and comprehensive,  as highlighted in section 4.4, is crucial in demonstrating the effectiveness of the visual hybrid encoding. This section effectively showcases how MaVEn performs under different scenarios, providing empirical evidence of its versatility and reliability.
3) The paper is generally well-written and organized. The methodology section is well-articulated and provides a clear explanation of how MaVEn operates.

Weaknesses:
To solidify the claims regarding the efficacy of the discrete visual symbol sequences used in MaVEn, it would be recommended to conduct experiments comparing the performance of these different discrete representation techniques, such as VQGAN[1] or VQVAE[2].
[1]Taming Transformers for High-Resolution Image Synthesis.
[2] Neural Discrete Representation Learning.

Limitations:
yes.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a Multi-granularity Visual Encoding framework (MaVEn) for better multi-image reasoning. MaVEn combines discrete visual symbols and continuous representation sequences, as well as designing a dynamic reduction mechanism to efficiently and effectively process and interpret information from multiple images. Experimental results demonstrate its effectiveness in various multi-image benchmarks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-structured and clear in its presentation.
2. I am highly impressed by the author's methodological design, particularly the Multi-Granularity Hybrid Encoding component, which I believe makes valuable and insightful contributions to the research community.
3. The thorough experiments and visualizations presented in the paper effectively demonstrate the efficacy of the proposed method.

Weaknesses:
1. In Stage 3, would the adjustments to the Visual Projector affect the performance of the Patch Selector, since that component has been frozen?
2. The steps of training the patch selector using Grounded SAM annotated data seem a bit redundant. Directly selecting patches based on the similarity between the patch and the discrete token may be a simpler and more effective approach.
3. The role of the continuous tokens has not been well validated. In Figure 6, the attention seems to barely focus on the continuous tokens. Does this suggest that the continuous tokens have little impact on the performance, and they could potentially be discarded? Is it possible that the current evaluation design is unable to fully reflect the role of the continuous tokens? 
4. In line 202, '567' should be '576'.

Limitations:
None.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces MaVEn, a Multi-granularity Visual Encoding framework that enhances Multimodal Large Language Models (MLLMs) in multi-image reasoning by combining discrete visual symbol sequences with traditional continuous representation sequences. Experimental results show that MaVEn significantly improves MLLMs' understanding in complex multi-image scenarios and boosts performance in single-image contexts.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper is well-organized, from problem, motivation, approach and experimental validation. 

2. The proposed innovative multi-granularity approach includes 1) hybrid visual encoding and 2) dynamic reduction mechanism. The hybrid visual encoding captures both coarse-grained semantic concepts and fine-grained features, effectively bridging the semantic gap between visual and textual data.To enhance processing efficiency, a dynamic reduction mechanism is proposed, which selectively reduces long-sequence continuous features. This approach maintains essential information while reducing computational overhead.

3. The paper validates MaVEn’s effectiveness using several benchmarks, including DEMONBench and SEED-Bench, which encompass multi-image reasoning and video understanding tasks. MaVEn achieves superior performance compared to state-of-the-art models like LLaVA1.5, Otter, and others.Besides multi-image tasks, MaVEn also performs well in single-image benchmarks such as Visual Question Answering (VQA) and MMBench, showcasing its versatility.

Weaknesses:
see questions.

Limitations:
n/a

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces MaVEn, a framework designed to improve Multimodal Large Language Models (MLLMs) in understanding and reasoning across multiple images. Unlike current MLLMs, which are mainly focused on single-image interpretation, MaVEn integrates both coarse-grained semantic concepts and fine-grained details. This combination bridges the gap between visual and textual data, enhancing the model's ability to process multiple images. The framework also includes a mechanism for efficiently handling long sequences of features. Experiments show that MaVEn boosts performance in multi-image scenarios and also provides benefits for single-image tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The concept is logical, and the paper is straightforward to read.
2. The concept of using both discrete and continuous visual tokens is intriguing.
3. I appreciate the author's use of figures 2 and 3, which help clarify the overall framework and training process.

Weaknesses:
1. In Tables 1, 2, and 3, the author does not compare some of the latest methods, such as mini-Gemini, MiniCPM, XComposer, and InternVL.
2. The paper primarily claims its main advantage is in multi-image tasks; however, the author only tests on DEMON and SEED benchmarks. Testing on additional multi-image benchmarks, such as MMBench-Video and MME-Video, would be more compelling.
3. Figure 6 is unclear. In the first figure, I see only two vertical lines on discrete tokens. Does this mean that only discrete tokens have attention weight?
4. The paper does not report the computational complexity compared to other methods.
5. Figure 5 shows that the selected tokens are mostly related to objects. Would it be beneficial to directly use Grounding Sam for token selection? A comparison might be interesting to see.
6. The training pipeline is complex and involves four stages. What is the training cost?

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
sNz7tptCH6;"REVIEW 
Summary:
This paper introduces Adaptive Token Tuning (ATT) to improve the transferability of adversarial examples generated from ViTs. ATT is an improvement over traditional gradient-based algorithms, consisting of three independent methods. The first method reduces gradient variance by rescaling the token gradients computed throughout different modules and layers in the ViT. The second method is a scheduled, semantic-guided patch-out approach to increase the diversity of input during the iterative gradient ascent procedure. The third method is a truncation approach for attenuating the model-specific attention occurring at the deeper layers of the ViT. Experimental results on ImageNet show an improvement over existing transfer-based attacks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality:**

The proposed ATT algorithm includes three strategies that work in combination to improve the transferability of perturbations generated from vision transformers. Each method is developed based on insights from previous work and improves upon them. The proposed method is original.


**Quality/Clarity:** 

The presentation is clear, and the methods are well-explained. A thorough evaluation of transferability on a variety of target models, including transformers and both undefended and defended CNNs.


**Significance:** 

The proposed work is a valuable addition to methods for generating more transferable perturbations from vision transformers.

Weaknesses:
1. Ln76: Blackbox attack also includes those when target model has limited access, i.e., query-based attacks.

2. The statement in ln 294-295 requires further consideration. The analysis itself does not show that the ATT perturbation captured more features than those generated by other algorithms. While the improvement suggests this, there is no analysis proving the statement.

3. Several statements in the paper requires references, for instance, Ln218-219.

Limitations:
No discussion on limitation of the work. The checklist says the limitation is discussed in Sec 2 4 and Appendix A1, but I could not find any discussion.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes an Adaptive Token Tuning (ATT) method to enhance the transferability of adversarial attacks on Vision Transformers (ViTs). The method introduces three optimization strategies: adaptive gradient re-scaling to reduce token gradient variance, a self-paced patch out strategy to enhance input diversity, and a hybrid token gradient truncation strategy to reduce the effectiveness of the attention mechanism. Extensive experiments demonstrate the superiority of ATT over existing methods in terms of attack success rate and transferability across various models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The combination of adaptive gradient re-scaling, self-paced patch out, and hybrid token gradient truncation is novel and well-motivated, providing a fresh perspective on improving adversarial attack transferability for ViTs.
2. The experimental results show that ATT significantly outperforms state-of-the-art methods, achieving higher attack success rates and better transferability.

Weaknesses:
1. In the paper's formulation, there are multiple hyperparameters. Do these hyperparameters interact with each other? Why is γ set to 0.5?
2. In Figure 2, is the model used for prediction the same as the target model for the attack? From the figure, it appears that the proposed method indeed makes the model focus on the classification target more quickly. However, how do you explain that as the number of iterations increases, the attention no longer focuses on the classification target? The targets shown in the figure are all dogs, just different types, so the attention should still focus on the dog.
3. Have the effects been tested on defended ViTs?

Limitations:
No.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
n this paper, the authors investigate three strategies to boot the transferability of adversarial attacks on Vision Transformers, including an adaptive gradient re-scaling strategy, a self-paced patch out strategy, a hybrid token gradient truncation.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1 The experiments are solid.

2 The soundness of the method is good.

3 This paper is well written.

Weaknesses:
1 The title of the  paper might be imappropriate to conclude the techniques proposed by authors. The overall methods are composed of three partitions:

- An adaptive gradient re-scaling strategy to reduce the overall variance of token gradients. It is related to the back propagation for crafting adversarial samples. 

- A self-paced patch out strategy to enhance the diversity of input tokens. It is related to the adaptive forward propagation for calculating the adversarial loss. 
- A hybrid token gradient truncation strategy to weaken the effectiveness of attention mechanism. It is also related to the back propagation for crafting adversarial perturbations. 

We can see that none of these points are related to the token tuning. (Maybe I am wrong, I hope authors can point out my mistakes.) As I understand it, token tuning usually refers to optimizing the token after tokenizing the input images.  Since the designed attacks craft pertubations directly on input images like most of previous attacks. I think it might be imappropriate to conclude the proposed methods with the phrase ""Adaptive Token Tuning"". 

2 In line 72, the authors claim that ""robustly trained defense models still fail to meet security requirements"".  The observation contradicts with the findings observed in [1].  I conjecture this is because the authors choose to attack secure models which is published in 6 years ago. Considering the rapid development in the adversarial community, I recommend to attack recent sucure models trained by $l_{\infty}$ AT provided by robustbench [2].

3 The novelty of this paper is fair. The design principles of the proposed methods originates from previous papers and in this paper, they simply make them adaptive. Higher flexibility often brings better transferability. However, it also brings additional costs: it will increases the difficulty of hyperparameter tuning. In addition, more new insights are needed to guide community better leverage ViT to attack deep models.

4  The conparison with the baseline methods in Table 1 and Table 2 might be unfair. It seems that the proposed methods acquire better transferability than other attacks. However, considering the proposed attack proposed in this paper combines the power of three attacks: TGR, PNA, PatchOut. I think the appropriate baseline is TGR+PNA+PatchOut. It helps us more precisely pinpoint the gains brought by the proposed method.

Limitations:
There is no single section to discuss the limitation and the broader Impact of the paper. Can you explain them with more details?

[1] https://github.com/Trustworthy-AI-Group/TransferAttacks

[2] https://robustbench.github.io/

[3] Are Transformers More Robust Than CNNs? in NeurIPS 2021.

[4] When Adversarial Training Meets Vision Transformers: Recipes from Training to Architecture, in NeurIPS 2022.

[5] A Light Recipe to Train Robust Vision Transformers, in SaTML 2023.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the enhancement of adversarial attack transferability on Vision Transformers (ViTs) through innovative adaptive token tuning techniques. It addresses the vulnerability of ViTs to adversarial attacks by introducing three main optimization strategies: an adaptive gradient re-scaling method to uniformly reduce gradient variance across ViT layers, a self-paced patch out strategy to increase input diversity and mitigate overfitting by dynamically discarding less important perturbation patches, and a hybrid token gradient truncation method designed to attenuate the attention mechanism's effectiveness, adjusting truncation factors across different modules for optimal balance. Extensive experiments demonstrate that these methods not only improve the transferability of adversarial examples across ViTs and CNNs but also significantly enhance attack success rates by an average of 10.1% compared to state-of-the-art transfer-based attacks. Notably, this approach achieves a remarkable average attack performance of 58.3% on defended CNNs, highlighting the ongoing challenges in securing robustly trained defense models against sophisticated adversarial strategies.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper significantly enhances the transferability of adversarial attacks on Vision Transformers (ViTs) by introducing three main optimization strategies: an adaptive gradient re-scaling method, a self-paced patch out strategy, and a hybrid token gradient truncation method. It also provides a comprehensive theoretical analysis of the proposed methods, detailing the mechanisms through which each contributes to improving adversarial robustness and effectiveness.

2. Extensive experimental results substantiate the effectiveness of the proposed methods, demonstrating their efficacy across various settings and models.

Weaknesses:
1. This paper enhances the transferability of adversarial attacks on Vision Transformers (ViTs) through adaptive token tuning, introducing three main optimization strategies: an adaptive gradient re-scaling method, a self-paced patch out strategy, and a hybrid token gradient truncation method. However, it lacks ablation experiments for these three strategies.

2. The authors propose an improvement on the PatchOut method, termed Self-Paced Patch Out, which is an input transformation approach that intuitively could enhance adversarial transferability. However, comparing it in Table 1 alongside methods that do not incorporate input transformation, such as TGR, seems unfair.

Limitations:
The authors suggest that attention modules in certain ViT layers are redundant for image classification and adversarial perturbation generation, potentially leading to overfitting. This assertion raises questions about the underlying mechanisms. Could the authors provide more detailed analysis or empirical evidence on how these redundant attention modules affect the transferability of adversarial attacks?

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
OcO2XakUUK;"REVIEW 
Summary:
This paper considers the problem of learning to defer (L2D),  where a classifier is allowed to defer a decision to an expert (possibly expensive to query) and trained to accurately predict while minimising the expert cost. 

A major contribution of this paper is establishing consistency guarantees for surrogate losses. 
Specifically, the authors derive a loss that encompasses existing surrogate losses. 
The authors provide sufficient conditions for the proposed surrogate loss to have realisable H consistency and for H consistency bounds. 
From the derived H consistency bounds, the authors show that certain choices of the expert cost leads to a surrogate loss that is both Bayes and realisable H consistent.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is very well written and does not have major issues in clarity.

This paper seems to provide a general  framework for analysing surrogate losses for L2D, and the established results appear to be novel. 
I am not actively working on L2D and my evaluation might not be accurate.

Weaknesses:
Due to my lack of experience in the field, I find it challenging to adequately adjudicate the significance of the work. 
Realisable H consistency effectively assumes that there is a perfect classifier that does not need an expert. 
This preposition seems to be strong and diminishes the point of having an expert.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies learning to defer, focusing on the single-stage and single-expert setting. It introduces a family of surrogate losses based on comp-sum losses [Mao et al., 2023b] and establishes their realizable H-consistency (under mild conditions). In addition, when the base loss is the logistic loss $\Psi_{log}$ or the generalized cross entropy loss $\Psi_{gce}$, H-consistency bounds are proven (under mild conditions) when the cost function of deferring is classification error. When the base loss is the mean absolute error loss $\Psi_{mae}$, H-consistency bounds are proven (under mild conditions) when the cost function of deferring is general. Note that H-consistency implies Bayes consistency. The results also close an open question raised by Mozannar et al., 2023. The relationship between realizable H-consistency and H-consistency bounds is then analyzed for learning to defer. Finally, the proposed surrogates are evaluated empirically and compared with existing baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality**

- A new family of surrogate losses based on comp-sum losses (focusing on $\Psi_{log}$, $\Psi_{gce}$, and $\Psi_{mae}$) [Mao et al., 2023b] for learning to defer (the single-stage and single-expert setting).
- Conditions for realizable H-consistency are identified (Theorem 4.1). In particular, $\Psi_{log}$, $\Psi_{gce}$, and $\Psi_{mae}$ all satisfy the conditions. This result is more general than Mozannar et al. [2023]
- When the cost function of deferring is classification error, H-consistency bounds are proven for base losses $\Psi_{log}$ and $\Psi_{gce}$ (Theorem 4.2).
- When the cost function of deferring is general, an H-consistency bound is proven for base loss $\Psi_{mae}$ (Theorem 4.3).

The results also close an open question raised by Mozannar et al., 2023 (Section 4.4). The relationship between realizable H-consistency and H-consistency bounds is then analyzed for learning to defer (Section 5). Related work is adequately cited and compared.

**Quality**

The submission is technically sound. Claims are well supported by proofs.

**Clarity**

The submission is generally clearly written and well organized.

**Significance**

The work closes an open question raised in previous work. Its work to connect both realizable H-consistency and H-consistency bounds might be useful in other learning settings.

Weaknesses:
**Originality**

- The realizable H-consistency result (Theorem 4.1) only applies to a subset of comp-sum losses.
- The H-consistency results (Theorems 4.2 and 4.3) require case-by-case analysis. Is it possible to prove such results for all comp-sum losses?

**Quality**

- Some experiments in realizable settings can help confirm the realizable H-consistency result.

**Clarity**

- Section 5 is a bit unclear. Lines 300-302: Do you mean that in the realizable setting, all surrogate minimizability gaps vanish? Or is it only for comp-sum losses?

**Significance**

- The applicability of this work might be limited to learning to defer.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors provide a framework of surrogate loss functions for learning to defer under the multi-class classification problem. By examining the deferral loss function and choosing different surrogates for the indicator functions, the authors provide a novel class of surrogate loss functions for learning to defer and prove the realizable $\mathcal{H}$-consistency and $\mathcal{H}$-consistency for some cases. The authors also verify their proposed loss with numerical results.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The analysis is novel and provides new insights for learning to defer literature;
2. The paper addresses the problem of achieving $\mathcal{H}$-consistency and realizable $\mathcal{H}$-consistency simultaneously.

Weaknesses:
1. (Major) The proposed loss is not practical in many critical settings when the cost to consult an expert is high; Both terms of the derived loss function contain the exact value of $c(x, y)$ while knowing $c(x, y)$ itself needs consulting the expert if $c(x, y)$ is related to the expert's response. Thus, training a model using the proposed surrogate loss requires querying not only the true label but also the expert's response to every single sample, which is somewhat against the motivation of deriving the problem of learning to defer. See the derivation of equation (2). (Addressed, improve my rating to 6)
2. (Minor) Some of the results are restricted to the case $c(x,y) = 1_{g(x) \neq y}$, which is limited compared to the vast problem settings of learning to defer;
3. (Minor) The presentation is a little confusing: there are three consistencies (Bayes consistency, $\mathcal{H}$- consistency, and realizable $\mathcal{H}$-consistency), while their relationships should be formally summarized into some propositions; the existing surrogate loss functions' consistencies should also be summarized into one table with indicator ""yes"", ""no"", or ""not proved"" for each consistency.
4. (Moderate) The numerical experiments don't specify the expert algorithm; the experiments are also very restricted to the case of $c(x,y) = 1_{g(x) \neq y}$.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes considers the setting of learning to defer: a machine learning system can choose to either classify an instance or defer the decision to an expert which incurs a variable cost. The objective is to minimize the deferral loss of the system. To solve this problem, prior work has proposed surrogate losses with certain theoretical guarantees with respect to the original deferral loss. This work proposes the first surrogate loss RL2D that is realizable H-consistent, satisfies an H-consistency bound and thus is bayes consistent. This resolves an open problem proposed from prior work. Empirically, the authors showcase that the proposed surrogate exceeds or matches prior surrogates on three different datasets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
Originality: The surrogate loss proposed in the work is novel as well as the proof technique for the theoretical properties. The derivation of the surrogate loss is different from prior work, however, it is not clear how the technique can be generalized to other settings.

Quality: I have verified a good portion of the theoretical derivations and they seem sound. The experimental setup follows similar protocol to prior work and is sound. The paper is very strong theoretically.

Clarity: very well written, clearly stating contributions of prior work and setting the stage for readers unfamiliar with the setting and the literature. Derivation and theoretical properties very well stated and easy to follow along.

Significance: The paper settles an open problem from prior work at AISTATS and in turns I believe concludes (barring any breakthroughs) a line of work on deriving surrogate losses for learning to defer. I think this is important because now the community can focus on other settings and other considerations beyond theoretical consistency properties. However, I don't think the paper has a lot to offer in terms of techniques/methods for the community beyond the learning to defer problem as the derivation rely on some algebra of the deferral loss. Therefore, I don't expect this to be widely read by the community, but will be instead read in great detail by the community working on learning to defer and related problems.

Weaknesses:
There are no weaknesses with regard to the theory in this paper beyond the generalizability of the approach taken to related problem settings.

However, the experimental setting is quite limited in terms of showcasing the behavior empirically of the newly proposed method. For the use of the surrogate in practice, it is not clear in which scenarios (if any) is the surrogate superior to prior work.

Moreover, it is not clear how do the theoretical properties help in practice.

Limitations:
Yes they have.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
6lx34fpanw;"REVIEW 
Summary:
This paper proposed a Federated Learning model that support Bayesian inference. To alleviate potential bias induced from local client data, a regularisation constraint on model-data mutual information is introduced. The authors show that the MCMC inference with the regularisation can be implemented through the stochastic gradient Langevin dynamics. The author also proves a generalisation bound.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Information-theoretical modelling of federated learning is not very common. More exploration in this area is important.
- The authors provide justifications to most of the design decisions and they look sound to me.

Weaknesses:
- The writing is sometimes difficult to follow. The relevance of some theoretical analyses is not always clear. It would be better to introduce Algorithm 1 in the main text early on. Steps in line 10 and line 16 seem to be the key differences of the proposed method; They should be reflected in Figure 1. 
- The proposed method is computationally more complex than the point estimates. The experimental results of FedMDMI do not show significantly better performance compared to FedEP. A comparison of computational cost is required to understand the potential advantages and compromises compared with all baselines.
- FedMDMI has been evaluated on small models that may not fully reflect its performance in scenarios requiring larger and more complex architectures such as ResNets. Evaluating FedMDMI on such models would provide a more comprehensive evaluation of its scalability and generalization capabilities.
- The Dirichlet distribution is used to simulate class imbalance data heterogeneity. Using heterogeneous datasets provided by TensorFlow Federated and LEAF would ensure that the method is evaluated under natural heterogeneous data distributions.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes an approach to mitigate training failure in the heterogeneous federated learning setup. The approach combines Bayesian perspective of posterior inference on the client side and regularization of mutual information between weights and data in order to reduce the effect of difference in the local datasets. The authors provide computable approximations of the values involved and provide information theoretic bound on the generalization in federated setup. Experiments show that the method succeeds and outperforms several baselines.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper gives a detailed explanation of the approach for heterogeneous federated learning with mutual information regularization and posterior estimation. A generalization bound is provided and extensive experiments are performed.

Weaknesses:
The motivation to introduce posterior inference in federated setup is not clear: The problem of the point estimation and inability to have uncertainty of the predictions is equally valid for the centralized setup as well. The further description of the approach is convoluted, it seems that the main reason to resort to posterior estimation is to be able to obtain tractable computation for the mutual information term and corresponding generalization bound from PAC-Bayes perspective.

The method is motivated by scarce local data that might lead to overfitting and prevent from training a global model when aggregated, but this setup is not checked empirically. Only the heterogeneity with respect to labels distribution is evaluated.

Limitations:
Limitations are addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors introduce a method for federated learning to bypass problems caused by inter-client data heterogeneity.
For this, they introduce a Bayesian approach with information-theoretic regularizer, that will prevent local models from overfitting. Specifically, the authors add a model-data regularizer at a global level and then show how it could be computed in a federated fashion. The local optimal posterior appeared to be the Gibbs posterior, and the authors employ SGLD to sample from it. To show the efficacy of the approach, authors conduct a series of experiments on image and text data at federated datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I think the paper has the following strengths:
- I find the paper very easy to follow, and the suggested idea is very interesting and natural;
- It tackles an important problem of data heterogeneity in Federated Learning;
- The paper provides detailed theoretical derivations and experimental evaluation;

Weaknesses:
I find the following things are downsides:
- The loss used in the optimization is a result of several levels of approximations.
First, the global model-data MI term is upper-bounded by the sum of local model-data MI terms. 
Second, each local model-data term is itself upper-bounded by the RHS of Eq. 17. 
I think that the discussion on the tightness of the upper bound is missing.

- I feel that some baselines are missing. Probably the first and classical approach to combat the problem of data heterogeneity was FedProx [1] (which was cited), but it is not compared with.
Also, it would be interesting to see, where the method is placed if compared with personalized approaches, that are specially built to deal with data heterogeneity. E.g. FedPop [2] (Bayesian), FedRep [3] (not Bayesian).

[1] Li T. et al. Federated optimization in heterogeneous networks //Proceedings of Machine learning and systems. – 2020. – Т. 2. – С. 429-450.

[2] Kotelevskii N. et al. Fedpop: A bayesian approach for personalised federated learning //Advances in Neural Information Processing Systems. – 2022. – Т. 35. – С. 8687-8701. 

[3] Collins L. et al. Exploiting shared representations for personalized federated learning //International conference on machine learning. – PMLR, 2021. – С. 2089-2099.

- Minor typos:
1) Lines 142-143 collapsed (negative vspace?);
2) Figure 2: The left y-axis lives in [0, 1], right y-axis in [0, 100].

Limitations:
The authors have a separate section on limitations. However, not all of them are addressed (see the Weaknesses section, about upper-bound on a loss).

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper considers the problem of Bayesian Federated Learning when there’s data heterogeneity and class imbalance across clients and develops a posterior inference approach for model parameters through mutual information regularization of the data and global parameters in local posteriors. This is achieved via using the KL formulation of the mutual information and showing that the optimal local posterior is a Gibbs distribution. To infer local posterior Stochastic Gradient Langevin Dynamics is used and to aggregate the local posteriors into the global posteriors a simple average of the local samples is considered. The theoretical results suggest that the proposed algorithm results in the convergence of the local and global posteriors and corresponding generalization bounds are provided. Results are shown on a few datasets where competitive test performance and uncertainty calibration are achieved.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
*   The problem considered is timely and important. In almost every application of federated learning, client data heterogeneity and scarcity are inevitable. For data scarcity, considering the model uncertainty (through Bayesian modeling) and for data heterogeneity removing the biases of local and global posteriors due to biases in local datasets are sensible directions.
    
*   Accompanying the empirical results and intuitions with theoretical arguments is another strength of the paper.
    
*   The writing is clear and understandable, and the organization of the arguments is intuitive and logical.

Weaknesses:
*   Although competing (Bayesian) methods are not specifically designed to deal with data heterogeneity there seems to be very little to no gain achieved by the model. The results in the current form seem weak to me. The performance gains are quite marginal (e.g. Fig. 3) both for uncertainty calibration and test performance. Are the performance differences provided in tables 2, and 3 statistically significant? Shouldn’t we assume that as the degree of heterogeneity increases the proposed method is more effective? Why don’t we see a larger gap as a function of $\\alpha$?
    
*   A few important papers seem to be missing from the introduction such as \[1\]. Some methods are not considered for the comparisons such as \[2,3\].
    
*   The only information about the time (and memory) complexity is a sentence somewhere close to the end of the paper. While an important motivation of the paper is to reduce the computational and memory cost, empirical timing results aren’t shown. Can you include a detailed timing comparison as a function of data heterogeneity, dimension and size of the network, etc? Intuitively variational methods should do a much better job compared to sampling-based methods. Although the stochastic version of the Langevin Dynamics is used here still they should take much longer than variational methods to converge.
    

\[1\] Cao, Longbing, et al. ""Bayesian federated learning: A survey."" arXiv preprint arXiv:2304.13267 (2023).

\[2\] Chen, Hui, et al. ""FedSI: Federated Subnetwork Inference for Efficient Uncertainty Quantification."" arXiv preprint arXiv:2404.15657 (2024).

\[3\] Kim, Minyoung, and Timothy Hospedales. ""Fedhb: Hierarchical Bayesian federated learning."" arXiv preprint arXiv:2305.04979 (2023).

Limitations:
See the weaknesses section.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
GN2GXjPyN8;"REVIEW 
Summary:
This paper uses a pre-trained conditional diffusion model for antibody design. This diffusion model is fine-tuned using a direct energy-based preference optimization method, focusing on optimizing residue-level energy preferences to enhance the generation of antibodies with desirable structures and high binding affinities. The authors also compared their method with SOTA baselines on 55 cases from RAbD benchmarks and showed good results.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed algorithm is novel and interesting. The authors combine diffusion models and DPO to solve the specific problem of antibody design. 
2. The algorithm is clear. The authors both define the diffusion process and DPO formation with very clear definitions. Meanwhile, all the figures with protein structure are very clear and informative. 
3. The results are convincing. AbDPO archives good results on CDR total energy compared to other methods.
4. The experiments part is very detailed. The authors conduct experiments on 55 of 60 antibodies on both their methods and the compared baselines.

Weaknesses:
Lack of explanation of SE(3)-equivariant neural network. The author uses the diffusion model with such an equivariant neural network from Luo et al. Lacking such an explanation may hurt the understanding of the whole method. 

**Minor:**
- There are some writings inconsistent. For example, equations under line 130 are not marked with numbers, but equations under line 138 have numbers. The ABDPO in everywhere this paper is written as \textsc{AbDPO}, but in line 242 is ""ABDPO"". In line 235, the first letter in pyRosetta should be capitalized since it is a proper noun. Checking these format issues will improve the consistency in the future published version.

Reference:
----------------------------------
Luo et al: Shitong Luo, Yufeng Su, Xingang Peng, Sheng Wang, Jian Peng, and Jianzhu Ma. 2022. Antigen-Specific Antibody Design and Optimization with Diffusion-Based Generative Models

Limitations:
Limitations are adequately addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new perspective for antibody design-- incorporating the energy factors aiming to minimize the overall energy of designed sequence and structure. It involves diffusion to maintain the sequence-structure co-design. Towards the variance of energy factors, the paper proposes the idea of gradient surgery to simplify it. The paper conducted extensive experiments, showing its performance towards other baselines. The paper is easy to follow.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The paper proposes an energy-based preference optimization approach to achieve better rationality and binding affinity, which is inspiring in the field of antibody design.
* The paper decomposites the energy factors, simplifying the gradient calculation process.
* The paper achieved significantly lower energy than other approaches, reflecting the effectiveness of the model design.
* The paper clearly states why it chooses energy as the main evaluation metric with strong statistical evidence.
* The paper provides a detailed sample-level comparison of existing baseline approaches, which is a solid solution and beneficial to successive works.

Weaknesses:
* Although the authors stated why they chose energy as the main metric, the AAR is about 10% lower than dyMEAN. The authors did not provide a solid reason for what caused the result (Are all the lower results considered hacked or biased? Maybe a sample-level analysis would be more convincing.)
* The derivation of the final loss for fine-tuning could be simplified.
* The adaptation of RLHF into this approach should be further clarified.

Limitations:
As stated in the article.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes an approach for fine-tuning diffusion models for the design of antibodies. The core diffusion-based generative model comes from Luo et al. [36] and to my understanding there are no technical changes to it. The second component is direct preference based optimization, inspired by fine-tuning of large language models. This is the main technical contribution and builds entirely on the works by Rafailov et al [41] and Wallace et al [46]. The reward signal comes from binding free energy that is decomposed at the residue level. Different components include attraction and repulsion forces that can be linked to antibody function. To overcome possibly diverging gradients associated with different energy components, the authors propose to leverage gradient surgery from Yu et al [51] that essentially increases cosine similarity between gradients for different tasks/energy components.

Empirical evaluation is focused on qualitative aspects and whether the approach is able to discover better binders than initially given complexes, measured using binding free energy.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
I think it is an interesting approach to merry molecular simulations and physics based energy calculations with diffusion and generative models. Especially given the small number of available crystal structures in the SAbDAB database.

Adding energy-based signal via direct preference optimization is an interesting re-purposing of that method.

Empirical evaluation goes beyond amino-acid recovery rate and RMSD metrics. The “success” at generating better binders quantified via improvement in binding free energy relative to the initial complex is an interesting metric.

Weaknesses:
Table 1 indicates that the approach is able to design better binders, measured via binding free energy relative to the initial complex. However, this comes at the expense of increasing the number of hydrophobic residues which is typically associated with non-specific binding. This would in all likelihood be useless binders and from the perspective of function no better than baselines.

It would be interesting to see the results of a baseline that “fine-tunes” relative to ddG score directly. In my understanding, the results in Table 1 are vanilla baselines. None of them (e.g., MEAN, dyMEAN, HERN) has for instance been used in combination with iterative improvement algorithm and some physics based simulator. How would this compare to fine-tuning relative to the directed preference optimization?

It would be interesting to see the results relative to different physics-based simulators. I’m not sure how many different simulators were used to generate “fine-tuning” signal?

Would it be possible to include additional metrics such as lDDT, TM, and some metrics characterizing the fit on angles?

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper applies direct preference optimization to antibody design. Specifically, it uses Rosetta binding energy to guide a pre-trained diffusion model to generate antibody CDR structures with low binding energy.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* Optimizing antibody binding energy is an important problem.
* The proposed gradient surgery procedure is technically interesting.

Weaknesses:
* To evaluate binding energy using Rosetta, it is necessary to run side-chain packing and energy minimization to clean up the predicted structure. Therefore, it usually takes couple of minutes to evaluate binding energy using Rosetta for just one structure. In other words, it is computationally expensive to guide diffusion models using Rosetta.
* Rosetta side-chain packing is stochastic and non-deterministic. Therefore, if we relax the structure generated by diffusion model multiple times, the calculated Rosetta energy will be very different, and the standard deviation can be very high (sometimes it can be twice or three times higher than the mean). In other words, it is very tricky to construct a preference dataset because you need to compare the binding energy distribution between two CDR sequences, and their standard deviation is very high. 
* Despite the high standard deviation, the reported binding energy in this paper does not have standard deviation. It seems like the authors only calculate Rosetta binding energy once for each structure instead of running side-chain packing or energy minimization multiple times and take the average. Therefore, the reported results may not be statistically significant.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
eKHQbgvL3G;"REVIEW 
Summary:
This paper tackles the problem of point tracking, where the task is to track the movement of a single point in a video. Point tracking has experienced a fairly recent deep learning revival starting with PIPs [22 in paper ref], which was inspired by a handcrafted method named Particle Video from Sand and Teller [1], with more recent follow-ups such as TAPIR [6 in paper ref], Omnimotion [8 in paper ref], and Cotracker [7 in paper ref].

Specifically, this paper tackles a key challenge faced by these contemporary point tracking models: the computational burden introduced by the cost volume operation, which is a correlation between the pointwise feature being tracked and the feature maps of all video frames in the current window being processed. This operation needs to be performed for each point being tracked, resulting in tensors as large as B x T x N x C x H x W (batch size, window length, number of points being tracked, num feature channels, video height, video width) being processed. In response, these models spatially downsample the feature space by up to a factor of 4 (in Cotracker's case) and 8 (in PIPs' case), which results in a decimation of high frequency information that can be useful in tracking points. As a consequence, these models experience a large reduction in tracking accuracy due to the downsampling.

The authors of this paper propose an alternative approach. Instead of downsampling the feature space, intelligently crop each frame to the most important region in the frame, i.e., somewhere around the point being tracked. The authors introduce a framework that takes advantage of a pretrained instance segmentation model (SAM [1 in paper ref] in this case) to segment the instance upon which the queried point is placed, to use the segmentation mask to query semantically neighbouring points, to track all these points using a pretrained tracker (TAPIR, PIPs, Cotracker, etc.), to aggregate all trajectories to a single instance trajectory, to crop each video frame about each instance trajectory point, and to re-do tracking of the original queried point in this pruned space. They even show how this can be performed recursively / progressively, where the entire process can be repeated to get an even more refined trajectory estimate. The authors also show how their framework can act as an accurate video object segmentation model, where the synergy between the point tracking and the segmentation model allows both to improve on their respective tasks (tracking and segmenting). They show how this can exceed the zero-shot video object segmentation performance (in the DAVIS dataset) of SAM-PT (a version of SAM that relies on a point tracking model to enable zero-shot video object segmentation) and various class-based video object segmentation models.

The authors evaluate their framework through exhaustive experimentation, with four main experiments:
* Evaluating point tracking performance for dynamic objects (i.e., testing on TAP-Vid-DAVIS)
* Testing the generalizability of their framework on different point tracking models.
* An ablation study where they ablate different components of their model (search space pruning using tracked query point vs. instance trajectory, and the recursive pruning scheme).
* Zero-shot video object segmentation.

Overall, the authors show substantial gains on standard benchmarks and demonstrate the synergistic effectiveness of using a pretrained instance segmentation model with a pretrained point tracker. They even show that there is no net computational efficiency loss in the additional FLOPS introduced by the segmentation model and the recursive process, since the tracking accuracy gains far outweigh the additional compute costs.

[1] Sand, P., Teller, S. Particle video: Long-range motion estimation using point trajectories. In CVPR 2006.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
* Originality:
    * One might point to SAM-PT as a similar idea, but as the authors dutifully pointed out in the paper, it is a different approach. SAM-PT uses a point tracker to (effectively) track the movement of an instance, and when combined with the instance segmentation capabilities of SAM, this effectively results in a video object segmentation model in a zero-shot manner. In contrast, this paper introduces a novel approach to improving point tracking accuracy (the other way around compared to SAM-PT) where SAM is used to restrict the tracking search space, allowing them to track without downsampling the feature space spatial dimensionality and demonstrating substantial improvements by doing so. No other tracking model, as far as I'm aware, have introduced anything close to this method.
    * Related works have been adequately cited and the paper is very clear in how it differs from prior methods.
* Quality:
    * Claims are well supported by a thorough analysis on the results of exhaustive experimentation.
    * Authors are careful and honest about evaluating both the strengths and weaknesses of their framework.
* Clarity:
    * The paper is concisely written. At no point did I have trouble understanding what the authors were trying to communicate, whether it was through their math or vocabulary. I'd like to commend the authors in the care taken to introduce each concept. For example, I enjoyed their clarification of mathematical notation in L67-70 and their concise description of the benchmark metrics in L225-233.
    * The tables and their captions are well-presented, clearly showing the gains of their framework.
    * All experiment parameters necessary to recreate results have been listed, explained, and motivated both in the main manuscript and in the appendix.
    * Overall, just about every question I can think of as I was reading the paper was almost immediately answered as I kept reading. This, to me, is an indication of a well thought out paper that flows nicely.
* Significance:
    * Definitely the most impressive part of the paper: the results. They are significant in two ways, in my opinion: 1) The framework improves on the state of the art across almost all benchmarks and with all the backbone tracking models they used; 2) This framework is extremely flexible in that you can mix and match any pretrained tracker and pretrained instance segmentation model. It's not tied to a specific model on either the tracking side or instance segmentation side.

Weaknesses:
* Originality:
    * No weaknesses.
* Quality:
    * No weaknesses.
* Clarity:
    * No major weaknesses, but I do have some suggestions and questions relating to clarity. I have provided these in the Questions section below.
* Significance:
    * No weaknesses.

Limitations:
The authors have adequately addressed the limitations of their framework and its potential negative societal impacts. Limitations have been discussed in Appendix F and Section 4, with honest and descriptive commentary. Potential negative societal impacts have been sufficiently discussed in Appendix F.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper,  the authors propose TrackIME, a framework to tracking points in video. The proposed TrackIME leverages a segmentation model to improve both its efficient and effectiveness. TrackIME achieves SOTA performance on TAP-VID dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposed method achieves competitive results while retaining a low computation cost (comparing with TAPIR)
2. Using segmentation model to help points tracking has never been explored by prior works.

Weaknesses:
1. The main contribution of proposed method is not clear to me. Using segmentation model to help track points within an instance is an intuitive top-down solution for points tracking
2. The proposed method outperform SAM-PT in a relative small margin in terms of instance-level tracking, which raise my concerns about the capability of model for tracking points. It seems to me that the improvement on the point tracking performance is limited to help tracking instances/objects.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a new framework for video point tracking from instance motion. By integrating existing segmentation and point tracking base models, the performance of point tracking is significantly improved from object-by-object optimization. Ablation experiments and extensions in zero-shot video object segmentation further validate the effectiveness of the proposed framework. The paper is generally well written and organized.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Instance segmentation and motion estimation provide an overall motion prior from the global perspective, which strengthens the spatial associations for point tracking. Based on this, coupled with an iterative post-processing step to mitigate the loss of visual information due to feature space downsampling, also contributes to high-resolution point tracking. 

Ablation experiments and extensions in zero-shot video object segmentation further validate the effectiveness of the proposed solution.

Weaknesses:
At the trajectory aggregation step (eq 3), multiple moving points are usually selected corresponding to each object, but just averaging them according to visibility is to simple. Direct averaging does not satisfy common cases such as rotations and scale changes, and even if this is used for subsequent inference, a better initial value would be valuable. A straightforward modification would be to fit the affine motion model with these selected points. 

For eq 3, 8 and 12, whether the visibilities can be regarded as confidence weights needs to be further analysed, since the supervision only have a 0-1 occlusion maps. Does this address the cases where an object is occluded for a few frames and then reappears?

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
OGaZVSS0Cx;"REVIEW 
Summary:
The authors present the first mini-batch algorithm for kernel k-means. The algorithm itself is simple and works the way one would expect mini-batch kernel k-means to work. The authors improve the running time of an iteration of kernel k-means from $O(n^2)$ to $O(n(k+b))$ for the mini-batch version of the algorithm. Additionally, they show that using a specific learning rate function, there is an upper bound on the number of iterations of the algorithm.

The main challenge in the design of this algorithm is to keep track of the intermediate centers as storing the points in feature space is infeasible, as they are updated iteratively as in Lloyd's algorithm. For this, the authors design a recursive update rule to keep track of the quantity $\| \phi(x) - C_i^j \|^2$ for each iteration $i$ and each center $j$. They show that in a new iteration this quantity can be updated by considering the distance of each point in the dataset to the centers of mass of the clusters in the mini-batch and the previous centers.

Finally, the authors provide an experimental study of the mini-batch algorithm on four datasets and compare it to a non-kernel mini-batch algorithm and the full kernel k-means algorithm.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The algorithm offers improved running time bounds that are interesting to practitioners using kernel k-means in practice. It is also the first algorithm for mini-batch kernel k-means.
- The main theorem's bound on the number of iterations is nice to have and a good follow up to paper [26].
- The theoretical analysis is cleanly written and easy to follow.

Weaknesses:
- The techniques, while elegant are not particularly novel in terms of theory. The proofs mostly follow from analyzing the inner product terms in the k-means formulation. 
- A number of the proofs in the main body of the paper could have been moved to the appendix, as they do not give the reader more of an understand of the big picture and are very detail specific.
- There is no discussion of the experimental results.
- While the authors state the paper is mostly theoretical, I believe this algorithm is mostly interesting to practicioners and therefore a more thorough focus on the experimental evaluation with more parameters, additional datasets and thorough discussion would strengthen the paper in my eyes.

Limitations:
The authors included a checklist in the appendix of the paper, but have not discussed practical limitations in the main body of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the first mini-batch kernel $k$-means algorithm, which significantly reduces running time compared to the previous kernel $k$-means methods relying on the full datasets. With the proposed mini-batch kernel $k$-means algorithm, each iteration can be executed in time $O(n(k+b))$, improving the complexity of $O(n^2)$ for fully-batch methods. The authors also provide theoretical guarantees, ensuring that the algorithm can terminate (reach a convergence) within $O(\gamma^2/\epsilon)$ iterations with high probability, where $\gamma$ is the bound on the norm of points in the feature space. When initialized with the $k$-means++ seeding method, the algorithm achieves an $O(logk)$-approximation. Experimental  evaluations confirm that the mini-batch kernel k-means algorithm performs significantly faster than its full-batch counterpart while maintaining solution quality.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed algorithm achieves significant improvements on time complexity compared with full-batch kernel $k$-means methods.

The paper provides theoretical analysis, ensuring the algorithm's termination and performance bounds if initialized with the $k$-means++ seeding method.

Weaknesses:
The techniques used in this paper are largely based on the work of [1]. Mini-batch $k$-means method is not new for clustering problem. The main contribution of this paper is to combine the idea of mini-batch $k$-means with the kernel $k$-means versions. It should be noted that the theoretical bounds given in this paper are not entirely novel.

There are some technical issues in the proofs (details see questions),  potentially undermining the theoretical guarantees.


[1] Gregory Schwartzman. Mini-batch $k$-means terminates within $O(d/\epsilon)$ iterations. ICLR 2024.

Limitations:
Although this paper mainly gives theoretical results for clustering problems, it lacks discussions on broader impact as required by the NeurIPS guidelines.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors propose the first mini-batch kernel k-means clustering algorithm. It is a variant of Lloyd's algorithm that was introduced by Sculley that takes a batch of random b points instead of the full set of points and a weighted avaerage with the current centers while updating the centers. This paper attempts to translate this idea in the *kernel* k-means setting. The resulting algorithm has the same approximation guarantee as the original k-means but it terminates faster and consumes less time per iteration.

Their analysis follows the recipe of Scwartzman who used an early stopping condition when the improvement on the batch drops below some user-provided parameter. The main challenge in the kernel setting is that the underlying Hilbert space could be large or even infinite-dimensional. This is prohibitive as Scwartzman's bound on the number of iterations depends on the dimension. The authors bypass this by instead giving a bound on the Hilbert norm of the points which can be bounded in practice for example using normalized kernels.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The authors coduct detailed experiments that compares their algorithm favorably with the prior works. Specfically, the ARI and NMI scores were noticably better across a variety of 4 datasets.

I liked the paper. I think it has a decent theoretical and experimental contribution.

Weaknesses:
New ideas are limited. Mostly an adaptation of Scwartzman's work in the kernel setting

Limitations:
None.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The article presents the first mini-batch kernel k-means algorithm, which significantly improves running time compared to the full batch kernel $k$-means with only a minor negative effect on solution quality. The proposed algorithm runs in $O(n(k+b))$ time per iteration, as opposed to $O(n^2)$ for the full-batch version. The authors provide theoretical guarantees for the algorithm's performance, demonstrating that it terminates within $O(\gamma^2/\epsilon)$ iterations with high probability when the batch size is $\Omega((\gamma/\epsilon)^2 \log(n\gamma/\epsilon))$. Experimental results confirm the efficiency and effectiveness of the algorithm.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Improved Efficiency: The mini-batch approach drastically reduces the running time from $O(n^2)$ to $O(n(k+b))$ per iteration, making it feasible to handle large datasets.

Theoretical Guarantees: The algorithm includes a thorough theoretical analysis, ensuring termination within a specific number of iterations and providing an approximation ratio when using $k$-means++ initialization.

Flexibility with Kernels: The algorithm works well with popular normalized kernels (e.g., Gaussian, Laplacian), making it versatile for various applications.

Practical Relevance: Early stopping conditions align with practical machine learning workflows, increasing the algorithm's usability in real-world scenarios.

Weaknesses:
Approximation Quality: While the solution quality is comparable to the full-batch version, the approximation ratio depends on the batch size and initialization, which may not always guarantee optimal clustering.

Parameter Sensitivity: The performance heavily relies on parameters such as batch size and learning rate, which need careful tuning.

Complexity in Implementation: Implementing the recursive distance update and maintaining inner products can be intricate, potentially increasing the implementation complexity.

Potential Issues

Stochastic Nature: The inherent stochasticity of mini-batch algorithms can lead to variations in performance, and convergence to local minima is not guaranteed.

Parameter Initialization: Poor initialization of cluster centers can significantly affect the algorithm's performance and convergence speed.

Data Dependence: The effectiveness of the algorithm may vary depending on the dataset characteristics, such as the distribution and dimensionality of the data points.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
5ClpGA0u9K;"REVIEW 
Summary:
The paper proposes a new method, called Energy Rank Alignment (ERA) to finetune large language models (LLMs) for molecular generation in a similar fashion to Reinforcement Learning from Human Feedback (RLHF). The paper first introduces how the alignment task  in LLMs is very similar to creating property-conditioned molecules from SMILES strings, which are token-based generation techniques. In the introduction, the paper distinguishes ERA from common RLHF methods, such as PPO and DPO, by stating that it has a minimization objectives and leverages a reward function. Next, the paper describes related work for using LLMs for molecular generation and RLHF for language models and reiterates the differences of ERA compared to PPO and DPO.

In Section 2, the paper outlines the definition of ERA which mostly center on the derivation of relevant loss functions that the algorithm aims to minimize. In its definition, the ERA loss makes use of the KL divergence to arrive at the final formulation at the end of Section 2 leading up to the on-policy loss formulation for ERA. Section 3 provides a theoretical analysis of the ERA loss and its gradients, as well as its connections to the regularized entropy objective.

Section 4 describes the experiments for molecular generation using ERA, including unprompted and prompted generation. The paper also includes a sub-section on general alignment settings of LLMs related to IMDB movie reviews. The results generally show a distribution shift between models finetuned with ERA and those that were not. The paper subsequently ends with a conclusion and discussion of limitations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The provides proposed an interesting method and finetuning objectives that is useful for conditioned molecular generation and LLM alignment. The strengths include:
* A novel method for designing property conditioned molecules that is also applicable to LLM alignment. [Originality, Significance]
* A detailed derivation of the ERA loss, as well as a theoretical analysis on relevant properties. [Quality, Clarity]
* Experiments that generally support the distribution shift induced by the ERA method.

Weaknesses:
The weaknesses of the paper mostly center on expanding relevant related work and baselines for experiments:
* The authors do not discussion related work to training of transformer models and LLMs using reinforcement learning to arrive at molecules with desired properties. Some examples include [1] [2] 
* The experiments do not include baseline evaluation of DPO and PPO, which would have provided relevant details for how ERA performs compared to established baselines. 
* The paper could be strengthened by providing additional details related to experimental settings (see questions)


[1] Ghugare, Raj, Santiago Miret, Adriana Hugessen, Mariano Phielipp, and Glen Berseth. ""Searching for High-Value Molecules Using Reinforcement Learning and Transformers."" In The Twelfth International Conference on Learning Representations.

[2] Blaschke, Thomas, Josep Arús-Pous, Hongming Chen, Christian Margreitter, Christian Tyrchan, Ola Engkvist, Kostas Papadopoulos, and Atanas Patronov. ""REINVENT 2.0: an AI tool for de novo drug design."" Journal of chemical information and modeling 60, no. 12 (2020): 5918-5922.

Limitations:
The authors briefly discuss limitations at the end of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce “Energy Rank Alignment”, a novel alternative to PPO and DPO for policy optimization when an explicit reward model is available. ERA is shown to work for enriching chemical libraries for proxy objectives that are fast and easy to compute, and has clear benefits in the simplicity of tuning the strength of regularization to a reference and entropy of samples with two decoupled parameters. This controllability allows ERA to avoid greedy policies and the sort of mode collapse often observed using DPO.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The ERA approach is interesting and clearly defined. It is well-suited for many preference optimization settings, where an explicit reward model is available and alternative methods do not take advantage of this. The authors show results on multi-objective optimization to illustrate that the approach is not limited to greedy optimization of single objectives.

Weaknesses:
The main weakness of the paper is the evaluation with respect to lead optimization of small molecules. This is a notoriously difficult kind of evaluation to make meaningful with purely in silico experiments. One clear opportunity for the authors to improve their evals, while respecting the constraints imposed by easily-computable reward functions, is to incorporate some kind of online evaluation. Comparing DPO and ERA in an online setting would be informative and more relevant for the chemistry community.

Limitations:
Partially

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study an important problem about searching through chemical space, where the number of possible molecules grows combinatorially with the number of atoms. They focus on aligning large autoregressive models trained on chemical compound databases to generate molecules. The energy rank alignment (ERA) algorithm is proposed to use an explicit reward function to produce a gradient-based objective for optimizing autoregressive policies.  The authors offer theoretical insights into the relationship between energy rank alignment (ERA) and proximal policy optimization (PPO), direct preference optimization (DPO). Their experiments show that ERA is scalable, does not require reinforcement learning, and performs well compared to DPO when preference observations per pairing are limited.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors study a significant problem about generating molecules with desired properties based on autoregressive models by proposing the energy rank alignment (ERA) algorithm. 

2. This paper is well written.

3. The proposed methods work reasonably well.

Weaknesses:
1. Diversity, novelty and uniqueness are all important properties for drug discovery as discussed in previous works. To verify whether the models can be used to improve the process of drug discovery, the paper may benefit from comparing the aligned models with the reference model based on these metrics.

2. Missing the discussion of the related works which also focus on molecule optimization and drug discovery for both traditional and state-of-the-art methods, such as [1] [2] and so on.

3. The authors propose using reinforcement learning for drug optimization, a well-established method frequently employed in prior works, such as [3,4]. Additionally, advantage-based and multi-objective policy optimization are well-known in the reinforcement learning literature. A more comprehensive analysis of the limitations of this approach, along with a comparison to other existing methods, would have been beneficial.

[1] Drugassist: A large language model for molecule optimization.

[2] Automatic chemical design using a data-driven continuous representation of molecules.

[3] Optimization of molecules via deep reinforcement learning. Scientific Reports. 2019. 

[4] Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning. Nature Machine Intelligence. 2021.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
9U0nLnNMJ7;"REVIEW 
Summary:
This paper empirically explores compressing language models with pruning and knowledge distillation. It summarizes the best practices of pruning and distilling language models, which are supported by extensive experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well-written, and the best practices are easy to follow, which is useful in practical LMs.
2. The paper includes sufficient experiments to support the main results (best practices).

Weaknesses:
1. The novelty of compressing LMs with pruning and knowledge distillation is limited because the method in this paper seems to be a simple combination of these two widely used techniques. Although the main contribution of this paper may be the best practices summarized from extensive experiments, it is better to highlight the difference between the final choice in this paper and the approaches in previous work like [1].
2. Extra computational cost should be considered. It seems the pruning process and the online inference of knowledge distillation requires extra computation. Besides the trained tokens, it is better to additionally compare the FLOPs of model training in the main experiments, like in Figure 1 and Table 2.


[1] Sheared LLaMA: Accelerating Language Model Pre-training via Structured Pruning. 2024. In ICLR.

Limitations:
Suggestions:
The number of significant figures should remain consistent in Table 2 and Table 3.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper the authors explore compression of LLMs via pruning and Knowledge Distillation. They try out a variety of approaches for pruning as well as the retraining step and provide a comprehensive analysis of best practices for getting compact LLMs from their larger counterparts. The authors explore pruning across width and depth using importance scores and then retraining on a relatively small dataset via Knowledge distillation to get a capable compact model. This paper provides super interesting insights into model pruning which can be used in practice.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is very well written and provide excellent explanations for all of their modeling choices.

2. I really enjoyed reading the key takeaways of the paper being structured as best practices. Each point in the best practices part of the paper provide some key insight into model compression and what works and what does not work.

3. I also enjoyed reading the comprehensive details that are written in the appendix.

Overall this is a very well thought out research paper with proper explanation for each modeling decision that is taken.

Weaknesses:
1. Small nitpick: The figures and tables need much much more detailed captions. I should have some idea about what the table wants to say just from the caption.

2. While results on benchmarks are appreciated, I always feel that these benchmarks do not reflect everything about LLMs. I would have liked some form of qualitative study on comparing the generations from the models. Some form of human evaluation on say 25-50 long form generation examples would be great given that there is not enough time during rebuttal for a larger scale human evaluation.

Limitations:
The authors have mentioned limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes Compact Language Models via Pruning and Knowledge Distillation, which combines various tricks and methods to compress a 14B model to 8B while achieving better performance than training from scratch.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper conducts extensive experiments, comparing the latest baselines, and the authors summarize extensive tricks for pruning the model.

The pruned 8B model performs well, surpassing the model trained from scratch.

The paper is well-written, and the overall structure is good. Despite having many conclusions, it does not confuse the reader.

Weaknesses:
Will the authors open-source the code? If the code and data are open-sourced, I would raise my score.

When pruning a 70B model to 7B, would the method in the paper still work? Would this model perform better than pruning a 13B model to 7B?

Limitations:
na

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
cdYIL6OQr6;"REVIEW 
Summary:
This paper introduces a novel mixture of experts model that applies local differential privacy to the gating mechanism. Their methods leverages the one-out-of-n gating mechanism and provides specific generalization bounds.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The overall insight of the paper is clear and strong.
2. Improve the tightness of bounds on the risk for mixtures of experts models.
3. Reduce complexity by relying on fewer parameters.

Weaknesses:
1. The section 3 talks about PAC-Bayesian bounds for mixtures of experts, but it lacks insight into why PAC-Bayesian bounds are applied instead of other bounds. More explanation is needed here, similar to the explanation needed in section 4 regarding Rademacher bounds.

2. In the experiment section, it is unclear why the chosen dataset is used for the experiments and why only 5 epsilon values were selected.

3. Even though there are very few existing guarantees, the experiment should include other methods as baselines and compare the results.
4. For the experiment section, only consider mixtures of n linear experts in binary classification tasks seems easy. Need to add other classification tasks.
5. There is no description about the datasets used in experiments.

Limitations:
1. Lack many reference as I motioned in questions part. 
2. The paper lacks a smooth flow, making it difficult to follow. Specifically, there is no clear insight or reasoning provided to explain why the existing mechanisms or bounds were chosen, as highlighted in weakness 1.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel approach to regularize mixtures of experts by imposing local differential privacy (LDP) on the gating mechanism. The authors provide theoretical justifications and derive PAC-Bayesian and Rademacher bounds tailored to this approach. Experiments conducted on various datasets demonstrate that using LDP as a regularizer improves the generalization ability of the models, especially in cases prone to overfitting. The method offers a balance between leveraging neural networks for gating and maintaining robust theoretical guarantees, making it a valuable contribution to the field of machine learning.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
This paper demonstrates originality by integrating local differential privacy (LDP) into the mixture of experts model, addressing privacy concerns while improving model generalization. The theoretical contributions, including PAC-Bayesian and Rademacher bounds, are rigorously derived and tailored to the new approach. The clarity of exposition makes complex concepts accessible, and the experiments validate the practical benefits of the method. The significance lies in enhancing the robustness and scalability of mixture of experts models, making them more applicable to real-world scenarios prone to overfitting.

Weaknesses:
The primary concern regarding this paper lies in its significance. There have been previous works that incorporated differential privacy (DP) into the construction of mixture of experts models with privacy considerations. This paper, however, utilizes local differential privacy (LDP) to analyze the theoretical aspects of mixture of experts models. The introduction of LDP significantly alters the generalization behavior of these models because both LDP and DP are methods that inherently enhance algorithm robustness, thereby affecting generalization. If the main goal of the paper is to enhance privacy, it is imperative to compare this approach with existing DP-based methods and highlight what specific aspects LDP protects that traditional DP cannot. Without this comparison, the added value of using LDP over existing DP methods remains unclear. On the other hand, if the focus is on analyzing the generalization of mixture of experts models, the paper must justify the rationale behind incorporating LDP for this analysis, as LDP is not inherently required for mixture of experts models. The paper needs to elaborate on why LDP is a suitable and necessary tool for this analysis and how it fundamentally impacts the generalization properties of the models in a meaningful way. Additionally, while the theoretical contributions are substantial, the practical implications need to be demonstrated more robustly through experiments. Comparing the results directly with models using traditional DP methods would strengthen the paper by showing the practical improvements and specific scenarios where LDP outperforms DP. Furthermore, using a broader range of datasets could better illustrate the claimed benefits in robustness and scalability. By addressing these concerns, the paper can more convincingly argue the necessity and advantages of using LDP in mixture of experts models, thereby enhancing its significance in the field.

Limitations:
No.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides generalization bounds for a particular type of mixture of experts (MoE) networks. They focus on MoE architectures where an input $x$ first goes through a gating function $g$, and then gets routed to a single (one out of n) expert $i \in [n]$ according to the $g(x) \in [0,1]^n$ distribution. The final output of the MoE network is the output of the expert $h_i(x)$.

The authors observe that when the gating function $g$ has certain regularization properties, which correspond to local differential privacy (LDP), then the resulting network has better generalization bounds than what appears in the existing MoE litterature. The authors provide such bounds. 

Finally, the authors evaluate LDP-regularized routing on binary classification tasks with mixtures of linear models. The results show that LDP regularization outperforms an un-regularized baseline.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
Mixture of experts models are still understudied, especially from a theoretical standpoint, so I appreciate the new analysis provided by this paper. The connection with differential privacy is creative and seems fruitful, although I have some reservations about it (see below). 

The bounds do improve on existing generic bounds for this type of MoE models. The paper is well-written and easy to follow, and the experimental code is available.

Weaknesses:
First, it is worth emphasizing that the MoE networks in this paper do not satisfy local differential privacy themselves. The gating network is not even *trained* with differential privacy. This paper only uses local differential privacy as a regularization condition on the *gating network only* at *inference* time, which does not provide meaningful privacy guarantees. That is not immediately clear from the title of the paper. To be fair, this work does not try to achieve any privacy goals, and is entirely focused on generalization bounds. But if privacy is not needed, then it is not clear why DP is the right tool for the job. The paper directly uses LDP (it could have been called something like ""exponentially regularized routing"") but does not motivate this choice. Are there other forms of regularization that could achieve similar or better bounds? While there are some known connections between robustness, differential privacy, and generalization, they are not mentioned here. In the context of MoEs, some large models already regularize their gating functions (e.g., the Switch Transformer adds some ""jitter"" noise to the routing logits). 

Next, the paper motivates the study of MoE models by mentioning recent progress with LLMs such as the Switch Transformer, which uses multiple layers of experts (deep MoE) and combines the output of different experts. All the modern LLM MoE models I am aware of are such deep MoEs. Meanwhile, the paper focuses on simple shallow MoE models with a single gating network followed a single layer of experts, which limits the potential impact of the paper in my opinion. While theoretical bounds may be of interest even on shallow MoE models, I would appreciate at least some discussion about whether the authors' approach can generalize to deeper models. 

Finally, the experiments have some limitations, which mostly stem from the two previous concerns. 
* The authors only evaluate a single, rather simplistic (3-layer MLP gating network followed by linear experts), MoE architecture. More concerningly, they use a fixed number of experts (n = 100), thereby missing an opportunity to evaluate their claim that ""we can have many more experts with almost no penalty from the theoretical point of view"".
* The only baseline is ""No LDP"", which I think is a quite weak baseline. It is not entirely surprising that adding some regularization, in the form of LDP routing, improves generalization compared to a completely un-regularized baseline. How about other forms of regularization, such as dropout, clipping, or jitter noise (which already exists in the context of MoEs)?
* Another baseline would be a non-MoE model, with a comparable number of parameters, e.g., even a simple, dense, multi-layer perceptron. Showing that shallow MoEs outperform dense models would alleviate concerns about the practical relevance of this work.

Minor comments:
* Table 1 might be more readable as a graph.
* It is quite surprising to see MNIST being qualified as a ""large"" dataset, for which a 4-layer network takes 3 hours to train on a GPU, in 2024.
* Also, it is unclear why MNIST has to be broken down into 3 binary classification tasks.

Limitations:
The authors adequately addressed the limitation they identified (difficulty of tuning epsilon), even though this is not the main limitation of this work in my opinion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors consider the mixtures of experts models, in particular the one-out-of-n gating mechanism for ease of theoretical analysis, and show that applying a soft-max, which is also the exponential mechanism, on the gating mechanism gives LDP and can improve generalization. The privacy techniques are largely the same as previous work, PATE, but specifically applied to mixtures of experts. The authors then provide theoretical analysis showing generalization bounds for this approach.


Unfortunately, I’m not familiar enough with the mixture of experts literature to evaluate the novelty of applying the soft-max and the corresponding theoretical guarantees. I am rather surprised though that the soft-max has never been applied and am still somewhat confused upon what the previous techniques were in the one-out-of-n mixtures of experts.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The authors show that choosing an expert through the soft-max provides better generalization. The privacy guarantee is then essentially proportional to the regularization factor (\beta) for the soft-max application. Further they give theoretical generalization bounds for this approach.

Weaknesses:
Unless I am mistaken (please correct me if I’m wrong) the authors are not providing privacy guarantees for the model itself, but only one inference call to the model. In particular, if feature vector x is input to the model, then the expert is chosen randomly according to the soft-max / exponential mechanism, which is \epsilon-LDP. If inference was then run again, suppose even on the same feature vector, then the random draw from experts would occur again. This is known as composition in the privacy literature, and the privacy guarantees	would now be 2*\epsilon. 

Providing privacy guarantees on only one inference call to a model is both not very useful nor interesting due to the composition properties.

Limitations:
See weaknesses section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors  take the first step (I though so at first) to MoE under LDP theoretically.  I read again and found that the author seems to have raised the utility lower bound of existing studies. Few experiments could be found. Perhaps, I am not an expert in MoE, but it really leave a  hard time.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Important poblem.

Weaknesses:
1. Perhaps I am not an expert in MoE, and I cannot tell from the author's introduction that there are any challenges. 
2. The experimental results and application scenarios are not clear.

Limitations:
see weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
oEmyoy5H5P;"REVIEW 
Summary:
The paper is a review of algorithmic recourse (AR) literature. The authors deploy a systematic framework to investigate research trends in algorithmic recourse and evaluate their incorporation of practical concerns like societal and institutional considerations of AR, or lack thereof. The review finds that current research is  focused on methods and technical considerations. The authors encourage researchers in AR to consider real-world implications of their work and conduct user studies.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
- Paper is well-organized and easy to follow
- Section 2 provides solid background information on algorithmic recourse
- The questions in Section 4 are pertinent

Weaknesses:
While I agree with the points being made and appreciate the findings in the paper, I question their novelty. As mentioned in Section 4.6, there are papers (albeit in smaller numbers than we would want) that already provide real-world examples and attempt to discuss ethics within recourse. Previous work by [Doshi-Velez and Kim](https://arxiv.org/pdf/1702.08608), [Vaughan and Wallach](https://www.jennwv.com/papers/intel-chapter.pdf) have called for more user studies in interpretable ML, which resulted in studies like [Sixt et al.](https://openreview.net/pdf?id=v6s3HVjPerv). Considering that many researchers working on recourse is also in the field of ML interpretability, I am not sure if the paper's results and call for more user studies are very substantive.

Spending more time differentiating this work from other related works (especially other literature reviews like [70]) rather than listing their contributions in Section 2.2 may be helpful in making your case.

A more thorough discussion and evaluation of results (attempted in Section 5) may resolve some of these questions. As it stands, there is a disconnect between Section 4 and 5. The message of the first part of Section 5 (lines 318-350) is not clear. The second paragraph of the section does not seem to be a discussion of survey results but rather an argument the authors are trying to make (without using the results). The paper would benefit from expanding on the contents in lines 352 to 356, pointing to results in Section 4 and bridging them to the suggestions in Section 5.1.

The paper reads more like a position paper, trying to convince researchers in algorithmic recourse to not only focus on technical methods (which, again, I agree with). But I am not sure if a literature review or a position paper suitable for NeurIPS, considering its call for papers, does not seem to suggest so.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper provides a review of previous works that study ""algorithmic recourse"", i.e. conceptual and practical approaches for giving people actionable recommendations to change how they are impacted by algorithmic systems. This literature is deeply connected with counterfactual explanations and understanding models through small changes to test data, answering questions such as ""how would the model M produce a different output if changed attribute x about myself"". The authors review 127 archival publications and answer 9 questions about how these works frame and study algorithmic recourse.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
In terms of originality, quality, and clarity:
- While the primary novel contributions of this draft are to highlight themes in previous work, the overall level of novelty is reasonable. Some concerns here, see below.
- Quality: the ""Systematized review"" methods are described such that they are replicable and seem justified. I don't expect readers to have major issues with inclusion criteria of papers, or any of the analyses presented.
- Clarity: Writing is clear throughout.

In terms of significance, the paper could have impact on future work studying algorithmic recourse, and might motivate NeurIPS community members (including those in companies or working with governments) to support recourse methods. This would be a large positive impact.  

 This kind of review can certainly be useful to researchers trying to incorporate ideas or findings from recourse-related research. The calls to engage with HCI and systems-level thinking are reasonable (though, some of the broader discussion/motivation in the paper is more convincing on this front than any of the empirical results from the 127 recourse-related papers). If a version of this paper were able to unify definitions in the recourse space, this could be powerful (though further expansion of Section 2.2 might be necessary: the paper does note that reference [70] is highly similar -- the current draft was a bit vague in comparing these and clarifying the added contribution here.).

A few other notes: There are 9 overall sub-research questions answered. Overall, these results seem likely to be useful to researchers entering the algorithmic recourse field (though, see below, some of these felt very general and not domain-specific in the current draft). The paper does fit into the ""Social and economic aspects of machine learning"" category listed in the CFP this year.

Weaknesses:
Overall, I do think the current draft may not achieve the full impact that a future revision could provide.

The current discussion section feels like it largely echoes other calls in the community to apply systems thinking to ethical/responsible/pro-social AI/ML initiatives, and while each of the suggestions has some connection to one of the analyses, the current draft is not totally clear about the extent to which these recommendations stem from the findings vs. are motivated by first principles. The paper is overall very critical of the AR field, i.e. ""Why hasn't this field engagement with any real world deployments"". However, it's also not entirely clear in the current draft how any of the general recommendations would be applied in specific AR domains.

One aspect of the paper that I think would have been most directly helpful to the NeurIPS audience in particular would be to take a stance on how recourse should be defined -- is the definition on line 62 ""endorsed"" by the paper? Is the ""imagine a counterfactual input x*"" an advisable approach to take for future work. Does this review support the definition, highlight core definitional or epistemic issues, etc.

Ultimately, given the intended goals of this draft, it seems success and impact (on top of the core empirical contribution provided by writing a systematized review) here are dependent on the ability for the provided recommendations to shape future research positively. While the five recommendations here could have a some positive impact, taking a stronger stance on the core definitions and framing of recourse ""tasks"" could have an even larger impact.

Limitations:
- No major concerns regarding unmentioned social impacts.
- Regarding the limitations of systematized literature review, the current draft discusses these reasonably.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper provides a comprehensive review of the algorithmic recourse research literature, concentrating on understanding the recourse research ""in the wild"", by focusing on the practical application of these techniques in real-world scenarios. The authors then provide some suggestions to practitioners to push future research to better practical applications.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-written and well-structured. Considerable effort has been put into this work to provide a comprehensive review of the area, highlighting the need for a more down-to-earth approach when considering recourse. The data collection and analysis are well-motivated and described sufficiently (Section 3 and Section 4). The recommendations in Section 5.1 are on point and all true, and they highlight issues that everyone in the community is aware of but that are largely ignored.

Weaknesses:
I feel NeurIPS is not the right venue for this kind of contribution, since this paper does not provide the level of technical novelty required by the conference. Being a review, I think it does not fit the requirement of ""new and original research"" given by the Call of Papers. I suggest the authors not be discouraged, since I think the contribution is still valuable for the community. Potential other venues I believe are more in line with the scope of this work could be the following (the order is random):
- IJCAI Survey Track (https://ijcai24.org/call-for-papers-survey-track/)
- ACM FAccT (https://facctconference.org/)
- AAAI/ACM AIES (https://www.aies-conference.com/2024/)
- ICML Position Papers Track (https://icml.cc/Conferences/2024/CallForPositionPapers)
- ACM Computing Surveys (https://dl.acm.org/journal/csur) 
- TMLR (https://jmlr.org/tmlr/)

Lastly, I would like to point out some potential additional papers on algorithmic recourse which could complement some remarks made by the authors:
- Line 182 ""We did not identify any applications evaluated with humans in the loop"": there has been some development in providing human-in-the-loop algorithms to identify better recourse options:
  - [1] De Toni, Giovanni, et al. ""Personalized Algorithmic Recourse with Preference Elicitation."" Transactions on Machine Learning Research, https://openreview.net/forum?id=8sg2I9zXgO
- Recommendation 4, ""Accounting for emergent effects"": there has been some research regarding providing recourse to multiple individuals, where they are competing for a limited pool of resources, looking also at the fairness of these systems:
  - [2] Fonseca, João, et al. ""Setting the right expectations: Algorithmic recourse over time."" Proceedings of the 3rd ACM Conference on Equity and Access in Algorithms, Mechanisms, and Optimization. https://dl.acm.org/doi/pdf/10.1145/3617694.3623251
  - [3] Bell, Andrew, et al. ""Fairness in Algorithmic Recourse Through the Lens of Substantive Equality of Opportunity."" arXiv preprint arXiv:2401.16088, https://arxiv.org/pdf/2401.16088

I also point the authors to some new papers considering human-in-the-loop interfaces for recourse (Recommendation 1, Section 5.1):
- [4] Esfahani, Seyedehdelaram, et al. ""Preference Elicitation in Interactive and User-centered Algorithmic Recourse: an Initial Exploration."" Proceedings of the 32nd ACM Conference on User Modeling, Adaptation and Personalization. https://dl.acm.org/doi/pdf/10.1145/3627043.3659556
- [5] Koh, Seunghun, Byung Hyung Kim, and Sungho Jo. ""Understanding the User Perception and Experience of Interactive Algorithmic Recourse Customization."" ACM Transactions on Computer-Human Interaction. https://dl.acm.org/doi/pdf/10.1145/3674503

Limitations:
The authors have highlighted the limitations of their work in Section 5.2.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors present a survey regarding algorithmic recourse scientific literature. In their work, the authors analyze what types of contributions do the authors choose to make to the AR research, what are the criteria covered in the authors’ definitions of AR, what are the criteria covered in the authors’ definitions of actionability, the roles of end users, what types of real-world considerations motivate existing research, what types of real-world considerations are seen as challenges for future work, what types of group-level dynamics are addressed in the existing research, what are the approaches to the realistic evaluation of proposed methods, and what are the open source and documentation practices in AR research. They conclude their paper by providing recommendations on how to make future algorithmic recourse solutions better suited for real-world needs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- the authors invested much effort into explaining the procedure followed to ensure a high-quality survey
- the authors very synthetically review scientific literature related to algorithmic recourse and provide a great insight into the field within a few pages
- the authors reviewed a vast amount of literature (165 references!)

Weaknesses:
We did not identify important weaknesses. While an extensive survey could be created following this one, providing in-depth details for each of the sections, we understand this cannot be done within the constraints established for this venue.

Limitations:
The authors have adequately acknowledged the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
ASA2jdKtf3;"REVIEW 
Summary:
This paper extends the framework of multi-agent influence diagrams (MAIDs) to explicitly capture complex forms of reasoning corresponding to Theory of Mind (ToM) as required for the interaction of Multi-Agent Systems with human users.  It introduces the framework of incomplete information MAIDs (II-MAIDs) for explicitly modeling higher-order beliefs in multi-agent interactions alongside probabilistic and causal dependencies between variables. Using results connecting EFGs to MAIDs, the authors demonstrate a natural mapping between strategies in the two frameworks that preserves expected utilities according to the agents’ subjective models.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
The approach is well situated within the state-of-the-art of related work in agent models with a game theory component and, as far as one can judge, appears technically sound within the broad remit of causal and influence diagrams (IDs). 
The paper is very well structured and the authors did their utmost to keep it relatively accessible by alternating formal sections with intuitive descriptive summaries. It remains somehow tedious to read, owing to the large number of definitions, whose numbering alternate with that of theorems. 
The rationale for building a framework on top of MAIDs rather than EFGs is well introduced, together with the mapping between strategies in MAIDs and EFGs and the choice of working at the interim stage. This culminates with Theorem 20, until section 5.1 raises some issues around the relevance of Nash Equilibria.

Weaknesses:
The major issue I would raise for this paper is one of relevance to NeurIPS, even in the extended sense. While a major rationale for the paper appears to be its potential application to AI Safety, in the NeurIPS context there does not seem to be enough outreach to current AI models, at least in a way in which they could be interfaced to the proposed ID model. This means some consideration of how current models may form ‘beliefs’, and this was not entirely obvious from the paper’s Title and Abstract. Perhaps my expectation was unrealistic, but I had imagined an attempt to unify formal ToM issues with ToM properties that are known to be associated to LLM, under a framework where this approach would federate or wrap formal agentic methods around, say Agentic LLM. With this comment I am not criticising the authors for not having written another sort of paper, I am simply pointing the perceived gap that may exist between this approach and the NeurIPS constituency. Further evidence would be the absence of references to NeurIPS paper and the relative dearth of mainstream AI venues in the references (to the notable exception of AIJ). Overall, it appears that AAMAS might be a better venue to host this type of paper. 

The paper does not really clarify its ToM framework which references both “multi-agent interactions” as well as “higher-order intentional states” but these aspects are not part of further formal developments. It also mentions “belief hierarchies of arbitrary and infinite depth” and this raises the issue of whether such a formal approach is realistic when it comes to ToM, in particular in the interactions between agents and human users. 

Despite an early reference to AI Safety and a mention in the paper’s abstract, there is little in the paper that actually progresses the discussion on AI Safety, which is only used marginally through ID examples, such as the one of Figure 2.

Limitations:
The limitation section begins with a number of upbeat statements that would better be placed in the conclusion or parts of the abstract. The main identified limitation, which echoes the discussion of section 5.1 is verbatim: “The main limitation of our work is the lack of a useful solution concept.” appears a quite severe restriction. While not affecting the solid grounding of the approach it considerably restricts its impact at its current stage of development.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new framework Incomplete Information Multi-Agent Influence Diagrams (II-MAIDs) for modeling complex multi-agent interactions involving theory of mind (ToM) and higher-order beliefs. The authors prove the equivalence between II-MAIDs and Incomplete Information Extensive Form Games (II-EFGs) at the interim stage. The paper also shows the existence of Nash equilibria in II-MAIDs under certain conditions.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The II-MAID framework fills a gap in existing game-theoretic models by allowing for inconsistent beliefs and higher-order reasoning. The paper is built on solid mathematical foundation with formal definitions and proofs.

Weaknesses:
* From my perspective, the proposed II-MAID framework appears overly complicated for modeling Theory of Mind (ToM), which is fundamentally a straightforward psychological mechanism observed in daily human interactions. The paper's approach may overcomplicate a concept that should be more intuitively represented.
* The paper introduces numerous assumptions and definitions without clear explanations which hinders the readability. As a non-expert in the field, some details in the paper are difficult to read. 
* It is unclear whether the model can be scaled and applied to larger, more realistic scenarios, where ToM takes place more frequently.
* The paper lacks experiments that validates the model.

Limitations:
N/A, see weaknesses.

Rating:
3: reject, not good enough

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work extends the theoretical framework of multi-agent influence diagrams (MAIDs) with incomplete information (II-MAIDs) to explicitly capture this complex form of reasoning. The primary theoretical contribution is the proof of the existence of Nash equilibria, although, in general, these equilibria are impossible for agents to identify.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This work is game-theoretic in nature, and overall, the presentation quality is good and smooth to the best of my knowledge.

2. Although I think the assumption made in this work generally makes sense to me: agents have consistent beliefs as part of our commonsense, which can be derived from a common prior distribution, I agree that there are settings with no common prior available. The setup is a less constrained setup.

Weaknesses:
1. One of my major concerns is the audience of this work. Given that this work is submitted to the safe ML track of NeurIPS, I expect more discussion on the relevance of this framework to AI safety. The author should elaborate on what they imply by “safety” rather than making a very brief claim about its relevance in the related work and conclusions sections.

2. The discussion of theory of mind is also lacking, given that this is well-motivated. There have been extensive studies on machine theory of mind, ranging from early studies [1-2] to recent studies on LLMs [3-4]. There has also been research connecting Theory of Mind to Game theory [5] and Interactive POMDP [6]. See the survey [7] for details. Overall, this work needs significant improvement in discussing related work for readers to evaluate its contribution and relevance to NeurIPS.

[1] Rabinowitz, Neil, et al. ""Machine theory of mind."" International conference on machine learning. PMLR, 2018.

[2] Jara-Ettinger, Julian. ""Theory of mind as inverse reinforcement learning."" Current Opinion in Behavioral Sciences 29 (2019): 105-110.

[3] Sap, Maarten, et al. ""Neural Theory-of-Mind? On the Limits of Social Intelligence in Large LMs."" Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing. 2022.

[4] Ma, Ziqiao, et al. ""Towards A Holistic Landscape of Situated Theory of Mind in Large Language Models."" Findings of the Association for Computational Linguistics: EMNLP 2023. 2023.

[5] Yoshida, Wako, Ray J. Dolan, and Karl J. Friston. ""Game theory of mind."" PLoS computational biology 4.12 (2008): e1000254.

[6] Çelikok, Mustafa Mert, et al. ""Interactive AI with a Theory of Mind."" Computational Modeling in Human-Computer Interaction. 2019.

[7] Albrecht, Stefano V., and Peter Stone. ""Autonomous agents modelling other agents: A comprehensive survey and open problems."" Artificial Intelligence 258 (2018): 66-95.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
VTcGX5HO19;"REVIEW 
Summary:
This paper proposes using Bayesian Kernelized Tensor Factorization (BKTF) as a new surrogate model for Bayesian optimization (BO). BKTF approximates the objective function using a low-rank tensor factorization, with Gaussian process priors placed on the latent factors to capture dependencies and enable uncertainty quantification. The key advantages are the ability to handle complex functions that are non-stationary and non-separable, and the sharing of information across dimensions to enable a more global search compared to standard GP models with local kernels. Inference is performed via MCMC sampling. Experiments on benchmark optimization functions and hyperparameter tuning tasks demonstrate improved performance over GP-based BO, especially when the initial sample size and evaluation budget are limited.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The BKTF surrogate is a novel and creative approach to extend BO to handle more complex objective functions. Modeling the objective as a tensor factorization with GP priors on the factors is an elegant way to introduce non-stationarity and multi-scale correlations in a principled Bayesian framework.

The method is grounded in a clear mathematical framework, with full details of the model specification and MCMC-based inference procedure provided. Positioning BKTF as a type of deep GP offers useful insight into its expressive power.

The paper includes extensive experiments on a range of synthetic functions and real-world hyperparameter tuning tasks. The results convincingly demonstrate the advantages of BKTF over standard GP-BO in terms of optimization performance and sample efficiency, especially in the realistic setting of a very limited evaluation budget.

The paper is clearly written, with the methodology explained in detail and the experimental setup and results presented thoroughly. The authors also discuss the limitations of their work, including the scalability challenges and the restriction to a grid-based search space.

Weaknesses:
The main weakness is that the proposed BKTF method is only compared against basic GP-based BO with standard kernels. To fully demonstrate the advantage of the BKTF surrogate, comparisons should be made to more advanced GP models such as deep kernel learning, deep GPs, and other scalable GP variants. Without these comparisons, it's difficult to assess how much of the performance gain is due to the specific BKTF approach vs. simply being a more flexible GP.

The BKTF model bears significant similarities to existing works on scalable GPs, such as ""Gaussian Processes for Big Data"" and ""Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)"", which also use inducing points on grids to obtain tractable approximations. The relationship of BKTF to these methods is not discussed, and it's unclear whether BKTF provides any substantial advantages over these existing scalable GP approaches.

The experiments on the synthetic test functions are somewhat limited in their dimensionality (only up to 10d). Given that BO is most useful for optimizing high-dimensional black-box functions, testing on some higher-dimensional benchmarks would be valuable. The scalability of BKTF as the dimensionality and grid size increase is not fully investigated.

The MCMC inference procedure may become prohibitively slow for high-dimensional spaces or large evaluation budgets. The paper does not report the wall-clock time of the experiments, which makes it hard to assess the computational feasibility of BKTF in practice, especially compared to alternative approaches.

For the hyperparameter tuning experiments in Section 5.2, the strong performance of BKTF with very few iterations seems counterintuitive and is not fully explained, since the BKTF model has many parameters and would be expected to require a substantial amount of data to train effectively. This seems to contradict the results on the synthetic test functions, where GP-EI performs equally well in the first few iterations.

Limitations:
The relationship of BKTF to existing scalable GP methods is not thoroughly discussed. The paper does not make clear how BKTF differs from or improves upon approaches like ""Gaussian Processes for Big Data"" and ""Kernel Interpolation for Scalable Structured Gaussian Processes (KISS-GP)"", which also exploit grid structure. A more thorough comparison is needed to justify BKTF as a novel contribution.

The cubic scaling of the covariance matrix computations in the number of grid points, which could make BKTF infeasible for high-dimensional or very fine-grained grids. Some discussion of potential ways to scale up BKTF, e.g., by exploiting grid structure or using sparse approximations, would be valuable.

The fact that BKTF relies heavily on a sensible grid specification, and may fail badly if the grid is poorly chosen. Some experiments showing the sensitivity of the results to the grid choice would help assess this risk.

The lack of comparison to a wider range of flexible surrogate models beyond standard GPs, including more sophisticated GP models as well as other probabilistic regression approaches. The current experiments are not sufficient to establish BKTF as the best choice for BO in practice.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the Bayesian Kernelized Tensor Factorization (BKTF) as a surrogate for Bayesian optimization (BO). This model uses a CP decomposition to define a set of random basis functions drawn from a GP prior. These latent functions are then weighted by another set of random variables. This defines a probabilistic model that can perform uncertainty quantification, which can then be trained by performing MCMC sampling over the hyperparameters. The acquisition function is calculated by computing the first and second moments of the samples, and calculating the upper confidence bound (UCB).

This procedure results in a non-stationary, non-separable model that can capture complex functions. This model is tested on a range of synthetic and ML hyperparameter BO benchmarks, each of which are non-separable functions that exhibit high degrees of interaction between variables.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality:**
This paper is the first to use kernelized tensors in the Bayesian optimization setting. Gaussian processes are a common surrogate in this setting, and this proposes an alternate surrogate with good arguments for its adoption.

**Quality:**
The paper demonstrates the performance of the BKTF surrogate on a wide range of benchmarks. The performance is strong, justifying the claims in the paper. Further, many supplementary results are provided to further investigate the modelling decisions made.

**Clarity:**
The explanation of the BKTF model and fitting process is explained well, providing a clear description of the model. Specifically, the 2D example in Figure 1 provides a clear motivation for modelling functions that have a high degree of interaction.

**Significance:**
The proposed method is a strong surrogate for Bayesian optimization, one that can model functions with mixed input spaces and high degrees of interaction between variables. This presents a good addition to the range of available surrogates in the field.

Weaknesses:
**Comparison to other methods:**
The authors claim that a strength of their method is that their method is non-stationary and non-separable. However, I do not feel that the paper sufficiently justifies this explanation for the model's success, for the following reasons:

I do not believe that the GP ARD is separable. The authors present that the ARD kernel is the product of $D$ independent kernels in 3.2. However, this is not how these kernels are implemented. Instead, (specifically for stationary kernels) they are expressed as functions of weighted distance [5], where $d=\sqrt{\sum_{d=1}^D (x_d - x'_d)^2/l_d}$. These kernels cannot be written as products of 1-dimensional kernels, and are non-separable. The authors also suggest that these experience the curse of dimensionality as the dimension increases, but this effect is not severe in the low-dimension regime studied in the paper.

The authors claim that the flaw of using additive GP kernels is that they:
> [require] strong prior knowledge to determine the proper interactions and [involve] a large number of kernel hyperparameters to learn

I do not find this argument convincing. For low dimensional problems, additive kernels can include all interactions up to order D, and learn the weighting of each order of interaction [1]. In fact, I believe that the number of kernel hyperparameters is of a similar order to the BKTF method. Additive kernels also work well with MAP estimation of the hyperparameters (especially for the low-dimensional problems investigated here), although I do not see why MCMC could not also be used for additive kernels if the number of kernel hyperparameters is considered too large. The additive GP baseline should therefore be order D, not order 1, to provide a fair comparison against existing non-separable modelling methods.

It is unclear why the authors compare to SaaSBO, a technique designed for high dimensional (D>100) spaces that places a strong prior on the lengthscales of the inputs (with the prior belief that few of the inputs are important, which is not the case for the test functions used).

Moreover, the authors do not provide comparisons against methods that are designed for non-stationary settings e.g. [1, 2]. 

(Minor comment) I would like to see MCMC over the GP kernel hyperparameters to obtain a fully Bayesian acquisition function, as in [6].

**Hyperparameter choices with BKTF:**
I disagree with the authors that BKTF is robust to rank misspecification. Figure 13 shows that the CRPS doesn't converge until 30 observations for the rank 2 model. Including the initial 30 datapoints, this model is fit on 60 datapoints, for a 2D problem - the GP model provides a much better fit to the data for <60 datapoints.  Since these models are used in a BO setting where few datapoints is common, this behaviour suggests that rank *is* an important parameter, and further that the model does not fit well with few datapoints. 

I would also want to see this experiment repeated on higher dimension test problems, to see if the problem is exacerbated in high dimensions. Moreover, the CRPS of the rank 2 model seems to converge only to that of the GP models, suggesting the performance over GP may not be solely due to modeling quality: this should be further investigated.

**Experiments:**
It is unclear how the authors handle categorical inputs for the baselines. The authors should be using methods designed for mixed input spaces, such as [4].

Following from the discussion on CRPS, this paper would benefit from some experiments on the quality of the fit of the model.

(Minor comment) It would be interesting to see the (arguably more popular) Matern 5/2 kernel compared to the 3/2 kernel, especially for the GP baselines.


[1] Snoek et al. ""Input Warping for Bayesian Optimization of Non-stationary Functions""    
[2] Eriksson et al. ""Scalable Global Optimization via Local Bayesian Optimization""    
[3] Duvenaud et al. ""Additive Gaussian Processes""    
[4] Ru et al. ""Bayesian optimisation over multiple continuous and categorical inputs""    
[5] Williams and Rasmussen. ""Gaussian Processes for Machine Learning""    
[6] Snoek et al. ""Practical Bayesian Optimization of Machine Learning Algorithms""

Limitations:
The authors provide good discussion on the limitations of their approach.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new surrogate model for Bayesian optimization, based on a functional tensor factorization. The approach discretizes the model to a pre-specified grid and uses MCMC sampling for inference. Bayesian optimization is carried out by selecting promising points from the pre-specified grid, as quantified by an acquisition function. The paper includes experimental results on synthetic functions as well as ML hyper-parameter tuning tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Nice experimental results on the ML hyper-parameter tuning task.
- Generally well written paper.

Weaknesses:
- Optimization method appears to be restricted to an a-priori defined grid of possible candidate points.

- I am concerned about reproducibility of the benchmarks, as the code submission not complete, e.g. appears to be missing implementations of basic functions like baseline GP fitting (`fitrgp`) and predicting (`predict`), the additive GP model mentioned in line 274 of the paper, the continuous optimization of the acquisition function. 

- Grid-based GP-UCB, GP-EI baselines are missing for ML tuning tasks (Figure 3). This is notable, because these experiments contain discrete variables, which requires a rounding operation if they are relaxed to a continuous space, as is done by the paper. This rounding operation is likely to degrade the performance of ""continuous"" GP-UCB and GP-EI, as it will can be prone to sampling ""between"" integers it has already seen, reducing its sample efficiency.

Limitations:
> A limitation of BKTF is that we restrict BO to a grid search space in order to leverage tensor factorization; however, we believe that designing a compatible grid space based on prior knowledge is not a challenging task.

An important limitation to highlight here once more that it goes from ""not challenging"" to prohibitively expensive as the dimension increases.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method for Bayesian optimization where the prior is a low-rank sum of tensor products of GPs. An MCMC scheme is developed for approximate updating and UCB sampling. Several experiments show strong performance relative to baselines on artificial function optimization and ML hyperparam tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This is a seemingly new approach to BO with a more flexible type of prior, which shows good empirical performance.

Weaknesses:
It's not clear what is new relative to previous BKTF papers [10,11], other than the scheme for using UQ in UCB sampling.

The clearest potential drawbacks to the approach are the memory and compute costs. These should be reported for the experiments.

Limitations:
The paper motivates the approach in part because standard methods assume the generating process is stationary, but the BKTF is also stationary. It’s nonstationary only after conditioning on values of $g$.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
k0qTnbQxzR;"REVIEW 
Summary:
The paper presents CogCoM, a novel approach to training large Vision-Language Models (VLMs) using a mechanism called Chain of Manipulations (CoM). This mechanism enables the model to solve visual problems step-by-step with evidence, inspired by human cognitive processes like marking and zooming into images. CogCoM integrates manipulations such as grounding, zooming, and OCR into the VLM architecture, allowing it to handle various visual problems without external tools. The model is trained using a robust data generation pipeline and evaluated across multiple benchmarks, demonstrating state-of-the-art performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Advantages of the Paper

1. **Explainable Reasoning and Manipulation Mechanism**: CogCoM generates intermediate steps with evidence, making the reasoning process transparent and explainable, which is crucial for complex visual tasks. The model incorporates a flexible set of manipulations that can be adapted to various visual problems, improving its versatility and problem-solving capabilities.

2. **Data Generation Pipeline**: The paper introduces an efficient pipeline for generating high-quality training data, which is essential for training VLMs to perform detailed visual reasoning.

3. **Superior Performance**: CogCoM achieves superior results across multiple benchmarks, including detailed visual question answering and visual grounding, showcasing its effectiveness and robustness.

These advantages highlight the paper's contributions to advancing the capabilities of VLMs in solving detailed and complex visual problems through a novel, human-inspired approach.

Weaknesses:
Weakenss in Points

This paper is generally good but I can still spot the following issues.

1. **Design of Figures and Tables**: The figures in the paper are not well-designed. The first and second figures are repetitive in meaning, and the colors in the first figure are too light (consider adding black outlines to the boxes). The font size in the second figure is too small to be legible on smaller screens. Additionally, the captions for Table 2 and Table 3 are too close to the tables, violating the submission guidelines.

2. **Lack of Discussion on Related Work**: The paper lacks a discussion of existing related work. It should consider citing and comparing with at least other agentic LMMs such as LLAVA-Plus[1] to provide a comprehensive comparison and context.

[1] https://arxiv.org/abs/2311.05437

Limitations:
See above Weakness part.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces the Chain of Manipulations (CoM) mechanism for data generation to enhance visual reasoning in VLMs. The authors developed a data generation pipeline, producing 70K high-quality samples, and created the CogCoM model. CogCoM achieves state-of-the-art results across nine benchmarks, demonstrating significant improvements in various visual tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The CoM introduces a new data generation mechanism that enables VLMs to perform step-by-step visual problem solving with supporting evidence.
2.A data generation pipeline is proposed, producing a dataset of 70K high-quality samples.
3.The trained model, CogCoM, achieved SOTA in nine benchmarks.
4.This paper is well-written and easy to understand.

Weaknesses:
1.During data generation, the process relies entirely on GPT-4 for prompting and existing models (GroundingDino, PaddleOCR) for generation. As mentioned in the appendix, inaccuracies in these current visual models can affect the quality of generated data and the model's reasoning capabilities. However, the system lacks validation or filtering mechanisms to enhance data quality.
2.To highlight the specific improvements brought by CoM, it would be helpful to provide results both with and without the incorporation of CoM data. This would clarify the impact of CoM, especially since CogCoM integrates a significant amount of additional data such as MultiInstruct and LLaVAR during the instruction tuning stage as shown in Table 1.
3.The CoM dataset includes 6K high-quality manually annotated math samples, but no test results for math problems are provided. Clarification is needed on whether the purpose of this math data is solely to enhance the model's reasoning capabilities.
4.The paper emphasizes that CogCoM is a model capable of multi-image multi-turn understanding, but no corresponding test results (qualitative or quantitative) are provided.
5.In the model section, some parameters are not specifically explained, such as the maximum turns the model can accept and the predefined threshold.
6.Typos error: Line 288 CogOM->CogCoM

Limitations:
see weakness

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Drawing inspiration from human cognition to solve visual problems through localizing, zooming, etc., this paper introduces a new framework called CogCom, which solves visual problems by automatically combining six types of basic manipulations. When facing a visual problem, CogCom can use reasoning to solve each step and employ basic tools to aid in the problem-solving process. To achieve this goal, CogCom constructed a data generation pipeline that leverages GPT4 to build the training data for   CogCom. The CogCom leads to performance gains compared to its baseline CogVLM on several benchmarks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The CogVLM makes gains based on CogVLM on several benchmarks. 
2. The pipeline that leverages GPT4 to construct manipulation pipelines for problem-solving is reasonable.

Weaknesses:
1. The VQA benchmarks reported in Table 1 are not very convincing. It would be beneficial to consider more modern and challenging benchmarks such as MMBench, MathVista, and SeedBench.
2. The comparison of baseline methods seems to be based on relatively outdated approaches. It might be more informative to compare them with more recent LVLMs like LLaVA-1.5, Monkey, and ShareGPT4V.
3. It would be helpful to discuss a closely related work ViperGPT [3] and V* [4]. ViperGPT shares an idea for solving visual problems via planning tool pathways. V* shares the idea of searching and zoom-in progressively.
4. The differences with some other related works should be discussed [5][6]. 

[1] Monkey: Image Resolution and Text Label Are Important Things for Large Multi-modal Models

[2] ShareGPT4V: Improving Large Multi-Modal Models with Better Captions

[3] ViperGPT: Visual Inference via Python Execution for Reasoning

[4] V*: Guided Visual Search as a Core Mechanism in Multimodal LLMs

[5] CoVLM: Composing Visual Entities and Relationships in Large Language Models Via Communicative Decoding

[6] DualFocus: Integrating Macro and Micro Perspectives in Multi-modal Large Language Models

Limitations:
The limitations have been discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
M2QREVHK1V;"REVIEW 
Summary:
This paper considers fairness issue of image restoration and proposes Group Perceptual Index to measure the distance between restoration distribution and gt distribution. Experimental and theoretical results demonstrate that the superiority the proposed perceptual fairness over previous method.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1.	The fairness is important for the image restoration community and the topic is interesting to study. The proposed Group Perceptual Index can be reasonable to measure the fairness properly.
2.	The paper includes solid theoretical and experimental results which provides evidence for fairness measurement.
3.	The paper is well-written and easy to follow.

Weaknesses:
1. The main results are mainly based face restoration. Can this method be useful for general scenario of image restoration?
2. The paper proposes a measure to detect the fairness issue. But can you suggest some potential solutions to address this problem?

Limitations:
Yes, the authors discuss limitations carefully.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work introduces a new method for assessing fairness in image restoration, called the Group Perceptual Index (GPI). This measure quantifies the statistical difference between the distribution of a group's original images and the distribution of their restored versions. The authors illustrate the effectiveness of GPI by applying it to advanced face image super-resolution algorithms.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
- the problem tackled in this paper is of practical importance
- paper is written well
- the proposed method is theoretically sound and is shown to work in meaningful ways when used on the problem space of image super-resolution

Weaknesses:
- Usefulness of the method is validated only on the super-resolution solution. Given the fact that the proposed method has potential to impact various image restoration algorithms, it would have been interesting to see how well it does on other image restoration application such as image denoising, deblurring etc.
- It is also not clear what kind of changes to the existing super-resolution methods might result in better fairness handling. Some insights into why certain methods are not good at fairness handling as compared to others might have helped the future works.

Limitations:
Authors has addressed the limitations to a satisfactory extent.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This study presents a novel method to evaluate fairness in image restoration using the Group Perceptual Index (GPI). GPI quantifies the statistical disparity between a group's original images and their restored versions. Fairness is assessed by comparing GPIs across multiple groups, striving for perfect Perceptual Fairness (PF) where all GPI values are identical. The research provides theoretical insights into this innovative fairness concept, drawing comparisons to existing frameworks, and showcases its practical implementation through advanced face image super-resolution algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-structured and includes sufficient theoretical explanations.
2. The concept of GPI is logically sound.
3. The paper demonstrates that the proposed method outperforms other baseline methods.
4. The paper thoroughly discusses both the advantages and limitations of the proposed method. The advantages highlight the method's effectiveness and potential benefits, while the limitations are clearly outlined, providing a balanced view of its capabilities and areas for improvement.

Weaknesses:
1. The authors introduce a novel method to evaluate the fairness of image restoration. However, it is important to note that this method has been validated exclusively on image super-resolution tasks. Further validation on other types of image restoration tasks would be beneficial to demonstrate its broader applicability and robustness.
2. How can sensitive attributes be detected and acquired? The impact of sensitive attributes deserves an in-depth discussion.

Limitations:
Limitations have been thoroughly discussed and adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper reveals that the conventional definition of fairness for image restoration is restrictive and often causes controversy. To address this issue, the authors introduce a new approach to measure fairness in image restoration tasks by proposing the Group Perceptual Index (GPI). Specifically, they propose assessing the fairness of an algorithm by comparing the GPI of different groups, where perfect Perceptual Fairness (PF) is achieved if the GPIs of all groups are identical. They theoretically study this notion of fairness and demonstrate its utility on state-of-the-art face image super-resolution algorithms.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The paper reveals the existing fairness measures such as Representation Demographic Parity (RDP) and highlights their limitations. It shows that these measures can be overly simplistic and may not detect subtle biases that affect different groups.
2. The paper proposes the Group Perceptual Index (GPI) as a measure of fairness in image restoration, which is a novel and significant contribution.
3. It provides a theoretical analysis of the properties of GPI and its relationship to other fairness measures.
4. The authors use a variety of datasets and experimental setups to demonstrate the effectiveness of GPI, which are convincing.

Weaknesses:
1. Group Perceptual Index (GPI) also increases the complexity of the evaluation process of image restoration algorithms, compared with the traditional fairness method, because it involves comparing the distributions of different groups.
2. The experiments use synthetic datasets generated from high-quality, aligned face image datasets like CelebA-HQ. 
3. The paper only evaluate the proposed method on the face dataset and does not provide the results on other kinds of image data.

Limitations:
The paper rethinks the fairness in image restoration and proposes a novel method, called Group Perceptual Index (GPI), to measure the fairness for image restoration models. The proposed method can effectively detect subtle and malicious biases enhances the robustness and security of image restoration systems

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
PnSTlFUfcd;"REVIEW 
Summary:
The paper proposes an online shielding approach for safe RL that does not assume prior knowledge of environment dynamics and utilizes finite-horizon model checking with learned approximations of the environment dynamics. It specifically focuses on RL with regular safety properties provided as a PCTL formula. The authors present a framework that dynamically identifies unsafe actions and deploys a safe backup policy when necessary. The main technical contributions of the paper are:

- Definition of a constrained RL problem based on regular safety properties.
- Presentation of model checking algorithms to verify finite-horizon satisfaction probability.
- Development of sample complexity results for statistical model checking procedures.

The novelty of the paper lies in its approach to reinforcement learning with regular safety properties without requiring prior knowledge of environment dynamics. Unlike traditional shielding approaches that need full environment models or simulators, this framework uses learned approximations and finite-horizon model checking. The authors represent the synthesis problem as finding an optimal policy under a constraint that the resultant product Markov chain (from the policy) and the DFA (from the safety property) has probability $\leq p_1$ of violating the safety property in finite horizon $H$. The authors make use of the CMDP formulation to separate the reward and safety considerations by treating the safety property as a cumulative constraint in the CMDP.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Overall, this is a very strong submission and very clearly written. The authors present their problem statement precisely and also compare against many other related settings. This is challenging to do in safe RL since it is such a wide field with many parallel approaches, but I feel the authors did a commendable job here, especially in demonstrating how related and alternative formulations can be represented in their setting. 

This is a very interesting combination of shielding from temporal logic specifications with a CMDP formulation. Especially, as the framework allows for regular safety properties instead of just invariant properties, it allows for quite general specifications. 

The authors also do a good job in showing the generality of the work as it pertains to levels of model knowledge.

Weaknesses:
The main weakness I can see in this paper is perhaps a lack of experiments in settings with more complex models and safety specifications. Especially settings in which safety and optimality of the reward are in conflict, it would be interesting to see how the agent is able to achieve a trade off. 

Another consideration is perhaps motivation of using a CMDP as a framework for safe RL. I don't see this as well motivated in the paper. There are several approaches to safe RL that use MDPs and are able to use probabilistic model checking type techniques to give guarantees for cost thresholds.

Limitations:
There is some discussion of limitations in the section before the conclusion. I agree with the authors on the downsides of separating reward and safety.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a new safe RL approach, building on safety shields.
The idea is to leverage model-checking techniques during the RL training to block actions that are identified as unsafe in the shield and use a learned backup policy if this is the case. In contrast to previous approaches, the ""meta-algorithm"" presented does not require an a-priori known model of the safety aspects of the environment. The approach comes with theoretical guarantees on the satisfaction of finite-horizon PCTL specifications and the evaluation assesses the potential of the proposed algorithm.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I'd first like to pinpoint that the paper is quite clear and very well written.

The proposed approach has several key advantages. First, it doesn't necessarily require providing a shield beforehand, in contrast to previous work. Second, the properties the agent needs to enforce during training are specified via PCTL, a well-established specification formalism that is not prone to exponential blow-up in the size of the automaton for translating the formula (as is the case for LTL). Third, the approach comes with guarantees on the shield: (i) when the hyper-parameters of the optimization procedure are well-chose, one can bound the probability of failure of the system from the initial state, (ii) the optimal policy found under the PCTL constraint is ensured to be a feasible policy for standard constrained MDP objectives. Finally, the authors discuss and provide guarantees under different assumptions, namely, the access to a model of the safety-relevant aspect of the environment (as in previous work), under a black-box model, and when one has access to an approximate model such that the total variation between the true and approximate transition probabilities is bounded. Additional statistical guarantees are provided for these last two assumptions. 

Experiments successfully highlight the potential of the approach.

Weaknesses:
**Beyond tabular settings.**
The main concern I have with this paper is the fact that the guarantees seem to solely hold in the tabular setting, i.e., when the state-action space is finite and tractable. Notably, in the second round of experiments, the authors use Dreamer-v3 to learn an approximate model of the environment. Although the resulting method seems to outperform constrained RL methods, the theoretical guarantees do not hold in practice. 

**On the assumptions.**
Assumption 5.2 is confusing. Indeed, when reading it for the first time, I thought one needs to have access to a *generative model*, i.e., a black box model of $\mathcal{P}$ that can be requested at any time, under any state and action, akin to the setting of [1, 2]. Specifically, this would mean that one doesn't necessarily need to sequentially execute the environment to obtain samples (as this is the case in RL), but, at any time, for any given state-action pair $(s, a)$, one could request the model to obtain a finite number of samples from $\mathcal{P}(\cdot \mid s, a)$. Note that this is not compliant with RL. When reading further, it seems that Monte-Carlo model checking only needs to produce episodes, which is fully compliant with RL. Thus, the distinction is really important here.

On another point, Assumption 5.3 looks rather restrictive. How to ensure that the approximate model learned through Algorithm 1 (line 300) yields a bounded total variation? This should be discussed in the main text. Moreover, the linked guarantees (Proposition 5.5) are not evaluated in the experiments. It could be interesting to have an example of how the statistical guarantees can be applied in practice.

[1] Michael J. Kearns, Yishay Mansour, Andrew Y. Ng: A Sparse Sampling Algorithm for Near-Optimal Planning in Large Markov Decision Processes. Mach. Learn. 49(2-3): 193-208 (2002)\
[2] Yujia Jin, Aaron Sidford: Towards Tight Bounds on the Sample Complexity of Average-reward MDPs. ICML 2021: 5055-5064

Limitations:
Apart from the points raised above, the limitations of the work have been successfully addressed in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies RL with 'regular' safety properties. The constraint of safe RL is based on the satisfaction of a logic formula in probability. The action from the 'backup' policy will proactively override the potentially unsafe action from RL to ensure/optimize safety, a typical shielding mechanism in formal safe control methods. The authors demonstrated the effectiveness of their approach against CMDP and regular RL (Q-learning) in two examples.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The approach is sound, as a typical shielding method, should work well in a safe RL setting, in 2 examples shown in the paper. 
The problem studied in this paper is important.

Weaknesses:
1. Novelty. Novelty is my biggest concern for this paper. First, Problem 4.1 has already been discussed and solved in [1][2]. Second, the shielding approach is nothing new, a very standard way in formal methods. You can even trace back to simplex architecture with an advanced controller with safety back control. Third, the model-checking approach of this paper is not novel. 
2. Significance. The experiments are weak with only 2 baselines (1 as original RL, 1 as CMDP) on 2 simple examples.

[1] Wang, Yixuan, et al. ""Enforcing hard constraints with soft barriers: Safe reinforcement learning in unknown stochastic environments."" International Conference on Machine Learning. PMLR, 2023.
[2] Wachi, Akifumi, et al. ""Safe exploration in reinforcement learning: A generalized formulation and algorithms."" Advances in Neural Information Processing Systems 36 (2024).

You may want to extend the related works part including two papers above and more.

Limitations:
The reviewer would like to know the limitations discussion by the authors in the rebuttal phase.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents an approach to online shielding for reinforcement learning agents. Namely, safety is formulated in probabilistic temporal logic with a parametric threshold as an indicator for reachability of the goal state. The proposed algorithm checks the reachability probability threshold in each state of the environment and raises a warning when the threshold is violated. A pre-trained backup policy is then proposed to be deployed which overrides the action of the agent. The approach is evaluated on tabular and visual RL benchmarks.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper addresses an interesting and valuable problem. 

Evaluation includes visual RL benchmarks, which are interesting to provide online safety for.

Weaknesses:
The paper gives a lot of choice to the reader to compose a problem setting of their interest. It does not, however, provide precise enough approach description for each of them. This makes the contributions blurred. Described problem settings have been extensively studied before and the proposed approach does not significantly improve on them. It is also claimed that the approach can be used both during training and deployment, it is, however, not clear if the authors formulate these as two distinct settings and evaluate separately or not. In the latter case, it would be a dangerous simplification. Figures in the evaluation section are not readable.

Presentation: The presentation suffers from imprecise narrative leaving multiple questions until the evaluation section. Assumptions are introduced twice and it is not clear what exact problem the authors propose to address, or to what exact problem setting it generalizes. The use of ""etc."" and ""some other"" give the impression that more problems can be addressed than presented in the evaluation.

Minor:
- p.7: ""if need be we""
- ""don't"" --> do not

Limitations:
The approach is of limited novelty and employs existing components. Technical details in terms of the exact problem setting and guarantees are insufficient to judge the contribution. The approach is not yet placed in the larger context of online safety for RL agents, which would require more precise problem formulation and discussion of limitations.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses safety and constraint compliance in deploying reinforcement learning (RL) systems. Such issues have triggered a vast body of research in the area of safe RL over the last few years. The paper introduces a safe RL framework for so-called regular safety properties, focusing on satisfying these properties with high probability. The paper compares and places this setup to common constrained Markov decision processes (CMDP) settings and presents a meta-algorithm with provable safety guarantees to prevent violations of regular safety properties during training and deployment. The approach is evaluated in both tabular and deep RL settings.

Soundness:
3: good

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Safe RL is of utmost importance in order to enable RL agents to be successfully deployed in the real world. This paper tackles an important problem: How to ensure safety if the safety criteria go beyond standard reward or simple reachability features. The motivation of the paper is well done, and the placement in the literature is mostly complete and extensive.

Weaknesses:
While this paper addresses an important research question, I feel it is not ready for publication at a major ML conference. I will first summarize the issues that I see and then elaborate in more detail.

1. The claims made in the intro are not properly met. This concerns particularly the claim that the approaches are most permissive in terms of prior knowledge. 

2. The evaluation is not sufficient. The experiments are very sparse and miss important information to assess the quality of the research.

3. The paper is packed with formal definitions, most of them being standard in the formal methods community. Conversely, little space is spent on describing the methods in detail and providing a proper evaluation.

I overall feel that the paper aims too high and wants to solve all kinds of aspects in model-based safe RL with shields. It would have been better to pick certain key aspects more clearly. As an example, the paper claims to 
- not need a model
- to learn a model
- to use most of the available model checking paradigms (and even introduce them)
- evaluate with tabular and deep RL

Many of these aspects are then not properly discussed. 

I will now comment on the previously listed weaknesses.

1. In the introduction, the authors claim to operate in a most permissive setting where environment dynamics are not known. Yet, then, they introduce three model-checking paradigms, which are essentially standard numerical model checking, Monte Carlo-based statistical model checking, or model checking with approximate models. In particular, the last version is shady. What is a guarantee for a learned model? How can a claim be made for a most permissive setting but still obtain hard guarantees when, as a consequence, all guarantees rely effectively on statistics or a learned model? Other approaches are simply very clear about their assumptions, such as knowing a model. The model learning procedure is standard, and if the whole 'permissiveness' of the setting depends on the fact that a model can learned, I do not see a novel approach here. 

Very importantly: While a model is learned, no safety guarantees can be given, defying the notion of shielded RL. This aspect is not discussed in the paper. A step further, even the safe fallback policy that is explained in Section 6 relies on learning. 

2. The evaluation only considers a few environments and compares a simple tabular and deel RL agent. What I would have liked to see is how the different assumptions and model checking paradigms affect the learning, the safety, the performance. I believe, with a thorough evaluation in this direction, the paper would be much stronger. If we can believe that it is feasible to learn a model, then I would like to see a comparison between the strong assumption of knowing the model, and having learned a model, and what the effects on safety guarantees are. 

3. The contributions only start at page 5, and are then still interleaved with standard definitions. It is important to have a paper safe-contained, but, as an example, why is it necessary to introduce both LTL, PCTL, DFA in detail? And then, the evaluation even uses PCTL^*. To me this seems as if the space had been filled up with long definitions, while it should have been used more on the contributions of the paper. 

Generally, I feel the paper follows a great direction, especially investigating Shielded RL with a learned model. With a stronger evaluation and emphasis of this part, the research and contribution could be much improved in my opinion. 

Minor comments:

- proposition 4.2 is obvious and seems like an overformal statement of a simple fact. 

- Def 4.3. Make clear that this is reward engineering

- Non-Markovian cost: Compare to reward machines

- The tradeoff between safety and exploration has (for instance) been investigated in 

Carr et al.: Safe Reinforcement Learning via Shielding under Partial Observability. AAAI 2023.

- Learning the model, proposition 5.5: I think the topology (graph) of the model needs to be known to estimate the probabilities.

Limitations:
The limitations of the work in terms of assumptions and their real-world relation are not properly explained in the paper.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
2Bef9YxSJc;"REVIEW 
Summary:
The paper studies an important and open question, how much user behavior knowledge (generally captured by collaborative filtering models) are present in large language models. This has been a topic attracting significant research interest in recent years. The authors propose that simple linear mappings done on top of LM encoder representations are sufficient to capture collaborative filtering signals in recommendations, and propose a new recommendation method, AlphaRec, which takes pretrained language model content embeddings as input, transforms them via MLPs and lightweight graph convolutions, followed by a contrastive loss. The authors conduct experimental analysis for AlphaRec in both standard settings and zero-shot settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
**S1**: the topic studied is important. It is generally believed that language model and collaborative filtering (recommendations) learn different representation spaces, and methods to bring the two spaces closer is of significant interest to the large community working on search, recommendations, ads, and related topics.

**S2**: the particular approach proposed (linear mapping from textual space to collaborative filtering space) is understudied in prior work on LLM and (Generative) CF, despite numerous papers in recent years.

Weaknesses:
**W1**: the writing in this paper, esp. recommendation system paradigm related discussions, misrepresents (or ignores) significant prior work done in the field. e.g., ""AlphaRec follows a new CF paradigm, which we term the language-representation-based paradigm."" and related writings. 

- Content signals and/or embeddings have been used as the dominant recommendation paradigm in the field, even well before the seminal YouTube DNN paper [1] was published (see e.g., Pazzani and Billsus, 2007 [12]). For recent examples of related work, see e.g., [10, 11] from Pinterest and Meta in KDD'22 (but one should be able to easily find similar papers in WWW KDD etc in prior years as well). 
- Replacing ResNet/ViT- or GPT-/BERT- generated embedding with LLaMa- or Mixtral- generated embeddings cannot and should not be viewed as a paradigm shift, especially given the core architecture of AlphaRec is not substationally different from prior work.

**W2**: AlphaRec needs to be compared with stronger baselines. This applies to many major experiments in the paper. Here are some examples of baselines missing, which may significantly change conclusions obtained and discussions etc:

- vs ID-based recommenders (Table 3).
    - Equation (2) and line 171-173 for $N_u$ already captures set of items that a user is related to (""user interaction history"" / ""user engagement history"") to a large extent. Thus, the authors should compare AlphaRec with at least some SotA sequential/generative recommenders, such as SASRec, BERT4Rec, TIGER, HSTU [3, 4, 6, 7]. All of them are missing in the current paper.
    - Given AlphaRec uses the transposed item id representation - the one layer $N_i$ formulation (equation (2)), relevant work in recent years include Dual contrastive network [8] and User-centric ranking [9]. The authors should compare with or at least discuss some work in this category as related work. 
- Zero-shot performance. (Table 4)
    - ""Book Crossing"" is not a commonly used benchmark dataset. The ""Industrial"" dataset (per citation [1] on line 273) seems to be a small-scale ""Yelp"" dataset, and should be renamed to avoid confusions.
    - For ML-1M, the SotA approach one year ago (LLMRank [14]) already achieved 53.73 NDCG@20, significantly higher compared with 32.15  (AlphaRec) in this work. 

**W3**: many other formulations/experiments/writings could be significantly improved. Examples include:

- The proposed task formulation does not reflect how recommendation systems work in practice. e.g., ""Line 97-99. Personalized item recommendation with implicit feedback aims to select items i ∈ I that best match user u’s preferences based on binary interaction data Y = [yui], where yui = 1 (yui = 0) indicates user u ∈ U has (has not) interacted with item i [58]."" -- here ""selecting the item that user will interact with"" is not the same as ""selecting the item with the highest reward"", as the interaction itself can be negative (e.g., disliking a recommendation, abandoning session, etc.). See [1, 2] for references.

- A key contribution of this work should be the linear mapping finding. But Table 1 uses a questionable set of baselines for both LMs and CF baselines, which weakens linear mapping related claims.
    - To claim ""Moreover, with the advances in LMs, the performance of item representation linearly mapped from LMs exhibits a rising trend, gradually surpassing traditional ID-based CF models"" -- I would expect the authors to compare with a single set of models (e.g., LLaMa-2 7B 13B 70B or GPT-3 1.3b 2.7b 6.7b 13b 175b) trained on identical data. As it stands, all models are trained and/or finetuned with different data, so a simpler hypothesis explaining the LM (Linear Mapping) trend is that people are including more and more data into LLM pretraining/finetuning stages, which happen to capture more and more aspects relevant to recommendations. 
    - On the CF side, ""MF"" ""MultiVAE"" and ""LightGCN"" do not represent SotA baselines on Amazon Review datasets (see W2). 

- Table 1. Please highlight the particular K used for Recall and HR metrics (hard to find in the paper, applies to other tables too).   Most work on recommendation models also report HR/NDCG/etc. over at least 2-3 Ks to help readers understand how metrics vary with different approaches.


- Table 4. [1] should not be labeled as an ""Industrial"" dataset. The cited paper (per line 273) is Ni et al. ""Justifying Recommendations using Distantly-Labeled Reviews and Fine-Grained Aspects"" which in turn seems to be an publicly available review dataset provided by Yelp. Please use appropriate language as the current writing leads readers to think that AlphaRec is an industrially deployed system. Please refer to industrial papers (e.g., KDD ADS track papers [2, 9, 11, 12]) for how to describe testings done on publicly available industrial sampled datasets (like Yelp), vs real deployments. 

- Misc: Contrastive loss is widely used and should not be viewed as a contribution of AlphaRec. See [15, 11] etc.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper states that LLM encodes collaborative signals that make it easy to connect language representation space with an effective recommendation space. Thus, it proposes an effective collaborative filtering model AlphaRec that takes as input only the transformed LLM representations of textual descriptions of items and is trained by InfoNCE loss and graph neural networks. The proposed method outperforms traditional ID-based models and other LM-enhanced methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well-written and easy to follow. 
2. The paper conducts extensive experiments validating the effectiveness of the methods and proves the validity of the design through ablation study and anlysis.
3. The proposed method exhibits significantly good zero-shot recommendation performance

Weaknesses:
1. One of the most important motivations of the work is that the paper declares large language models encode collaborative signals which indicates the advantage of using representations of large language models for recommendations compared to id embeddings. However, how the preliminary experiments prove this point is insufficiently discussed in the paper. Advanced large language encodes more semantics of the textual descriptions and thus yields better performance. Why this alone doesn't fully explain the performance gain of LLMs should be more explicitly discussed in the main paper.
2. The novelty is limited. Using semantic embeddings of items has been widely used in recommendations. The novelty mostly lies in using the representations of large language models and the implementation details of how to make it effective when combined with traditional recommendation frameworks like non-linear transformations.
3. The paper states that language representation-based methods have low training costs. Still, if taking into account the costs of generating language representations, the computational cost is much higher than ID-based methods.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes AlphaRec, a novel method to incorporate both knowledge from pre-trained language models and collaborative signals. Authors firstly reveal the advantages brought from pre-trained embedding model, and then propose three modules within AlphaRec. An MLP layer to transform pre-trained embedding to item-representation. A graph convolution to aggregate neighbor’s information, and the InfoNCE loss to train introduced parameters within the MLP for each dataset. Overall, the novelty of this paper lies within exploration of NLP encoded embedding on RecSys. The graph convolution and InfoNCE loss are already widely used techniques.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. A good exploration on new direction (language-representation-based) RecSys
2. Experiments are conducted from different angles for analyzing their model.

Weaknesses:
1. Insufficient baselines.
2. Uncleared model name definition.

Limitations:
See Weakness.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes AlphaRec, an LLM-based recommender system that utilizes language representations of item textual data for recommendations.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ Investigating ID paradigm and LLM paradigm is important.
+ The method is simple but seems to be effective.

Weaknesses:
- In this paper, what most confuses me is the usage of the terminology ""collaborative filtering"" throughout the paper. In traditional recommender system, collaborative filtering information means the interactions among users and items. The authors find that using LM as feature extractors to get user/item embeddings from meta-data can achieve similar results as if CF is used for recommendation. However, this seems to be fundamentally different than LM has the ""collaborative information"", as for most online service platforms, the interaction data should be confident and open source LMs won't be able to train on that data. Therefore, the main claim in the paper seems questionable.

- It would be beneficial if we could have results on more diverse datasets.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Vn0FWRImra;"REVIEW 
Summary:
The authors consider stochastic bandit submodular maximization. For this problem, there are two known regret upper bounds: a $\sqrt{T}$ regret upper bound with large coefficients obtained by naively considering the problem as the multi-armed bandit problem, and a $T^{2/3}$ regret upper bound with a relatively small coefficient obtained by the well-known greedy maximization procedure. This paper clarifies that the minimax regret of stochastic bandit submodular maximization can be roughly characterized by the minimum of these two bounds. Additionally, the paper demonstrates that this minimax regret can be achieved by a UCB-based algorithm.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
In online submodular maximization with bandit feedback, the focus has often been on achieving a $T^{2/3}$ regret guarantee with reasonable coefficients, and focusing on the $\sqrt{T}$ regret upper bound obtained by naively applying MAB is new. To my knowledge, this paper presents the first lower bound for bandit submodular maximization, which is an important contribution to the community. Instead of using the commonly employed approximation regret in bandit submodular maximization, it uses regret defined based on the comparator determined by the greedy algorithm, which is somewhat appealing. Although the paper gives an impression of not being thoroughly proofread (see Weakness), it is overall easy to read.

Weaknesses:
One of the major contributions of this paper is the lower bound, but its explanation could be improved. 
While an intuitive explanation is provided from Lines 168 to 172, it is unclear why regret minimization algorithms (such as ETC and the ""standard"" MAB algorithm) appear in the discussion of the lower bound. 

The authors are expected to provide a more detailed explanation of existing algorithms in Section 3. The current explanation lacks clarity on how $\hat{f}$ is computed.

The regret definition involves a comparison with $S^{k,\mathbf{0}}$. Although the authors state that comparing with $S^{k,\mathbf{0}}$ may be impossible in a noisy setting, can the authors present a lower bound for this statement?

In Section 1.2, the authors mention the use of the Lovasz extension for online submodular maximization. Isn't this technique used for submodular minimization?

Additionally, the paper overall gives the impression of not being thoroughly proofread. To highlight some minor points:
L120-122: [22] in Theorem 4.1, [24] in Theorem 1, .... -> Theorem 4.1 in [22], Theorem 1 in [24], ....,
L160: Showed -> showed, L232: abbreviation ETC is not defined, Sec3: There are $S^{i}$ and $S^{(i)}$, which are confusing, L268: UBC -> UCB.

Limitations:
not appicable

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors investigate lower bounds for regret for combinatorial multi-armed bandit (CMAB) problems under submodular rewards and bandit feedback.   For stochastic environments, there has been an open question about the gap between $O(\sqrt{T})$ dependence typically seen in MAB problems (and submodular CMAB with extra feedback) and the $O(T^{2/3})$ dependence typically achieved using explore-then-commit (ETC) algorithms for bandit feedback in past works.  The authors propose a related but different notion of regret then used in previous works and show this regret does indeed have a $\Omega(T^{2/3})$ dependence.  Their formula generalizes the ``small’’ horizon $T$ regime studied in those past works and identify a minimax bound that interpolates $\Omega(T^{2/3})$ for small $T$ and $\Omega(\sqrt{T})$ for large $T$ (i.e. essentially large enough that separate super-arms can be treated as arms in a standard MAB algorithm).  The authors include experiments on toy functions to illustrate the interpolation.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
### Major
- This paper is among the first to tackle lower bounds for combinatorial (cardinality constrained) MAB with monotone submodular rewards and bandit feedback in stochastic environments.  
    - To do so the authors argue that a notion of regret based on nesting of near-greedily selected sets, which the authors point out was implicit in the proofs of several prior papers (all of which adapted the same classic greedy approximation algorithm from the offline setting), should be used instead of the $1-1/e$-regret that has been used in the past.  
    -  For this alternative (greedy) regret, the authors prove a $\Omega(T^{2/3})$ lower bound (for the so called “small $T$” regime), strongly suggesting that there is indeed a hardness gap between CMAB problems with bandit feedback and other studied classes (standard MAB, linear CMAB, even submodular CMAB with semi-bandit feedback).  
  - The authors also propose an algorithm with matching upper bound regret.  The algorithm basically adapts the greedy algorithm (like previous ETC algorithms had) for some of the horizon and then switches to UCB-style over nesting super-sets)

### Minor
- The lower bounds and algorithm proposed is not only for the so called “small” $T$ regime but for large enough $T$ that one can sample super-arms as regular arms in a standard MAB algorithm to get $\sqrt{T}$ dependence.    The lower bound and their matching algorithm identify the trade-off (basically to what cardinality $i^*$ does one follow the greedy before switching to treating all (nesting) cardinality $k$ superarms as standard MAB arms).
- The authors include experiments with two classes of toy functions to illustrate the trade-offs between ETC style algorithms (designed in prior works for “small” $T$) and UCB style (known to get $\sqrt{T}$ regret bound for very large $T$).

Weaknesses:
### Major
- [21] proved $\Omega(T^{2/3})$ lower bounds for the adversarial setting under a slightly different feedback model (though is relevant for any explore-then-commit strategies) and for a general class of problems (including submodular CMAB as a special case) which is not discussed.


### Minor
- The experiment section does not clearly specify $n$ or $k$ or $i^*$ for the problem instances considered.  In the main text, for $n$, there is only a mention in line 263 only for the weighted set cover “For $n=15$, we use…” but that makes it sound like you are using multiple values of $n$.  Since $i^*$ is described as a “critical quantity” (line 231), its value should be transparent in the experiments. Only Figure 1’s caption, which is for the submodular cover, clearly mentions $n$ and $k$ (without mention of $i^*$’s value).
- I think more discussion on what ``small’’ entails for some potential applications would be valuable.  In line 169 the example of $T= O(n^4)$ is given.  In the experiments even for a small $n$ and $k$ for toy experiments it is around $T$ is in the millions to billions that there is a tradeoff, though would such large $T$ be plausible for problems mentioned in the introduction as motivation?  Would the rewards remain i.i.d. for each super-arm over such a large horizon?


### Very Minor
- for the summary 127-133, this is pointing out that for instances of a special sub-class which have a strictly better $\alpha=1$ worst-case approximation guarantee, the $\alpha$-regret of the larger class with provably harder instances behaves weirdly (even giving negative regret values).  That is an important observation, but $\alpha$-regret bounds are for a class of problems, arising from the worst cases, and there are harder cases for cardinality-constrained submodular optimization than modular functions, where the $(1-1/e)$ approximation coefficient comes from.  
- line 80 missing parentheses
- line 268 “UBC”
- line 287 ‘were’--> ‘where’
- In the experiments, 1-Gaussian noise seems strange given the functions are bounded in [0,1].
- Figure 2 legend use power of 10 notation, or add commas
- I think the intro may flow better if you open with problems that are submodular and mention they are important, then say people have been trying to solve them but it is unclear the extent that current methods are the best that could be had

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the problem of stochastic submodular bandits. The main contributions are twofold: it provides a new lower bound for the problem and proposes a novel UCB-type algorithm whose regret matches this lower bound.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper introduces notation clearly and provides a sufficiently thorough comparison with related work.
- The lower bound proof is new. While it adheres to the standard framework for proving regret lower bounds in stochastic bandits, the challenging aspect lies in constructing the problem instance. The problem instance constructed is original.
- The proposed algorithm showcases an interesting insight on the hardness of the problem, even though it combines well-known approaches for online submodular maximization.

Weaknesses:
- Sub-UCB is a modified version of the algorithm in Nie et al. (2022). While this algorithm has a lower regret upper bound, the algorithm itself is an incremental change compared to prior work.
- The lower bound is demonstrated to hold for the minimax expected regret $R_{gr}$, which differs from the $\alpha$-regret commonly used in the literature. While the former is shown to be an upper bound for the latter, it can potentially be much larger. Consequently, a lower bound for $R_{gr}$ does not necessarily imply a corresponding lower bound for $R_\alpha$. Therefore, I believe it is not sound to argue for the minimax optimality of the problem at hand based on this specific change in the regret notion adopted and analyzed.
- It seems that the lower bound result needs to be conditioned on a harsh prerequisite that T needs to be very large. For example, when $k=10$, T needs to be at least $10^11$ for the lower bound to hold.

Limitations:
See Weakness and Questions.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the stochastic submodular bandit problem. In this problem, the decision-maker needs to select a subset with size at most k from a known ground set, and then the decision-maker gains a stochastic reward associated with the subset. The expectation of the reward is a submodular function. This problem is a natural extension of combinatorial linear bandit which relaxed the linearity assumption of the reward function. 

This paper mainly focuses on the lower bound of the submodular bandit problem, which is a recognized open question. Different from other literature of submodular bandit, this paper defines a new notion of regret which is called robust greedy regret. Roughly speaking, the new regret compares the algorithm gained reward with the reward gained by the output subset of an offline “approximated” greedy algorithm. The authors prove a lower bound of their defined notion of regret. This lower bound changes from $kn^{1/3} T^{2/3}$ to $n^k T^{1/2}$ as the size of T, and this results implies that $T^{2/3}$-type regret is inevitable when T is relatively small(polynomial to k and n). 
This paper also proposed an algorithm that nearly match the lower bound. All their results are in the sense of the new defined “robust greedy regret” rather than commonly used γ-regret.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
The major contribution of this paper is the lower bound. Even though the first algorithm for (adversarial) submodular bandit has been proposed for 17 years, we do not have any regret lower bound now. Given this fact, I think a lower bound result on this problem should be extensively encouraged. 

Back to the previous algorithms for submodular bandit (both stochastic setting and adversarial setting), all these algorithms only achieve $O(T^{2/3})$ regret. The main reason is that these algorithms must clearly distinguish whether the current round is an exploration round or an exploitation round. The technique in this paper somewhat explains the intuition behind that. That is, the small size subset will bring more information but cost large regret, and the full-rank subset incurs low regret but bring less information. This naturally divide the subset with different size to exploration action and exploitation action. In this sense, I think this paper do give some intuition on the hardness of submodular bandit and this difficulty also suggests that the submodular bandit is completely different in nature from the linear bandit.

Overall, this paper provides me important knowledge and I believe it will also bring new knowledge to the people who are working on submodular bandit.

Weaknesses:
The main weakness of this paper is that the regret lower bound is in terms of the new defined notion of regret—“robust greedy regret”. However, almost all papers about submodular bandit or more generally combinatorial bandit are considering the notion of a scaled “$\gamma$-regret”. As the authors state, robust greedy regret is always larger than gamma regret, so a lower bound of robust greedy regret does not imply anything about the lower bound of $\gamma$-regret. Also, the major difficulty of proving the lower bound of $\gamma$-regret is “the immediate regret can be negative”, which does not exist in traditional 1-regret. However, by defining the notion of robust greedy regret, the authors can avoid this difficulty. The robust greedy regret lower bound is actually a traditional 1-regret lower bound in the hardcase authors have constructed. So I think this result is still quite far away from figuring out the hardness of $\gamma$-regret.

As the author states, $\gamma$-regret may not be a good metric of submodular bandit problem or other combinatorial bandit problem. But the robust greedy result depends too much on the form of offline algorithm. Also, not all algorithms for submodular bandit are based on the greedy algorithm. For example, the algorithms in [1][2] are totally different from the offline greedy algorithm and they are also not based on the “offline algorithm to online” style. So it’s even hard to define a robust ”xxx” regret. These papers are also missed in the related literature. Thus, I think it’s inappropriate to state “$\gamma$-regret is not appropriate for this problem”. At least, $\gamma$-regret provides a way to evaluate all-type algorithm rather than restrict the algorithm in the category of “offline to online conversion”.

Please note, even if I proposed this weakness. I still appreciate the author's contributions to the lower bound results and I think the contribution of this paper outweighs the weaknesses.

[1] Zongqi Wan, Jialin Zhang, Wei Chen, Xiaoming Sun, Zhijie Zhang, ""Bandit Multi-linear DR-Submodular Maximization and Its Applications on Adversarial Submodular Bandits"", ICML 2023

[2] Stephen Pasteris, Alberto Rumi, Fabio Vitale, Nicolò Cesa-Bianchi, “Sum-max Submodular Bandits”, AISTATS 2024.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors of this paper study the submodular maximization problem with bandit feedback. By adopting regret as the metric, the authors prove the minimax bound of the regret for their problem. A UCB-based algorithm is devised to tackle the submodular optimization problem and the authors prove that this algorithm's regret almost matches the minimax bound. In sum, this paper provides some new and solid theoretical results for submodular optimization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1. The authors prove the minimax regret bound of the submodular maximization problem with bandit feedback. 

S2. The authors also devise the algorithm Sub-UCB that can match the minimax bound up to logarithmic factors.

S3. The theoretical results in this paper seem correct.

Weaknesses:
W1. There are some typos in the paper. For example, in the abstract ""we prove, we prove that"".

W2. The experiment is not interesting as it is a pure numerical simulation. What about applications of submodular optimization such as influence maximization where the objective can only be obtained via sampling and is uncertain?

W3. Instead of (1-1/e)-regret, the authors adopt regret as the metric for the reason that (1-1/e)-regret may be loose. Although the greedy solution's approximation is 1-1/e in theory, in practice the approximation ratio can often be close to 1. For example, for the modular objective case mentioned by the authors, greedy-based algorithm is actually optimal. Therefore, the justification of using regret rather than (1-1/e)-regret is not that strong.

Limitations:
W1, W2, W3

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
AO5MjDuHpr;"REVIEW 
Summary:
This paper proposes Tree of Attributes Prompt learning (TAP). Unlike previous works that rely on unstructured class descriptions, this approach distillates structured knowledge graphs associated with class names from LLMs. Text/vision prompts and vision-conditional pooling module are designed to extract instance-specific text features. Extensive experimental results demosntrate its improved performances.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Overall, the idea of distillating structured knowledge from LLMs in the task of prompt learning is new and interesting.

- The paper designed an effective prompt learning framework to capture fine-grained attributes, using vision expert tokens and vision-conditional pooling layer.

- The illustrated way to generate structure tree of attribute from LLMs can also be used in other tasks.

- From the experiments, using structured knowledge leads to better performances than unstructured descriptions in base-to-novel and few-shot classification tasks.

- The visualization of class activation maps and attention weights look good. The paper is well written and easy to follow.

Weaknesses:
- Apart from the new framework, the method highly relies on the quality of tree of attribute generated with GPT-3.5-turbo. There is no study on the robustness aganist different LLMs, different generation prompts, or varying attribute sets.

- The loss includes a model regularization and its effectiveness is not discussed.

- In Figure 2, it is not too clear to me about $I_1 T_1$, $I_2 T_2$,etc. They seem not be discussed in the text parts.

Limitations:
One limitation is its reliance on LLMs (GPT) to generate the tree of attribute. When generating more complex responses, it is challening to ensure the quality and variances. How to keep a balance between the diversity of attribute sets and relevancy of attributes to classification is important.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new method called ""Attribute Prompt Learning Tree (TAP)"" to improve the performance of CLIP on zero-shot and few-shot classification tasks. The authors leverage large language models (LLMs) to generate more descriptive text prompts and introduce a hierarchical tree-like structure to systematically generate and integrate these descriptions, ensuring a layered and comprehensive understanding of the visual content. The method also learns specialized ""domain expert"" prompt tokens that focus on different visual attributes and uses a vision-based pooling module to extract text features for specific instances. Extensive experiments show that TAP outperforms state-of-the-art methods on zero-shot and few-shot classification tasks across multiple datasets

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1), The idea that utilizing LLM to generate tree-like prompts makes sense. This structured description approach is significantly different from the existing simple text prompt methods and provides an efficient way to improve VLMs.

2), The image-conditional pooling module looks like good for capturing instance-specific features.

3), Experiments and visualization demonstrate the effectiveness of the proposed model.

Weaknesses:
1), TAP introduces many textual and visual prompts, which leads to high computing and time costs. This may limit its applications.

2), TAP first generates hierarchical token prompts, while it seems like TAP does not use such a hierarchical structure to integrate the output of the text encoder. It only uses a pooling strategy to update the text encoder output with the visual feature. That is, TAP also does not utilize these relationships in the prompt graph.

3), TAP can be viewed as a multimodal prompt tuning method. What is the main difference between TAP and MAPLE,  ALIGN.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a method that aiming to align the vision modality with not only the category name but also the whole concept subgraph the noun represents in the knowledge graph. This is achieved by adding a bunch of attributes branches attached to this concept. The authors argue that this integration of attribute knowledge will make the alignment more transferrable and thus result in a good performance boost in terms of zero/few shot results.
Basically, this work focusing on the topic of textual prompt enrichment task that is investigated before but implement in a different manner. Additionally, the proposed method use seperate tokens to learn different aspectrs of attributes of given images, working as 'domain expert'.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Might be the first work trying to align the vision image with structured data. It is quite interesting considering that most text prompts now are less organized and noisy. And structured data, as pointed out in the recent research of LLM, may lead to better reasoning skill for a foundation model.
2. The proposed vision-conditional pooling can help the model filter out descriptions that are not direct appeared in the image.
3. Recieve good results on different classification datasets with the model trained with this method.

Weaknesses:
1. The attributes description is generated by the LLM, which could contain hallucinated content. While there are many reliable sources of knowledge such as wikipedia or conceptNet, this paper seems skip these sources to obtain some accurate attributes.
2. Though this paper decide to use a tree structure to represent the concept. The built tree is not encoded in a structure-awared manner. They are still feed as langauge tokens to the LLMs. 
3.  in equation (5), what is $v_y^a$ stands for? 
4. The author argued that the vision-conditional pooling, which is bascially a cross attention layer between the visual and language modal. The authors believe this this design will make the model filter out non-exisiting material in the text description. However, we know that due to the quirk of softmax function. You can never make some tokens attention to be '0'. Thus, the model is learning some spurious correlation aftertall.

Limitations:
Not applicable.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The TAP method structures textual descriptions in a hierarchical “concept-attribute-description” format, effectively creating a knowledge graph from large language models (LLMs) for each category name. This structure allows for a more comprehensive and detailed understanding of the visual content. The paper reimagines learnable prompt tokens as ""domain experts,"" each specializing in different aspects of the image, supplemented by a global perspective provided by the CLS token. To address potential misalignment between general descriptions and specific image content, the paper introduces a vision-conditional pooling module. This module extracts instance-specific text features, ensuring optimal image-text alignment.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed method incorporates structured tree of attribute into prompt tuning that provide richer supervisory information compared to unstructured attribute information. A set of experiments has been conducted, and the results look promising.

Weaknesses:
One major limitation of the method is that it requires human review to ""ensure the quality of the example"" (L175). Recall that one major advantage of prompt tuning is that it can adapt large models quickly to specific tasks. However, the requirement of human reviewing in the proposed method is not consistent with this goal. In addition, it is not clear how many human efforts are needed here, and how to handle the potential human bias in quality evaluation. 

The paper lacks cross-dataset experiments, which is typically provided in existing PT papers. The results are important to examine the domain generalization capability of the method. 

For training details, different learning rates were used for different datasets, however, existing methods typically use a same LR for all datasets. From this point, the comparison is somewhat unfair.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new prompt tuning method for adapting the vision-language model.  The authors design the tree of attribute prompt learning to substitute the categorical description for adapting the vision-language model. A vision-conditional pooling module is proposed to extract instance-specific text features. Extensive experimental results demonstrate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. A tree of attribute prompt learning method is proposed to guide the adatpation of VLM with the hierarchical semantic information.

2. This paper is well-written and easy to follow.

Weaknesses:
1. According to the experiment, the performance improvement of TAP is marginal, e.g., the few-shot performance on most of datasets.  Although the visualization results of VCP layer are impressive, the improvement of this module is also very slight compared to average pooling. 

2. The core motivation of this method is learning fine-grained attributes to adapt VLMs. However, similar ideas have been explored in previous works , e.g., APPL[1], MAP[2].  Please discuss the differences.

[1] AAPL: Adding Attributes to Prompt Learning for Vision-Language Models

[2] Multi-modal Attribute Prompting for Vision-Language Models

3. The construction of ToA depends heavily on the prior information on the category of attributes suitable for the dataset. However, one of the most capability of VLM is its zero-shot ability in the open-vocabulary context. What's the performance of the proposed method in the domain generalization setting?


4. The model details in Figure 2 are not presented very clear, especially the input & output streams. This figure should be refined for better clarity. 


5. The mechanism behind Equation (5) and the function of VCP needs more clarification. Why conduct constrastive learning between expert token P_a^v and attribute embedding v_c^a generated from P_a^v itself, instead of P_a^v and the embedding of attribute descriptions D?

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
JNDcFOczOf;"REVIEW 
Summary:
This paper studies preference-based RL (PbRL) where instead of the expected return, the agent optimizes a risk measure based on preference feedback. The authors study two settings called ""iterated"" and ""accumulated"" quantile risks, otherwise known as nested and static risks. They provide sublinear regret bounds for both approaches and instantiate a hard-to-solve MDP to establish a lower bound.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed research is well-motivated, as it combines the accessibility of preference-based feedback with risk-sensitive RL, which is crucial for safety-critical applications. 

If correct, the contributions are meaningful, as they englobe both nested and static risk measures. 

The text has a nice flow. I have to say that I reviewed a previous version of this submission and the structure is much clearer now.

Weaknesses:
- Some used terminologies are non-standard for the risk-sensitive RL (RS-RL) community. For example, the authors discuss ""iterated"" versus ""accumulated"" risk measures. As far as I know, most of the RS-RL works name these ""nested"" versus ""static"" risk measures [1,2,3]. This confused me, as I could not understand the abstract until the middle of the introduction. 

- line 62: ""the optimal policy becomes history-dependent, which is more general than assuming the trajectory reward is a linear function of the sum of per-state features"". Why? This deserves more explanation, as it justifies the novelty of this work compared to Chen et al (2023). 

- Previous works have studied RL under trajectory feedback, although not in the PbRL setting, see e.g., [4]. Therefore, contribution 3. seems overstated. 

[1] Hau, J. L., Petrik, M., & Ghavamzadeh, M. (2023, April). Entropic risk optimization in discounted MDPs. In International Conference on Artificial Intelligence and Statistics (pp. 47-76). PMLR.

[2] Hau, J. L., Delage, E., Ghavamzadeh, M., & Petrik, M. (2024). On dynamic programming decompositions of static risk measures in Markov decision processes. Advances in Neural Information Processing Systems, 36.

[3] Tamar, A., Chow, Y., Ghavamzadeh, M., & Mannor, S. (2015). Policy gradient for coherent risk measures. Advances in neural information processing systems, 28.

[4] Efroni, Y., Merlis, N., & Mannor, S. (2021, May). Reinforcement learning with trajectory feedback. In Proceedings of the AAAI conference on artificial intelligence (Vol. 35, No. 8, pp. 7288-7295).

Limitations:
Although some limitations are not addressed in this work (see previous remarks), most are properly described in Sec 6.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies and proves the applicability of two risk-aware objectives to Preference-Based Reinforcement Learning (PbRL), i.e., iterated and accumulated quantile risk objectives. The authors design an algorithm called Risk-Aware-PbRL (RA-PbRL), which can optimize both iterated and accumulated objectives. Furthermore, the authors provide a theoretical analysis of the regret bounds. The results demonstrate that the regret bounds of algorithm RA-PbRL under both the iterated and accumulated objectives are sublinear with respect to the number of episodes. Finally, the authors present empirical results to support their theoretical findings.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.	The studied problem, i.e., applying the iterated and accumulated risk-aware objectives to PbRL, is relevant and useful for some applications such as healthcare and AI systems. The considered reward model depends on the feature of the trajectory, instead of the feature of the state-action pair, which is more general than the prior works.
2.	The authors consider both the iterated and accumulated risk-aware objectives, which encompass the popular CVaR objective. In addition, the authors also design an algorithm and provide regret analysis for both objectives. The algorithm design and theoretical analysis are well executed.

Weaknesses:
1.	The proposed algorithm is very straightforward, which seems to simply combine the confidence set construction and the risk-aware objective. This algorithm is computationally inefficient and hard to implement in practice. Can the authors explain more on how to implement this algorithm?
2.	What is the intuition behind the factor $\min_{\pi,d} \omega_{\pi}(d)$ in Theorem 4.1? In particular, why the probability that the feature is not zero will influence the regret? More discussions on the regret due to reward estimation are needed.
3.	It seems that $L_G$ appears in the upper bound (Theorem 4.2) but not in the lower bound (Theorem 4.4). Why the authors said that “it demonstrates the significant impact of LG” below Theorem 4.4? In addition, it seems that $dim_{T}$ appears in the lower bound, but not in the upper bound. Can the authors explain more on it?
4.	This paper needs careful proof-reading. There are many typos. For example, the factor $\min_{\pi,d} \omega_{\pi}(d)$ in Theorems 4.1 and 4.3, and the $O(…)$ notation in Theorems 4.1 and 4.2. In Line 130, “use” should be “used”. In Line 197, the “and” should be moved to the front of “$V^{\pi}_i$ in Eq.5 …”?

Limitations:
Please see the weaknesses above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper incorporates risk-awareness into Preference-based Reinforcement Learning (PbRL). Specifically, it tackles the issue that under PbRL, the reward is episodic, meaning that it can only be computed on full trajectories. The authors adapt both iterative and accumulated quantile risk objectives to deal with episodic rewards.

Additionally, the paper presents an algorithm (RA-PbRL) to incorporate these objectives into PbRL.

Lastly, the authors provide regret boundaries for both iterative and accumulated quantile risk objectives with RA-PbRL.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
[Quality, Clarity] The paper is well written (particularly the introduction). As far as I could follow the mathematical development is robust, with regret upper- and lower-bounds being established for both types of risk considered.

Weaknesses:
*  **W1** [Quality]: As the manuscript already mentions, the experimental setting is very simple. It would have been interesting to train more complex settings, particular from actual human preferences.
 *  **W2** [Significance]: I think more could be done to stress the important of risk-awareness in the PbRL setting. After reading the paper, it was still not clear to me which applications would benefit from RA-PbRL.

Limitations:
The limitations have been correctly addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper focuses on the theoretical analysis of risk-aware preference-based reinforcement learning and introduces Risk-Aware-PbRL (RA-PbRL) to optimize both iterated and accumulated risk-aware objectives.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper proves that both iterated and accumulated quantile risk-aware objectives can be applied to one-episode reward settings. This may provide a theoretical foundation for future episodic RL or PbRL methods focusing on risk-related objectives.
- This paper provide analysis of regret guarantee on the proposed risk-aware algorithm.

Weaknesses:
- I think it is necessary to clearly clarify all the strong assumptions, not only the linear reward function assumption, as these strong assumptions typically cannot be met in real-world control scenarios. This would make the theoretical results in this paper more applicable and useful for researchers using deep PbRL methods to address real-world control problems.
- Why is the regret of RA-PbRL higher than PbOP when $\alpha$ is small (Fig. 1(b) and 2(b))? Can the authors provide an intuitive explanation for this phenomenon?

Limitations:
None.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
AthZ2g6VE2;"REVIEW 
Summary:
This paper proposed LoCoDL, an algorithm than combines communication compression with local training. The authors proved the convergence results under regular assumptions, achieving comparable rate with existing SOTA algorithms. The experimental results also show that LoCoDL behaves best among tested algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The combination of communication compression and local training is novel.
2. The convergence results achieves SOTA for large $n$ and nearly SOTA for small $n$.
3. The algorithm behaves empirically better than ADIANA, an existing theoretically SOTA algorithm.
4. The algorithm is simple.
5. The target problem setting is novel and general.

Weaknesses:
1. Throughout the four experimental settings, the number of nodes, $n$, is comparable to the feature dimension $d$. As the convergence rate of LoCoDL is suboptimal when $n$ is small, I believe it important to compare LoCoDL with ADIANA when $n$ is at least 10$\times$ or 100$\times$ smaller than $d$ to see whether LoCoDL beats ADIANA in these scenarios.
2. The experimental datasets are relatively small. It's recommended to conduct experiments on MNIST or larger datasets.
3. It is not easy to capture the intuition behind each algorithm line. It's recommended to give more detailed explanations on how the algorithm is developed.

Limitations:
As stated in the conclusion part, the algorithm is limited to single-directional, deterministic setting without partial participation.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes LoCoDL, a new GD-based distributed training algorithm that employs both communication compression (CC) and local training (LT). It achieves double acceleration and a SOTA convergence rate for strongly convex problems.

A crux of the algorithmic improvement is maintaining two local estimates, intuitively enabling efficient LT (similarly to SCAFFOLD) and efficient CC (i.e., compressing values' differences instead of values themselves).

The paper offers a thorough theoretical analysis that proves the main claim and conducts some experiments that show LoCoDL's benefits compared to previous algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is timely and important, and I enjoyed reading it. It appears to set a new bar for distributed communication complexity in the strongly convex case (and with full participation?).

While some works assume similarity between local client functions, this work allows these functions to be arbitrarily different.
Also, interpreting the added term $g$ is intuitive and compelling (viewpoints 1-4). 

The theoretical claims are rigorously proven, and some experiments demonstrate the efficiency of LoCoDL compared to previous techniques.

Weaknesses:
The practical applicability of LoCoDL is unclear. Namely, it does not apply to NNs and possibly is less efficient for partial participation use cases (this part is unclear). Either providing concrete evidence of why this contribution is important for modern practical use cases or slightly rephrasing the paper as a theoretical (and important) contribution would strengthen the claim.   

Strengthening the evaluation section is also advised. The submission would be strengthened if the author provided an experiment other than logistic regression to demonstrate the efficiency of LoCoDL in another task.

Additional points: 

1.	“are smooth,  so their gradients will be called. “ This sentence is unclear.

2.	“is slower than  broadcasting the same message to an arbitrary number of clients.” Are there any real FL systems that employ broadcasting? Or do you mean sending the same message? (The term “broadcast” may be confusing here.)

3.	“In this work, we focus on the  uplink communication complexity, which is the bottleneck in practice.“ The second part of the sentence should be softened or extended with real evidence that this is the case. 

4.	“No other compressor can be used, which notably rules out any type of quantization.” Why is this the case? Why quantization cannot be applied according to the selected pattern? 

5.	“Instead of the cumbersome permutation-based compressor of the latter.” is there a specific challenge in implementing permutation-based compressors? Maybe it's worth specifying specific setups where this is insufficient or cannot be applied.

6.	“Thus, LoCoDL sets new standards in terms of communication efficiency. “ Do you mean: our experiments indicate that...?

7.	What is the communication complexity with partial participation (PP) (i.e., $\rho=1$)? When considering PP, is LoCoDL the current SOTA, or are there better alternatives? 

8. It seems that some elements of LoCoDL have some similarities to DoCoFL [1], which also uses an anchor for the model that allows clients to obtain only a compressed correction to that anchor. Can the authors shed light on this similarity?

[1] Dorfman, Ron, et al. ""DoCoFL: Downlink compression for cross-device federated learning."" International Conference on Machine Learning. PMLR, 2023.

Limitations:
The paper does not have a dedicated limitation section. The conclusions section discusses potential future extensions. Outlining the limitations clearly is advised. For example, is the PP use case relevant here?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an algorithm (LoCoDL) that leverages two well-known methods of local training. It reduces the communication load in distributed learning.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper addresses the interesting problem of distributed learning.

Weaknesses:
1. Generally, compressing the error and feeding it back to the updates is a well-known technique to reduce the variance in distributed learning. The idea of the algorithm is marginal with respect to the previous known algorithms (feeding back the error and aggregating with proper coefficients). 
2. Also, the experiments should include the accuracy versus iteration (or time) to see after how many iterations (or how much time), the performance shown in Figure 1 is achieved. So, there are lots of work to improve the experimental part.

Limitations:
justification, experiments

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
VXxj3XZ1X8;"REVIEW 
Summary:
This work demonstrates that overparameterized neural network models, which have many non-unique solutions, can lead to inconsistencies in representing the mouse visual cortex. It suggests a novel approach called ""adaptive regularization,"" where the regularization parameters in the loss term are learnable rather than fixed. The study also examines other approaches, such as normal regularization and pruning, and finds that these methods are beneficial for improving representation consistency. The proposed method significantly enhances consistency. This contribution is significant to the computational neuroscience community.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
1. This work addresses a critical problem in computational neuroscience: reproducibility. It proposes solutions to mitigate this issue, which have significant impact on the computational neuroscience community. 

2. The authors rigorously define the consistency of computational models and thoroughly examine how regularization strength affects neuronal properties.

Weaknesses:
1. The study is limited to only one type of model, the Rotational Equivariance CNN.

2. In the Methods section, the terms ""Embedding"" and ""Mask"" are not defined. Moreover, Lp and L1 are described as functions of model parameters, but no parameters are defined in the text. This lack of clarity poses an issue for understanding. For instance, Lp could be interpreted in two different ways: as a function of the core but not the readout, or as a function of both the core and the readout. The same ambiguity applies to L1.

3. There are two types of computational neuroscience models regarding their outputs. The first type is task-optimized models, whose outputs are task-related, such as object class, object representation, or action. These models are trained to perform downstream tasks, such as supervised object classification, reconstruction, or playing games [1,2,3]. The second type is neural response fitting models, whose outputs are predicted neural responses, and they are trained to predict these responses directly. However, the reasons why the authors chose response fitting models over task-optimized models are not mentioned.

Reference

[1] Performance-optimized hierarchical models predict neural responses in higher visual cortex

[2] Unsupervised neural network models of the ventral visual stream

[3] Using deep reinforcement learning to reveal how the brain encodes abstract state-space representations in high-dimensional environments

Limitations:
This study investigates the consistency of neuronal properties and the prediction performance of regularized models. However, the models used by the authors are trained to predict neural responses. There is another type of computational model that is trained to perform downstream tasks, with neural-like representations emerging from the training. Future work could consider these latter models.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present a systematic investigation of the use of deep neural network fits to biological neurons as the basis for neuron cell type classification. The authors explore how various factors such as regularization and model pruning can influence both the predictive model fit and the consistency of the neural clustering.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper presents a well-motivated, calculated set of experiments for evaluating how well deep neural networks can be used as models of mouse visual cortex. With increasing interest in this area and increasingly bold claims, work like this is a breath of fresh air. The controlled experiments and attention to detail provides compelling evidence that though DNNs may be our best working computational model of visual cortex, the are likely not a global optimum.

Weaknesses:
Overall, the biggest weakness I think is some lack of explanation. As someone who is very familiar with work in computational neuroscience but not a neuroscientist myself, there were a few things that felt under-explained and it wasn't clear if it was because I'm not the correct audience or because the authors did not provide enough context in the text. The authors do a good job of providing clear motivation for some of the fundamentals (L24-53) but then leave out context when things get a little more technical specialized (i.e. comparisons between readout mechanisms, significance of biological properties and tuning indices, etc) as a result it feels unclear what the reader is expected to know before reading.

- The font in the figures is a little unprofessional. Comic sans? In this economy?
- Figure 2G is hard to parse. I recommend splitting up the histograms
- The intuitive explanation of the factored vs gaussian vs adaptive readouts is not super clear to me. It might be worthwhile to spend more time on it, especially to motivate your proposed adaptive mechanism.
- minor typos (""initialiation"" in L183)
- L216 is nonsensical. I think some words are out of order
- Confidence intervals in Table 1 would be nice to gauge significance. Also, the table format makes it a little hard to visualize. Perhaps a graphical representation would be better.

Limitations:
Limitations are clear.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies models trained to predict the responses of neurons in visual cortex. The model has a shared multilayer network core followed by a final layer which maps the core features into individual neuron responses. The key question that this paper asks is, How reproducible are the individual neuron properties and ""cell-type"" clusterings inferred by such techniques? The main conclusion is that sparsity-inducing regularization is important for finding consistent cell properties. Pruning certain channels in the core is also shown to improve consistency.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
Consistency and reliability of these methods for understanding visual cortical processing is important for the scientific interpretation of these models. The design of the approach seems sound. A number of metrics are used, both scores as well as response properties used in neuroscience.

Weaknesses:
* I found the comic sans font and cartoon diagram style distracting in Figs 1-4.
* It isn't clear that much is gained from the t-SNE plots in Fig 2. Some evidence of different clusters, yes, but it's known that these kind of plots can be deceiving.
* How do you pick the optimal regularization parameters? By what metric/cross-validation procedure? (Line 242 gives $\gamma=10, \sigma=0.1$) Validation performance is reported but I'm unclear of the splitting procedure and whether there was a train/valid/test 3-way split or not.
* Minor points are made in ""questions"" section

Limitations:
The authors mention that their work is limited to a particular type of core architecture. It's also limited to being applied to just one dataset, whereas open data in other animals (or collected by other research groups, e.g. the Allen Institute's work) are available. I am not suggesting the authors do this in their revision but that they mention it as a limitation.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper ""Reproducibility of predictive networks for mouse visual cortex,"" explores the reproducibility of neuronal embeddings in the mouse visual cortex using deep predictive models. By introducing adaptive regularization and iterative feature pruning, the authors address key issues related to model overparameterization and provide a robust framework for achieving consistent functional representations of neurons. The work lays the groundwork for future research aimed at developing reliable and interpretable models for understanding neuronal function. 

The primary goal is to investigate the stability and reproducibility of neuronal function embeddings derived from deep predictive models. These models aim to predict neuronal responses to sensory inputs and have been proposed to define functional cell types via unsupervised clustering. The paper addresses the concern that deep models are often highly overparameterized, leading to multiple solutions that can represent the same neuronal function, thereby questioning the reliability of embeddings for downstream analysis.

The paper demonstrates that L1 regularization, which was used in early models, is crucial for obtaining structured and consistent neuronal embeddings when newer readout mechanisms are used. A novel adaptive regularization scheme is introduced, which adjusts the strength of regularization for each neuron. This method improves the consistency of neuronal embeddings across different model fits while maintaining predictive performance. The paper proposes an iterative feature pruning strategy to reduce the dimensionality of performance-optimized models by half without losing predictive performance. This pruning improves the consistency of neuronal embeddings concerning clustering neurons.  The consistency of neuronal embeddings is evaluated using the Adjusted Rand Index (ARI) of clustering partitions across models, correlations of predicted responses across models, and the consistency of tuning indexes describing known nonlinear functional properties of visual neurons.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Adaptive Regularization Scheme: The introduction of an adaptive regularization method that adjusts the regularization strength per neuron is a significant novelty. This approach helps achieve better consistency in clustering neuronal embeddings without compromising on predictive accuracy.

- Iterative Feature Pruning: The feature pruning strategy to address overparameterization in deep models is another novel contribution. By systematically reducing the model’s dimensionality, the authors enhance the robustness and consistency of the neuronal embeddings.

- Comprehensive Consistency Evaluation: The paper provides a comprehensive evaluation of model consistency across different dimensions (embedding clustering, predicted responses, and tuning curves). This thorough approach highlights the robustness and reproducibility of the proposed methods.

- The paper is well-written with generally sufficient referencing to the previous methods.

Weaknesses:
- Bias from Regularization and Pruning: While the adaptive regularization and pruning strategies improve consistency, they may introduce biases that affect the biological validity of the neuronal embeddings. Over-regularization, for instance, can reduce the model’s expressive power and lead to less biologically plausible representations.

- The study focuses primarily on models with a rotation-equivariant convolutional neural network (CNN) core. The authors acknowledge that other core architectures, such as regular CNNs or Vision Transformers, were not evaluated. This limitation means the findings may not generalize across different model architectures.

- Despite the improvements in clustering consistency, there is a trade-off between consistency and predictive performance. The pruning and regularization strategies, while improving consistency, sometimes result in a drop in predictive performance, which is not ideal.

- The introduction of adaptive regularization adds another layer of hyperparameters (e.g., the log-normal hyperprior parameter) that need to be carefully tuned. This increases the complexity of the model training process and may require significant computational resources.

- The focus on achieving high consistency in clustering and embeddings may overshadow other important factors, such as the interpretability and biological relevance of the model outputs. Balancing consistency with these factors is crucial for developing useful predictive models in neuroscience.

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
6U5fCHIWOC;"REVIEW 
Summary:
This paper proposes a method to upper-bound the generalization gap $G(w)$ (discrepancy between the empirical risk and the theoretical risk for a given parameter $w$ of our model) by tractable _topological_ quantities. 

Namely, given a set $W = \{w_\tau,\dots,w_T\}$ (typically, a sequence of iterates for a SGD between iterations $\tau$ and $T$, where $w_\tau$ is already near a local minima of the empirical risk), the authors derive bounds of the form

$$\sup_{w \in W} G(w) \leq \sqrt{\frac{\text{Topological quantity } + \text{(Information theory quantity)} + \log(\zeta^{-1})}{n}}$$

with probability $1 - \zeta$. The work focuses on building actual topological quantities that make this claim true, proposing two possible candidates: 
- The _$\alpha$-weighted lifetime sums_ that (roughly) computes the maximum spanning tree of $W$ and then assign a weight to it, 
- The _positive magnitude_ that (roughly) computes the positive mass of $\beta = K_s^{-1} \mathbb{1}$, where $K_s$ can be understood as a Gaussian/Laplace kernel on $W$ for some distance $\rho$ and bandwith $s$ (and $\mathbb{1}$ is a vector full of $1$s). 

Contrary to previous similar works, their theoretical results directly hold in (mostly) practical situations: they do not require observing a time-continuous trajectory and the quantities involved are (at least theoretically) directly computable (in practice, this requires approximation). 

Experimentally, the authors observe through an extensive set of experiments that the upper-bound they propose performs better, in terms of correlation with the actual generalization gap, than the other kind of topological upper-bound previously introduced.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The work proposes an original and promising approach to tackle an important question. 

Contrary to most previous works, the experiments go way beyond ""toy-models"", and the results are reported on modern architectures like transformers, showing that the method is usable in practice on ""real"" setups. Experimental results are extensively reported, supporting the reproducibility of the work. 

The presentation of related works / comparison of SotA (with which I am not completely familiar) seems to be quite comprehensive.

Weaknesses:
In my opinion, the main weakness is that the (main) paper is kinda far from being self-contained and it is somewhat hard to get an intuitive grasp on the proposed approach. The appendix (about 40 pages!) contains crucial information (proofs, missing definitions, etc.) that couldn't be carefully investigated (*). 

While the contribution itself is seemingly solid, this prevents me from claiming that the work is theoretically sounded and limit the understanding if one stick to reading the main paper. For instance, if one looks at Theorem 3.4, I cannot understand the role of $\tau$ and $T$ (as far as I can tell, they only appear in $I_\infty$ in the rhs, but since this term is not defined I do not know what happend if, say, $T = \ŧau$ or $T = +\infty$). Similarly, without reading the proofs, I have no clue on why taking $\alpha = 1$ should be natural, what is the role played by the constant $q$ in the $(q,L,\rho)$-Lipschitz assumption, etc. 

I understand that being comprehensive is impossible given the space limitation, but I believe that it is part of the contribution to produce an accessible less-than-10-pages-long presentation of the work. 

(*) Note that I quickly parsed the supplementary material, which I found quite enlightening, but cannot guarantee the correctness of its content.

Limitations:
Limitations have been discussed by the authors, which is appreciated.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Prior work has sought to provably bound the generalization of a neural network based on a complexity measures, eg using some form of evaluation of mutual information between the data and the training path, however such proofs have relied on the topologies from the asymptotic infinite training case and other impractical assumptions. This paper seeks to extend the provable generalization bounds to a practical training regime with discrete time steps. They attempt to make such complexity measures more tractable by, rather than a full derivation of those intrinsic dimension measures utilized in prior works, looking to leverage instead those dependent underlying variables on which intrinsic dimension is based, which are more accesible during training, resulting the the measures of ""alpha weighted lifetime sums"" and ""magnitude"".

In addition to the forms of experimental validation comparing the complexity measures to a resulting generalization gap, the scope of the paper includes extensive theoretical discussions that are slightly beyond the competence of this reviewer to fully evaluate, and so this review should be considered as taking much of these aspects at ""face value"". That being said, relying on assumption of rigor in such aspects, I believe the scope of the paper and implications of the claims would likely merit some form of recognition from the conference, like an oral presentation or spotlight.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
- Originality
I have seen some manner of complexity measures as a form of performance metric discussed in prior work, including things like sharpness measures, however I have yet to see anyone claim bounded forms of generalization guarantees accessible during intermediate stages of training, suggesting that this could be a significant improvement towards such applications.

    - Quality
The paper was ambitious in scope and for the most part appeared dense in significance. 

    - Clarity
Some of the benchmarking was more intuitive as the complexity compared to generalization charts as opposed to the Table 1 for instance, I don't know if that could be simplified in some fashion (sometimes less information is better and save the half page of numbers to the appendix for instance).

    - Significance
Taken at face value the availability of a provable form of generalization guarantee efficiently available in real time during training is potentially quite significant towards mainstream practice.

Weaknesses:
It is hard be this reviewer to fully assess the theoretical merit of the derivations, and hope that some of the other reviewers may be more competent in this sense.

Limitations:
It would be hard to fully validate the claims of ""provable generalization bound"" without some more significant survey of the appendices, which are outside the competence of this reviewer.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
- The authors provided a novel topological-complexity-based uniform generalization error bound, constructed on the $\alpha$-weighted lifetime sums or positive magnitude. This bound shows better correlation with the generalization error compared to existing bounds.
- The authors proposed an implementation scheme based on dissimilarity measures between neural networks, enabling the quantification of generalization across different model architectures without the need for domain or problem-specific analysis.
- The authors confirmed that their topological complexity term exhibits better correlation with the actual generalization gap in large-scale neural networks, such as ViT and GNN.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- More Realistic Generalization Error Bound
  - This paper addresses a significant limitation of previous topological complexity-based generalization error bounds, which only hold under discrete parameter updates based on SGD or Adam, or when the number of iterations is taken to infinity. By overcoming this limitation, the bound provided in this paper offers a more practical and realistic understanding of generalization performance.
  - Additionally, the paper presents a method to compute this complexity for large-scale neural networks such as ViTs and GNNs, enhancing its practical applicability.
- Innovative Combination of Two Interesting Topological Metrics for Generalization Error Bound
  - The authors leverage the fact that the $\alpha$-weighted lifetime sums based on minimum spanning trees, a quantity from persistent homology, are related to a pseudo metric, and apply this to generalization error analysis.
  - They also focus on a topological quantity called Magnitude and its connection to metric spaces, successfully providing another generalization error bound.
  - These achievements are the result of effectively combining techniques from topological data analysis, which goes beyond mere application and demonstrates significant originality.
- Correlation with Generalization Performance Confirmed through Numerical Experiments on Large-Scale Neural Networks
  - The proposed topological complexity is computationally feasible even for large-scale neural networks. This is crucial for understanding the generalization performance of models like LLMs in the future.

Weaknesses:
- Concerns Regarding Assumptions
  - This bound only holds under bounded losses. Thus, as the upper bound B increases, the bound becomes vacuous and diverges under unbounded losses.
  - Additionally, the relationship between the upper bound B of the loss and the topological complexity is unclear, making the overall interpretation of the bound difficult. For instance, under large B, the correlation of the topological complexity might be negated, resulting in a bound that does not correlate well overall. Providing an intuitive discussion on this aspect would help clarify the significance of the bound.
  - I am not well-versed in the assumptions based on pseudo metrics, so I cannot assess the validity of the (1,L,\rho)-Lipschitz continuity assumption. However, as the authors mentioned, this assumption limits the applicability of the bound to certain pseudo metrics like the Euclidean distance or data-dependent pseudo metrics.

- Concerns Regarding Mutual Information in the Bound
  - Although the authors reference literature to assert that the mutual information in their proposed bound is tighter than the information-theoretic quantities in traditional bounds, they do not provide concrete methods for calculating or estimating this quantity.
  - This means that while some terms in the proposed generalization error bound are computable, the bound as a whole is not computationally evaluable. Thus, although the topological quantities used in the bound correlate with generalization performance, this does not guarantee the overall tightness of the bound. In practice, even if the topological complexity decreases, an increase in mutual information could render the bound vacuous.
  - The paper claims that the proposed bound is a uniform generalization bound. Therefore, it is necessary to discuss whether this bound achieves uniform convergence. If the mutual information term can be bounded by a constant, uniform convergence might be guaranteed, but this is not trivial and depends on the behavior of this quantity. Otherwise, the bound might fail to ensure uniform convergence.

- Challenges in Handling Hyperparameters and Lack of Detailed Discussion
  - If I understand correctly, the evaluation of the bound and topological complexity is closely related to the iteration step \tau at which the measurement begins and the scale value s in PMag, both of which need to be determined for the bound to hold. These settings carry the risk of arbitrariness in evaluating generalization error. However, the paper does not provide sufficient discussion on the correlation between these changes and generalization performance (incidentally, there is a notation conflict between Kendall’s coefficients and \tau here). For instance, providing more candidates for s and analyzing the sensitivity of the correlation degree to changes in its value, or verifying the variation in experimental results under multiple starting point candidates \tau, would allow for a minimum level of discussion.

- Lack of Comparison with Other Information-Theoretic Quantities or Metrics Strongly Correlated with Generalization
  - The numerical experiments presented in this study focus solely on topological complexity and compare the contributions of this research with existing studies in terms of correlation with the generalization gap. As a result, the relationship between topological quantities and generalization, and their standing in the context of other discussions, such as generalization based on, e.g., gradient variance (e.g., Jiang et al. (2019)) or mutual information (e.g., Russo and Zou (2016); Harutyunyan et al. (2021); Steinke et al. (2020)), remains unclear.
  - Thus, it remains uncertain how the proposed measure compares to other evaluation metrics in terms of superiority.

Citation:
- Jiang et al. (2019):  Jiang et al. Fantastic Generalization Measures and Where to Find Them. ICLR2020. https://openreview.net/forum?id=SJgIPJBFvH
- Russo and Zou (2016): D. Russo and J. Zou. Controlling bias in adaptive data analysis using information theory. AISTATS2016. https://proceedings.mlr.press/v51/russo16.html
- Harutyunyan et al. (2021): Harutyunyan et al. Information-theoretic generalization bounds for black-box learning algorithms. NeurIPS2021. https://arxiv.org/abs/2110.01584
- Steinke et al. (2020): Steinke et al. Reasoning About Generalization via Conditional Mutual Information. COLT2020. https://arxiv.org/abs/2001.09122

Limitations:
- This paper is a theoretical study aimed at providing a more realistic evaluation of generalization error. The data used in the experiments, such as MNIST, are open-source, which indicates that potential negative social impacts are appropriately controlled.
- While limitations are discussed in Section 6, there appear to be additional potential limitations as mentioned in the questions above. Addressing these would provide a more comprehensive understanding of the research's boundaries and enhance the validity of the study.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper makes significant contributions by establishing new theoretical connections between generalization and topological complexity measures, specifically $\alpha$-weighted lifetime sums and positive magnitude. The authors introduce these novel measures and link them to generalization error using innovative proof techniques.
Experiments show that these measures correlate highly with generalization error across various architectures and datasets. The work offers simpler, less restrictive generalization bounds, removing the need for complex geometric assumptions. These flexible measures are adaptable to different domains, tasks, and architectures, providing practical, theoretically justified tools for understanding and predicting generalization in modern deep neural networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The authors establishes new theoretical connections between generalization and topological complexity measures, specifically focusing on $\alpha$-weighted lifetime sums and positive magnitude. Also, introduces and elaborates on new topological complexity measures, such as the positive magnitude, which is a modified version of the magnitude measure. These measures are linked to generalization error using new proof techniques.
The paper respects the discrete-time nature of training trajectories and investigates topological quantities suitable for practical topological data analysis tools, which leads to computationally efficient measures. It proposes generalization bounds that are simpler and less restrictive compared to existing methods, removing the need for complex geometric assumptions and making them more practical.

Weaknesses:
Maybe I get this wrong but in line 113, is $Y\subset X$ or $Y\subset A$?

Limitations:
Lack of understanding of IT terms.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
0NMzBwqaAJ;"REVIEW 
Summary:
The paper analyzes token-level training dynamics in continued pretraining, identifying four loss patterns: persistent low loss, persistent high loss, increasing loss, and decreasing loss. Motivated by these patterns, the paper proposes a modification to language modeling called Selective Language Modeling (SLM), which only trains on a subset of the input tokens. This subset is selected by training a high-quality reference model and computing the ""excess loss"" of the target model -- i.e., the token-level difference between the target model and reference model loss. The model trained using SLM, Rho, achieves strong performance on math and other benchmarks relative to a model using normal continual pretraining.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
S1. The four categories of token-level loss are interesting and (to the best of my knowledge) not a previously noted phenomenon. The authors provide an interesting analysis of this phenomenon, and use it to motivate their method.

S2. The idea of selecting a subset of tokens to train on is clever and appears effective. The results, particularly on math benchmarks, after continual pretraining with SLM are quite strong and compared to sensible baselines. 

S3. The analysis is relatively comprehensive and contains several interesting points, especially the section comparing the correlation between token losses and downstream performance for selected/unselected tokens.

Weaknesses:
W1.  *Concerns about training time/cost*. The ""10x faster""/""5x faster"" claims in Figure 1 don't factor in the cost of pre-scoring each token by both the reference and training model. Can you measure and report this cost? It seems like what the figure actually shows horizontally is *data efficiency*, not speed of training. More generally, I think the claims about efficiency need to be more clearly explained-- the *data* efficiency claim is well-supported, but ""efficient"" used more broadly (e.g. lines 83, 114-115, 205) generally suggests a time or space efficiency claim, which I don't think the paper supports (or even really intends to claim). 

W2. *Scope of claims in title/abstract*. The title and start of the abstract suggest that the method is meant to be applied throughout pretraining, but the paper focuses on continued pretraining. Additionally, the eval focuses predominately on math datasets, which involve many tokens which may be relatively infrequent in pretraining corpora but frequent in-domain. This seems like the ideal domain for this kind of strategy--and, as Figure 5 shows, the gains are much more modest for other tasks. It seems the main finding is that ""SLM is a strong method for continual pretraining for math tasks (and slightly beneficial for general domain tasks)"", but the title/first 10 lines seem to suggest ""SLM should be used instead of CLM for pretraining from scratch,"" which isn't supported or claimed elsewhere in the paper. 

W3. The reference model should be included in the results tables as well. Does Rho outperform the reference model used for token selection?

W4. Doing hard selection cutoffs seems a bit heavy-handed; it's possible that weighting examples according to their ""excess loss"" might lead to higher performance. The authors do mention this as a future direction in the appendix.

Limitations:
Limitations listed look reasonable.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors explore how loss for specific tokens changes in continued pre-training and note that they fall into four categories (high->high, high->low, low->high, low->low) with each category having at least 10%. They run continued pre-training on tokens that are learnable and domain-useful (judged by reference model) and find that this leads to higher accuracy with less tokens used. The main results are in the math domain, but there are also a variety of other results (tool use generalization, general domain, etc.).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
For originality, I'm not deeply acquainted with related work, but it seems that the authors are (based on Related Work section in appendix). This works seems novel and well-contextualized with respect to related work. The experiments are of high quality and explore a few domains/problems. The paper is generally clear and easy to read. I think this work seems significant in that future research/application could use it (especially with a particularly low-quality dataset).

Weaknesses:
* The greatest weakness (in my opinion) is that ""tokens"" in many cases could refer to ""number of tokens after % filtering"" and ""total number of tokens before filtering."" This may be making some results misleading. This ambiguity is present throughout the paper. Just one example is in 3.3 - is 80B the total before filtering or after filtering?
    * Relatedly, in the case when it's after the % filtering, the ""x-axis"" should be total number of tokens before filtering in my opinion, because the % filtering isn't making training cheaper. I believe the results will still look good after these changes (the numbers in Table 1, for example, are great). But I think Figure 1, for example, should use total tokens (not after % filtering) if it's not already.
* It seems like OpenWebMath is very messy. How would this method work on a clean dataset (like the small, high-quality reference dataset). Is the benefit of the method mostly in ""cleaning"" the data, or in selecting useful tokens?
* How was the % for filtering chosen for the experiments?

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a method to train LLMs on the most influential tokens selectively. They suggest training a reference model on a small high-quality corpus using the standard CLM loss. They then compute the excess loss of each token in the training corpus as a difference in losses of the reference model and target model on that token. Finally, the target model is trained of the k% subset of the training corpus with the highest excess loss. The paper describes continued pre-training experiments for 1b and 7b models to demonstrate the effectiveness of this method. The experiments show improvements compared to standard continually pre-trained baselines and some open models in terms of performance on popular benchmarks and training efficiency (number of training tokens required to match the performance of open models).

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The Selective Language Modelling method proposed in the paper is a novel approach to pre-training LLMs. The authors' experiments demonstrate significant improvements in training efficiency which is an important problem in LLM pre-training. The paper also describes a study of LLM training dynamics which could provide useful insights to other researchers working in the field for further exploring efficient token selection strategies for LLM pre-training.

Weaknesses:
The experiments in the paper are performed in the continued pre-training setting and the impact of the original pertaining performance is not discussed in the paper. It is possible that the method might not work well if the base model is undertrained.

Limitations:
While the authors have discussed the limitations of their work, an additional direction for the future could be to study the use of multiple domain specialist reference models to select influential training tokens.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
3sWghzJvGd;"REVIEW 
Summary:
This paper investigates the generalization capabilities of world models in RL, particularly with respect to latent representation errors, which arise when observations are encoded into a low-dimensional latent space. The authors provide a bound on latent representation error when using CNN encoder-decoder architectures. The world model is framed as a stochastic differential equation to characterize the impact of latent representation errors on generalization in terms of either zero or non-zero drift. The authors provide theoretical analysis which shows that these errors can result in implicit regularization in the zero drift case, and propose a Jacobian regularization scheme to tackle the unwanted bias term in the non-zero drift case. Finally, when performing model rollouts for learning a policy, the authors study the effect of these errors on the value function. Experiments on Mujoco tasks demonstrate that the proposed Jacobian regularization enhances robustness to noisy states, reduces the detrimental impact of latent representation errors, and improves convergence speed for longer horizon tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- World models are a popular area of research in the RL community, but there is a lack of theoretical understanding. This paper takes one step towards theoretically analyzing the generalization capabilities of world models.
- The analysis of the effect of latent representation error is a novel theoretical contribution, to the best of my knowledge.
- The results in the paper seem mathematically sound and provide useful insights. The empirical results demonstrate that the Jacobian regularization, which naturally arises from the theoretical analysis, is helpful in improving robustness.
- As a very theory-heavy paper, the authors structured the writing such that it makes it easy to follow each individual result (though there is some room for improvement here, see weaknesses).

Weaknesses:
While the paper studies a previously unexplored problem, there are some questions about the significance of these findings and the use of drift and diffusion terms to represent the error. Other areas for improvement include explaining the insights from the theoretical analysis more clearly, describing the experimental settings in more detail, and supporting certain claims with more evidence.

- Studying the effect of latent representation error is certainly useful, however, with recent advances in representation learning approaches, one can learn reasonably good representations such that the reconstruction error is negligible. When it comes to model-based RL, a much bigger issue is the compounding model error, which is a result of error in the latent/state dynamics model predictions. A comment from the authors on this aspect would be helpful.
- The decomposition of latent error into drift and diffusion terms seems a bit contrived. It is not clear how the error can be expressed in this form, and what defines the scenarios of zero versus non-zero drift.
- The interpretation that propagation of latent error leads to the model exploring novel states seems somewhat questionable. My understanding is that the erroneous states improve robustness similar to noise injection, but will most likely not be valid states belonging to the state space of the MDP. Some reasonable evidence is required to support this statement.
- The paper presents several results and including some intuitive or low-level explanation for each of those results would greatly improve readability. Additionally, due to the large amount of mathematical notation used throughout the paper, it would be helpful to include a notation table in the appendix for easy reference.
- The experimental setting is not sufficiently clear, especially in the introduction when the authors refer to Table 1. With regards to the perturbations - are they applied to every state in the trajectory? For masking, is the same mask used for every state, or is the mask also sampled randomly? With regards to injecting encoder error - how to interpret the $\mu_t$ and $\sigma_t$ values?

Limitations:
There is little discussion on the limitations of the analysis. Some points worth discussing could be the impact of various assumptions when deriving the results, the fact that the analysis is mostly focused on a specific setting - learning from pixels using a CNN encoder and an RNN latent dynamics model, and further investigation of the compounding model error problem.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the generalization capability of world models via a stochastic differential equation formulation. They try to understand latent representation errors on generalization, with both zero-drift representation errors and non-zero-drift representation errors. They found that zero drift latent representation errors are implicit regularization and thus bring generalization gain. Jacobian regularization is proposed to enhance training stability and generalization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ A deep understanding of the generalization of world models via stochastic differential equation formulation;
+ A careful study of the different effects of zero drift and non-zero drift on gn

Weaknesses:
+ The unseen images are produced via global/partial Gaussian noises and rotation, which seems more on the robustness side rather than the generalization of unseen images;

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores the generalization capability of world models in reinforcement learning. In particular, they investigate the latent representation error in world models. They show that zero-drift representation error is inherently a regularizer for the learned model functions. On the other hand, they show that the non-zero-drift representation error accumulates errors and Jacobian regularization can be used to alleviate the issue. They demonstrate their proposed approach improves stability, convergence, and performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This work investigates an interesting aspect, the generalization of world models that learn the dynamics of the environment. Very limited work has been done in this facet of RL, thus it will share significant insights with the DRL research community.

2. The paper followed a structured methodology to analyze the world model and its representation errors. They interpret the learned model function as stochastic differential equations (SDEs) and model the variation as Brownian motions. 

3. I liked the way they theoretically analyzed it case-by-case and established connections with prior findings. 

4. The paper articulately presents the findings of zero-drift error as a regularizer and the Jacobian correction term for non-zero-drift representation error.  It systematically proves its hypotheses and shows evidence against the claims. They presented corresponding formulas and interpretations.

Weaknesses:
1. The paper is very thorough in terms of theoretical derivation. However, in my opinion, the experimental section of the paper is somewhat lacking. It utilizes only two tasks from Mujoco to prove the efficacy of the approach. More diverse tasks from other benchmarks and robust perturbations will certainly improve the paper. 

2. The experimental evaluation is limited to reward comparison. However, it would be interesting to see some visualization of how the trajectories unfold in the case of both types of errors and with Jacobian regularization.

Limitations:
While the paper discusses the potential social impact of the work, it doesn’t discuss any limitations. I believe the characterization of the models as SDE and the use of Brownian motion as variation have certain contributions to the identified claims. Other interpretations may alter the findings.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
3Z0LTDjIM0;"REVIEW 
Summary:
This paper proposes a novel framework for approximately solving graph diffusion equations using a local diffusion process. In addition,  the proposed method can effectively localizes standard iterative solvers by designing simple and provably sublinear time algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ The problem is well motivated and the paper is well-written.
+ Extensive experiments are conducted to showcase the efficiency of graph diffusion framework to approximating graph diffusion equations.
+ The paper provides a good summary of existing graph diffusion equations.
+ The authors provide code and details of implementations.

Weaknesses:
- In this paper, the authors explore 18 different graphs, can the authors show/provide which local GDE solver achieves better performance on which type(s) of graphs? 
- Computational complexity/cost is missing.

Limitations:
Not applicable.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper, ""Faster Local Solvers for Graph Diffusion Equations,"" addresses the efficiency of computing Graph Diffusion Equations (GDEs) such as Personalized PageRank (PPR), Katz centrality, and the Heat kernel, which are essential for various graph-related problems like clustering and training neural networks. Traditional methods for solving GDEs are computationally intensive for large-scale graphs. This paper introduces a novel framework for approximating GDEs using local diffusion processes, which significantly reduces computational time and improves scalability by leveraging the localization property of diffusion vectors. The proposed local solvers are highly parallelizable and suitable for GPU implementation, offering up to a hundred-fold speed improvement and applicability to large-scale dynamic graphs. The paper also discusses the potential for these methods to enhance local message-passing mechanisms in Graph Neural Networks (GNNs).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The introduction of a novel framework for localizing the computation of GDEs using local diffusion processes is a significant contribution. This approach reveals the suboptimality of existing local solvers and provides a more efficient solution.


The paper offers a solid theoretical foundation, proving that popular diffusion vectors have strong localization properties using the participation ratio. It demonstrates that Approximate Personalized PageRank (APPR) can be treated as a special case of the proposed framework, providing better diffusion-based bounds. The design of simple and fast local methods based on standard gradient descent and the local Chebyshev method for symmetric propagation matrices is well-founded.


Experimental results on GDE approximation for PPR, HK, and Katz show significant acceleration over standard methods. The proposed local solvers demonstrate up to a hundred-fold speed improvement. The paper also shows that the new methods can be naturally adopted to approximate dynamic diffusion vectors, and they outperform standard PPR-based GNNs in training speed.

Weaknesses:
Precision Limitations: While the paper shows significant speedups for lower precision settings, the performance gain diminishes as higher precision is required. This limitation could affect the applicability of the methods in scenarios where high precision is crucial.

Sequential Nature: (Please correct if I am wrong.) Despite improvements, the reliance on sequential updates in some methods (like LocalSOR) still poses challenges for achieving maximal parallel efficiency.

Complexity of Analysis: The runtime analysis for some of the proposed methods, such as the Local Chebyshev method, is noted as complex and remains an open problem. Maybe simplifying this analysis or providing more intuitive explanations could enhance the paper’s accessibility.

Limitations:
See above

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new local iterative framework for solving graph diffusion equations (GDEs). Specifically, the framework approximates GDEs through a local diffusion process, leveraging the strong localization properties of diffusion vectors such as personalized PageRank, Katz centrality, and Heat Kernel. The proposed local solvers can achieve sublinear runtime complexity under certain monotonicity assumptions. Empirical results demonstrate that these solvers significantly accelerate their standard counterparts on several large-scale benchmark datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-structured and clear.

2. The theoretical analysis is rigorous and sound, providing runtime complexity bounds for some of the proposed local solvers, which are better than their standard counterparts.

3. The effectiveness of the proposed methods is well-supported by experimental results on large-scale benchmark datasets.

Weaknesses:
1. It is not clear how graph structures, such as sparsity and spectral properties, impact the runtime complexity of the local solvers. A discussion on the potential influence of graph structures on runtime complexity would be helpful.

2. The runtime complexity analysis assumes that the updates of local solvers satisfy monotonicity properties. However, LocalCH does not satisfy these properties during updates. Establishing runtime bounds for LocalCH may require different techniques.

3. In Figure 7, when $\epsilon \geq 2^{-31}$, the running time of LocalGD is the same as GD, indicating that LocalGD may not speed up the standard counterparts when $\epsilon$ is sufficiently small.

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a suite of fast methods to approximately compute graph diffusion vectors such as Personalized PageRank, Katz centrality and the heat kernel. A notable feature of the proposed methods is that they are easily parallelizable and hence can achieve further acceleration on GPU. The authors also provide a running time bound for each method that they introduce. Empirical results show that the new local methods can achieve up to a hundred-fold speedup when compared to their global counterpart.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written and easy to follow.
- The local diffusion framework is applicable to computing several important graph diffusion vectors.
- The experiments are reasonably comprehensive.

Weaknesses:
- When compared with APPR, the speedup is not really captured by Theorem 3.3 and Corollary 3.6. The authors only showed that $\overline{\mbox{vol}}(\mathcal{S}_T)/\bar\gamma_T \le 1/\epsilon$, but it requires strict inequality to achieve nontrivial speedup in terms of worse-case running time. It is not clear how tight this bound is in general. If the bound is tight, then in the worst case there is no speedup. The authors should comment on if there are classes of graphs over which there will be a notable gap between the 2 quantities $\overline{\mbox{vol}}(\mathcal{S}_T)/\bar\gamma_T$ and $1/\epsilon$, so that the result provides a meaningful improvement.
- Because the theorems concerning the worst-case running time do not seem to capture a clear improvement over simple baseline local methods, it is unclear where exactly the speedup reported in Table 2 comes from.
- In LocalGD and LocalCH, the authors did not provide an update rule (or even a definition) for $\mathcal{S}_t$. Since $\mathcal{S}_t$ is part of the local diffusion process, the authors should specify how $\mathcal{S}_t$ is updated or defined at each step. I guess that $\mathcal{S}_t$ depends on the termination condition, but I could not find where the authors mention about it in the main paper.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
08oUnmtj8Q;"REVIEW 
Summary:
The authors developed a few-shot evolutionary optimization framework to effectively solve the multi-objective EOPs and constrained EOPs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed method can solve the multi-objective EOPs and constrained EOPs with little data, especially for the engineering problems.

Weaknesses:
The learning results may rely on the relation degree of different tasks.

Limitations:
The overhead of the algorithm need to be further decreased.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new surrogate-assistant evolutionary algorithm that utilizes a Gaussian process with Deep Kernel Learning as the surrogate model. The method employs few-shot meta-learning to learn from multiple tasks to construct the surrogate. It is then integrated with the existing MOEA/D-EGO algorithm to create a new approach. Experiments are conducted on the DTLZ benchmark problems and a gasoline motor engine calibration problem to evaluate the performance of the proposed algorithm.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method is instantiated and tested in expensive multi-objective optimization and constrained optimization scenarios.
2. A real-world problem is considered in the experiments.

Weaknesses:
1. Many important algorithmic details are unclear. For instance, the main distinction between the proposed MDKL and the existing DKL algorithms is its ability to learn from a set of related tasks, yet its implementation is not clearly explained. How parameters from different source tasks collectively form the experience, and how parameters from both source and target tasks jointly create this experience, are not clearly addressed.
2. The effectiveness of the algorithm is primarily tested on expensive multi-objective optimization problems, but state-of-the-art algorithms in this field were not selected for comparison.

Limitations:
Even if a clear definition of task similarity cannot be provided, it is recommended to offer some hints to help users apply the algorithm.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduce a meta-learning framework into few-shot optimization to assist the surrogate modelling in expensive evaluation setting. The authors parameterize a mapping function to get the hidden feature of the solution space and then integrate such mapping into a gaussian kernel function as a deep kernel. They then facilitate meta-training of the proposed deep kernel over a group of related tasks to attain an experience model, by maximizing the posterior likelihood. During the online optimziation of the target task, the experience model is firstly adpated to the new task in the same way above and then updated acoording to its accuracy in terms of the predicted objective value. The experimental results show that the proposed framework achieves competitive performance against some strong baselines over EMOPs and ECOPs benchmarks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The idea of integrating meta-learning into the kernel-learning based surrotgate methods is novel, and might improves the surrogate-based optimziation towards generalizable setting.

2. The expriments result show that the proposed FSEO framework is at least competitive with the existing baselines, which is acceptable and should be encouraged for further development.

3. The overall writing is clear.

Weaknesses:
Before the next round of author-reviewer rebuttal, following concerns exist:
1. Given that the likelihhod-based loss function (Eq. 4) should be maximized to fit the samples from all of the related tasks, why its update should follow a gradient descent rather a gradient ascent? Correct me if I was wrong.

2.  line 144 ~ 146, the authors state that the U update interations roots from the smaller number of available related tasks. I can not understand the reason behind, can you explain it more?

3. The neural network $\phi$ used in the deep kernel function is a 2-layer MLP, which limits the FSEO to meta-learn surrogate function among the related tasks with the same slution dimension. However, in practice, related tasks might not share the same dimension. I would appreciate the authors to provide realistic scenarios where FSEO is eefective. Besides, the effectiveness of the FSEO on traditional single-objective tasks should also be verified to make it more convincing that FSEO is a general framework.

4. Although the overall writing of this paper is not bad, it is still difficult for less-skilled readers to understand the whole picture. In particular, the content in Section 3.2 and Section 3.3 should be carefully refined to make sure the potential readers fully understand how the DKL and MDKL operate. For now, it is too simple and ambiguous.

Limitations:
The limitations listed in Conclusion are quite brief. I would appreciate the authors to explain more about the two limitations listed here.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes Meta Deep Kernel Learning (MDKL), a new surrogate for SAEAs. MDKL consists of a deep kernel with meta-learning. Empirical studies demonstrate its effectiveness in expensive multi-objective optimization and constrained optimization.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. This paper is well-written and easy to follow. The technical details are well presented.
2. This paper extends deep kernel and meta-learning-based surrogates into evolutionary algorithms.
3. This paper investigated multi-objective optimization and constrained optimization.

Weaknesses:
Meta-learned deep kernel surrogates have already been well-studied in Bayesian Optimization [1]. The authors are also aware of this as they mentioned in Related Work. I think this paper does not present significant new advancements based on the previous work.

First, the authors claim that MDKL is specially designed for optimization, while the previous work is not. In this regard, I do not see many differences between MDKL and previous meta-learned deep kernels. The authors claim that the advantage of MDKL lies in continuous adaptation; however, most models support parameter updates or fine-tuning. The authors also did not sufficiently explain the relationship between continuous adaptation and optimization problems, or what significance it has for optimization problems.

Second, the authors propose that one of the novelties of this paper is taking expensive multi-objective optimization problems (EMOPs) and expensive constrained optimization problems (ECOPs) into account. MDKL, as a surrogate, can be integrated into almost any expensive optimization algorithm. It seems to be able to cooperate with Bayesian optimization as well. The authors simply replaced the surrogate in a multi-objective optimization algorithm with MDKL and conducted some experiments, without providing any new analysis, insights, or proposing any new methods specifically for MOPs or COPs. Therefore, I believe this paper does not make a significant contribution to solving EMOPs and ECOPs.

[1] Martin Wistuba and Josif Grabocka. Few-shot Bayesian optimization with deep kernel surrogates. ICLR 2021.

Limitations:
No concerns.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
RGnjY6l2HT;"REVIEW 
Summary:
The paper proposes UniEdit, a framework that allows the editing of videos. More specifically, UniEdit allows manipulation via text prompts to change the visual style or the motion pattern that is visible in the video. Moreover, it also targeted steering, e.g. via segmentation masks. They achieve this by introducing an additional reconstruction branch and a motion-reference branch into a u-net based diffusion network and share the values of the attention layers which are party designed specifically for this work. The method allows editing videos without retraining and creates very good results.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The core idea is straightforward and well-presented. There are just a few hyperparameters to select. They have a large amount of visual content showing the quality of their contribution. Moreover, they performed extensive human experiments to rate the videos.

Weaknesses:
The implementation may not be entirely reconstructable. Hopefully, this issue will be fixed when they publish the code as promised.

Even if the method description is understandable, the math is sometimes not entirely correct. For example in line 212/212, M is a matrix but the notation says that it is a scalar from a set $\{ -\inf, 1 \} $ or $\{ 0, 1 \} $. The authors should be encouraged to revise the math present in the paper.

**Minor weaknesses:**
Sometimes the English writing is a bit weak. For example: 
 - 100-110: Some parts are not forming complete sentences, e.g. ""Other improvements like efficiency [1], training strategy [19], or additional control signals [16], etc.""
 - 187: I think it should be ""an additional network"" or ""additional networks""

Limitations:
Unfortunately, there is no benchmark to evaluate the method. This is not the author's fault and they tried to do their best to create baselines. However, this makes it harder to rate the results.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper suggests UniEdit, a tuning-free method for editing the motion of a given video. The authors use a pre-trained text-to-video diffusion model and utilize its motion prior, to performing motion editing on a video while keeping the appearance of the original video. During the denoising process, they apply structural/content features injection from the reconstruction branch of the original video to maintain the input video's structure or content. The motion is edited according to a text description used to denoise another reference branch which is then used for injecting features into the editing videos. The results improve over the existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* Successfully applying feature injection for video diffusion models. 
* Impressive results.

Weaknesses:
1. Novelty.  Feature injection for image editing is a known technique [56].  Applying injection to video models is important and challenging, but not novel enough in my view.
Showing that the injection of motion features from the reference motion branch into the edited video, constrains the output motion, is important, but not surprising given the observation of [56]. 

2. Given that the main insight of the paper is that “the temporal self-attention layers of the generator encode the inter-frame dependency”, there is not enough analysis of this besides the visual results and Figure 6, which shows the relation between the optical flow magnitude and the temporal attention on one example qualitatively. Showing a quantitative analysis, and analyzing the features during the denoising process for different layers, could support this claim better and show the importance of this insight.

Limitations:
Yes, the authors discuss both, limitations, and broader impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on developing a tuning-free framework capable of editing both the motion and appearance of videos. They introduce UniEdit, an approach designed for text-guided motion editing that maintains the original content of the source video. By utilizing two branches—an auxiliary reconstruction branch and an auxiliary motion-reference branch—they achieve both content preservation and effective motion editing.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper pointing out the problem of existing methods of not being able to keep the non-edited area and propose using spatial self-attention module, spatial cross-attention module and temporal self-attention model to solve the problem. From the experiment results, it shows that the edited results exhibit the editing task correctly while maintaining the unedited area.
2. The paper is overall clear and well-written
3. This paper provides versatile applications like motion editing, stylization, rigid/non-rigid object editing, and background editing.

Weaknesses:
1. The number of the participants in the user study might not be representative enough.

Limitations:
This method is inherently influenced by the T2V model used.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
LnJ2EGKTXh;"REVIEW 
Summary:
This paper introduces a framework for generating paired instruction and robot code for further fine-tuning LLMs for robot-specific tasks. A symbolic simulator is used to check the correctness of the generated code and an LLM is prompted with chain-of-thought reasoning to align generated instruction. The resulted dataset was used to fine-tune a robot specific LLM and later tested on benchmark tasks.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper tackles two challenges of automated data generation for robot code synthesis, one is to check the correctness of the code by grounding it to logical state of objects and the other is to align the generated the instructions to provided robot capabilities. By designing principled and general modules that tackle each of these problems, RoboInstruct is shown to generate useful data for finetuning general purpose LLM to robot specific code generation applications.

Weaknesses:
RoboSIM can only check for semantically meaningful steps of the code and may not catch lower-level error that requires spatial/geometric reasoning, or even reasoning about physics, including commands that take in numerical parameters e.g. move(0,0.2,0), rotate(0.75). This seem to limit the usefulness of RoboInstruct to certain types of robot APIs.

Limitations:
see weakness

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces ROBO-INSTRUCT, a novel framework designed to generate synthetic training data for fine-tuning small language models to create domain-specific robot programs. The framework features two main components: ROBOSIM, an algorithm that validates programs using angelic execution, and INSTALIGN, which aligns instructions with the generated programs.
The key contributions of this work include the development of ROBO-INSTRUCT to enhance the code generation performance of small open-weight language models for domain-specific robot programs. This framework introduces ROBOSIM, which features a dynamic world synthesis and evaluation process for generating relevant world states and performing automated code checks for diverse tasks. Additionally, it includes INSTALIGN, a procedure that refines instruction-code pairs to improve alignment between instructions and the code generated by SELF-INSTRUCT. By fine-tuning the Codellama-Python-7B model using ROBO-INSTRUCT, the model significantly outperforms several other open-source and most proprietary models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper demonstrates strong clarity and organization, making complex concepts accessible to readers. Each section flows logically, and technical details are explained effectively, ensuring that the methodology and findings are easy to follow.
- The paper thoroughly reviews and incorporates current literature.
- The paper meticulously validates all its claims through experimental results and detailed analysis. The effectiveness of ROBO-INSTRUCT, ROBOSIM, and INSTALIGN is demonstrated convincingly through empirical data and comparisons with existing models and benchmarks. This empirical validation ensures that the contributions are not just theoretical but substantiated with practical evidence.

Weaknesses:
- While ROBO-INSTRUCT offers significant advancements for fine-tuning language models in robot programming, there are some weaknesses to consider. Firstly, it heavily relies on SELF-INSTRUCT for generating initial programs, potentially introducing biases from the base model's training data. This could limit the diversity and quality of the generated programs.
- Moreover, while ROBO-INSTRUCT shows promising results on benchmarks like ROBOEVAL, its application to real-world robot programming tasks requires thorough evaluation and validation. Real-world robot environments often present unpredictable challenges that benchmark datasets may not fully capture, necessitating further testing to assess the framework's robustness and generalizability in practical applications.

Limitations:
The limitations of the work are clearly stated

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tries to improve the performance of small open-sourced LLMs to generate code that can run successfully on a service robot simulator to solve tasks. The idea is to use another small model to generate program data using SELF-INSTRUCT and fine-tune a 7B model for the robotics domain. The authors note that data generated by SELF-INSTRUCT may have good diversity but lack correctness. To this end, they build a simulator RoboSIM that takes the programs generated by SELF-INSTRUCT and verifies the correctness of the execution in addition to syntax errors given a predefined robot API. They further modify the instructions to align with the verified programs better. Overall they show the 7B LLM fine-tuned on this clean data can outperform a GPT3.5-Turbo in the robotics domain.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-written and motivated. Figures are easy to understand and helpful in conveying high-level ideas. The idea to build a pseudo-simulator that tracks world states without running the generated programs through an actual simulator is novel. It is amazing that synthetically generated data filtered by simple heuristics such as requiring programs to pass the world state tracking RoboSim is sufficient to help improve the performance of an actual simulator.

Weaknesses:
While the paper is well-written for the scope it sets for itself, I am not sure if the contribution is significant enough. There are many works using LLM to generate data and fine-tune domain-specific models, so the idea behind this paper is not super novel. The performance gain is also limited by the rather heuristic method considering the gap between the best model presented by the paper and GPT-4 it sought out to beat. In fact, given the 17% performance gap, simple baselines could be using GPT-4 to generate the programs and fine-tuning small models or using GPT-4 as the critic to filter programs. These simpler heuristics may yield better results and prove the Robo-Instruct method (which is also rather heuristic) proposed by the paper unnecessary. For reference consider 
[1] Improving Small Language Models on PubMedQA via Generative Data Augmentation
Therefore, I am not sure the contribution of this paper is all that significant.

Limitations:
Building RoboSim to track world states might work only for simple pick-n-place or task-planning problems. How to generalize this heuristic of tracking world states to more complex problems is unclear. Some analysis of the scope (suitable for what kind of problem class) of this approach is needed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces ROBO-INSTRUCT, a novel framework designed to improve the code generation capabilities of smaller open-weight language models (LLMs) for domain-specific robotic tasks. ROBO-INSTRUCT leverages two key components:

1. ROBOSIM with DYNAMICEVAL:  A task-agnostic simulator that dynamically synthesizes a consistent world state based on the robot's actions within the program. This allows ROBOSIM to identify execution errors and validate generated programs even for diverse and complex tasks.
2. INSTALIGN: An instruction-program alignment procedure that utilizes Chain-of-Thought reasoning to refine the generated instructions. This ensures that the instructions better reflect the intent of the generated robot program, improving alignment between the two.

The paper evaluates ROBO-INSTRUCT by fine-tuning a Codellama-Python-7B model and testing its performance on ROBOEVAL, a benchmark for service mobile robots. The results demonstrate that the ROBO-INSTRUCT fine-tuned model significantly outperforms other open-weight models

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novel framework: Introduces ROBO-INSTRUCT, a unique approach to generating training data for fine-tuning smaller LLMs on domain-specific robot tasks.

Dynamic world synthesis: ROBOSIM's ability to dynamically create relevant world states allows it to validate diverse programs generated by SELF-INSTRUCT, overcoming the limitations of traditional simulators.

Instruction-program alignment: INSTALIGN effectively refines instructions to better reflect the program's intent, improving the quality of the training dataset.

Strong empirical results: Demonstrates that ROBO-INSTRUCT significantly improves the performance of small open-weight LLMs, enabling them to surpass even some proprietary LLMs.

Cost-effective and private:  Provides a potential alternative to deploying proprietary LLMs for local robot deployment, offering cost-effectiveness and privacy benefits.

Weaknesses:
Limited novelty: The idea of using a sim/emulator to verify the generated program has already been explored in previous works such as Chain-of-code, which is not mentioned by this work. 

Limited scope: The paper focuses on a specific domain (service mobile robots), and it is unclear how well ROBO-INSTRUCT generalizes to other robot domains.

Lack of real-world evaluation:  The paper only evaluates ROBO-INSTRUCT on a synthetic benchmark. Real-world deployment and testing are required to further assess its practicality.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
kOp0kiXZ3a;"REVIEW 
Summary:
The paper addresses challenges in model quantization for deep neural networks (DNNs), focusing on optimizing quantization-aware training (QAT) across multiple bit-widths with weight-sharing. To this end, this paper introduces a novel quantization method that exploits the highest integer precision to achieve nearly lossless bit-switching, reducing storage without relying on full precision. Key contributions include: (1) Adaptive Learning Rate Scaling: A technique that dynamically adjusts learning rates for different precisions to address competitive interference and inconsistent gradient issues during one-shot joint training. (2) Double Rounding: An extension for one-step rounding quantizer in fixed-precision quantization to improve accuracy. Experimental results on the ImageNet-1K dataset show that the proposed methods surpass state-of-the-art approaches in both multi-precision and mixed-precision scenarios, achieving higher efficiency and accuracy.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- This submission is well-written, as well as with good figures in Sec.4.
- The authors conduct extensive experiments on multiple datasets and multiple networks.

Weaknesses:
- Some analysis is missing. For example, I'm wondering whether the second rounding leads to more quantization errors, as the first rounding is used to produce INT8 weights and second rounding is then performed to quantize lower bit-width, the twice quantization is possible to cause more clipping errors and rounding errors, some analysis could enhance the strength of proposed methods. 
- Some designs should be further clarified, e.g., why ALRS is applied only for the scaling factors? Intuitively, weights of small bit-width is induced large gradient variance by STE, and thus the weights of small bit-width should also benefit from using smaller LR. 
- Fig. 1 is a bit confusing, some colored arrows are not well explained. 
- This works essentially lies in the research of mixed-precision quantization, so I think it is better to compare more MPQ (e.g., HAQ, DNAS, LIMPQ, etc) research in the Sec.4. Moreover, some recent papers on multi bit-width quantization are missed on the , e.g., [1] (PTQ-based) and [2][3] (QAT-based), which could be included into the Related Work. 

[1] Xu, Ke, et al. ""PTMQ: Post-training Multi-Bit Quantization of Neural Networks."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 14. 2024.

[2] Tang, Chen, et al. ""Retraining-free model quantization via one-shot weight-coupling learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024. 

[3] Zhong, Yunshan, et al. ""MultiQuant: A Novel Multi-Branch Topology Method for Arbitrary Bit-width Network Quantization."" arXiv preprint arXiv:2305.08117 (2023).

Limitations:
Please refer to the weaknesses. Overall, this paper currently needs more experiments and analysis to reveal some designs are reasonable.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a QAT scheme to jointly optimize a single model with different precisions. The authors apply their scheme on various CNN-based models on CIFAR-10 and ImageNet datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written
2. The ablation study is strong in my opinion and they evaluate various aspects of their scheme

Weaknesses:
1. I think the main limitation of the paper is the models and datasets. I believe that the study should be done on larger models (LLMs for example) as a architecture goal. For example, the authors show that they do not save a FP32 master copy of the model in their scheme. However, ResNet style models (or MobileNet) are easy to fit in even moderate GPUs and I don't think FP32 master copy is a big problem in that case (please correct me if I'm wrong).

2. I couldn't find a source-code to reproduce the results of the paper in my side.

Limitations:
yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper discusses advanced methods in multi-bit model quantization. Specifically, this paper proposes a method for one-shot joint training of multiple precisions. To this end, the authors introduce a double-rounding quantizer that leverages the highest integer precision to achieve nearly lossless bit-switching while reducing storage requirements. Moreover, they also propose an Adaptive Learning Rate Scaling technique that adjusts learning rates dynamically for different precisions. Two proposed techniques mitigate the competitive interference between bit-widths caused by inconsistent gradients of different precisions during biased gradient estimation. They also extend their Double Rounding method to support one-shot mixed precision training and develop a Hessian-aware Bit-witdh sampling strategy. Experimental results on the ImageNet-1K classification task show that their methods outperform state-of-the-art one-shot joint QAT in both multi-precision and mixed-precision scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Eliminating the costs of retraining for mixed-precision quantization is a meaningful and challenging topic.

- The end-to-end experiments are sufficient, and the presentation is good.

Weaknesses:
- More uniquness analysis needed. The using of Hessian information seems a bit trivial, each layer's Hessian is just used to compare with the averaged Hessian trace. Firstly, as shown in recent zero-cost NAS research [1], the architectural proxies will be less effective as the training goes on, I'm not sure the Hessian information obtained on the initial full-precision model will remain useful as the quantization-aware training continues. Moreover, the sampling probability is modified with a simple ascending heuristic, which is not Hessian-aware. 

- Also applies here: the design of the double-rounding quantizer is similar to Bit-Mixer, Adabits, and ABN. Specifically, ABN also uses 

- ALRS needs further ablations. In ALRS, the authors use a fixed scaling ratio to bit-widths, e.g., 8-bit is 1, 6-bit is 0.1, and 4-bit is 0.01, the choice of these scaling factors still requires more ablation studies and discussions. 

- More comparisons needed. Since this paper adopts an ILP-based search algorithm to find optimal subnets, it is better to compare with these ILP-based mixed-precision quantization papers, e.g., [2] and [3]. 



[1] A Deeper Look at Zero-Cost Proxies for Lightweight NAS 
[2] Mixed-precision neural network quantization via learned layer-wise importance, ECCV 2022. 
[3] Hawq-v2: Hessian aware trace-weighted quantization of neural networks, NIPS 2020.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a bit-switching quantization method using Double Rounding, which applies rounding twice to achieve nearly lossless switching without storing a full-precision model. They also introduce Adaptive Learning Rate Scaling (ALRS) to adjust learning rates dynamically across precisions, ensuring consistent quantization updates. Additionally, they develop Hessian-Aware Stochastic Bit-switching (HASB) for one-shot mixed-precision training, optimizing bit-width distribution based on layer sensitivity, thus eliminating retraining stages.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. ALRS heuristic can help practitioners who wish to train mutli-precision model

2. Authors made extensive experiments on vision models and compare to previse methos

3. Most sections are well written

4. Code is given

Weaknesses:
**Novelty** is limited and I am not highly motivated that the problem is important.

1.	The main contribution is to not same 32bit weight and different quantization parameters but only the high bidwith using a pretty straightforward idea of double rounding during training
2.	The ALRS is based on observation and heuristic to fix it. It is nice and helps for when trying to use 2 bits as well. Yet, I am not sure it is important for methods that don’t use the double rounding.

**Motivation**

3.	Since we usually don’t switch models based on data I am not sure why this is important. Do we really have edge device that switch on a daily base model precision and thus need to store in small local memory the 32bit model? Can you elaborate why and where multi precision is really important.

4. No results on more recent models (LLMs)

Limitations:
The authors partially discuss limitation

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
eM3Wzs6Unt;"REVIEW 
Summary:
The paper presents an off-policy hierarchical RL method, based on the HiT-MDP formulation of a Semi-MDP. The HiT-MDP formulation treats the option $o$ as an extension of the original state $s$ (which can be chosen by an extended action), and combines initialization-, termination- and option-policy in a single Markovian master policy $p(o\_{t}|s\_t,o\_{t-1})$. The policy in the extended state-action space, thus, decomposes into the high-level and low-level policies, $p(o\_{t}, a\_t | s\_t, o\_{t-1}) = p(o\_{t} | s\_t, o\_{t-1}) p(a\_t | s\_t, o\_{t})$, which can be trained using standard RL algorihms. 
Compared to the prior work, the paper makes the following contributions:
- Whereas previously PPO was used for reinforcement learning, the paper proposes to use SAC, resulting in improved sample efficiency
- The paper motivates the algorithm from a control-as-inference perspective

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The proposed method seems to be technically sound, and using off-policy agents for HiT-MDPs seems sensible. (Quality)

The provided code clarifies the implementation which helps reproducibility. (Quality)

The presentation is mostly clear. (Clarity)

Applying an off-policy agents to HiT-MDPs seems to be novel and effective (Origingality, Significance)

Weaknesses:
Originality
-----------
One of the main weaknesses of the submission is the limited novelty. Replacing the PPO agent of MOPG by a SAC agent seems to be straightforward, so this contribution is quite incremental. Indeed, the authors of HiT-MDP stated, that their ELBO ""can easily be extended to a SAC-like algorithm"" [35]. Furthermore, given that MaxEnt-RL was already derived from a control-as-inference perspective, deriving the special case of an HiT-MDP using this technique does not seem to be significant contribution either. I also don't see the value of this derivation that would justify devoting so much space on it; couldn't we just argue that we apply SAC to such particular form of an MDP?


Quality
---------
The experimental evaluation seems to be another weakness of the submission. While the method is evaluated on a reasonable number of MuJoCo environments, where it outperforms a reasonable number of baselines, the choice of baselines is not convincing because it looks like the method is only compared to on-policy algorithm. The submission claims that there method ""significantly outperforms existing on-policy and off-policy option variants"", but it is not clear to me to which off-policy baselines this claim refers to. It would be important to focus to flat and hierarchical off-policy methods in the experiments, such as [19], [50], [33] and Hao et. al (2023).  Furthermore, the choice of environments is not convincing, because it does not include more challenging long-horizon tasks that are typically used for evaluating HRL methods, such as Ant-Maze. While the performance on the standard locomotion environments is reasonable, the reported numbers don't seem to improve on the SOTA of flat-RL methods.   

The paper does not discuss the hyperparameter search although it states in the questionnary  that these details are provided in the main content and the appendix.

The paper argues that it did not perform any ablations due to limited computational resources. However, I don't find this argument very convincing, since the experiments are performed on simple vision-free locomotion tasks, that can be run on standard workstation, not even requiring any GPU. Ablations on the number of options would be very useful.



Clarity
---------
I found the background material on control-as-inference a bit confusing. In particular, line 106 which states states policy improvement constitues an M-Step of an EM algorithm that *maximizes* the KL towards $P(\tau|\mathcal{E})$. I don't think any practical algorithm involves such maximization, since the optimum would correspond to a delta distribution on the least-likely trajectory. (

Visually, the presentation is rather bad. Figures are not on the top, and in particular Fig. 2 seems to hide some text, since the sentence in line 271 ends with "", which"". Fig. 2 itself could be improved by increasing the plot sizes (there are some unnecessary white spaces) and by making the legend more readable.  


Significance
-----------------
While I think that the proposed combination of the HiT-MDP formulation and SAC is somewhat interesting, the submission does not provide a convincing argument for the method. When should I use it, instead of existing (hierarchical or flat) methods?

References
----------
* Hao, C., Weaver, C., Tang, C., Kawamoto, K., Tomizuka, M., & Zhan, W. (2023). Skill-critic: Refining learned skills for reinforcement learning. arXiv preprint arXiv:2306.08388.

Limitations:
The limitations are adequately discussed and I don't have any concerns regarding negative societal impact of the work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the Variational Markovian Option Critic (VMOC), an off-policy algorithm for hierarchical reinforcement learning. VMOC aims to address exploration inefficiency and update instability in existing methods. Key contributions include: 1. Use of variational inference for update stabilization 2. Low-cost option embeddings for improved scalability. The authors evaluate VMOC on Mujoco environments, comparing it to other on-policy and off-policy methods. They report improved performance in learning option sets for complex tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. The paper is well-written, and the proposed method is theoretically justified.
2. The empirical evaluations show favorable results compared with existing methods.

Weaknesses:
1. Very similar ideas of the variational option framework have been proposed in [33] (off-policy) and [35] (on-policy). While [35] proposes an on-policy version, its off-policy version is also straightforward to deduce following [ref1]. The use of option embeddings is following [35].
2. The empirical evaluations are very limited; there is no ablative evaluation reported, which makes it hard to determine the contribution of the proposed method to the overall performance gain over various baselines.

References: 
[ref1] Levine, Sergey. ""Reinforcement learning and control as probabilistic inference: Tutorial and review."" arXiv preprint arXiv:1805.00909 (2018).

Limitations:
The empirical evaluations, especially ablation studies, are somewhat limited in scope.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces the Variational Markovian Option Critic (VMOC) which combines variation policy iteration and the option critic. VMOC also modifies HiT-MDPs, where options are represented as latent embeddings rather than triples of (init states, policy, termination condition), to the off-policy setting. The paper performs comparisons to option-based methods and PPO on 10 Mujoco environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and easy-to-read. The figures clearly highlight the performance of the method. The translation from theory to the practical algorithm is well detailed.

2. The advantage in sample-efficiency over other option methods and PPO is clearly seen in Figure 2 across Mujoco environments. In fact, this gain looks to be in atleast two orders-of-magnitude (of fewer steps required by VMOC) which is amazing. The underlying MaxEnt objective in VMOC appears to be very useful with exploration in the high-dim mujoco envs.

Weaknesses:
1. It is not clear if this gain in sample-efficiency will transfer to discrete environments or is somehow applicable only in continuous envs. Perhaps the authors can perform comparisons on Atari or Procgen to demonstrate the same? It would be great if the authors could also discuss the changes in the algorithm in the discrete and continuous settings (perhaps such as the sampling of a_t from the replay buffer?)

2. It is unclear if all methods use the same number of options (e.g. the value used in VMOC appears to be 4). A clear ablation of various design choices like number of options would help demonstrate that VMOC is thoroughly better than the other option methods and is not brittle to hyperparameter choice. 

The analysis of the actual options learnt is also missing (this is for example seen in the option critic paper). This, alongside an analysis of the number of options, is crucial to understand if the method is actual learning composed actions that are further composable and generalizable or degenerating to something simple like learning the action primitives (although the latter would apply more to a discrete rather than continuous env).

3. Minor comment: The location of Theorem 1 in the preliminaries makes it unclear if it is a contribution of the authors or well-known statement. Perhaps the authors can clarify?

4. Another minor comment: It would be great if the authors could discuss other ways of combining options in the related work such as in [1] and [2].

[1] The Option Keyboard: Combining Skills in Reinforcement Learning, Barreto et al, NeurIPS 2019

[2] Exploring with Sticky Mittens: Reinforcement Learning with Expert Interventions via Option Templates, Dutta et al, CoRL 2022

Limitations:
The authors have addressed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces e Variational Markovian Option Critic (VMOC), which learns actions and options simultatenously. They build upon the Hidden Temporal Markovian Decision Process (HiT-MDP) [1] to build a novel off-policy algorithm that utilizes entropy augmented rewards. Their method learns options’ embedding vectors (rather than conventional option tuples utilized in Semi-MDP [2]). They benchmark the learning performance of their method against several competitors on many classic control benchmark environments. 

*References*
1. Li, C., Song, D., & Tao, D. (2023). Hit-MDP: learning the SMDP option framework on MDPs with hidden temporal embeddings. In The Eleventh International Conference on Learning Representations.
2. Sutton, R. S., Precup, D., & Singh, S. (1999). Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2), 181-211.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- Extensive comparison against 8 competitor algorithms on 10 benchmark tasks
- A novel Soft Option Policy Iteration Theorem

Weaknesses:
The paper does offer a potentially interesting contribution to the wider research community. But it is held back by the lack of clarity and polish in writing. For example, two glaring signs of a hasty submission:
1. Sec 4 and Sec 5 are both titled experiments. Sec 4 is only 1 paragraph, and it essentially repeats the same information from the introductory paragraphs of Sec 5
2. In Sec. 5, line 271 just trails off without completion. I believe the authors moved around the images to correct for vertical space and accidentally hid the text.

While the experimental results focus on learning curves, where VMOC does well, they fail to provide other relevant evaluation metrics:
1. What do the learned options look like? A good evaluation could follow Fig. 5 and Fig. 6 from [1]
2. How many options are learned? Digging through the appendix, it says that they learned 4 option vectors. This leads to another question: how do they choose the number of options to learn?
3. The VMOC algorithm listed in the appendix only describes the gradient update process. No details about action sampling or other hyper-parameter tuning are described here
4. The environments used are challenging for model-free RL algorithms. That said, they may not be satisfactory for showcasing the potential of learned options. 


*References*
1. Li, C., Song, D., & Tao, D. (2023). Hit-MDP: learning the SMDP option framework on MDPs with hidden temporal embeddings. In The Eleventh International Conference on Learning Representations.

Limitations:
Much like Soft Actor-Critic, this work develops a novel Soft Option Critic style algorithm. I believe this line of work is very interesting and potentially impactful in the near future. However, their current draft is not well-written and hard to follow. Their experimental evaluation is also insufficient.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Yq2dYPkfRU;"REVIEW 
Summary:
This paper studies the generalization measured by gradients via a uniform gradient stability. For $\beta$-uniformly stable algorithms, the paper gives generalization bounds of order $O(1/n+\beta+\sqrt{E_Z[\|\nabla f(A(S);Z)\|_2^2]/n})$, which yields fast rates if $E_Z[\|\nabla f(A(S);Z)\|_2^2$ is small. The paper then uses this generalization measured by gradients to derive generalization error bounds under a PL condition. Applications to empirical risk minimization, gradient descent and stochastic gradient descent are given for strongly convex, smooth and Lipschitz problems.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper gives high-probability bounds for generalization gap on gradients, which include the gradient norm at the minimizer. This can imply fast rates in an interpolation setting. Under the PL condition, this gives fast rates of order $O(1/n^2)$.

The paper provides comprehensive applications to several algorithms such as empirical risk minimization, gradient descent and stochastic gradient.

Weaknesses:
The high-probability analysis based on uniform stability follows largely from existing work. I do not see enough novelty in the analysis. It would be helpful if the authors can summarize the challenges in the analysis and their novelty. 

As stated in the paper, Theorem 1 and Theorem 2 only improve the existing results by a constant factor. This improvement is not significant. 

As stated in the paper, the generalization by gradients is mostly interesting for nonconvex problems. However, for the applications in Section 4, the paper considers strongly convex problems. Also the results require smoothness and Lipschitz continuity. These assumptions seem to be a bit strong.

The paper gives fast rates under the case $F(w^*)=O(1/n)$ in Section 4. Note that Section 4 considers strongly convex problems. Then, the objective function should be of the form $F(w)=G(w)+\mu\|w\|^2$, where $G$ is related to loss. Then, if we require $F(w^*)=O(1/n)$, one needs $\mu\|w^*\|^2=O(1/n)$. Suppose we assume $\|w^*\|=O(1)$. Then, this requires $\mu=O(1/n)$. In this case, the generalization bound would be vacuous since $n\mu=O(1)$.

For SGD, the computation cost seems to be high. For example, in Theorem 6, the paper requires $T=n^4$ while in Theorem 13 the paper requires $T=n^2$. This high computational cost may not be appealing for large-scale problems.

In the proof of Lemma 1, the paper uses $\|\nabla F(A(S))\|_2\geq \mu\|A(S)-w^*\|$. This inequality does not generally hold under a PL condition. Indeed, Theorem 2 in Karimi et al 2016 require $\|A(S)-w^*\|$ to be replaced by the distance between $A(S)$ and the set of minimizers.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work shows high probability excess risk bounds of $O(1/n^2)$ for several algorithms under strong convexity, smoothness, Lipschitz continuity and low noise assumptions using algorithmic stability.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The results of the paper are interesting, showing a risk bound of $O(\frac{1}{n^2})$ using algorithmic stability.

2. The paper uses a novel technique, involving the stability of gradients to demonstrate excess risk bounds and presents applications of this technique in convex optimization.

Weaknesses:
1. The problem setup of the paper is not clearly detailed before the technical section, including the assumptions used for proving the results.

2. The presentation of results and related works is somewhat lacking. It would be beneficial if the authors summarized the results from previous work and compared them to the results in the current paper, including the set of assumptions made in each work.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper achieves the high probability excess risk bounds $\mathcal{O}(1/n^2)$ for empirical risk minimization, projected gradient descent and stochastic gradient descent under strong convexity, smoothness and Lipschitz continuity assumptions.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Please see Summary.

Weaknesses:
1.The paragraph from line 52 to line 65 first analyzes the non-convex problems. Then, it suddenly mentions that this paper explores the stability of stochastic convex optimization algorithms with strongly convex losses in line 61. And, it doesn’t mention its non-convex analysis. It is so confusing. So, I suggest authors rewrite this paragraph to benefit readers’ understanding.

2.The paragraph from line 70 to line 73 is unnecessary since we can not obtain any useful information.

3.In Related Work, it is unnecessary to list the literature related to uniform convergence since it is not helpful for readers’ understanding of the contributions of this paper. It may be better that authors list the detailed literature related to high probability bound.

4.Theorem 1 in this paper is not the sharpest p-moment bound for sums of vector-valued functions. Authors demonstrate their bound is indeed tighter than Theorem 1 of [1]. However, [2] also provided a bound (Theorem 1) that is likely tighter than Theorem 1 in this paper. Note that, [2] also used Marcinkiewicz-Zygmund’s inequality to prove their bound. Besides, the paragraph from line 131 to line 136 states “On the other hand, in Section 3.2, we will carefully construct vector-valued functions which satisfies all the assumptions in Theorem 1 and ensures M = 0 at the same time. Under this condition, we can eliminate the first term.”. This point is also considered in Theorem 1 of [2]. I think that authors just consider the improvement to [1], but omit other related work.

[1]J. Fan and Y. Lei. High-probability generalization bounds for pointwise uniformly stable algorithms. Applied and Computational Harmonic Analysis, 70:101632, 2024.

[2]X. Yuan, P. Li. Exponential generalization bounds with near-optimal rates for $L_q$-stable algorithms. ICLR, 2023.

5.In Section 3.2, authors build some relationships between generalization error and stability parameter $\beta$. Authors think these relationships are under non-convex, non-smooth, non-PL conditions. These bounds are not the final generalization bounds but the relationships. After giving the stability bounds, the generalization bounds are finally determined. However, in Section 4, authors provide the stability bounds under (strongly) convex and smooth conditions. Therefore, authors didn’t remove (strongly) convex and smooth conditions for generalization analysis.

6.The symbol $M$ is repeatedly used in Theorem 1 and Theorem 2. Therefore, I suggest authors should carefully check their symbol settings.

7.In Remark 4, authors compare their Theorem 3 with the bound in [3]. It is unfair since, as mentioned in the above 5., the bound in [3] is a final result but Theorem 3 is not.

[3]Y. Xu and A. Zeevi. Towards optimal problem dependent generalization error bounds in statistical learning theory. Mathematics of Operations Research, 2024.

8.In line 633, $F(A(S)) - F(A(S))$ is wrong.

9.In line 633, Equation (31) should be an inequality.

10.In line 152, authors state “In nonconvex problems, we can only find a local minimizer by optimization algorithms which may be far away from the global minimizer. Thus the convergence does not make much sense in function values.”. So, they use uniform stability in gradients. However, in Section 4, they provide some uniform stability bounds in gradients for strongly convex problems. It is a paradox.

11.The form of the relationship in Theorem 3 is very normal. The method to obtain an excess risk bound $\mathcal{O}(1/n^2)$ is very simple. I think other normal generalization results (like [1]) in gradients can derive the excess risk bound $\mathcal{O}(1/n^2)$. The main contribution of this paper may be the simple method combining PL condition with some usual decompositions as shown in Proof of Remark 5. However, as mentioned in the above 10., the uniform stability in function values is more unreliable than the one in gradients under strongly convex condition.

Limitations:
Considering the 4. in Weaknesses, I suggest the author reconsider whether their result is the sharpest.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the standard statistical learning setting and derives $\mathcal{O}(\frac{1}{n ^ {2}})$ ($n$ denotes the number of samples) high-probability bounds for the excess risk $F(A(S)) - \inf_{w} F(w)$ ($A$ denotes the algorithm and $S$ denotes the training set) of ERM, PGD, and SGD. The best-known bounds prior to this work were $\mathcal{O}(\frac{\log n}{n})$ for ERM, PGD, that was derived using algorithmic stability, and $\mathcal{O}(\frac{1}{n ^ {2}})$  or ERM, SGD that was derived using uniform convergence. However, the latter demanded $n = \Omega(d)$ samples, thereby introducing the an undesirable dependence on $d$. The current paper shows that it is possible to obtain $\mathcal{O}(\frac{1}{n ^ {2}})$ bounds for the algorithms (without any dependence on $d$) under the lens of stability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The considered problem is interesting and the contributions of the paper show that sharper bounds (at par with uniform convergence, albeit without any dependence on $d$) are possible for ERM, SGD, and PGD for strongly convex and smooth stochastic convex optimization. Below I explain the roadmap taken by the authors in doing so, which also sheds light on some of the other aspects of the paper.

1) The authors first study the generalization gap via gradients, i.e. the quantity $\||\nabla F(A(S)) - \nabla F_S(A(S)\||$ for the statistical learning setting under the assumption that the function is Lipschitz and the algorithm is uniformly stable in gradients (Theorem 1 and 2). Under the nonconvex setting, the authors obtain dimension-independent bound for the generalization gap via gradients. This bound is subsequently studied under the assumption that the function is smooth and satisfies the Polyak-Lojaseiwicz (PL) condition. The obtained bound is a function of the gradient norm obtained at the end of optimization, i.e. $\|| \nabla F_S(A(S) \||$. 

2) To obtain excess risk bound for the algorithms, the (i) authors show that the algorithms are uniformly stable in gradients; (ii) translate the excess risk bound to a bound on the gradient via the PL inequality (recall from the premise that these algorithms are analyzed in the strongly convex and smooth setting of stochastic convex optimization, therefore PL holds vacuously); (iii) use triangle inequality to relate the bound on the gradient norm to the generalization gap, and use the bound on the generalization gap via gradients (see 1 above).

Weaknesses:
I found several typos in the main theorems in section 4. For example, the stability equation in Lemma 4 should be written with respect to the output at the iteration instead of the output of the ERM.  Also, why is the reference in Theorems in section 4 to Theorem 3, instead of Lemma 1? From my understanding (explained in the Strengths above), Lemma 1 is a specific instantiation of Theorem 3 to smooth + PL functions, which is exactly the premise in Section 4. What exactly is $w$ in the bound $F(w) - F(w^\star)$ in Theorem 5? I expect it to $w_{T + 1}$ (or $w_T$).

I don't see the point of Marcinkiewicz-Zygmund’s inequality with improved constants. The whole paper is about the improved dependence with respect to $n$, so I don't see a point in improving the specific constants in the inequality. 

I need some more clarification in lines 204--207. The authors state that Klochkov and Zhivotovskiy obtained $\mathcal{O}(\frac{1}{n})$ style bounds for the excess risk. What's the assumption on $f$ considered by them? The authors mention that they can obtain $\mathcal{O}(\frac{1}{n ^ {2}})$ bounds with an extra PL and smoothness assumption. Is this something for which Klochkov and Zhivotovskiy, 2021 could only obtain a suboptimal $\mathcal{O}(\frac{1}{n})$ bound? Earlier, my interpretation was that this work obtained $\frac{1}{n}$ bounds for ERM, and PGD for smooth and strongly convex stochastic convex optimization, but lines 204--207 made my understanding unclear. I want to make sure the authors are not invoking extra assumptions to get improved dependence.

Along similar lines as above, Lines 259--261 seem to be saying inconsistent things (is the assumption just smoothness, or strong convexity and smoothness). I would appreciate clarifications from the authors.

Minor Typos: 1) In definition 1, $\gamma$ and $\mu$ should be strictly positive; (2) Line 182: $\gamma$-smooth instead of $\gamma$-smoothness.

Based on the authors' responses, I would be happy to revise my assessment of the paper.

Limitations:
Yes, the authors have adequately addressed this.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
B2cTLakrhV;"REVIEW 
Summary:
This paper studies the problem of learning DAGs from observational data with incorporating prior knowledge represented as partial order constraints by extending from existing continuous DAG learning methods such as NOTEARS and DAGMA. The paper first presents a method that shuts down the corresponding cells of the adjacency matrix of the DAG according to the permutations of the partial orders and then proposes an augmented acyclicity-based method with improved efficiency. The paper conducts experiments on simulated datasets and a real-world dataset. In addition, the paper also provides comprehensive theoretical motivations and analysis to the research problem and the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem studied in the paper is novel and has significance in practice. To my knowledge, it is a new (sub-)problem of DAG discovery from observational data, which has not been studied before.

2.  The proposed method is well motivated both conceptually and theoretically. The notations and definitions are rigorously defined.

3. The experiments are comprehensive and support the claims of the paper.

Weaknesses:
Not all my comments below are weaknesses and some of them are questions.

1. In Eq 8a, a (relatively) straightforward method is introduced, which is argued to be less efficient, motivating the development of the augmented acyclicity-based method. Is the straightforward method implementable? If so, it would be better to compare it with augmented acyclicity-based method in terms of the performance and efficiency.

2. I find it hard to fully understand Eq 9c and 9d. What's the definition of $\mathcal{A}(W, o)$ and why does it have the formulation in 9c? What's $W_{o,i,j}$? Does it mean $W$ is a three dimensional tensor?

3. To learn $W$, one needs the algorithm to be differentiable in terms of $W$. For the method in Eq 8a, one might need to backpropagate gradients through permutations. If so, how can we do this without continuous relaxation? For the augmented acyclicity-based method, it seems that one needs to conduct a few algorithms (Algorithm 2, 3, 4). Are the operations in these algorithms differentiable in terms of $W$? More discussions are needed.

4. The proposed method introduces a new hyperparameter $\gamma$. How to determine $\gamma$ in practice as there is no ground truth to do validation?

5. In the experiments of real-word data, Linear NOTEARS is used as the baseline. How do we know the (non)linearity in the real data?

Limitations:
The authors adequately addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper contributes interesting new theoretical results for the field of differentiable graph structure learning and showcases how to practically exploit those results for improved structure learning. Differentiable structure learning converts the combinatorial optimisation problem of finding the correct graph structure of a problem into an optimisation problem by parametrising an adjacency matrix according to the structural equation model (SEM). Existing methods focused on enforcing DAG structures on these parametrised matrices such that DAG graphs can be learned. This paper instead focuses on a new type of constraints, trying to enforce that the learned graphs adhere to a given set of orders on the underlying variables. It is first proven (Theorem 3) that adherence to a set of orders $\mathcal{O}$ is equivalent to a constraint comprised of $|\mathcal{O}^+|$ terms, which is deemed computationally infeasible. Then the paper continues to prove its main result, showing that adherence to the set of orders $\mathcal{O}$ is also equivalent to a constraint over maximal paths in the transitive *reduction* $\mathcal{O}^-$ of $\mathcal{O}$. Finally, this theoretical result is implemented such that it can take any existing SEM-based structure learning algorithm and augment it with given orders. The increase in performance is shown on a number of synthetic and more realistic structure learning benchmarks when comparing with and without the exploitation of given orders.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The structure of the paper is very clear and hence reads well. As someone who is not deeply familiar with the details of differentiable structure learning, the incremental and logical build-up of the preliminaries is also appreciated. In general, the authors put significant effort into making the paper self-contained, improving readability further. 

2. The paper is very formal in its notation and methodology, without going overboard on mathematical notation or jargon. Again, this allows for someone not intimately familiar with the topic to more easily follow the proofs and reasoning of the paper.

3. The idea of the paper is certainly interesting and novel, especially considering the reference that incorporating orders can significantly reduce the search space of possible structures and the hardness of structure learning [1]. The overall idea of trying to incorporate prior knowledge of any form also ties in well with the rising popularity of neurosymbolic methods [2, 3].

4. The authors clearly also put in a lot of effort to ensure a reader can follow the theoretical arguments on a high level throughout the text without getting lost in mathematical intricacies during reading. The many intermediate comments and examples are very helpful to keep the story focused, which is never easy when the road to a general result requires multiple intermediate results.

5. The empirical evidence is very convincing. I thank the authors for including aggregates and variability metrics over multiple runs, which is sadly not a given anymore.

[1] Teyssier, M., & Koller, D. (2005, July). Ordering-based search: a simple and effective algorithm for learning Bayesian networks. In Proceedings of the Twenty-First Conference on Uncertainty in Artificial Intelligence (pp. 584-590).

[2] Garcez, A. D. A., & Lamb, L. C. (2023). Neurosymbolic AI: The 3 rd wave. Artificial Intelligence Review, 56(11), 12387-12406.

[3] Marra, G., Dumančić, S., Manhaeve, R., & De Raedt, L. (2024). From statistical relational to neurosymbolic artificial intelligence: A survey. Artificial Intelligence, 104062.

Weaknesses:
While I overall did enjoy reading the paper, I do have a number of questions and concerns:

1. The paper does not clearly distinguish its own theoretical contributions from previously known results. While I appreciate all lemmata and theorems are proven either in the main body or in the appendix, I would like to see clear statements of which results were already known, as some seem to relate to the provided references. In particular, I suspect Theorems 1, 2, and 3 were already proven before, though I am unsure about 3. Same for lemma 1. If some or all of these results are indeed novel, then explicitly stating they are can only further increase the impact of the paper.

2. Theorem 2 is a nice result showing how knowing a total ordering can indeed considerably reduce the complexity of a DAG structure learning task. However, it is not used in the rest of the paper. If Theorem 2 is not a new result, I would remove it to improve the flow of the paper and give more space for further clarifications.

3. I am not sure if I can agree with Remarks 2 and 6, where it is stated that the result of Theorem 3 does not lead to a computationally feasible solution while that of Theorem 4 does. It seems that using Equation 8b introduces a number of terms that is quadratic in the number of variables, as it is bounded by the number of possible pairs over those variables. While I can see that using Equation 9b is certainly more efficient for a single sequential ordering, the picture is less clear when $\mathcal{O}$ consists of many (sequential) orderings. In general, the trade-off between orderings in $\mathcal{O}^+$ or maximal paths in $\mathcal{O}^-$ does not seem clear, especially since no bounds on $|\mathcal{P}(\mathcal{O}^-)|$ are given.

4. While I generally agree with the provided proofs of all statements, there are a couple of places where I do have concerns about the validity of the results (see precise questions below). Importantly, I am unsure whether the exact statement of the main Theorem 4 holds, leading me to maintain a lower score for now until my questions are clarified.

5. The experimental section certainly considers enough datasets and data-generating configurations, but it does seem to miss one important comparison. I would have liked to see an experimental confirmation of the trade-off between using Equation 8b compared to Equation 9b, for both run time and general performance. Especially since the complexity of 8b is used as a motivation to discard it in favour of 9b. Moreover, some of the metrics could be better explained, even if only in the appendix. For example, I am not familiar with the details of the structural Hamming distance (SHD) or False Discovery Rate (FDR) and the notion of run time is also ambiguous.

Limitations:
The limitations are very well described in the appendix. I would only have like for it to be (partly) present somewhere in the main body of the paper, especially how any optimisation-based paradigm can not fully guarantee constraint satisfaction.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper provides a solution for imposing partial ordering information into differentiable DAG learning, which is a very important problem. It also proposes an efficient implementation based on rigorous theoretical justification. With this prior information, even with fewer samples, better structural recovery can be achieved in experiments.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
1. This paper imposes partial ordering into the differentiable DAG learning problem. It is a very interesting problem to the community.
2. The paper is theoretically well-supported.
3. This paper addresses computational issues when imposing partial ordering into differentiable DAG learning. The experiments indicate that better structural recovery can be achieved with the information of partial order.

Weaknesses:
1. I found there are some points in the paper that are hard to understand. A mild suggestion is to add examples to illustrate these concepts, which would make the paper more readable. For instance, toy examples to explain what $\mathcal{O}^+$ and $\mathcal{O}^-$ are. Especially in section 3.3, where there are many definitions, theorems, and lemmas, examples would help readers grasp the points and have a better understanding of the paper.
2. Typo: Line 204 $\mathcal{O}^-)$ should be corrected to $\mathcal{O}^-$.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an approach to integrate partial order constraints into differentiable structure learning for causal discovery. The key contributions are:
* Formulating an equivalent constraint set of path prohibitions to implement partial order constraints in the graph space.
* Proposing an efficient method to integrate partial orders by augmenting the acyclicity constraint.
* Proving the theoretical correctness and completeness of the proposed augmented acyclicity constraint.
* Demonstrating the effectiveness of the method through experiments on both synthetic and real-world datasets.

The authors show that their method can significantly improve the quality of recovered causal structures while maintaining computational efficiency, especially for long sequential orderings. They also demonstrate that using partial order constraints can reduce the required sample size for accurate causal discovery on real-world data.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper addresses an important gap in differentiable structure learning by enabling the integration of partial order constraints.
* The augmented acyclicity approach handles long sequential orderings more efficiently, addressing a key limitation of a naive implementation.
* The experiments cover both synthetic and real-world datasets, demonstrating the method's effectiveness across various scenarios.
* Results on the Sachs dataset show significant improvements in structure recovery with reduced sample sizes, highlighting the method's impact for real-world applications.
* The proposed method is designed as a plug-and-play module that can be integrated with various differentiable structure learning algorithms.

Weaknesses:
* Several statements are known or follow easily from existing results. 
* As noted in the limitations section, the method's efficiency can degrade with complex partial order structures involving multiple chains.
* The paper doesn't extensively discuss how sensitive the method is to incorrect or conflicting partial order priors, which is possible in applications.
* While the paper compares to baselines without priors, it doesn't compare to other methods that might incorporate different types of prior knowledge.
* The paper doesn't provide an in-depth analysis of how sensitive the results are to the choice of hyperparameters, particularly τ in the augmented acyclicity term.

Limitations:
Limitations are addressed in Appendix F.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
tRRWoa9e80;"REVIEW 
Summary:
The authors tackle semantic binding, where T2I models often fail to correctly reflect the relations between objects (object binding) or objects and their attributes (attribute binding). To address this, they introduce Token Merging (ToMe), a method that aggregates related tokens to a single composite token, which ensures they share the same cross-attention map. To do that, two training-free optimizations are introduced. The first, Semantic Binding loss, which makes sure the composite token leads to noise prediction that is consistent with the full phrase the token is based on. The second, Entropy loss, which helps the tokens focus exclusively on their designated regions.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Creating a composite token and then optimizing it is a creative and elegant approach to the problem. I'm curious to see what future work can be built on top of this contribution.
* The focus on object binding is indeed missing from the literature, a topic that is well-addressed in this work.
* The idea of end token substitution to remove attribute information is simple.

Weaknesses:
* The related work underplays the role of Attend-and-Excite and Linguistic Binding in Diffusion Models (SynGen) have on this paper. The former introduced the idea of semantic guidance, while the latter used dependency graphs to extract syntactically related tokens from prompts and presented a loss that encourages cross-attention maps of syntactically related tokens to agree. Both of which are ideas this paper meaningfully builds on. 

* You do not describe how you obtain the syntactic information: entities and their relevant objects nor if it is automatic or an input constraint. This is particularly confusing because in section 3.2 you begin describing exactly that, but do not delve into the specifics. Furthermore, in section 4.1 (line 248), you mention spaCy and that you ""identify each object and its corresponding attributes for token merging"", but do not provide actual details about your method. Does it mean it can accurately capture *all* syntactic structures? It is possible that I misunderstand, but it feels like the syntactic analysis portion of this work borrows from the SynGen paper. If this is the case, give appropriate credit. If it is not, then add details, as this is part of the method and is quite unclear.

* If I understand correctly, there are no human evaluation experiments, a user study would provide merit to ToMe’s superior performance. As a side note, in table 1, ‘Human-preference’ is a slightly misleading title. If I understand correctly, it is the output of the Image Reward human-preference model, but it sounds as if it is a human-evaluation result.

* I find that despite the claim that ToMe works well on highly complex prompts, there are no such examples or experiments given. For example, Figure 5 depicts prompts that are supposed to be complex, but they are no different from the simplistic prompts in Attend-and-Excite (“a {color_1} {subject_1} and {a color_2} {subject_2}”), only that they address object binding.

Limitations:
See questions 3 and 4.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a method to mitigate semantic binding, a common phenomenon in text-to-image models. While previous methods explicitly control the attention maps so that nouns and attributes attend to the same regions, the authors propose combining the nouns and attributes into a single token. This approach enforces the attention maps to concentrate in the same area. Additionally, they introduce an inference-time optimization loss to ensure the attention maps are focused, and that each composed token retains the same semantic meaning as all the different tokens that comprise it. The authors also analyze the information encoded in different tokens at the output of the text encoder.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The authors present an interesting analysis of the information encoded tokens, specifically they show that much of the semantic information is also encoded in the EOT tokens.
2. The authors provide a method to mitigate semantic binding in prompts of the form: “A <noun_A> <Attribute_A> and <noun_B> <Attribute_B>”, that outperforms or performs comparably to existing methods across various benchmarking subsets.

Weaknesses:
1. It is not clear how the model performs on more complicated prompts where both the noun and its sub-objects have attributes, such as: “A blue cat wearing sunglasses and a yellow dog wearing a pink hat.”
2. The method is on the slower side compared to other existing methods, due to inference time optimization.
3. This work is missing a user study, as the proposed automatic metrics can be inaccurate, and human evaluation is common in previous works.

Limitations:
The authors propose limitations relating to the underlying models, but I would be interested to learn in which areas of semantic binding the paper still struggles, such as with more complex prompts or a larger number of nouns, etc..

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to solve the semantic binding problem in T2I models. The authors introduce a semantic binding method by merging tokens of entities and related attributes. Besides, several other tricks like semantic binding loss and entropy loss are introduced to improve the performance of semantic binding.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The idea is straightforward and it is also reasonable that it will work with some other tricks.
- The paper is well-written.

Weaknesses:
- The importance of a few tricks like entropy loss is not well-emphasized.
- The motivation for some tricks like substituting the EOT token is not well-explained.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper focuses on the problem of lack of semantic binding in text-to-image generation models, and specifically on the misalignment between objects and their sub-objects. The paper introduces a training-free T2I method named ToMe after analyzing the properties of CLIP text embeddings and diffusion models. Utilizing the composition recognition ability of diffusion models, ToMe binds the embeddings of objects with their associated sub-objects together, and retains only the semantic information in the [EOT] tokens. ToMe is shown to be simple yet effective from the experimental results.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and easy to follow.
2. The authors present detailed analysis of the CLIP text embeddings including their coupled and entangled properties and the semantically additive properties of text embeddings shown in diffusion models. 
3. The structure of the proposed ToMe model is training-free, simple and mostly clear.
4. The authors provide extensive experimental results including quantitative, qualitative and ablation studies on the T2I-CompBench and GPT-4o Benchmark (introduced in this paper).

Weaknesses:
1. The Iterative composite Token Update (Sec. 3.2.2) is not clear enough to me. The authors introduce two losses: semantic binding loss and entropy loss. And since ToMe is training-free, it is not clear in the paper how these losses help the update the tokens at test time.

Limitations:
The authors presented the inherent limitation of SDXL that ToMe is based on like producing artifacts in generated images, and unable to generate images with complex layouts. Also the limitation of CLIP to produce text embeddings could restrict the performance of ToMe.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
yaYJlpidX1;"REVIEW 
Summary:
The paper proposes a novel technique to automatically discover in-context continual learning dynamics for image classification task sequences through meta-learning. In order to achieve this purpose, the approach relies on 2 main novelties: 
* Using self referential weight matrices on top of an image encoder - SRWM, as self-modifying that adapts itself to the stream of inputs, is an natural model for continual learning. 
* Encoding continual learning desiderata in the meta-objective, i.e. backward and forward transfer. 

The authors first apply the approach in a classic two-task setting (Split-MNIST) that allows them to showcase and analyse the emergence of in-context catastrophic forgetting phenomena, and to show that using their ACL loss can help reduce it. They further evaluate their method and compare them to replay-free baselines from the CL and meta-CL literature, showing an advantage of their approach in scenarios with up to 3 tasks. 

The authors further test the limits of their approach by comparing it to more recent learning to prompt techniques for continual learning, leveraging the power of pretrained large models. This scenario show a limitation of the technique in more complex scenarios with more tasks, more diverse and complex data.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper takes an interesting perspective on continual learning, leveraging the interesting properties of SRWM and the capability of meta-learning to encode the desired behavior in the meta-learning objective. The combination of these two contributions is novel to the best of my knowledge, and lead to interesting insights. 

* The approach leads to interesting performance in relatively simple scenarios, outperforming some of the existing continual learning techniques. 

* I also particularly appreciated the authors discussion of the method limitations. Both the experiments with learning to prompts and the discussion provide very valuable insights that can help building on the work in the future.

Weaknesses:
* In my opinion, the main limitation of the approach is its practicality. From the experiments reported in Table 4, it seems that the approach requires to met-train on a sequence of similar length and/or complexity to provide its potential. This is not possible to know in advance in practice. Moreover, one limitation that the authors have not mentioned is that the meta-objective seems to require keeping in memory a number of copies of the model that is equal to the number of tasks. This can quickly become cumbersome for real applications that can require more complex models and very long sequences of tasks.  

* While the authors focus on classic benchmarks for continual and meta-learning, these benchmarks are artificial, relatively simple and lack of diversity. Different works highlight the limits of these benchmarks, I invite the authors to look at ""Meta-Album: Multi-domain Meta-Dataset for Few-Shot Image Classification"" Ullah et al. 2023, and ""NEVIS'22: A Stream of 100 Tasks Sampled from 30 Years of Computer Vision Research"" Bornschein et al. 2023 for examples of more realistic benchmarks. 

* It would be interesting to add a discussion of the cost of the approach (computation, memory, ...). Even is it gives a substantial boost in many cases, it would be interesting for practitioners to compare what they gain to what they pay.

Limitations:
The authors provide a detailed discussion of the work limitations, both in the experiments and the discussion sections. Some other limitations are highlighted in the Weaknesses paragraph above.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on Automated Continual Learning which is different than handcrafted continual learning. It uses self referential neural networks to meta learn their own in-context continual learning algorithm. First, the paper shows the emergence of in-context catastrophic forgetting. Second, the paper analyze the performance of proposed method (ACL) and finally the paper discuss the limitation of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is clearly written and easy to follow
- The paper introduces original idea of Automated Continual Learning
- The paper identifies ""in-context"" catastrophic forgetting

Weaknesses:
- The paper claims to do in-context continual learning but the concept of in-context learning is not clearly explained.
- The paper mainly focus on two task and five task settings but it would be more helpful to see the more different settings such as three task or four task
- How is the size of SRWM affects the maximum sequence length that can be train?

Limitations:
Authors have addresses the limitation of the work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper describes a method for in-context continual learning (CL) by using a type of meta-learning neural architecture based on ‘self-referential weight matrices’ (SRWM). Proposed in prior work, these models learn to modify weight matrices iteratively as they process more and more inputs. In this work, they are given few-shot examples from different tasks and iteratively update the weight matrices as the examples are processed. This update process is referred to as “in-context” learning in this work. The key innovation is to define the loss function of SRWM training to optimise for both forward (improving performance of subsequent CL tasks) and backward (improving performance of previous CL tasks) transfer while achieving good performance on the current task. Experiments are conducted on commonimage classification meta-learning benchmarks such as Split-MNIST and Mini-ImageNet. Results show the proposed method prevents catastrophic forgetting (without using replay), outperforming existing meta-learning baselines on the evaluated benchmarks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Studies the problem of in-context catastrophic forgetting via a two-task toy setting and reveals the issue when training with no backward transfer loss term. This is shown to be mitigated by including the backward transfer loss term.

Proposes an in-context CL method using models based on SRWM and a novel loss to mitigate catastrophic forgetting as more tasks are learned. The method does not use a replay buffer.

Studies and covers standard image classification meta-learning tasks such as Split-MNIST, FashionMNIST, and CIFAR-10. On Split-MNIST, shows improvements over existing CL and meta-baselines in both domain and class incremental evaluation settings. The improvements, when additional 5-task fine-tuning is used, is significantly above baselines. 

The paper is clearly written, with thorough literature review.

Weaknesses:
One weakness of the proposed method is that the number of loss function terms increases with the number of CL tasks, as pointed out by the authors in Appendix A.5. This prevents this method from being scaled to more practically relevant settings where a large number (much more than 2 or 3 that this paper has mostly focused the experiments on) of tasks are considered in a CL setting. Method of reducing the loss terms would strengthen the paper.

Another weakness, which is also noted by the authors in Table 4 and Section 4.3, is that the performance of the proposed model and method is poor compared with those based on pre-trained transformer models, even on an easier evaluation task. The authors in Section 5 also discuss a potential connection between LLM transformer training as an implicit version of the proposed model and method. Given these existing strong and more widely adopted methods, it is unclear how much value the proposed method adds. SRWMs are not widely used and LLMs training can scale to a massive number of tasks with a single loss [1] (albeit not CL). A more detailed explanation of the application of the findings of this paper beyond those interested in SRWMs would be helpful.

Another weakness of this paper is its focus on image classification meta-learning tasks only. It is helpful to know the generality of this method, for example on language modelling tasks or multimodal tasks. An experiment demonstrating the method in CL language tasks would be helpful.

[1] Finetuned language models are zero-shot learners. Wei et al. ICLR 2022.

Limitations:
Limitations have been adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies the problem of catastrophic forgetting (CF) by formulating continual learning (CL) as learning from a sequence of demonstrations of tasks. The paper proposes a meta-learning objective function that includes backward transfer terms. These terms compute the error of the predictor on previous tasks after receiving demonstrations of the current task.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The approach of formulating (continual learning) CL as learning from a sequence of demonstrations of tasks is interesting.
- The experiment shows positive results when compared to non-meta-learning approaches

Weaknesses:
- The paper is difficult to follow. Many definitions and the algorithm are not very well explained.
    - The motivation of formulating (continual learning) CL as meta-learning is not well presented.
    - Some details of the architecture are mentioned in the background section only (e.g. replacing self-attention with SRWN and the multi-head version.)
    - The details of the training and inference process are not well presented.
- The training process can be very costly and poorly scaled with the number of tasks and the number of examples per task. In each step over a sequence of demonstrations, the method needs to compute and store a new weight matrix in order to perform back-propagation. It might require more memory during training and at inference.
- Even being a meta-learning approach, the model still needs fine-tuning when given a new task to adapt to a new number of tasks.

Limitations:
There are no negative social impacts. My suggestions have been listed in the previous sections.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
qLtLQ4KUCq;"REVIEW 
Summary:
This paper proposed GSAAL to simultaneously address three changeling problems in outlier detection: inlier assumption (IA), curse of dimensionality (CD), and multiple views (MV).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper has a good flow.
The paper proposed the first outlier detection method that explicitly addresses IA, CD, and MV simultaneously. 
The paper has strong theoretical and empirical evidence to show the advancement of the proposed method. 
The experimental design is solid and the numerous visual examples help to facilitate understanding.
The paper has good reproducibility with open codes.

Weaknesses:
some (but few) places to improve.

Limitations:
The authors have analyzed the limitations sufficiently.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents this generalization of GAAL Generative Subspace Adversarial Active Learning (GSAAL) for outlier detection to address the limitation of the previous work such as multi-view and the curse of dimensionality, where the theoretical convergence, the scalability of the algorithm are discussed. Experiments on real dataset and synthetic tabular dataset are carried out to establish the validity of the approaches.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The manuscript presents a method called generative subspace adversarial active learning for outlier detection in multiple views. The proposed method called GSAAL provides the proof of convergence, the computation complexity and aims to address the curse of dimensionality. The outlier detection in high dimensional space indeed is an important and challenging solution. Thus, the proposed method can be a good solution to address this difficult problem.

The manuscript has compared the performance of GSAAL with other outlier detection approaches with detailed visual illustration and AUC. The experiments show advantages of the proposed solution over other competing methods. The experiments seems to be detailed.

Weaknesses:
The novelty of the work appears to be small. Theoretically, the derivation of theorem 1 is very similar to GAN derivation. 

In this case, the paper needs to compare their solution both theoretically and experimentally with the related work for outlier detection using GAN such as [1] https://arxiv.org/pdf/1906.11632 such as AnoGAN, BioGAN and EGBAD. 
[2] https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-022-00943-7
If we compare the main equation (2) in the manuscript with the formulation in reference [1] with conditional GAN and BioGAN, it seems the main difference are the proposed method used multiple detectors and accumulated the performance, which should not be considered a large distinction.

Due to the lack of the comparison with generative adversarial network based approaches such as AnoGAN and EGBAD, the potential improvement of the purposed method against the state-of-the-art approaches is not clear. The novelty of the paper does not stand on the safe ground. The theoretical derivation is also similar to GAN derivation.

Limitations:
Limited innovation and lack of critical comparison with important reference are the main issues of the current manuscript.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The main contribution of this paper is to improve existing work on Generative Adversarial Active Learning (GAAL) by using multiple discriminators for multiple views to detect outliers in tabular data. The training mechanism is similar to existing works. The paper also introduces a theoretical analysis on Multiple Views (MV). As claimed by the authors, GAAL addressed the problems of Inlier Assumption (IA) and Curse of Dimensionality (CD), but missed Multiple Views (MV), which is the main focus of this paper. The experimental results compare the proposed method to GAAL and some other classical methods such as OCSVM and KNN, ....

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper introduces an interesting view about MV and proposes a new method to address this MV problem together with theoretical analysis.

Weaknesses:
* The empirical results are not strong (or at least unclear in the way the authors presented in the main paper); most of the experiments are on synthetic datasets.
* The results on the real dataset do not seem to show significant improvements compared to existing work (or at least it is hard to observe this when reading the paper). Perhaps the authors could improve the writing and highlight the results better. It is unclear to me why the experiments on the real dataset were put in the Appendix, as it is an important result.
* The paper claims at the beginning that it not only improves the MV problem but also the IA and CD problems, but this is hard to see with the current writing of the paper. Could the authors highlight the experiments in the paper to prove that claim?

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
CcNw4mVIxo;"REVIEW 
Summary:
This paper proposes to use spiking neural networks (SNNs) to slice the event stream in an adaptive manner before passing the voxelized events to the downstream inference model. The first step of the proposed method divides the input event stream into voxelized event cells with the same temporal interval. An SNN, constructed to have a scalar output, then takes the event cells recurrently. The timestamps when the SNN generates spikes are considered to be the slicing positions. The slicing SNN and the downstream inference model are trained together using the membrane potential-driven loss and the linear-assuming loss. The feedback-update strategy allows the two networks to be trained end-to-end. Extensive experiments on a toy example, object tracking, and recognition demonstrate that the proposed method can be easily integrated into existing models, bringing a noticeable performance improvement.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper focuses on an interesting yet underexplored problem, which is to use a data-driven model to adaptively construct the event voxels. The proposed method is intuitive, and the key idea is convincing. It is clear that the authors have put in great effort in preparing this submission.

2. The key technical contribution involves two parts. First, the feedback-update strategy allows supervision signals from the downstream ANN to back-propagate to the SNN. Additionally, the membrane potential-driven loss and the linear-assuming loss control the spiking time through the supervision of the membrane potential value. The two parts complement each other, leading to an end-to-end trainable model.

3. Additionally, the paper also discusses how the hyperparameter $\alpha$ can be tuned together with the SNN weights and analyzes the implication of different $\alpha$ values to the spiking behavior. 

4. The experiments are very extensive. The SpikeSlicer has been validated on several event-based applications, demonstrating its prediction quality, efficiency, and the fact that it can be easily incorporated into existing models.

Weaknesses:
1. Despite the strengths above, the key design appears to be a bit simple. As a potential NeurIPS paper, this work is relatively weak on the technical sophistication and theoretical insights. However, this is complemented by extensive experimental evaluation and empirical analysis. 

2. While SNNs are efficient and consume less energy than ANNs, SNNs are also less capable than ANNs. Since the speed of the entire SNN+ANN prediction pipeline is going to be slow anyway, it may be worthwhile to investigate whether using an ANN as an event slicer can lead to better prediction quality.

3. While the proposed losses are justified by proposition 1 and empirical analysis, it is unclear if the proposed feedback-update strategy is the best way to identify the desired trigger time $n^*$. In particular, it is unclear if the argmin operator can return any meaningful signal during the initial training stages.

Limitations:
Yes, the authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work proposes a novel method for adaptively sample event data and subsequently preprocess it, utilizing a spiking neural networks (SNNs) as module.

The sampling method involves a feedback mechanism that triggers the activation of the SNN.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Tests are done on dataset with different lighting conditions. Being robust to different event rates.

Weaknesses:
The experiments conducted do not contain tasks such as optical flow, object detection, or image reconstruction. The type of tasks tested is limited.

Limitations:
The implementation of the code appears to be challenging, which may affect its reproducibility.
The application of this algorithm in embedded systems seems to be constrained due to the use of Spiking Neural Networks (SNNs).
Furthermore, it is unclear whether the code will be made publicly available.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors designed a plug-and-play event processing method, SpikeSlicer, to split event streams with an adaptive amount. The proposed method is a lightweight SNN, constrained by a custom Spiking Position-aware Loss (SPA-Loss) to regulate neuron states. Additionally, a downstream ANN refines the slicing decisions using a feedback-update training strategy.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The proposed plug-and-play event processing method, SpikeSlicer, based on SNN representation, which can be used for various vision tasks with event cameras. 
* The downstream ANN refines the slicing decisions using a feedback-update training strategy, and the performance of the downstream ANN provides feedback to adjust the representation.
* Experimental results demonstrate that SpikeSlicer can effectively enhance the performance of object tracking and recognition with event cameras, while also leveraging the advantages of neural computation in processing speed and power consumption.

Weaknesses:
* The comparison algorithm for event-based object tracking, DiMP, is from 2019. Why not try the latest methods? In recent years, many studies have focused on improving the effectiveness of event stream representation to enhance the performance of event vision tasks. 
* Many methods for object detection and tracking with event cameras have not been compared.

Limitations:
There is not much discussion on this aspect.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
Z2f4Laqi8U;"REVIEW 
Summary:
Authors propose a greedy structure learning algorithm in the context of ancestral graphs. The score function is based on the normalized likelihood. The score is decomposable using the concept of ac-components. Limited experimental results are presented as experimental evidence.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- The multivariate cross-entropy decomposition is explained in details, especially the link between its definition and its application in the context of ancestral graphs. Leveraging this framework allows a flexible and effective definition of scoring criteria for the class of graphical models.
- The usage of ac-connecting paths and ac-connected subsets for the likelihood decomposition (theorem 1) is the main result of the paper and allows to map the structure of an ancestral graph to a specific estimand.

Weaknesses:
- In the case of ancestral graphs, it's crucial to understand the differences between edge patterns. I would suggest to refactor Figure 1 to improve readability, e.g. isolating different edge patterns combinations with horizontal lines.
- The performance of the proposed solution is poor. For instance, Figure 2 most of the metrics overlap with existing solutions.
- The experiments keep the number of samples fixed even is the number of the parameters of the model increase, reducing the information available for the biggest models. Moreover, a sample size of 100 for BARLEY (114,005 parameters) is rather unrealistic.

Limitations:
- The experimental evidence is poor: few reference models with rather sparse hyperparameters space exploration.
- The search and score algorithm limits the number of possible exploration paths to the barely minimum.
- It is unclear what is the actual computational burden of the proposed score.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors present a novel likelihood decomposition of ancestral graphs, which is based on a novel concept ac-connected subset. With this decomposition, the authors present an efficient hybrid causal discovery method. But it is worthy to note that in the implementation of the causal discovery method, they use the approximate local scores limited to the closed surrounding vertices of each node and edge, but not the exact likelihood score.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The likelihood decomposition of ancestral graphs is very novel and could be useful in the future.
2. For me, I encourage the studies on the MAG/PAG learning methods. It is a hard and fundamental problem.
3. The paper is clearly written.

Weaknesses:
See questions.

Limitations:
No.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper defines a score for ancestral graphs through an inclusion-exclusion expansion and implements it into a hybrid search approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The likelihood decomposition (Theorem 1) is nice and there seems to be a slight improvement in the empirical testing.

Weaknesses:
There are close overlaps to published and preprint works by Hu and Evans, so it is hard to assess the novelty here. For example Theorem 1 (in another form) is from reference 14. 

Then in the simulation studies, relevant methods from the references are excluded, even though the greedy search approaches (like references 13 and 32 for example) are not exact, but also heuristic. These (and similar) would need to be included in the benchmarking to judge the importance of the work.

Limitations:
The limitation on the depth of the inclusion/exclusion is discussed in the limitation section, but maybe this wasn't so clear throughout the paper.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a greedy search-and-score algorithm for causal structure discovery in ancestral graphs which allow for both directed and bidirected edges.The paper provides an explicit decomposition of the likelihood function of ancestral graphs
in terms of multivariate cross-information over relevant ‘ac-connected’ subsets of variables which is related to head-and-tail factorization developed in prior work. In the main theoretical result authors show that cross-entropy and the likelihood function of an ancestral graph can be decomposed in terms of multivariate cross-information for ac-connected components. As a corollary this implies that two ancestral graphs are Markov equivalent if and only if they have the same ac-connected subsets of vertices.
Authors use the likelihood function decomposition in terms of cross-information of ac-connected componenetsto propose a search-and score algorithm that computes scores for nodes and edges and probes orientations of edges to minimize the score.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper proposes a new empirical algorithm for causal structure discovery in ancestral graphs. The practical algorithm is motivated by a theoretical decomposition of likelihood function in terms of multivariate cross-information summed over ac-connected components. The developed algorithm is scalable to graphs with several dozens of vertices and links (thousands of unknown parameters)

The authors provide experimental results on both synthetic datasets and challenging benchmarks from the bnlearn repository, showing that developed algorithm typically has higher precision and similar recall to prior MIIC algorithm.

Weaknesses:
The proposed algorithm seems to be only empirical and lacks theoretical guarantees and relies on MIIC as a base of the algorithm. To my understanding, the paper does not provide a guarantee that the algorithm always converges in a reasonable number of steps (even if not to a global minimum). Multivariate cross-information over a large set of variables may be very sensitive to noise, which I think is a weakness compared to algorithms that use cross-information only over small number of (e.g. triples) of variables.

Also as the authors point out Theorem 1 is equivalent to the decomposition established in prior work, which makes novelty of the main theorem somewhat questionable.

I also think that it will be beneficial to the reader if the connection between the theoretical results in Theorem 1 and the proposed algorithm are explained a bit more deeply with a more clear connection of how that theorem is being used.

Limitations:
na

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
1f82rnwCbl;"REVIEW 
Summary:
The authors take a specific natural language game (One Night Ultimate Werewolf) and study the performance of RL-trained LLM agents with respect to the mathematically derived (by the authors) Nash equilibrium solutions.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
[1] The paper chooses ONUW as a game that has the Nash equilibrium solution to be used as an oracle/reference. The paper then applies learning to improve the 0-shot answers/behavior of LLMs. The learning is performed with offline RL over the collected game episodes.

[2] Even though the paper laser-focuses on ONUW, its methodology and results can be a valuable stepping stone to solving real-life problems.

[3] As negotiations are an integral part of the society, its automation and improvement of effectiveness is a valuable direction of research. This paper makes a small but definite step in the direction of automated LLM-powered negotiations.

[4] The authors do a great job of rigorous and comprehensive research of ONUW optimal strategies for players by proving theorems for several specific cases.

[5] The formalized way to assess the performance of the learned policies with the help of NashConv (distance to equilibria) is very valuable and insightful.

[6] In 6.3 and 6.4 the authors show that the policy learned with ChatGPT4 can be transferred to Gemini thus demonstrating the generalization across backend LLMs.

[7] As described in E.2 the use of text embeddings to encode the state is intriguing.

[8] The broader impact is well discussed.

Weaknesses:
[1] Per se the entire paper is bound to the ONUW game and its rules. The ability of the bundle of proposed algorithms and proofs to transfer to different games or game-like environments like financial markets is not demonstrated.

[2] I have not found any examples of the discussions between the LLM players in the manuscript. Further, the code is not provided by the authors. Thus there is no way to see the actual discussions between the LLM players and assess the value and quality of the generated discussion.

[3] Artificial prior knowledge is inserted into the solution that constraints the model to choose between 6 options: Honest/Deceptive and Evidence/Accusation/Defense. The proposed model is seemingly limited by these options and does not allow learning or discovery of alternative discussion strategies.

[4] It would be good to have a random choice baseline and compare the proposed method to its performance.

[5] The paper studies a game with one round of negotiations that has limited practical use in the light of the fact that negotiations normally are conducted in several rounds.

Limitations:
The limitations are well discussed or clear in principle.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper delves into the strategic aspects of discussion in the context of a population social deduction game, ""One Night Ultimate Werewolf"" (ONUW) game. By analyzing Perfect Bayesian Equilibria in scenarios with and without discussion, the authors highlights the pivotal role of discussion tactics in influencing players' beliefs and utilities. They propose a novel framework that employs reinforcement learning to instruct an LLM agent, enabling it to decide and execute strategic discussion policies. The authors empirically show that their framework can recognize and approximate the equilibira, and achieve strong performance across diverse ONUW settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. The paper introduces a new complex communication game, ONUW, and provides a clear explanation of the problem formulation. The ONUW game offers interesting role deduction challenges and seems it can be a benchmark for evaluating the deduction ability of LLM agents.
2. The authors present thoroughly theoretical analyses on a specific setting of the ONUW game, which prove the significance of discussion on influencing other players' belief and provide a solid foundation for their following proposed method.
3. The idea of integrating RL policy into LLM agents to improve their strategic ability in communication games is innovative. It has the potential to be applied in future work related to strategic LLM agents in various situations. And the empirical results shows the efficacy of this method.

Weaknesses:
1. By integrating RL policy into the reasoning process, the authors aim to improve the discussion ability of LLM agents, which provides a new angle for constructing LLM agents. However, applying RL policy to select discussion tactic seems analogous to selecting action candidates in related work [1].
2. Since the RL policy is trained on game logs generated by LLM agents, it is possible that players in the training dataset appear again during testing, resulting in less convincing results.
3. Typos：
   1. In line 115, 126, player i's information state should be $h^i \in \mathcal{H}^i$.
   2. The symbol $q$ is used twice as the probability that Werewolves vote for Player 3 (line 194), and as the discussion policy in Equation 6.
   3. In line 224-225, it seems that $\theta^i$ refers to player i's derived belief on all players' types, while $\theta$ refers to the groundtruth. These two symbols should be further distinguished.

> [1] Zelai Xu, et al. ""Language agents with reinforcement learning for strategic play in the werewolf game"".

Limitations:
The authors have stated the limitations about their work in Section 7.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors investigate the social game ‘One Night Ultimate Werewolf’ (ONUW) as a testbed for their framework on RL-instruct-tuning an agent to select optimal discussion tactics. They prove the existence of a Perfect Bayesian Equilibria in the game ‘Werewolf’ when the game consists of a single round and show the effectiveness of their approach in a three-player setting as well as a five-player setting to prove generality.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The authors introduce a novel RL-instructed LLM-based agent framework specifically tailored for strategic discussions in ONUW focused on identifying the best discussion tactics in communication scenarios as well as providing an environment for further testing.

In addition, the authors further provide a theoretical analysis of the ONUW game, formulating it as a Multi-Phase Extensive-Form Bayesian Game and establishing the existence of Perfect Bayesian Equilibria in different scenarios

Weaknesses:
The primary weakness of the paper is noted by the authors, where the discretization of the space of discussion tactics results in what could be an oversimplification of the dynamics that would typically occur in such a game. For example, what could occur in a typical werewolf game is that the werewolf makes no effort to defend themselves, implying the other villagers are bullying them in an attempt to garner false sympathy. 

The authors also acknowledge another weakness in that identifying and discretizing the space of discussion tactics in a game is a manual and time-consuming process. While the experiments in the paper imply generalization to X number of players, this generalization is still limited to the ONUW game.

A final weakness is that the results are somewhat limited by the experiments only being performed with LLMs rather than including human players as well. However, the reviewer acknowledges the additional time and monetary costs performing such experiments would occur.

Limitations:
The authors touch on some limitations of their work, however it is unclear whether the topics addressed in the questions (the impact of human participants, the word required to pre-define the decision tactics) were not included due to being outside the scope of the work, or not relevant given some metric or detail the reviewer may have missed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents an innovative framework for enhancing the discussion capabilities of language agents in the game ""One Night Ultimate Werewolf"" (ONUW) using reinforcement learning (RL). The authors propose a multi-phase extensive-form Bayesian game formulation for ONUW, analyze perfect Bayesian equilibria in both discussion and non-discussion scenarios, and develop an RL-trained discussion policy. Experimental results demonstrate the effectiveness and generalizability of the framework across various game settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The theoretical analysis of the game, including the formulation as a Bayesian game and the derivation of equilibria, is thorough and well-executed. The experimental design is robust, utilizing state-of-the-art LLMs and a novel RL training methodology.

2. The paper is well-structured and clearly written. The authors provide detailed explanations of the game mechanics, the theoretical framework, and the RL training process, which make the complex content accessible to readers.

Weaknesses:
1. Comparison with Related Work: While the paper provides an innovative approach to using RL and LLMs in the Werewolf game, there is a need for a more detailed comparison with closely related works, particularly those combining LLMs with reinforcement learning strategies. For instance:
Xu et al. (2023) [1] also explore strategic play in the Werewolf game using language agents trained with reinforcement learning. A comparative analysis highlighting what differentiates the current approach from Xu et al.'s methodology would clarify the novelty and the specific advancements made.
Wu et al. (2024) [2] utilize offline RL and a dataset-driven approach to enhance reasoning in LLMs within the same game context. Discussing how the methodologies differ, especially in terms of model training, dataset utilization, and resultant agent behavior, would strengthen the current work's positioning within the field.

2. Lack of Human Evaluation: The experimental section primarily focuses on win rates to demonstrate the effectiveness of the proposed framework. However, Werewolf (ONUW) involves complex human interactions and strategic discussions that might not be fully captured by win rates alone. The game's social and psychological aspects, such as bluffing and persuasion, are crucial:
It would be beneficial to include human evaluations to assess the quality of the AI's gameplay and its ability to mimic human-like strategic discussions. This could involve subjective assessments from experienced human players regarding the AI's ability to integrate seamlessly into human gameplay, its strategic depth, and its communication effectiveness.

LLM+RL：[1] Zelai Xu, Chao Yu, Fei Fang, Yu Wang, and Yi Wu. Language agents with reinforcement learning for strategic play in the werewolf game. arXiv preprint arXiv:2310.18940, 2023.
LLM+offline RL, dataset: [2] S Wu, L Zhu, T Yang, S Xu, Q Fu, Y Wei, H Fu, Enhance reasoning for large language models in the game werewolf.  arXiv preprint arXiv:2402.02330, 2024

3. Dataset Size and Composition: Detailed information on the size and composition of the dataset used for offline RL is necessary. Understanding the diversity and representativeness of the game logs in the dataset would help in assessing the potential generalizability and robustness of the trained models. This includes the number of game sessions, variety of player strategies, and the range of game outcomes included.

Limitations:
The authors should consider expanding the discussion on the limitations related to the discretization of discussion tactics and the potential over-reliance on specific datasets. Suggestions for future work could include exploring methods for dynamic tactic generation or adjustment based on real-time gameplay feedback, which could help in developing more adaptable and robust AI agents for complex communication games. Additionally, addressing the computational demands and proposing more resource-efficient models could make the technology more accessible for broader applications.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
WCnJmb7cv1;"REVIEW 
Summary:
This paper presents Empowerment via Successor Representations or ESR, a technique that builds an assistive agent that maximizes the human collaborator's ability to influence the world. The key motivation the authors provide for building an assistive agent that seeks to empower the human rather than explicitly provide support by inferring the human collaborator's reward function is that inferring such a reward function can be challenging. The authors present a formulation for empowerment and connect empowerment maximization to reward maximization in Section 3. The authors then present an implicit approach to estimate empowerment via several learned representations and present loss functions for how these representations can be inferred and utilized. Finally, the authors present a set of experiments displaying that this technique can outperform a related prior work (AvE) as well as a random baseline.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Strengths
+ The presented formulation is very interesting and shows promise. To the best of my knowledge, the utilization of such representations to estimate empowerment is new.
+ Generally, the results are impressive. It is clear that ESR outperforms the prior technique by large margins in the chosen domains.

Weaknesses:
Weaknesses
- Paper's claims should be better grounded:
1. For the proof in Section 3.3, the conclusion is that an assistive agent maximizing the mutual information between a future state and a chosen skill by the human minimizes the worst-case regret incurred by the human. Is this only true for a two-agent scenario and full observability? Does the proof have any assumptions for the robot and human maintaining the same representation and action space?
2. The paper mentions that inferring a human's reward function can be error-prone and challenging. However, I would think inference of several representations (especially online), could lead to similar outcomes (poor consequences etc.) It isn't clear why inferring these approximate representations and seeking to maximize empowerment would work better than reward inference. I'm specifically thinking about the cases mentioned in the introduction, where humans have changing preferences, act suboptimally, etc. 
3. The term high-dimensional is used in the introduction but the chosen domains seem to be relatively low-dimensional compared to other collaborative testbeds encountered in robotics (see [1]) and human-AI collaboration (see [2]). Could you clarify this term?
- Evaluation is limited: 
1. As this work aims to develop assistive agents that work with humans, it should compare with actual humans in a user study to validate the synthetic findings. 
2. The evaluation is only conducted with respect to one baseline (AvE) and a random agent. The author's key rationale behind this technique was assisting humans without inferring reward functions, so it would be beneficial to test against a framework that actively infers the user's reward function. 
- Some claims are not well-explained.
1. The introduction notes that AvE does not scale to high-dimensional settings but does not make it clear why this framework failed to scale.
2. The authors show and mention AvE underperforms a random agent but does not mention why. Could you provide further details? Also, could the authors comment on why ESR was able to outperform AvE by such a wide margin and note any qualitative findings about the learned collaborative behavior?


[1] Liu, Puze, et al. ""Safe reinforcement learning of dynamic high-dimensional robotic tasks: navigation, manipulation, interaction."" 2023 IEEE International Conference on Robotics and Automation (ICRA). IEEE, 2023.

[2] Paleja, Rohan, et al. ""The utility of explainable ai in ad hoc human-machine teaming."" Advances in neural information processing systems 34 (2021): 610-623.

Limitations:
Yes.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a method for assistance via empowerment based on contrastive successor representations, while also introducing a number of theoretical results about the relationship between assistive empowerment (understood as maximizing the mutual information between human actions and future states) and assistive reward maximization in information geometric terms. The proposed method, empowerment with successor representations (ESR) is shown in experiments to outperform a baseline method, AvE, which estimates empowerment using Monte Carlo rollouts. The experiments show that ESR scales to more complex problems than AvE, including environments with image based observations.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper introduces a new method, ESR, for training agents to assist users by maximizing empowerment, which does not require the agent to infer human goals or preferences (as in methods based on inverse planning or inverse reinforcement learning). Unlike previous methods for assistance via empowerment, which make use of Monte Carlo rollouts to estimate variance, ESR's method for estimating empowerment is more scalable, drawing upon ideas in contrastive representation learning to effectively estimate the mutual information between human actions and future states. This allows ESR to be applied to more complex problems, increasing the applicability of assistance via empowerment to more contexts. The underlying ideas behind this method are interesting, and the method appears to be effective (at least with respect to the AvE baseline). As such, I believe the paper will be of some interest to others working on AI assistance and human-AI alignment.

Weaknesses:
While the ideas behind this paper are interesting, and the method seems to work reasonably well, the presentation of the framework could be significantly improved. The empirical evaluations could also be made more rigorous by comparing against non empowerment-based baselines.

**Presentation**

This paper was hard to understand on its own, even after reading the Appendix. I had to read Eysenbach (2021) to really understand the theoretical results, and van den Oord (2019) to understand how the proposed method worked. I think a well-presented paper wouldn't have had that issue.

In the information geometry section, important quantities such as the skill variable $z$ and state occupancy measure $\rho(s)$ are not properly defined. This makes it hard to understand all the different state occupancy measures $\rho(z)$, $\rho^+(z)$, $\rho^*(z)$ that are introduced, and what exactly the prior over states means (or why the human would be able to choose this prior). As a result, it's hard to evaluate the soundness of the arguments (without, e.g., reading Eysenbach (2021)), or whether the assumptions about the human policy underlying Lemma 2 are reasonable ones. It's also not explained how the mutual information $I(s^+; z)$. between future states $s^+$ and human skills $z$ is related to the definition of empowerment in Equation (1), which involves the (conditional) mutual information $I(s^+; a^H_t | s_t)$ between future states and human actions $a^H_t$. Since this is not explained, it's hard to see how Section 3.2 and 3.3 are relevant for actually maximizing empowerment as it's defined in Section 3.1.

In section 4, where the ESR algorithm is introduced, it's also not explained how or why the contrastive representation learning objective in Eq. (7) should lead to right density ratios to be estimated upon training convergence. One has to read van den Oord (2019) to understand why, so this section is not really accessible to readers not already familiar with contrastive learning methods. Notation is also not clearly defined -- for example, why does $\phi$ switch from being a 3-argument function in Line 191 to becoming 2 argument function in Line 198? I have the inkling that this has to do with marginalizing out the assistant's action $a^R$, but this is not explained. I'm also confused why $a^R$ needs to be part of the successor representation at all. By including $a^R$, won't the resulting mutual information you're estimating end up being $I(s^+; a^H_t | s_t, a^R_t )$, which is conditional on *both* the current state $s_t$ and the assistant's action $a^R_t$?

Perhaps one reason why this paper ends up being hard to follow is because it's trying to do too much by both introducing the results in Sections 3.2-3.3, while also introducing a (seemingly unrelated) method for empowerment maximization in Section 4. As it stands, these two parts of the paper feel quite disjoint to me, and it's not obvious how they form a cohesive whole. It might have been better to focus on just one or the other -- e.g. on just explaining and carefully justifying the method in Section 4 -- so that everything is more understandable and cohesive.

**Evaluation**

Even though one of the main selling points of assistance via empowerment is that it does not require performing goal inference (which can be hard to scale when the goal space is large), the experiments do not compare ESR against any goal inference methods. This is in contrast to the original AvE paper by Du et al (2020), which does conduct fairly thorough comparisons against goal inference baselines. As a result, it's hard to evaluate exactly how valuable ESR is, and whether one should prefer it over goal inference as an assistance method. This should be addressed in future versions of the paper.

Limitations:
The authors adequately discuss the technical limitations of their approach. They also have appropriately noted the risks of focusing solely on empowerment for assistance, as this might empower actors who already unjustly have too much power.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a new training objective that motivates the agent to assist humans by maximizing their empowerment, agnostic of their rewards. The proposed empowerment objective is derived from mutual information between human actions and future states, which is estimated via contrastive representation learning and can be formulated as an RL reward. The authors empirically show that this training objective improves performances in a grid world game and the overcooked game over existing baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The writing of the paper is clear and easy to follow.
- The proposed method is novel to my knowledge.
- The authors have shown theoretical insights into their definition of empowerment to a certain degree.

Weaknesses:
- My main concern is the lack of baselines. The only baselines that the author chose to compare are AvE, which is another empowerment-based method that this work is based on, and random. From the results, AvE shows to be weaker even than random in most of the scenarios, which makes it less appealing as a baseline. At least for the Overcooked domain, there are plenty of baselines in [1] to choose from (or at least argue why they are not chosen).
- Following the previous point, even though there may not be enough empowerment-based methods to compare, the authors can still provide ablation studies to justify the design choices. As of now, the empirical result feels thin to me.
- The authors could provide more insights in the experiment section. For example, how well does the proposed method estimate empowerment? Or qualitatively,  what does the agent do to increase empowerment?  More analysis can help us better understand the method.

[1] Carroll, Micah et al. “On the Utility of Learning about Humans for Human-AI Coordination.”

Limitations:
The authors have addressed the limitations and potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the problem setting of human-agent collaboration where the agent learns to empower human decision-making to exert greater control over the environment. By connecting empowerment to reward maximization, the paper proposes the ESR method, which learns an intrinsic reward function based on learned representations of future states. Experiments demonstrate that ESR outperforms the AvE baseline in a gridworld task and Overcooked.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. To the best of my knowledge, the ESR method is novel. ESR provides the new insight of connecting empowerment and reward maximization. This allows using empowerment as an intrinsic reward for training agents with RL.

1. The paper shows ESR greatly outperforms the AvE baseline on more complex obstacle gridworlds and in Overcooked. In more complex environments, AvE is only able to achieve near-random performance. Results are also validated over multiple random seeds. 

1. The code is provided to reproduce results.

Weaknesses:
1. The paper only shows results on 2 of the Overcooked environments. This decision is not justified and with 15-20 seeds on the main Overcooked results, it is likely not an issue of sufficient compute to run these experiments. Even if the performance is worse on the remaining environments, the paper should also report numbers on the full Overcooked setting with ""Asymmetric Advantages"", ""Forced Coordination"" and ""Counter Circuit"".

1. While the method addresses the problem of coordination without knowledge of the human's goal, the experiments demonstrating this are contrived. In Overcooked, the goal is fixed per task setting and is serving a dish. While ESR outperforms AvE, it greatly underperforms baselines from population-based training in [1]. This shows the large gap in empowerment versus the underlying objective of the task. The paper does not show results in a setting where the goals and preferences of the human are complex to communicate.

1. The description of AvE lacks suffcient detail given it is the only baseline and addresses the same problem. Why does AvE struggle in more complex environments? How does it differ from ESR? 

Minor: 

1. The acronym ESR is not defined in the main text and is only defined in the caption of Algorithm 1. 

References:

1. Carroll, Micah, et al. ""On the utility of learning about humans for human-ai coordination."" Advances in neural information processing systems 32 (2019).

Limitations:
Yes, the paper discusses the limitations and safety risks in Section 6.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the problem of human-AI collaboration. The goal is to train an cooperative policy that can work with human together in the environment to achieve a shared goal.

The key idea of this paper is to maximize the influence of the human's actions on the environment, which is called empowerment. The paper provides an information geometric interpretation of empowerment and develops a algorithm based on SAC for estimating and optimizing empowerment. This method does not require to infer human's reward model.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Code is provided.
2. The method removes the need to infer human reward function, making it scalable to high-dim space.
3. The method is theoretically grounded and is simple to implement.

Weaknesses:
* No experiments involving real human subjects are conducted. 
* As suggested by the limitation, it's infeasible to train such a policy with a real human subject, especially in the task like overcooked which takes 1.5M steps to train...
* The paper only compares to one baseline AvE. As the overcooked is a widely used environment studying human-robot collaboration, more results are welcome.

Limitations:
The limitations are well addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
TKozKEMKiw;"REVIEW 
Summary:
This paper formulated the problem of finding an optimal decision tree as Markov Decision Problem and solve the scalability problem using an information-theoretic test generation function. This method provides a trade-off between the train accuracy and tree sizes, the decision tree naturally offers interpretability over ML algorithms.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper is well-written and well-organized, it combines RL and decision tree generation, and building MDP before constructing the decision tree.

Weaknesses:
1. The paper lacks sufficient novelty. The approach of constructing a Markov Decision Process (MDP) and using Decision Trees (DT) to generate actions is not new. Specifically, Algorithm 1 appears to still rely on Classification and Regression Trees (CART) for splitting criteria, which diminishes the originality of the proposed method.
2. The evaluated scenarios in the paper are not clearly articulated. The algorithm has not been tested against well-known benchmarks, unlike other optimal DT algorithms. This makes it difficult to assess the comparative performance and robustness of the proposed approach.
3. The advantages of using this algorithm instead of CART are not clearly demonstrated. Both algorithms control tree size and depth. However, CART is known to converge faster and offers a simpler implementation. Without clear evidence of the benefits, it is hard to justify the use of the proposed method over established techniques like CART.
4. The definition of actions generated by the tree is ambiguous. It is not clear whether the actions are discrete or continuous. If the algorithm is designed to build an MDP, it should be tested on general reinforcement learning (RL) tasks to validate its effectiveness and applicability in broader contexts.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors pose binary decision tree construction within the framework of Markov Decision Processes. They first propose methods for constructing an MDP from a decision tree construction problem, exploring varying test generating functions that trade off the coverage of the search space vs the size of the search space. They then apply Dynamic programming to solve the resulting MDP and show this learnt method can both create binary trees that minimise the loss over a dataset but that it can also be used to add additional losses a user may have over decision trees such as the prior that trees should be small, making them interpretable. They evaluate their proposed method, comparing with other high performing methods such as Quant-BnB, MurTree and a DeepRL method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Well written paper and easy to understand the method
- Clearly an important direction of research
- Thorough experiments with appropriate baselines and good range of datasets to ensure the conclusions generalise to a wide range of datasets
- Code fully provided, along with implementations of baselines used

Weaknesses:
- Various versions of Reinforcement Learning for binary tree construction have previously been explored. While the implementation in this paper is ultimately different and appears to significantly improve performance, there is limited novelty of the approach. Novelty largely comes down to the test generating functions explored and the addition of extra losses (interpretability) in addition to just the dataset accuracy.

- Small formatting issue
     - Table 1 is too small

Limitations:
- Limitations are appropriate addressed

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper models the construction of decision trees as a reinforcement learning problem. Currently SOTA algorithms for constructing decision trees have the drawbacks that 1) they take long to compute at depths > 3, and 2) the trees constructed are complex and difficult to interpret. By modelling the problem as an RL task, the authors hope to make the construction of decision trees scale to larger sizes. They present Dynamic Programming Decision Trees which models tree construction as a MDP solved using dynamic programming. They evaluate the accuracy of trees produced by their approach empirically against other commonly used approaches.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**Originality:** The approach presented is a novel approach to constructing dynamic trees.

**Significance:** As datasets become larger and interpretability becomes more important, having an approach that scales DT construction to larger trees is needed now. This makes this work rather significant.

**Clarity:** The first half of the paper (up to Sec. 4) was clear. It becomes harder to understand after. Providing an intuitive explanation certain equations would be helpful. E.g., why is probability $p_l = |X_l| / |X|$?

**Quality:** The technique designed is sound and the experiments chosen were the correct ones to demonstrate their claims.

Weaknesses:
The experimental evaluation is weak. From what I understood, the algorithms being evaluated were run only once and evaluated once. The results do not statistically back up the authors' claims. Multiple runs with statistical significance testing is needed.

There is no actual analysis on the interpretability of the trees produced, only the complexity of the trees.

Limitations:
The authors mention one limitation (test generation) as a problem. It seems that that would make it difficult for DPDT to actually scale to larger trees. Is that not so? Is scalability not a limitation then?

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes to use an approach for learning interpretable
decision trees using markov decision processes. The results are shown
to be competitive with branch and bound methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
None of notice, given the listed weaknesses.

Weaknesses:
There exists extensive experimental evidence challenging the claims
about the interpretability of decision trees, while simultaneously
demonstrating the need for decision trees to be explained, since these
can otherwise exhibit arbitrary explanation redundancy.

As a result, and at present, there is no practical justification
whatsoever to learn so-called interpretable optimal decision trees.
It is absolutely unclear that optimal decision trees will provide any
advantage, regarding computed explanations, over decision trees
induced with heuristic algorithms.

Given the above, I cannot recommend acceptance.

Some references on the necessity of explaining decision trees.

Xuanxiang Huang, Yacine Izza, Alexey Ignatiev, João Marques-Silva: On
Efficiently Explaining Graph-Based Classifiers. KR 2021: 356-367

Gilles Audemard, Steve Bellart, Louenas Bounia, Frédéric Koriche,
Jean-Marie Lagniez, Pierre Marquis: On the Computational
Intelligibility of Boolean Classifiers. KR 2021: 74-86

Yacine Izza, Alexey Ignatiev, João Marques-Silva: On Tackling
Explanation Redundancy in Decision Trees. J. Artif. Intell. Res. 75:
261-321 (2022)

Gilles Audemard, Steve Bellart, Louenas Bounia, Frédéric Koriche,
Jean-Marie Lagniez, Pierre Marquis: On the explanatory power of
Boolean decision trees. Data Knowl. Eng. 142: 102088 (2022)

Gilles Audemard, Steve Bellart, Louenas Bounia, Frédéric Koriche,
Jean-Marie Lagniez, Pierre Marquis: On Preferred Abductive
Explanations for Decision Trees and Random Forests. IJCAI 2022:
643-650

João Marques-Silva, Alexey Ignatiev: No silver bullet: interpretable
ML models must be explained. Frontiers Artif. Intell. 6 (2023)

Limitations:
These were listed above. I believe the paper is solving a non-relevant
problem given practical and theoretical evidence regarding the
non-interpretability of decision trees, be these optimal or not.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
GxpkSJjbQR;"REVIEW 
Summary:
This work introduces a novel approach to diagnosing Alzheimer's Disease using a decentralized expert system. This system leverages blockchain technology and Federated Learning to enhance data privacy and manage large volumes of MRI data effectively. The key innovation lies in integrating these technologies to address the challenges of traditional diagnostic methods, which often suffer from delays and inaccuracies, especially in the early stages of the disease.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. This work presents a pioneering integration of blockchain technology and Federated Learning to enhance Alzheimer's Disease (AD) diagnostics, addressing privacy concerns and data management challenges.
2. The proposed decentralized expert system architecture, which includes anomaly detection for patient-submitted data, showcases a comprehensive approach to AD diagnostics, emphasizing AI-driven MRI analysis.

Weaknesses:
1. While the system shows promising results, the article does not provide extensive comparative data against traditional centralized systems or other decentralized approaches, which could validate its superiority more robustly. This work lacks of comparative performance data.
2. The complexity of the blockchain and Federated Learning components might pose usability challenges for less technically adept users, potentially affecting the system's adoption.
3. There are no more details of the algorithms this work used, maybe give out more meaningful algorithm design for the specific model you are using.

Limitations:
1. The accuracy of the AI model heavily depends on the quality and consistency of input data, which might vary significantly across different healthcare settings.
2. The use of advanced technologies such as blockchain might limit the accessibility of the system for users not familiar with such technology, potentially restricting its applicability.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors assume that applying blockchain platforms to combine datasets for Alzheimer’s Disease and then using federated learning for multi-centralized training can improve diagnostic performance. However, the manuscript lacks technical details and experimental evidence. All descriptions are conceptual, making the manuscript a proposal rather than a technical paper.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
It is interesting to apply blockchain platforms to combine datasets for Alzheimer’s Disease and then using federated learning for multi-centralized training.

Weaknesses:
There are no technical details and no experiments. Details can be found in Questions and Limitations.

Limitations:
1.	No technical details and experiments.
2.	The literature review of Alzheimer’s Disease diagnosis is not complete, especially regarding AI-based approaches.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a decentralized expert system designed to predict early-stage Alzheimer's Disease using AI-driven MRI analysis. The system leverages blockchain technology and Federated Learning to ensure data privacy and security while performing anomaly detection on patient-submitted data. The architecture includes a Web3 application for patients to upload biological information and MRI images securely. The decentralized approach aims to improve early detection and intervention for Alzheimer's Disease, providing a more comprehensive representation of AD patterns and enhancing model performance through data diversity.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper encapsulates a few novel ideas. They can be summarized as follows:

1. Handling the security and sensitivity of patient medical information is of paramount importance. The authors were motivated by a very relevant problem and presented an approach to blockchain technology with stated aim of providing robust data privacy and security. By building on decades on research on this topic, this approach has the potential to be extended in future with the general updates in this domain.
2. While there are some confusion around their use case (see weakness below), the authors leveraging Federated Learning and a decentralized system to mitigate the challenges associated with model training on centralized data repositories, such as data bottlenecks and privacy concerns
3. The system aims to provide early-stage prediction of Alzheimer's Disease, which is crucial for timely intervention and improved patient outcomes.

Weaknesses:
However given the commendable motivations there are several challenges with the current paper,

1. First and perhaps the most important aspect is that the paper fails to present the real-world challenges associated with the adoption of such decentralized approaches, especially as it pertains to patients engaging with blockchain wallets and data submission interfaces. Also, the primary use-case for the decentralized approach is not evident - is model training the prime use-case or is the main use case patients being able to generate inferences on their own medical records. Overall, the usage scenario around the setup needs to be better motivated and established
2. The paper also lacks formalism around the presentation. For example, if the primary contribution is the architecture around the decentralized AI approach, the design principles needs to be better justified and articulated. A system architecture diagrams needs to be established as well. Similarly, the ""proof"" around the decentralized approach is not a rigorous mathematical proof. Rather the logic is derived from a hypothesis that more diverse data should lead to a better model. This is a hypothesis at the best and needs to be experimentally validates
3. Finally, the paper is lacking in experimental validation. For example, the proof needs to be backed by real world experiments. Also, this is not the first paper to posit a federated learning approach to medical AI prediction. Some of the SOTA methods in this space needs to be compared against

Limitations:
Please see the weakness above

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces an innovative decentralized expert system designed for early prediction of Alzheimer's Disease (AD), leveraging blockchain technology and Federated Learning. Traditional diagnostic methods often result in delays and imprecision, particularly in early-stage AD detection, while centralized data repositories face challenges in managing vast volumes of MRI data and maintaining patient privacy. The proposed system addresses these issues by combining blockchain for secure, decentralized data management and Federated Learning for collaborative AI model training across multiple institutions. The system includes robust anomaly detection mechanisms to ensure data quality and integrity, enabling precise early-stage AD predictions. This comprehensive approach aims to revolutionize disease diagnostics by enhancing data privacy, security, and collaborative efforts in the medical community.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents a novel integration of blockchain technology and Federated Learning for early AD prediction, which is innovative in addressing data privacy, security, and collaborative AI model training.
- The proposed system is well-conceived, with a detailed architecture and implementation strategy. The inclusion of anomaly detection mechanisms to ensure data quality adds robustness to the system.
- The approach has significant potential to improve early-stage AD detection, which is crucial for timely intervention and better patient outcomes. The decentralized nature of the system promotes data privacy and security, addressing major concerns in medical data management.

Weaknesses:
- The integration of blockchain and Federated Learning introduces significant computational complexity and potential delays due to off-chain processing and communication overhead.
- The system's scalability is a concern as the volume of data and the number of users increase, necessitating ongoing optimization to ensure efficient performance.

Limitations:
The authors have addressed several limitations, including data quality and consistency, computational complexity, and model generalizability. However, further discussion may be needed on:
- Ensuring that the AI model is unbiased and fair across different demographic groups can be difficult, especially if the training data is not representative.
- Real-time processing and predictions might be challenging due to the decentralized nature and the need for off-chain processing.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
dhFHO90INk;"REVIEW 
Summary:
The paper develops a simple but novel deep learning framework to implicitly optimize a seed input design to iteratively improve upon a particular property $$ g(\cdot) $$, until it reaches a fixed-point. In this way, the paper develops a novel framework for design optimization and provides a theoretical derivation to showcase that this implicit setup leads to an optimal learned neural network function $$ f^*(\cdot) $$ whose iterative refinement of a seed design , approximates the direction of gradient of the property function of interest. The framework has been evaluated on three diverse design tasks and compared with several state-of-the-art design methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed method is well motivated and novel for the problem of design optimization with neural networks without adversarial classification (which are inherently hard to train).


- The various experiments on toy as well as realistic design tasks like airfoil design optimization, antibody binding affinity design optimization demonstrate that the proposed method based on a simple `pair-wise matching` criterion is able to achieve better results than state-of-the-art models previously used for the same design task.


- The insight and the proof demonstrating that the constructed optimal function $f^*(\cdot)$ approximates the direction of the gradient of a property function $g(\cdot) $ even though it is not explicitly trained to do so, is interesting and will benefit the space of design optimization.

Weaknesses:
- Further analysis about the effect of $ \Delta_x $ and $\Delta_y  $, their selection methodology and their sensitivity in various design contexts is necessary as the matching step critically depends on these two parameters.


- The method has been demonstrated only in the context of designs that improve a single property $ g(\cdot) $. However in reality designs usually need to consider multiple properties simultaneously. This brings into question the applicability of the current method to practical problems. 


- A rigorous analysis of the ""goodness"" of the seed design and the effect of said goodness on the convergence speed / accuracy is necessary.

Limitations:
A limitation of the current work is its inability to incorporate more than a single property of interest. However, the authors have explicitly mentioned this limitation and the reviewer believes that despite this, the proposed method is still valuable and can serve as a good foundation for future research in design optimization.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper address design optimization, which is the process of optimizing over a ""design"" parameter space to optimize over one or more observable outcomes in many scientific and engineering problems. The proposed framework ""PropEn"" uses a three step process, first identifying a ""matched dataset"" that pairs every sample with another with a better outcome; this is followed by training a model that takes a sample and predicts, essentially creating an autoregressive sampler. The technique is evaluated on multiple science and engineering benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The matched objective is an interesting application of regressive sampling, and a different view on augmenting a small dataset, which is a common issue in many science problems. 
* The paper goes into theoretical justifications as to why a matched reconstruction objective is useful, suggesting that it approximates the gradient of an explicitly trained property predictor. 
* Its also shown that the generated samples have a high likelihood close to the training samples, indicating that they are indeed sampling from the training distribution.

Weaknesses:
* PropEn is an interesting idea for design optimization but it needs to be more thoroughly benchmarked against existing methods. For example, there is a host of methods that do Bayesian optimization, and variants thereof, that can work with as few as 10 samples (albeit in lower dimensions) which the paper does not consider. I think it sheds light into the behavior of PropEn in lower dimensions. More recent methods that use DNN based UQ estimates are able to go beyond 5-10D datasets effectively as well. 
* At a high level, matching and matched reconstruction appear to be very closely related to diffusion models. In my understanding, the diffusion process can be framed as matched reconstruction, where the property to maximize is the likelihood of the sample. Framing matched reconstruction through the lens of diffusion might not only strengthen the paper's formulation but also provide other theoretical insights. 
* Its unclear to me why neighborhoods in the design space are constructed using L2-balls, why is this the best way to identify similarity in x? How sensitive is the optimization to the size of the neighborhood? Or in another sense, a related question is -- how sensitive is matched reconstruction to the choice of sampling? for e.g. if you start with an LHC sampling (or any other variants) vs other types. 
* On small amounts of data, what kinds of regularizations keep the main model from overfitting?

Limitations:
Yes, it appears so.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes the method PropEn, which is inspired by the concept of matching techniques in econometrics. Using PropEn (specifically in scenarios with a lack of large datasets), the authors can expand the dataset, which will inherently help in design improvement, etc. To do this, they train a network to learn a mapping from an initial sample to another sample with an improved target attribute selected during the dataset curation phase. This model can further be sampled auto-regressively until it converges to a final candidate design.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is very well written, and the motivation, methods, and results are clearly presented and easy to understand. 
- The proposed method is simple, generalizable, and appears effective. 
- The experiments and corresponding results are well-defined and support the claims made in the paper.

Weaknesses:
- Additional experiments that would add substantial value to the work would be to extend the data matching method to other models, rather than a simple encoder-decoder model with a reconstruction loss. As an example, provide experiments for diffusion models where the conditioning signal is the x_0 seed (or x_i in autoregressive sampling), and the generated design is x_{i+1}, which would solidify your claims about the generalizability of the data matching method.
- Have the authors considered active learning approaches? This work is very close to active learning ideas, applied in the context of low data regimes.
- The aspect of evaluating the target properties on the neighboring samples is not clearly explained. 
- the examples shown are simple and not really huge.

Limitations:
No limitations are mentioned by the authors.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a generative framework for property enhancement. The proposed framework consists of only a generative model, and it's missing a discriminator that is usually found in other frameworks for guided design. This is achieved by training the generative model on a ""matched"" dataset that consists of paired examples $(x, x')$ where $x'$ is an ""enhanced"" version of $x$ from its immediate proximity (""enhanced"" meaning it has a higher value of the property that is sought to be maximized in the guided design process, and ""proximity"" depends on the problem domain).

A mathematical analysis of the proposed method shows that the learnt generative function converges to the gradient of the function computing the property and that examples sampled by the function are as likely as the training set. These analysis motivate the iterative application of the generative function to converge to a stationary point with enhanced property.

The authors claim that this framework is particularly well suited for applications with scarce data, and apply it to a toy problem and airfoil optimization, as well as protein optimization. In the latter application the method shows superior performance compared to other methods, as measured by wetlab experiments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper presents a simple method with mathematical justifications why it should work.

1. Additionally to the toy examples, the method is applied to airfoil enhancement and protein optimization. The predictions of the latter are confirmed by wetlab experiments, and the reported results are better than state-of-the art methods.

Weaknesses:
1. The paper is missing several important implementation details. I would be especially interested in knowing the details of the training datasets and the constructed matched datasets, which I could not find anywhere in the paper or the appendix. Section B.1 in the appendix should be expanded to explain in detail what models were used. For example, line 571 states that a ""ResNet with 3 blocks"" was used to generate one-hot encoded AhO aligned sequences. But it's not clear to my why a 2D convolutional architecture would be used for such a problem, how exactly the data was represented, and what motivates the use of that precise model.

1. The airfoil optimization is missing a strong baseline. From the presented results I find it hard to judge whether the method indeed works well across domains. I'm not familiar with the protein optimization experiments, so I cannot judge the power of the method by the presented results, especially since important implementation details are missing (see above weakness). Since all the findings in Section 3.2 are based on very few datapoints (per optimization target) and there is a wide intra-method variability, I would like to see some statistical analysis for presented data.

Limitations:
The ""NeurIPS Paper Checklist"" in the appendix still needs to be filled in (currently it only shows the instructions).

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
eYUbBKxABP;"REVIEW 
Summary:
The paper presents a formalization of fairness metrics intended to ease analysis of discrimination by automated decision making systems in the UK. While there is a relatively applied angle, the bulk of the contribution is intended to be a generic and re-targetable mathematical formalism.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This paper shows significant strength in its understanding of nuance with the way law works–something that is sorely missing from the vast majority of CS papers that attempt to handle legal concepts. I was very pleased overall by the mapping the authors performed between relevant legal concepts in the UK and their formal model of fairness. The bulk of the contribution here is in the modelling–which while it results in a simple formulation, should not be taken to undercut the value of the contribution.

Non-US legal contexts often get left out of the literature, even common law jurisdictions–yet they impact a significant number of people, and this work takes formalising fairness across that rubicon.

Weaknesses:
I do not have any major scientific critiques, though there were areas where the clarity of the paper could improve.

Lines 240-274 were written in harder to parse prose than the bulk of the rest of the paper. I had to reread that area multiple times.

The case study in Appendix A was actually very useful for understanding the authors' formalism and it is a shape that some of that context was not woven into the paper as concrete examples of how to understand the math.

The discussion on proxy discrimination never seemed to finish? I wasn't able to understand its meaning under UK law.


Missing a ref to Homer on L299.

All these are very minor issues. I'm substantially in favour of accepting this paper.

Limitations:
Ultimately, adherence to a formalism is *not* what courts generally take into account. While statistical analyses may be used to advance a given line of argument, the standards used are open-textured–and this is an inherent limitation of this line of work.
It also would have been good to see where this formalism sits under EU law (or representative EU-member law) or perhaps a discussion of how civil law jurisdictions handle these sorts of issues.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper maps existing literature and law on algorithmic fairness onto a decision-theoretic framework. It describes various desiderata (e.g. statistical parity) and legal restrictions (e.g., legitimate aims) in terms of expectations, distributions, estimation error, etc.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is well-written and survey a large literature. It appears to state legal tests (particularly under U.K.) with care, while being careful not to overclaim about what its definitions actually establish.

Weaknesses:
n/a

Limitations:
n/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
- There is a gap between the definitions of fairness studied in the computer science literature, and the definitions of fairness operationalized by courts adjudicating discrimination claims. This limits the usefulness of the CS definitions.
- Amongst work attempting to reconcile legal and computational definitions of fairness, little has focused on anti-discrimination law outside the US.
- This paper makes four contributions in this context:
    - (1) It formalizes elements of anti-discrimination law into a decision-theoretic formalism
    - (2) If analyzes the legal role of the data-generation process
    - (3) It proposes conditional estimation parity as a legally-informed target
    - (4) It provides recommendations on creating SML models that minimize the risk of unlawful discrimination in automated decision-making

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- The paper’s focus is interesting–the fairness literature is biased towards the US, and I imagine most fairness researchers would be unaware of subtle differences between UK and US anti-discrimination law.
- Because UK law is influential around the world, understanding how it regulates fairness in algorithmic systems has global importance.

Weaknesses:
- Much of the paper reads like a review of anti-discrimination law. This makes it difficult to parse out (1) what the technical contributions are, (2) why they’re novel, and (3) why they matter. 
- It’s extremely unclear what the technical payoff of the paper’s modeling choices are. The fairness field is overwhelmed with different definitions/frameworks. Why is the one proposed by the author’s meaningful over others? 
- It seems like an essential point to the paper’s argument is that prior work hasn’t studied UK anti-discrimination law. But if the paper wants to successfully extend that into an argument about modeling choices, I think it needs to explain why the existing definitions of fairness do not work for UK law.
- The recommendations provided are extremely general. Are these new or different from the many recommendations that already exist in the fairness/responsible AI literature?

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the issues around existing fairness metrics and bias detection/mitigation methods not corresponding with legal notions of fairness, specifically under UK anti-discrimination law. The authors propose a theoretical framework for a data-generating process that aims to formalise the legitimacy of decisions and features in the data. Further, they propose a new metric ""conditional estimation parity"" which compares estimation errors for different protected groups.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well written and coherent. It translates potentially inaccessible legal scholarship and discussions clearly for a technical audience.
2. There is interesting discussion and the paper combines existing literature well. Although these discussions are not particularly novel, UK Equality Law in particular is rarely discussed and the investigations done here are useful to extend the literature for this niche.
3. The work addresses some big limitations in existing literature such as existing fairness metrics not aligning with legal notions of discrimination, particularly under non-US regulations, not considering context of what features are legitimate for an application or considering the estimation errors of decisions.

Weaknesses:
1. A lot of the paper is background or a collation of existing literature. The main contribution is the new conditional estimation metric metric but this metric relies on the true DGP and evaluating the estimation error which, as stated, can be complex in practice. This could make it difficult to use the metric in practice.
2. I understand it would be hard to use the metric for evaluating discrimination in existing datasets for the reasons specified above and also due to the inherent context-dependency of the metric (which is a benefit) but it could be useful to include some experimentation or results in a hypothetical scenario to show how it might be used in practice. As there are no results as such to comment on, it is difficult to assess it's significance.
3. The conclusions drawn such as ""Assess data legitimacy"" or ""Build an accurate model"", although justified with evidence in the paper, are not novel and are pretty standard, common-sense recommendations. 
4. Overall, the main novel contribution is the new metric but this is a small part of the paper. The rest of the paper is a nice collation and narrative of existing literature but I am not sure it significantly advances the field.

Other comments:
1. I can't see where SML terminology is introduced - I assume this means supervised machine learning?
2. In Section 1.4, DGPs are mentioned for the first time. It would be useful to have some more background to them before this - what exactly is a DGP? I do not believe it is ever explained.

Limitations:
The authors are honest about the strengths and weaknesses of their work (although some are hidden away and not pointed towards in the checklist). It would be useful to improve the discussion of limitations in Section 1.4 as it only mentions the limitation of applicability only in the UK.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides a UK-and-European-law-based view of anti-discrimination law as it relates to fair machine learning and automated decision systems. It does a good job laying out the doctrine, arguing correctly that work in this area to-date is very centered on US legal concepts such as disparate treatment vs. disparate impact. Although I am willing to believe that there are subtle differences that drive important aspects of fair ML analysis, as the paper claims, I think the specifics of these differences could be made much clearer and need to be for the paper to have the impact it should.

Of particular note, the paper is very well situated in the surrounding literature. Although this contextualization should make the contributions more clearly offset from prior work, as presented I find the opposite: it is difficult to tell what is new as a contribution here. For example, while the contributions are clearly identified in 1.4, I think it would aid the paper if they appeared higher in the intro and were clearer about what is new and why it matters. The example in Appendix A could be used as a running example to show where new concepts are needed and what about existing work does not capture this different legal regime. In particular, after claiming that disparate treatment/disparate impact are distinct to direct & indirect discrimination, the definitions given from 105-114 seem to align tightly to the former. And while I'm not a lawyer, I don't believe that disparate impact claims require a showing of intent under US law either, so I found that distinction somewhat confusing.

On the technical level, the discussion of the true data generating process should really be contextualized in the literature on measurement and construct validity, specifically with respect to work by Jacobs & Wallach, which in particular encompasses the material in 2.3 on estimation parity (at least in part). Also, the causal analysis components of the discussion of data generation could cite more of the work of Kohler-Hausmann and also Hu (one paper from these authors is cited, but others are also relevant and speak more directly to causality and counterfactual fairness claims).

As a final observation, although the ML community talks in terms of ""fair"" outcomes, it is often conceptually clearer (and more in line with legal analysis) to use the same techniques as tools for identifying ""unfair"" activities or outcomes. Phrasing some of the claims this way may condense some arguments and tighten the presentation overall. Related to this, the discussion of these tools as part of an overall practical strategy for risk management is important and should receive more attention. For example, it would be good to discuss how the measures proposed would be used in real legal analysis of an example, such as in litigation or a regulatory proceeding.

I was also a bit confused about the analysis of constructed proxies for protected variables in 2.7. I understand that it's necessary to look beyond a formalistic view of whether a specific attribute is considered, but what happens if the proxy for a protected attribute is (say) the sum of two legitimate attributes? Why is it good enough to use only legitimate features? Also, at 393-394 it might be valuable to look at the recent paper on ""Less Discriminatory Algorithms"" and compare the approaches and outlooks.

Incredibly minor: 
* There is a missing period at 81.
* At 284-288, there is a latent call to questions of ecological validity which could be made more explicit

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* Generalizing beyond the US legal context is important and valuable and this paper does a good job explaining the UK and related legal systems' approach to anti-discrimination law.
* The paper is well written and well situated in existing literature

Weaknesses:
* Novelty is at times hard to identify. I think it's there, but the claims on what it covers should be clearer. In particular, the discussion of the decision-theoretic framing seems a bit under-attended even though it's potentially very useful.
* Some important concepts are missed, notably theories of measurement and construct validity/reliability are at least partially re-invented when they should just be treated as background.

Limitations:
I believe the limitations are expressed well.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
fzdFPqkAHD;"REVIEW 
Summary:
The paper presents ATS (Agent-To-Sim), a framework to enable agent behavior modeling from multiple casual video captures in indoor scenarios captured during long spans of time. The proposed pipeline consists in (1) 4D reconstruction of the scene geometry and observer and agent motion, and (2) controllable agent behavior learning and generation.

For the first stage, multi-video registration uses coarse-to-fine registration to globally align the cameras to a shared canonical space derived from DINOv2 per-frame features (initialized with a walkthrough clip of the environment) and then jointly optimizes the 3D structures while adjusting the cameras locally with novel featuremetric losses (which makes the optimization robust to changes of lighting and appearance and improves alignment accuracy) and standard photometric and regularization losses. With the proposed (annealed) swapping of latent per-video codes during optimization, missing information is shared across videos, while video-specific details are kept.

For the controllable agent behavior modeling, in order to generate plausible interactive behaviors, the generated behavior conditions on an encoding of the scene, observer, and past from the agent's egocentric perspective, which avoids overfitting to specific locations in the scene. Then, the ego-perception-conditioned generation of full body motion proceeds hierarchically via diffusion: Generated goals Z condition generated paths P, which finally condition generated body motions G.

The included experiments reflect the quality of the 4D reconstructions achieved by the proposal, the improvements in displacement errors compared to two baselines (as well as ablations of the proposed method), and a qualitative analysis of the effects of the behavior conditioning signals.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Great technical achievement to reconstruct agent behavior in indoor settings, exploiting the shared information across different videos captured at different times via robust alignment based on semantic features from foundational image models (DINOv2) and diffusion-based short-term hierarchical motion generation.
- Plausible long-horizon generation of agent motion for different bodies, conditioned on the environment, observer, and past trajectory.
- Despite the complexity of the system, the description is relatively brief and complete, whig, along with the rest of the paper, is excellently written.

Weaknesses:
- The paper focuses on environment-aware motion of agents in the presence of a (human) observer. Even if out of scope for this paper, it would be interesting to discuss more complex agent-environment interactions (see my questions below).
- I believe the current experiments use a small number of environments/scenes, which makes it hard to justify considering the system for larger-scale deployment, but I'll be happy to update my score if the authors correct me.

Limitations:
The authors reasonable address limitations and social impact in the appendices.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper discusses using an iPhone's RGBD camera to collect several hours of videos within a room over a time span of one month. Through these multi-view videos, a 4D reconstruction of the room is generated. A collection of rigid bodies is used to simulate agents (such as cats, dogs, etc.) in the room. Utilizing goal-conditional path generation technology, users can ultimately control the movement of these agents by setting goals.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The video presented in this paper is very effective; it reconstructs 4D video from a single view and reconstructs a complete room from multiple views.
2. In addition to reconstruction, the paper also discusses how to control the movement of the agent through goal-condition path generation.
3. Intuitively, I think this is a good paper and may inspire researchers in the field of 4D reconstruction.

Weaknesses:
1. While I am not an expert in 4D reconstruction, I find the presentation of this paper rather unclear, particularly the methodology section, which is extremely difficult to understand. My confusion began around lines 126-127. What are the color and feature descriptors of the video? I later noticed that ψ is described as the DINOv2 [40] feature of the input image. So, is ψ a feature of an image? How to obtain it? The paper should clarify this. Additionally, what is X, and is it a point cloud obtained from a mobile phone? If so, how does the point cloud acquire its color in Equation 2?

2. I suggest using a table to explain each symbol in detail. If the explanation of a symbol requires context from the paper, ensure it is as understandable as possible. For technical terms, provide detailed explanations within the paper. A comprehensive symbol table in the appendix would significantly enhance the paper's clarity.

3. The paper lacks detailed quantitative experiments to demonstrate the effectiveness of the method.

Limitations:
The authors adequately addressed the limitations and potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents Agent-to-Sim, an approach to learn a 3D agent in a 3D environment from casual videos of the same agent captured over a long horizon. ATS first conducts 4D spatio-temporal reconstruction from the set of videos, including a deformable agent, the background scene, and a moving observer. This is done with a coarse-to-fine video registration method. Then, given the 4D reconstruction, ATS learns a hierarchical diffusion model over the agent's goal, path, and pose trajectories.  The overall approach is tested on a dataset of iPhone videos for over several types of agents and motion patterns.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- I am not a subject matter expert in this field. However, the paper was clear and well-written such that even a non-expert like myself can understand the proposed high-level approach. The attached supplementary materials give a great visual overview of the paper.
- The paper outlines several limitations of the proposed approach and future directions to address them. The limitations are meaningful and help the reader better understand the problem setting, modelling assumptions, and future directions.
- The paper tackles a challenging problem on the path towards building scalable and realistic simulators.

Weaknesses:
- Certain technical details are not clear for readers unfamiliar with the related literature. This limits understanding and reproducibility. See questions.
- Evaluation of the method seems limited and is mostly limited to qualitative comparisons. I suppose this is inevitable given that ATS tackles a new problem setting than related work. However, it does limit the reader's ability to evaluate the significance of this methodology.
- For behavior generation evaluation, I don't understand why certain baselines were selected. In particular, FaF seems like a detection + multi-agent motion forecasting paper for self-driving, so it's not immediately clear how it can be adapted to this setting.

Limitations:
The authors have adequately addressed limitations and potential social impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a method for learning interactive behaviors of various agents, including humans, cats, dogs and a bunny, by leveraging unstructured videos captured casually. The various videos are registered together in a common frame, offering a 4D reconstruction of the agent and the environment. Based on this reconstruction, the multi-modal distribution describing different agent behaviors is learned by using diffusion models and Control UNets.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper addresses the very challenging problems of learning agent behaviors from a collection of unstructured videos captured over different sessions. To learn interactive behaviors, both the trajectories of the agent and the surrounding environment need to be reconstructed, as to have relevant context of the behavior. Additionally, the motion of the camera/observer need to be reconstructed as well, to allow the registration of the videos in a common frame. As the videos are collected over a potentially large period of time, change in the environment can occur, complicating the tasks of registration and reconstruction.

The idea of using ego-perception encoding for the learning and generation of plausible interactive behaviors is another strong point. After the agent and the environment are reconstructed, ego-perception encoding is learning perception codes of the scene, the observer and past trajectory, factors that condition the generation of the agent's body motion.

Behavior generation considers the generation of the goal and the conditioned generation of the path, taking into account the goal.

Weaknesses:
There are numerous models employed in the proposed framework. Due to the limited space available, few details are provided about their motivation and their implementation. This makes both understanding of the work and its reproducibility very challenging. 

A particular aspect which is not addressed in detail is the modeling of the agents, especially of animals like cats that are quite challenging due to their non-rigid nature. In particular, it is not clear how eq.2 is combined with eq.3, and why the same number of ""bones"" (b=25, L.137) is used for all agents. Also, the nature of G^b is not discussed in detail. 

Additionally, details on how NeRF-type reconstructions are combined with feature descriptors, and how this helps in handling layout changes is not discussed in detail.

More examples like the previous can be given for different aspects covered in the paper, like camera localization (eq.6), scene alignment (eq.7) and behavior learning (eq.10 and 11). Each of these aspects would certainly require more space for describing in detail the corresponding models and support the relative claims in the experimental evaluation. 

Regarding experimental evaluation in particular, only high-level results regarding the agent behavior prediction are provided, while it would be crucial to quantitatively assess the quality of 4D reconstruction and, importantly, to include a detailed ablative study.

Overall, although some very interesting ideas are proposed in this work, both for 4D reconstruction of agent behaviors and behavior learning and generation, I think that the paper is too densely packed without having enough space to describe the paper contributions in sufficient detail. In my view, even describing in detail one of the 4D reconstruction or agent behavior modeling parts alone would be challenging in the space available. This affects also the experimental evaluation, as not all claims are supported by the results.

### Minor comments
- L.35: ""Such systems do not scale well""
- Figure 1, caption: incomplete sentence ""conditioned different observer trajectories""
- L.88: ""whiling accounts""
- L.113: what ""longitudinal videos"" are?
- Figure 3, caption: what does ""low latency"" means in this context?
- L.215: ""we collect the a""

Limitations:
Limitations of the work are discussed in the Appendix.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
DztaBt4wP5;"REVIEW 
Summary:
This paper firstly identifies a condition discrepancy in diffusion models that generation results conditioned on the training text prompts can significantly differ membership datasets and hold-out datasets of the model. Based on this observation, the paper proposes a novel method for membership inference that exploits the difference of losses conditioned on groundtruth text prompts and null (or partially null) text prompts as the feature. This method yields satisfying result.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
a) The paper investigates the membership inference of diffusion models, which is a meaningful topic with positive societal impacts.

b) The paper formally reveals an important phenomenon that diffusion models over-fit the condition, which is a finding useful for both membership inference and other scenarios (in case it is real).

Weaknesses:
a) The primary weakness of this paper is its fundamental setting, that we cannot assume that we have accesses to the groundtruth conditions (text prompts) c in real-world membership inference of diffusion models. The whole proposed method is based on this unrealistic assumption so that its meaning to the progress of membership inference would be quite limited. For example, we do not know the prompt used in the training of Stable Diffusion and SDXL, where membership inference is in need for copyright data detection. As baselines, SecMI [1] and PIA [2] discuss how different prompts c (null, groundtruth, blip) influence their performance and show that with BLIP caption their methods still hold to be effective. This robustness is in need for real-world usage of membership inference. However, this work is totally based on the groundtruth prompts so that it seems noLimt to have this necessary robustness.

b) In addition to a), the baseline comparison could be unfair, because one can easily adapt a similar conditional mechanism to SecMI [1] and PIA [2] to enhance their performance, which is not included in their default implementation. Notably, this is an extra information so that it seems certain to improve their performance.

c) Two evaluation setups, both over-training and real-world training, do not meet the real world scenario. For example, the author trains models on MS-COCO (2500 images) for 150,000 (over-training) and 50,000 (real-world training) steps and evaluates the proposed method on them. This means 60 steps/image and 20 steps/image. However, Stable Diffusion is trained only 1 step/image on LAION [3]. Hence, there is a gap between the evaluation setup of the paper and that of the real world. By contrast, baselines like SecMI [1] and PIA [2] are evaluated on Stable Diffusion & LAION, indicating their effectiveness on real-world membership inference.

d) The finding of condition discrepancy has been revealed by [1] to some extent, for it shows the clear difference when using blip prompts and groundtruth prompts. This makes the novelty of this finding doubtful.

References:

[1] Duan, Jinhao, et al. ""Are diffusion models vulnerable to membership inference attacks?."" International Conference on Machine Learning. PMLR, 2023.

[2] Kong, Fei, et al. ""An efficient membership inference attack for the diffusion model by proximal initialization."" arXiv preprint arXiv:2305.18355 (2023).

[3] https://huggingface.co/runwayml/stable-diffusion-v1-5

[4] Ma, Zhe, et al. ""Could It Be Generated? Towards Practical Analysis of Memorization in Text-To-Image Diffusion Models."" arXiv preprint arXiv:2405.05846 (2024).

_______________________
According to the rebuttal, I have raised my score to 7.

Limitations:
See Weaknesses.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new MIA metric tailored for text-to-image diffusion models. More precisely, they assume that conditional overfitting is more severe that unconditional one. Based on this assumption, a new MIA metric (CLiD) is proposed. The CLiD metric shows superior performance in various text-to-image diffusion models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem (MIA) is a very important problem which requires a lot of thought, especially with the increased usage of diffusion models which are essentially trained on the entire internet. Current MIAs tailored for Diffusion Models (DM) mainly focus on **uncondition** DM. This work bridges the gap between **conditional** and **unconditional** DM MIA. 

2. The idea is straightforward and further validation using gradually truncating operation is quite insightful.

3. The experiment is very comprehensive. The exps on overfitting and real world scenarios reveals that current MIAs rely on severe overfitting and are not as strong as they claim (while CliD is more sensitive to overfitting). The idea to choose the threshold is more reasonable than current method (globally chosen). The exps about distribution consistency reveals the selection of nonmembers will seriously affect the performance of MIA, which gives a more reasonable setting of pretrained DM MIA. These insights above are interesting and very helpful for the MIA community.

Weaknesses:
1. There are some typos. For example, FPR@1%FPR $\rightarrow$ TPR@1%FPR in Tab.3 and Tab.4. 

2. One related work should be included and further discussed: 

Wen, Yuxin, et al. ""Detecting, explaining, and mitigating memorization in diffusion models."" The Twelfth International Conference on Learning Representations. 2024.

Limitations:
The authors have already addressed the limitaions of this paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, the authors propose a novel membership inference attack for text-to-image diffusion models. By examining the discrepancy between the text-conditional predictions and unconditional predictions, the proposed method outperforms the SOTA method by a significant margin. In the end, the authors also show the proposed attack is robust to various defenses.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written.
- The method is simple and the motivation behind it is straightforward.
- The results look very promising. The proposed method outperforms all other methods by a big margin.
- The authors compare the method to various recent methods.
- The authors include various adaptive defenses, and the method seems to be very robust.

Weaknesses:
- The paper claims the setting is grey-box, but I feel like it's just a white-box setting since the attacker needs the model weights to perform the loss calculation, and there's no such API in the real world. I feel like the authors should emphasize it, even though previous works claim such a setting is ""grey-box.""

Limitations:
The authors do include the limitations in the appendix, and I appreciate it.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses potential unauthorized data usage and the privacy concerns in text-to-image diffusion models. The authors introduce a novel membership inference method, Conditional Likelihood Discrepancy (CLiD), which leverages the identified phenomenon of conditional overfitting in these models. They propose two practical membership inference methods, CLiD_th and CLiD_vec, which indicate membership by measuring the KL divergence between conditional distribution of image-text pairs and the distribution of images. The results shows superior performance of their methods compared to existing baselines, particularly in real-world training scenarios with common data augmentation techniques. Also, their method shows robustness to overfitting mitigation strategies like early stopping and adaptive defenses. Their experiments across multiple datasets validate the effectiveness and robustness of CLiD in detecting training data in text-to-image diffusion models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Conditional Likelihood Discrepancy is a novel method for membership inference on text-to-image diffusion models is a significant contribution which significantly outperforms existing methods.

2. For the empirical validation of assumption 3.1, authors provide thorough results using various metrics such as FID, Wasserstein Distance, Kernel MMD, and 1-NN.

3. Experimental results show that CLiD_th and CLiD_vec methods significantly outperform existing baselines in terms of ASR, AUC, and TPR@1%FPR. This includes various training scenarios with data augmentation, highlighting the robustness and effectiveness of their approach.

4. By focusing on a foundation text-to-image diffusion model like SD which is widely used in practice, the paper addresses a timely and relevant problem of data privacy and unauthorized usage in a practical context.

Weaknesses:
1. Since the theoretical results depend on assumption 3.1, I believe that validating this assumption on other dataset domains is important. Would you observe the same phenomenon if you test on domain specific datasets, such as Faces (CelebA, FFHQ, ...).

2. Authors do not present empirical results on the benchmark that previous papers have test on, such as CelebA, Tiny ImageNet, and CIFAR datasets.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
anyZgGLQ6n;"REVIEW 
Summary:
The issue of OOD actions is a well-known concern in offline RL research. The issue of OOD states, however, is relatively unexplored. In this paper, the authors shed light on the significance of OOD state correction and propose a SCAS algorithm to guide the agent back into high-value ID states when encountering an OOD state. To achieve this goal, SCAS first defines a conditional state transition probability $N^*(s'\mid s)$ skewed towards high-valued states and regularizes the policy by aligning the dynamics induced by the policy on a perturbed state $\hat{s}\sim \mathcal{N}(s, \sigma^2I)$ with the value aware state transition probability $N^*(\cdot \mid s)$. Also, theoretical analysis shows that SCAS implicitly guarantees the correction of OOD actions, thereby removing the need for additional regularizers.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors highlighted the significance of OOD state correction in offline RL, a problem other researchers often neglect. Also, multiple experiments were conducted to show the competence of SCAS and its robustness towards the choice of hyperparameters.

Weaknesses:
1. The authors did not cite works from the DICE literature(e.g., Nachum et al., 2019; Lee et al., 2021; Mao et al., 2024). Although they do not explicitly correct OOD states, I believe they are deeply related to SCAS because they aim to align the policy's state distribution $d^\pi$ with the dataset's state distribution, thus preventing the policy from visiting OOD states.

2. The choice of $\exp(\alpha V(s))$ as the empirical normalizer seems extremely arbitrary. Providing some rationale behind this choice is recommended.

3. All of the experimental analysis was performed on MuJoCo environments, which have deterministic transition dynamics. Since SCAS utilizes a deterministic environmental model, it may not perform well in environments with stochastic transition dynamics.

4. NeoRL is a relatively uncommon benchmark compared to D4RL. A brief explanation of NeoRL would be helpful for the readers.

5. Some experimental details are missing from the paper.

    * Do Figures 1a to 1d share the same embedding function? How was this embedding function obtained? Was it by running t-SNE on the set of 200,000 samples(50,000 samples each from the dataset, CQL, TD3+BC, and SCAS)?
    * The points in Figure 1d seem to be the union of the points in Figures 1a, 1b, and 1c. Am I correct?
    * Why does Figure 2 omit Q values of off-policy RL, SDC, and OSR for higher numbers of optimization steps?
    * Section 6.3 states that varying steps of Gaussian noise were added to the actions during test time. Figure 3 indicates that the authors added noise up to 40 steps. However, trajectories are usually longer than 40 steps. How were those 40 steps selected? Did the authors add noise to the first 40 interactions with the environment? 

6. Minor comment: The meaning of the term *$\mathcal{D}(s)$ weights* on Line 162 is unclear.


### References

Lee, Jongmin, et al. ""Optidice: Offline policy optimization via stationary distribution correction estimation."" International Conference on Machine Learning. PMLR, 2021.

Mao, Liyuan, et al. ""Odice: Revealing the mystery of distribution correction estimation via orthogonal-gradient update."" arXiv preprint arXiv:2402.00348 (2024).

Nachum, Ofir, et al. ""Algaedice: Policy gradient from arbitrary experience."" arXiv preprint arXiv:1912.02074 (2019).

Limitations:
The authors have adequately addressed their work's limitations and potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents SCAS (OOD State Correction and Action Supression), a model-based regularization approach that effectively addresses the challenges of out-of-distribution (OOD) states and actions in offline reinforcement learning (RL) algorithms.
The method unfolds in two main stages: (1) training a transition model, and (2) training a policy that incorporates a model-based regularizer. 
This regularizer is designed to steer the policy towards high-value in-distribution (ID) successor states and thereby away from OOD successor states.
While the primary focus of SCAS is to resolve OOD state issues, this paper shows that the regularization strategy not only mitigates visiting OOD states but, as a byproduct, also suppresses OOD actions in the training of the value function.
Empirical results demonstrate that the proposed method performs comparably on standard benchmarks, showing its effectiveness in refining offline RL algorithms.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**1. Theoretical connection between OOD state correction and OOD action suppression**

A notable strength of this paper is the establishment of a theoretical link between OOD state correction and OOD action suppression. The authors articulate how the mechanism designed to correct OOD states implicitly suppresses OOD actions, by showing an optimal policy of the regularized objective will produce actions inside of the support of behavior policy for every dataset state. This implicit suppression also explains successful empirical results on offline RL benchmarks.

**2. Robustness to Hyperparameter Selection**

As detailed in Section 6.2 and illustrated in Figure 4, the proposed approach demonstrates considerable robustness to variations in hyperparameter settings. 
This attribute is particularly valuable in practical applications of offline RL where optimal hyperparameter settings can be elusive or computationally expensive to determine.

**3. Evaluation on standard benchmarks and implementation code is attached for reproduction.**

The paper's evaluation methodology is another major strength. The authors have rigorously tested their approach on standard benchmarks, D4RL and NeoRL, providing a comprehensive assessment of its performance relative to existing methods. Moreover, the inclusion of implementation code enhances the paper’s contribution by facilitating reproducibility and further experimentation.

Weaknesses:
**1. Unclear Motivation for Value-Aware OOD State Correction**

This paper proposes shifting the OOD state distribution not to a standard ID state distribution, but to a high-value ID state distribution instead. This choice raises questions about the specific objectives of value-aware state correction. Is it strategically targeting high-value data points, or is it intended to mitigate the effects of distribution shifts? It seems that the manuscript primarily focuses on the former, in that case, it would benefit from a comparison with existing works like DW[1], which also focus on high-value data points for behavior regularization. If the goal is indeed to mitigate the effects of distribution shifts, then it is crucial for the manuscript to better articulate and emphasize how value-aware OOD state correction contributes to the robustness against OOD state visitation. Moreover, a clearer explanation of how this approach aligns with offline RL principles like pessimism and robustness to OOD states would greatly strengthen the manuscript's validity and impact.

**2. Lack of Comparisons with Comparable Offline RL Approaches**

While the paper tackles a fundamental problem in offline RL, the evaluations presented are somewhat constrained by a lack of comparison with key recent advancements in the field. Notable works such as DW[1], EDAC[2], RORL[3], and SQL/EQL[4], which explore similar offline RL settings, are not considered in the main experiments. This gap is particularly significant as DW[1] employs a value-aware strategy that closely mirrors the approach of this paper, focusing on constraining policies to high-value data points. To convincingly justify the need for correcting OOD states—or for employing a value-aware correction approach—a comparative analysis with these influential studies is essential. This would not only place the findings in a broader context but also potentially highlight the contributions and advantages of the proposed method.

[1] Hong et al., “Beyond Uniform Sampling: Offline Reinforcement Learning with Imbalanced Datasets.”, NeurIPS 2023.

[2] An et al., ""Uncertainty-Based Offline Reinforcement Learning with Diversified Q-Ensemble."", NeurIPS 2021.

[3] Yang et al., “RORL: Robust Offline Reinforcement Learning via Conservative Smoothing.”, NeurIPS 2022.

[4] Xu et al., “Offline RL with No OOD Actions: In-Sample Learning via Implicit Value Regularization.”, ICLR 2023.

Limitations:
The authors included their limitations in Section 7 and limitations are addressed appropriately.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on OOD state issue, an important but overlooked issue in offline RL. This paper proposes aligning the OOD state towards In-Distribution (ID) states with high value, named as value-aware OOD state correction. Additionally, the paper discovers that the overestimation of OOD actions can also be mitigated during the implementation of OOD state correction. The experimental results demonstrate that the proposed algorithm is superior in performance and robustness.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper presents an intuitive solution to the OOD state iusse, while also mitigating the overestimation of OOD actions. 

2. The algorithm is highly robust, requiring only a single set of hyperparameters across different environments. This robustness is particularly important in the context of offline reinforcement learning, as online interaction for tuning parameters is costly or even dangerous.

Weaknesses:
1. The paper proposes that during the process of OOD state correction, the overestimation of OOD actions will also be addressed. I have some doubts about this point. The constraint $R_2$ seems to only regularize the policy to output actions at OOD states that result in the next state being within the offline dataset. However, there is no regularization on the policy's output actions at ID states. Therefore, I am confused about how the paper solves the traditional issue with OOD actions. If I have misunderstood anywhere, please point it out.

Limitations:
Please see weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a regularization term in offline RL, which can simultaneously address OOD state correction and OOD action suppression without pretraining a state transition model $N(s'|s)$. It shows good performance in D4RL benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I found it an interesting topic to consider the OOD state issue. Most existing offline RL methods focus on addressing the OOD action issue, while this paper highlights that OOD states may also cause training collapse. Another good property of the method is that it only needs one single hyperparameter configuration to achieve good empirical performance. The theory is also solid.

Weaknesses:
The method utilizes a deterministic policy, which is often regarded as lacking expressiveness. Thus, the performance is not as good as diffusion-based policy methods.

Limitations:
I believe the theory of the paper is solid, while the experimental performance is my concern. I am happy to increase my score if the antmaze version mismatch issue is resolved.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
mOK4yD8JFd;"REVIEW 
Summary:
This paper proposes a method to fuse a pair of short-exposure (noisy) and long-exposure (blurry) captures to produce clean and clear polarized snapshots. The proposed method consists of three phases to reconstruct the irradiance, texture, and polarization.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is well-written, the proposed method is well-described, and experiments using both synthetic and real-world data are conducted to evaluate the effectiveness of the proposed method.

Weaknesses:
It is difficult to fully understand and evaluate the real-world experiments. Please refer to the corresponding questions for further details.

Limitations:
Limitations of this paper are well discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a polarimetric imaging framework that can produce clean and clear polarized snapshots by complementarily fusing a degraded pair of noisy and blurry ones. It adopts a neural network-based three-phase fusing scheme with specially designed modules tailored to each phase, which can not only improve the image quality but also preserve the polarization properties.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The idea of using complementarily fusing to achieve quality-improved and property-preserved polarimetric imaging is novel and reasonable. As obtaining high-quality polarized images is significant in polarization-based vision applications, while previous methods based on single modality (either noisy or blurry) tend to suffer from various artifacts, the proposed method could be a practical way to increase the performance of polarization-based vision applications.

The network module designs are also reasonable. All modules (Irradiance restoration, Polarization reconstruction, and Artifact suppression) are carefully and specially designed to solve the problems in the fusing process, which means the authors do spend efforts in observing and analyzing the properties in both the noisy and blurry polarized snapshots. 

The idea is clearly presented, and the experiments are sufficient. The performance improvement shows that the proposed method is effective.

In addition to the experiments on synthetic and real data, the authors also show the results of reflection removal, which makes the paper convincing.

Weaknesses:
The authors say that they adopt the PLIE dataset [32] as the source data to generate their own dataset. However, they do not tell the reasons why to choose the PILE dataset [32]. For example, [25] also provides a dataset (LLCP dataset) similar to the PLIE dataset [32], so why not choose the LLCP dataset as the source data? Any reasons?

Limitations:
The authors have adequately addressed the limitations.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the first method for polarimetric image enhancement by fusing noisy and blurry pairs. While a short exposure polarimetric image produces sharp but noisy DoP and AoP, a long exposure makes them smooth but blurred. To effectively exploit the complementary advantages of these two images and satisfy the physics constraints of the polarimetric image, this paper proposes a three-phase fusing scheme. Experimental results show that the proposed method outperforms existing polarimetric image enhancement methods.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
+ The first method for polarimetric image enhancement by fusing noisy and blurry pairs.
+ Propose a novel fusing scheme to effectively use complementary polarimetric information of noisy and blurry pairs and retain polarimetric cues by directly processing DoP and AoP.
+ Experimentally validate the effectiveness of the fusion of noisy and blurry polarimetric image pairs and the proposed network. The accurate restoration of polarimetric cues is critical for downstream tasks.

Weaknesses:
- While the proposed method improves the pSNR of DoP and AoP, their SSIMs are almost the same as PLIE [32].
- Requiring two shots is undesirable for some downstream tasks.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
tRjgapiCpm;"REVIEW 
Summary:
The paper presents a heuristic approach for evaluating the privacy of DP-SGD when only the last model iteration is released. This method contrasts with traditional analyses that consider all intermediate updates, offering a more practical assessment for scenarios where adversaries only access the final model. The proposed heuristic is experimentally shown to provide reliable privacy leakage estimates, making it a valuable tool for pre-audit privacy assessments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Focus on a good and important question.
2. Good explanation and clear paper layout.
3. Propose a new analysis neither from the theoretical nor empirical point of view.

Weaknesses:
I think the proposed method is interesting and new but I still have some questions.

1. I know the linear loss function assumption is common in theoretical analysis but it seems that the proposed method wants to have contributions in the empirical case, so why still make the linear assumption? 

2. While I appreciate the effort to introduce a Heuristic analysis, I remain skeptical about its necessity and effectiveness. The primary benefit of theoretical analysis is its precision and rigor, which often include the flexibility to adjust bounds as needed. If the goal is to find a more relaxed lower bound on privacy risks, this can often be achieved by simply loosening the constraints within the existing theoretical framework. Introducing a separate heuristic analysis seems to complicate matters without providing clear advantages. 

3. I do not think you are using a correct baseline. When you make that only the last iteration model can be seen assumption, it is not fair to use normal DP-SGD analysis. I think it is better to use the theoretical analysis from those hidden state papers you cited. I am curious if you compare your proposed method with those methods, will you still get the same conclusion? 

4. I find Table 1 in the paper somewhat unclear and would appreciate further explanation from the authors regarding its purpose and implications. The table suggests that similar levels of heuristic ε are achieved across varying batch sizes, yet there is a noticeable increase in the standard privacy budget for smaller batches to maintain comparable performance. This observation seems to underscore the well-known impact of batch size rather than demonstrating an advantage of the proposed heuristic method.
Could the authors elaborate on how this data relates to the efficacy of the heuristic analysis?

Limitations:
Please check the weaknesses.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a heuristic privacy analysis of releasing only the final model iterate of differentailly private gradient descent (DP-SGD). The analysis is based off of the worst-case differential privacy guarantee of DP-SGD with linear losses, under the assumption that the heuristic can be applied to more general loss functions in order to approximate the privacy loss.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
* The premise of the paper (a heuristic privacy analysis of releasing only the final model iterate of DP-SGD) is very interesting, and Theorem 1 is a cool result.
* The paper thoroughly assesses the limitations of the heuristic (in Section 4).

Weaknesses:
* I don’t know how useful the heuristic analysis would be in practice — beyond a lightweight sanity check — since ultimately it is just a heuristic and not a rigorous upper or lower bound on the privacy loss.

* The empirical study of the heuristic looks to be very thorough, but sparse on interpretation. I would have appreciated more discussion on the figures, and didn’t really feel like there was a strong take-home message from the paper.

* Algorithm 1 is DP-SGD with a regularizer, but in practice it is somewhat rare to use explicit regularization with DP-SGD. So I’m not sure that the heuristic would be widely applicable to the more common implementation of DP-SGD without regularization.

Limitations:
The authors have adequately addressed the limitations of their work.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a heuristic privacy analysis for DP-SGD that focuses on releasing only the last iterate, as opposed to all intermediate iterates. The authors argue that this approach is more realistic and provides sharper privacy guarantees in practical scenarios. The heuristic is based on a linear structure assumption for the model and is validated experimentally through attacks/privacy auditing.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper is well-written. The paper introduces a new heuristic analysis of DP-SGD for linear loss functions and also critically examines its limitations, and identifies areas for further research.

Weaknesses:
To my understanding, this paper offers a tighter privacy accounting analysis specifically for linear loss functions. However, I find its applicability limited since it cannot be extended to general ML tasks where the loss functions are not linear. Additionally, the fact that the privacy adversary has access to all intermediate iterates of the training process makes DP-SGD overly conservative is quite well-known. The main challenge remains in developing tight privacy accounting analyses for iterative algorithms like SGD.

Limitations:
The limitations are adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors provide exact DP guarantees for cases where only the last iterate of DP-SGD is shared with the malicious clients, and linear models with linear loss functions are used. They propose their DP bound to be used as a heuristic that approximates the true DP guarantees for cases where more complex models are used. They show that for normal DP-SGD training, the predictions of their heuristic fall between the standard DP bound computed under the assumption that all intermediate iterations of DP-SGD are shared with the attacker, which is a strict upper bound of the true DP guarantee when only the last iterate is shared and DP-SGD with full batches and only last iterate sharing. They also compare their method against SoTA DP attacks and show that under most circumstances, their heuristic value for the DP is higher. They suggest that this is the result of the attacks not being good enough at precisely estimating the true DP guarantees. Finally, the authors demonstrate that their heuristic under unrealistic circumstances can underestimate the true DP guarantee but argue this only happens under hand-crafted losses and gradient updates, which do not happen in practical circumstances.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The last iterate setting is important
- The linear function DP bound is exact 
- The linear function DP bound has interesting properties 
- The counter-examples for the DP heuristic themselves seem interesting and probably can be adapted to other settings

Weaknesses:
- I am confused by L234. The authors propose to maximize their heuristic over all $t\leq T$, while beforehand (e.g. in Figure 1/Section 2) they advocated to computing the heuristic for a single $T$. Which one is the exact proposed heuristic by the paper?
- In Figure 2, I am not sure how we adapt existing techniques to the last-iterate-only setting? Can the authors explain in more details?
- Can the authors explain in Figure 1, what network and dataset were used?
- The authors do not provide code. I am not sure about the reason, but I will give them the benefit of the doubt that the reason is indeed related to anonymity 

**Nits:**  
- Eq. 8. I assume you do indexing from i = 1. In that case, $A_{T-i}$ should be $A_{T-i+1}$ instead. If you do 0-based indexing, even more fixes to the equation are needed.
- I believe Eq. 7 should be multiplied by $\eta$ on the right-hand side
- Equation at L442, left-hand side should be $m_T$ not $m_t$
- I believe the last equation at L459 should have $(1-q)^{n-k}$ instead of $(1-q)^{k}$. I also believe $n$ is $T$ in this equation
- The definition of $l(m)$ in L109 is confusing as $m$ is considered input to the function, while in the rest of the paper $m$ is used as a parameter. Consider putting $x$ instead.
- Consider defining the hockey-stick divergence in terms of both its pdf and cdf in the appendix to ease unfamiliar readers. I had to read quite a bit on my own to understand it. 
- Consider adding some information in the appendix as to how to deal with the mixed discrete-continuous probability for $P$. I assume many readers will be unfamiliar. 
- Consider deriving the formulas for $P$ and $Q$ in Section 4.2 in the appendix. They are not obvious. 
- Consider having an appendix section that quickly recaps how [NSTPC21] and [NHSBTJCT23] work. Their operation is critical for understanding Section 3. I ended up reading them to get an idea of what was going on there.

Limitations:
The authors acknowledge the limitations of using the heuristic to compute the DP bounds

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
O6YRAOfHGt;"REVIEW 
Summary:
After rebuttal

While I think there are still some problems with this paper, e.g. the short training duration, and the slight exaggeration of claims (that SHED outperforms UED). I think, however, that the idea is nice, and getting RL environment design to work better is a good goal.


-----


This paper aims to improve Unsupervised Environment Design in two ways.
First, it introduces a hierarchical MDP formulation, where the top level corresponds to the teacher, and the lower level corresponds to the learning agent. Each transition in the top-level MDP involves training the lower level agent on generated levels. Related to this, they develop a state representation for the adversary, which is the performance of the agent on a fixed set of diverse levels.

Separately to this, they use a diffusion model to upsample the number of experiences for the teacher, effectively training on synthetic data, to improve sample efficiency.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- I think the H-MDP formulation itself is very valuable; it moves away from treating the generation of environments as a black-box, sparse reward, multi-step generation process (as in PAIRED), and towards a more informed process, where the teacher gets feedback in terms of the state (i.e., the performance vector).
- The analysis in the appendix investigating the ability of the model to generate good synthetic trajectories is useful.

Weaknesses:
- Major
	- The results do not very convincingly demonstrate that SHED is better than current SoTA. Looking at figure 3 particularly, I would say that ACCEL has about the same performance as SHED. However, comparing against the RL baseline, SHED does do better.
	- The method is limited in the types of environments it can generate. For instance, mazes are generated using an LLM instead of directly placing blocks. This method therefore is not quite as broad in scope as PAIRED or ACCEL, which can generate arbitrary environments.
	- Relatedly, in the minigrid experiments, do all methods generate the levels in the same way using an LLM, providing the difficulty numbers? It would be good to compare this against the standard version of ACCEL that directly places blocks in the maze level, as it does not have the same restriction as SHED.
- Minor
	- The figures can be improved:
		- Make the alpha value on the error bars a bit less
		- Keep colours consistent across figures, so the same method has the same colour
		- Keep capitalisation consistent across the figure labels.
	- line 80, the period ends on the line after the maths, it should end on the same line.
	- Footnote 1: Jiang et al. (2021) use (amongst others) the positive value loss, which is not quite the GAE, as it clips it at zero before summing.
	- equation one, you use $\beta_t$ but $t$ does not seem to be defined? Should this be $\beta_k$?
	- Line 159, PARIED should be PAIRED
	- There is no reward scale in figure 4
	- Figure 9's caption can be made clearer. I understand it to be the performance of each method in different testing environments. 
	- Line 718 does not link to a figure.
	- Figure 11's caption: zero-shot and not zeros-shot
	- Capitalise the first word in the title of appendix C.2
	- In terms of notation, in line 96, $\pi^*$ usually has a dependence on $\theta$ (e.g. $\pi^*_\theta$) to indicate it is optimal w.r.t. that particular level.
	- Line 217, maybe add a citation to the first sentence, as I thought that is what you do, which confused me for a second.
	- line 237 space after period.
	- Line 241 ""given"" instead of giving?
	- Lines 296 - 297 are a bit confusing, as the word environment is used three times.
	- The assumption in theorem 1 is pretty strong.

Limitations:
I think the authors can list a few more limitations. 
Primarily, the restriction on the type of environment that can be generated, i.e., it needs numerical parameters, and generating a maze outright is challenging. This is quite a large difference to prior settings.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel approach to Unsupervised Environment Design (UED) that addresses the challenges of efficiency by introducing a hierarchical MDP framework and using synthetic data. This framework involves an upper-level RL teacher agent that generates training environments tailored to a lower-level student agent's capabilities. The paper proposes the Synthetically-enhanced Hierarchical Environment Design (SHED) method, which uses generative modeling to create synthetic trajectory datasets, thereby reducing the resource-intensive interactions between agents and environments. The effectiveness of SHED is demonstrated through empirical experiments across various domains, showing superior performance compared to existing UED methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The use of diffusion models to generate synthetic trajectories is a novel approach that effectively reduces the computational burden of training the teacher agent.
- The paper provides comprehensive experiments across different domains, demonstrating the effectiveness and robustness of the proposed method compared to state-of-the-art UED approaches.

Weaknesses:
- The proposed method introduces significant complexity, particularly in the implementation of the hierarchical MDP and the generative modeling components. This might limit the accessibility and reproducibility of the approach.
- While the empirical results are promising, the evaluation is limited to a few specific domains. It would be beneficial to see broader applicability across more diverse and complex environments.
- Figure 4 is not properly formatted (no values on the axes).

Limitations:
The limitations are discussed in Appendix F.1 but I think the authors should discuss the limitations in the main paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers the Unsupervised Environment Design problem, where a teacher agent seeks to design environments to train a student. Methods such as PLR, PAIRED and ACCEL have recently shown promising performance for random, RL and evolutionary generators. This paper proposes a handful of modifications, using RL with a different objective vs. PAIRED (performance on held out set vs. regret) and also proposes to add synthetic data to accelerate the RL process.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* This is an interesting method in a relevant area of research. UED seems to be one of the most active areas of research with plenty of opportunities for impact.
* The use of evaluation environments is sensible and novel.
* The idea of combining this with Genie is incredibly exciting. It would be interesting to hear how this could be possible or could work. Is there any way to show a simple proof of concept?

Weaknesses:
* There appear to be two confounding features of the method, the new objective for PAIRED and then the synthetic data. Why do they make sense to combine in this way? It just feels like the authors tried to do ""enough for a paper"" rather than contribute something meaningful that people can build on. I say this because its unclear how these two independent features interact with other existing algorithms. Maybe we should just do ACCEL with synthetic data for instance? Did the authors try that? If it is in the Appendix already and I missed it then I will increase my score.
* The performance gains are fairly minor, and presented in an unclear fashion with just a bunch of curves on a single plot. Can we get some more rigorous analysis for example using the recommendations from Agarwal et al, ""Deep Reinforcement Learning at the Edge of the Statistical Precipice""?
* The Maze experiment seems to have many inductive biases and seems distinct from the diffusion based approach for BipedalWalker and LunarLander. What happens if ACCEL has access to ChatGPT as an editor and then uses replay? This seems like a simpler extension that alone could be a strong paper - although it would resemble ELM (Lehman et al 2022) so it wouldn't be particularly novel.
* The related work is very light. This is disappointing since the paper builds on so many related areas, such as synthetic data, diffusion models, UED, language models for evolution, procedural content generation etc.

Limitations:
Covered in the Appendix.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors of this paper use hierarchical MDP formulation and a teacher agent trained by RL to perform curriculum learning. To address the sparse data available for the teacher agent, this paper uses diffusion models to synthesize datasets for training. This paper performs experiments on lunar lander and bipedal walker environments to validate their claim.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Data sparsity is one of the main limitations of using a teacher agent in curricular RL. This paper uses diffusion models to synthesize a dataset for the teacher agent.

Weaknesses:
- This paper designs the teacher agent via hierarchical MDP to model the learning process of the student agent to perform curricular RL. However, Fingerprint Policy Optimization (Paul et al, 2019) also has a similar idea of modeling the learning process of the student agent. It would be interesting to explain more about how this paper's idea is related and contributes to this line of thought.

- A fully trained algorithm on the BipedalWalker should approach a cumulative reward of 300. Even the modified version used in the ACCEL paper is measured on a scale of 0 out of 300. However, from Figure 3, it appears all baselines perform less than 50 on the BipedalWalker benchmark. It is questionable whether all baselines were fully trained with the right settings. Also, the performance of the proposed algorithm and those of the baselines are statistically too similar to see whether SHED improves over the baselines in Lunar Lander and BipedalWalker benchmarks. Finally, other than the version of ACCEL in this paper not performing as well as the ACCEL in the original paper, I am curious whether ACCEL can be considered state-of-the-art in the benchmarks as written in line 324. Genetic Curriculum (Song et al, 2022) reports higher cumulative reward on the BipedalWalkerHardcore environment.

- Figure 4 has no scale on timestep and reward.

Limitations:
The authors has addressed the limitations of this paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
tUHABDZP0Q;"REVIEW 
Summary:
The authors motivate the work by identifying limitations in existing approaches that integrate knowledge distillation into domain adaptation frameworks. Specifically, they note that coarsely aligning outputs over all source and target samples neglects the network capacity gap between teacher and student models, leading to poor distillation efficiency.
The proposed RCD-KD framework uses an adversarial discriminator module to align teacher and student representations between source and target domains. It also formulates target sample selection as a reinforcement learning problem, using a novel reward function based on uncertainty consistency and sample transferability. A dueling Double Deep Q-Network (DDQN) is employed to learn the optimal selection policy.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Well-motivated: The authors clearly articulate the limitations of existing approaches and provide a compelling rationale for their proposed method.
Comprehensive evaluation: The experimental results are extensive, covering four different datasets across various time series tasks. The comparisons with state-of-the-art methods are thorough and demonstrate consistent improvements.

Weaknesses:
Theoretical foundation: While the paper provides a detailed description of the proposed method, it lacks a strong theoretical foundation or analysis. Adding theoretical insights or guarantees would strengthen the contribution.
Computational complexity: The paper does not provide a detailed analysis of the computational complexity of the proposed method compared to existing approaches. Given the focus on resource-constrained devices, this information would be valuable.
Hyperparameter sensitivity: Although some hyperparameter settings are provided, a more comprehensive analysis of the method's sensitivity to different hyperparameters would be beneficial.
Limited discussion on failure cases: While the paper shows impressive results, a more in-depth discussion of scenarios where the method might fail or underperform would provide a more balanced view.

Limitations:
The authors acknowledge some limitations of their work in the conclusion section. They note that pre-training a cumbersome teacher with advanced UDA methods involves more training time than other approaches. Additionally, they mention that using only the distance between teacher's and student's logits to assess sample transferability might overlook intrinsic information from the feature space.
These are valid limitations, and it's commendable that the authors have included them. However, the discussion could be expanded to include potential implications of these limitations and possible strategies to address them in future work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a knowledge distillation method for unsupervised domain adaptation models in time series classification. After pre-training the teacher model with existing domain adaptation methods, the proposed Reinforced Cross-Domain Knowledge Distillation (RCD-KD) method selects suitable target domain samples for knowledge distillation with reinforcement learning and distills knowledge from the pre-trained teacher model to a smaller student model. Empirical experimental results on four public time series datasets demonstrate the effectiveness of the proposed method over other state-of-the-art benchmarks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The writing and presentation of this paper are good. The setup of the proposed problem and the proposed method are described clearly. 

* This paper proposes a new distillation method. It makes some contributions in using reinforcement learning for target sample selection in knowledge distillation.

* Experiments in four public datasets show the effectiveness of the proposed method, which outperforms some domain adaptation and knowledge distillation methods. There are also some ablation studies to validate the designs of the reward and the training losses.

Weaknesses:
* The proposed problem seems to be a simple two-stage combination of domain adaptation and knowledge distillation and may not be realistic enough, which does not show the significance of solving them together in one problem. Besides, authors claim that it can help ‘on edge devices with very limited computational resources’, but the experiments only distill from one bigger CNN to a smaller CNN, which does not make a difference in enabling deployment on edge devices. 

* Authors should explain more on why using reinforcement learning to select samples works better than directly using designed metrics such as uncertainty and transferability. Reinforcement learning will add a lot of computation costs to the training process and it is unclear what the learned selection policy looks like. It seems that the proposed reward cannot solve the claimed issue that ‘in the cross-domain scenario, teacher’s knowledge on each individual target sample may not be always reliable’. Besides, this paper proposes a distillation method for time series data, but the method does not show its special designs for time series. 

* The domain discrimination loss is confusing. How does it enable ‘transfer the domain-invariant knowledge’ to the student model? If the teacher model already learns domain-invariant knowledge, why don’t we achieve this by distilling teacher features to the student model?

Limitations:
Authors discussed the limitations in the conclusion part.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a Reinforced Cross-Domain Knowledge Distillation (RCD-KD) framework for time series data, aiming to effectively transfer knowledge from a cumbersome teacher model to a compact student model across different domains. The RCD-KD framework leverages an adversarial domain discriminator to learn domain-invariant features and a reinforcement learning-based sample selection module to dynamically select informative target samples for knowledge distillation. The proposed method significantly improves the student model's generalization ability on the target domain compared to conventional knowledge distillation and domain adaptation techniques.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. By incorporating an adversarial domain discriminator, the framework can effectively learn domain-invariant features, enabling the student model to generalize better to the target domain.
2. The reinforcement learning module dynamically selects informative target samples based on the student model's capacity and uncertainty, mitigating negative transfer and improving the efficiency of knowledge distillation.
3. Extensive experiments on four public datasets across three tasks demonstrate that the proposed RCD-KD framework consistently outperforms other state-of-the-art domain adaptation and knowledge distillation methods.

Weaknesses:
1. The framework utilizes the distance between teacher and student logits to assess sample transferability, potentially overlooking intrinsic information from the feature space. More comprehensive discussions on different feature distances could further enhance sample selection.
2. While the experiments demonstrate the effectiveness of the proposed RCD-KD framework on several datasets, it is unclear how well the method would scale to larger and more complex time series datasets with higher dimensional feature spaces. The computational efficiency and scalability of the framework under such settings remain to be investigated.
3. The paper acknowledges that grid search was used to tune hyperparameters such as α1, α2, λ, and τ. However, the sensitivity of the framework's performance to these hyperparameters is not thoroughly analyzed. The framework's robustness to different hyperparameter configurations could potentially limit its practical applicability without extensive tuning.

Limitations:
See the weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a reinforcement learning-based active learning method designed to dynamically select target data for knowledge-transfer, whose goal is to bridge the network capacity gap between teacher and student networks within a domain adaptation framework incorporating knowledge distillation. Specifically, the authors propose a novel reward mechanism aimed at learning an optimal policy for selecting target data, considering the student network's capacity. 

Another novel aspect of this paper is its successful demonstration of the framework's effectiveness on time-series data. This modality is less explored in both domain adaptation and knowledge distillation research fields.

##################################### Post Rebuttal #####################################

All of my concerns have been properly addressed, and my questions have been answered during rebuttal. I am happy to raise my score to ***Strong Accept***.

##################################### Post Rebuttal #####################################

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The paper is well-written and easy to follow.

2. The proposed reward function for selecting target data in domain adaptation from a large network to a smaller one is novel. This method integrates principles from active learning, reinforcement learning, knowledge distillation, and domain adaptation.

3. The authors offer a detailed description of their implementation in the paper, along with the accompanying code. This makes it straightforward for practitioners to apply their methods or build upon their work.

4. Formulating network output entropy as a reward function for data selection represents a novel approach that effectively incorporates concepts from reinforcement learning into a classification model.

5. The performance lift of the proposed method is very significant in time-series data, which promotes the interests of active-learning-based domain adaptation.

Weaknesses:
### Majors:

1. Using model output entropy as a measure of uncertainty level is a common technique in active learning. While applying this to promote uncertainty consistency between teacher and student networks is novel, it is important to acknowledge the context of active learning within the paper. It would be not good to borrow ideas without attribution.

2. Based on my experience, running such a reinforced loop for data selection is particularly time-consuming, especially concerning the Markov chain state update mentioned from Line 127 to 134. Therefore, it would be beneficial for the authors to conduct a time complexity analysis of their proposed method.

### Minors:

1. I believe active learning is highly related to the proposed work. Conducting a literature review on active learning would offer practitioners valuable context, enhancing their understanding of the proposed research.

2. In Line 112, ""divergence"" would be a more appropriate term for describing the difference between distributions, rather than using ""distance"".

Limitations:
N/A

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
qNYYb4nOKA;"REVIEW 
Summary:
* This paper studies the evaluation of information extraction, particularly LLM-based IE, in scenarios where human-annotated data is unavailable.
* The proposed evaluation framework relies on the `Needle in a haystack` evaluation. That is, an LLM is first used to generate a piece of information (needle) given the original text; then, the needle is infused into the document, and the quality of IE is assessed by whether the needle can be successfully extracted. 
* In addition to this evaluation framework, the authors also discussed several aspects to be considered when using LLM-based IE for processing long documents.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
An interesting application of `Needle in a haystack` evaluation in information extraction.

Weaknesses:
* The writing quality is not great, and several areas require further clarification
	* The current paper structure is confusing; not sure what role Sections 3 and 4 play in this paper, e.g., whether the authors were proposing a new LLM-based IE approach
	* I suggest providing a formal definition of IE studied in this paper because it is very confusing to know what information is extracted. For example, in the abstract, `entity and its properties` is mentioned; in Section 3, `short paragraphs of text` seem to be the information extracted `from the continuous text`; also see Q2 
* The main contribution of the paper is an automatic framework to assess the quality of the IE; however, the authors didn't conduct any experiments to demonstrate the effectiveness of the proposed framework (e.g., whether the evaluation results correlate with human judgments); the other main limitation is the authors evaluate the quality of extraction based on the proportion of successfully extracted needles but totally ignore the correctness of extracted information (precision)
* The experiments are conducted on private datasets with only several toy examples described in the paper; it will be very difficult for others to reproduce the results. I would suggest conducting experiments at least on some document IE datasets, for example, from news or biomedical domains.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on the quality evaluation of information extraction (IE) performed by large language models (LLMs). It discusses the methods to handle the input/output size limitations of the LLMs and their performance in IE. It also introduces additional scores to evaluate the extraction quality and discusses how to interpret them.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper analyses the technical limitations of LLMs complicating the extraction of information from a long context.
2. This paper presents to insert a needle into the data to evaluate the performance of IE without labeled data.

Weaknesses:
1. The analysis of the performance of LLMs in IE is not new and has various analysis, such as in the following papers:

> [1] Evaluating ChatGPT's Information Extraction Capabilities: An Assessment of Performance, Explainability, Calibration, and Faithfulness (Li et al., 2023)

> [2] Is Information Extraction Solved by ChatGPT? An Analysis of Performance, Evaluation Criteria, Robustness and Errors (Han et al., 2023)

> [3] When does In-context Learning Fall Short and Why? A Study on Specification-Heavy Tasks (Peng et al., 2023)

Among the papers, the authors in [3] also analysed LLMs' limitations in long context understanding, which is similar to the conclusion of this paper. 

2. This paper lacks a thorough literature review in LLM for IE as well as new evaluation formats, such as [1, 2, 3] and the following paper:

> [4] Evaluating Generative Language Models in Information Extraction as Subjective Question Correction (Fan et al., LREC-COLING 2024)

3. This paper only focuses on the NER task but lacks the other IE tasks, e.g. relation extraction and event extraction. Additional experiments are required to test the generalisability of the method. The number of samples tested is also limited (see ""# entities used for evaluation"" in Table 3).

Limitations:
See ""Weaknesses"".

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a framework to capture information extraction quality in the absence of humanly labelled and curated datasets. It explains how an approach on how to include the schema, and the role and limitations of LLM's (specifically gpt-4-1106-preview).

Experiments are done (I guess), by ""extracting information"" from long business documents originating from the healthcare sector. Several scores are presented according to the SUSWIR metrics. It delves into the ""lost in the middle"" phenomenon. It introduces the MINEA score, a newly proposed metric.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
It tries to address a relevant problem in the field (curated benchmark data is hard to come by).

Weaknesses:
- The paper is from the start extremely vague and misses concrete statements and explanations about the work done. The contributions are unclear, the data is essentially undefined, for most of the work what exactly is being done is simply unclear.
	
- Even the task of ""Information Extraction"" is not concretely described in a way that is reproducible.
	
- Line 7-8: ""The framework focuses on information extraction in the form of entity and its 
	properties"". 
	
- Table 1: it is completely lost upon me what is being presented here.

-  ""We extract information from several long documents from our business case"". What are these documents? What are they originating from?

-  The scores mentioned are ""redundancy"". How is this measured? What do these scores represent? Is lower or higher better? Even these basic questions are not answered. All of this in the appendix (where it shouldnt be), and the further tables are not better.

- The work is very dry. There are no figures that explain or examplify what the problem is, or how this framework is supposed to fit.
	
- The related work section is short and doesn't address the original point (evaluation in absense of benchmark data).

- It is unclear to me how this work should contribute in any form to evaluation in the absence of benchmark data.
	
- The introduced MINEA score is ""explained"", but not examplified or mathematically defined.
	
- All examples are screenshots of data in JSON format rather than helpful explanations.

Limitations:
No. The paper does not concretely address the limitations of this metric. There are no good, bad examples provided.

Rating:
1: Very Strong Reject: For instance, a paper with incorrect statements, improper (e.g., offensive) language, unaddressed ethical considerations, incorrect results and/or flawed methodology (e.g., training using a test set).

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes an automated framework for evaluating the quality of IE tasks using LLMs. The framework introduces a scoring method called MINEA, which creates evaluation criteria by injecting artificial data (""needles"") into documents. The paper also discusses how to deal with the limitations of LLMs when processing large amounts of data, and introduces an iterative extraction process to improve the completeness of the extraction and reduce repetition.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
s1. The introduction of the MINEA score is somewhat innovative.

s2. The paper is clear explanations of the proposed framework.

Weaknesses:
w1. Lack of Originality: The originality of the paper is insufficient. Related work has already mentioned using the ""needle"" method to evaluate the information extraction capabilities of LLMs. While this paper adds the use of large models to help create the needles, the contribution is still lacking.

w2. Insufficient Experimental Description: The description of the experimental setup is missing, including the experimental environment, data sources, and dataset sizes. However, the paper spends too much space on toy examples.

w3. Unreliable Conclusions on Length Limitations: For the experiments on the input and output length limitations of models, the paper only tested one model, making the conclusions unreliable.

Limitations:
L1. The paper should provide a comparison to existing work to highlight the improvements.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
jrVoZLF20h;"REVIEW 
Summary:
This paper proposes Local-Global SIRENs, which partition the space into different regions and fit each region with smaller local INRs, leading to croppable INR by cropping the weights relative to the local regions. The model further proposes to use local and global feature extraction to improve the fitting performance. The experiments show that their method supports cropping INRs for Image, Audio, Video and CT.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The idea of using partition-based INRs for croppable INRs is novel. Croppable INRs are an interesting application for partition-based INRs.
2. The paper is well-written and easy to follow.
3. The cropping performance looks fancy and the fitting performance is slightly improved compared to SIREN due to the global feature extraction.

Weaknesses:
1. Cropping INRs is a natural property of partition-based INRs, which makes the contribution minor. 
2. While the authors mention their method supports automatic partitioning, they do not show the detailed implementation of how to automatically partition the space.
3. Even though the authors have conducted an ablation study on partition factors, the range of experimented partition factors is not enough. I recommend the authors to try Partition
Factors as low as (2,2) and as high as (512, 512) for the 512 * 512 images and show how the partition affects the fitting performance.
4. Cropping based on simple grids may be impractical for real scenarios. I recommend the authors improve their method by implementing semantic segmentation-based cropping. See partition-based INRs with semantic segmentation [1].
5. I am confused about the conclusion that enlarging the partition factors leads to faster training while decreasing the partition factors enhances reconstruction accuracy (line 293). Since SIREN is just your Local-Global SIREN with partition factors (1,1), if your conclusion is right, SIREN should have better performance than your LG SIREN. As pointed out in [1], partition should generally improve the fitting performance with larger partition factors. And please provide some explanation about why increasing the number of partitions enhances overall speed. 

[1] Liu, Ke, et al. ""Partition speeds up learning implicit neural representations based on exponential-increase hypothesis."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.

Limitations:
The limitations and potential negative societal impact of their work have been discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a method for learning patchwise INRs that are integrated with a global INR. The method is designed with cropping in mind, and this cropping can be achieved by pruning the relevant patchwise INR - similarly, cropping is limited to the pre-defined patches. This allows for novel post-training cropping, where the INR can be cropped in a way that reduces the number of model weights (and therefore the storage space). The method shows benefits beyond cropping - faster training time (or better visual quality at equal epochs). It is also flexible, and can be applied to various MLP-based INRs.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
[S1] The paper is exceptionally easy to follow, with good reasoning/motivation, well-explained method, and experiments that support the claims of the paper. Last paragraph of the intro, Figure 1, and Figure 3 are especially good in this regard.

[S2] While on face the applications seem limited, since the pruned parameters can be discarded, this method could be quite useful in the compression regime, where the MLP INR could be cropped by the end user and size would be reduced accordingly. The faster training time is also beneficial in this setting.

[S3] The method has benefits beyond its target problem (cropping) - better quality with less training.

[S4] The experiments are very thorough, proving the flexibility of the method with applications across multiple models and domains.

[S5] The supplementary even goes beyond what's necessary to provide comparisons with e.g. KD for INR training.

Weaknesses:
The patch-based formulation is a little unsatisfying in the following ways.

[W1] It doesn't reveal anything very new about the INRs themselves. Rather than discover pruneable parameters that correspond to cropping, or developing some objective that imparts some locality on the representation, the local vs. global distinction is very rigidly enforced as a prior.

[W2] The image can only be cropped according to the pre-defined patches.

Separately, 

[W3] Partly connected to W1, the post-training editing is totally restricted to only cropping. This might have applications to compression, but is otherwise at this point more of a novelty, in the sense that it is not very practical.

Limitations:
Yes.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new INR architecture to admit easy cropping of the target datum to a certain partition, allowing one to save memory and inference cost without any retraining. Comparing with training a new INR for the target partition, the approach lets one utilize the global context as well. The idea is to train multiple local networks, and modulate their intermediate features with the features of a global network. Through experiments on image/audio/video encoding, the paper demonstrates that the method enjoys faster and more accurate fitting than training one INR per partition. Also, the paper shows that the method works when combined with many INR architectures, e.g., SIREN and INCODE.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- A notable strength of the proposed method is that it is very simple and easy to use, making it likely to be scalable and generally applicable. I believe that the method can also be combined well with the triplane-based neural fields or instant-ngp.

- The method is also very clearly presented. In particular, Figure 3 is very effective in delivering how the proposed architecture works.

- Lastly, I appreciate the fact that the paper provides experiments on many different modalities, from image to video.

Weaknesses:
- **Utility of croppability?** The key weakness of this paper is the motivation. Apart from ""saving storage & compute,"" the practical utility of having a croppable INR is unclear; will local-global INRs also be useful in performing any subsequent ""editing"" operations? I suspect that this is why authors provide section 4.4, where the authors ""extend"" the LGS to parameterize the larger image than the one originally considered. However, for such purposes, there are already other good meta-learning-based solutions such as [26]. I am not sure why the model should be croppable for such applications.

- **Novelty & Ablations.** The proposed method bears much similarity with [26], which modulates the local model with another global model. The key difference here is how we modulate; this paper uses an extra linear layer to process the local+global features, while [26] uses multiplications (later works, such as functa, used addition). To fully understand what this paper contributes, there should be an explicit comparison with these similar methods as a baseline.

- **Evaluation.** If I understood correctly, the evaluation is mostly based on how the model fits the seen coordinates. I wonder how these affect the generalizability of the learned signal to unseen coordinates. As this is one of the key strengths of having a global context, I have enough reasons to believe that LGS will work well. However, an explicit verification is needed.

- **Hyperparameter tuning.** I wonder how the hyperparameters for the models and the baselines are selected. In particular, how were the values of ""omega"" and the learning rate selected? These two are quite critical in determining the fitting speed, so this point should be crystal clear.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
1PCsDNG6Jg;"REVIEW 
Summary:
Replicability is a notion of stability for learning algorithms, recently proposed by Impagliazzo et al. [ILPS22] to address the replicability crisis pervasive in scientific studies using statistical procedures. A learning algorithm $A$ is a function that takes as inputs a dataset $S \in (\mathcal{X} \times \mathcal{Y})^*$ and a (random) string $r \in \\{0,1\\}^*$, and outputs a hypothesis $f: \mathcal{X} \to \mathcal{Y}$. The algorithm is replicable if for independent draws of same sized datasets $S, S’$ and random $r$, $A(S, r) = A(S’, r)$ with high probability. Put simply, if two scientific labs use the same replicable algorithm to analyze independent datasets S and S’, and arrive at different conclusions, they will have a hard time blaming statistical fluctuations in the data.

This paper presents a variety of results that connect replicability to other well-studied concepts in learning theory, such as (realizable) online learnability and private learning. In particular, the authors show a computational separation between replicability and **online learnability**, assuming the existence of one-way functions. They also present a lifting procedure that transforms an efficient replicable PAC learner for a hypothesis class $\mathcal{H}$ over the uniform distribution on $\\{\pm 1\\}^n$ to replicable learners over any marginal distribution whose sample and time complexity depends on the complexity of the marginal. This lifting procedure is then used to design an efficient replicable algorithms for **learning parities over distributions that are far from uniform**. Furthermore, they show that any **pure DP learner** can be turned into a replicable one in time polynomial in all relevant problem parameters, except for the ""representation dimension"" of the hypothesis class.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This is a very well-written paper with solid technical contributions. Though the paper touches on various concepts in learning theory, the polished presentation ensures that the reader is not overwhelmed by the breadth of coverage. As replicability is a relatively new concept in learning theory, I was not familiar with it. However, the connections between replicability and other concepts are rather surprising and pleasing to someone new to the topic. The clear and well-organized proofs, each employing diverse techniques, were relatively easy to follow.

I found the application of the lifting procedure to replicable parity learning and the distinction from Gaussian elimination particularly interesting. For the uniform distribution, it is straightforward to see that Gaussian elimination is replicable for data labeled by parity functions. However, one can define simple distributions on which Gaussian elimination is *not* replicable (though the zero-one loss *is* small with high probability). This highlights the necessity and the non-triviality of the lifting procedure for efficient replicable learners over the uniform marginal. It also shows that computational separation between replicable learning and statistical query (SQ) learning extends to non-uniform marginal distributions.

Weaknesses:
Given that this is a solid and well-polished paper, I did not find any significant weaknesses, only a few minor points below.

- **Elaborating replicability with simplified, realistic statistical examples.** The paper begins by addressing the replicability crisis in scientific fields, but once the authors define replicability within the learning theory setup, the initial narrative gets lost. What would the random strings model in a real-world scientific study? Does it model the PRG seed number that experimenters use in their PyTorch code? Taking a simplified but realistic example (e.g., a hypothetical FDA approval procedure based on clinical data collected by independent labs), and mapping the A's, S's and r's to real-world concepts would be helpful.

- **Ambiguity in the big questions.** I found some of the ""big"" questions Q1-Q4 [page 2-3] too generic to be useful. In particular, questions like ""How does replicable learning *relate to* online/private learning?"" are extremely underspecified because ""relate to"" can have multiple interpretations. It would have been more helpful if the authors posed more specific motivating questions, such as ""Is there a computational separation between replicably learnable classes and online learnable classes?""

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In the paper, the authors discuss the connection and differences among replicability, a novel stability condition for learning algorithms proposed by Impagliazzo et al. [2022], online learning, and differential privacy from a computational perspective. Their first contribution is a computational separation between online learning and replicable PAC learning. In particular, under standard cryptographic assumptions, there is a concept class that can be replicably learned, but no efficient online learning algorithm exists. The second contribution is a method to extend a replicable PAC learner that works under the uniform distribution to ones that work under more complex distributions, whose probability mass functions can be computed by decision trees. The final result is a way to transform a purely differentially private learner into a replicable one. Combining these with some existence hardness/equivalence results provides a figure (Figure 1.1) referred to by the authors as the ""computational landscape of stability.""

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Figure 1.1 is an excellent summary of all known relationships between different notions of algorithmic stability. I believe the communities behind all three areas (replicable, online, DP) could benefit from such a roadmap. All the results are clean and well-motivated. 
The one I like the most is the lifting result for replicable learner. Intuitively, it feels like replicable learners and statistical query algorithms are almost equivalent as most replicable algorithms we know are based on the idea of making each statistical query fired by the algorithm replicable. The only exception known before is learning parities under the uniform distribution over boolean hypercube. The authors demonstrate the possibility that there may potentially be much larger gaps between SQ algorithms and replicable learners.

Weaknesses:
While the results are conceptually novel and interesting, the techniques used are more or less standard. For example, the lifting result follows from building a replicable version of the routine from Blanc et al. [2023] for learning the decision tree structure of the distributions, and the core of it is just to estimate the influence of distributions in a replicable manner (via random statistical query rounding).

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study the computational relationship between *replicable* learning algorithms, a recent notion of algorithmic stability [ILPS22] promising two runs of the algorithm over independent data output the same hypothesis with high probability assuming shared randomness, and other classical notions of algorithmic stability such as SQ-learning and differential privacy (as well as the closely related topic of online learning). The equivalences between such notions are well understood statistically, but this is the first paper to make a systematic attempt to study *computational* aspects of the problem.

More formally, the authors present both negative results (separations), and positive results (efficient transformations) between these notions:

On the negative side, the authors show:

1. A concept class based on one-way sequences which has no efficient online learner but can be learned replicably in polynomial time. This result is based on work of Bun separating private and online learning, but requires a new idea on the replicable side. In particular, two independent runs of the algorithm (with fresh samples) must identify a shared early element in the one-way sequence (using their shared randomness) to output the same unique hypothesis on the remainder. This is done via a new replicable quantile estimation lemma that leverages a variant of [ILPS22]'s randomized rounding methods.

2. A separation between SQ learning and replicable learning based on affine parities. While this class was already known to separate the two under the uniform distribution, no separation (or even efficient replicable algorithm) was known for more general distributions. Based on work of Blanc, Lange, Malik, and Tan, the authors give a general procedure to lift an efficient replicable algorithm for the uniform distribution for certain classes to a replicable algorithm for general distributions whose computational efficiency scales with the decision tree complexity. They then give examples of distributions with low decision tree complexity where the trivial `gaussian elimination’ based algorithm for the uniform case fails but their lifted algorithm runs in polynomial time (they also observe these distributions remain hard for SQ). This makes progress on a question of ILPS'22 about replicable algorithms where Gaussian elimination fails.

On the positive side:

1. There is an algorithm transforming any pure private learner for a class C to a $rho$-replicable learner in time exp(rep-dim(C))*poly(eps^{-1},\rho^{-1}), where eps is the accuracy parameter and rho is the replicability parameter. Their algorithm uses a classic procedure of Beimel Nissim and Stemmer which generates a finite approximation of C using a pure private learning, then learn this representation with the finite learner of [BGH+23]. To get polynomial dependence, this is run for constant error then boosted afterwords using replicable boosting.

Soundness:
1: poor

Presentation:
3: good

Contribution:
3: good

Strengths:
Replicability is a critical notion in machine learning and the sciences in general, and has recently been a fruitful notion more generally in the study of algorithmic stability, leading e.g. to advances in differential privacy. Prior works in the area largely focus on the statistical complexity of replicable learning. Understanding the *computational* cost of replicability, both in general and as compared to other notions of stability, is a critical open problem and clearly of interest to the NeurIPS community. This paper is the first to initiate a systematic study of this problem, and makes initial progress on understanding connections with pure privacy, SQ, and online learning, standard notions in the literature.

Related to the above, the authors make progress on an open problem of [ILPS’22] to design efficient replicable algorithms for parities beyond the uniform distribution (namely in this case for distributions with constant decision tree complexity). This takes some work to formalize and is a reasonable contribution on its own from a computational standpoint.

Weaknesses:
The work has two main weaknesses.

First, the authors seem to have misunderstood prior separation results in the literature, and as a result, the presentation of `computational separations’ with respect to privacy in the paper (namely in Figure 1.1 and the exposition) is wrong. Namely, the authors claim that there is an “efficient transform from pure DP learning to replicable learning” and “no efficient transform from apx DP learning to replicable learning”, where “efficient” is in terms of eps and rho (error and replicability) but not the underlying dimension of the problem, but this seems false.

In particular, there actually *is* a transformation from approximate DP to replicability that is “efficient” in this sense. Correlated sampling can be run in time roughly scaling with the output domain of the private algorithm, then boosted via ILPS from constant accuracy/replicability in polynomial time in these parameters. In fact, *any* learning problem that is solvable replicably can be solved in time polynomial in eps and rho by boosting/amplifying, so “efficiency” in these parameters is not very meaningful. The question of efficiency instead should be one of domain-size/dimension, which (as the authors to their credit highlight several times) is not efficient in the given reduction.

Part of the confusion here seems to stem from the result of [BGH+23] giving a computational separation between apx DP and replicability. In Prop 5.1, the authors state [BGH+23] exhibit a PAC problem which is efficiently learnable under apx-DP, but cannot be replicable learned assuming one-way functions exist. As far as I can tell, this is not shown in [BGH+23]. First, the separation given by [BGH+23] is not for PAC-learning, it is for a somewhat contrived statistical task; separating the two in the PAC setting is open. Second, the separation has nothing to do with one way functions (which seems to be a different result in their paper), and relies on public key encryption. Third, the separation is in terms of the dimension/size of the space, not accuracy or replicability parameters.

The second weakness of this paper is that, while extra technical work is certainly required for several of the results in this paper (namely the online bound, and generalizing BLMT23), the ideas in this paper do not go substantially beyond known methods in the study of replicability. The online vs replicability result is not too much of a jump from its use in work of Bun separating privacy and online learning, (the new replicable quantile estimation method takes work but is fairly straightforward from techniques in ILPS22). The SQ/distribution-lifting result also follows largely from combining techniques of [ILPS22] with [BLMT23]’s non-replicable method which already relies mostly on statistical estimation sub-routines.

Overall, the authors have identified an important problem and made some nice partial progress in this front (including progress on an open problem of ILPS22), but combined with the issues above and without introducing substantially new ideas to the study of replicability I cannot recommend the work for acceptance in its current form.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work contributes to the recently evolving area of replicable learning. This work establishes three main results

1. It is known that online learning algorithms can be replicable and replicable learning algorithms yield online learning algorithms. The work focuses on the computational complexity of these transformations and establishes a negative result--there exist concept classes that are efficiently, replicably, and learnable but there are no efficient online learning algorithms (assuming the existence of one-way functions).
2. Recent work showed that PAC learning algorithms under the uniform distribution can be black-box converted into distribution-free PAC learning algorithms. This work shows the transformation can be made replicable.
3. It is known there exist concept classes that are approximately DP learnable, but not efficiently, replicable learnable.  This work shows that if the concept class is pure DP-learnable, then it is efficiently replicable learnable.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This solid work clarifies several questions on replicable learning in the context of computational efficiency. The paper is very well written and the proofs are rigorous.

Weaknesses:
Main weakness is that obtained by combining known works. It is not clear to be that there is not that much novelty in the proofs.  for example, 

1. Proof of Theorem  2.1 proceeds in two parts.  a) existence of a concept class that is not online learnable, b) designing a replicable learning algorithm for this class. (a) is known by prior works, (b) replicable algorithm follows by the (now) standard technique of random thresholding/rounding. 
2. Please see Q2. 

Though the proofs may not be novel, the final results that the authors obtain are interesting.

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
oW6s6zFYj9;"REVIEW 
Summary:
This work belongs to ANN2SNN and proposes a novel coding scheme and neuron model to enhance the efficiency and accuracy of Spiking Neural Networks (SNNs) while reducing energy consumption. The Stepwise Weighted Spike (SWS) coding scheme improves information encoding by stepwise weighting input signals and introducing negative pulses, reducing the number of coding spikes needed. The Ternary Self-Amplifying (TSA) neuron model further enhances accuracy by progressively weighting the input through residual membrane potential adjustments and incorporating negative residuals and thresholds. Introducing silent periods allows the neuron to receive more input information before firing, significantly improving accuracy with minimal latency. Experimental results on datasets like MNIST, CIFAR10, and ImageNet demonstrate that the SWS coding scheme achieves better performance with fewer coding and computing steps, performing well even in very deep SNNs and achieving accuracy comparable to Artificial Neural Networks (ANNs) with the same structure.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Originality: This work introduces the Stepwise Weighted Spike (SWS) coding scheme, which is a novel approach in the field of Spiking Neural Networks (SNNs). The proposed method compresses spikes by weighting their significance in each step of neural computation, which enhances the performance and reduces the energy consumption of SNNs. Ternary pulses are a relatively new method in SNN, so the improvement of the ternary SNN encoding method has a relatively high degree of originality.

Quality: The paper provides a comprehensive set of experiments to validate the proposed SWS coding scheme. These experiments demonstrate that the SWS coding scheme significantly reduces operations and latency compared to existing neural coding schemes. The paper outlines the parameters used during training and provides justifications for the chosen experimental settings. 

Clarity: The introduction of the paper effectively motivates the work by discussing the limitations of current SNN coding schemes and proposing SWS as a solution. The methodology is clearly presented, with detailed descriptions of the new coding scheme and the Ternary Self-Amplifying (TSA) neuron model. Important symbols and their meanings are well-explained, contributing to the overall clarity of the paper.

Significance: The paper makes a significant contribution by proposing the SWS coding scheme, which enhances the efficiency and performance of SNNs. This new method addresses critical issues such as high latency and energy consumption in existing coding schemes, making it a valuable addition to the field. By improving the encoding of information in spikes, the SWS scheme has the potential to advance the development of more efficient and lower-power computing systems, thereby providing new options for the choice of coding schemes in SNNs.

Weaknesses:
In the ImageNet experiments in Table 2, SWS and other comparative ANN-SNN methods used different baselines, which is why the '$SNN\  Acc$' results are much higher than those of the comparative methods. However, the ‘$\Delta ACC$’ does not seem to show a significant difference (except for Hybrid training and Spiking ResNet). Using the same network architecture and pre-trained weights would be more credible.

Limitations:
The authors have not explicitly addressed the limitations or potential negative societal impacts of their work. To improve the transparency and completeness of their research, the authors could consider the following constructive suggestions:

1)  Limitations:

Create a dedicated ""Limitations"" section in the paper to discuss any constraints, assumptions, or potential weaknesses of the proposed SWS coding scheme.

Reflect on the robustness of the results to violations of assumptions, such as noiseless settings, model specifications, or dataset dependencies.

Discuss the scope of the claims made in the paper, including the generalizability of the approach across different datasets and scenarios.
Address factors that may influence the performance of the SWS coding scheme, such as computational efficiency and scalability with varying dataset sizes.

Consider possible limitations related to privacy and fairness concerns in the implementation of the SWS coding scheme.

2)  Negative Societal Impact:

Explicitly acknowledge the potential negative societal impacts of the SWS coding scheme, such as privacy risks, fairness considerations, or unintended consequences.

Discuss how the technology could be misused or lead to harmful outcomes, even if not intended by the authors.

Consider mitigation strategies to address any identified negative societal impacts, such as controlled release of models, monitoring mechanisms, or additional safeguards.

Emphasize the importance of ethical considerations and responsible deployment of the SWS coding scheme in real-world applications.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel Stepwise Weighted Spike (SWS) coding scheme designed to improve the efficiency of Spiking Neural Networks (SNNs) by compressing spikes and weighting their significance in each step of neural computation. This method addresses the issues of high delays and energy consumption associated with existing SNN coding schemes, as well as the complexity of neuron models and training techniques. The authors also introduce a Ternary Self-Amplifying (TSA) neuron model, incorporating a silent period to support SWS-based computing. This model is designed to minimize the residual error resulting from the stepwise weighting process. The experimental results provided in the manuscript demonstrate that the proposed SWS coding scheme significantly outperforms existing neural coding schemes, particularly in very deep SNNs. Key improvements include reduced operations and latency, enhanced overall performance, and lower energy consumption.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.	Innovative Approach: Introducing the SWS coding scheme and TSA neuron model is innovative.
2.	Performance Improvement: This paper provides experiments showing that the proposed methods outperform existing coding schemes regarding both performance and energy efficiency.

Weaknesses:
1.	Clarity and Detail: Some sections of this paper could benefit from more detailed explanations, particularly in the description of the SWS coding scheme and TSA neuron model. This would help in understanding the underlying mechanisms and their advantages.
2.	Comparative Analysis: While the experimental results are promising, there is no proof from the experimental results that the encoding method proposed is more advantageous.
3.	There are some grammatical errors in the paper. Such as the second paragraph of Section 3.3, ""The neurons only integrates input and performs stepwise weighting"". It is recommended that a uniform representation be used for ""spike"" and ""pulse"".
4.	Symbol design problem, ""t"" in Eq. (3) becomes ""n"" in Eq. (5).
5.	There are many long paragraphs and sentences in the paper, making it difficult for readers to accurately understand the meaning of the paper.
6.	The description of the problem in the third paragraph of Section 1 and the end of Section 2 is not clear, making it difficult for readers to understand the problem that the article really wants to solve.
7.	The description of the encoding method in Eq. (7) is difficult to understand. According to Eq.  (7), the encoded value $A_j$ should have no time step. However, in the experimental part, the method of this paper has 8 time steps.

Limitations:
The paper mentioned that due to the setting of the neuron's silent period, the delay increases. It can be seen from the experiments that the overall latency of the method is lower, which can be regarded as solving this limitation. At the same time, this article does not have potential negative social impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new coding scheme called Stepwise Weighted Spike (SWS) coding scheme for spiking neural networks to enhance the efficiency and reduce the number of operations and thus energy consumption. The SWS coding scheme tackles challenges associated with temporal and rate coding, such as heightened latency and energy usage. It achieves this by compressing spikes and assigning them varying weights at each computational step. Additionally, the paper introduces the Ternary Self-Amplifying (TSA) neuron model, which incorporates a silent phase to mitigate residual errors arising from the weighting procedure.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The SWS coding scheme enhances information capacity and reduces the number of spikes, leading to lower energy consumption and higher accuracy as compared to other coding schemes. The effectiveness of this approach is demonstrated using different datasets.

Weaknesses:
1. Which model of a spiking neuron is being employed in equation 3 (line 120)? What is the reset mechanism here after the neuron fires? Are the weights allowed to have negative values? The description of the model is unclear. 

2. The notion of residual error intuitively makes sense but it is confusing. Please define the residual error mathematically (line 139) for better understanding. 

3. Why ANN-(sws)SNN conversion is opted instead of directly training the SWS based SNN?

4. There are some recent works [1,2,3] with TTFS encoding which claims better results in regard to energy-efficiency and low-latency. First, these works need to be cited in the related work section. In my opinion, a detailed comparative analysis with other models and encoding schemes (for instance with [1,2,3]) needs to be carried out. 

[1] Göltz, J., Kriener, L., Baumbach, A. et al. Fast and energy-efficient neuromorphic deep learning with first-spike times. Nat Mach Intell 3, 823–835 (2021).

[2] I. M. Comsa, K. Potempa, L. Versari, T. Fischbacher, A. Gesmundo and J. Alakuijala, ""Temporal Coding in Spiking Neural Networks with Alpha Synaptic Function,"" ICASSP 2020 - 2020 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP), Barcelona, Spain, 2020, pp. 8529-8533, doi: 10.1109/ICASSP40776.2020.9053856.

[3] Stanojević, Ana et al. “An Exact Mapping From ReLU Networks to Spiking Neural Networks.” Neural networks : the official journal of the International Neural Network Society 168 (2022): 74-88.

Limitations:
There is no potential negative societal impact and and one limitation related to the inclusion of silent period is noted in the main text.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce a novel encoding method called Stepwise Weighted Spike (SWS) and a corresponding new neuron model named Ternary Self-Amplifying (TSA) for classification tasks utilizing the ANN2SNN training method. The proposed SWS encoding method assigns weights to the importance of spikes at each time step. The TSA neuron, which employs the SWS encoding method, features a lower threshold and includes a silent period.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Comprehensive method analysis: the authors conduct a thorough analysis of the Stepwise Weighted Spike (SWS) process, proposing a lower threshold and a silent period method to address residual error issues.

2. Superior Performance: the proposed method demonstrates superior performance in the field of ANN2SNN classification tasks.

Weaknesses:
1. Effectiveness of SWS: Various encoding methods, such as rate encoding and Time-to-First-Spike (TTFS) encoding, can be applied to different neurons and models. However, as illustrated in Figure 5, the SWS encoding method alone is ineffective without incorporating a lower threshold and a silent period. It only functions effectively when a neuron employs SWS encoding along with these additional components. Therefore, the paper should emphasize the neuron model rather than the encoding method, as it is not a universally applicable approach.
2. Lack of Experiments: The ablation study shows that the introduction of a silent period is the primary contributor to the improved performance. This raises doubts about the effectiveness of the SWS encoding method itself. Can the authors provide performance metrics for rate encoding combined with a lower threshold and silent period (if applicable) to ensure a fair comparison?

Limitations:
See weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce a new spike coding scheme, which allows them to directly convert quantized ANN to their coding scheme. They demonstrate the effectiveness of their conversion on several pre-trained ANN with minimal loss in performance at the cost of an increase in latency.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- strong experimental results
- coding scheme appears to be novel

Weaknesses:
- limited connection to spiking neurons, a more straightforward motivation would be a temporal encoding of quantized ANN

Limitations:
- method only applicable to conversion from pre-trained ANN
- no demonstration of training of a model using this coding scheme.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
0XKvW4ijxp;"REVIEW 
Summary:
The paper introduces a new framework for learning-augmented online algorithms. Learning-augmented algorithms (aka algorithms with predictions) are a very active subfield of beyond worst-case algorithm analysis. For online algorithms, which cope with uncertainty on their input, it gives an algorithm an addition information in form of a prediction. This prediction can be the (unknown) online instance or any other form of additional information. A learning-augmented algorithm is usually analyzed w.r.t. the quality of the given prediction.
In the most commonly used setup for online algorithm, the prediction is assumed to be generated upfront from some black-box predictor, or arrives online together with requests. There are also models where multiple predictions are initially available to an algorithm.
The authors of this paper introduce a new framework which integrates both the predictor and the learning-augmented algorithm. 
The main motivation is that a predictor might update its prediction while the online instance is being revealed, and, thus, increase the prediction's quality over time. 
In their model, they assume that there are given a set of hypotheses, which can be thought of different characteristics which the coming input could have. 
The predictor can compute at any time a new prediction using these hypotheses and the input which has been revealed up to that time.
The authors distinguish between two settings: the easier ""realizable"" setting, which assumes that the hypothesis which corresponds to the actual input is contained in the hypotheses class, and the harder ""agnostic"" setting, which does not make this assumption.
The authors then apply this framework to three well-studied online problems in the learning-augmented area: caching, load balancing and non-clairvoyant scheduling. Some of their new bounds improve over previous work.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper introduces an interesting generalization of the traditional learning-augmented framework. I think that this new view on learning-augmented algorithm can have an impact on this rapidly evolving field, since it untangles concepts which have already been used partially. I also appreciate the clear structure of the framework into predictor and algorithm.
- The framework is conceptually well motivated and the authors show that it can be applied to at least three different and well-studied problems, which proves that it is practicable and can be used for relevant problems.
- The authors present improved guarantees for previously well-studied online problems.

Weaknesses:
- A minor weakness can be the algorithmic novelty for the concrete applications. It seems that the algorithms follow in many cases the predictions given by the predictor. The predictors are tailored to the specific applications and are also rather simple.
- Another minor weakness is the close relation to the established 'multiple prediction' setting. From my understanding, one can see the hypothesis class the set of given predictions, and the goal (in the agnostic setting) is to guarantee a bound w.r.t. the best one.

Limitations:
As far as I see, all limitations have been properly addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new framework to use machine learned predictions to improve online algorithms. Recent work that has done this (in a slightly different framework) goes under the name learning-augmented algorithms (LA algorithms) or algorithms with predictions. The main difference in this work compared to the usual LA algorithms is that these authors propose integrating the learning problem into the algorithmic problem by allowing the algorithm to adaptively update its prediction based on the new information it receives. Additionally, instead of maintaining a single prediction, this framework maintains a hypothesis class of information about the instance. The paper exemplifies this framework on caching, online load balancing, and online non-clairvoyant scheduling. For all 3 problems they study a realizable and agnostic setting, where in the realizable setting, the hypothesis class contains h(I) for I the actual underlying offline instance, and in the agnostic setting, the information corresponding to the true offline instance is not necessarily in the class.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- This framework could indeed be a better way to study algorithms with predictions. In combining the learning process with the algorithmic procedure we get the benefit of (1) demystifying how the prediction is obtained and (2) allowing ourselves the flexibility of updating the prediction as we receive new information about the input.
- Using this more all-encompassing approach, the framework is able to improve upon some bounds from LA algorithms. 
- The authors did a good job in the related work differentiating their model from that on algorithms with a prediction pf portfolios, as well as data driven algorithms.

Weaknesses:
The algorithms proposed are augmenting simple procedures with this richer, more flexible hypothesis class, and the analyses involved are very straightforward. This isn’t inherently a bad thing. But there is a lot of work in the LA algorithms scene lately, and the work that stands out to me the most is the work that highlights some interesting technicality of the problem that wasn’t understood through classic worst-case analysis approaches. I’m afraid the problems that this model has been applied on did not result in technical components that stood out, and thus could lead to this paper blending into an already noisy scene. I will leave more specific related questions that may address this fear under “questions”.

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper considers a new formulation for learning-augmented algorithms/algorithms with predictions and applies it to the fundamental problems of online caching, online load balancing, and non-clairvoyant scheduling.  In the new formulation, the predictions/predictor is made more explicit and is a part of the solution process.  The predictions are given by a hypothesis class ${\cal H}$ of size $\ell$, which contains information about the ""plausible instances"" we are trying to solve.  One can think of ${\cal H}$ as coming from past data and the goal is to use this information to get improved algorithms.  This is done in two settings: the realizable and agnostic settings.

In the realizable setting, the true instance is realized in ${\cal H}$, so we can compare to the optimal cost in hindsight.  For this, the paper provides algorithms with the following bounds:

 - Caching: ${\rm OPT}+ k \log \ell$
 - Load Balancing: $O(\log \ell \cdot {\rm OPT})$
 - Non-clairvoyant scheduling: ${\rm OPT} + \ell \sqrt{ 2 \rm OPT}$

In the agnostic setting, the true instance may not be realized in ${\cal H}$, so instead we compare to the best hypothesis in ${\cal H}$.  For this, the paper provides algorithms with the following bounds:

 - Caching: ${\rm OPT} + O(\mu^* + k \log \ell)$
 - Load Balancing: $O( \mu^* \cdot \log \ell \cdot {\rm OPT})$
 - Non-clairvoyant scheduling: ${\rm OPT} + \mu^* +  O(n^{5/3} \log \ell)$

where in each, $\mu^*$ is a problem-dependent notion of distance from ${\cal H}$ to the true instance.

Some lower bounds are given showing tightness or almost-tightness for these results.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
This paper gives a very different approach to learning-augmented algorithms that is interesting and has the potential to generate more ideas.  While the algorithms once given the predictions are simple (they are all essentially some notion of follow-the-prediction), this work shows that these simple algorithms can work well if fed carefully constructed predictions (given by the prediction algorithms designed in this paper).  Lower bounds are given showing that the results are somewhat tight.

Weaknesses:
- I think it would help the presentation to give examples of hypothesis classes to make the main conceptual idea more clear and provide a stronger connection to practice.

- There is no experimental evaluation.  While some works in this area lack an experimental evaluation, there is a standard setup that has been used for the caching problem by Lykouris and Vassilvitskii [32] as well as Antoniadis et al. [4].  Providing such an evaluation could elucidate the construction of hypothesis classes and provide further evidence in favor of this paper's approach.

- The writing has some awkward phrases and grammar throughout, e.g. the following (among others):

   - line 62 ""... will be much smaller then ..."" -> ""... will be much smaller than ...""
   - lines 107-108 ""... using arguably simpler approach ..."" -> ""... using an arguably simpler approach ...""
   - lines 306-307 ""... under certain assumptions about input."" -> ""... under certain assumptions about the input.""
   - lines 374-375 ""In offline setting, ..."" -> ""In the offline setting, ...""

Limitations:
Limitations and assumptions have been made clear.  Potential negative societal impacts from this work are very unlikely.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies three online algorithms in the learning augmented model: caching, load balancing, and non-clairvoyant scheduling. The goal seems to be to 'online learning' flavored learning-augmented algorithms. Rather than having a single predictor that synthesizes or predict something about the online input, the authors propose a framework where we have access to a large class of hypothesis such as the set of all past inputs. The goal is to be competitive with respect to the best hypothesis for the online input in hindsight. For example in the caching problem, the hypothesis are a large set of input sequences, and the goal is to get close to OPT in the case that the online sequence is actually one of these inputs in our set of hypothesis. In the cases where the online input is not part of the set of hypothesis, the authors obtain error guarantees depending on the hamming distance of the online input and the closest one in the hypothesis.

For the problems studied, the authors match or improve upon (in some parameter regimes) prior works which can be interpreted to be in their framework.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The motivation of the formulation is natural: having access say to the entire history of past inputs could allow one to directly learn the best predictor from data
- The framework matches the best bounds for load balancing, and improves the caching result.

Weaknesses:
- The $n^(5/3)$ additive error for non-clairvoyant scheduling  seems to be a meaningless guarantee, since there are $n$ jobs which are assumed to have maximum job length $1$. So any scheduling guarantees that the jobs will finish in $n$ time. 
- The algorithms, for example for caching, must iterate over the entire set of hypothesis every time, which seems quite inefficient. 
- I'm also not sure if the formulation exactly captures the motivation of directly learning a good predictor from data. For example in caching, the performance seems to be 'bottle-necked' by a single past instance in the set of hypothesis, in the sense that we can only ever expect to do as good as the 'most similar' input in our set of past inputs. Rather, it would be more natural if one would learn from all of them simultaneously (so for ex 'learn' something from the first half of one past input and the second half of another). This is not the fault of their algorithms (which already achieve close to optimal performance), but rather the formulation itself. This is why I still think the standard prediction framework maybe more natural since a predictor can learn to synthesize information and implicitly combine different aspects of different past inputs seen, rather than relying on a single past input alone. 
- There are no experiments, even simple synthetic ones. It would be interesting to see even in synthetic case if the framework can actually be carried out.
- There are no algorithms in the main body and it is hard to judge algorithmic improvements.

Limitations:
No societal consequences.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
B3jt0Ran2t;"REVIEW 
Summary:
The paper describes the problem of matching students to daycare centers, with each family allowed to express preferences about the joint allocation of all siblings within the family. The authors present a modified notion of a stable matching, in which a family may choose to withdraw one of its children from a daycare in favor of a different child, so long as the daycare still prefers this assignment among alternatives with the first child removed. Under this stronger notion of stability, they show that the existing SDA algorithm may produce unstable outputs. They present an extension to the algorithm ESDA, whose successful outputs meet the new stability condition. They also show that the algorithm will be successful with high probability for a particular distribution over problem instances. In this distribution, children populate a daycare preference order by selecting from a fixed distribution over the daycares, and families aggregate these preferences into preferences over joint allocations, using an arbitrary aggregation function. Daycares sample a preference order over children from a Mallows model, with low dispersion. 
Finally, the authors show some empirical results from real Japanese municipalities in which ESDA produces stable matchings in all cases.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The algorithm is a heuristic (for good reasons of computational hardness of the problem). Hence, I characterize the contributions as follows:
* The authors define the problem, generalizing from couple-matching instances that may be expressed as families of size at most 2
* The authors present a new notion of stability in the matching which seems to be justified, given that families are generally empowered to prevent one of their children from attending a particular daycare.
* The authors present the ESDA algorithm, which internalizes the new stability notion in the details of the algorithm execution, and produces stable matchings whenever the algorithm succeeds
* Empirical analysis shows the algorithm succeeding on real-world instances
* Finally, the authors show that real-world instances have some strong properties in terms of the similarity across daycares of the preference ordering for students. They also incorporate this observation into the algorithm design, and are able to show that under a certain random model of problem instances, the algorithm succeeds with high probability. In the real-world examples, the preferences of the daycare are largely provided by the municipality, so the assumption is very likely to hold.
* As a smaller point, I appreciate that the authors presented some analysis of the behavior of the algorithm when the dispersion of the mallows models becomes high.
* An additional smaller point: earlier results that operate with a vanishing fraction of couples in the population seem unsatisfying. The theoretical results in this paper instead allow a constant fraction of the families to have siblings, but place stronger constraints on the similarity of orderings of the daycares, which seems better justified.

Weaknesses:
My first question is about goodness of fit of the paper to NeurIPS. The best fit from the CFP is:
* Social and economic aspects of machine learning (e.g., fairness, interpretability, human-AI interaction, privacy, safety, strategic behavior)
specifically for strategic behavior. However, I'm not sure this should be called economic aspects *of machine learning* specifically. I'll leave this issue with the area chair---my personal view is that it's not a great match. There are some related papers that have appeared in past conference instances (for instance, on deferred acceptance variants, but with more of a focus on computational complexity of an algorithmic approach, such as https://papers.nips.cc/paper_files/paper/2019/hash/cb70ab375662576bd1ac5aaf16b3fca4-Abstract.html).

Second, I'm concerned about the family preference model in section 4.1. In particular, the title of the paper says ""Large Markets,"" and as the size of the market grows beyond a small geographic area, it seems like that geographic preferences (for nearby daycares) will play a role. However, the random model is based on a single global distribution of preferences that applies to all families across all locations. This distribution is then further constrained to place similar probabilities on all daycares. There is no analysis of the empirical data to justify this uniformity assumption. Additionally, the model assigns independent preferences to two siblings of the same family, which seems to miss a) the fact that a family may have certain specific desires, and b) the family's geo preferences will apply similarly to all children, and c) sending multiple siblings to the same daycare may provide complementarities such as less logistic overhead for transport.

Limitations:
I think the authors have done a good job here.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study a variant of the many-to-one matching problem called the stable matching problem with siblings, which generalizes the stable matching problem with couples. In this problem, some families $f \in F$ may have more than one and at most $k$ siblings, ordered by age $(c_1,\dots,c_k)$. Each family $f = (c_1,\dots,c_k)$ expresses a joint linear preference order for daycares, denoted as 
$>_f \subseteq D \times D \dots \times D$,

 where $>_{f,j} = (d_1,\dots,d_k)$ represents the $j$th preference of family $f$, and $d_i$ corresponds to the preference for child $c_i$. Note that $>_f$ is an ordered set—a tuple. Each daycare $d \in D$ expresses a linear preference order $>_d \subseteq C$ for a subset of children and a maximum capacity $Q(d)$.

The objective is to find a (stable) matching such that no blocked pair exists. In essence, a blocked pair is a tuple (of edges) $(x_1,\dots,x_\ell)$ and $(y_1,\dots,y_\ell)$ such that swapping $x_i$ with $y_i$ results in a new matching that assigns children to daycares with higher priority for at least one family while not negatively impacting the assignment of any other family or daycare preferences.

A stable matching might not exist for restrictive settings of the problem, as the authors illustrate with a simple example in Appendix B.3. My understanding is that if the preferences form cycles, it becomes impossible to find a feasible solution that satisfies these preference constraints. However, in a daycare market where priorities are generated from a specific distribution, particularly random, the authors demonstrate that the probability of a stable matching existing converges to $1$ as the number of children $n$ approaches infinity.They present algorithms to solve the problem and conduct experiments on synthetic and real-world datasets, demonstrating that they can find feasible solutions in most instances.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper addresses a challenging and relevant variant of the many-to-one matching problem, the stable matching problem with siblings. The authors provide a comprehensive approach by defining the problem, presenting algorithms to solve it, and conducting thorough experiments on both synthetic and real-world datasets. Their work not only demonstrates the feasibility of finding stable matchings under specific conditions but also highlights the practical applications and implications for real-world daycare allocation scenarios.

Weaknesses:
While the paper makes significant contributions, there are some areas that could be improved. The writing is occasionally imprecise, making it challenging to follow the arguments and understand the definitions clearly. In particular, the choice of notation can be confusing (see detailed comments and questions). The structure of the paper is somewhat disorganized, with most of the proofs deferred to the appendix. Considering the strict page limits, this may be reasonable. However, Sections 3 and 4 could be compressed and written more concisely, and some proofs (or at least proof sketches) can be included in the main paper. I have only reviewed the proofs at a high level and have not verified the claims in sufficient detail. Given the strict reviewing timeline, this is the best I can do.

Limitations:
The authors do not discuss limitations and potential negative social impact of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the problem of daycare matching with siblings, an extension of matching with couples. Here, children in families (of size 1 or larger) are matched to daycares. Families have ranked preferences over the tuples of daycares their children end up at (since their preference for one child at one daycare may affect their preference of another child at some daycare), and daycares have preferences over children. In most cases, daycares do not differentiate between children within a family. This is an important problem to solve in Japan, and the authors actually worked with the Japanese daycare matching market in order to produce this work.

Their contributions are: 1) introduce the problem along with notions of rationality/stability/assumptions/etc, 2) propose an extended sorted deferred acceptance algorithm and prove that it will only return stable matchings and will fail to recognize a possible stable matching with probability approaching 1 as the problem grows, and 3) run experiments on their algorithm.

Their model is defined in a pretty standard way according to stable matching literature. The novelty, of course, is the introduction of families generalizing the size of couples. Their stability definition uniquely allows children in the same family to pass along seats to each other, so that a family may use that to their advantage in forming a blocking coalition. They assume that daycares have similar rankings over children and that they are drawn according to the Mallows Model, and that families only have few daycares they are interested in.

The algorithm itself works much like deferred acceptance. First, single children can propose to daycares per usual. Then, families with multiple children begin proposing, presumably according to their full ranking of matching tuples. When a single child is unseated from a daycare, they can simply propose to their next choice. When a family f has an unseated child when family f' is processed, the algorithm attempts again under a new order where f' goes before f. This can cause many iterations.

In the experiments, they use real datasets from Japan as well as larger synthetically-generated datasets. They compare their algorithm to a baseline constraint programming solution, showing that their algorithm returns the same solution faster.

Quick note: diameter is introduced in the main body but only used in the appendix. Perhaps move it to the appendix.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Stable matching is a very well-respected area of research, and this seems like a very natural formulation of the problem. It is particularly interesting that the authors are working directly with the market in need and they seem to have been given positive feedback about their work, so this work will almost definitely have a valuable use case. For the most part, the paper is written very well and it is very easy to get a high level understanding of most aspects of the project. Overall, I am very pleased with this paper and would be excited to see it at NeurIPS.

Weaknesses:
I am a bit concerned about the literature review provided. I am aware there is much more research that has been conducted on matching markets with complementaries (I am not knowledgeable enough to know what papers would be most useful), and I know there are various papers in this field. However, very few previous works are cited in this paper. It would be great if the authors could clarify the place of their work in the context of current literature and give confidence that this problem or a generalization of it has not already been studied. In fact, this is very important to motivate the paper.

Otherwise, there are a few points in the paper that are unclear. Much of it is very high level and lacks details, which is okay because it writes a narrative, but it comes at a cost of understanding the details of the proofs. More notably, I think the authors didn't spend enough time explaining their algorithm. I found it somewhat vague and I was uncertain about how it worked, and yet it is an integral part of the paper. This definitely needs to be improved.

Limitations:
Everything seems adequate.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the existence of a stable daycare-children matching in the presence of siblings from the same families with same preferences over the daycares. The authors particularly study the case when the daycares have similar preferences over the set of children, and the market size is large. They propose a variant of the Sorted Deferred Acceptance algorithm to compute the stable matchings.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The problem is well motivated by the real-world observation that stable matchings exist in the markets as opposed to what the theory suggests. This observation allowed the authors to make necessary adjustments to the assumptions that are sufficient for the theory to work out.
2. The authors take a systematic approach to the problem. They first define a new notion of stability that takes siblings into consideration and show that stable matchings may not exist in the presence of siblings and that the previous algorithms do not work for this new notion of stability. They then consider a specific random daycare market, mention the drawbacks of the existing methods of computing stable matchings, and then prove that a modification to the existing algorithm can find stable matchings with the new definition of stability.
3. The results by themselves are quite interesting; that stable matchings exist even in the presence of siblings with complementaries.
4. The analogy is drawn between the related work in stable matchings with couples and stable matchings with siblings

Weaknesses:
1. The assumption that day cares have similar priorities over children is slightly unrealistic.
2. The random daycare market for which the results are derived is somewhat restrictive.

Limitations:
Limitations sufficiently addressed by the authors.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
RdAfUp4LcD;"REVIEW 
Summary:
This work extrapolates the concept of Linear Mode Connectivity (LMC) modulo model invariances to differentiable tree ensembles (DTE). The authors revealed that, in contrast to neural networks (NNs), permutation invariance is insufficient to provide LMC in DTE and propose two additional tree-specific invariances that enable LMC after taking them into account: subtree flip invariance and splitting order invariance. In addition, they provide a modified DTE architecture that does not posses these additional invariances, however still enjoys LMC with only permutation invariance akin to neural network models. This work proposes two algorithms for building LMC given two independently trained DTEs, based on similar methods from NN LMC literature. The claims are supported by a detailed empirical evaluation.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
Honestly, I enjoyed reading this paper. Although I am not specialized in tree ensembles, I have certain expertise in LMC, and was pleased to find that it is also relevant for DTE models. I think that this contribution is novel and significant.

The paper is very well-structured. It was very easy to follow despite having no significant experience in decision trees, the authors did a good job preparing the reader in Sec. 2. 

Section 3 presents the main contributions of this work, which is done very well using both detailed and intuitive text description and auxiliary images illustrating the main concepts.

Empirical evaluation is excellent, involving multiple datasets, hyperparameter options, and random seeds. The authors tackled many important questions concerning the study of LMC in DTEs and even compared with NN LMC, which I specifically liked.

Weaknesses:
It is hard for me to formulate substantial flaws in this work but a couple of remarks that I put in the next section. 

The main weakness of this work is lack of theoretical support and practical implications. However, I acknowledge that these are the same limitations that are attributed to LMC in neural networks, which is a significantly more broad and well-studied field than LMC in tree ensembles. I hope that future work will address these disadvantages in some way. 

Also, I believe that the text could be slightly polished to eliminate typos and small inaccuracies. For instance, the value $D$ in line 127 is not defined at its first occurrence.

Limitations:
The authors discuss the limitations of their methods in Section 3.2.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides an analysis of types of neural networks called soft trees from the linear mode connectivity point of view. The authors enumerate 3 types of invariances inherent to soft trees and study linear mode connectivity between different solutions (by solution they understand a trained ensemble of soft tree models) after weights or activations matching that account for these invariances. They also study linear mode connectivity for a special case of soft trees - decision list-based tree - that has only one type of invariance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well written
- Authors claim that it is the first paper to study linear mode connectivity for soft trees

Weaknesses:
## Insufficient contribution
- In my opinion, the main contribution of this paper is a showcase that different architectures need to account for different invariances when LMC is analyzed, e.g. MLP and soft trees have different invariances. I think that this insight alone is not enough for a paper, because it sounds quite obvious even without analysis.

## Questionable results
- It is very important to make sure that interpolation results are not computed between the models which are almost identical (that can happen if there is not enough diversity in training recipes). Could you please provide results with distances (any kind of them, e.g. L2 or cosine similarity) between the solutions in Figure. 5 for ""Naive"", ""Tree Permutation"" and ""Ours"" parameter transformations?
- I would expect decision list trees to be much weaker than soft trees because they have less parameters. Could you please report its performance or show me where I can find it?
- Model merging is mentioned as one of the applications for linear mode connectivity (LMC), however, no results for model merging are provided.
  - line 32: ""In addition, LMC also holds significant practical importance, enabling techniques such as model merging [6, 7] by weight-space parameter averaging.""

## Questionable explanation
- I could not find a related work section.
- What is ""Ours"" in Table 2?
- I did not find in the main text any explanation (even after looking into algorithms in appendix, which I found very confusing) for the operation of weights matching (WM) and activation matching (AM) in case of such invariances as ""Perm"", ""Order"" and ""Flip"" (Notation is from Table 1). Since invariances are the main part of the whole analysis, could you please elaborate more?
- Another important part of parameter transforms includes Linear Assignment Problem (LAP), but I could not find any details for it neither.

Limitations:
- There is no theoretical justification for why and in which scenarios linear mode connectivity exists for soft trees.
- The paper does not propose any practical application for the linear mode connectivity between soft trees. While it can be argued that this paper is an analysis paper, some practical applications can be useful in motivating this kind of analysis.
- I did not find the code of the project while in the survey it is written that code is provided in supplementary material.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper empirically shows that separately trained tree emsemble models can show Linear Mode Connectivity (LMC) when considering tree invariant operations.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The exploration of LMC on tree emsemble models is interesting.
- The computational process is clearly stated which makes this paper easy to follow.

------

After reading author rebuttal and discussion with other reviewers, I decide to increase my rating of this paper to borderline reject.

Weaknesses:
This paper does not provide any insights into the question of LMC in neural networks, as it is exploring a totally different model. Although it is always interesting to consider LMC in another senario, I find the contribution of this paper rather insignificant and incremental, since it is basically applying the same idea of [1] to another model. I do not want to deny the author's valueable efforts in exploring symmetry in a new model and using it to achieve LMC, but I just feel that the contribution of this paper may not be sufficient for it to be accepted by this conference.

One possible direction I can suggest for the authors to enhance the current paper is, if any non-trivial theory about LMC can be made on the tree ensemble model setting, then this work will be much more exciting. The underlying reason why neural networks can be made linear connected is not yet clear, and the is hard to study due to the non-linear nature of deep NNs. If the authors can show that the tree ensemble model can be an alternative model to study LMC from a theoretical perspective, then this will make the current work more valuable and intresting.

[1] Git Re-Basin: Merging Models modulo Permutation Symmetries

**Regard writting**
The intro is kind of confusing for readers who are not familiar with the tree ensemble models. It's even unclear whether 1) it is a new model ensembling method for neural networks, or 2) it is a new model, or 3) it is a training method. Although those questions are addressed after reading the detailed definition of tree ensemble in Section 2.2, I think it is better to make it clear in the intro to avoid any confusing.

Limitations:
Authors discussed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to achieve LMC for soft tree ensembles. Akin to achieve LMC for neural network after accounting for permutation invariance, the authors introduce three different kinds of invariance in soft tree ensembles: tree permutation invariance, subtree flip invariance, splitting order invariance. Additionally, the authors demonstrate that better LMC can be achieved after considering all three kinds of invariance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea of extending LMC from neural networks to differentiable tree ensembles is interesting. 
2. Invariances beyond permutation variance are identified for differentiable tree ensembles. The authors demonstrate the effectiveness of accounting for these invariances when doing matching.

Weaknesses:
1. I am not familiar with differentiable tree ensembles, therefore, I would suggest the authors put more efforts on explaining tree ensembles and illustrating the invariances.
2. Another concern is about the motivation. This study is motivated by the question ""Can LMC be achieved for soft tree ensembles?"" but why would we achieve LMC for the tree ensembles? I would expect more elaboration on the motivation.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
TIhiFqGOYC;"REVIEW 
Summary:
This paper introduces a novel framework aimed at enhancing the abstract reasoning capabilities of large language models (LLMs) through a method called ""Meaningful Learning."" It specifically targets the challenge LLMs face in abstract reasoning despite their robust general reasoning abilities. The authors identify a notable gap in performance between general and abstract reasoning tasks and propose a structured approach to narrow this gap by using AbsR, a tailored dataset that includes generic facts coupled with guided explanations to foster deeper learning and understanding.
Key contributions include:
1. Introduction of the Meaningful Learning framework for improving abstract reasoning in LLMs.
2. Development of the AbsR dataset for training.
3. New metrics and Empirical evaluation across different settings

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Novel approach to improving abstract reasoning in LLMs through generic fact guidance.
2. Comprehensive evaluation across multiple settings demonstrating reasonable performance improvements.

Weaknesses:
1. Limited scale: The experiments are conducted on relatively small models (7B-13B parameters) compared to state-of-the-art LLMs. Without such it's hard to judge such method's applicability in real world and more complex tasks as mentioned in 5.1.
2. Human evaluation scale: The human evaluation of the AbsR dataset is conducted on a relatively small sample (200 instances) (Appendix E).
3. Lack of comparison to more recent reasoning techniques: The paper doesn't compare MeanLearn to recent advances in LLM reasoning capabilities.
4. Weaknesses in Evaluation Metrics: It's not convincing that Perplexity-based evaluation for classification tasks is good and the use of AbsAcc needs more clarification.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The study introduced a specialized dataset and learning approach that aimed to enhance LLMs' use of generic facts in reasoning. The results indicate that this methodology not only improved their general reasoning skills but also significantly developed their abstract reasoning abilities, suggesting a shift from mere memorization to a deeper, more nuanced understanding and application of information. The paper is well composed and demonstrates good, consistent performance across several benchmarks.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- the concept of AbsAcc is interesting and natural, as demonstrated by the experiments in Table 1, where the small difference between vanilla accuracy and AbsAcc for human subjects highlights that vanilla accuracy alone may not be an adequate metric for assessing abstract reasoning. 
- the paper is well-composed and straightforward to comprehend.

Weaknesses:
- the experiments currently focus on earlier versions of LLMs, including LLaMA-2 and GPT-3.5-turbo-0125. It is advisable to also include more recent models like LLaMA-3 and GPT-4/4o to ensure the consistency of the performance evaluations.
- some of the terminology used appears complex; for instance, in the memory learning section, the distinction between Knowledge and Reasoning examples (K-example and R-example) seems minimal. In the K-example, when a user poses a question, a supporting fact is cited while the R-example not. Additionally, I'm confused by the definition of abstract reasoning. It looks akin to tasks in commonsenseQA (https://aclanthology.org/N19-1421.pdf). The authors might need to elucidate the differences between their approach and that body of research.

Limitations:
- some of the terms mentioned in the paper maybe a little bit fancy and overestimated, please refer to the weakness section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores the abstract reasoning abilities of LLMs by creating a specific evaluation metric and a dataset called AbsR, developed using GPT-4. It presents a method, Meaningful Learning (MeanLearn), to improves both general and abstract reasoning accuracies of LLMs by teaching them to apply generic facts in varied scenarios.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- Proposing a dataset to study the potential of LLMs in abstract reasoning.
- Proposing MeanLearn to improve the abstract reasoning capabilities of LLMs by teaching them to use generic facts for reasoning.

Weaknesses:
- The paper is rushed, and not well organized. For instance, Some experimental results are presented in section 2, and others appear later in section 5 and after.
- The methodology section is half a page. More details on the model is needed. For instance, in Eq. 3, how the first and second equations will be utilized in the model.
- Incomplete related work section, specifically, insufficient related work on Abstract Reasoning.
- Some experimental results are in section 2: Abstract reasoning study, some later in section 5: Experiments. In section 2, the paper jumps into some results and experiments too early and without proper background.
- Experiments are limited. For example, it is unclear how the methods compare to [1] 

- [1] Xu, Yudong, et al. ""LLMs and the Abstraction and Reasoning Corpus: Successes, Failures, and the Importance of Object-based Representations."" Transactions on Machine Learning Research.

Limitations:
- Lack of sufficient experiments.
- Limited related work, and background sections.
- A thorough writing revision would be beneficial.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper addresses the challenge that LLMs face in abstract reasoning, where they often struggle to apply general facts to new situations despite their impressive performance in other reasoning tasks. To tackle this issue, the authors introduce an abstract reasoning dataset called AbsR, which incorporates generic facts and guided explanations to teach LLMs how to leverage such facts for reasoning. They also propose a learning paradigm named Meaningful Learning (MeanLearn) that simulates the human process of implicit knowledge acquisition, enabling LLMs to implicitly learn and apply generic facts without explicit input. Through experiments on various out-of-distribution reasoning and language understanding benchmarks, the paper demonstrates that MeanLearn improves the general and abstract reasoning capabilities of LLMs, moving beyond simple memorization towards a more nuanced understanding and application of knowledge.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Useful Resource: This paper introduces an abstract reasoning dataset (AbsR) that provides generic facts and guided explanations for reasoning tasks. The dataset and code will be publicly available, facilitating reproducibility and further community research.
2. Empirical Evidence: Comprehensive experimental results and ablation studies that validate the effectiveness of the proposed method.  Improvements are observed in both general and abstract reasoning performance of LLMs across various benchmarks.
3. Broad Applicability: The approach's effectiveness is shown on multiple LLMs of varying sizes, indicating broad applicability.

Weaknesses:
1. Evaluation on Advanced LLMs: The observation that the benefits of MeanLearn seem to diminish with better-trained LLMs, like LLaMA3, raises a crucial question about its broader applicability. While the method shows promise with smaller models, it's essential to assess its performance on more powerful LLMs like Mistral, Phi3, and Qwen 2. This would provide a clearer picture of its potential contribution in the context of rapidly advancing language models.

2. Data Augmentation and Training Dynamics: The use of GPT4 for constructing the AbsR dataset, with annotation quality comparable to human annotators, opens up interesting possibilities for data augmentation. Exploring the impact of increasing the training data size, potentially using more cost-effective models like GPT4o or open-source LLMs, could reveal the potential for further performance gains. Additionally, analyzing the training dynamics of MeanLearn by varying the proportion of training data (e.g., 20%, 50%, 80%) would provide valuable insights into its saturation point and the diminishing returns of additional data.

3. Expanding Reasoning Benchmarks: The paper's focus on commonsense reasoning benchmarks (Com. and ARC) should be complemented by evaluation on other reasoning domains, such as arithmetic reasoning. Including benchmarks like GSM8K or MATH would provide a more comprehensive understanding of MeanLearn's capabilities across different reasoning tasks.

4. Presentation Clarity and Improvements: The paper's presentation could benefit from several improvements. The introduction should explicitly connect abstract reasoning with other types of reasoning, providing a broader context for the research. The pilot investigation in the introduction is quite confusing as it does not introduce the datasets, metrics, and setup used. Confusing figures, like Figure 2, should be explained in detail to ensure clarity and understanding.

5. Updated Related Work: The related work section should be updated to include recent efforts in tuning-based methods, ensuring a comprehensive overview of the current landscape in abstract reasoning research.

Limitations:
The authors adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
HhnpPISAUH;"REVIEW 
Summary:
This paper provides an interesting approach HiCS-FL to investigate the client sampling problem in federated learning, especially for the non-iid setting. This paper estimates the clients' data statistical heterogeneity (label distributions) via the client-updated gradients of the output layer's weights. By using this distance information, the server can distinguish which client owns more balanced data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper found an interesting relationship between the last layer's bias and the label distributions of a given client data.
- The proposed distance metric can be used to estimate the local data heterogeneity and distinguish which one's data is more balanced.
- The paper is well-written and easy to follow.

Weaknesses:
- A potential privacy issue may be raised since it needs to access the individual client update (i.e., gradient) information.  More client's data information may be leaked from the gradient inversion attacks [1]. 


[1] Evaluating Gradient Inversion Attacks and Defenses in Federated Learning, NeurIPS 2021.

Limitations:
See Weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper address data heterogeneity by clustering clients via the gradients of the output layer to distinguish between clients with balanced from those with imbalanced data. HiCS-FL assigns different importance to the clusters according to their average estimated data heterogeneity. The paper found that there is a correlation between the gradient of the output layer and the label distribution.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is very well written. The paper did a good job of summarizing what others have done and how HiCS-FL differs. 

Figure 2, and Figure 3 shows considerable gain over the baselines on common FL datasets. 

The method doesn't incur computation or communication overhead and leads to 2x speed up over other methods.

Weaknesses:
The approach of clustering to address heterogeneity might be practical as there are simpler methods that can achieve the same goal. 
The experiments were done on a small set of clients, 50. As the number of clients increases, the more pronounced the effect of heterogeneity can have.

Limitations:
The paper doesn't list limitation but I think a limitation there is the experimental setting. Using  Dirichlet to generate clients is unrealistic (I know this is common in FL papers), and the paper focuses only on classification tasks.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses the challenges posed by non-IID data in FL systems, particularly under communication constraints where only a small fraction of clients can participate in each training round. It introduces HiCS-FL, a novel client selection method. HiCS-FL estimates the statistical heterogeneity of a client’s data using updates from the network’s output layer to cluster and sample clients more effectively. Based on the heterogeneity information, clients are clustered, and those with more balanced data are preferentially sampled. Experimental results demonstrate HiCS-FL's superior performance compared to state-of-the-art methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The introduction of a hierarchical clustering-based method that considers data heterogeneity is a significant advancement over existing client selection techniques. The solution considers the balanced and imbalanced client data to train the model, which is a very interesting perspective. The extensive experiments on multiple datasets and comparison with several baselines demonstrate the robustness and generalizability of HiCS-FL. This paper is well-written.

Weaknesses:
- The paper primarily focuses on label imbalance when discussing data heterogeneity. However, other types of imbalances, such as feature imbalances within the data, are not considered. Addressing these other forms of data imbalance could provide a more comprehensive solution.

- The method involves collecting updates from the fully connected layer, which may inadvertently reveal the label distribution of a client’s data. This could raise privacy concerns, as it potentially compromises the confidentiality of the client's data distribution.

- The paper does not compare HiCS-FL with other relevant algorithms, such as Oort and Auxo, which also focus on client selection and clustering. Including these comparisons or discussions would provide a clearer picture of HiCS-FL's relative performance and effectiveness.

[1] Oort: Efficient Federated Learning via Guided Participant Selection 
[2] Auxo: Heterogeneity-Mitigating Federated Learning via Scalable Client Clustering

- The assumption that clients can be clustered into M groups, where M is larger than the number of selected clients (K) per round, may not be practical in large-scale systems. Specifically, the paper assumes that all clients are available during the initial clustering phase and that no new clients join the system later, which is unrealistic in dynamic FL environments.

There might be additional overhead associated with the proposed clustering method. The paper should ensure that the experimental comparisons account for this overhead to maintain fairness in the evaluation of HiCS-FL’s performance.

Limitations:
See weakness

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a novel client selection method to address federated learning scenarios where clients exhibit varying degrees of data imbalance. The authors estimate the label distribution entropy of clients on the server side using the gradient of the output layer's bias. Based on this estimation, they cluster and sample clients, utilizing those with more balanced data to train the global model. Experiments in the paper demonstrate that the proposed method, HiCS-FL, achieves better and faster convergence compared to baselines. The authors provide detailed theoretical proofs to support their approach.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This paper proposes HiCS-FL, a new federated learning method designed to adaptively handle clients with varying label distributions. It achieves better convergence and greater efficiency compared to baseline methods in experiments.

2. HiCS-FL utilizes the gradients of the output layer's bias to estimate clients' statistical data heterogeneity. This estimation is then used to cluster and sample clients, enabling efficient training of the global model with relatively data-balanced clients.

3. This paper provides a detailed theoretical analysis of HiCS-FL.

Weaknesses:
See questions.

Limitations:
The authors mention leaving studies of system heterogeneity to future work in the first paragraph of the Introduction, but there is no further discussion on the limitations of the proposed approach. There are no concerns raised about the societal impacts of the research.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
C3t6GMPnC5;"REVIEW 
Summary:
This paper explores the capabilities of Mamba state-space models (SSMs) in comparison to Transformer large language models (LLMs) in various downstream learning tasks. Despite Mamba's success in some areas, the paper identifies challenges and limitations in achieving performance parity with Transformers on standard benchmarks, particularly in in-context learning (ICL), mixed-precision fine-tuning (MPFT), and parameter-efficient fine-tuning (PEFT). The study demonstrates that while Mamba models have robust recurrent dynamics and can achieve significant speed and memory efficiency gains through fine-tuning techniques, their downstream learning improvements still lag behind those of Transformers.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy to read.

2. The study shows that Mamba’s recurrent dynamics are robust to small input changes, which is validated both theoretically and empirically. This robustness ensures stability in training and fine-tuning processes.

3. Despite initial shortcomings in ICL performance, extensive experiments demonstrate that mamba models exhibit strong potential for improvement through efficient fine-tuning. The models can achieve up to 81.5% of the ICL performance improvement, highlighting their adaptability with appropriate tuning methods.

Weaknesses:
I appreciate the authors for providing a theoretical analysis to demonstrate the controllability of implementing AMP on Mamba blocks, and the experiments indicate that PEFT is also suitable for Mamba. However, I have several concerns:

1. The authors define the Mamba process as a generalized operation: $x_t=F_{\theta}(x_{t-1},u_t)$, but the actual output of Mamba is $y_t = \bar{C_t}x_t$.Therefore, the theoretical analysis provided in the paper pertains to the stability of the hidden state under small perturbations. Is it possible to extend this analysis directly to the output $y_t$? Since the stability of the hidden state does not necessarily imply the stability of the output.
2. Theorem 1 ensures the feasibility of implementing LoRA on Mamba blocks but focuses on the $W$ matrix, neglecting the consideration of the most crucial transition matrix $\bar{A_t}$ in Mamba. Does this mean that the $\bar{A_t}$ matrix was not subjected to LoRA during fine-tuning? If so, is it possible to consider applying PEFT to the $\bar{A_t}$ matrix as well?
3. As an empirically-driven paper, would it be possible to include more backbones for comparison in future versions? Currently, the only baseline for comparison is Pythia.

Limitations:
The authors raise some limitaions, for example, the lower-precision method is not explored in this paper, but the authors claime to solve them in the future.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores Mamba's downstream learning capabilities through two primary aspects: (i) fine-tuning and (ii) in-context learning. Specifically, it examines the training stability and robustness of fine-tuning when mixed precision is applied, as well as Mamba's ability to perform in-context learning. The contributions of this paper include:

- Theoretical analysis of the stable dynamics of Mamba.
- The theoretical analysis is corroborated by the experiments.
- Experimental demonstration of Mamba's limitations on real datasets in terms of ICL.
- ICL performance improvement of Mamba through fine-tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The theoretical analysis section, although I did not verify the proofs, offers valuable insights and paves the way for more complex analyses in future work.
- This paper is well-motivated.

Weaknesses:
The weakness is majorly from the ICL part. 

In fact, the authors show that pretrained Mamba cannot learn well via ICL, but can learn well after fine-tuning. This fact indicates that this limitation does not come from Mamba architecture itself, which is also consistent with the observation of other works such as [1]. Therefore, the limitations observed in this paper is just a general limitation caused by training recipes, which is not Mamba-specific and has been studied in many works. The solution is also standard, and the improvements are also expected since once trains well, Mamba should be able to perform in-context learning as shown in [1]. 

Moreover, many questions are still unclear. For instance, why does Mamba suffer from such limitations? 

Therefore, the study in terms of the ICL part is lack of depth, novelty, and technical contribution. 

In terms of the mixed precision part, there are also many places that are unclear to me. I suggest the authors to use more space in discussing the speciality of Mamba compared to Transformers, and what structure of Mamba caused this problem. If recurrence is the main cause of the problem, having more experiments of similar models such as GLA, linear attention, etc would also help readers to understand more about the phenomenon. 

---

References

[1] Park, Jongho, et al. ""Can mamba learn how to learn? a comparative study on in-context learning tasks."" ICML 2024.

Limitations:
The work is well-motivated, but the study lacks depth.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper looks at improving start space models or Mamba by enabling mixed precision handling to improve inference and fine-tuning. The results show similar performance with a significantly reduced memory requirement

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
There are extensive results compared to full-precision models
The authors provide a proof of the theorem to back up their claim
The change to the mamba block is clear and easy to implement by others

Weaknesses:
The actual change is relatively minor in quantity but does deliver the author's required memory reductions.
The works don't use the larger models available due to limitations on memory requirements still

Limitations:
the limitations are well discussed at the end of the paper and generally relate to LLM or transformers in general too

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
47CdPNiWUB;"REVIEW 
Summary:
This paper proposes a methodology called Rockafellian Relaxation (RR) to mitigate the impact of labeling errors in neural network training. The method is architecture-independent and integrates concepts from adversarial training to address dataset imperfections robustly. Through theoretical justifications and a series of experiments on standard datasets like MNIST and Toxic Comments, the paper demonstrates that RR can significantly improve the performance of neural networks trained under various corruption levels. The paper’s contributions are particularly valuable as they provide a new tool for improving training accuracy in the presence of label noise, enhancing the robustness and applicability of machine learning models in diverse and error-prone real-world settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper's approach to using Rockafellian Relaxation for addressing labeling errors is innovative, especially the combination with adversarial training concepts.

Quality: The method is grounded in solid theoretical justification, and the empirical results show marked improvements over existing methods.

Clarity: The explanations of the methodologies and the algorithms are clear and detailed, making it easier to understand the operational aspects of the proposed solution.

Significance: The significance of this work lies in its potential to improve training robustness across various domains and dataset imperfections, which is highly relevant for deploying machine learning models in error-prone real-world environments.

Weaknesses:
Computational Complexity: The added complexity might limit the practical application of the method in scenarios with constrained computational resources.

Limitations:
Generalization to Different Noise Types: While the method is tested against uniform label noise, its effectiveness against other types of noise is not thoroughly investigated.

Dependence on Hyperparameter Tuning: The effectiveness of RRM is likely sensitive to the choice of hyperparameters, such as the regularization term and the parameters controlling the adversarial component. The paper does not provide extensive guidance on hyperparameter selection, which could affect the reproducibility and ease of application in different scenarios.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes a loss reweighting scheme to train models in the presence of label errors. When training an NN with empirical risk minimization in this setting, one would want to assign a weight of zero to all datapoints that are mislabeled and a weight of one to all datapoints that are correctly labeled. This paper presents an automated method for accomplishing this weighting, called the Rockafellian Relaxation Method (RRM). It is noted in Theorem 3.1 that the inner minimization objective of RRM reduces to a linear programming problem, despite RRM being non convex in general. After relating RRM to distributionally robust optimization techniques, the adversarial variant of RRM is introduced (A-RRM), which includes adversarial perturbations to induce adversarial training as well as loss reweighting. Experiments on four datasets show that RRM and A-RRM outperform other methods in both adversarial settings and settings with high proportions of noisy labels.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work addresses two different types of robustness: robustness to label noise and robustness to adversarial feature perturbation. It should be of interest to those who are generally interested in robust and trustworthy machine learning. Furthermore, the proposed training method has strong theoretical foundations, and its relation to other optimization formulations is discussed in detail. The theoretical results are validated in experiments that cover different datasets and types of data corruption.

Weaknesses:
The experimental section lacks a relevant baseline for comparison. As it stands, it is unclear how this compares to other noise-reduction techniques. The relationship to other techniques is discussed in the related work section, it would be nice if the purported benefits of this approach were borne out empirically.

The introduction of adversarial training in section 3.5 is under-motivated. Based on the earlier sections, it is unclear how label and adversarial feature corruptions are related to each other, why we would want to achieve robustness to both, and whether previous approaches have attempted this before. I would suggest explicitly motivating this earlier in the paper.

Limitations:
The limitations are briefly discussed in the paper. As noted above, one main limitation is that it only studies $\ell_\infty$ bounded FGSM attacks. Furthermore, this paper only considers the uniform label noise model, and does not consider the case when label corruption might be correlated with features.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents Rockafellian Relaxation (RR), a new method to address labeling errors in machine learning datasets. RR is a loss reweighting technique that enhances neural network robustness against labeling errors and adversarial attacks, working across various data domains and model architectures. The key contribution is an approach that mitigates label corruption and class imbalance without needing clean validation sets, offering a practical solution for training robust models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper introduces Rockafellian Relaxation (RR), a novel loss reweighting methodology that addresses learning with noisy label problems

- The authors provide a solid theoretical basis for RR, relating it to optimistic and robust distributional optimization formulations. RR is also designed to be architecture-independent, making it a versatile tool applicable across different neural network architectures.

- The method does not rely on having clean validation data, which is of advantage in many real-world applications.

Weaknesses:
- While not explicitly mentioned, the iterative nature of the RR algorithm could potentially be computationally intensive, especially for large datasets.

- The method assumes a specific model of label noise (e.g., uniform label noise), which may not hold in all real-world scenarios. 

- The paper could benefit from a more comprehensive comparison with other state-of-the-art methods for handling noisy labels, such as GCE [1], ELR[2], to better position RR in the existing literature.

[R1] Generalized Cross Entropy Loss for Training Deep Neural Networks with Noisy Labels

[R2] Early-Learning Regularization Prevents Memorization of Noisy Labels

Limitations:
Authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
8ohsbxw7q8;"REVIEW 
Summary:
The paper studies the problem of learning graph diffusion generative models on arbitrary non-differentiable objectives using policy gradients. Authors argue that the recently proposed DDPO technique doesn't work well on the discrete, graph-related learning tasks and consider a modified objective and a corresponding gradient estimate which they refer to as Graph Diffusion Policy Optimization (GDPO). They show that GDPO performs significantly better on a number of reward functions and datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
## GDPO seems to be efficient 

Discrete diffusion models are valuable class of machine learning models and being able to trained them on non-differentiable objectives might might unlock interesting applications, in particular, in optimizing molecular graphs as authors demonstrated.

## The paper is well-written and easy to follow

I can clearly answer most of my own questions about how GDPO works and it should be easy to reproduce main results.

Weaknesses:
## Underexplored potential of modern RL techniques

While the proposed GDPO is valueable regardless, it is unclear to me that the biased objective / gradient estimate of eager gradients is necessary. Neither DDPO nor this paper explores even the simplest techniques such as actor-critic, PPO / TRPO or more sophisticated versions of importance sampling. DDPO showed good performance on image tasks without such advances and it could well be that it would still work well on graph tasks.

## Lack of bias-variance analysis

Authors acknowledge that eager gradients is a biased version of the standard policy gradients but don't provider either theoretical or empirical analysis of the bias-variance tradeoff between the two methods. 

In the appendix (line 660) authors a strange statement that importance sampling is used to reduce variance. It is not obvious to me all. Importance sampling can both reduce or increase variance depending on the proposal or the policy generating the trajectory. What it achieves is that it allows to train on experiences generated by policies other than the current policy being optimized.

Limitations:
See above, I believe the paper needs a clearer discussion of the bias introduced by policy gradients. At the moment, it is not clear what is the connection of GDPO to the reverse diffusion MDP.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces graph diffusion policy optimization (GDPO), a policy gradient method for optimizing graph diffusion probabilistic models with respect to non-differentiable reward signals. By establishing the connection between a T-step denoising process and a T-step Markov Decision Process (MDP), policy gradient methods can be applied to graph diffusion models. While a previous work (DDPO) report s competitive generation quality for image diffusion models trained with the classical REINFORCE algorithm, the authors empirically observe a convergence issue when applying REINFORCE to graph diffusion models. This issue is possibly due to the increasingly vast space constituted by discrete graph trajectories as the number of nodes in the graph increases. To address this issue, the authors propose GDPO, a modified policy optimization objective. Empirical studies demonstrate the effectiveness of the proposed modified objective.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
**S1.** The proposed methodology is neat and well-motivated.

**S2.** The paper is well-written and easy to follow.

**S3.** The empirical studies demonstrate the effectiveness of the proposed approach.

Weaknesses:
**W1.** Some experiment settings are not completely clear from the description. See the questions below.

**W2.** The proposed modification coincides with the idea of training a denoising network to predict the original uncorrupted graph rather than perform one-step denoising. Some discussions are expected.

**W3.** Classifier-based and classifier-free guidance are two popular approaches for training conditional diffusion models and have been previously explored for graph diffusion models. Some discussions on the potential pros and cons of the RL approach against them are expected.

Limitations:
The paper considers discrete time graph diffusion models. Whether the proposed approach is effective for continuous time graph diffusion models remains unexplored.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Graph Diffusion Policy Optimization (GDPO), a novel approach to optimize graph diffusion models for arbitrary objectives using reinforcement learning. The key contributions are:

1. Formulating the denoising process of graph diffusion probabilistic models (DPMs) as a Markov decision process and proposing an ""eager policy gradient"" method tailored for graph DPMs to address high variance issues in standard policy gradient approaches.
2. Demonstrating state-of-the-art performance on various graph generation tasks, including general graph generation and molecular graph generation with complex objectives.

The authors show that GDPO significantly outperforms baseline methods, including other graph generation techniques and adaptations of diffusion model optimization approaches from the image domain (e.g., DDPO).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Novelty and Originality:  Introduction of the ""eager policy gradient"" method, to address the high variance issues encountered with standard policy gradients in graph diffusion models. Despite the lack of theory supporting it, it's a clever  solution to a significant challenge in optimizing these models for arbitrary objectives.

2. Clarity:
   - Clear problem formulation: The authors provide a well-structured explanation of the challenges in optimizing graph DPMs and why existing methods fall short.
   - Effective visualization: Figure 1 offers a clear overview of the GDPO method, aiding understanding of the approach.
   - Detailed ablation studies: The paper includes thorough analyses of different components and configurations of GDPO, which helps clarify the contribution of each aspect of the method.

3. Significance:
   - Strong performance improvements: GDPO demonstrates substantial gains over state-of-the-art baselines across various graph generation tasks. For example, in molecular graph generation, it achieves up to a 19.31% improvement in hit ratio for generating effective drugs.
   - Sample efficiency: The method achieves good results with relatively few queries (e.g., 1/25 of the training samples), which is crucial for applications where reward evaluation may be computationally expensive, such as drug discovery.
   - Broad applicability: GDPO is flexible and can be applied to a wide range of graph generation tasks with complex, multi-objective reward functions. This versatility enhances its potential impact on the field.

4. Technical Quality:
   - Thorough experimentation: The authors provide extensive experiments on both general graph generation and molecular graph generation tasks, lending credibility to their claims.
   - Careful analysis of baseline methods: The paper includes a detailed study of  DDPO (a related method for image diffusion models) failing on graph DPMs, which strengthens the justification for GDPO.
   - Consideration of practical aspects: The authors address important practical considerations such as the impact of different reward weightings and the number of trajectories used for gradient estimation.

Weaknesses:
- Limited theoretical analysis: While the eager policy gradient is empirically effective, the paper lacks a rigorous theoretical treatment of its properties, particularly regarding the bias-variance trade-off.
-  The paper would benefit from a comparison to other RL-utilizing graph generation methods, particularly MolGAN https://arxiv.org/pdf/1805.11973 , which also applies RL techniques to molecular graph generation.
- Scalability concerns: The paper does not explore the method's performance on very large graphs (e.g., 500+ nodes), leaving questions about its scalability unanswered.
- Limited exploration of failure cases: While the authors provide a failure case related to novelty optimization, a more comprehensive exploration of scenarios where GDPO struggles would provide valuable insights into its limitations.

Limitations:
I think the limitations in the appendix are acceptable

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
tVO3b68Oyp;"REVIEW 
Summary:
This paper proposes a two-stage speech language model with semantic tokens and acoustic tokens similar to AudioLM ([Borsos et al., 2022]).
-   The semantic tokens come from a speech tokenizer that can group a variable number of frames into a single token. To train such a speech tokenizer,
    1.  This paper first takes inspirations from syllable-like structures uncovered from HuBERT, and produces an initial segmentation (Section 3.1).
    1.  An iterative process is then applied to improve the segmentation (Section 3.2).
    1.  Finally the tokens are obtained by clustering of the mean-pooled frame features (Section 4.1).
-    The acoustic tokens are identical to the HuBERT-based tokens in ([Hassid et al., 2023]), referred to as ""mHuBERT"" in this paper.

Experiments with the proposed model demonstrate the following when compared to previous work,
-   Better unsupervised syllable segmentation
-   Lower speech reconstruction WER
-   Better or competitive accuracy in speech language modelling tasks (sWUGGY, sBLIMP, tStoryCloze) with lower compute
-   Better speech continuation quality

[Borsos et al., 2022]: https://arxiv.org/pdf/2209.03143 ""AudioLM: a Language Modeling Approach to Audio Generation""
[Hassid et al., 2023]: https://proceedings.neurips.cc/paper_files/paper/2023/file/c859b99b5d717c9035e79d43dfd69435-Paper-Conference.pdf ""Textually Pretrained Speech Language Models""

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
-   Originality: This paper proposes an original method for producing syllable-like segmentation of speech in an unsupervised manner.
    -   It uses conditional probabilities from a masked language model instead of feature similarity ([Peng et al., 2023]) to detect initial syllable boundaries.
    -   The use of an iterative process to further improve the segmentation quality is also original.
-   Quality: This paper is well-motivated. The experiment design is sound. Ablation studies included in the experiments provide valuable insight to various modelling choices.
-   Clarity: The experiment results are reported in an easy-to-interpret manner.
-   Significance: The proposed model is an competitive speech language model with a lower inference computational cost.

[Peng et al., 2023]: https://arxiv.org/pdf/2305.11435 ""Syllable Discovery and Cross-Lingual Generalization in a Visually Grounded, Self-Supervised Speech Model""

Weaknesses:
I think the method and results in this paper would make a good paper for NeurIPS, however I cannot make a recommendation for acceptance because this paper needs substantial revision to improve its readability. A non-exhaustive list of issues making the paper hard to follow includes the following,
-   References to items not yet introduced
    -   Lines 153-154, the phrase ""our loss"" make it sound like a referrence to the masked language model loss discussed in the previous sub-section, whereas in fact it is referring to Equation (3), a yet-to-be-introduced loss for SylBoost.
    -   Lines 159-162 give a very vague description of the ""similarity matrix"" and the ""cut algorithm"" which can only be known if the reader has already seen the subsequent Section 3.3.
    -   Starting at line 188, Section 4.2 makes repeated references to ""mHuBERT"". ""mHuBERT"" appears to be name given to the acoustic tokens in ([Hassid et al., 2023]) by this paper (line 241). ([Hassid et al., 2023]) itself does not use this name, so an ordinary reader would not be able to tell what an ""mHuBERT"" model is when they work through Section 4.2.
-   Confusing terminology
    -   ""pretraining"": This paper makes a liberal use of the term ""pretraining"" to the point it's very difficult to tell which is the model being ""pretrained"". For example,
        -   Line 113 mentions a ""pretrained HuBERT teacher model"", then line 119 says ""during pretraining, the **student** model ..."". The teacher and the student are presumably not trained at the same time, yet the use of ""pretraining"" in this context make it appear that the contrary is happening.
        -   Line 225 says ""for all pretraining experiments"". A reader will have to look really closely to see this means ""training of the speech LM"", not ""pretraining HuBERT, etc"".
    -   ""Agglomeration"" vs ""SylBoost"": This paper appears to use these two terms interchangeably. Agglomerative clustering is apparently also used  (line 183). This makes it difficult for the reader to tell when ""agglomeration"" is mentioned, whether the authors intend to refer to SylBoost or just the clustering.
-   Confusing equation
    -   The unnumbered equation between line 126 and line 127 defines the similarity matrix from MLM probabilities. It makes reference to
$Y_t$ without specifying which $t \in M$ is used to define $C_{r,c}$. As a result, after having read the paper 6 times over, I still do not know how to compute $C_{r,c}$.
-   Writing style
    -   Overall the writing style of this paper is very wordy, inconcise and disorganized. Often the same message can get through with far shorter sentences. Most of the paragraphs read like a dump of the stream of consciousness of the author instead of a technical document intended for actual readers. For example,
        -   Lines 102-112 would be a lot easier to understand with formal notations and a concrete example.
        -   Lines 127-131 appear to be a mere repetition of the equation above, without any new information.
        -   Lines 242-255 contain a large amount of disorganized modelling details.

[Hassid et al., 2023]: https://proceedings.neurips.cc/paper_files/paper/2023/file/c859b99b5d717c9035e79d43dfd69435-Paper-Conference.pdf ""Textually Pretrained Speech Language Models""

Limitations:
The authors adequately addressed the limitations.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper first introduces an algorithm named LossPred that generates syllable-level speech segmentation without any training or supervision. The algorithm works by analyzing the prediction loss of speech tokens under different mask positions.

With the initial boundaries proposed by LossPred, the paper proposes further training a pretrained HuBERT / data2vec2 model by minimizing the sum of squared distances between feature vectors of each token and the average of feature vectors within the corresponding segment. This process is called SylBoost, and it further improves syllabic segmentation performance and efficiency.

Finally, the paper proposes training a Generative Spoken Language Model (GSLM) with the speech tokens obtained from quantized SylBoost units. Compared to existing GSLMs trained on other discrete representations, SylBoost encodes speech into much shorter sequences, significantly boosting training and inference efficiency.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The proposed speech representation learning and unit discovery algorithms, LossPred and SylBoost, are novel. While the idea of improving computational efficiency through dynamic or fixed-rate downsampling of speech representation is not new, this paper appears to be the first to successfully apply dynamic-rate downsampled representations with a very low sampling rate of 5Hz to Generative Spoken Language Models (GSLMs).
2. The presentation of the paper is of high quality and clarity. The authors report extensive experimental results, which effectively demonstrate that the proposed method outperforms various state-of-the-art (SotA) methods.
3. The topic addressed in this paper is significant, as very low sampling rate speech representations can benefit various tasks, including speech understanding and generation.

Weaknesses:
1. As pointed out by the authors, the proposed LossPred and SylBoost methods seem to be restricted to speech representation learning. It might be difficult to apply these methods to music, singing voice, speech with noisy backgrounds.
2. LossPred is slow in evaluating the loss prediction matrix. Each sentence requires about 200 Transformer network evaluations.
3. LossPred is highly heuristic. There seems to be no theoretical guarantee that the HuBERT model combined with LossPred reveals syllabic boundaries instead of revealing only phoneme or word boundaries.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies learning low bitrate speech units that preserves semantic information. As presented in the paper, the proposed approaches achieve SoTA performance on tasks like ASR and ZeroSpeech. The proposed approach also shows benefits in terms of compute resources — as claimed by the authors, 30x faster to train, and also benefits in terms of inference and transmission due to low bitrate.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall, the proposed multistage approach — first using the HuBERT like model to extract syllable-like noisy segmentation, then bootstrapping pseudo-syllabic units iteratively makes sense to me. The proposed approach also shows clear benefits in terms of performance and efficiency. 

Good performance: Compared to baseline approaches like SD-HuBERT, the proposed method achieved higher accuracy on syllable boundary detection and clustering, ASR, and also shows better continuation metrics as shown in Table 7 for generative spoken language modeling experiments. All those evaluations all positively demonstrate the strong associations with syllables of the generated speech units, while it does show lier-bitrate compared to the baselines compared in the paper.
The authors also conducted ablation studies to further demonstrate a couple design choices.



Efficiency: As claimed in the paper, the proposed technique is capable of achieving extremely low-bitrate compared to the counterpart speech units, while still being able to achieve good performance in a wide range of tasks, with the efficiency in both training and inference phases.

Weaknesses:
Demonstrating efficiency: As efficiency is also one selling point of the paper, it would be great if the authors can demonstrate the training efficiency and low-bitrate benefits in a more comprehensive way, like visualizing the GPU training time vs Performance, and also bitrate vs unit quality for certain tasks.



Limited use cases: The proposed approach focuses on learning semantic units for speech applications. It’s unclear if the proposed methods can be applied to other important non-speech use cases like understanding acoustic environment, and understanding speaker’s identity and emotion. 



Understanding Unit Quality: To demonstrate the unit quality for synthesizing the audio and for generation, should the author also compare with other related works (like [1] and [2]) in terms of reconstructing the original signal? Like in [1] (see Table 1), the authors compare the different approaches in terms of reconstruction performance using a couple of metrics like MEL, STFT and ViSQOL score, and also semantic task performance.

[1]: https://arxiv.org/abs/2405.00233
[2]: https://arxiv.org/abs/2306.06546

Limitations:
Not aware of potential negative societal impacts

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an approach for extracting syllable-like units from speech SSL models for use in a transformer-based language model. The motivation is that, compared to baseline acoustic units, which tend to mimic phonetic units in their time resolution, syllable-like units have lower time resolution, which makes them easier to model using techniques from the language domain. The authors propose an adaptation of the SD-HUBERT approach to extract units that can be used in Generative Spoken Language Modeling.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors identify an important limitation of why using language modeling techniques is a challenge in the speech domain, and their proposed approach seeks to address the limitation.

Weaknesses:
Overall, I found the submission difficult to follow. Please see my additional comments below.

line 2 -> Transformers do not require the inputs to be tokenized. The tokenization step is performed so that we can use language modeling techniques in speech.

line 18 -> Generally speaking, there is no requirement for the SSL representations to be powerful or abstract. 

Line 20 -> I don't see how the example of young children motivates your SSL description from the previous sentence; the transition is incoherent.

line 22 -> What does performant mean in this case? What is the connection between composing highly realistic text and the ability of a model to provide features for a downstream task? You seem to conflate the two goals, even though they are not necessarily the same.

line 23 -> The statement on this line is not clear. Several successful speech language model methods were introduced in the literature, what about previous approaches that make them fail? Please consider clarifying.

line 31 -> The temporal resolution impacts the LM part of the problem. Why is it important if we want to extract features for a downstream task?

line 37 -> What does ""syllable-like"" mean in this case? Can you elaborate on the time resolution it represents? Why is it important to start with a ""syllable-like"" unit? What makes it suitable for GSLM? What challenges from prior work are you addressing when using ""syllable-like"" units?

Line 38 -> I would refrain from using words like ""breakthrough"" and instead let the reader decide if the improvement is indeed a ""breakthrough.""

line 48 -> I disagree with labeling your method as ""train-free"" since it relies on a pre-trained HuBERT model. 

line 51 -> The distinction between the first and second contributions needs to be clarified. If the boundaries from the first contributions are not good on their own, then why mention them as a contribution? 

Line 102 -> It is not clear how/where you do the masking. Do you do it on the raw input, mel-spectrogram, or the extracted features? 

line 113 -> Shouldn't the approach be ""train-free""? Why do we have a student/teacher model that we are training?

line 147 -> The authors must refine the motivation for why syllabic units are useful for this application. Why not use word units instead? 

line 189 -> Superior compared to what?

line 198 -> I suggest leaving any experimental details to the experiments sections.

Table 1 -> Can you try any non-neural baselines for boundary detection? What would the performance be if we used heuristics based on energy, zero-crossing rate, or changes in Prosody to get rough boundaries?

Table 1 -> What makes Data2Vec2 better than HuBERT for extracting boundaries?

Table 1 -> What happens if you apply SylBoost to Feat-Sim?

Table 1 -> Please describe the metrics and abbreviations in the captions.

Table 2 -> What does the underline represent?

line 221 -> Implement what exactly? Please re-write the sentence.

line 237 -> typo

Table 3 -> What does the underline represent?

*Estimated.-> What does estimated mean? If prior work does not explicitly give this information, then it is better to leave it out.

line 263 -> What is the R score?

line 281 -> Please present the tables in the order they are referenced in the text; you currently jump from Table 1 to Table 4 and then go back to Tables 2 and 3.

line 340 -> Communication is not the last name of the first author from [16]

Limitations:
Can the authors comment on the trade-off between resolution and ease of modeling (and quality)? What do we lose/gain using syllable-like speech units in a language modeling paradigm?

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
J8m0DEjxEO;"REVIEW 
Summary:
The main claim of this paper is that adversarial suffixes against large language models (LLMs) function by distracting the model from the original harmful goal to the suffix itself. The authors then propose a modification to GCG attack by incorporating a regularization term that increases the attention score on the adversarial suffix.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
### 1. Originality and significance

The main claim of the paper is an interesting hypothesis that aims to unfold the inner workings of adversarial attacks on LLMs. This type of question can lead to a nice interpretability tool and/or a potential mitigation. Hence, the significance of this research question is clear. 

While some existing works start to look into “features” or neurons that correspond to these jailbreak attacks, the attention weights have not been deeply studied to the extent of my knowledge so it could be a nice complementary explanation.

### 2. Experiment coverage

The experiments on the attacks are relatively thorough. The authors compare their method against three existing SOTA attacks (GCG, AutoDAN, and ICA) on various open-source models. The transfer attack experiments in Section 3.4 also cover a broad range of closed-source models. The evaluation metrics are also comprehensive, including both the keyword matching and the GPT-4 evaluation.

Weaknesses:
### 1. Attention score measurement and interpretation

I first notice that in Figure 2, the attention scores on all parts (system, goal, suffix) can all go up as the optimization progresses and that the attention scores can be larger than 1 in Table 2 and 3. This suggests that the attention scores are before softmax and hence, not normalized to sum to 1. Please feel free to correct me if I’m mistaken.

1. If this is the case, it makes the score much more difficult to interpret and compare across different attacks. The absolute unnormalized value of the attention scores does not mean much because, for example, even if the score increases for the suffix portion, it may gets smaller relative to the other portions (system or goal). This is major flaw that undermines the main conclusion of the paper.
2. If that authors have not already done so, I would like to ask that all the reported attention scores be normalized (after softmax). The autoregressive generation also contributes to the attention scores, i.e., attention score of the target token $x_{n+2}$ also includes the target token $x_{t+1}$ along with all the prompt tokens $x_{1:n}$. I’m not sure what is the best way to normalize their effect. One way is to simply leave them out of the softmax, but there could be an interesting trend that we fail to capture this way. Another way is to report *difference* between average unnormalized attention score on the goal vs on the suffix portions. This also gives us a relative score but ignores the system portion.
3. In Figure 2 (left), ASR also increases along with the attention score on the goal, contradicting the main claim of the paper that higher attention score on the suffix is better.
4. It is unclear to me how Figure 5 supports the main claim of the paper. The attention pattern on ""Vanilla"" is strikingly similar to that of  ""ICA"" on the goal segment. Based on the color bar, the ICA attention score also seems higher than the Vanilla which contradicts the claim that the attack ""diverts the model’s attention away from the goal towards themselves.”

### 2. Section 3.3: Generalize AttnGCG to other attack methods

1. The purpose of this experiment is unclear to me. If the authors wish to prove their claim that higher attention weight on the suffix leads to a better attack, there should be a better controlled experiments than running GCG or AttnGCG on prompts generated by the other methods. This experiment entangles the initialization method with the attention score.
2. It might be interesting to see AttnGCG with varying values of $w_t$ and $w_a$.
3. I’d suggest an experiment where the attention loss is incorporated into AutoDAN (or other attacks) optimization objective. This would better emphasize the transferability and the usefulness of the attention loss across multiple attack algorithms.

### 3. Limited empirical improvement

While the main idea could help improve interpretability to these adversarial attacks, the attack that is inspired by this observation, AttnGCG, does not lead to significant improvement in the attack success rate, especially in the transfer setting. In the white-box setting, the improvement seems consistent across models, but the small margin suggests that attention score is not the most important factor that determines the success of the attack.

That said, it is sufficiently convincing to me that AttnGCG performs better than GCG and may replace it for evaluating the safety of LLMs.

Limitations:
Limitations and negative societal impact have been adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes a new adversarial attack strategy on LLMs which improves over existing adversarial attacks. For this the authors propose a new regularizer that maximizes the weight of attention corresponding to suffix tokens, which naturally results in minimizing the weight for the other tokens present in the input prompt. Using this additional regularizer with GCG results in improved attack success rate. The authors also show that this attack is transferable to other attack methods like ICA and AutoDAN.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) The paper is well motivated and the proposed loss follows well with the reader’s intuition.
2) The results are promising and the gains over the existing GCG attack are significant.
3) The comparison is comprehensive, involving different models.

Weaknesses:
1) It is not clear how the transferability of the same suffix tokens is for different goal prompts. This is important to investigate because GCG shows that the generated attacks are universal and can transfer on different goal prompts. I am currently a bit skeptical that the transferability on using the proposed attack might be limited because the generated suffix tokens might be more specialized for the given goal prompt. This is expected because now the generation of the suffix tokens is largely conditioned on the target target tokens due to the proposed regularizer. 

2) I believe it might be possible that using the proposed attack the model ends up outputting something potentially harmful but completely unrelated with the input prompt. This might be a possibility because the proposed approach inherently minimizes the attention on the goal tokens, which means the context of the input might become less relevant. It would be great if the authors could share some analysis on transferability of adv prompts and also share the generated text for GCG and AttnGCG.

3) It is not clear why maximizing the attention weights for suffix tokens should always lead to a stronger attack? This is also evident from tables 2 and 3 where AutoDAN has a lower value of goal attention score but stil leads to weaker attack as compared to GCG (see Table-4). Thus the argument presented in 162-163 seems questionable. 
In general, it is not clear why authors did not attempt to analyze the defenses like the ones proposed in [1]. Particularly, I believe it is important to analyze if the proposed attacks are able to bypass detection filters based on perplexity [1]. 

[1] Jain, Neel et al. “Baseline Defenses for Adversarial Attacks Against Aligned Language Models.” ArXiv abs/2309.00614 (2023)

Limitations:
Yes, the authors have addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a refined GCG method named AttnGCG for Large Language Model jailbreaking attacks. They focus on the attention scores of the input components, refining the loss function by adding an Attention Loss term. The attack success rates are greatly improved. Various experiments are provided to support the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1)	An interesting finding is that as the attention score on the adversarial suffix increases, the effectiveness in safeguarding LLM diminishes.

(2)	The experiments are conducted on various LLMs to prove the effectiveness of the AttnGCG.

Weaknesses:
(1)	It is unclear whether the increased success cases correspond to the 'regret' cases observed in GCG. The authors proposed AttnGCG to address the issue where the model successfully generates target tokens but then rejects the request; however, the results remain ambiguous.

(2)	In the success case illustrated in Figure 4, the attention scores at the boundary between the goal and the suffix are significantly higher than in other regions. Is this a common phenomenon in success cases? If so, why does this occur?

(3)	In Appendix A.3, the table shows that the system prompt for Llama-2 and Llama-3 is set to None, which is different from most jailbreaking papers, including the original GCG. How does this influence the attacking success rate? The authors should also report the success rate under the standard system prompt.

I will reconsider my score if all these problems are adequately addressed.

Limitations:
The authors adequately discussed the limitations of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new jailbreak attack method against LLMs, called AttnGCG. The method integrates a loss of maximizing the attention scores of the adversarial suffix. The paper provides experimental results to show the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written and easy to follow.

Weaknesses:
My main concerns are as follows.

- Will increasing the attention scores of adversarial suffixes make the responses focus on the content in adversarial suffixes?  
- The discussion in lines 151-164 is weak. Specifically, in Figure 4, AttnGCG explicitly increases the attention scores of adversarial suffixes, so it is natural to have higher adversarial suffix attention scores. It is not convincing to say ""uncover the underlying reasons for successful attacks within the model’s attention mechanism"".  
- In Table 3, AutoDAN achieves 0.227 goal attention score, while the scores of GCG and AttnGCG are 0.8657 and 0.793. Does the observation mean that AutoDAN is better than AttnGCG?
- Some content seems to be redundant, e.g., Figure 1 and Algorithm 1.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
6qJKrOulTr;"REVIEW 
Summary:
The authors propose a mathematically rigorous methodology based on Riemannian geometry for attributing network importance of tokens in a transformer models input space (e.g. image patches, or ~words in the textual domain). The proposed methodology—whilst based on sound theory—translates into an intuitive algorithm involving what appears to be a relatively inexpensive eigendecomposition. Experiments on 3 datasets across both the image and NLP domains explore how the features correlate with ground-truth inputs in the text domain, in addition to first steps towards exploring how the features affect the networks’ output logits.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- A major strength of the paper is the mathematically solid approach in attempting to identify regions of the **input** space that explain transformers’ model decisions. This is an important area of study: in contrast to many recent mechanistic interpretability methods finding latent network representations (that are intrinsically hard for humans to interpret by default), salient features in the pixel/text input space are much more readily interpreted by humans.
- Whilst I am unfamiliar with geometric deep learning, the authors do a fantastic job of presenting the technical content in a digestible manner without sacrificing depth or rigor.

Weaknesses:
# [W1] Feature importance comparisons

Feature importance-based explanations are motivated on [L303] as quantifying the contribution of features `""to a model prediction""`. More concretely, around [L180], the authors motivate the eigenvalues of the pullback metric found using their method as ultimately deducing the importance of each segment (e.g. image patch)  `“with respect to the the final prediction”`. 

Consequently, a major weakness of the paper is how there is no comparison with related work for how well the proposed method’s identified important features alter the **output** logits (e.g. upon ablation).

I am slightly confused by why the authors did not adopt the established “perturbation test” experimental protocol in the baseline [1] against which they compared, to provide experimental evidence in favor of this. Currently, the only comparisons made around [L274] measure the features’ importance as they correlate to the *input’s* labels.

Concretely, the authors could, for example, ablate particular patches of MNIST and observe that the resulting performance drops correlate with the pullback metric’s eigenvalues. This would provide stronger evidence of the authors’ claims about the features affecting the networks’ output, and (crucially) ground the results in contrast to those achievable by existing methods.

# [W2] Limited experimental results & improvements

There is a lot of interesting theory here, but ultimately this is a paper with a concrete applied goal of feature attribution in transformer models. With such a new methodology with many technical details, I believe there is an extra burden of proof on the authors to demonstrate this somehow leads to additional insights / practical gains. As such, it is a relative weakness of the paper that so few experiments are performed to justify the methodology.

Beyond toy datasets, it would be interesting to see how the method performs on more complex ones (not necessarily larger ones), such as TinyImageNET. Here, we could visualize much more easily if the method helps identify salient features of animals’ body parts (for example) as being important features for classification. MNIST experiments alone in the image domain are hard to interpret given the similarity of all the input data. 

Furthermore, the method provides an almost insignificant increase of just `0.07` cosine similarity (over the baseline in [1]), on just a single dataset (and with just two baselines—for example, how does GradCAM perform here?). This is not sufficient evidence to convince me as a reader that the proposed methodology should be adopted. 

---

- [1]: Chefer, Hila et al. “Generic Attention-model Explainability for Interpreting Bi-Modal and Encoder-Decoder Transformers.” 2021 IEEE/CVF International Conference on Computer Vision (ICCV) (2021): 387-396.

Limitations:
Some limitations are indeed addressed throughout. However, (unless I have missed something, in which case I apologise!) I can only find the limitations of the small number of datasets used stated in the NeurIPS checklist. This needs to be stated explicitly in the main paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work attempts to find the set of inputs that generate the same neural network predictions. To this end, the authors interpret the layers of the network as transformations of the input manifold. This interpretation is used to defined equivalence classes over the inputs and to define feature importance. Finally, the tools are used to identify equivalence classes for MNIST digits and for hate speech detection with BERT.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
Section 2 does a thorough job of introducing a manifold interpretation to neural networks.  This introduction is then used to motivate multiple algorithms for finding equivalence classes of the inputs---or the setup of inputs that result in the same prediction---and identify features that are important.

Weaknesses:
The main contribution of this work is to introduce a tool for analyzing which set of inputs produce the same output. However, this is exactly the Fisher information matrix (with respect to the inputs) and has also been introduced in prior work (https://arxiv.org/abs/2104.13289). Could the authors clarify what the differences are and what the additional novelty. If the ""local data matrix"" introduced in https://arxiv.org/abs/2104.13289 is identical to the tools in this work, I think it severely diminishes the contributions of this work. Furthermore the experiments are extremely similar (such as Figure 1). 

The second weakness is the limited number of experiments. The work does not show any quantitative results: Figure 1, Figure 2 and Figure 3 is just 1 example and is not indicative of why the tools are useful. The experiments in section 4 primarily discuss wall-clock time. It would significantly help if claims such as ""(Line 266) we notice that the perturbation-based algorithm ends up producing monochrome ..."" are substantiated quantitatively. Overall, the work doesn't provide novel tools and the experiments lack a novel usage of these tools and do not reveal any new insights.

Limitations:
The authors address limitations of their work but it can be expanded upon. For example, the authors can discuss the time required to compute Eigenvalues, and other limitations such as not having any Eigenvalues to be 0 in Algorithms 3 / 4. Furthermore, their algorithm should work (in theory) for infinitesimal steps in the input manifold.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a method for exploring equivalence classes in the input space of Transformer models using a solid mathematical theory. By analyzing the Jacobian of the model, the method reconstructs and navigates these classes, offering a powerful tool for understanding Transformer interpretations and enhancing explainability. The proposed method is expected to solve problems in Computer Vision and Natural Language Processing tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
I write both strength and weakness. 

First, I must disclose that I have no prior study or background in both Transformers and manifolds. While I am conceptually aware of them, my knowledge is limited to that extent, and I lack confidence in reviewing the technical details. Therefore, please consider my review comments as feedback from a layperson in this field, focusing on the overall mathematical consistency and readability of the paper.

This paper describes mathematics in a clear and understandable manner that even a layperson like myself can grasp. Each definition and theorem is stated accurately, and I believe that the general concepts can be understood with basic knowledge.

I personally feel that the objective of this paper is not clearly conveyed. While the paper claims to contribute to explainability and sensitivity analysis through the analysis of input manifolds, the logic behind this was not clear to me in the Introduction and Preliminaries. Although the concepts of explainability and sensitivity analysis become clearer in the later chapters, it might be beneficial to provide a bit more explanation in the Introduction.

Additionally, it might be helpful to clearly define the equivalence class mathematically.

Since I am not familiar with the existing literature, I was unable to judge the novelty of this work.

Weaknesses:
See above.

Limitations:
N/A.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper develops a novel theoretical framework grounded in Riemannian geometry for analyzing the input space of Transformer models, and introduce two algorithms, SiMEC and SiMExp, which facilitate the exploration and interpretation of equivalence classes within this input space. These methods offer new insights the internal mechanisms of Transformers, and provide new understanding of how these models perceive and process input data which can be very useful in the field of explainable AI.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1 novelty: This paper provide an innovative application of Riemannian geometry to analyze the input spaces of Transformer models, which is very novel in the area.

2 Theory: This paper establishes a solid mathematical theory on how Riemannian geometry is applied to Transformer models. Based on this theory, SiMEC and SiMExp are developed to explore the input spaces of Transformer models.

Weaknesses:
In experiment, the MNIST dataset is a little bit trivial, as the pixels of the background is essentially zero. It is nice to see the application of the proposed algorithm on natural images like CIFAR.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
XYDMAckWMa;"REVIEW 
Summary:
The paper proposes a loss for training flow matching (rectified flows, stochastic interoplants) that is based on integrating the target velocity over the available data as the regression target. This reduces the variance of the gradient estimator and can also be applied in the stochastic variant of flow matching.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
To the best of my knowledge, the idea is novel to use a larger batch of data for the velocity target in the flow matching objective than as the input for the network. This reduces the variance in the loss estimate.

The additional compute over the FM loss seems negligible, since only the target velocity field is adapted, but the network still receives batches. This makes it an attractive improvement to flow matching training.

Weaknesses:
I genuinely like the idea, but cannot recommend acceptance in the current presentation. I am happy to be corrected on any point and adjust my score.

Below I grouped the weaknesses by category.

## Theoretical contributions can be simplified and potentially reveal existing result (update: largely addressed, but presentation to be improved)

I think the theoretical derivation can be greatly simplified:

1. The notation is overly complicated. Why not use the standard notation for joint $p(a, b)$, conditional $p(a|b)$ and marginal $p(a)$ probabilities instead of index notation, where the index sometimes means ""joint"", ""conditional"" or ""marginal"", and sometimes indexes time $t$ or a condition such as $x_1$? (or $\\rho$ instead of $p$).
2. I think this reveals that the new loss is simply obtained by writing the target velocity in the ExFM loss as the expectation over the training data $\\rho_1$: Eqs (8, 10) say that the target velocity is given by $\\int w(t, x_1, x) \\rho(x|x_1, t) dx $, that is just average the velocities over the entire training data, weighted by the probability that the path actually  (where $x$ is sampled from the linear conditional paths). Given this observation, it seems to me that the actual contribution of the paper does not lie in this new loss, but how to efficiently estimate this integral, which is currently hidden in Appendix B.

In fact, I think that sections 2.1 and 2.2 can be merged to a simple importance sampling argument in the original flow matching argument.

I also think that the authors are missing that their third contribution has already been derived in the same form by their reference [10] in Eq. 4 and I think the second contribution is a simple extension to different conditional flow fields resulting from different inversions $\\varphi^{-1}$.

## Evaluation on tabular data is wrong (update: fixed)

The NLL defined in Appendix H.5.3 does not contain the volume change, which is an integral part of the negative log-likelihood. Did you use this equation for evaluation? My current score reflects the belief that volume change was accounted for.

Also, in Table 3, sometimes the highest values and sometimes the lowest values in each row are marked bold. Which model is better and does this use the incorrect formula? Please update without e-notation, adapting -1.29E+02 to -129, this is hard to read.

## Toy data evaluation (update: fixed)

It is easy to construct a very good approximation for the moons distribution by taking a Gaussian mixture of values for Table 4.

## Typos (has no influence on my recommendation)

Here is a list of what I found:

- l. 30: introduced -> introduce
- l. 33: base -> based
- l. 65: $rho$ -> $\\rho$
- l. 70: need -> needed
- l. 92/93: using map -> using the map
- l. 100: we return to end of the standard CFM loss representation -> ?
- l. 105: just (unknown) -> just the (unknown)
- eq. 7: the integral shares variables with the outside expression, e.g. add tilde on the integration variables
- l. 124: inevitable -?> invertible
- l. 126: have -> has
- eq. 16: consider moving numerical tricks like using softmax from the theory section of the paper, page 6 already introduces a lot of notation.
- throughout: dispersion -> variance

Limitations:
I do not think that the limitations of the work are properly addressed in the theoretic part, in contrary to the statement of the authors in the paper checklist. In particular, I did not fully understand how accurately Eq. (10) (which is part of the loss) can be estimated. One question that can be a way towards addressing this is by expanding on when the assumption in line 172/173 is valid (and why this Jacobian is even a problem).

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes an analytic formula for the vector field satisfying the continuity equation for the given change of density which interpolates between two distributions in the sample space. This is a common setting in the Flow Matching model [6] (Rectified Flows [1], Stochastic Interpolants). 

The authors apply the formula for several special cases like linear interpolation between samples and propose to use this formula for the training. They study the proposed training procedure empirically for synthetic examples and CIFAR-10 images.

[1] Liu, Xingchao, Chengyue Gong, and Qiang Liu. ""Flow straight and fast: Learning to generate and transfer data with rectified flow."" *arXiv preprint arXiv:2209.03003* (2022).

[6] Lipman, Yaron, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. ""Flow matching for generative modeling."" *arXiv preprint arXiv:2210.02747* (2022).

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The presentation of the proposed method is clear.

Weaknesses:
The proposed formula is already well-known in the community. It was proposed in [1] (see eq. 4, Def 3.1, Sec 5.1 for demonstration) and [2] (see Appendix D). In [3] (see eq. 4), the authors propose to use an analogous formula for training a generative model and conduct a much more exhaustive empirical study. Moreover, [4,5] develop different applied methods building upon this formula.

The downside of this formula is also very well-known in the community, i.e. one has to use large batch sizes to accurately estimate the vector field, which comes with a significant computational cost. Moreover, for any finite size, the estimation of the vector field is biased and the estimation of the loss is biased which would deteriorate generation quality when used to train large-scale models. This limitation is not adequately studied in the paper, e.g. the models are not compared in terms of the training time and memory used.

Given that the main contribution of the paper has already been proposed and the empirical study of this formula is unsatisfactory, I cannot recommend this paper for acceptance.

Minor comments:

- The authors refer to the original FM framework [6] as Conditional Flow Matching, which was proposed in [7].
- The objective proposed in Flow Matching is already tractable, so it is not clear what “a tractable form of the Flow Matching objective” means.
- There is a typo in line 65.
- The derivative notation used is inconsistent with its description in the text.
- From eq. 19, I assume that “dispersion” means “variance”.

[1] Liu, Xingchao, Chengyue Gong, and Qiang Liu. ""Flow straight and fast: Learning to generate and transfer data with rectified flow."" *arXiv preprint arXiv:2209.03003* (2022).

[2] Neklyudov, Kirill, Rob Brekelmans, Daniel Severo, and Alireza Makhzani. ""Action matching: Learning stochastic dynamics from samples."" In *International conference on machine learning*, pp. 25858-25889. PMLR, 2023.

[3] Xu, Yilun, Ziming Liu, Max Tegmark, and Tommi Jaakkola. ""Poisson flow generative models."" *Advances in Neural Information Processing Systems* 35 (2022): 16782-16795.

[4] Scarvelis, Christopher, Haitz Sáez de Ocáriz Borde, and Justin Solomon. ""Closed-form diffusion models."" *arXiv preprint arXiv:2310.12395* (2023).

[5] Xie, Tianyu, Yu Zhu, Longlin Yu, Tong Yang, Ziheng Cheng, Shiyue Zhang, Xiangyu Zhang, and Cheng Zhang. ""Reflected Flow Matching."" *arXiv preprint arXiv:2405.16577* (2024).

[6] Lipman, Yaron, Ricky TQ Chen, Heli Ben-Hamu, Maximilian Nickel, and Matt Le. ""Flow matching for generative modeling."" *arXiv preprint arXiv:2210.02747* (2022).

[7] Tong, Alexander, Nikolay Malkin, Guillaume Huguet, Yanlei Zhang, Jarrid Rector-Brooks, Kilian Fatras, Guy Wolf, and Yoshua Bengio. ""Improving and generalizing flow-based generative models with minibatch optimal transport."" *arXiv preprint arXiv:2302.00482* (2023).

Limitations:
The authors do not provide a necessary literature review nor study the limitations of the proposed approach (see Weaknesses section above).

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a novel approach to training flow-based generative model by deriving the conditional flow matching objective function with respect to the flow function. The author argues that this new method of training will reduce variance, add stability, and ultimately lead to faster convergence. Additionally, the reformulation allow derivation of the exact vector field expression, and in some simple cases, enables the computation of the oracle trajectory solution.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- If the derivations are correct, this methodology could potentially add some innovations in the field of flow matching.

Weaknesses:
- The paper is difficult to follow and lacks clear writing and organization. Specifically, the authors' use of notations is very confusing.
- The mathematical computations do not appear to be very rigorous and some assumptions seem very incorrect. I may have misunderstood some derivations, so please correct me if I'm wrong (see Questions section).
- The experimental results are not very robust or convincing. For instance, while the paper proposes that one of their contributions is the reduction of variance during training, many of the results demonstrate larger variance across numerous, different metrics.
- Overall, the paper feels like it requires substantial revisions and is far from being polished.
Typos:
- Line 65, rho_1$\rightarrow$ $\rho_1$.
- Line 130, practical $\rightarrow$ practical form.

Limitations:
The author has adequately addressed the limitations.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This works aims at producing a lower variance loss for flow matching. This is done by using the formula for the ground truth marginal velocity field, estimating it using self-normalized importance sampling, then regressing onto this estimated marginal velocity field.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Variance reduction for CFM is a useful avenue of research

Weaknesses:
- This paper lacks polish. I felt this paper is cumbersome to read, with a lot of heavy notation that can be drastically simplified, whereas the proposed algorithm is quite simple.
- The proposed approach is not exactly novel. The ""Explicit Flow Matching"" objective is just the original ""Flow Matching"" objective where we regress onto the optimal velocity field. The practical implementation being proposed is a simple application of self-normalized importance sampling.
- Importantly, the proposed approach leads to a biased objective (when estimated through a minibatch) where the optimum is not guaranteed to be the correct velocity field. This is not a problem for the low-dimensional experiments but importance sampling becomes more problematic in high dimensions.

Limitations:
The proposed ExFM objective is equivalent to the original FM objective, and the paper should not over-claim this contribution. This objective is intractable, so the use of smart estimation methods is interesting in its own right.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
cEb305kE1V;"REVIEW 
Summary:
The authors introduce the idea of implicit optimization, and coupled feature extraction for images, to achieve robust image registration. I liked the overall idea and was eager to gain insight into how implicit optimization

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
I really liked the overall ideas here. 

An implicit optimization layer does indeed address a DL shortcoming that hs been pointed out before (several DL methods show that some fine-tuning of the output deformation field will improve registration for many domains/moels). 

I like the idea of using this in combination with a feature extractor and using those learned features to drive the optimization, with potential benefits down the line. 

The paper is also pretty clear and concise, which I like.

There are extensive experiments, with measures of variance, which is a great start.

Weaknesses:
Unfortunately, I found the rest of the paper (beyond the core idea) lacking and having several weaknesses. 

Importantly, the authors mischaracterize important relevant literature and conceptual ideas, that I think are incorrectly described in the motivation, and not compared to properly in the experiments. This makes it challenging to assess and gain insights from the contribution.

Incorrect characterization: the authors say a few statements that to me seem directly false (and are important to the paper). For example, on line 50 they say ""Moreover, design decisions like sparse keypoint learning for affine registration [103, …] do not facilitate dense deformable registration"" -- they repeat this in several parts of the paper (e.g. line 107). This is wrong -- their first citation, for example, 103, uses keypoints for *deformable* registration (along with affine). A few lines lower they say ""Current DLIR methods are not robust to minor domain shift like varying anisotropy and voxel resolutions, different image acquisition and preprocessing protocols [62, 53, 70, 43]."" -- which is incorrect and not supported by the citations. First, citations 62, 53, 70 are from before 2012 and do not discuss DLIR methods at all (but just general registration), whereas citation 43 *is explicitly tackling and achieves robustness to domain shift*. It may well be that the authors' method does better (I am not sure, see below), but the claim is incorrect. Many other papers tackle distribution shift in DL registration -- see Mok et al, 2023. Another crucial omission is related to the authors' claim that DL methods may not output local minima results -- which is true, but plenty of works propose to take the output of neural networks and perform a bit of instance-specific optimization of the resulting field to get to that local minima -- essentially 'fine-tuning' the field for a couple of seconds (e.g. VoxeMorph TMI 2019, but plenty of other methods as well after that for example from Matthias Heinrich's group). This is crucial to the current paper, since the proposed method essentially does the same thing at inference -- runs a forward neural network (albeit just as a feature extractor), and then performs an optimization for the image pair -- and while it's done differently (and more elegant in some sense) than the existing literature, these approaches are very related. Overall, I was excited about the method but overall found the motivation/related-works either misleading or lacking rigor -- perhaps the authors are simply not aware of the abilities of existing literature mentioned above, but this does  limit the novelty and insight substantially

In the experiments, it seems to me that some obvious results are missing:
. I am not sure why methods used in Figure 3 (e.g. [43]) are missing from Table 1. It seems like a crucial comparison. Deformable KeyMorph [103] is missing from the whole experiments section, and is close to the existing method in that it separates the feature extraction (there via keypoints, but using a parallel net) from the optimization. Training keymorph on oasis and testing it seems like an important comparison if we are to extract some insights into how to decisions in the proposed method (the feature extractor and the implicit optimization) improve our insights in the field.
. Overall the results in Table 1 do not seem impressive -- comparable at best with existing methods. This is totally okay in my book, if the authors are able to communicate other interesting insights. Unfortunately, I do not believe this is the case.
. In the domain-shift section the authors show that their method tens to outperform the DL methods. However, their method gets the benefit of doing instance-specific optimization (the proposed layer) after feature extraction, at the cost of some GPU work for each pair. This is what instance-specific optimization does at the end of DL methods (as discussed below), which was employed in several papers, but this is not included in the comparison! This is a peculiar omission to me -- it should be included for completeness, but importantly it is also crucial to understand whether the proposed method behaves differently -- perhaps there is some advantage, in several situations, to the proposed method, or perhaps it offers more guarantees, etc -- we simply don't know. Minor: it would also be interesting to understand what are the limits of this domain shift of this model -- does it generalize to more substantial variations in modality, or 7mm slice spacing found in clinical sequences?
. I also find the claim that DL methods do not work without crops peculiar - most DL methods are convolutional and hence not size specific, and some (e.g. SynthMorph, which the authors refer to here) does not even require both images be of the same size.
. Since one of the contributions of the paper is the parallel feature extraction (to be used with the optimizer), it seems to me that it would be an important ablation to take the features of some robust method (does not have to be registration, even a domain-shift-robust segmentation network will do, or a 'robust foundation model', etc) and see if that can be combined with an optimizer. This would help provide insights if the formulation of the proposed model and the end-to-end training is useful. 
. The claim in 4.4 is also missing some reaosnable comparison -- would it not be possible to take the displacement field of any other DL method, initialize your favorite parametrization (freeform, diffeomorphic, etc) with that, and run the (any) optimizer? It seems like this is easily doable and a reasonable comparison?

Limitations:
Yes the paper has a limitations section. It would be nice if the authors could comment on how this method can be used on CPUs -- by far the standard hardware available to non-ML users (neuroscientists, clinicians, etc).

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel image registration framework that aims to bridge the gap between classical and learning-based approaches. It incorporates fidelity optimization directly into the neural network as a layer. The framework employs end-to-end implicit differentiation through an iterative optimization solver, ensuring that the features learned are both registration and label-aware. Additionally, the warp functions derived are guaranteed to represent local minima of the registration objective within the feature space. The authors report that this framework performs exceptionally well on in-domain datasets and remains robust against domain shifts, such as anisotropy and variations in intensity profiles. Furthermore, the framework is designed to allow seamless switching between different transformation representations at test time without the need for retraining.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper's motivation is both clear and innovative, effectively merging classical optimization techniques with neural networks by embedding an optimization layer within the network. This enhances data consistency and steers the optimization toward local minima.
2. A significant technical achievement of this paper is the backpropagation of gradients through the optimization layer using the implicit function theorem. This demonstrates the paper's technical depth and is the key contribution of the paper.
3. The analysis of loss landscapes provided in the paper is insightful. The flattening of the feature space by neural networks introduces a wider range of possible gradient directions, which, when combined with fidelity loss, enhances overall performance.

Weaknesses:
1. A major limitation is the framework's registration accuracy, as measured by the Dice score, which does not demonstrate clear advantages over neural-network-only methods. For instance, in Table 1 on the OASIS dataset, the proposed LKU-Net variant achieves a Dice score similar to that of TransMorph-Large. This calls into question the practical benefit of integrating classical optimization into the network.

2. The absence of specific smoothness measurements in comparison with other methods, such as the percentage of negative Jacobian determinants (||J||<0) or the standard deviation of the logarithm of the Jacobian determinant, is a significant oversight. Without these metrics, it's unclear whether the observed increase in Dice score represents a genuine improvement or merely a trade-off with the deformation field's smoothness.

3. The paper lacks clarity in the reproducibility of results compared to other methods. For example, the learn2reg OASIS leaderboard indicates that LKU-Net can achieve a Dice score of 88.5 on the OASIS dataset without explicitly optimizing for smoothness. Similarly, TransMorph scores 88.5. Additionally, in Table 4, the performance of LKU-Net with Dice supervision is paradoxically worse than without, which is counterintuitive and raises concerns about the implementation fidelity and methodological consistency.

4. While the introduction of an optimization layer with a fidelity function is a key contribution, the paper falls short in comparing its method to other registration methods that also combine learning with optimization. This lack of comparative analysis leaves unanswered questions regarding the true effectiveness and novelty of the proposed method compared to existing approaches.

Limitations:
The paper commendably integrates a fidelity function with an optimization layer into a neural network, but there are several limitations:
1. The optimization layer has not demonstrated significant performance improvements compared to traditional neural network methods on OASIS leaderboard, and the implementation issues in the other datasets/methods raises concerns about the completeness of the result. Clarifying scenarios where this integration is advantageous could enhance the paper's impact.
2. The absence of quantitative smoothness metrics for registration performance is a critical gap. Introducing these metrics would strengthen the comparative analysis with existing methods.
3. The paper lacks a comprehensive comparison with major competitors combines optimization with learning. Expanding this analysis would provide clearer insights into the framework's unique contributions.
4. Concerns about implementation and reproducibility persist.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduced DIO, a differentiable implicit optimization layer to a registration network that aimed to bridge the gap of classical-learning-based image registration, considering the incorporation of weak supervision like anatomical landmarks into the learned features. The authors decoupled feature learning and optimization and trained a deep network to predict multi-scale dense features registered through a black box iterative optimization solver.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**S1.** The proposed model significantly improves over SOTA models on domain shift experiments.

**S2.** The paper is well-written

**S3.** Multi-scale optimization seems an interesting approach that might be a possible module that can be integrated in deformable image registration.

Weaknesses:
**W1. Possibilities of getting artifacts from different voxel sizes.** How do different voxel sizes ensure that the velocity or transformation field is differentiable or invertible? I believe this approach might introduce artifacts and lose fine details when propagating the source image to match the target image. And how processing the image features independently will preserve the diffeomorphism property in generating the transformation field? Can the image matching term efficiently capture the intensity differences likewise treating the input images as pairwise? Overall, treating the input images separately from the feature extractor raises several questions regarding the credibility of the transformation field $\phi^*$. 

**W2. Motivation for using multi-scale optimization.** I found the motivation of using multi-scale is somewhat underdeveloped. What is the rationale behind using such kind of optimization considering different source/target image features? 


**W3. Applicabilities of learned multi-scale dense features from sparse images.** In Sec. 4 the authors tried to show that DIO learned interpretable dense features and compared to the classical methods DIO preserved the gradient in the loss function. On the other hand, the authors also discussed that deep networks recovered affine transform with $~90$% overlap. I wonder what is the advantage of capturing multi-scale dense features compared to existing DLIR methods such as VoxelMorph, TransMorph, etc. 

**W4. Experimental supports.** Though the authors are getting comparable performance in image registration (Tab. 1), they are achieving improved results in testing out of domain/distribution datasets. However, the authors might want to show their model's performance without adapting their proposed multi-scale optimization. Basically, is the optimization scheme or the multi-scale features helping the complete registration model in achieving those bits of improvements? and the important question is why? Two interesting sets of experiments that validate the domain shift hypothesis can be the following - 
*(i) train on some of the other datasets (excluding OASIS) and test on the rest,* and 
*(ii) train on multiple datasets, including OASIS, and test on the rest.* 


Overall, I appreciate the authors for working in the domain of image registration which is very relevant as well as important in the medical imaging domain. However, the current version of the manuscript lacks some important experimental justification and further experiments. With that being said, the current version of the manuscript is under the threshold of acceptance. However, I am open to reconsidering the initial rating if the above concerns are adequately justified.

Limitations:
Limitations have been discussed in the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
Oo7dlLgqQX;"REVIEW 
Summary:
This paper conducts experiments to verify the alignment between human and LLM responses to the ACS survey. Particularly, the paper questions existing literature suggesting that LLMs can be used as proxies for measuring responses to survey questions, suggesting instead that LLM choices are biased by the ordering of the questions, and when order choice is randomized, models tend to present uniformly random survey responses, thereby closely modeling the behavioural characteristics of sub-groups whose aggregate statistics are close to a gaussian. The paper suggests that using LLMs as human-proxies for multiple choice surveys is a questionable strategy.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors test 42 different LLMs in their experiment, testing base, instruction-tuned, and RLHF-tuned models. This is comprehensive and substantive, and a lot of work. The results they find agree across model size and type barring one outlier in instruction-tuned models over one survey.
2. The authors test the use of randomized choice order and the original choice order, finding that randomizing the response order results in a uniform distribution of responses.
3. The authors investigate the effect of using instruction-tuning to train models.
4. The authors also test surveys besides the ACS, and find that the results persist.
5. The authors interpret findings in earlier papers and provide explanations for why the LLM responses more closely resembles responses from certain demographic sub-groups, i.e that these distributions are Gaussian
6. The paper is well-written and presents a simple and elegant experiment, and clear and consistent takeaways.

Weaknesses:
1. Order choice bias is a well-known phenomenon in LLMs (Lu, Yao, et al. ""Fantastically ordered prompts and where to find them: Overcoming few-shot prompt order sensitivity."" arXiv preprint arXiv:2104.08786 (2021)). Therefore the big finding in this paper is that LLM responses model a Gaussian when order choices are randomized using a gaussian distribution.
2. Please use a different color for the sub-group dots in fig. 5 -- it is confusing because the same color is used for survey responses in earlier figures.
3. Takeaways are harder to gauge from Fig. 3. Please use simpler aggregate statistics like means, confidence intervals, variance and save this figure for the appendix.
4. Please plot the log linear scale as a dashed line in Fig. 2 for easy comparison.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on evaluating 42 different language models on the American Community Survey, highlighting how model responses are governed by ordering and label biases and in general the fact that any demographic correlation with specific subgroups is actually due to the fact that those subgroups aggregated statistics are closest to uniform.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The study in the paper is well-designed and discussed, considering a very large variety of LLMs and in particular by considering both base and instruct-tuned models. The presented results are very relevant for any study that plans to either examine LLMs for understanding underlying characteristics and, most importantly, for researchers who plan to use LLMs as a proxy to study human sub-groups. The strong A-bias showed in the paper is a very important take-away.

Weaknesses:
There are two main weaknesses that I have encountered while reading the paper, namely:
1) It is not clear what the authors would expect the models should do based on their training data. It is true that we don't know exactly on what these models have been trained, but if we consider for instance the Common Crawl as the main corpus, would we expect that models trained on it would somehow produce answers correlating with certain sub-groups? I think a larger discussion in this paper on what we should reasonably expect models answers to be is needed.

2) given the emphasis on sub-groups, I would have expected the authors to explore impersonation of LLMs as a way of seeing whether that would steer responses to surveys in the direction of what we would expect those sub-groups typical responses. This would clarify whether LLMs have the ability of producing answers relevant to specific sub-groups, if instructed to do so.

Limitations:
I think authors have addressed the main limitation of this work (which would be the focus on US survey) by examining other surveys.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper prompts LLMs with 25 multiple choice questions (on basic demographic information, education attainment, healthcare coverage, disability status, family status, veteran status, employment status, and income) from the 2019 ACS. The authors use eight kinds of prompts which vary in additional context, instructions, and asking in the second person. However, each time, the next-token probabilities are used to determine the immediate reply by the LLM to the multiple choice question. To evaluate the responses and the differences between LLM and human generated responses, the authors compute the normalized entropy and use KL divergence. They find that ""smaller"" LLMs are vulnerable to ordering and labeling biases, and that after correcting for these through randomized answer ordering, the LLMs trend towards uniform distributions in their responses (~high entropy). Instruction-tuning seems to increase the variance in the entropy measure for LLMs, but nonetheless the entropy remains higher overall compared to the human generated responses. The authors state that the main takeaway from their paper questions the popular methodology of using survey responses from LLMs using multiple choice questions. They challenge prior work and give the explanation that models consistently appear to better represent subgroups whose aggregate statistics are closest to uniform.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper has a clear goal to challenge previous work on survey-derived alignement measures by offering the explanation that models consistently appear to better represent subgroups whose aggregate statistics are closest to uniform.
- The paper shows that LLMs should not be used out-of-the-box to replace human responses in census data.
- The paper is generally well-written and clear which makes it easy to follow.

Weaknesses:
While I find the paper enjoyable to read and it challenges important earlier findings on LLMs, I am not convinced that the current experiment fully supports the claims:
- The 2019 ACS uses stratified sampling. Therefore, I believe that the variance is already being increases to obtain a representative sample. Since the additional context given to the LLM is very limited + the draws are independent, it seems like a very hard task to generate a matching distribution by the LLM. This is less important when assessing, for example, the political view of an LLM.
- First-token probabilities may be a biased measure to obtain the replies (see Wang et al., 2024).
- Additionally to the first-token probabilities, more advanced prompting techniques, such as Chain-of-Thought (Wei et al., 2022a), could improve the coherence and dependencies in the responses, especially for the sequential generation.
- I believe that the sensitivity to ordering and labeling biases is known (Wei et al. (2022b) and Wei et al. (2023)).

Overall, the authors show that independent draws from an LLM with limited context generates a uniform distribution, which questions earlier findings made used by such as methodology. While I agree with the authors on that statement, I believe that their experiment adds little value to further support their claim on ""better represent subgroups whose aggregate statistics are closest to uniform."" I believe additional, more fine-grained analysis is required for this.

References
-------------
Wei, J., Wang, X., Schuurmans, D., Bosma, M., Xia, F., Chi, E., ... & Zhou, D. (2022a). Chain-of-thought prompting elicits reasoning in large language models. Advances in neural information processing systems, 35, 24824-24837.

Wei, J., Tay, Y., Bommasani, R., Raffel, C., Zoph, B., Borgeaud, S., ... & Fedus, W. (2022b). Emergent abilities of large language models. arXiv preprint arXiv:2206.07682.

Wei, J., Wei, J., Tay, Y., Tran, D., Webson, A., Lu, Y., ... & Ma, T. (2023). Larger language models do in-context learning differently. arXiv preprint arXiv:2303.03846.

Wang, X., Ma, B., Hu, C., Weber-Genzel, L., Röttger, P., Kreuter, F., ... & Plank, B. (2024). ""My Answer is C"": First-Token Probabilities Do Not Match Text Answers in Instruction-Tuned Language Models. arXiv preprint arXiv:2402.14499.

Limitations:
The paper does not explicitly state the limitations of the experiments but carefully reassesses its findings in the conclusion. The checklist points to Section 2 where, for example, the authors point to the prompt ablations.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper critically examines possible pitfalls of using the responses of LLMs to survey queries to study the model alignment. They found substantial bias, e.g., with respect to the order of response option,

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper examines a very important methodological topic that has gained significant attention also beyond computer science: measuring the values and opinions, in which LLMs are rooted. As such, the topic is very relevant, interesting, timely, and certainly fitting for the conference.

The paper is well written and well motivated. The provided material (i.e., code and documentation) are exemplary. Results are presented in a clear and concise way.

Weaknesses:
The paper is motivated by surveys on the ""demographics, political opinions, or values best represented by current language model"". For this, the paper mostly relies on the ACS dataset. However, this questionnaire mostly covers demographic information, for which naturally the LLM cannot have a ""correct"" answer. It would be criticial to see for which type of question the high entropy responses actually hold. From my own experience, I would not expect at all that similar uniform distribution also would occur for (political) opinion or value-based questions.

Minor note: I would recommend to add ""forward mentioning"" in Section 2 that other datasets will also be covered in Section 5

Although a lot of LLMs have been included in the study, only the large-scale models of OpenAI have been studied. To see if the observations hold also for other, similary large models, including also other commercial providers (i.e., Anthropic or Google) would be nice. This is not required for the key results of the paper, however.

Limitations:
The limitations are mostly described well in the paper. For an exception, see weaknesses.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
476zUsqFZB;"REVIEW 
Summary:
This paper attempts to address the problem of low interpretability of reaction prediction methods by proposing modeling step-wise polar reactions. To model such mechanisms it uses an existing dataset PMechDB. The authors propose an approach to model such reaction by first selecting the right atoms to react from the input molecules using learned models and then react them.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
- Several existing chemistry reaction prediction models are benchmarked on the PMechDB dataset.
 - A way to integrate reaction mechanism information is introduced.

Weaknesses:
- The paper has substantial clarity problems: 
  - Table captions are insufficiently informative, requires going deeper into the text to understand what results are actually presented (e.g. 'Table 3: Top-N Accuracy of Trained Models').
  - Figures 5 and 6 are formatted inconsistently with the rest of the file.
- Citation quality is poor:
  - Could provide more references to prior work overall. e.g. section 3.3 describes prior work on sequence to sequence modelling without any references. 
  - PMechDB is introduced in a way that makes it unclear, whether the database is a contribution of this work or not.
- Novelty is not prominent. Method in [18] (OrbChain) is already working with similar task on a similar dataset.
- Evaluation is insufficient:
  - Source code for reproduction has not been provided.
  - The resulting models have not been evaluated on the global datasets, making it unclear whether the fine tuning as specified in this work improves the performance in general rather than on the test set of PMechDB.
  - Error bars are not provided.
  - Not benchmarked against a comparable method, referenced in [18].

Limitations:
The authors address some of the limitations of their work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Current reaction prediction models lack interpretability for chemical reaction prediction. This paper evaluates the various machine learning models on the PMechDB dataset which contains polar elementary steps. Besides, this paper proposes a new system: PMechRP, which achieves the highest top-5 accuracy.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Strengths:  
1. A new benchmark has been introduced, which improves the interpretability and causality of a chemical reaction.

2. Several methods are evaluated.

Weaknesses:
Weaknesses:

1. This paper seems like a technique report.

2. The main conference track is not suitable for this paper. I think the dataset & benchmark track is more suitable.

3. Writing is poor.

Limitations:
N/A

Rating:
1: Very Strong Reject: For instance, a paper with incorrect statements, improper (e.g., offensive) language, unaddressed ethical considerations, incorrect results and/or flawed methodology (e.g., training using a test set).

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
Previous reaction prediction models formulate the forward chemical reactions in an end-to-end manner, which only considers the input state and output state while ignoring intermediate states describing the electron redistribution changes. This work tries different models on a new benchmark dataset PMechDB. Experimental results demonstrate the effectiveness of the transition state information in the new benchmark dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The motivation is quite clear. This reviewer agrees with the importance of the exploration of intermediate electron transfer. This is particularly important for the chemical reaction simulation, benefitting the understanding of reaction mechanisms.

Weaknesses:
(1) The technical contribution of this work is very limited. This reviewer does not see enough improvements from the algorithm side. Also, it seems the dataset is not proposed by this work. The contribution of this work is overall limited.

(2) If this work intends to propose a new benchmark, then much more comprehensive reaction models should be covered. Currently, two important reaction models are not discussed: ""non-autoregressive electron redistribution modeling for reaction predictions"" and ""A Generative Model For Electron Paths."" In addition, the evaluation metric and the new task are not clearly described. More detailed descriptions should be provided for clarity.

(3) The presentation of this work is not very clear. This reviewer does not fully understand how the multi-step information helps the reaction modeling. A good example illustrating the significance of the intermediate step information is required. At this stage, this reviewer thinks the multi-step transition information can be easily captured by recursive modeling of single-step reaction models. Currently, this reviewer does not see what new challenges are brought by the intermediate step.

(4) This work is very similar to the published paper ""AI for Interpretable Chemistry: Predicting Radical Mechanistic Pathways via Contrastive Learning."" This reviewer does not see many differences between the submitted work and this prior work.

Limitations:
Constructing the benchmark dataset with ground-truth multi-step electron transition states is very hard. This may hinder the further development of this direction.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
the paper describes a new approach to predict polar reaction mechanisms, which is the most important class of chemical reaction mechanisms. this can be quite useful for chemical reaction prediction.


this reviewers rating is based on the current presentation of the manuscript, if the authors are willing to enhance the clarity of the manuscript, this reviewer is willing to increase their score.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Addressing an underexplored but important problem
- decent results
- interesting results with pre-trained methods, that in some cases are surprising (T5 seem to work not as well despite multi-task pretraining)

Weaknesses:
- Model and data processing descriptions are quite short and should be expanded, and presented coherently in one location in the manuscript. From the description in the manuscript I would likely not be able to re-implement the method
- It is not immediately clear which ensemble is shown in table 4
- maybe not so much innovation from the ML side?

Limitations:
ok

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
uNZpvFlsg9;"REVIEW 
Summary:
The paper introduces a novel unsupervised evaluation method for large language models (LLMs): it uses a peer-review mechanism of a models' anonymized answers by other models. The approach assigns a (learnable) capability parameter to each LLM and solves a constrained optimization problem to maximize the consistency between capabilities and scores. The end result is a ranking of the evaluated models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper introduce a novel approach to an important practical problem: ranking the quality of the ever-growing number of open- and closed- source LLM available to the public. The paper is reasonably easy to follow, and the empirical results appear to be sound.

Weaknesses:
The paper could be further improved on several directions:
1) you should dedicate a full section to the iterative elimination of models; what is the benefit of eliminating the weaker ones rather than keeping them around? how di you come up with the threshold of 60% to remove? can you learn this threshold automatically? is this threshold optimal for these 15 models? what happens if you start with, say, 100 models? what happens to your results (and the curves in Fig 5) if you stop earlier (all three metrics, not just CIN)? What if you continue to eliminate all models until you are left with one? is there any relationship between the order in which the models are eliminated and their final rank?
2) are there any scaling issues for 100, 1K, 10K, or 100K models? how about cost: is it cheaper to fine-tune a ""baseline"" model than to pick the best one out of 10K candidates?   
3) while the three metrics you use are meaningful, you should also present results for Precision@1 and -say- RBP@3; after all, we care a lot about identifying the the top models
3) Fig 5 should be extended to nine graphs (3 metrics * 3 datasets); for each of the 9 graphs, you should also show illustrative three ranked lists: PiCO's, PRE's, and the target one. As always, the devil is in the details: not all ""CIN = 1"" are created equal. Performance-wise, it is almost irrelevant if you have the bottom-2 models inverted; no necessarily so if the inversion is between the top-2 models

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies how to estimate LLMs' performance ranking without human preference annotations. In particular, it proposes to leverage three metrics (PEN, CIN, LIS) to evaluate the estimation quality, gives an estimation mechanism that first asks a list of LLMs (called ""reviewers"") to rank pairwise answers to user questions independently, and then aggregates their ranking via a weighted sum approach. A consistency optimization determines the weights of each reviewer.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The problem of LLM evaluation without human annotations is critical in resource-limited applications. The most important and interesting contribution of this paper, in my opinion, is proposing the problem of estimating the performance rank of LLMs instead of any metric of an individual LLM. The paper also reveals an interesting assumption that better reviewers are expected to be better answer generators, which leads to their consistency optimization approach. Overall, the paper is well-written and easy to follow.

Weaknesses:
While I find the proposed problem interesting, there are still a few limitations, unfortunately.

***Unclear implication of ground truth ranking***: The technical part of the paper starts by introducing a ground truth ranking (equation (1)) without giving its physical meaning. It simply assumes ""[...] alignment with human preferences"", but it is not clear what human preferences mean in this context. 

***Evaluation metric is strange***: One of my major concerns is on the choices of evaluation metric. All the three proposed metrics, PIN, CIN, LIS, in the authors' own words, seem originally used for time series comparison. However, the goal here is to compare rankings, not time series. Thus, it is unclear why we should not use the standard ranking comparison metrics, e.g., Spearman's rank correlation coefficient or Kendall rank correlation coefficient.

***Consistency optimization algorithm is not provided***: The core of the proposed ranking estimation method is the optimization problem (7). It does not seem to be a standard optimization problem, but I could not find (even a discussion on) any clue on how to solve it in this paper.

***An optimal solution to the consistency optimization formulation can be useless***: I find the following optimal solution to the problem (7): just set weight w to be 0 for all LLMs. It is an optimal solution as G and w are identical and thus the objective is always maximized.  However, this solution is undesired. I probably misunderstood something, but this seems to suggest the formulation is incorrect. 

***Consistency optimization formulation seems brittle to query distribution biases***: Another problem with the formulation is that it seems brittle to data distribution bias. E.g., suppose M1 is indeed better than M2 for some query q. And let us replicate many copies of q in the dataset D. Then the grade G1 can be arbitrarily large. In other words, the grade of an LLM is proportional to the number of battles involving it in the dataset D, which should not be the case.

***Choices of LLMs for evaluation***: In line 547, the authors write ""For our analysis, we meticulously selected 15 LLMs"". What is the principle of the meticulous selection? Other than open-source and close-source, the selection is quite arbitrary. For example, I am quite surprised to not see GPT-4 and Claude included in the reviewer LLMs.

***Comparison with a simple baseline***: One simple baseline is to ask a powerful LLM(e.g., GPT-4, Cluade-3) to give a preference for each answer question pair, and then take the vote to determine the ranking. I would suggest to compare the proposed method with this simple baseline.

Limitations:
No. The limitations are not well discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors propose a more reliable evaluation system to rank the abilities of different large language models (LLMs). Previous evaluation methods typically suffer from two main drawbacks: (1) benchmark data leakage and (2) cost-intensive and potentially biased human evaluations. To address these issues, the authors introduce an unsupervised evaluation mechanism to measure the abilities of LLMs and derive their rankings. The core idea of this mechanism is to first collect the answers from each LLM, then treat each LLM as a 'reviewer' to rate the quality of the other LLMs' answers, and finally optimize the internal agreement of the reviews among all LLMs. They also conduct experiments on three datasets to validate the effectiveness of their proposed mechanism.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. Unsupervised Evaluation Method: The paper introduces PiCO (Peer Review in LLMs based on Consistency Optimization), a new unsupervised evaluation method that leverages peer-review mechanisms to measure the capabilities of LLMs automatically, particularly without human-annotated data. The unsupervised nature also makes it scalable and less subjectively biased.
2. Consistency Optimization Framework: The proposed approach includes a constrained optimization method based on the consistency assumption, which helps in re-ranking LLMs to align more closely with human preferences.
3. New Evaluation Metrics: The paper proposes three new metrics—Permutation Entropy (PEN), Count Inversions (CIN), and Longest Increasing Subsequence (LIS)—to evaluate the alignment of LLM rankings with human preferences. These metrics can further inspire future work.

Weaknesses:
1. Reliance on Consistency Assumption: The effectiveness of the method relies on the consistency assumption that higher-level LLMs can more accurately evaluate others. However, a natural concern is, ""Does this assumption always hold true in practice?"" I suggest the authors further discuss the applicability of their method.
2. Complexity of Implementation: The framework involves a complex review process, which requires substantial computational resources to support the LLMs' inference. Can you provide some details on the number of tokens consumed and a comparison of consumption with baseline methods?
3. Consideration of Multi-Agent Methods: Since the proposed method employs multiple LLMs, I think more recent and advanced evaluation methods based on multi-agent systems should also be considered in the experiments, such as AgentVerse.
4. Details of ELO: The ELO system is essential for the proposed mechanism. However, it is only mentioned in the appendix. I suggest the authors add more details about the background of ELO and how it is adopted in the proposed mechanism.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The highlight of the work is the proposed method of evaluating Large Language Models without relying on human feedback.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The proposed evaluation method is a novel attempt of automating the LLM improvement process. Such method worth further exploration. It could be adapted to many of the LLMs and potentially bring us more insights.
- By eliminating the involvement of human, the proposed evaluation method limits the bias brought by human labelers. The observations presented are also interesting as LLMs can sometimes surprise us.
- Great presentation and visualization.

Weaknesses:
Some of the equations and notations in the paper seems unnecessarily complicated, which can be reorganized when polishing.

Limitations:
The idea is straightforward and make sense to me. But LLMs can be trained to bypass such systems, which may lead to potential fairness or security problems.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
D80tRH9CXF;"REVIEW 
Summary:
The paper examines prediction and estimation risk of ridgeless least squares estimator in the setting of a general error structure. The iid assumption on the error structure is often not valid in settings such as time series data , panel data, grouped data etc. The current paper introduces a theoretical framework which investigates the variance component estimation of both prediction and estimation risks in the above mentioned data settings. The benefits of overparametrization which has been seen in iid context has been shown to exist in the dependent error structure context as well.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Following are the strengths of the paper-

- Investigation of prediction and estimation risk under non i.i.d. regressor errors with specific focus on time series and cluster data

- Explicit quantification of the variance component of both the risks (as mentioned above) which depends on the trace of the error covariance matrix and the trace of a function of design matrix as a separable product.

- Explicit analysis of the variance and bias term of both the risks (as mentioned above) in the high-dimensional asymptotics

- Well constructed numerical experiments to support the theory

Weaknesses:
Following are the weaknesses of the paper

- The theoretical results particularly the bias component analysis section could have been more rigorous and better written. There are some notational discrepancies and theoretical inconsistencies.

- Some remarks following theorem 3.4 and 3.5 where the design matrix $X$ has a known distribution say Gaussian would have been useful  examples to get insight on the results proved in the theorems

- Some notations such as $a(X)$ and $b$ used in theorem 3.4 have been clarified later in the appendix. It would be better to introduce them in the sketch of the proof if you are using them anyway there.

Limitations:
The authors have adequately addressed the limitations of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper explores the prediction risk and estimation risk of the ridgeless least squares estimator under more general assumptions on regression errors. It highlights the benefits of overparameterization in a realistic setting that allows for clustered or serial dependence. The paper establishes that the estimation difficulties associated with the variance components of both risks can be summarized through the trace of the variance-covariance matrix of the regression errors. The findings suggest that the benefits of overparameterization can extend to time series, panel, and grouped data. The paper is a theoretical work that discusses various aspects of linear regression models, providing details on the assumptions and proofs for the theoretical results presented. It also includes information on the experimental setting and provides code and instructions for reproducing the main results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This study addresses an important research gap by considering more realistic assumptions on regression errors. It provides exact finite-sample characterizations of the variance components of prediction and estimation risks, includes numerical experiments that validate the theoretical results, and demonstrates the relationship between the expected variance and the covariance of the regression errors. Additionally, it analyzes the bias components of prediction and estimation risks, offers a comprehensive overview of linear regression models covering various theoretical aspects, and provides detailed proofs for the theoretical results, ensuring the validity of their claims.

Weaknesses:
Is it possible to provide validation on large-scale data?

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates the properties of minimum norm (ridgeless) interpolation least squares estimators, analyzing prediction risk and estimation risk under broader regression error assumptions, including clustered or serial dependence. This diverges from the typical assumption of i.i.d. errors with zero mean and common variance. The paper shows that the challenges in estimating the variance components of prediction and estimation risks can be captured by the trace of the variance-covariance matrix of the regression errors.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper provides a more general theoretical analysis of minimum norm interpolation least squares estimators, going beyond the restrictive i.i.d. error assumption.

2. The paper suggests that the benefits of overparameterization can extend to a wider range of regression settings, including time series, panel, and grouped data.

Weaknesses:
While the paper examines broader error structures, it might not fully grasp the complexity of real-world regression challenges, which could involve even more intricate patterns of error dependence.

Limitations:
The authors have addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the ridgeless least-squares estimator, and derives its prediction and estimation risk. One of the assumptions used is that the expectation of the noise variance matrix is finite and positive-definite. This is more general than the assumption that this expectation is some positive multiple of the identity matrix.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper has an easy-to-follow introduction that motivates the need to derive theoretical results under general assumptions on regression errors.
- Related works are sufficiently discussed. The most relevant papers are those of Chinot et al. [9] and Chinot and Lerasle [8], which are based on different noise assumptions that this paper makes.
- The technical presentation is clear with examples and figures to help the reader understand the notations and results.

Weaknesses:
The major concern I have is whether the paper makes sufficient technical contributions. Even with the more general assumption on noise (Assumption 2.1), the technical change in the proofs seems very small compared to prior work. For example, the proof of Theorem 3.4 is short and relatively straightforward (and this might further simplify if we make Gaussian assumptions on data rather than left-spherical assumptions. Gaussian assumptions are what I like to make personally). It is always nice to have short and concise proofs whenever possible, but this might also indicate that the paper is not very technically solid.

Limitations:
N.A.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
B1tCaKP5nB;"REVIEW 
Summary:
The authors proposes a method for testing conditional independence in presence of discretisation.
They assume the variables to be jointly Gaussian, and that some of them are accessible only after discretisation; thus the data contain a mix of discrete and continuous variables.
Discretization might remove some conditional independencies. Assume X1 to be independent from X2 given X3.
It might be that X1 becomes instead dependent on X2 given \tilde{X3}, where \tilde{X3} is the discretised  X3.
The authors develop a way to infer the latent correlation on the real value variables and they propose a novel test for conditional independence for  the setting mixed continuous and discrete variables.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Testing conditional independence with mixed types of variables is an interesting topic and the work is original. 
The presentation is good, even though I could not follow the development of the bridge equations (this might be because I am not familiar with the adopted techniques).

Weaknesses:
* I am skeptical about the specific research question addressed.
X1 and X3 are independent given X2; yet they might be not independent given the discretised version of X2.

For instance, with ref to  Fig 1a,   X1 and X3 are formally dependent given \tilde{X2}; yet the induced dependence might be very weak.
I argue that the strength of the induced dependence depends on how the discretisation is done.
Example: X2 is human height, discretized into bins of few centimeters; then \tilde{X2}  is practically as informative as X2 and the induced dependence is likely to be negligible, in which case it might be sensible not rejecting H0.

The author did not discuss the impact  of the adopted discretization approach on the induced dependence.
Also, there is no compelling example in which discretisation induces a strong dependence.


* In the first set of experiments the test is better calibrated than the competitors, but it has by far less power. Overall, these results are not very strong.


* The competitor tests (Z-test and chi-square) are not modern.
There is no comparison against  existing tests for mixed variables; I can cite for instance Bayesian Independence Test with Mixed-type Variables, Benavoli et al. 2021.
Another simple baseline which I think should be present: test conditional independence having discretized all variables and use modern test for discrete variables (as a starting point, I suggest  those available in bnlearn https://www.bnlearn.com/documentation/man/conditional.independence.tests.html )

Limitations:
No potential negative societal impact.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Authors propose a test for conditional independence in case of discretized variables, i.e. variables that originally were defined over a continuous domain and are then mapped to a discrete domain. In this case, a binary domain. Authors propose to bridge the unobserved continuous variables with the observed discretized variables with equations modeling the original covariance/precision matrix coefficients. Both theoretical and experimental evidence support the proposed testing  methods. 

Typo at page 13, line 491, equation 17, missing closed bracket.

Citation 3, 4 are the same reference.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The paper contributes in a significant way on a topic which is crucial in the context of structure learning/causal discovery. Specifically, the proposed theoretical framework is solid and sound, explaining the logical steps that lead to the conditional independence test. The major strength points of this contribution are:

- The self-contained graphical representation of the discretized variables and their original continuous ones,
- The flexibility of the bridge equations, that can be adapted to specific cases without compromising the theoretical soundness.
- The performance of both the unconditional and the conditional independence test.

Weaknesses:
The only weakness is that I would do more experiments.

Limitations:
The only limitation, that is also discussed by authors, is that the ""discretized"" variables are in fact ""binary"" variables, which limits the applicability of the proposed test.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a novel statistical method for testing conditional independence (CI) when some of the data is discretized. Initially, the authors introduce bridge equations to estimate covariance and establish asymptotic normality, facilitating an unconditional independence test. For the conditional independence test, they employ nodewise regression to recover precision coefficients. Theoretical analysis and empirical validation are provided to showcase the method’s effectiveness.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper introduces a conditional independence test tailored for scenarios with discretized data, which often encountered in financial analysis and healthcare due to data collection or measurement constraints. The CI test is highly adaptable, capable of handling situations where both variables are discretized, both are continuous, or one is discretized. Numerical experiments on both synthetic and real-world datasets demonstrate superior performance in various scenarios.

Weaknesses:
The development relies on the assumption of a multivariate Gaussian distribution, which is rather stringent.

Limitations:
The development relies on the assumption of a multivariate Gaussian distribution, which is rather stringent.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses a critical issue in Conditional Independence (CI) testing methods, specifically when the available data is a discretized version of the original continuous data. Traditional CI testing methods often assume that discretized observations can directly substitute for continuous variables, leading to erroneous conclusions. To overcome this limitation, the authors introduce a novel CI test tailored for discretized data. The key innovation lies in using a bridge equation and nodewise regression to estimate the precision coefficients that reflect CI relationships among latent continuous variables.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper tackles a highly relevant and important problem within the realm of statistical analysis and CI testing.
The paper is well-written and presents the concepts clearly.
The proposed method is novel, and the theoretical contributions are solid, providing a robust foundation for CI testing in discretized data settings.

Weaknesses:
Assumption of Multivariate Normality: A primary limitation is the assumption that the data follow a multivariate normal distribution. This assumption simplifies the derivation of bridge equations for unconditional independence testing and the use of nodewise regression for the CI test. However, it restricts the applicability of the method to this specific class of variables. It is unclear how the method would perform with unknown or non-normal variables.

Discretization Modeling: The paper models discretization as a binarization operation applied to observed variables. This assumption may not hold in all practical scenarios. The performance of the proposed method on datasets with different types of discretization (beyond binarization) remains unexamined and is an important consideration for real-world applications.

Empirical Results: According to the empirical results, the proposed Discretized CI Test (DCT) shows smaller power compared to baseline methods. This indicates that while the method is innovative, its practical effectiveness in terms of power may be limited in some scenarios.

Limitations:
See above comments.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
4VL5QWQFBV;"REVIEW 
Summary:
The current watermarks applied to AI-generated images rely on adding additional information, limiting their ability to detect unauthorized use of data. This paper introduces a new implicit watermarking scheme, which first utilizes the disentangled style domain to detect unauthorized dataset usage in text-to-image models., so as to achieve self-generalization and mutual exclusivity within the style domain anchored by protected units. In addition, this paper introduces the concept of watermarking distribution and establishes a verification mechanism for copyright ownership of hybrid or partial infringements. It is worth noting that this paper implements One-Sample-Verification for dataset copyright verification in AI mimic generation. This paper also designed experiments to verify the robustness, generalization and ablation of multiple data sets.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The problem exploited in this paper is important and needs to be addressed.
- The idea of disentangling the image into content and style seems effective.

Weaknesses:
- This paper is really hard to follow. The method's modules are not explained very well, and the input, output and training details of each module are not clear.
- Too much mathematical description makes it difficult for readers to quickly understand the process and principle of the method, and the meaning of each mathematical symbol is not clearly described.
- The model structure diagram is less relevant to subsequent descriptions. The paper also lacks simple examples of the data used in the experiment.
-Only the watermark distribution concept is proposed, and the copyright ownership verification experiments of hybrid or partial infringements are lacking.

Limitations:
The authors addressed the limitations of this paper, which focuses only on the disentangled style domains of protected units, potentially making it difficult to resist attacks that modify deep features of style domains. Future research could incorporate specific adversarial samples.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces an implicit watermarking scheme that leverages disentangled style domains to detect unauthorized dataset usage in text-to-image models. The proposed method aims to address limitations in traditional watermarking techniques by using implicit z-watermarks for dataset copyright verification, achieving better robustness against various attacks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed framework is able to protect the dataset copyright. This is extremely important in an era of generative AI. Besides, rather than directly protecting the image, this work proposes to protect the styles. Such a thinking may bring some new insights into this area.

Weaknesses:
1. The writing of this paper has some problems. The description of its first section is totally in a chaos status. 
2. In some specific situations, I agree that the styles are necessary to be protected. However, in most situations, copyright law typically protects original works of authorship. An individual style, which may consist of a particular technique or aesthetic, is not considered a tangible, original creation. Styles are more akin to ideas or methods, which are generally not protected under copyright law. If the styles are not protected by the laws, why do we need such a method mentioned in this submission? 
3. Defining and enforcing copyrights for individual styles would be highly subjective and impractical. Styles evolve and are influenced by many sources, making it difficult to establish clear boundaries for what constitutes a protected style. 
4. Granting copyright protection to individual styles could stifle creativity and innovation. Artists and creators often build upon existing styles and techniques to develop new works. Restricting the use of styles could hinder the creative process and limit artistic freedom. For example, if the Cubist style were copyrighted, it could prevent new artists from experimenting with and developing this style further, thereby limiting artistic progress.

Limitations:
I am uncertain about how practical it is to implement this method on a large scale. The experiments provided in this paper only cover limited data.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces an innovative implicit $z$-watermarking scheme using disentangled style domains to protect dataset copyrights in text-to-image models. It achieves structured delineation of copyright boundaries, self-generalization, mutual exclusivity, and effective verification for hybrid or partial infringements. The method demonstrates high robustness and reliability against various challenges, marking a significant advancement in protecting copyrighted content in AI-generated visual data.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. This paper designs a novel implicit $z$-watermarking scheme via disentangled style and content domains to protect dataset copyrights. Meanwhile,  instead of embedding invisible information into images, the proposed self-generalization module and mutual exclusivity module are used to explore the style boundaries.

2. This paper is very detailed and well-formulated, with precise wording, clear definitions, and easy to understand.

3. Extensive experiments demonstrate the SOTA performance of the proposed method, such as DCT-DWT-SVD, RivaGan, and SSL. Obviously, the proposed $z$-watermarking is hard to be removed by some illegal mimic models.

Weaknesses:
1. In table I, it is suggested to compare with some recent and SOTA watermarking methods, such as Trustmark and RoSteALS.

[1] TrustMark: Universal Watermarking for Arbitrary Resolution Images.

[2] Rosteals: Robust steganography using autoencoder latent space. In CVPRW 2023.

2. Can $z$-watermarking resist some watermark removal or attack methods, such as DDIM inversion or VAE. The authors could provide some results to improve the completeness of the experiment.

Limitations:
The author has clearly presented its limitations.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
While text-to-image models excel in generating high-quality images, they also raise issues of unauthorized dataset copyright protection. This paper proposes a novel implicit watermarking scheme that detects and protects dataset copyrights by disentangling the style domain  to generate watermarks. The proposed method achieves One-Sample Verification, significantly improving existing copyright protection mechanisms.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The authors are the first to utilize the disentangled style domain to detect unauthorized dataset usage in text-to-image models, and they have effectively implemented a method called z-watermarking to enhance copyright protection.

2. The authors conducted comparative experiments with state-of-the-art protection methods and robust experiments under various conditions. They also performed comprehensive ablation studies, demonstrating the effectiveness and robustness of their method. The ablation studies verify the importance and effectiveness of each module proposed by the authors.

Weaknesses:
1. This paper involves many complex modules, and in Section 3 (Method), the authors use many symbols and subscripts. However, the annotations for these symbols and subscripts are not very clear. For instance, in line 144, $\mathcal{E}_z(z_x|(x, ϕ), z) = s$. It is not clear what the input to the style domain encoder is. If ‘|’ denotes a probabilistic condition, then the style domain encoder only has one input, but Figure 2 appears to show two inputs, which is confusing. For the numerous symbols and labels, it is recommended that the authors provide a unified introduction in each section.

2. The paper introduces the operational flow of the model in three parts. However, the authors seem to focus more on explaining each module rather than the connections between modules and the overall operational flow. As a result, after reading these parts, it remains difficult to grasp the overall process of the proposed method. The paper appears technically sound, but there is still significant room for improvement in introducing the technical flow.

Limitations:
The authors have addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
4FwlejUlg5;"REVIEW 
Summary:
This paper investigates the problem of latent post-treatment bias in causal models where there exists some proxy variables of the latent confounder and post-treatment variables. The authors first derive a general form of latent post-treatment bias which is intractable in most situations (except in special cases such as linear SCM). The authors state that the latent post-treatment bias can be arbitrarily bad for existing proxy-based causal inference methods. They then propose an identifiable VAE-based causal inference algorithm under the assumption that at least one dimension of each sufficient statistic of the latent prior is invertible. The proposed method is evaluated on both synthetic and real-world datasets to demonstrate its causal effect estimation capability with the presence of both latent confounders and post-treatment variables.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
• Causal reasoning in the context of latent confounder and post-treatment variables is an important topic especially with observational data.

• The authors clearly state the necessary assumptions for the identifiability of true latent variables, and the logic of determining the dimensions of $\boldsymbol{C}$ and $\boldsymbol{M}$ is well presented.

• The paper has a well-established theoretical basis.

Weaknesses:
•	For the illustrative example in the introduction, it might be better to explicitly specify what the post-treatment variable is.

•	Other existing works [1-3] on identifying latent confounder/mediators based on the iVAE architecture should also be included in the related work.

•	The role of post-treatment variables $\boldsymbol{M}$ seems to be a bit ambiguous. To be specific, is Theorem 4.1 valid for all types of relationships between $\boldsymbol{M}$ and $Y$?

•	The illustration of (iv) in Assumption 3 is a little confusing, as it assumes one extra degree of freedom on the prior parameters of $\boldsymbol{Z}$ and is critical to the identifiability of $\boldsymbol{Z}$ from $\boldsymbol{X}$. More explanation on this point will be appreciated.

•	The empirical evaluation consists of only one real-world dataset, which somehow limits the applicability of the proposed method.

References:

[1]. Zhou, D., & Wei, X. X. (2020). Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE. Advances in Neural Information Processing Systems, 33, 7234-7247.

[2]. Sorrenson, P., Rother, C., & Köthe, U. (2020). Disentanglement by nonlinear ica with general incompressible-flow networks (gin). arXiv preprint arXiv:2001.04872.

[3]. Jiang, Z., Liu, Y., Klein, M. H., Aloui, A., Ren, Y., Li, K., ... & Carlson, D. (2023). Causal Mediation Analysis with Multi-dimensional and Indirectly Observed Mediators. arXiv preprint arXiv:2306.07918.

Limitations:
The authors do not include a paragraph discussing the limitations and potential societal impact of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors deal with latent post-treatment bias for proxy-based methods which are employed for causal effect estimation.
They show that post-treatment variables can be latent and mixed into the observed covariates along with the latent confounders.
The authors transform the confounder-identifiability problem into a tractable pair-wise conditional independence test problem.
They prove that the latent confounders and latent post-treatment variables can be identified up to bijective transformations. Finally, they provide experimental analysis for their approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper deals with a very interesting problem.	 The proposed method appeared to be theoretically robust. The method is evaluated with proper experimental analysis on synthetic and real-world datasets and compared with multiple benchmarks.

Weaknesses:
Here I provide some weaknesses of the paper:
* Bi-directed edges in Figure 1 are not defined properly.
* Do-operator in equation 3 is not defined in detail.
* Assumptions in Assumption 2 should be described in more detail.
* The proposed method seems to depend on a lot of assumptions. Assumptions 1,2,3 each contain multiple assumptions. The authors should explain how their assumptions hold for the real-world scenarios they considered in their experiment section.

Limitations:
The authors discussed a very few limitations of their paper but more discussion should be done.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of causal inference with observational data, particularly when direct measurement of confounders is infeasible. The authors propose a new method, Confounder-identifiable Variational Autoencoder (CiVAE), to mitigate post-treatment bias using observed proxies for both latent confounders and latent post-treatment variables. The paper provides a theoretical analysis under specific assumptions and validates the proposed approach through experiments on both simulated and real-world datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The paper investigates a critical question concerning the mitigation of post-treatment bias, which is essential in various practical scenarios.
* The ideas presented in the paper are clear and easy to follow, and the theoretical analysis is well-established.

Weaknesses:
* In practical scenarios, interactions among latent factors are often present and can significantly impact the estimation. It would be beneficial if the authors could elaborate on how their method addresses these interactions and whether there are any theoretical guarantees regarding their handling in the proposed approach.

* The theoretical guarantees rely on strong assumptions, and the assumptions are hard to verify in practice. In assumption 1, the paper assumes an injective function of latent confounders and latent post-treatment variables into the observed proxy. This is a strong assumption,  and it will be much harder to meet the assumption in general when the function is nonlinear. The specific setup with strong assumptions limits the practical applicability of the proposed approach. It would be helpful if the authors could provide examples where these assumptions hold and demonstrate how they can be verified.

* The experiment lacks sufficient details on setup and implementation. Could the authors provide more specific information to enhance understanding of the empirical results?

Limitations:
* The proposed method relies on very strong assumptions to ensure identifiability, which can be challenging to verify in practical applications.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors investigated the issue of latent post-treatment bias in causal inference from observational data. They showed that estimator of existing proxy-of-confounder-based methd, i.e., DEV (f(X)), is an arbitrarily biased estimator of the Average Treatment Effect (ATE), when the selected proxy of confounders X accidentally mixes in latent post-treatment variables (Theorem 3.2). To address this issue, they proposed the Confounder-identifiable VAE (CiVAE), which identifies latent confounders up to bijective transformations under a mild assumption regarding the prior of latent factors. They showed that controlling for latent confounders inferred by CiVAE can provide an unbiased estimation of the ATE. Experiments on both simulated and real-world datasets demonstrate that CiVAE exhibits superior robustness to latent post-treatment bias compared to state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Being able to recover latent variables (cofounders, post-treatment variables, or others) from observations is challenging and important. Ignoring latent variables or assuming non-existence of latent variables is unrealistic and can lead to the wrong conclusion and decisions. The authors further motivated the importance of recovering latent cofounders, post-treatment variables and the consequence of not doing so  (Theorem 3.2). The solution provided shows originality and quality.

Weaknesses:
The presentation can be improved.

Limitations:
n.a.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
tAOg1HdvGy;"REVIEW 
Summary:
The paper addresses the challenge of balancing multiple stakeholder interests in online recommendation systems, which include platforms, items (sellers), and users (customers). To tackle this challenge, the authors introduce a novel fair recommendation framework, FAIR, formulated as a constrained optimization problem to balance these competing interests. They further explore this problem in a dynamic online setting, introducing FORM, a low-regret algorithm that simultaneously learns user preferences and enforces fairness in real-time recommendations. Through theoretical analysis and a real-world case study, the paper demonstrates that FORM can maintain platform revenue while achieving fairness for both items and users, contributing to more sustainable and inclusive digital platforms.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper tackles the practically relevant and challenging problem of balancing platform revenue, user fairness, and item fairness


The paper proposes an online algorithm to learn parameters of the problem while ensuring some given fairness constraints, not merely assuming that the parameters are given as done in most existing papers


The paper provides sound and thorough regret analysis of the proposed algorithm 


The empirical results on real-world data shows that the proposed method effectively and flexibly control user-item fairness while maximizing the platform revenue

Weaknesses:
This is not a weakness specific to this paper, but choosing the right fairness notion and the values of $\delta$ is non-trivial


Even though the paper performs offline experiments on public real-world datasets, it fails to perform any online production A/B test; so it might be risky to be overly optimistic about the empirical results of the paper until the proposed approach is verified in an online environment and provides some tangible business benefits

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper focuses on an important and interesting research direction: achieving multi-sided fairness for recommendation. Specifically, the authors aim to answer two research questions: (1) What constitutes a fair recommendation within a multi-sided platform? and (2) How would a platform implement a fair recommendation in a practical online setting? The contributions of this work include a novel multi-sided perspective that achieves within-group and across-group fairness among users/items, and also an online recommendation algorithm for multi-sided fairness. Empirical and theoretical results show the advantage of the authors' contributions over prior baselines, especially in online settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The motivation of this paper is clear, and it is well-written. The theoretical proof appears solid.
2. Prior works on fairness-aware recommendation generally focus on either a single side or an offline setting. This work combines both aspects and proposes a novel framework and algorithm, which is useful in real-world settings.
3. The authors compared their method with some well-studied offline and online fairness-aware recommendation methods, demonstrating that their proposed algorithm achieves a better tradeoff between fairness and revenue.

Weaknesses:
1. After doing a quick search on ""multisided fairness + recommendation"", it seems some related works [1, 2, 3] are missing. Even though the setting might be different, the authors should mention and discuss them somewhere in the paper since the topics seem related.
2. An efficiency comparison on the algorithm running time is missing.

Ref:

[1] Naghiaei et al. CPFair: Personalized Consumer and Producer Fairness Re-ranking for Recommender Systems, SIGIR, 2022.

[2] Wu et al. Joint Multisided Exposure Fairness for Recommendation. SIGIR, 2022.

[3] Wu et al. A multi-objective optimization framework for multi-stakeholder fairness-aware recommendation. ACM Transactions on Information Systems, 2022.

Limitations:
1. In the checklist, the authors claim that ""We have clearly stated the assumptions needed for our theoretical results and the setups adopted for our numerical experiments"" and ""There is no negative societal impact of the work performed"". However, these claims are too strong and I actually do not think the limitations and negative societal impact of this work are clearly or comprehensively discussed. For instance, one of the important challenges in recommendation is the ""cold-start"" problem, the author did not discuss whether their algorithm is capable of applying in this scenario, or it is also some limitation and future work. Also, one potential negative societal impact to me is the leakage of user/item sensitive information as the algorithm needs to use the ""type"" information. This is also not discussed or mentioned.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a novel fairness recommendation framework called FAIR, and an online algorithm called FORM for solving multi-stakeholder fairness problems. The main contributions include 1. The FAIR framework is proposed to balance the platform revenue and the fairness of multi-stakeholders (items and users) through the form of constrained optimization problems. The framework can flexibly define fair solutions for items/users and adjust the trade-off between platform goals and stakeholder fairness.2. A FORM algorithm is designed for simultaneous learning and fair recommendation in online settings. The algorithm balances learning and fairness by relaxing the fairness constraints and introducing random exploration.3. Theoretical analyses demonstrate that the FORM algorithm achieves sub-linear regret in terms of both gain and fairness.4. A case study on Amazon review data verifies the effectiveness of the proposed method. The paper solves the fairness problem in multi-party recommender systems, and the proposed method can flexibly weigh the interests of multiple parties while maintaining the platform's revenue and demonstrates good performance in both theory and experiment. This is an important research direction, which is significant for the fairness of practical recommender systems.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The needs of multiple stakeholders (users, items) are considered simultaneously and Fairness parameters can be adjusted as needed.
2. Theoretical bounds on algorithmic performance are effectively proven.
3. It has a degree of practicality, with case studies demonstrating application to real data.

Weaknesses:
1. The computational complexity of the algorithm, especially its scalability in large-scale systems, is not discussed in detail in the paper.
2. The choice of δ_I and δ_U may require a lot of experiments to find suitable values.
3. The model assumes that the user type is known, which may not always hold in practical applications.
4. the paper mainly focuses on short-term fairness and does not delve into the impact of long-term fairness.
5. this paper mainly uses gain and fairness regret as evaluation metrics and more metrics may be needed to fully evaluate system performance.

Limitations:
please refer to ""weakness""

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a fair recommendation framework, FAIR, for balancing the interests of multiple stakeholders in recommendation systems, namely the platform, sellers, and users. The framework is formulated as a constrained optimization problem that addresses fairness concerns for both items and users in a dynamic online setting. The paper proposes a no-regret algorithm, FORM, which ensures real-time learning and fair recommendations. The effectiveness of the framework and the algorithm is demonstrated through theoretical analysis and a case studies on real-world data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper addresses the often-overlooked complexity of balancing the interests of multiple stakeholders in optimizing recommendation systems. The proposed FAIR framework offers a solution by ensuring fairness for both items and users while maintaining platform revenue. The extension to a dynamic online setting where data uncertainty is present is reasonable. The FORM algorithm's ability to learn and adapt in real-time is a merit for practical applications in environments where user behavior and preferences are nonstationary. This work also provides a robust theoretical foundation for the proposed framework and algorithm, including proofs of sub-linear regret bounds.

Weaknesses:
While the paper presents promising results on a dataset with 30 items and 5 user types, it does not thoroughly address how the framework and algorithm would scale to much larger datasets with a large set of items and more diverse user profiles. My main concern is the proposed solution's scalability, a critical factor for real-world deployment but the proposed solution involves solving a constrained optimization problem at every time step, which could be complex and computationally intensive. This complexity might limit the feasibility of implementation for platforms with limited computational resources. I urge the author to include some discussion about the computational complexity of the proposed solution and also the limitations regarding the scalability.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
znBiAp5ISn;"REVIEW 
Summary:
There's a large performance gap for graph tasks, especially graph classification tasks, between the spiking neural networks and artificial neural networks. The authors proposes the problems as the neuron's under starvation and illustrated the reason of the problem. To solve the problem, TAS-GNN was proposed.

The main contributions of the paper are as follows:
1: Starvation problem of spiking neurone in GNNs in graph classification tasks are identified.

2: A strategy was proposed to address the spike frequency deviations on the basis of the correlation between graph topology and spike frequency patterns.

The authors conduct experiments on 5 popular datasets and use several different designs of GNN layer. The results show competitive potential of the TAS-GNN.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1:This is a well-written paper, from the formulation of the problem to the solution. The author's motivation for the use of graph topology is clear.

2:The method of using topology-awaregroup-adaptive neurons shows competitive results compared with other baselines. The ablation study makes the result more persuasive. 

3: The Figures in the paper are quite straightforward, easy to follow.

Weaknesses:
1: The name of the paper is ""Topology-Aware Spiking Graph Neural Networks"". However, as I can tell the only graph topology used in the method is nodes degree, which is used to group the neurons. I wonder if it is appropriate to name it as ""topology aware"", or the author can explain it more.

2: The analysis regarding the performance of the method is lack of discussion. For instance, in some datasets, such as MUTAG and IMDB-Binary, the proposed method achieve quite competitive results while in PROTEINS it doesn't. It's betted to explain what cause the phenomenon, like the characteristics of the datasets? Also, in table 2, the results of GAT and GAT+TAG in IMDB-Binary are the same. It's better to make an explanation about them.

3: There're several typos and basic grammar mistakes in the paper that will affect the presentation of the paper. In line 120 "" and apply is to""; The sentence in line 123 is hard to understand

Limitations:
na

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper primarily discusses integrating Spiking Neural Networks (SNNs) into Graph Neural Networks (GNNs) to address several key challenges in graph classification tasks. Specifically, the paper proposes a new method called TAS-GNN (Topology-Aware Spiking Graph Neural Networks) which leverages the topology of graphs to improve the performance of spiking neural networks in graph classification tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
（1）The authors clearly articulate the performance gap between existing Graph Neural Networks (GNNs) and Spiking Neural Networks (SNNs) in graph classification tasks.
（2）The authors conduct an in-depth analysis of the performance degradation of spiking neural networks in graph classification tasks and introduce the ""neuron starvation"" problem.
（3）The authors propose topology-aware group-adaptive neurons (TAG) based on the graph's topology, a novel approach that helps address the neuron starvation issue.
（4）The authors provide a detailed description of how to convert input graphs into spike representations, perform message passing, and classify the graphs.
（5）The authors validate the method's generalizability and effectiveness by using multiple public datasets (such as MUTAG, PROTEINS, ENZYMES, NCI1, IMDB-BINARY) in the experimental section.

Weaknesses:
（1）The authors mention several application areas and challenges, but the references and comparisons to existing literature are not sufficiently comprehensive.
（2）Although the methodology section describes the main steps, it lacks detailed descriptions of some key aspects such as threshold initialization and the specific training process.
（3）Although there are some ablation studies, the analysis of the individual contributions of each component is insufficient, making it difficult to determine the specific impact of each component on the overall performance improvement.

Limitations:
(1) While the paper discusses the neuron starvation problem and the sensitivity of initial thresholds, it does not explicitly outline the broader limitations of the proposed TAS-GNN method. It would be beneficial to include a dedicated section that explicitly lists and discusses the limitations of the current work.
(2) The paper does not thoroughly address how TAS-GNN scales with extremely large datasets or very high-dimensional graphs. Including an analysis of computational complexity and memory usage for larger graphs would provide a clearer understanding of the scalability limitations.
(3) While multiple datasets are used, the paper could further discuss the generalizability of TAS-GNN to other types of graph-based tasks beyond classification, such as regression, clustering, or even dynamic graphs.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a novel approach called TAS-GNN (Topology-Aware Spiking Graph Neural Networks) to address the performance gap between spiking neural networks (SNNs) and artificial neural networks (ANNs) in graph classification tasks. The authors identify a ""starvation"" problem in spiking neurons within GNNs, where many neurons do not emit any spikes during inference, leading to severe information loss. This problem is more pronounced in graph classification tasks, where the test set graphs are independent from the training set, unlike in transductive or inductive learning settings.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.	This paper identifies a critical ""starvation"" problem in spiking neurons within Graph Neural Networks (GNNs), where many neurons do not emit any spikes during inference, leading to severe information loss. This problem is more pronounced in graph classification tasks, where the test set graphs are independent from the training set
2.	The paper proposes a novel approach called TAS-GNN (Topology-Aware Spiking Graph Neural Networks) to address the graph classification problem.

Weaknesses:
1.	The authors use the node degree instead of the concept of topology, there’s a large gap between the graph topology and node degree.
2.	The authors solve the graph classification task as a contribution, which is not a significant challenge for spiking graph neural networks.
3.	The advantage of Spiking Neural Networks (SNN) is their low energy consumption. However, the paper does not mention the feature, so it is unclear why graph neural networks should be combined with SNN. The motivation behind TAS-GNN is not clear.

Limitations:
The authors adequately addressed the limitations.  The authors should discuss more details of the potential negative societal impact of the work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes topology-aware spiking graph neural networks with adaptive thresholds based on a group of neurons for graph classification. The paper first diagnoses the poor performance as the existence of neurons under starvation caused by the graph structure. Then the paper proposes the adaptive threshold among neurons partitioned by degrees, as well as the learnable initial threshold and decay rate to reduce the sensitivity. Experiments on several datasets show superior performance of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper proposes the first SNN design to target graph classification.

2. This paper identifies the starvation problem and proposes a novel topology-aware group-adaptive technique.

3. Experiments show superior performance on several datasets, some outperforming ANNs.

Weaknesses:
1. The proposed method seems to be a hybrid ANN-SNN model rather than a pure SNN design. The paper did not discuss how this will affect the deployment of the model on potential neuromorphic hardware, since SNNs mainly target those hardware to obtain energy efficiency.

2. The paper did not discuss the (theoretical) energy efficiency estimation, which is a major motivation for considering SNNs as stated in Introduction.

3. Or if the motivation is to get models with better performance than ANN, then Table 1 does not include state-of-the-art ANN results for comparisons.

Limitations:
The authors discussed limitations in Appendix A.1.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
UTwuDTpdNO;"REVIEW 
Summary:
The paper addresses the vulnerabilities of Federated Learning (FL) systems to various adversarial attacks, including model poisoning and backdoor attacks. The proposed solution is a Meta Stackelberg Game (meta-SG) framework designed to offer robust and adaptive defenses against these attacks. The approach formulates adversarial federated learning as a Bayesian Stackelberg Markov game (BSMG) and employs a meta-learning method to find optimal defense strategies. Theoretical analyses show the algorithm's efficiency in convergence, and empirical results demonstrate its effectiveness against different attack types.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The proposed framework considers both untargeted and targeted attacks.

2. The paper uses RL to realize online adaptation which is close to the real world.

3. Inspired by meta-learning, the method is robust to unknown attacks.

4. In the pre-training phase, the paper uses generated data to decrease the concern of privacy leakage.

5. The paper provides sufficient experimental results.

Weaknesses:
1. Could you explain more about the necessity of adding the gradient adaptation (from BSE to meta-SE)? Although BSE is ex-ante when knowing the distribution $Q$, the model $\theta$ is changed during training and it could capture emerging information. Could you provide empirical results comparing BSE and meta-SE to show the advantage of meta-SE?

2. Considering adaptive/mixed attacks, the paper misses two relevant frameworks: MixTailor [1] and RobustTailor [2]. They can adjust aggregation methods during training. Especially, RobustTailor also simulates a game between the defender and the attacker, and it proposes a mixed attack. This kind of method could be included in experiments as a baseline.

3. Because the whole method is complicated with 2 stages, comparing computational cost with other baselines is necessary.




[1] Ramezani-Kebrya, Ali, Iman Tabrizian, Fartash Faghri, and Petar Popovski. ""Mixtailor: Mixed gradient aggregation for robust learning against tailored attacks."" arXiv preprint arXiv:2207.07941, 2022.

[2] Xie, Wanyun, Pethick, Thomas, Ramezani-Kebrya, Ali, and Cevher, Volkan. ""Mixed Nash for Robust Federated Learning."" Transactions on Machine Learning Research. 2023.

Limitations:
The authors mentioned limitations in Section 5. The main one is the privacy concern.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper titled ""Meta Stackelberg Game: Robust Federated Learning against Adaptive and Mixed Poisoning Attacks"" proposes a new approach to enhancing the security of Federated Learning (FL) systems. The paper identifies that existing FL defenses are inadequate against adaptive and mixed attacks. To address this, they introduce a Meta Stackelberg Game (meta-SG) framework, which employs a Bayesian Stackelberg Markov game (BSMG) and a meta-learning approach. This framework aims to provide a robust and adaptive defense mechanism against various poisoning attacks, including model poisoning and backdoor attacks. The proposed method is theoretically proven to converge to an equilibrium efficiently and is empirically validated to perform well against strong adversarial attacks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ Introducing the Meta Stackelberg Game (meta-SG) framework is innovative, offering a new perspective on defending against adaptive and mixed attacks in FL. 

+ The paper provides a solid theoretical foundation, proving that the proposed algorithm converges to a first-order ε-equilibrium, proving the method's efficiency. 

+ Extensive experiments demonstrate the effectiveness of the meta-SG framework, showing significant improvements in defense against various attack types compared to existing methods. 

+ The meta-learning component allows the defense mechanism to adapt dynamically to different attack scenarios, enhancing its robustness in uncertain environments.

Weaknesses:
- The proposed approach seems computationally intensive, requiring significant resources for pre-training and adaptation, which might limit its practicality in real-world applications. Although it proves that it can converge in Theorem 3.3, it would be beneficial to have empirical evidence, such as the method's run-time overhead. 

- While the framework is tested against several attack types, the scope of attacks considered might not cover all possible real-world adversarial strategies, limiting the generalizability of the results. The paper especially makes it unclear what attacks are used in pre-training, whether they are the same, and how different they are compared to the real FL environment when testing and generating results. 

- The proposed method's scalability to larger and more diverse FL environments remains unclear, especially given its computational demands.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a game theoretic model for robust federated learning. The technique is composed of pre-training and online adaptation. During pre-training, a meta-policy for the defender is solved as a Bayesian Stackelberg Markov game. The defense policy is further polished during the online adaptation stage.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The Stackelberg game is designed to counter unknown/uncertain attacks by an adaptive adversary. 

Theoretical bounds for sample complexity are provided. 

Empirical results demonstrate the effectiveness of the proposed technique.

Weaknesses:
The main weakness is the slight violation of privacy as the technique needs a portion of ground truth data from the clients. This has been disused as the limitations in the paper. 

Minor comments:

On page 2, ""including mixed attacks ,"" ---> extra space

Limitations:
Privacy violation is mentioned as a limitation of the technique. It's unclear whether future development can remove the dependence on the client-side ground truth.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a defense mechanism in federated learning that has adaptability inspired from meta learning. The authors formulates a Bayesian Stackelberg Markov game (BSMG), focusing on addressing poisoning attacks of unknown or uncertain types. The authors propose an equilbrium inspired by meta learning and then look at a local version of that. Empirical evaluations are done on MNIST and CIFAR.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper seems to have interesting results.

Weaknesses:
The writing is sloppy in many places and quite a few things are unexplored.
Federated learning's primary motivation is privacy, so the privacy loss from a core small dataset must be analyzed. The authors do acknowledge the loss but IMO that is not enough.

Behind all the motivation of federated learning, the core idea is in the equilibrium proposed - IMO, exploring this equilibrium in more detail would make the paper stronger (in fact, the problem makes sense even in simpler adversarial learning problems).

Is Def 3.1 just a differential equilibrium, in the style of ""Implicit Learning Dynamics in Stackelberg Games: Equilibria Characterization, Convergence Analysis, and Empirical Study"" but missing second order conditions?
Why are there no second order conditions? Just first order may not induce a local equilibrium, which is a meaningful equilibrium to achieve. This is where even more meaning needs to be discovered for the first meta-SE. The authors compare it to PBE in the appendix, but the notations of belief consistency and sequential rationality is what makes PBE (SE) convincing. Without any such (or similar) notion, a new equilibrium in a dynamic setting is not principled.

I do not understand why the ball $B(\theta^*)$ is used with a bound of 1? What is special about 1? (same for the other ball)
Proposition written informally (and no explanation) in the main paper does not make sense (e.g., Prop 3.4).

There are many typos when I started to look in appendix:
1) Eq F6, $\tilde{l}$ should have two inputs
2) Line 959, the parameters of $\tilde{l}$ is lost, without this the equations with $\theta'$ on left and no $\theta'$ on right is not well-defined.
3) I do not understand how the equation in line 964 came about - first it is said that it is an inequality but what is written is an equality.

Overall, typos do not inspire confidence.
Also, any defense mechanism should itself be subject to attack with the adversary possessing knowledge of defense mechanism - I do not see that in experiments.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers the problem of backdoor/poisoning attacks in federated learning (FL). In this setting, a single attacker has control over all malicious clients trying to employ different attack types (on each controlled client). This paper aims to create a defense mechanism against such adaptive attackers. To this end, the paper proposes a game-theoretic approach, which contains two stages: (1) Pre-training stage: before the FL environment, the defender first learns a good defense policy by simulating that environment using a small amount of truthful data against a simulated attacker with known possible actions (e.g. attack types used), and (2) Online-adaptation stage: the defender leverages the pre-trained policy and adapt it against the attacker at the real FL environment. This paper demonstrates the effectiveness of their proposed mechanism, as well as considers ablation studies where the previous assumptions do not meet: adaptation to attack methods at real FL environment are different (but similar) from ones seen at the pre-training stage.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is well-written and easy to follow.

Weaknesses:
I have some concerns, mostly about the practicality of the assumptions:

1. Assumption on the accessibility of data: the paper assumes that in the pre-training phase, the defender has access to a small amount of data, which is used to model the data distribution of clients using generative models. (1) It goes against privacy. (2) It is not clear that a small amount of data is representative enough to model client data distribution. (3) Things will be even more complicated if each client has its sub-population (data) that is (reasonably) different from each other. In this case, which malicious clients the attacker has control over will matter. 

2. Assumption of the similarity of attack types at the online adaptation stage: the paper assumes that the attack types in a real FL environment, though unseen, should be similar to those of the pre-training phase. This seems impractical, especially in a white-box setting, where the attacker will try to leverage the property of the defense mechanism to create an adaptive attack that is specified for that defense mechanism (see Carlini's works).

I also have concerns about the experiments:
1. Datasets and models used: Would it be possible to use more practical datasets and models instead of MNIST/CIFAR10 and ResNet-18? I would like to see results when the data distribution is complex enough that generative models cannot easily learn with a small amount of data. Right now, the amount of data used is still considerable, considering the dataset used. This would make the assumption of the accessibility to a small amount of data in the pre-training phase more persuasive.

Some comments on writing/paper organizations.
1. In Table 1, please highlight which results are the best. It would be more readable and easier for comparison. 
2. In Figure 2, it might be better to show smoothed curves.
3. In Appendix F, before each theorem/lemma/assumption, it would be better if an intuition/proof sketch for each one is provided. Also, if the proof technique/assumption is standard, please mention the corresponding references.
4. In the Conclusion, I think it would be more honest if explicitly stated that the major limitation of this paper lies in the practicability of the assumption. Right now, I only see privacy concerns mentioned, which is misleading. 

(Not really a weakness) It could be nice if source code is included (might be using an anonymous repo). 

Overall, I think this is a good paper if ignoring the practicability of the assumptions on the attack types/data (on the accessibility in the pre-training phase/distribution of accessed data in the pre-training phase/distribution of data in each client). I also did not find an experiment where the attacker leverages the information about defense mechanisms to instantiate a better attack scheme (white box attack). It is known that many proposed defense mechanisms failed in this scenario, though it seems obvious that it will go against the similarity assumption on the attack types at the online adaptation phase. However, I am also aware of the hardness of defense tasks in adversarial machine learning and it is good to have some initial results even under strong assumptions. I will try my best to be reasonable. Maths were not checked carefully, I will try to go into the details in the rebuttal phase.

Limitations:
See Weaknesses above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

";0
c78U5zi4eA;"REVIEW 
Summary:
This paper investigates the role of message passing (MP) in collaborative filtering (CF). Unlike most GNN-based CF research, which assumes that performance gains arise from improved representation learning through GNNs, this work questions that assumption. Through empirical experiments and theoretical analyses, the paper finds that the key contributions of MP in CF are: 1) forward message passing rather than back-propagation, and 2) benefits for low-degree users instead of high-degree users.

Based on these observations, this work propose a simple and efficient plug-in method called TAG-CF. TAG-CF can be easily integrated into any well-trained CF model by performing a single layer of message passing during the inference stage. To further reduce computational workload, TAG-CF+ conducts MP only on low-degree users. Experiments on four datasets verify the effectiveness of TAG-CF in improving CF models with almost no additional computational overhead.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper focuses on an important open problem: what is the role of MP plays in CF. This problem is worth investigating, and this work attempts to answer it from both empirical and theoretical perspectives.
2. This article is easy to follow, with a natural transition from the analysis of MP to the proposal of TAG-CF.
3. The proposed TAG-CF is efficient and effective. The plug-in nature of the method ensures the industrial application of the proposed method.

Weaknesses:
1. The discussion following Theorem 1 ""both BPR and DirectAU optimize the objective of message passing"" is not rigorous enough. Here are some aspects regarding to the discussion:
   * The assumption that $\|\mathbf{u}_i\|^2=\|\mathbf{i}_j\|^2=1$ does not hold in LightGCN, thus the discussion on the theorem is only empirical but not strictly hold for LightGCN. In other words, the BPR loss for real-world LightGCN may not be upper-bounded by the objective of message passing.
   * Even if the claim ""both BPR and DirectAU optimize the objective of message passing"" holds, there should be an additional experiment to show that directly train a CF model on the objective of message passing achieves comparable performance of LightGCN-BPR. 

2. Important baselines such as GFCF [1] and SVD-GCN [2] are missing. These works also dedicated to speed up GNN-based CF. On Gowalla dataset, both GFCF and SVD-GCN cost only around 30s for training, which is more efficient than the proposed TAG-CF. Moreover, the reported Recall@20 on Gowalla are 0.1849 and 0.1905 respectively, also shows comparable or better performance. The authors need to add these baseline in the experiment and discuss what is the advantage of TAG-CF compared to these baselines.

3. According to Figure1, it is not sufficient to draw the conclusion that ""Message Passing in CF Helps Low-degree Users More Compared with High-degrees"". In Yelp dataset, the improvement of degree=16 users is only about 15%, which is less than the value of most higher-degree users (degree from 17-34). And what is the improvement for the users whose degree is less than 16? Still, this empirical experiment on only two dataset is not enough to support this conclusion.

[1] Yifei Shen, et al., “How Powerful is Graph Convolution for Recommendation?” CIKM 2021

[2] Shaowen Peng, et al., “SVD-GCN: A Simplified Graph Convolution Paradigm for Recommendation”. CIKM 2022

Limitations:
See weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper rethinks the application of message passing mechanisms in collaborative filtering methods and makes two key findings: 1) Message passing (MP) improves collaborative filtering primarily through the forward pass rather than the backward propagation process, and 2) MP is more effective for cold-start users compared to warm users. Based on these findings, this paper proposes a novel test-time aggregation method tailored for collaborative filtering, supported by theoretical derivations to demonstrate its validity. Finally, extensive experiments validate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper rethinks the message passing mechanisms in collaborative filtering and makes two novel key findings.

2. The method has theoretical guarantees to ensure its effectiveness.

3. The structure of this work is logical, and the writing is well-organized.

Weaknesses:
1. Equation 4 seems unable to separate the forward pass neighbor representation from the backward pass gradient update in $s_{ij}$. Therefore, it is theoretically challenging to validate this finding, even though Table 1 empirically demonstrates that the forward pass primarily drives the performance improvement.

2. Typically, evaluation metrics for recommendation models use NDCG@10 and Recall@10, as users tend to focus on items ranked higher in the list while ignoring those ranked lower. Therefore, NDCG@10 and Recall@10 better reflect the true effectiveness of recommendation models.

3. It is necessary to explain some abbreviations, such as 'OOM' and '-'.

4. This paper has many grammatical problems, so it is suggested to improve it. For example, ""TAG-CF is extremely versatile can be used as a plug-and-play module to enhance..."".

Limitations:
See weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the role of message passing in collaborative filtering, providing empirical analysis of its impact. Based on their findings that message passing primarily benefits through additional neighbor representations during forward passes and helps low-degree nodes more than high-degree nodes, the authors propose TAG-CF, a test-time aggregation framework for collaborative filtering. TAG-CF demonstrates competitive performance with state-of-the-art graph-based collaborative filtering methods while significantly reducing computational overhead.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper experimentally analyzes the impact of message passing on collaborative filtering.
2. Based on the analysis results, an efficient TAG-CF method is proposed.
3. Extensive experiments were conducted on various datasets.

Weaknesses:
W1. Crucially, this paper overlooks related work, which do not require training.

W2.  The paper claims that TAG-CF can achieve performance similar to training-time message passing with just one aggregation at test time. More detailed analysis is needed to determine if this holds true in all situations and under what conditions these results occur.

W3. While it's claimed that performance improvement is greater for low-degree nodes, the theoretical explanation for this is somewhat lacking. A deeper analysis of why this phenomenon occurs is needed. In particular, it seems highly dependent on the degree cutoff.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper investigates on an interesting topic: how message passing is playing a role in graph-based recommender systems. Upon experiments on LightGCN model, authors posit the advantages of message passing lie in the forward pass to aggregate neighborhood information while at the same time, backward propagation has little effect on the results. Based on this assumption, authors propose test-time aggregation for collaborative filtering where the representation of users/items is aggregated at the testing time for more efficient training.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper identifies roles for message passing in graph-based recommendation. 
2. It provides more insights for researchers to understand the mechanism behind graph-based recommendation.
3. The proposed method is efficient.

Weaknesses:
1. Lack of pre-training description
2. Lack of hyper-parameter analysis

Limitations:
See weakness

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xtpY1kQmW9;"REVIEW 
Summary:
This paper appears to suggest that any decision is composed of two Bayesian decisions and it trys to evaluate the implications of this idea. 

I am very confused by this paper and really don't know what to make out of it. For example, the conclusion seems to be only a brainstorming session of random ideas and the rest of the paper does not appear to be much better.

At the very least, it is not well written, at worst the proposed approach does not make any sense.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Given that I don't properly understand what exactly the authors want to achieve, I am unable to formulate the strengths of this paper.

Weaknesses:
The presentation is very messy. The paper jumps from topic to topic without me understanding their relations to each other.

Limitations:
see above

Rating:
1: Very Strong Reject: For instance, a paper with incorrect statements, improper (e.g., offensive) language, unaddressed ethical considerations, incorrect results and/or flawed methodology (e.g., training using a test set).

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper discusses the implications of Bayes' theorem, making assumptions inspired by a thought experiment of communicating a message. Prior (and model) elicitation by solving a fixed point equation is discussed.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
* The paper takes a fresh look at decision marking under uncertainty, which is at the center of machine learning.
* The generality of the setting makes the discussion applicable to virtually all of ML.

Weaknesses:
While I am sensible to the topic of prior and model elicitation from coherence arguments, I believe the paper needs a thorough revision focussing on clarity. While I have some intuition now, it is still not crystal clear to me what the exact goal or claims of the paper are. See bullets below for constructive comments.

## Major
1. Section 4: what is the probability $P$? What is the underlying space and sigma algebra? What are they supposed to represent?  
2. Section 4 introduces several very strong assumptions, like $1-P(A\vert B) = P(B\vert A)$ (is it for all $A,B$ in some sigma-algebra or for a specific pair of events?), that are motivated by an analogy about communicating a message. It is not clear why I should be prepared to make these strong assumptions. The fact that I don't know what $P$ is supposed to model or serve as does not help. Is it a joint probability over the variables describing a decision problem, as in decision theory? In that case, will it be used in conjunction to a loss function to make decisions? Will it be judged by some measure of decision accuracy? Or are we in a de Finetti framework, coming up with a personal probability $P$ which we will use to make predictions about unobserved variables? My intuition is that we are dealing with the latter kind, but this should be explained. And the strong assumptions need to be motivated by more than an analogy about communication.
3. The information analogy which motivates imposing the fixed point equation (9) is unclear, as well to what probability and what events it should apply.
4. p5 L179: the sentence about the parameter being a dynamic parameter for a learning system is unclear. We haven't discussed any learning algorithm yet.
5. I am not sure I see where Eqn (11) comes from. $\lambda$ has been chosen to derive (10) from Bayes' theorem, but it doesn't have to be the right base to write (11), right? Same remark for (18).

## Minor
1. p7 L248: Although neural networks have been a popular class of models and algorithms, supervised learning is not synonymous with neural network training.
2. p7 L252: the meaning of ""the $\lambda$ expression"" is unclear.

Limitations:
This is fundamental work that does not have any immediate negative societal impact.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The purpose of this paper is to investigate the optimality of a classifier. It is known that the Bayes classifier is optimal, and it is likewise known that an explicit computation of the Bayes classifier is often very challenging if not impossible. This paper offers an analysis of the Bayes classifier as a sequential solution of two problems. An analysis and interpretation of a vase / faces example is presented and some theory is developed to further understand it. The paper concludes with an application.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors are exploring an idea which is novel, and the whole thinking about Bayes classifiers as comprising two sub-problems seems novel and worth pursuing.

Weaknesses:
I did not really understand the discussion with the vase, the sender and receiver. Perhaps the authors should somehow connect the Bayesian ideas to the description of the problem earlier? I think the paper would really benefit from rewriting Section 4 with the vase as a running example, because it is hard to connect the various decisions with the probabilities. Maybe it's worth to add more illustrations / diagrams for this? The authors are presenting novel ideas and it's hard to understand them as they are currently presented.

For the theoretical implications, I think it would be better to illustrate the approach on a simpler model like a linear one. 

The paper started by mentioning the Bayes classifier but does not come back to it as an example. 

The paper states that the Bayes classifier is broken up into two decisions, but those are just briefly mentioned in the vase / faces example. The authors should carry this thread of reasoning through the whole paper.

Limitations:
Yes.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
AAN46kUPXM;"REVIEW 
Summary:
In this paper, the author proposes ""Expressiveness,"" a metric that measures the dissimilarity of feature maps produced by different filters. Subsequently, the author introduces NEXP, a technique to prune filters based on their expressiveness. The proposed method is tested on tasks such as image classification and object detection.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The overall structure of the paper is clean and easy to follow.
2. Although I am not very familiar with the structured pruning literature, the application of the concept of representation power of neural networks in this field appears novel.
3. The experiments are comprehensive, including pruning at initialization (PaI), pruning after training, and other tasks like object detection.

Weaknesses:
1. The notation is very confusing. For example, in the section 'Generalization of concepts at a structural level,' $\ell$ is the index of a certain layer, but the upper-case K is the total number of layers, and the lower-case k is the index of a filter/channel in a certain layer.
2. The concept of expressiveness is not new in the context of pruning [1] and neural architecture search [2,3,4].
3. The performance improvement by NEXP is not prominent. For example, in Tables 1 and 2, NEXP often shows a higher compression ratio but lower accuracy. This makes it unclear if NEXP offers a significant advantage over other methods.

***
**Minor Mistakes:**

Line 163: ""where k the is"" should be corrected.

[1] Tanaka, Hidenori, et al. ""Pruning neural networks without any data by iteratively conserving synaptic flow."" Advances in Neural Information Processing Systems 33 (2020): 6377-6389.

[2] Lin, Ming, et al. ""Zen-NAS: A zero-shot NAS for high-performance image recognition."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021.

[3] Wang, Haoxiang, et al. ""Global convergence of MAML and theory-inspired neural architecture search for few-shot learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022.

[4] Chen, Wuyang, Xinyu Gong, and Zhangyang Wang. ""Neural architecture search on ImageNet in four GPU hours: A theoretically inspired perspective."" arXiv preprint arXiv:2102.11535 (2021).

Limitations:
The authors have discussed limitation in the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper works on weight pruning for CNNs. It proposes an evaluation metric, i.e., ""expressiveness"", to evaluate whether a neuron/groups of neurons should be pruned or not. The metric focuses on the neurons' ability to redistribute informational resources. As the evaluation of expressiveness requires data samples, the paper includes studies on arbitrary data or limited dataset’s representative samples. The experiments are conducted for image classification tasks on ResNet architectures, and the object detection task on YOLOv8m.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Weight pruning is an effective manner in reducing the redundancies in DNNs. Instead of focusing on weight importance, this paper considers the expressiveness of neurons in the information flow within a network. The proposed evaluation metric can also be combined with existing strategies with importance evaluation metrics as a hybrid pruning approach.

Weaknesses:
1. Limited practicality of the approach. The method is mainly focused on removing redundant filters from CNNs. Though CNN is one category of DNNs, recent works have shifted to more advanced model architectures such as transformers and Mamba, which are mainly composed of FC layers instead of CNNs. The practicality of the approach is highly restricted to SOTA model architectures for image classification tasks.   

2. Limited performance improvements. This is a major concern. The performance gain of the proposed method is not obvious compared with baselines. For instance. on CIFAR-10 VGG-16, SCP and reduce the parameters 15.28$\times$ with a 93.85\% accuracy while the proposed method can only reduce the parameters 5.62$\times$ with a slightly better accuracy 93.87\%. HRank also provides better performance than the proposed method with higher accuracy 93.96\% (0.09\% higher than NEXP) and higher reductions of FLOPs (4.26$\times$ (HRank) v.s. 4.01$\times$ (NEXP) ). On DenseNet-40, Hrank also shows better performance across all metrics. Hrank v.s. NEXP: Accuracy 95.05\% v.s. 94.64\%, parameter reduction 3.31$\times$ v.s.3.12$\times$, FLOPs reduction 3.38$\times$ v.s. 2.51$\times$.

Limitations:
Please refer to weaknesses and questions. The major concern is that the method does not show better performance than baselines, and is also limited in model architectures.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a new structured pruning approach NEXP. It works by computing the dissimilarity score of the feature activations across samples and removing those filters with smaller variances. Experimental results on several models and datasets demonstrate the effectiveness of the proposed approaches.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written with clear motivation and many of the important technical details are included.  
2. The authors have presented the experimental results well and the additional discussion provides deeper insights on the effectiveness of the proposed methods.  
3. The authors evaluated models beyond image classification, i.e., the proposed methods work well on YOLOv8 object detectors.

Weaknesses:
1. The conclusion section is too short and fails to characterize the main contribution of this paper. What does it mean as to “when” and “how” to prune? The authors should elaborate on these further.   

2. I think that this is not a scalable approach for computing the pruning metrics. The pruning metric proposed in the paper requires computing a N by N matrix for each filter in the network, where N is the number of samples. This could grow quite computationally infeasible for large networks and batch sizes. The authors also fails to discuss this aspect on the pruning efficiency in the paper.  

3. In terms of experiments, I am not sure why the authors compare each methods under different compression ratio? If the pruned models in each method have different parameters, it can be hard to compare the accuracy numbers.  

4. I feel a lot of the content in section 3 is not necessary and they can could go into a separate preliminary section. Section 3.1 and early parts of Section 3.2 takes up a lot of space and in the meantime do not provides us with the motivation and insights for the later introduced methods.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to handle network pruning problem. Specifically, it proposes to use a new importance measurement, called expressiveness, to decide the pruning process. It jointly considers the model state to leverage on the proposed measurement. In addition, it can also combined with typical importance based pruning methods to improve the model efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Network pruning is a valuable research direction to study on, espeicailly for current large-scale era where efficiency matters a lot.
2. The proposed new metric to measure the network importance is interesting.
3.  Empirical results show the method superiority.

Weaknesses:
1. Adding more discussion in the conclusion part helps to improve the paper readability.
2. Since this paper proposes a new pruning metric, it is better to show more visualization and network behavior analysis to illustrate the intuition.
3. The compared baseline models are relatively old, adding more recent publications helps to support this paper.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
HcqnhqoXS3;"REVIEW 
Summary:
This paper proposes the Multi-Task Prompt Decision Transformer (MPDT) algorithm for zero-shot multi-task offline reinforcement learning (RL). Leveraging a pre-trained language model (PLM) with prompt tuning, the MPDT innovatively decomposes multi-task prompts into task-specific and cross-task components. It also achieves zero-shot generalization through test time prompt alignment. Evaluations across various benchmarks demonstrate that MPDT outperforms prior multi-task (meta) offline RL methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written and easy to follow.
- The idea of decomposing prompts is straightforward for MTL and easy to understand.
- The experiments are comprehensive.

Weaknesses:
- My main concerns come from the novelty of the paper. The major contribution lies in the use of cross-task prompts and task-specific prompts, specifically the prompt decomposition and prompt distillation in Section 4.1.
- Regarding prompt decomposition, although the authors aim for cross-task prompts to contain common knowledge across tasks, the model and loss design do not ensure this. Specifically, since the authors use element-wise multiplication on $P_c$ and $P_k$, $P_c$ functions more as a common scaling factor for different task-specific prompts. After training, $P_c$ could become a constant scalar, and $P_k$ could simply scale as $1/N$ of $P_k^{teacher}$. The authors could provide the distribution of $P_c$ values to verify if it truly encapsulates common knowledge. And it is better to have a loss function to guide the common knowledge extraction during training.
- About the test time adaptation, if the authors believe that $P_c$ contains the common knowledge, I suppose we should train a randomly initialized $P_k$ (or select one from a training task according to task similarity) for the test task using the alignment loss. This way, the model will still take $P_k \cdot P_c$ as prompt input.

Limitations:
The limitations and social impact are provided in the conclusion.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Multi-task learning is a critical pursuit in decision-making, and Decision Transformer (DT) is a popular framework in solving various decision-making problems. The author observe the suboptimal performance of prior work utilizing DT for multi-task learning, and propose a new method, Multi-Task Prompt Decision Transformer (MPDT), to alleviate this issue. MPDT is mainly composed of these components:
- GPT2 pre-trained weights for initialization.
- Prompt decomposition: cross-task prompt + task-specific prompt to prevent gradient conflicts.
- Test time adaptation (TTA): dynamically optimizes the cross-task prompts during testing time.

Experiments are conducted on standard Meta-RL tasks, demonstrating prominent improvements compared to prior baselines. Ablation experiments are also extensively conducted.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality:
- The MPDT framework combining pre-trained weights, prompt decomposition and TTA together is original.
- The prompt decomposition technique to prevent gradient conflicts is orginal and well-motivated.

Clarity:
- This paper is well-written and well-organized.
- The method is easy to follow.

Significance:
- Though pre-training DT is a popular trend in decision-making, this is the first work to successfully apply it in multi-task learning, to the knowledge of the reviewer.
- The experimental results are prominent, compared with a set of strong baselines.

Weaknesses:
*Significant*
- It is unclear to the reviewer that whether it is fair to compare MPDT with baselines like MT-BC, Soft-Prompt, and Prompt DT. since MPDT adopts much test-time information. How can the authors guarantee that they use the same amount of information in the test set for all baselines? If it is truly unfair, it would make the experiments less convincing. Please provide explanations for this.
- Did the authors reproduce the results of baselines by themselves? If so, how did they pick the hyper-parameters? And for few-shot generalization, which appears to be a new benchmark designed by the authors, did the authors extensively tune the hyper-parameters of baselines?

*Major*
- It is always hard to claim the so-called ""SOTA"", which demands very rigorous statistical analysis. And it is rarely true in nowadays AI community, see https://rl-conference.cc/review_guidelines.html. Running 3 times is acceptable to present the performance, but not enough to support ""SOTA"". And the variances in Table 1,2 are very large (some [$\mu-\sigma$, $\mu+\sigma$] intervals are overlapped), thus making it hard to establish statistical improvements. The reviewer recommends removing this claim.
- The framework is a bit complicated and engineering. Specifically, the 3 components, namely pre-training+prompt decomposition+TTA, are orthogonal and not well-connected. And thus the technical novelty of the method is a bit lacking. The novelty concentrates on prompt decomposition+distillation, while solely applying them doesn't achieve good performance (Table 3).


*Minor*
- The wording ""Unseen Tasks Generalization"" and ""Few-shot Generalization"" in Section 5.2 are misleading, as few-shot generatlization is also for unseen tasks.
- The name of the proposed method, *Multi-Task Prompt DT*, might not be suitable, since it cannot reflect the differences from Prompt DT, which is already a well-developed algorithm in muti-task RL.

Limitations:
The limitations are discussed in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new method called Multi-Task Prompt Decision Transformer (MPDT) for efficient generalization to unseen tasks in offline reinforcement learning. MPDT involves two stages. First, the multitask training phase: MPDT is initialized with parameters from a pertained LM, which is GPT2. It decomposes the task prompt into a cross-task prompt shared across tasks and task-specific prompts. Second, the test time adaptation: The cross-task prompt is further optimized on unlabeled test tasks using test time adaptation by aligning the distributions of the test samples and training samples. The paper evaluates MPDT on seven meta RL environments from both MuJoCo and MetaWorld, showing its superior performance over baselines in generalizing to unseen tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Based on the review's knowledge, combining decision transformer and test time adaptation together for efficient multi-task RL solving is novel.

2. Keeping the weights of the pretrained LM frozen is a natural idea to leverages its rich prior knowledge.

3. Extensive experiments on seven Meta-RL environments demonstrate MPDT's effectiveness over DT-based baselines.

4. Ablation studies individually analyze the impact of different components including prompt decomposition, distillation, and test time adaptation.

Overall, MPDT appears to be a promising approach for multi-task offline RL by combining prompt-based techniques with test time adaptation in a novel way.

Weaknesses:
1. The paper appears to be hastily written and the presentation is hard to follow. (see question section)

2. The authors claim that a major improvement compared with other PDTs is the decomposition of prompt into the common part and task-specific part. I was expecting to see the common part will be trained across tasks and the task-specific part is within a specific task. However, in line 186, the authors say ‘We use standard normal distribution to initialize Pc, uk and vk.’ Besides, in Eq(4), they are equally optimized for each task. I didn’t see why the common part is the slow weights and the task-specific part is the fast weight, which is claimed by authors in line 167. Possible to explain here?

3. In line 192, the sentence ‘we obtain teacher task prompt $p_k^{teacher}$ for each task by using traditional prompt-tuning method individually. ‘ What method is used? How to learn it?  Is there a separate learning phase for $p_k^{teacher}$? If so, the method requires three training phases instead of two.  Also its quality matters. Can authors explain the details here?

4. In line 201, for test time adaptation, ‘ we randomly select a subset X of unlabeled test samples,….’ Can authors clarify what is this subset? A sequence in the DT is organized  as (r_t, s_t, a_t, r_{t+1}, s_{t+1}, a_{t+1}…). I recommend being specific what you select?

Limitations:
See my above comments for weakness and questions

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel Multi-Task Prompt Decision Transformer (MPDT), which leverages pre-trained language models as the initialization and adopts test-time adaptation. This approach achieves efficient generalization to unseen tasks through the prior knowledge from the pre-trained language model and by decomposing the task prompt. In the multi-task training stage, the prompt is decomposed into a cross-task prompt and a task-specific prompt, which can reduce gradient conflicts and computational load. Besides, the task-specific prompt is further decomposed into two low-rank vectors. Prompt distillation is also used to improve the quality of the prompt decomposition. In test-time adaptation, the method further optimizes the cross-task prompts on unseen tasks via alignment loss. An empirical study on Meta-RL benchmarks demonstrates the superior performance of MPDT compared to existing methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- This paper leverages GPT as the initialization to provide the prior knowledge for RL tasks, which is reasonable. The empirical study demonstrates that MPDT with the pre-trained language model initialization outperforms MPDT without the language model initialization.
- This paper utilizes prompt decomposition and only updates the prompt parameter in the multi-task training, which significantly reduces the trainable parameters. Compared with baselines, MPDT with fewer trainable parameters achieves superior performance. 
- The empirical study demonstrates that MPDT outperforms the baselines, and further analysis demonstrates the effect of the components in this method.

Weaknesses:
- This paper proposed utilizing the pre-trained language model as initialization but lacks an explanation of why the language pre-trained model could contribute to the RL tasks. 
- This method needs a long prompt to perform well compared with the Prompt DT. 
- This paper lacks detailed explanation on why using the cross-task prompt, task-specific prompt, and the prompt distillation.

Limitations:
See weaknesses and questions.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
1URMG6B3WW;"REVIEW 
Summary:
This paper introduces the KrwEmd algorithm, a novel approach to hand abstraction in Texas Hold’em-style games. The main contribution is the integration of historical information using K-recall winrate features and earth mover’s distance, addressing the limitations of previous imperfect recall abstraction methods. The algorithm demonstrates significant performance improvements over state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The integration of historical information via K-recall win rate features enhances the accuracy and reliability of hand abstraction.
2. KrwEmd significantly outperforms existing methods, showcasing its practical value and potential for real-world applications.
3. The algorithm is technically sound, with strong experimental validation supporting its claims.

Weaknesses:
Scalability: While the paper demonstrates KrwEmd's effectiveness in Texas Hold’em-style games, its scalability to more complex and diverse game scenarios remains to be explored.

Comparative Analysis: The paper could benefit from a more detailed comparison with existing hand abstraction methods to better highlight KrwEmd's unique advantages and limitations.

Limitations:
The paper primarily focuses on Texas Hold’em-style games. While this is a significant achievement, a discussion on the model's scalability and generalization to more complex and varied game scenarios would be beneficial. Including potential strategies to address these challenges would strengthen the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel approach to hand abstraction in Texas Hold'em-style poker games, addressing the limitations of current methods that often disregard historical information. The authors make two primary contributions: First, they develop KRWI (K-Recall Winrate Isomorphism), a new abstraction method that incorporates historical information from previous game phases. Second, they present KrwEmd, the first hand abstraction algorithm to effectively combine K-recall win rate features with earth mover's distance for hand classification. Through experiments conducted in the Numeral211 Hold'em environment, the authors demonstrate that KrwEmd significantly outperforms state-of-the-art algorithms such as Ehs and PaEmd in terms of exploitability, while maintaining the same number of abstracted information sets. This work shows that incorporating historical information can substantially enhance the performance of hand abstraction algorithms, potentially leading to more advanced strategic computation in large-scale adversarial games and stronger poker AI systems.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
* Overall, the paper provides a new technique that is promising for an important area of research
* The results indicate strong improvements over alternative methods

Weaknesses:
For me, the paper's primary weakness is the presentation method. I had trouble understanding the significance and nature of the contribution from the current submission. In general, a clearer description of this area of research for people who, e.g., work on games but don't focus on poker would be quite helpful. Some specific suggestions/areas for improvement are:

* Clearer introduction of key concepts: The paper jumps into technical terms like 'imperfect recall abstraction' and 'hand abstraction' without adequately explaining them for a broader audience. A brief explanation of why these concepts are important in poker AI would be beneficial.
* More intuitive explanations of the algorithms: The descriptions of PWI, KRWI, and KrwEmd are highly technical. Including some simple examples or diagrams to illustrate how these algorithms work could greatly improve understanding.
* Better contextualization of the contribution: While the paper claims to outperform existing methods, it's not clear how significant this improvement is in the broader context of poker AI. A discussion of the practical implications of this improvement would be valuable.
* Clarification of experimental setup: The Numeral211 Hold'em environment is not well-known. A clearer explanation of how this relates to standard poker variants would help readers understand the relevance of the results.
* More accessible presentation of results: The graphs and tables are dense with information but lack clear explanations. Simplifying these visualizations or providing more guidance on how to interpret them would be helpful.
* Glossary of terms: Given the many technical terms used (e.g., 'earth mover's distance', 'K-recall winrate feature'), a glossary could be a valuable addition to help readers keep track of these concepts.

Limitations:
There's limited discussion of limitations. It would be good to include an explicit limitations section. In particular, it would be good to discuss potential computational challenges in more detail, any limitations on the scope of the evaluation, and future work that might be planned.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper focuses on the problem of hand abstraction for Texas Hold-Em style poker games. Hand abstraction is the process of partitioning game histories into infosets which still contain enough information to make strategically advantageous decisions. Previous approaches have focused on abstractions that primarily focus on the future outcomes from each hand, but the authors suggest that it may instead be beneficial to also include past information. They design a hand abstraction algorithm called KwrEmd that outperforms previous work by incorporating historical information.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The results seem to show that KrwEmd outperforms other imperfect recall hand abstraction algorithms in terms of exploitability.

Weaknesses:
As somebody who is unfamiliar with the subfield of imperfect recall abstraction, I found the paper to be quite confusing throughout. The authors do not often provide intuition or examples for their method, and I found it difficult to tell exactly which contributions were novel compared to Fu et al. While it's reasonable for a paper to use technical language at times, well-written papers are usually understandable by a broader set of readers than just those in the specific subfield.

Limitations:
Not sure.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes KrwEmd, a novel hand abstraction algorithm for imperfect recall settings in Texas Hold'em poker. The algorithm leverages K-recall winrate features, incorporating historical information in addition to future information for constructing hand abstractions. The authors introduce two new isomorphism frameworks: Potential Winrate Isomorphism (PWI) and K-recall Winrate Isomorphism (KRWI). They demonstrate that KRWI outperforms existing methods like POI in identifying distinct infosets. KrwEmd, which combines KRWI with Earth Mover's Distance (EMD) for hand classification, shows superior performance compared to POI, Ehs, and PaEmd in the Numeral211 Hold'em environment.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
*Originality*: The paper presents a novel combination of K-recall winrate features and EMD for hand abstraction in imperfect recall settings, addressing a critical limitation of current approaches that solely rely on future information.
The introduction of KRWI and PWI provides valuable new tools for understanding and constructing hand abstractions in poker AI.

*Quality*: The experimental results in the Numeral211 environment demonstrate a clear improvement over existing methods, supporting the claims of the paper. The paper includes an appendix with algorithm details and supplementary experimental data.

*Significance*: The proposed KrwEmd algorithm advances the state-of-the-art in hand abstraction for imperfect recall settings, offering a potentially significant improvement for developing stronger poker AI agents.
The incorporation of historical information is a valuable contribution that benefit positively future research in poker and other imperfect information games.

Weaknesses:
I found the paper  challenging to understand, though this may be due to my limited background knowledge in poker AI and game theory.  While the authors provide a background section, the density of the technical content and the numerous specialized terms make comprehension difficult.

The description of the accelerated algorithm in Appendix A.3 could be expanded for better understanding. Additionally, a clear discussion of the limitations of the accelerated algorithm would be beneficial.

The paper provides limited information about the proposed algorithms, particularly KrwEmd. While the core concepts are presented, the details regarding implementation and specific design choices are limited. More in-depth explanation and pseudocode would enhance the paper's quality.

Limitations:
The authors acknowledge the computational complexity of clustering with EMD and introduce an accelerated algorithm. However, the paper lacks a dedicated section addressing the limitations of the proposed methods and the accelerated algorithm.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper develops new hand abstraction techniques for Texas Hold'em-style games (in general: games with ordered signals), which fare better than previous methods in both the number of hands identified, and performance (exploitability) in a simplified version of the game. 

Hand abstraction is a technique aiding the strategy construction in a Texas Hold'em, where concrete hands, or rather concrete signal infosets (i.e. ""possible words"" according to the information revealed so far), are replaced by abstract infosets, represented in an abstract feature space (here $\mathbb{R}^n$).

The core idea of the paper is to use the features of hands from previous rounds in the construction of the current round feature. More precisely, the paper investigates a simple method (KRWI = *k-recall winrate isomorphism*) of maintaining, at a given round, the collection of all potential-winrate isomorphims (PWI) features from previous $k$ rounds by concatenating them all together. PWI for an $n$-player game is a categorical probability distribution over $n+1$ events of a form *""this player outperformed exactly $l-1$ other players while losing to none""* for $l = 1, 2, \ldots n$ and *""this player lost to at least one player""*. Those distributions can be computed by a dynamic programming method. To reduce the cardinality of the space, the paper later clusters KRWI features with k-means using the Wasserstein distance, naming it the KrwEmd method.

All of the methods are benchmarked against currently used techniques that do not use historical information. Experiments find that KRWI identifies similar proportion of signal infosets as the previously used KROI. Using the metric of exploitability of the equilibrium (how it deviates from Nash equilibrium) of the strategy found by an imperfect-information game solver, authors find that 2-RWI-based approach performs almost the same as 2-ROI, and that KrwEmd outpefrorms previously used Ehs and PaEmd by a relatively large margin.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper proposes a reasonable extension of the currently used techniques for hand abstraction in Texas Hold'e, and shows that the new idea beats SOTA. It does not shy away from introducing the reader to the relevant background in a rigorous way (which is what made it possible for me to even start reading it). Experiments show a meaningful improvement, and provide additional insights (such as the decreasing worth of historical information).

Weaknesses:
From the perspective of someone not at all acquainted with the field of imperfect information games/games with ordered signals, the paper was quite hard to read and understand - even though (assuming that the authors agree with my summary), the contribution is a relatively straightforward idea.

The introduction was uninformative and confusing (I would recommend rewriting the whole second paragraph); the preliminaries, although presented in-depth and trying to be formal, also posed quite a few questions; sections 4 and 5 describing the main contribution lacked detail and justification (i.e. ideally I would like to see definition/theorem/proof style - otherwise the text is impossible to read for someone unfamiliar with the field), and the experimental setup is assuming a lot of background knowledge that was not explained neither in the main paper nor in the appendix (it was also difficult to gauge if the comparison between SOTA and the new approach was fair from the resources pov - the paper reports some numbers, but never an aggregated ""memory/time used"" for all methods).

Please see Questions below for a detailed explanation of what I found lacking or hard to understand.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
xZxXNhndXU;"REVIEW 
Summary:
The paper presents a novel 3D scene representation for novel view synthesis (nvs) in dynamic urban environments where, in particular, under heterogeneous imaging environments. The proposed representation relies on existing ingredients: 3D Gaussian Splatting, learned static/dynamic object instances, and  a global scene graph.

The resulting system yields very strong results on a series of public autonomous driving benchmarks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
### + Readability.
Overall, in its current state, the paper's readability is relatively good. The main ideas, concepts, are mostly well discussed, conveyed, and articulated, throughout the paper.

### + Practical usefullness of the considered problem.

### + Structure, and Organization of the Contents.
The presentation is mostly on point and each dedicated section of the paper is properly balanced. The use of text real-estate is fair.

### + Relative simplicity of the conceptual contribution.

### + The amount of implementation details is very good.

### + The reported performance.

### + Implementation details for reproducibility: excellent.

Weaknesses:
### - (1) Positioning of the conceptual contribution vs. the competitive landscape.

In particular, the proposed method looks very much like a revisit of Panoptic Radiance Fields [49] by replacing the NeRF component byt 3D Gaussian splats. 

While this is perfectly fine, this merits a targeted, transparent discussion in the main paper to help the reader understand the whereabouts of how the proposed contribution relates (or not) with such pieces of litterature.

### - (2) How much does it cost?

Missing piece of information regarding the resource usage, memory footprint, typical timings etc to better understand the downsides of using the provided method.

### - (3) (To a lesser extent) Certain contents in the paper are unclear.

Figure 4: what is happening? Adding color annotations or boxes would definitely help.

Limitations:
The authors provide one dedicated paragraph that reasonably addresses such considerations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper aims to perform view synthesis for dynamic urban scenes. This paper adopts 3DGS as scene geometry and uses neural fields to model the dynamic appearance of urban scenes. The neural scene graph is introduced to handle the movement of dynamic objects, and a deformation field is used to handle local articulated motions. Experiments show that the proposed approach outperforms baseline methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The presented pipeline well handles the dynamic appearance of urban scenes.
2. The experiments are sufficient and validate the effectiveness of the proposed approach.
3. The idea of combining neural fields with 3DGS is sound and effective.

Weaknesses:
1. The method presented in the paper takes 0.17 seconds to render an image at a resolution of 1550x2048, which is significantly slower than conventional 3DGS. Is the trade-off of such a significant sacrifice in rendering speed for quality improvement justified? Does the author have any solutions to address this issue?
2. The paper needs to evaluate the extent to which neural fields impact the rendering speed of 3DGS.
3. The pipeline figure of the paper should be clearer. The connections between the various modules are not easily discernible from the figure and its caption. For instance, it is not clearly depicted how the latent codes obtained from the scene configuration are inputted into the neural fields. Then, how are neural fields combined with 3DGS to represent static scenes and dynamic objects? The figure only shows simple association arrows. However, these modules are not merely input-output relationships. There are some combination operations between them.
4. The paper uses neural fields to represent appearance, which reduces the memory footprint but may also significantly impact rendering speed. Has the paper considered how to address this issue?
5. In Figure 2 of the paper, regarding the neural fields section, the symbols for static opacity correction and dynamic deformation are inconsistent with the descriptions in Section 3.2 of the paper. This is quite confusing.
6. I am curious whether the combination of neural fields with 3DGS could make the optimization of 3DGS unstable?
7. The non-rigid objects mentioned in the paper refer to cars, right? Or other objects? I did not see how the paper describes the modeling of cars. Although the paper mentions the use of scene graphs for modeling, I did not see how dynamic cars are represented using scene graphs. Does the paper treat dynamic cars as non-rigid objects directly? In this case, how can the large range of movement of dynamic cars be handled?

Limitations:
The limitations from the introduction of neural fields should be discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a hybrid neural scene representation for dynamic urban driving scene modelling. The method utilizes 3D Gaussians as an efficient geometric scaffold and neural fields to represent appearance, thereby reducing memory. To account for transient scene geometry variations caused by weather, seasons, and other factors, the authors introduce an opacity attenuation field that modulates the scene geometry. For modeling dynamic actors in the scene, an object-centric representation is used, with a non-rigid deformation in the canonical space to animate objects such as pedestrians. Experiments demonstrate that the proposed method achieves state-of-the-art performance while rendering faster than previous methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
* The paper is well-written and easy to follow.
* The decomposed representation of appearance significantly reduces memory usage.
* It models transient scene appearance and geometry, as well as non-rigid objects like pedestrians.
* The evaluation and ablation study are comprehensive.
* The paper demonstrates visually superior results compared to baselines such as SUDS and ML-NSG.

Weaknesses:
* The rendering of the proposed scene representation requires query appearance from the neural fields, it is unclear whether this will impact rendering speed compared to spherical harmonics representation.
* This paper lacks a comparison with recent neural field baselines such as UniSim and NeuRAD for urban driving scenes. Additionally, there is no comparison of the speed to 3D Gaussian baselines.
* How to control the non-rigid objects in the scene? e.g., animating the pedestrians given a sequence of human poses.
* Is it feasible to render other sensor modalities in autonomous driving, such as LiDAR?

Limitations:
The authors discussed the limitations of modeling other sensor phenomena such as rolling shutter effects, motion, and more complex camera models. They also discussed the broader societal impacts of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper works on novel view synthesis (NVS) for large-scale, dynamic urban scenes. This paper proposes a neural scene representation called 4DGF, which uses 3D Gaussians as an efficient geometry scaffold while relying on neural fields as a compact and flexible appearance model. The proposed method integrates scene dynamics via a scene graph at global scale while modeling articulated motions on a local level via deformations. The method significantly outperforms baselines in terms of speed and rendering quality on three benchmarks.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The idea of combining Gaussian Splatting and neural fields to model geometry and appearance, respectively, is very interesting. This makes a lot of sense considering the efficiency and the advantages of each of the two representation. This is definitely a more scalable approach to large-scale scenes compared to prior work.

2. Extensive experiments have been conducted to validate the proposed method, this includes comparing with recent baselines on three benchmarks and the ablation studies that carefully examine each component. Moreover, the rendering quality improvement and the speedup is very significant on all three datasets.

3. The paper is very well-written and easy to follow. Implementation details are sufficiently discussed for reproducibility.

Weaknesses:
1. I appreciate the authors' including a video in the submission. I found sometimes there's a large foggy region near the camera (e.g., the regions on the right during the 5-6th second), do the authors have any explanations on that? Is it caused by any limitations discussed in Sec. 5?

2. I understand that this paper mainly focuses on large dynamic scenes. I am curious how this hybrid representation performs on 3D statics scenes (e.g., the benchmarks that the original 3DGS have been tested on). This seems to be a more straightforward way to see the effect of using neural fields instead to model appearance.

Limitations:
The limitations seem to have been sufficiently discussed.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
up0qqbdeQu;"REVIEW 
Summary:
The paper supplies a post hoc method to tune the ResNet based CLIP method on multi-label recognition task. Firstly, the method includes class concept representation, which is an alternative of the default prompt “The photo of a {class}”. It is the average of class description sentence embedding from a text description source (MSCOCO and git3.5 generated caption in the paper). Secondly, the paper proposed a sequential attention to iteratively transfer the the visual features to align with the class concept representation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Training-free enhancement: The proposed method significantly improves zero-shot and prompt-tuning performance without requiring additional training or labeled samples, making it computationally efficient.
2. Robust performance: Experimental results on multiple benchmark datasets (MS-COCO, VOC2007, and NUS-WIDE) show substantial performance gains, demonstrating the method's effectiveness.

Weaknesses:
1. Lack of clear differentiation: While the authors mention TaI-DPT and claim differences, the paper does not clearly articulate the advantages of the proposed method over TaI-DPT, leaving the comparative benefits ambiguous.
2. Unfair comparison in Table 5: The Class Concept Representation is based on the MS-COCO dataset, making direct comparisons with the baseline CLIP method potentially unfair due to inherent advantages provided by the dataset-specific information.

3. Limited model implementation: The paper only implements the ResNet-based CLIP model and does not explore transformer-based CLIP models. It is unclear whether the method is ineffective for transformer-based models or if there are specific reasons behind this omission. This limits the generalizability of the findings.
4. Ambiguous terminology: The paper uses the term ""training-free"" in its title, yet it describes the approach as ""test-time adaptation"" within the content. This inconsistency can lead to confusion about the nature of the proposed method.
5. Dependent on source text description: It seems that text description source need to be carefully selected.  A comparison of different description dataset can be interesting.

Limitations:
see disadvantage and question

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a class concept representation for zero-shot multi-label recognition in a label-free manner and introduces a context-guided visual feature that enhances the alignment of the visual feature of VLM with the class concept.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	This paper presents a novel class concept representation for training-free multi-label recognition tasks using VLMs from massive text descriptions inspired by how human forms concept on words.
2.	This paper proposes a context-guided visual feature, which is transformed onto the same text feature space as class concepts using sequential attention, to better align multi-modal features.
3.	The method presented in this paper synergistically enhances the performance of ZSCLIP and other state-of-the-art just-in-time tuning methods, with a minimal increase in inference time.

Weaknesses:
1. Tip-adapter is the proposed training free method in 2021, it would be better to choose the newer training free method in few shot setting.
2. It would be more appealing to emphasize label-free in the abstract.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a method to adapt without training a large vision-language model for the task of multi-label recognition. They introduce a class concept representation, based on averaging the representation of image descriptions relevant to each class, to replace simple hand-crafted text prompts (e.g., “a photo of {class name}”). Furthermore, they propose to use a context-guided visual process to align visual features with the class concept representation. Experiments conducted on several benchmarks and in zero-shot and partial labeling settings show state-of-the-art performance compared to relevant baselines. Combination with some baseline methods further shows the improvements that can be obtained with the proposed method. Ablation studies show the contribution of each component of the method and the sensitivity to some of the method's parameters.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method achieves state-of-the-art performance
- The proposed method does not require training and can be seen as a form of test-time adaptation
- The method can be combined with existing prompt-tuning methods

Weaknesses:
- Parts of the method descriptions, especially the Context-Guided Visual Feature, are unclear
- The method relies on thousands of text descriptions relevant to the target classes, which could hinder the scalability of the methods with a large number of classes

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes class concept representation for zero-shot multi-label recognition. The paper also proposes context-guided visual representation, which is in the same linear space as class concept representation, with sequential attention. Experiments show the proposed methods improved the performance of zero-shot methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper uses class concept representation for training-free multi-label recognition tasks
2. The paper proposes context-guided visual feature using sequential attention.
3. Experiments show the proposed methods improved the performance of zero-shot methods.

Weaknesses:
1. The class concepts from averaging the vectors of text descriptions need to be verified. E.g. What text/image embeddings are the closest to the class concepts? What clusters do the concepts belong to? Since taking the average for class concepts ""was guided by the prior work on prompt ensembling [4]"" L280, it is not a novel representation for class concepts. 
2. Eq 2,3 needs further explanation. What is ""t"" in the equation? If ""t"" is transpose, what dimensions are swapped for a tensor T? Take k=1 as an example, how do the dimensions change in each step of the equation? In experiments, there should be ablation studies on G and the value of each Mg. Also, is T randomly reshaped? It would be better to have ablation studies on random reshaping or reshaping by clusters.
3. What is the implementation detail for partial label learning with the proposed method?

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
l0c1j4QvTq;"REVIEW 
Summary:
The paper proposes DACER — an actor-critic that uses the inverse diffusion process as policy. Additionally, some noise is added to actions to increase the entropy (similar motivation as in SAC but implemented differently). Empirical evaluations and ablations show that the method is quite capable.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Originality: good. The paper proposes a novel method how to use diffusion as policy representation for a SAC-like algorithm.

Quality: good. The method is presented and evaluated thoroughly.

Clarity: excellent. The paper is easy to follow.

Significance: good. The method will be of interest to the RL community.

Weaknesses:
1) No comparison to an algorithm with multimodal policy. The key advantage of the diffusion policy compared to the usual Gaussian is its multimodality. But there are other algorithms with multimodal policies, e.g., [Reinforcement Learning with Deep Energy-Based Policies](http://proceedings.mlr.press/v70/haarnoja17a.html?ref=https://githubhelp.com) and many newer works (check ""Cited by"" on that paper).
2) No clear demonstration where multimodality helps. Some toy task or better not-toy task where one sees a clear advantage would be nice to see.

Limitations:
Please add a paragraph discussing the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel method using diffusion models as a policy parameterization for online reinforcement learning. The method works by learning a Q function and backpropagating through the reverse diffusion process in order to update the diffusion policy weights, similarly with Diffusion-QL. In order to aid exploration, the authors estimate the entropy of the policy and use that to update $\alpha$, which is used to adjust the noise added to actions after sampling.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper's initial results are strong and I think with further experimentation it could be a strong paper
 - Few papers have studied the application of diffusion policies to online RL, and this paper presents a fairly straight-forward method in that direction which seems to achieve strong performance compared to baselines.
 - The presentation of the method is generally easy to understand
 - The use of GMM to estimate the entropy of the diffusion policy is a novel and creative trick

Weaknesses:
- A glaring issue with the paper is that the authors chose not to compare to any other papers using diffusion policies for online RL. They claim without citation that these papers perform worse than SAC although these papers claim to outperform SAC. In order to fully contextualize this paper I feel that these comparisons are essential.
 - As I discuss in the following section, several key results are missing.
 - The presentation is generally pretty sloppy as I discuss at several points in the following section.
 - I worry that the method may be very slow, and that the gains in performance may not be enough to justify the added computational cost. It would be useful to see some data about the comparative wall clock times of the algorithm and baselines.

Limitations:
The authors discuss the limitations of their work, although they do it in the appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes using the reverse diffusion process as the policy for actor-critic-based online reinforcement learning. An EM-based mixture model is fitted to estimate current diffusion policies' entropy to balance exploration and exploitation. The proposed method, DACER, demonstrated on-par or improved performance compared with RL with deterministic and single-mode Gaussian policies in various tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. It is the first work that I know of directly using online RL to train diffusion models by directly back-propagating through the diffusion chain.

2. The proposed method performed on-par or better than conventional RL methods with deterministic or single-mode Gaussian policies in some continuous control tasks.

3. The writing of the paper is easy to follow, and the contributions are clearly outlined.

Weaknesses:
1. My major concern is that the calculation of entropy using GMM seems not incorrect; Eq. 15 is not equivalent to the entropy of GMM presented in Eq. 12. GMM does not have a closed-form solution for its entropy [1]. However, it could be approximated in various ways. If the author used any of the approximations, the reference should be mentioned in the paper. Furthermore, in the ablation study (Figure 4) comparing DACER with linear decay entropy, the entropy regularization does not show much performance difference. Considering the extra compute overhead by applying EM in the inner loop, I’m not convinced it is worth the effort.

2. Another major concern of mine is that the proposed method seems to be very computationally heavy; for each policy update, the gradient needs to backpropagate through the entire diffusion chain. Furthermore, an inner-loop EM algorithm needs to be implemented to approximate the policy entropy. Although the training time for Humanoid-v3 is mentioned in section 5, there is a lack of comparison with other baselines implemented in the same framework (JAX) and the same hardware, which is necessary for understanding the extra computational cost.

3. DACER does not show significant performance improvement over DSAC in most environments except Humanoid-v3 and Ant-v3. Given the large computational overhead, it is not sure if it is really worth using the method in given tasks.

4. In Figure 1, the iteration of PPO and TRPO are reported in the number of network update steps, which is uncommon and needs proper justification, as the number of network updates does not necessarily reflect the number of simulation steps for these two algorithms.

5. I am also concerned that the policy representation experiments can not fully show the representation power of diffusion policy. Diffusions are models with rich representation power as they can capture multi-modal action distribution [2] at the same state. For the task with 2d state space presented in Fig. 2, the multi-modality in action distribution only exists in the state of (0, 0), which is also why single-mode policies like DSAC and TD3 could also partially solve the task. Although the result of DACER looks slightly better than DSAC, I’m not fully convinced that this result suggests a better representation power for diffusion policy.

[1] Robin S, Scrucca L. Mixture-based estimation of entropy. Computational Statistics & Data Analysis. 2023 Jan 1;177:107582.

[2] Jia X, Blessing D, Jiang X, Reuss M, Donat A, Lioutikov R, Neumann G. Towards diverse behaviors: A benchmark for imitation learning with human demonstrations. arXiv preprint arXiv:2402.14606. 2024 Feb 22.

Limitations:
The computational complexity is the main limitation of this work. Although it has been mentioned in the appendix, I feel like more discussion and evaluations are needed to better understand this limitation.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose DACER (Diffusion Actor-Critic with Entropy Regulator), an online RL algorithm that uses the reverse diffusion process as a policy in order to capture multimodal behaviours. To balance exploration and exploitation, the authors propose carrying out entropy regularization by estimating the entropy of the diffusion policy using a Gaussian mixture model. The paper demonstrates the proposed method on a set of simulated control tasks, comparing against a suite of established RL algorithms.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- To the best of my knowledge, the method proposed in the paper is a novel combination of new techniques – online actor-critic methods and diffusion models.
- The paper also discusses relevant offline and online RL algorithms that make use of diffusion policies for control, clarifying the differentiating factors of this work. 
- The paper compares against a suite of well-established RL algorithms on a range of simulated control tasks (ranging from simple to complex). The experimental results show that the proposed method generally leads to improvements in learned policy performance, with additional studies showing the multimodality of learned policies and ablations on each component of the proposed method.
- Learning multimodal policies is of significance to the RL community, and the proposed parametrization of entropy regularization for diffusion policies seems likely to be built on in subsequent work. The authors release relevant code, enabling future work to build upon this paper.

Weaknesses:
- The related works section would be strengthened by adding in references to online RL algorithms used for image generation (e.g. ""Training Diffusion Models with Reinforcement Learning"" (Black et. al, 2023), or ""DPOK: Reinforcement Learning for Fine-tuning Text-to-Image Diffusion Models"" (Fan et al., 2023)). While these methods are applied in an image generation setting rather than direct control, the methods for policy improvement are related (i.e. optimizing for maximum reward with a policy based on the reverse diffusion process, learned by backpropagating through the diffusion chain), with some differences (not using actor-critic methods, learning a q-value, etc.). 
- It would be more meaningful to see Table 1 as the mean of returns in the final 10% of iterations, rather than mean of *highest* returns, the latter which may be skewed by luck/noise in the evaluation.
- It would be interesting to see some measures of how computationally efficient the method is – since it requires stepping sequentially through the diffusion process to sample an action at each step. What are the tradeoffs?
- Rather than method iterations, the results in Figure 1 could be better contextualized by also looking at environment steps on the x-axis (i.e. how sample efficient is each method), since the meaning of “iteration” can vary per method.
- Minor note: The definitions of eq. (1) and (2) could be written more clearly (i.e. how do (s,a) affect the rewards on the right hand side of the equation?)
- Minor note: for clarity, the definitions of $\mathcal{L}_\pi$ and $\mathcal{L}_q$ should be stated.
- Minor note: figures 3,4,5 should have more descriptive captions.

Limitations:
The authors should include a section on the limitations of the proposed methods (e.g. computational inefficiencies?)

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
lYPAYmfQqm;"REVIEW 
Summary:
The paper studies the in-context learning capabilities of linear attention (ATT) and linear state space layers (SSM) on a linear regression task. It shows that for both ATT and SSM there exists a parametrization such that they perform as well as one step of preconditioned gradient descent (PGD) with optimal preconditioning in expectation, and that they cannot perform better.

The authors then propose a model for retrieval-augmented generation where in-context examples and the final query are correlated. For PGD, they provide an approximate expression for the optimal preconditioning weights and the related loss and relate it to the optimal weights in the noiseless, i.i.d. case, showing how the correlations between in-context examples and query translate into an increased effective sample size.

Next, the authors give an upper bound on the optimal expected loss (for 1-step PGD <=> ATT <=> SSM) in a low-rank adapter setting.

Experiments validate the theoretical results. 

A final experiment shows some initial results hinting that the SSM model studied (H3) performs slightly better than linear attention in an online setting and with a non-stationary task.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
In terms of results, the main original contribution is the analysis of the model with correlated training examples and query points. However, the analysis of the SSM model in the appendix seems non-trivial and should therefore also be considered as a significant element of originality.

The section on related work is well-written and covers relevant work.

The problem setup is very clean, making it very easy to read and understand the results. All results are clearly stated with the necessary assumptions and accompanied by clear explanations.

The questions studied are highly relevant to contemporary machine learning research and applications of AI.

Weaknesses:
* The main result (12) is not stated as a theorem, and the derivation in the appendix is not completely rigorous. In particular, it is not clear under which conditions the approximation in l.558 is justified. Unless the conditions for the approximations to be valid are stated more clearly, it is difficult to extrapolate from the experimental results to the full claim (A2).

* The result on LoRA is not interpreted, and it is not immediately clear what the implications are from the statement of the result. I am not sure if the result implies claim (A3).

* The additional experimental results (Fig. 3) seem somewhat preliminary and detached from the rest of the paper. Based on these results, it is not clear that the part of claim (A1) claiming an advantage of H3 is fully supported by the evidence.

Limitations:
The authors address the limitations of their work and acknowledge that their analyses are ""not precise and fully formal"".

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors examine the capabilities of Transformers with linear attention in performing in-context learning (ICL) by implementing a linear estimator through gradient descent. The existing studies mostly consider IID task and feature vectors and fully parameterized attention weights. This work expands on these studies by analyzing:

1. The landscape of 1-layer linear attention and 1-layer H3 (a state-space model), proving both can implement 1-step preconditioned gradient descent.

2. New risk bounds for retrieval augmented generation (RAG), showing benefits from distributional alignment.

3. The optimal risk for low-rank parameterized attention weights, illustrating how LoRA adapts to new distributions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors provide a comprehensive theoretical analysis of the landscape for ICL, extending existing results to more practical settings. It offers new insights into the benefits of distributional alignment and the capabilities of low-rank parameterization in attention mechanisms. The theoretical findings are supported by experimental results, enhancing the credibility of the conclusions.

Weaknesses:
1. The analysis is limited to single-layer linear attention and linear distribution data, which might not fully capture the complexities of multi-layer architectures. 2. The RAG and LoRA analyses are not precise and fully formal.

Limitations:
As mentioned in Weakness: 1. The analysis is limited to single-layer linear attention and linear distribution data, which might not fully capture the complexities of multi-layer architectures. 2. The RAG and LoRA analyses are not precise and fully formal.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies how transformers can use ICL to solve linear regression problems. It is shown that state space models, transformers are both capable of performing linear regression as well as gradient descent (which implements the least squares solution). There are results about LORA and RAG.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
The presentation is clear.

Weaknesses:
The analysis of vanilla regression is not novel, I think, and has been done for instance in Theorem 1 of Ahn et al (transformers learn to do preconditioned gradient descent) where it is even shown that a pretrained transformer (with GD) learns to do this in a similar setting (which is stronger than showing that it can learn it). Maybe for SSMs this result is new but I am not sure how strong of a contribution this is considering that it only establishes the existence of such a solution.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
dE1bTyyC9A;"REVIEW 
Summary:
The paper proposes a unified 3D segmentation frame work for six 3D segmentation tasks. It enhance the performance through building the inter-task connections. The model could achieve the state-of-the-art performance in individual tasks even comparing to the models specialized for individual tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed model unifies 6 different 3D segmentation task in one framework, with no modules specialized for specific task. The authors build explicit inter-task relations to further improve the performance of individual tasks. Detailed experiments and comparison are reported.

Weaknesses:
1. The writing needs improvement. Some notation and writing flow are confusing. For example, K_v in line 155 is not introduced. The multiplication in equation (3) is not clear (is it a matrix multiplication?). Positive sample and negative samples are not introduced in section 3.2 (line 206). mask_pos in equation (6) is not introduced.
2. No explanation on how to compare models/design choice when the evaluation metrics for different tasks are not consistent. For example, if model A is better in interactive segmentation while model B is better in referring segmentation, how to compare?
3. Without the extra fine-tunning trick, the proposed model has a worse performance over previous SOTA OneFormer3D [16] on SS and IS tasks (table 2). According to Table I and IV in appendix, the performance of the proposed method is questionable.
4. The authors empirically find the interactive segmentation is the best task for mask predictions (line 57-58) but no analysis on the reason. Only one value is reported in table 1, which could not validate interactive segmentation has superior performance.

Limitations:
The paper addresses the limitation and claims there is no societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes UniSeg3D, a framework to unify 3D point cloud segmentation tasks. Compared to previous work that unifies 3 tasks, UniSeg3D additionally incorporates interactive segmentation, text-referring segmentation, and open-vocabulary segmentation. In total, six tasks are unified in a single Transformer decoder architecture, and techniques such as knowledge distillation, contrastive learning, and two-stage fine-tuning are applied to boost performance. Consequently, UniSeg3D achieves comparable or superior performance against existing state-of-the-art baselines.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Novel architecture for incorporating six segmentation tasks under one single model, which appears to be effective and flexible for extending to additional tasks.
2. Solid experiments. The authors conducted experiments on 3 datasets and various ablation studies to demonstrate the effectiveness of the proposed method.

Weaknesses:
1. Marginal performance gain. While referring segmentation and interactive segmentation tasks enjoy noticeable improvements in UniSeg3D, the performance of generic segmentation tasks seems to be on par or worse than training alone (see Table 1). This led to questions about the motivation for the proposed unification. Further justifications, such as the standard deviation of the performances, could make the results more convincing.
2. Additionally,  in Table 6, unifying the additional 3 tasks turns out to hurt the performance of generic segmentation tasks. How do the authors justify their motivation?
3. Some minor issues in writing and notations. See the Questions below.

Limitations:
The authors have discussed their limitations in Section 5. Given the paper is titled A Unified Framework for 3D Scene Understanding, the reviewer would like to point out that point cloud segmentation is one aspect of 3D scene understanding, so it would also be worth discussing how such a unified segmentation framework can potentially help other 3D understanding tasks.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes UniSeg3D, a unified framework for six 3D segmentation tasks that achieves SOTA results on the six tasks. The authors propose to use knowledge distillation and ranking-based contrastive learning to enhance inter-task knowledge sharing and the overall performance. Extensive experiments are done to prove that UniSeg3D is powerful. Comprehensive ablation studies are performed to prove the effectiveness of the design.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. UniSeg3D is the first framework that unifies six tasks in 3D segmentation, and the comprehensive experimental results prove the effectiveness of the design.
2. Interactive segmentation-guided training is insightful. Analysis of its impact on inter-task formulation is comprehensive, enlightening future directions.
3. Well-written manuscripts with illustrative figures.

Weaknesses:
1. Since the feature for visual prompt is sampled from the superpoints, the quality of the visual prompt significantly influence the overall performance of the model.
2. The experiments are only conducted on ScanNet-based datasets, as a unified model, the authors should provide more experiments on different datasets to validate the method as done in previous works[1][2].
3. Unlike previous works[2][3], UniSeg3D directly learn the relation between text and 3D without any 2D supervision or guidance. It is a concern that how ""open"" is this framework in OVS task. Some visualized experiments on open-set text queries as in [2][3][4] could answer the question.


[1] Zhu, Ziyu, et al. ""3d-vista: Pre-trained transformer for 3d vision and text alignment."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.
[2] Nguyen, Phuc, et al. ""Open3dis: Open-vocabulary 3d instance segmentation with 2d mask guidance."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.
[3] Takmaz, Ayça, et al. ""Openmask3d: Open-vocabulary 3d instance segmentation."" arXiv preprint arXiv:2306.13631 (2023).
[4] Peng, Songyou, et al. ""Openscene: 3d scene understanding with open vocabularies."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2023.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
For the first time, this work proposes a unified model for several point cloud segmentation tasks, including panoptic, semantic, instance, OV, interactive, and referring segmentation. This work uses the typical query-based transformer perception architecture with the proposed knowledge distillation losses for superior performance, outperforming previous methods, which shows the effectiveness of the designed model.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This work addresses an important problem of 3D scene perception with a unified model and achieves great results.
2. The proposed model is simple and effective.
3. The overall writing is fluent and clear.

Weaknesses:
1. The main weakness of this work is that the proposed architecture is widely used for multi-modal perception tasks, limiting the model’s novelty. However, this is acceptable as long as the model’s performance is indeed great, as simple and effective models are usually similar.

2. The paper claims knowledge distillation losses as one of its contributions. However, according to the ablations, adding these losses only makes marginal changes.

Still, I think this work has a solid contribution to the field (if open-sourcing the codes with reproducible results) and should be accepted.

Limitations:
See above.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
Mdd3f8Cui8;"REVIEW 
Summary:
This paper proposes a framework to augment latent features from observed features, with the help of LLM. They frame the problem as a text-to-text reasoning problem.  The method can be adapted to different domains easily.  The method is also validated with a real world dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall, the presentation and logic flow are smooth and clear.  The methodology is also reasonable to me. And their experiments also validate the effectiveness of their method.

Weaknesses:
The key concern for me is that when using the LLM for inference and text generation, I worry about the social bias and fairness of the problem. Some research has shown that LLM is still biased in some sense, can the author conduct some evaluation on whether the latent feature is biased towards some sensitive attributes like race, gender, etc?

The other thing is that I wonder how much human labor effort and expert labor effort will be needed to have the latent features. 

Typo in line 203

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a unique form of LLM data-augmentation that attempts to generate informative latent variables to improve downstream tasks. They do this by transforming the latent feature mining task into a text-to-text propositional reasoning task.
Validation is performed with a case study in the criminal justice system and latent features align well with ground truth labels + significantly enhance downstream classifier performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Clear, well written paper with descriptive diagrams
- Using LLMs to infer latent space in this way seems to be a novel idea

Weaknesses:
- Type on line 203: ""whic serve""
- Potential for LLM biases in the latent variable finding. E.g. Marijuana usage does not necessarily require Substance Abuse Treatment.
- Lack of evaluation on multiple datasets/domains and no publically released code
- Lack of ablations exploring generalizability with x\% features removed

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a framework that uses LLMs to improve predictive modeling by augmenting observed features with inferred latent features. This approach transforms the latent feature mining task into a text-to-text propositional reasoning task, enabling LLMs to infer unobserved yet crucial factors from available data. The framework is tested through a case study in the criminal justice system, demonstrating improved accuracy in scenarios where collected features are weakly correlated with outcomes.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. It addresses the challenge of limited data availability by leveraging LLMs to infer latent features, to improve predictive modeling. 

2. The approach of transforming latent feature mining into a text-to-text propositional reasoning task is interesting. 

3. The validation on criminal justice data, shows potential for broader applications. The method's generalizability across different domains with minimal customization is a significant advantage, and the reduced need for extensive human-annotated training data makes it practical and scalable.

Weaknesses:
1. The paper does not adequately address how to measure the impact of errors introduced by the LLM-based solution on predicted outcomes, nor does it provide uncertainty estimates. This process surely introduces errors, as with any ML-based solution. How do we measure its effect on predicted outcomes, including uncertainty estimates? I suspect more labeled data would be needed to assess this properly (see Egami et al. @ NeurIPS 2023).

2. It is unclear whether the approach should be viewed as a form of dimensionality reduction (based on existing features) or if it extrapolates information not present in the original data. This ambiguity raises concerns about potential bias amplification. For instance, Figure 1 shows deductions made by the model that are not clearly supported by the evidence, suggesting that the method might be amplifying existing biases rather than mitigating them.

3. Connecting to the previous point, the method's ability to learn latent information that is causally predictive of the outcome, as opposed to relying on spurious correlations, remains uncertain. Conducting an out-of-distribution test, where the characteristics of individuals differ from the training data, would be crucial in evaluating the model's generalizability and causal inference capabilities. 

4. The rationale behind not using all available data directly in the LLM for prediction is not well-justified. Directly prompting the LLM with the full data might provide more accurate predictions without the need for dimensionality reduction.

Limitations:
The authors have acknowledged limitations of their work, particularly in addressing the ethical concerns associated with data collection and the need for privacy.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper used large language models to infer latent variables that are important for downstream prediction tasks to augment the existing models. In particular, the author demonstrated the use of the proposal on a criminal justice system use case, in which the LLM-mined-latent features significantly boost the prediction performance.

Overall the paper presents an interesting question and how LLM could help mining for latent features, but I have several questions regarding 1. the generalizability of the proposed method; 2. Appropriate combinations/baseline methods; and 3. The potential ethical implications of this method. I will detail these points in the strength/weakness sections below.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. On the outcome prediction results, using the latent feature seems to have boosted the performance by 7-10%, a big margin.
2. The proposed framework brings a level of formalism to the current crowded LLM for (social) science applications/work, including the text-to-text proposition work.

Weaknesses:
1. Despite the general initial framing in Section 3, it was not very clear how generalizable results from Sections 4-6 are — this includes not only the COT and the prompts used, but more critically the selection of what kind of latent features we are including. 
2. The current work does not seem to go into depth about what kind of latent features are LLM particularly good at constructing and which ones are particularly “bad” (eg subject to the most bias and systematic over-or-under-prediction). I think this is particularly relevant for social science applications where many of the categories and features are more of a “construct” and often qualitative in nature.
3. Have the authors compared the results by using text embeddings of the descriptions as an input feature? It’s interesting that the fine-tuning strategy is necessary for good performance, which seems to suggest that learning the intermediate classification rule is important since direct manipulation of natural language yields less impressive  results.

Limitations:
Mentioned above.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
4kCr61XYQJ;"REVIEW 
Summary:
This work extends Poisson-Gamma Dynamical Systems (PGDSs) by considering non-stationary transition dynamics to effectively capture the evolving dynamics of observed count sequences.

The authors propose a model where the underlying transition matrices evolve over time, based on three (gradually more complex and flexible) Dirichlet Markov chains.

For inference of the model, the authors make use of the Dirichlet-Multinomial-Beta data augmentation to derive a fully-conjugate Gibbs sampler.

Experiments showcase improved data-smoothing and forecasting performance of the proposed method across several real-world datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Extending PGDS models to accommodate time-varying transition dynamics is of interest and significant

- The proposed variations of Dirichlet-Markov chains provide flexibility in capturing different modeling assumptions

- Devising a closed-form Gibbs sampler for posterior inference of this model is significant.
    - The attained expressions seem correct to the best of my knowledge, although I did not carefully double-check the mathematical details of the derivation.

Weaknesses:
- The main limitation of this work is the assumption that the transition kernel is static within each sub-interval: i.e., the authors consider that the kernel can only change at discrete instants, while is constant within each sub-interval.

Limitations:
The authors address the main limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Existing PGDS models struggle with capturing the time-varying transition dynamics seen in real-world data. To address this, the submission proposed a non-stationary PGDS, allowing the transition matrices to evolve over time, modeled by Dirichlet Markov chains. Using Dirichlet-Multinomial-Beta data augmentation techniques, a fully-conjugate and efficient Gibbs sampler is developed for posterior simulation. Experiments demonstrate that the proposed non-stationary PGDS achieves improved predictive performance compared to related models.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed non-stationary Poisson-Gamma Dynamical System offers several notable advantages. 

Firstly, its ability to allow transition matrices to evolve over time addresses the limitation of state-of-the-art PGDS models in capturing time-varying transition dynamics, making it more suitable for real-world count time series. 

Secondly, the use of specifically-designed Dirichlet Markov chains to model the evolving transition matrices enhances the model’s capacity to learn non-stationary dependency structures. 

Thirdly, the application of Dirichlet-Multinomial-Beta data augmentation techniques facilitates the development of a fully-conjugate and efficient Gibbs sampler for posterior simulation.

Weaknesses:
I did not find any obvious weaknesses.

Limitations:
The authors discussed some future work directions in the conclusion.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work extends  Poisson-Gamma Dynamical systems (PGDS) to model non-stationary dynamics by replacing the constant transition matrix $\Pi$ with a time dependent one $\Pi^{(t)}$ and the original Dirichlet prior on the columns with three different Dirichlet Markov chain constructions. The manuscript describes am efficient Gibbs-sampler for inference.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The work addresses a relevant problem of modeling non-stationary dynamics in count time series. The provided extension relative to the original PGDS is sufficiently novel. I lack deep enough understanding of some parts related to the sampler, therefore I cannot assess if the construction of the sampler required new ideas or was a mechanical extension of the sampler for PGDS  (this being the main reason for my lower confidence score.). I tend to assume new ideas were necessary.

Weaknesses:
My major problem is the experiment evaluation. In Table 1 in the NIPS dataset we can see results like $14.014 \pm 4.387$ bolded, over values like $14.706 \pm 4.414$, or $17.105 \pm 6.449$. 
In ICEWS values like $0.214 \pm 0.008$ over $0.215 \pm 0.007$ , in USEI $4.596 \pm 0.562$ over $4.703  \pm 0.538$, in COVID $6.969 \pm 1.107$ over $7.566 \pm 1.095$. These are mainly smoothing results. In the light of this I am not confident in the statement “As the experiment results shown in Table 1, the NS-PGDS exhibits improved performance in both data smoothing and forecasting tasks.”.  We do not know how the confidence interval was computed, or how many repeats were made. The lack of statistical rigor in the evaluation stands in striking contrast with the sophisticated Bayesian model presented. 

Besides this, other possible problem with the evaluation is that the manuscript states that default paramerters were used for the benchmark methods “GP-DPFA, PGDS, GMC-RATE, GMC-HIER, BGAR”  while the present method used specific K based on the dataset. It is very hard to tell if this is a fair comparison or not.

Limitations:
No specific limitation section was provided. The part on future work in the Conclusion can be interpreted as pointing out some limitations of the current model, but a specific limitation statement would be preferable.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces non-stationary Poisson-Gamma dynamical systems, an extension of Poisson Gamma dynamical systems with a dynamic transition matrix. Decomposing the time steps into equally spaced subintervals, the transition matrices evolve between sub-intervals, remaining static within sub-intervals. The authors introduce three options for transitions to occur. The authors derive a Gibbs sampling scheme for exact posterior inference using data augmentation techniques and showcase the effectiveness of their method through a series of predictive and qualitative results.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
This is a well-written, organized paper that is easy to read. The proposed method allows for exact posterior inference through Gibbs sampling. The authors exhibit extensive predictive results across 4 datasets, although their method only exhibits marginal improvement as compared to Poisson Gamma dynamical systems.

Weaknesses:
I'm not convinced that the magnitude of the author's contribution, nor the significance of the paper is strong enough to warrant acceptance, and the methods produce only marginally better results than that of Poisson Gamma dynamical systems. The qualitative results are not groundbreaking.

Limitations:
The authors address the limitations of their work, stating intention to address these limitations (e.g. constant sub-interval lengths) in future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
PZCiWtQjAw;"REVIEW 
Summary:
The paper proposes a novel continual audio-visual sound separation task, aimed at continuously separating new categories of sound sources while maintaining the performance of previously learned categories.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The structure of the entire paper is clear, and the expression is fluent.
2. The experimental results can demonstrate the effectiveness of the method, and the quality of the separated audio shown in the visual video is also good.

Weaknesses:
1. The Cross-modal Similarity Distillation Constraint proposed in the paper includes two main innovations: 1) instance-aware semantic similarity and 2) class-aware semantic similarity. However, in terms of mathematical expression, compared to the Dual-Audio-Visual Similarity Constraint in reference [1], I believe the innovations of the two papers are almost identical.
[1] Pian W, Mo S, Guo Y, et al. Audio-visual class-incremental learning[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 7799-7811.
2. Although the paper has conducted experimental validation on the MUSIC-21 dataset, it does not provide sufficient information to assess the model's generalization ability on other datasets or in real-world scenarios, making it difficult to demonstrate its enhanced applicability in real scenarios where new sound sources are encountered.

Limitations:
The innovation of the Cross-modal Similarity Distillation Constraint in this paper may overlap with the Dual-Audio-Visual Similarity Constraint from the previous work. It suggests that while the paper introduces a novel approach, there might not be a clear distinction or significant difference from the existing method in terms of the core innovative aspects.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel task termed ""Continual Audio-Visual Sound Separation,"" aiming to address the practical challenge of separating sound sources for new classes in audio-visual scenarios while retaining performance on previously learned classes. This task is inherently challenging due to the inherent risk of catastrophic forgetting, where models trained on new data often exhibit performance degradation on previously learned classes. To tackle this challenge, the authors propose ContAV-Sep, a novel framework incorporating a Cross-modal Similarity Distillation Constraint (CrossSDC). This constraint preserves cross-modal semantic similarity across incremental tasks by enforcing both instance-aware and class-aware similarities through a combination of contrastive loss and knowledge distillation. Notably, CrossSDC integrates knowledge from past tasks into the contrastive learning process, ensuring the retention of previously acquired cross-modal correlations. Experiments on the MUSIC-21 dataset demonstrate that ContAV-Sep significantly outperforms existing continual learning baselines in terms of standard sound separation metrics (SDR, SIR, SAR) across multiple audio-visual sound separation base models. This work highlights the importance of cross-modal semantic correlation in continual audio-visual learning and provides a novel, effective solution for mitigating catastrophic forgetting in this domain.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Here are four strengths of the paper, presented as a list:
- Clearly identifies and addresses a novel and practical problem in audio-visual sound separation: continual learning in this domain. It is important to mention that this might be an important problem in the field and not so many people have provided good solutions for this problem.
- Proposes a novel framework, ContAV-Sep,  with a well-defined Cross-modal Similarity Distillation Constraint (CrossSDC) to tackle catastrophic forgetting.
- Empirically demonstrates the effectiveness of ContAV-Sep on the MUSIC-21 dataset, showcasing significant performance improvements over strong baselines.
- Provides a thorough analysis of the results, including ablation studies and exploration of memory size effects, highlighting the contributions of different components of the proposed method.

Weaknesses:
- Please replace the “mask” variable with something appropriate like \widehat{\mathbf{m}}
- Please fix weird fontsizes like the one in equation 9, in general the manuscript does not seem polished enough for a NeurIPS submission.
- Differences smaller than <0.1 dB in terms of SNR metrics are not either significant nor hearable (I would even argue for 0.5 dB but let’s follow the literature in this one), thus I would suggest rounding up all those performance numbers to a one decimal precision (it would also make the Tables less cluttered).
- I would like to see an even larger ablation in Table 3 to show the full extent of how the performance deviates with an even larger amount of samples per class and not only 4, a graph would make the visualization better here cause currently it does not convey any meaningful message. 
- The authors do not make a thorough investigation on previous works in the literature that employ continual learning techniques for related sound processing tasks. I will refer here only a couple of the works that I am aware of like in [A, B] but I am almost certain that there is no lack of thereof to try to include other works and try to make some empirical or theoretical conclusions on how those methods can become interconnected, employed together and in general how they relate.



[A] Wang, Z., Subakan, C., Tzinis, E., Smaragdis, P. and Charlin, L., 2019, October. Continual learning of new sound classes using generative replay. In 2019 IEEE Workshop on Applications of Signal Processing to Audio and Acoustics (WASPAA) (pp. 308-312). IEEE.

[B] Wang, Z., Subakan, C., Jiang, X., Wu, J., Tzinis, E., Ravanelli, M. and Smaragdis, P., 2022. Learning representations for new sound classes with continual self-supervised learning. IEEE Signal Processing Letters, 29, pp.2607-2611.

Limitations:
I think the authors include several limitations of their work, they should also identify the memorization of individual user’s data inside the memory of their continual learning method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces ContAV-Sep, the goal is to continuously separate new sound classes while maintaining performance on previously learned classes, addressing the challenge of catastrophic forgetting in continual learning. ContAV-Sep employs a Cross-modal Similarity Distillation Constraint (CrossSDC) to preserve cross-modal semantic similarity across tasks and retain old knowledge, integrated into an audio-visual sound separation framework.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method is a good solution to help maintain cross-modal semantic similarity across incremental tasks.
2. The paper has a clear writing structure, and the figures and tables are easy to understand.

Weaknesses:
1. The performance improvement of ContAV-Sep (with iQuery) is not significant.
2. What does iSTFT stand for in Figure 2? The symbols appearing in the figure need to be clearly explained in the caption.

Limitations:
See weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This approach defines an audio-visual sound separation task where sound separation is the task and during fine-tuning novel classes are added, in the regime of continuous learning. The goal is to avoid catastrophic forgetting which typically leads to decreased performance in task performance on classes which were learned early but then little or no instances arrive in later training stages. They then present a method to solve this task with audio-visual data. Several losses are defined to achieve cross-modal similarity of embeddings through incremental tasks and preserve previous learned knowledge. 

Their approach does not increase the number of logits to be able to accommodate more classes but instead creates separation masks, more suitable to audio separation than typical classification tasks.

Results are compared on meaningful baselines even though this is basically a new task definition and ablating the different loss components shows that the combination yields the best performance.

Additional videos and investigations in the supplementary work complete the work.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Especially in the videos the performance on very similar instruments is impressive. The instruments share a very similar frequency profile and their separation is a very hard problem.

The paper is very clearly written, the task is clearly stated and well contrasted and localized within the domain.

The mathematical notation is very clear to follow.

In general the paper defines an interesting and realistic task, does a thorough investigation and states clearly its limitations.

Weaknesses:
The paper defines a task and then solves it, which is always a bit easier due to limited competition. A direct comparison between sound separation was excluded by the authors stating that they do not compete in that sense, which is a limitation.

Figure 2 is very small and could be more self complete. The relation of the right side's illustration to the left side is not clear from the caption at all which only states the names of the concepts. Also, there is space left and right to increase the size. Probably this resizing was reduced to gain space. This leaves the text very small. It would be good to find space in a different way because the fonts are already extremely small within Figure 2. Some of the dataset details could probably go into the appendix.

This is not a summary paper. That the author's found so much related work is commendable but I would have preferred more detailed contrast to existing work, e.g. by picking out certain ideas and contributions, explaining them and then contrasting this work against them. Just having lists of 7 works and summarizing them as ""semantic segmentation"" seems more like a homework chore than a contribution to the paper. But my fellow reviewers may disagree.

Limitations:
The limitations are properly listed and the authors are upfront about them.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes a continual audio-visual sound separation framework to mitigate the catastrophic forgetting problem

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
As I learned, this is the first work that focuses on the catastrophic forgetting problem in audio-visual separation task.

Weaknesses:
1. The font size in Figure 2 is too small, reducing the readability of this paper.
2. The technique novelty is limited, the model architecture is totally the same as iQuery, and the most brightness point of this work is just proposes a cross-modal similarity distillation constraint, however, it just the combination of the contrastive loss implemented on the modalities and features extracted from different training step.
3. Experiments are not enough, all experiments are conducted on Music21, which just contains limited data among 21 classes. Experiments conducted on Music [1], VGGSound [4], and AVE datasets [2-3] can provide a more comprehensive evaluation.
[1] The sound of pixels.
[2] Audioset: An ontology and human-labeled dataset for audio events.
[3] Audio-visual event localization in unconstrained videos
[4] Vggsound: A large-scale audio-visual dataset

Limitations:
1. Limited novelty.
2. Insufficient experiments

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
u1b1dJtyxc;"REVIEW 
Summary:
The paper studies the existing ""brain score"" approach of evaluating how similar LLM representations are to human brain activity. First, they show that when using shuffled train-test splits on the Pereira dataset, which some prior studies use, a trivial temporal auto-correlation model performs similarly to GPT2-XL. Second, they show that untrained GPT2-XL's brain score is simply due to encoding sentence length and position. Third, they show that a trained GPT2-XL's brain score is largely explained by sentence length, sentence position, and static word embeddings, which are all non-contextual features.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper studies the important topic of understanding why recent research has shown similarities in LLM representations and human brain language activity. 
2. They highlight issues with existing neural datasets commonly used in the LLM-brain field, e.g., shuffled train-test splits on the Pereira dataset.

Weaknesses:
From most to least significant:
1. The paper writes: ""OASM out-performed GPT2-XL on both EXP1 and EXP2 (Fig. 1b, blue and red bars), revealing that a completely non-linguistic feature space can achieve absurdly high brain scores in the context of shuffled splits. This strongly challenges the assumption of multiple previous studies [2, 11, 10] that performance on this benchmark is an indication of a model’s brain-likeness"" (Lines 173-177). 
- I agree this shows that a model that exploits temporal auto-correlation, OASM, can achieve similar neural predictivity on the Pereira dataset as GPT2-XL. However, this does not necessarily mean that GPT2-XL's neural predictivity is attributed to temporal auto-correlation, rather than linguistic similarity. It also does not tell us the proportion of GPT2-XL's neural predictivity that can be attributed to each factor. Although GPT2-XL can theoretically exploit temporal auto-correlation artifacts, it may not be empirically doing so as it was optimized for language performance instead.
- Furthermore, OASM may be a much stronger method at exploiting temporal auto-correlation than GPT2-XL's architecture is capable of. The paper's results may highlight that the Pereira dataset is easy to ""cheat"" using temporal auto-correlation, but not that GPT2-XL or other LLMs are doing so.
2. The paper evaluates ""brain score"" using a metric they defined, out-of-sample R-squared, whereas the prior research they cite [2, 24] seemed to use Pearson correlation. Although they argue for the advantage of the metric they used, it is challenging to understand how their results relate to prior research. For example, they do not show the Pearson correlation that their OASM model obtains on Pereira, which would make it easier to compare to models in prior research. Furthermore, they only tested a single language model, GPT2-XL, whereas more recent research has used larger or different models.
- Additionally, the metric they defined seems to produce results close to 0 for GPT2-XL and less than 0.05 for all models too. In Figure 2b, the R-squared results cluster around zero, with many negative values. They obtain an average R-squared value that is positive (e.g., Figure 2a?) only because they clip negative values when averaging.
3. The paper provides a theoretical justification arguing that GPT2-XL can encode sentence length and sentence position (Lines 197-201). However, this does not necessarily mean that GPT2-XL's neural predictivity is attributed to sentence length/position, rather than contextual/semantic features. It also does not tell us the proportion of GPT2-XL's neural predictivity that can be attributed to the two factors.
- They compared GPT2-XL to two ideal models of sentence position (SP, represented as a 4-dimensional one-hot vector) and sentence length (SL, represented as a scalar). However, these ideal models may be a much ""cleaner"" representation of sentence length/position than the perhaps noisy GPT2-XL representation of sentence length/position that may not be cleanly and linearly decodable.
4. The paper writes: ""GPT2-XL only explains an additional 28.57\% (EXP1) and 16.7\% (EXP2) neural variance over a model composed of features that are all non-contextual."" However, the paper does not provide a noise ceiling for the metric they defined, out-of-sample R-squared. Consequently, it is unclear whether the small improvements in neural predictivity is due to hitting the noise ceiling.

Limitations:
Limitations not mentioned in the paper:
1. Please see Weaknesses 1-4.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors investigate the simplest set of features that can explain variance in neural recordings (fMRI, ECoG) during language processing. The authors focus on the surprisingly high alignment (""brain scores"") of untrained LLMs, but also investigate trained LLMs. The authors conclude that the predictivity performance of untrained LLMs can be explained by simple features such as sentence position and sentence length. The authors quantify the effect of autocorrelation on shuffled cross-validated train-test splits and find that predictors that account for the temporal structure in the neural data explain the data better than other (linguistic) features. Overall, the study highlights the importance of understanding why LLMs (or, any feature space for that sake) map onto the brain.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is generally well-written, and the analyses are well-motivated. The topic is timely.
- The paper is very comprehensive, and provides in-depth analyses of one widely used dataset (from Pereira et al. 2018) for LLM-brain mapping studies. The paper also investigates two other datasets, but in less depth. The authors run analyses across several seeds for the untrained models, and in general, include a good amount of well-motivated control analyses.
- The analyses of trained GPT2-XL are interesting (Section 3.3), and provide a good contrast to the analyses of the untrained models.

Weaknesses:
- The authors motivate the paper with ""attempting to rigorously deconstruct the mapping between LLMs and brains"", but do not really acknowledge other work doing so. The paper lacks a short relevant work section on other studies that ask why artificial models map onto human brain data (from language, e.g., Merlin and Toneva, 2022; Kauf et al. 2023; Gauther and Levy, 2019, ...). 
- I find it very odd that the authors include ""brain scores"" in their title and also motivate the study through Schrimpf et al. 2021, but then do not replicate almost any of the analysis choices in Schrimpf et al. 2021: the feature space pooling is different, the ridge regression, the evaluation metric. For instance, sum feature pooling is motivated because ""it provides higher alignment scores"", but other studies motivate last token pooling because it is conceptually better motivated (Transformers integrate over the context). It does not feel quite right to make decisions based on ""what gives the highest alignment"", because, perhaps sum feature pooling does indeed artificially inflate scores. Either way, it is not very suitable to link the title and most of the motivation of the paper based on one instance of prior work, and then make completely different choices. That being said, the choices are definitely well-motivated in most cases, but it makes the comparison with prior work different -- which is fine, the motivation should just be changed in that case. 
- The authors should make it more clear which voxels are used in which analyses. The authors mention that unless otherwise noted, the language voxels are used (line 75), but it is not always very clear. For instance, Figure 2d clearly includes voxels across several networks.
- Regarding novelty: Kauf et al. 2023 also investigated contiguous splits on Pereira2018 as a supplementary analysis (not to the same extent as in the current paper), and also discusses the problem of temporal auto-correlation.

Limitations:
The authors discuss limitations of their study.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper paper studies the topic of neural - brain representation mappings. They focus on three neural datasetse commonly used in LLM-to-brain mapping studies: Pereira fMRI, EcoG and Blank fMRI. Specifically, the study investigates the assumptions underpinning previous positive reports about the existence of mappings between brain representations and LLM internal representations. The study focuses in particular on GPT2-XL, which was shown to perform well on the Pereira dataset in particular, with which a series of brain-activation prediction experiments are performed.

The first presented result is that when shuffled train-test splits are used, the result is very different than when contiguous train-test splits are used, with opposite patterns on which layer performs best. This is particularly true for fMRI datasets. The authors then train an orthogonal auto-correlated sequences model on the shuffled split, which out-performs GPT-2-XL despite having a completely non-linguistic feature space. The authors take this as a signal that previous results should be challenged on their conclusion that high performance on this benchmark should be taken as an indication of brain-likeness.
 
Next, the authors investigate what explains the neural predictivity of an untrained GPT2-XL model, and they fi
nd that it is fully accounted for by sequence length and position. Following-up on that, they find that these
features are also main drivers for much of the neural predictivity of a trained GPT2-XL model.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- This paper presents a detailed study into why LLM activities may be predictive of neural activities, 'debunking' several previous claims. I think there is a lot of value in this
- The experiments seem sound (though I am not an expert in this field)
- The conclusions are interesting, and contain valuable lessons for future work on this topic

Weaknesses:
- The presentation could be improved, in my opinion. I don't always find everything completely clear. For instance, the notion of 'shuffled train-test splits' is quite important for the paper, but it is never really explained how they are specifically constructed, and how they differ from their 'contiguous' counterpart. (I can imagine multiple dimension in which one could shuffle)

Presentation suggestion: I think it may work better if the results of the different datasets are grouped together, result-by-result, rather than dataset by dataset.

Limitations:
The authors adequately address limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
There is a large body of research focused on measuring the similarity between language processing in the brain and in language models. Recent studies have shown that representations from Transformer-based language models exhibit a higher degree of alignment with brain activity in language regions. However, the authors mention that this inference is valid only for the subset of neural activity predicted by large language models (LLMs).
The primary aim of this paper is to investigate this question by analyzing three popular neural datasets: Pereira, Blank, and Fedorenko. To achieve this, the authors build voxel-wise encoding models to compare encoding performance between representations from language models and brain recordings in three settings: (i) shuffled train-test splits during voxel-wise encoding, (ii) untrained LLM representations and their alignment with the brain, and (iii) trained LLM representations and their alignment with the brain. The experimental results demonstrate that untrained language models are explained by simple linguistic features such as sentence length and position, while trained language models are explained by non-contextual features (i.e., word embeddings).

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This study primarily focuses on understanding the reasons behind the better alignment between language model representations and brain recordings. The exploration of various simple linguistic and non-contextual features, and their contribution to explaining the variance in brain predictivity over contextual embeddings, is valuable for the research community.
2. The authors tested different validation setups, including comparisons between untrained versus trained models and shuffled versus unshuffled data, to evaluate brain scores. 
3. Controlling features with different combinations provided valuable insights into the contribution of each feature to the performance of brain alignment.

Weaknesses:
1. While the main research question aims to investigate the simplest set of features that account for the greatest portion of the mapping between LLMs and brain activity, the insights remain unclear for specific language regions of the brain. For instance, considering language parcels based on the Fedorenko lab, do simple features explain all the variance in these language regions? Or do these features only account for early sensory processing regions?
2. It is a well-known fact that Transformer-based representations consist of both low-level and high-level abstract features. If embeddings from language models predict brain activity and this predictivity is only due to a simple set of features, it should be better interpreted using approaches like residual analysis (Toneva et al. 2022), variance partitioning (Deniz et al. 2019), or indirect methods as suggested by Schrimpf et al. (2021).
3. Shuffling train-test splits is not an ideal scenario for brain encoding, especially for continuous language. All prior studies follow unshuffled train-test splits, i.e., contiguous time points (TRs). Shuffling the train-test split can result in sentences from the same passage being present in both the train and test sets, which is not ideal for model validation.
4. What are the implications of this study for both the AI and Neuroscience communities? What are the final conclusions?

Limitations:
Yes, the authors have presented several limitations in the conclusion. However, these limitations do not have any societal impacts on this work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the similarity between large language models (LLMs) and human brain activity by analyzing brain scores, which measure how well a model predicts neural signals. The authors question the validity of using brain scores as a measure of similarity between LLMs and human cognitive processes. They analyze three neural datasets and find that simple features like sentence length and position explain much of the neural variance that LLMs account for. They caution against over-reliance on brain scores and emphasize the need for a detailed deconstruction of what LLMs are mapping to in neural signals.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The study provides a thorough examination of how various features (simple and complex) contribute to the neural predictivity of LLMs, offering a detailed deconstruction of the relationship between LLMs and brain activity. The replication of key findings using RoBERTa-Large, in addition to GPT2-XL, strengthens the validity of the conclusions drawn regarding the generalizability of the results across different LLM architectures

Weaknesses:
1.  The methodology and findings are not particularly novel. Previous studies have already suggested that untrained LLMs can achieve good brain scores and that sentence length and position are significant predictors. Thus, two of the three core contributions claimed by the authors are not unique to this paper.
2. While the authors conclude that over-reliance on brain scores can lead to over-interpretations of similarity between LLMs and brains, it is not clear how this conclusion is drawn from the experimental results. The study itself relies heavily on brain scores to make its arguments, and the authors do not explicitly state what aspects of previous work have been over-interpreted.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
8abNCVJs2j;"REVIEW 
Summary:
The paper discusses three drawbacks of traditional N:M sparse training and suggests using soft-thresholding over hard-thresholding for 2:4 sparse pre-training. It introduces the idea of rescaling sparse weights with a fixed scaling factor per tensor. Results from experiments in machine translation, image classification, and large generative language models demonstrate its effectiveness.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.	The idea is simple and clear. The proposed approach is very easy to understand and implement.

2.	The paper discusses several important issues for N:M sparse pre-training. These discussions are very useful for practice.

3.	The experiments verify the selection of some important parameters, and also show the effectiveness of the proposed approach on several tasks.

Weaknesses:
1.	Most of the techniques are discussed in other tasks, and the paper is only applied to N:M sparse pre-training, which is not innovative enough. So the contributions are not enough for NeurIPS. 

2.	It is very important to discuss the drawbacks of existing N:M sparse pre-training, but these discussions have not led to a new algorithm. There is insufficient inevitability between these discussions and the proposed algorithm.

3.	The experiments are weak. There is a lack of important comparison methods and real training acceleration.

Limitations:
Not applicable.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a framework to circumvent the common challenges associated with STE-based 2:4 pre-training due to pruning function discontinuity. In particular, their framework addresses 3 aspects of this behavior: descent direction, amount of descent, and sparse mask oscillation.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* This paper provides a comprehensive analysis of the limitations of traditional pruning techniques, with a particular emphasis on how their framework addresses these challenges. I believe there is a novelty in the exploration of continuous pruning strategies.
* Additionally, each component is comprehensively validated – for example, the problem of mask oscillation seems to be adequately documented in Section 3.3 with a viable exemplar to demonstrate impact.

Weaknesses:
* This work is centered on the potential of continuous pruning schemes for enabling faster sparse pre-training. However, I do see a strong resemblance with conventional soft mask approaches to pruning schemes. In particular, I wonder if the authors considered typical soft-pruning works in their comparisons, and if not, what was the reasoning for that experimental setting choice.
* I found the experimental results to be quite sparse in model selection and competitive method benchmarking. Predominantly this method benchmarks against SR-STE and the dense framework, however, there isn’t enough context as to why other pruning-based methods are not included in scope. Further, there seem to be a few different tasks ablated however comprehensive details on why each application and/or model was selected are missing. For example, if demonstrating the efficacy of a sparse pre-training method it, would be beneficial to see the scaling effect on large models, say ViT-B/L for ImageNet-1K or the SWIN architectures.

Limitations:
/

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors address the challenge of pre-training models, with 2:4 sparsity, to high quality (ideally matching a densely-trained model).  Building on existing methods, the authors point out three issues with discontinuous pruning functions: gradients move in the wrong direction, weight updates do not match expectation, and values can oscillate between being masked and not-masked.  Demonstrating these shortcomings and analyzing the source leads the authors to propose two modifications to the traditional straight-through estimator approach: 2:4-specific soft thresholding and fixed weight rescaling.  The former provides a continuous pruning function, and the latter compensates for the reduced weight magnitude caused by the soft thresholding.  Targeted experiments show that the proposed method does not suffer from the three shortcomings of traditional approaches.  In full training experiments, the authors also show that applying MVUE (an existing technique) to data gradients and using FP8 representations do not interfere with S-STE's success.  Machine translation, image classification, and generative tasks are used for testing various models with 2:4 sparsity applied to the MLPs of transformer blocks and the data indicates that the resulting models are superior to baselines, and in several cases comparable in quality to the dense baselines.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
**Originality**
While the individual advancements, soft-thresholding and weight rescaling, are rooted in prior work, I think the combination and the modifications made by the authors are highly original.  

**Quality**
The experiments performed are precisely what are needed, and no more.  Each claim is supported with one clear figure or table so that the reader immediately sees that the authors considered the various angles and have shown why the angle they have selected is the best.  The breadth of full training experiments shows that the proposed method applies to image classification, machine translation, and text generation with particular success in the latter two.  Further, they combined their method with two largely orthogonal compression techniques: FP8 data representations for training and sparsifying another tensor (data gradients).  This makes the results even more compelling.

**Clarity**
I found the submissions to be easy to read in general and with an appropriate amount of detailing previous and related work.  The few issues I had were minor and listed below, and they did not detract from the well-organized paper.

**Significance**
This work is of high importance.  It has done a great deal to close the gap between dense and sparse training for a constrained type of sparsity that offers practical acceleration in readily available hardware.  If it continues to be successful in broader experiments in the wild, then it could be widely adopted, saving significant resources in training new networks and reduce the latency of performing large-scale experiments.

Weaknesses:
**Originality**
N/A

**Quality**
A missing experiment is one that would show the relative importance of scaling factor *beta*.  It would be a good addition to the ablation study presented in Table 7.

**Clarity**

Table 2 was confusing at first.  It wasn't obvious that the first row, without S-STE, was just dense training, rather than a different DST baseline.  Once I understood this, its low loss value not being the ""winner"" made sense.  The second confusing bit was that the second row had the next-lowest loss, but was also not **emphasized**.  Again, I realized that this is because it would not lead to acceleration in the backwards pass.  This second point could be clarified by adding ""... and accelerates the backwards pass"" to the caption.

In line 256, the authors say they ""choose to sparsify del(Z) only in the backwards pass,"" which sounds like in the forwards pass, they do not sparsify del(Z).  Clearly, they don't, because this term is not involved in the forwards pass, but it caught me off guard - I think ""choose to sparsify only del(Z) in the backwards pass,"" as opposed to sparsifying both del(Z) and S(W), is more clear.

In line 331, I think the authors mean to say that FP8 quantization accelerates GEMMs up to 2x faster than their 16b counterparts.  (As detailed in Section 5.2, the upper bound of speedup from dense BF16/FP16 to sparse FP8 is indeed 4x.)

I noticed a few typos (there may be more lurking):

- Line 13: ""our method surpass"" -> surpasses

- Line 174: ""contains two main partitions"" -> parts

- Line 201: ""the closer *t* is from |*t3*|"" -> ""the closer *t* is to |*t3*|""

- Line 263: ""forwrad"" -> ""forward""

- Line 326: ""leaverage"" -> ""leverage""

**Significance**
Transformer blocks typically have two more weighted linear layers that were left dense in this work: the QKV projection before attention and the attention output projection.  Thus, there is potentially more memory and computation savings available, but without experiments, it is unknown if model quality will remain high if they were also made sparse.

Limitations:
The discussion of the limitations in the appendix is appreciated, but it seems to suggest that sparsity should give a theoretical 4x increase in throughput, but I believe this should be 2x.  (FP8 can theoretically give another 2x.)  Also, the sizes of the GEMMs that gave these results should be listed.  Finally, I'd point out that the method has only been tested on two out of four linear layers in Transformer networks, not other types of networks, including covolutional networks.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work studies an efficient method for pre-training, identifying three significant limitations in previous 2:4 sparse pre-training approaches: incorrect descent direction, the inability to predict the extent of descent, and oscillations in the sparse mask. Subsequently, the authors introduce a novel training methodology that integrates a continuous weight projection technique with a rescaling strategy, thereby enhancing pre-training efficiency and achieving performance comparable to conventional full-parameter training.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The structure of this work is well organized. It begins by highlighting the current challenges in 2:4 sparse pre-training, where the suboptimal performance largely stems from the discontinuity of the pruning function. This issue is then explored through various examples, followed by a demonstration of the proposed methods.
2. The experiments conducted encompass a variety of model sizes and datasets.
3. The motivation behind this work is well-founded and clearly articulated.

Weaknesses:
- The acceleration of S-STE is demonstrated solely in terms of theoretical gains. Seems like S-STE will introduce extra computation cost, which might sacrify part of the acceleration. Studying the end-to-end acceleration would further enhance the quality of this work.
- Is the discontinuity issue discussed in Section 3 associated with the learning rate? A larger learning rate results in more substantial weight updates, potentially causing more frequent flipping. Additionally, is this issue still a big problem during fine-tuning scenarios, where the learning rate is generally lower?
- The improvements of S-STE are modest, e.g., in Table 8, showing a 0.3 improvement in the F1 score for GPT-2 models with 124M and 350M parameters.

Limitations:
There is no end-to-end acceleration results evaluated of current methods.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
crlvDzDPgM;"REVIEW 
Summary:
The article presents a novel method for predicting drug-drug interactions (DDIs) by using a customized subgraph selection and encoding process. The authors propose a framework called Customized Subgraph Selection and Encoding for Drug-Drug Interaction prediction (CSSE-DDI), which leverages neural architecture search (NAS) to tailor the subgraph selection and encoding components for different datasets.Extensive experiments demonstrating superior performance of the CSSE-DDI framework compared to hand-designed methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The use of NAS to customize subgraph selection and encoding for DDI prediction is a novel approach that addresses the limitations of fixed, hand-designed methods.

2. The creation of extensive subgraph selection and encoding spaces allows for more accurate and context-specific predictions.

3. The framework's ability to adapt to different datasets and customize subgraph components based on data-specific characteristics is a significant strength.

Weaknesses:
1.The author applies NAS to the DDI task, but it does not exhibit the task-specific customization expected for DDI. For instance, the primary focus of the DDI task should be the interactions between drugs. However, the author raises the following issues: ""The relaxation strategy is a prerequisite for differentiable NAS methods. This is because the subgraphs in the selection space comprise different nodes and edges, making it challenging to design a relaxation function that unifies subgraphs of varying sizes. Additionally, to search within the subgraph selection space, we must first obtain all subgraphs within this space, but the task of sampling such a large number of subgraphs is computationally infeasible."" These issues are also present in subgraph extraction tasks on a single graph.

2.To address the problem of subgraph sampling, the author employs an implicit encoding sampling method. However, its effectiveness in terms of time efficiency and accuracy lacks sufficient evidence.

3.In the DDI task, the subgraph sampling space grows exponentially. The author's method for addressing the excessively large sampling space does not offer any advantages over the methods used to address the large sampling space in a single graph.

4.The author does not provide a comparison of the time overhead between their NAS method and the baseline methods. Given that NAS is likely to have significantly high time overhead, this comparison is crucial for evaluating model performance.

5.The author lacks comparisons with the latest baselines to highlight the performance advantages of their model. For instance, only one of the baselines chosen by the author is from 2023.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel subgraph-based approach for predicting drug-drug interactions (DDIs) that harnesses neural architecture search (NAS) to customize subgraph selection and encoding process. The authors first introduce refined search spaces to realize fine-grained subgraph selection and expressive encoding function searching. Then, based on the well-defined bi-level search problem, the subgraph space relaxation mechanism and the representation approximation strategy are proposed, enabling differentiable searching efficiently. Extensive experiments show that CSSE-DDI extensively outperforms the state-of-the-art approaches.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1.The problem and the method are well-motivated and formulated, with extensive experimental support. The results are solid.
2.The idea of customizing subgraph selection and encoding is a conceptually strong and justified innovation, which can distinguish this paper from other approaches, as shown in Table 1. 
3.The manuscript is well-structured and clearly written, facilitating comprehension.
4.CSSE-DDI demonstrates superior performance on two recognized benchmarks, indicating its practical effectiveness. The case study shows interpretability in the context of drug interactions, which is a very important aspect in such an application paper.

Weaknesses:
1. For the supernet training phase, in addition to the algorithm process, the authors are encouraged to provide an illustration to help the reader understand the corresponding steps more clearly.
2. A typo error in line 220: “Subgraph Repersentation“

Limitations:
The authors have adequately addressed the limitations.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work introduces CSSE-DDI, a searchable framework for DDI prediction, which refines search spaces for fine-grained subgraph selection and data-specific encoding. To improve search efficiency, CSSE-DDI employs a relaxation mechanism to continuousize the discrete subgraph selection space and use subgraph representation approximation to accelerate the search process. Extensive experiments demonstrate that CSSE-DDI significantly outperforms state-of-the-art methods, and the results are interpretable, revealing domain concepts like pharmacokinetics and metabolism.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
it introduces comprehensive subgraph selection and encoding spaces to cover the diverse contexts of drug interactions for DDI prediction. Faced with overwhelming sampling overhead, this work designs an effective relaxation mechanism to efficiently explore optimal subgraph configurations using an approximation strategy, enabling a robust search algorithm to explore the search space efficiently.

Weaknesses:
Examples of symmetric semantic patterns(headache, pain in throat) are not very convincing.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of predicting drug-drug interactions (DDIs), crucial for medical practice and drug development, using subgraph-based methods. It highlights the importance of customizing subgraph selection and encoding but notes the high cost of manual adjustments. Inspired by neural architecture search (NAS), the authors propose a method to search for data-specific components in the subgraph-based pipeline. They introduce extensive subgraph selection and encoding spaces and design a relaxation mechanism to efficiently explore optimal configurations. Extensive experiments demonstrate the method's effectiveness and adaptability.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Extensive experiments demonstrate the method's effectiveness. Compared to existing hand-designed methods, the CSSE-DDI framework shows superior performance, enhancing the proposed method's validity.
2. The writing quality is good.
3. Using NAS for precise prediction of drug-drug interactions is novel.

Weaknesses:
1. The motivation for using NAS to search components is not clear.
2. Despite the designed efficiency mechanisms, the inherent overhead of neural architecture search remains significant, especially for large-scale DDI prediction tasks.
3. While the paper compares the proposed method with existing ones, a more detailed comparative analysis, including discussions on computational costs and efficiency metrics, would provide a clearer picture of the trade-offs involved.

Limitations:
See weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
gC3BzNwqQp;"REVIEW 
Summary:
The paper studies a task of learning MNL parameters with delayed feedback. The authors study both the cases where feedback with extremely high delay is ignored or taken into account as well. They prove that for both settings the optimal regret is $\tilde{\Theta}(\sqrt{NT})$ where $T$ is the horizon and $N$ is the number of products. They also conducted experiments that support their theoretical findings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) Assuming that results are correct, the paper gives a first solution to a question that seems natural in the domain of online advertising.
2) The bounds are fairly tight.
3) The paper includes an extensive literature review.
4) The authors conducted experiments to support their theoretical findings.

Weaknesses:
1) I think that the main results could have been better presented. In the current formulation, it seems like there is no difference between the guarantees of DEMBA and PA-DEMBA, which makes the reader wonder why do we need both of them. It also makes it hard to understand what improvements can we hope to achieve in future work.
2) The setup seems kind of specific to this problem. It could have been interesting to know if one can define it more generally, and still use the same or similar solutions. This could have help the reader to conjecture if the same techniques may be used to solve other variations of the problem.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors consider the setting in which a business is required to select a set of options to a customer in order to maximize the generated revenue. This task is challenging as the options presented to a customer may interact with each other and alter the choice of the customer, and the feedback on the choice of the customer could be received by the business after a considerable delay. This class of tasks can be addressed in literature using Multinomial choice (MNL) models. The authors aim to address two challenges that arise in this scenario, namely the unknown MNL parameters and the delayed feedback. The authors consider two settings, *threshold* and *non-threshold*, and propose two algorithms to address them, DEMBA and PA-DEMBA, respectively.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
The paper is clear and well-written. A deep analysis of related works is provided by the authors, clearly stating what gap in the literature the work aims to fill.
The work combines existing ideas to propose a solution to a problem that might have some significance to some members of the NeurIPS community.

Weaknesses:
W1) The definition of the ""feedback observed by the seller"", $o_{i,s,t}$ is inconsistent and seems also incorrect. At line 148, it is defined as:
$$
o_{i,s,t} = c_{i,s,t} a_{i,t},
$$
whereas in the proof sketch of Lemma 4.1 it is used as $o_{i,s,t} = c_{i,s,t} a_{i,s}$.
Both definitions seem to be incorrect, as in the first case (i.e., with $a_{i,t}$), $o_{i,s,t}$ evaluates to 1 considering the option chosen by the consumer at round $t$ instead of round $s$, in which the product was sold.
In the second case (i.e., with $a_{i,s}$), this issue is solve however the definition of $c_{i,s,t}$ makes it so that $o_{i,s,t}$ would evaluate to 1 at every round $t \ge d_s + s$ (under the condition that $d_s \le \mu$), potentially causing a choice of the consumer to count more than once towards the estimation of the preference.

W2) The proof sketch of Lemma 4.1 at line 188 (and in the appendix) seems to contain an error, as the conditions of $c_{i,s,t}$, which in the definition are in a logical AND, are split using a summation, which seems to be incorrect, and could invalidate all the subsequent steps of the proof.

W3) The observation that ""each alternative can act as a substitute or competitors to others, impacting the customer's final decision"" stated in Section 1 would have been better represented in the problem formulation with attraction parameters that depend on the options in the proposed set. Indeed, the provided formulation of the customer choice probabilities works fine, but cannot comprehensively capture product substitution dynamics.

Limitations:
No limitations

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors studied the problem of learning with delated feedback under the Multinominal Logit model. Prior work in bandits with delayed feedback does not accommodate settings where multiple items can be offered simultaneously. The authors instead proposed two algorithms: DEMBA for thresholded setting where the seller discarded delay longer than a certain threshold, and PA-DEMBA when the threshold is infinity (when all delayed feedback are considered). Both algorithms achieve $O(\sqrt{NT})$ regret bound, and the authors provided matching lower bound up to a log term. Finally, the authors provided a set of numerical experiments to support their theoretical findings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The studied problem of delayed feedback in bandits, while not new, is interesting in the case where the seller can offer a slate of items to the customer at every round. 

- The paper is well-written and easy to follow.

- The theoretical regret guarantee is provided with matching lower bound. These results and the numerical experiments provided a complete set of result for this setting.

- The proof-sketch provided offers good intuition to help understand the analysis and the result.

Weaknesses:
- The analysis did not attempt to learn the unknown delay distribution.

Limitations:
The authors have addressed the limitations of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper works under the MNL bandit settings where the environment feedback is delayed, motivated by real-world application scenarios like e-commerce platforms, while balancing exploitation and exploration. For the two proposed algorithms, the authors provide corresponding theoretical analysis, resulting in regret upper and lower bounds. Experiments are also conducted to show the effectiveness.

Pros:

- The paper is generally well-organized with clear narratives and derivations. Indeed, delayed feedback is an important characteristic for modern recommender systems where users can remain neutral before disclosing their final preference towards recommendations.

- From my personal perspective, the theoretical analysis pipeline is novel, and the results look decent, with both the regret upper bound and lower bound presented.

Cons and questions:

- One question from my side is that for the current theoretical analysis, the regret bound mainly depends on the expectation of the delay, without modeling the skewness of the delay distribution $f_d$. In this case, I am wondering what the regret bound would look like if we take the skewness/variance of the distribution into account. For example, with $f_d$ being Gaussian, how will the variance interact with the final regret upper bound/lower bound? Is it possible to achieve a tighter regret bound when the distribution is decaying super fast compared to that of a long-tail distribution?

- Although the contribution of this paper mainly lies in the theoretical analysis perspective, it would be better if the authors could include more algorithms for comparison in their experiments. Some MNL bandit baselines from the related works section would be good.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Please see my comments above.

Weaknesses:
Please see my comments above.

Limitations:
Please see my comments above.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers on online learning problem in the setting of discrete choice models with delayed feedback. The paper assumes a multinomial logit model where a decision maker has some (unknown) valuation v_i for item i. When presented with a menu S of choices, the agent chooses a single item from S such that the probability of selecting item i is proportional to v_i. In the online learning setup, at each time step t, the learner offers an assortment (menu) S_t and then agent chooses one item from the assortment. The paper considers a setup with delayed feedback, where the feedback regarding which item was chosen is delayed (according to an unknown distribution).

The paper considers two settings - (i) with censorship - where feedback delayed by more than some fixed deadline \mu is censored, and (ii) without censorship - where feedback can be delayed indefinitely (but expected delay is known to the learner). In both settings, the paper presents algorithms that obtain almost best possible regret \tilde O(NT) where N is the total number of items and T is the time horizon.
The algorithms are based on UCB and perform learning in epochs - i.e. offer the same assortment at all time steps throughout an epoch in order to reduce variance. An epoch is determined by times when the learner receives explicit negative feedback, i.e., the agent did not pick any item from the assortment.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper considers online learning of discrete choice models in a general setting with delayed feedback. The setup is broadly applicable.

Weaknesses:
- I found the comparison with prior work a bit lacking. Since I am not directly familiar with works in this area, I would have appreciated more details about how the paper differs from prior work. In particular, Online learning with MNL choice models (but no delayed feedback) admits UCB based algorithms [Agrawal et al]. How much does the current work differ from that work? What additional technical complications are introduced by delayed feedback? Are they different from the challenges introduced by delayed feedback in other online learning settings (say classic MAB)?

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
GB5a0RRYuv;"REVIEW 
Summary:
This paper introduces the Materials Knowledge Graph, a pioneering graph database designed for materials science. It leverages advanced NLP methods and LLMs to extract and organize a vast amount of high-quality research into structured triples. It streamlines the discovery process by organizing information into nodes and edges, enhancing data integration and reducing the need for traditional experiments. The MKG also employs algorithms to predict material applications, offering a significant advancement in accelerating materials research.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1.	This paper demonstrates a thorough and systematic methodology, including data preparation, model training, entity resolution, and graph construction, ensuring a robust and credible knowledge graph.
2.	The MKG shows a cutting-edge approach to parsing and structuring vast amounts of scientific literature, offering a significant advancement in accelerating materials research.
3.	The application of link prediction algorithms for predicting material applications is a robust method for identifying new potentials in the field.
4.	Experiments shows the effectiveness of the MKG and the employed models, enhancing the credibility and reliability of the results.

Weaknesses:
1. As the field of materials science evolves, maintaining the currency and accuracy of the MKG could become increasingly complex, requiring continuous updates and curation.

Limitations:
The authors have made a commendable effort in addressing the limitations of their work. They acknowledge the dependency on manual annotation for data preparation, which could limit scalability and timeliness. 
However, the paper could benefit from a more detailed discussion on maintaining the currency and accuracy of the MKG as the field of materials science evolves. Continuous updates and curation will be crucial, and outlining specific strategies for this would strengthen the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an innovative way on leveraging the power of Large Language Models for the construction of a Material Knowledge Graph (MKG) and link prediction. The method includes annotating few scientific articles (abstracts) related to material science which are used for training and finetuning LLMs. After that step and using additional articles and a finetuned LLM, triples are generated. Entity resolution is performed by using different Natural Language Processing (NLP) techniques such as ChemDataExtractor, mat2vec and an expert dictionary. Finally, the MKG is constructed and used for link prediction with the aid of network-based algorithms and graph embeddings. This approach is compared with another technique called MatKG2 and experiments were conducted for finding the LLM that provides the best results and for evaluating the link prediction of the MKG.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
•	This is an innovative work for the fast and automatic construction of a Material Knowledge Graph with minimal annotation that could have a broad impact in the advancements of material science. 

•	It reuses effectively new technologies such as Large Language Models and other NLP techniques.

•	It does not require too many annotated documents or other manual tasks.

Weaknesses:
•	More experiments would have supported better this work. The result of this method could have been compared with some baseline experiments of simply using LLMs for the triples generation. Moreover, since this method is compared with MatKG2, it would have been useful to compare the results of precision, recall and F1 score of MatKG2 with the results of MKG. 

•	While the work seems quite interesting it not well written and several typos and syntactic errors have been found which are detailed below. Proof-reading would have been beneficial before submission.

- Line 64: “ A user-friendly databases.. “ -> “User-friendly databases..”

- Line 80: Acronyms NER and RE are used but they are introduced later in the text (line 85)

- Line 89: “… through query the MKG.” -> “… through querying the MKG.”

- Line 93: “the elaborate workflow” -> “the elaborated workflow”

- Line 98: “NERRE” -> I believe this refers to NER and RE

-	Figure 1 (b): ChemDataExactor -> ChemDataExtractor

- Figure 4: FMKG -> the material knowledge graph has been referred in the whole paper MKG. The acronym FMKG is introduced only in this figure.

- Line 233: “indicate”->”indicates"", “achieve”->”achieves”, “contribute”->contributes”.

- Line 240: “ChemDataExactor” -> ”ChemDataExtractor”

- Line 252: “The result shows in.. “ -> “The result is shown in…”

• Figure 3 and network-based link prediction is not well explained in the article. 

• The code and Knowledge Graph is not available for further evaluation. The authors state that they will only be available if the paper gets accepted.

Limitations:
There are some concerns about the scalability and maintainability of MKG which are not addressed in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The study presents an innovative pipeline for Knowledge Graph (KG) construction, specifically designed for efficient extraction of triples from unstructured scientific texts. The methodology enables fine-tuning of Large Language Models (LLMs) with limited annotated datasets, which is then utilized to extract structured information from extensive corpora of unstructured text. The authors have constructed a Material Knowledge Graph (MKG) that captures relationships between materials and their associated entities, such as properties and applications, derived from abstracts of 150,000 peer-reviewed papers.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper introduces a novel pipeline for KG construction that departs from predictive modeling, enhancing the authenticity and traceability of extracted structured data.

Quality: The authors demonstrate the effectiveness and credibility of the MKG through ablation experiments and similarity analyses based on node similarity and graph embedding. The results indicate the substantial predictive capacity of the MKG, with 48.5% of 'material-application' predictions validated within nine years, which is impressive.

Clarity: The paper provides a clear and comprehensive explanation of the methodology, including the fine-tuning of LLMs, extraction of structured information, and construction of the MKG. Detailed results and analyses support the authors' claims.

Significance: The MKG has significant potential in extending the depth of structured information extraction, improving entity labeling precision, and adapting the pipeline to other scientific fields.

Weaknesses:
Some figure captions, like ""Fig 1(a)"" and ""Fig 3,"" are unclear. Captions would benefit from additional context.

Certain acronyms, such as ""ER-NF"" on line 239 and ""NER” / “RE"" on line 80, appear before being defined.

Typo on line 140: ""task""s->task’s.""

Limitations:
The authors did addressed the limitations, and mentioned that strict normalization and entity resolution process can loss some correct entities.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper on constructing and applying a materials knowledge graph (MKG) in multidisciplinary materials science via a large language model (LLM) is valuable and well-written. However, it primarily focuses on application rather than strong technical contributions, with issues in experimental design, lack of non-LLM baselines for comparison, and insufficient comparison with sophisticated knowledge graph completion methods. It may be a good dataset track paper but not the main track.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The studied problem is of great value in the real world
- The paper is well-written and easy to follow
- The produced MKG could be very helpful for the computational and experimental material science

Weaknesses:
- W1: This paper is towards applications of LLMs in an engineering flavor instead of having strong technical contributions. The process includes prompt engineering, basic model fine-tuning, and human-in-the-loop entity resolution. And this paper looks like an extended application of Darwin, instead of an independent research work.
- W2: The experimental design is flawed. In particular, why the normalization is only applied to Darwin? Is it possible that other base models + normalization can perform better? And is the normalization only working well for Darwin?
- W3: Non-LLM baselines for NER and RE should be included for comparison.
- W4: The modified Jaccard similarity method is claimed as a specified KG completion algorithm for material science. Therefore, the experiments should include comparisons with more sophisticated KG completion methods, instead of only comparing with TransE, which is outdated.
- W5: Minor issues include but are not limited to: Typos in Figure 4, what is FMKG?

Limitations:
Yes.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
2vhkjOdlc8;"REVIEW 
Summary:
The paper ""Dinomaly: The Less Is More Philosophy in Multi-Class Unsupervised Anomaly Detection"" introduces a minimalist reconstruction-based framework for unsupervised anomaly detection (UAD) in multi-class settings. The framework focuses on four main components: Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction. Extensive experiments on MVTec-AD, VisA, and Real-IAD datasets show that Dinomaly achieves superior performance compared to state-of-the-art multi-class and even some class-separated UAD methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Using simple components like Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction to achieve superior performance is highly original. This is a significant departure from traditional methods that rely on complex designs and multiple modules. It challenges the conventional views: more complex architectures are necessary for better performance in anomaly detection tasks.
2. The methodology is well-detailed, and the experimental design is robust. The authors conduct extensive experiments on three well-known datasets (MVTec-AD, VisA, and Real-IAD), providing comprehensive performance metrics and comparisons with SOTA methods. The result is convincing, showing that Dinomaly not only outperforms existing MUAD methods but also surpasses some of the best class-separated UAD methods. 
3. The paper is generally clear, well-organized, and relatively reproducible.
4. The significance of this work is substantial, and makes a valuable contribution to anomaly detection, as it addresses a major challenge in UAD—achieving high performance in multi-class settings without resorting to complex, specialized architectures, and is potentially scalable.

Weaknesses:
1. The paper provides a detailed explanation of the proposed framework but lacks important justification and discussion, it is difficult for readers to realize the novelty and improvements brought by Dinomaly. The author may need to compare Dinomaly to specific previous methods, highlighting the differences and improvements. Discuss how the minimalist approach contrasts with more complex architectures and why this improvement is significant.
2. The motivations for choosing Noisy Bottleneck and Loose Reconstruction are not deeply explored. For instance, explain in more detail why Noisy Bottleneck helps prevent identity mapping.
3. The paper claims simplicity but there was no discussion of parameter number, computational complexity, or time complexity in the experiment. 
4. In Loose Constraint, the author claims that 1-group LC mixes low-level and high-level features which is harmful for anomaly localization. How to group the features into the low-semantic-level group and high-semantic-level group in 2-group LC?

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on the Multi-class Unsupervised Anomaly Detection task and proposes a minimalistic reconstruction-based anomaly detection framework — Dinomaly that consists of only vanilla Transformer blocks. In this framework, four key components (Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction) are introduced to alleviate the performance gap between multi-class and class-separated models. The paper conducts extensive experiments on three major datasets: MVTec-AD, VisA, and Real-IAD. Results show that Dinomaly outperforms current state-of-the-art methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
(1)The paper is well-written and has clear statements which make it easy to understand. 

(2)The design of Dinomaly is straightforward but innovative. The use of foundation transformers, noisy bottleneck, linear attention, and loose reconstruction is well-justified. 

(3)The paper generally outperformed existing SOTA methods and did enough experiments and comparisons.

Weaknesses:
(1)The method relies heavily on transformer architectures, which might limit its applicability to other types of models.

(2)Transformers can be resource-intensive, and the paper does not fully address the computational cost of training and inference.

(3)The method's generalization to other domains or types of anomaly detection is not fully explored.

Limitations:
Limitations are discussed in Supplementary Sec. A.4.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Dinomaly, a simple yet effective anomaly detection framework using pure Transformer architectures. It identifies four key components essential for multi-class anomaly detection: Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction. Extensive experiments on MVTec-AD, VisA, and Real-IAD datasets show that Dinomaly achieves superior performance, surpassing both state-of-the-art multi-class and class-separated anomaly detection methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors have conducted extensive experiments to validate the effectiveness of their method across multiple anomaly detection tasks.
2. The authors have proposed a simple yet effective framework that approaches and even surpasses the results of state-of-the-art methods in single-class anomaly detection tasks.

Weaknesses:
1. L53-55 'In addition, previous...': The authors should provide relevant evidence rather than subjective assumptions.
2. L77: Placing the Related Works section in the appendix is unconventional.
3. The first component proposed by the authors, Foundation Transformers, was already introduced in the ViTAD paper, which diminishes the overall contribution of the paper. 
4. The input resolution used in the authors' experiments is 448x448, while other comparison methods use 256x256 or 224x224. This is an extremely unfair comparison. Please include results with a 256 resolution in table for a fair comparison.
5. In the proposed Loose Loss, 90% of the feature points were selected. How was this 90% hyperparameter determined? Please provide ablation study results.
6. The authors have employed Linear Attention to reduce computational load while maintaining similar performance. It is recommended that the authors compare the parameter count and FLOPs of their method with those of the baseline methods to demonstrate its efficiency. Additionally, it is suggested to conduct ablation studies to verify the computational efficiency of Linear Attention. 
7. Other methods, such as RD4AD, SimpleNet, and UniAD, perform under the proposed settings. The authors can conduct a more equitable comparison.

Limitations:
It is recommended that the authors evaluate the performance of different methods under fair settings.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces Dinomaly, a minimalistic unsupervised anomaly detection (UAD) method designed to bridge the performance gap between multi-class UAD and class-separated UAD. Utilizing pure Transformer architectures with key components such as Foundation Transformers, Noisy Bottleneck, Linear Attention, and Loose Reconstruction, Dinomaly achieves superior performance on MVTec-AD, VisA, and Real-IAD benchmarks, surpassing state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Dinomaly effectively bridges the performance gap between multi-class and class-separated UAD, achieving superior results on popular benchmarks such as MVTec-AD, VisA, and Real-IAD. 
2.  It utilizes a simple, straightforward approach with pure Transformer architectures, avoiding complex modules or specialized tricks.
3. The detailed ablation study demonstrates the effectiveness of each component—Noisy Bottleneck, Linear Attention, Loose Constraint, and Loose Loss—in enhancing anomaly detection.

Weaknesses:
1. The method might be perceived as too application-oriented, lacking broader theoretical contributions.

Limitations:
See the weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Dinomaly simplifies the anomaly detection process by eliminating the need for complex designs, additional modules, or specialized techniques. It relies solely on basic Transformer components such as self attention mechanisms and multi-layer perceptrons (MLPs) to perform anomaly detection for multi class images.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper has a clear motivation and contribution. The paper effectively proposes the viewpoint of ""less is more"" in multi class unsupervised anomaly detection, emphasizing how the simplicity of model architecture can achieve or surpass the performance of more complex systems.

Weaknesses:
I hope to provide a specific explanation of the information provided by the decision-making process for identifying anomalies in the model.

Limitations:
The author candidly acknowledged the limitations of the work and provided the problems that need to be addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
9XDYEEBRV6;"REVIEW 
Summary:
This paper focuses on coded computing for machine learning and derives loss-minimizing encoding and decoding functions.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Please see the “Questions” section.

Weaknesses:
Please see the “Questions” section.

Limitations:
Yes.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors consider the problem of improving the reliability of distributed computing architectures by encoding the input data before it is processed by worker machines such that a good approximation of the desired output can be reconstructed using only a subset of the workers’ outputs. They utilize learning theory to determine a regularized objective function for the decoder as well as a loss function for the overall system, and provide solutions for the encoder and decoder which optimize their derived upper bounds on the loss function. The objective function is based on kernel ridge regression, which leads to a second-order smoothing spline solution for the decoder. Both the noiseless and the noisy cases are considered, and the proposed method is compared with the existing approach of Berrut coded computing in terms of its convergence rate in the noiseless setting and its empirical performance. To evaluate the empirical performance, distributed inference is performed for deep learning models of varying sizes and tasks with varying sizes for the output vector. The experimental results show that LETCC consistently achieves a lower estimation error on the model outputs.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
# Originality
The main contribution which differentiates the LeTCC method from prior works like BACC appears to be the introduction of regularization which causes the optimal solution to be based on smoothing splines rather than interpolation. Existing papers which have considered optimizing cost functions only focus on learning how to generate parity data points. Overall, this paper brings a novel method to the table and adequately cites related works.
# Quality
Although I lack experience with some of the mathematical tools used, to my knowledge there is nothing incorrect about the technical results. The experimental methodology seems to allow for a fair comparison between LeTCC and BACC for the use case of image classification with a deep learning model. In terms of reconstruction accuracy, the experimental results validate that LeTCC is superior to BACC, in some cases by a significant margin.
# Clarity
The main content of the paper is laid out clearly with some small typos but no major issues.
# Significance
The method for coded computing introduced in this paper represents an advancement in the level of accuracy that can be achieved when the function applied by the worker nodes is a complex deep learning model. The authors show that this advancement can be achieved by adapting the encoder and decoder to the function through the tuning of the regularization weight, which is an interesting idea.

Weaknesses:
# Originality
This paper distinguishes itself sufficiently from existing works, so I see no issues in terms of originality.
# Quality
LeTCC is proven to have a reconstruction error that scales better than BACC in terms of the number of worker nodes, but its scaling in terms of the number of stragglers is not directly compared. Based on Theorem 9 in the BACC paper, it appears that the max error along a single dimensions scales as $S^2$. The mean-squared error (MSE) for LeTCC scales as $S^4$, but since the max norm is upper bounded by the Euclidean norm it is unclear how to compare these results. It would be nice to compare the theoretical dependence on the number of stragglers, especially since the experimental results are mixed (in two cases the difference in MSE shrinks as $S$ increases, and in the other case the MSE grows as $S$ increases). 
# Clarity
The authors should explicitly state how many trials were averaged over to produce the plots in figure 3 to help readers interpret and/or reproduce the results.

I identified the following typos in the paper:
- Line 502: There is a “loc” subscript missing
- Line 49: The sentence starting on this line is grammatically incorrect, I presume that the second comma is supposed to be replaced with the word “is”
- Figure 3: The title of the top-left plot seems to have the values of N and K reversed

# Significance
Without more details on the hyperparameter optimization process and the relative computational complexity of fitting the encoder and decoder functions as compared to those of BACC, it is hard to say whether LeTCC is more practically advantageous to use than BACC. It could be that the overhead introduced by hyperparameter tuning outweighs the benefit from reduced reconstruction error in some cases. It would have been helpful to see how tolerant LeTCC is to changes in the smoothing parameters, i.e. how much variation can be allowed while still outperforming BACC. One can also see from Figure 3 that a large difference in the mean-squared error does not consistently translate to a large difference in relative accuracy (at least when deep models are used), which makes it less likely that LeTCC’s performance benefits would be worth the additional overhead. As it currently stands, these issues reduce the practical value of the contribution made by this paper.

Limitations:
The authors are upfront about the limited scope of their work. However, as mentioned in the weaknesses section, the extra overhead introduced by the need for hyperparameter tuning is something that I would have liked to have seen addressed in the paper. As mentioned in the weaknesses section, the comparison of error convergence rates with BACC is also limited.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes a learning theory-based novel framework for coded computing with a focus on distributed machine learning applications. The proposed method sends mixtures of input samples to the worker nodes that compute the desired results on the mixtures. An encoder and a decoder functions are fitted at the master node using input samples and the result from workers, respectively. Finally, the decoder function can be used to estimate the output of computing function for the input samples. It is shown that the loss function (divergence of the estimated output with the true output of computing functions) is upper bounded by the generalization error of the decoder regression and training error of the encoder regression under some conditions. Experimental evaluations are provided to show the efficacy of the proposed method over the state-of-the-art baseline.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method leverages learning theory for coded computing which seems novel and interesting.
2. Rigorous theoretical analysis is provided including convergence rate derivation and recoverability analysis.

Weaknesses:
1. The experimental section does not look comprehensive enough for the following reasons:
   (a) Only one baseline is considered.  
   (b) Authors acknowledge in Section 3 that the computational efficiency of encoder and decoders is a crucial factor. Yet, this was not reported in the numerical section.

2. The computing functions (which are generally pre-trained neural networks) are evaluated on (probably) mixtures of input samples at the worker nodes. Those neural networks were trained on the data distribution. However, the mixture of input samples might not fall on the support of data distribution. Although for images, this does not seem to be an issue, there could be unexpected behavior in general.

Limitations:
Yes, but it is spread out throughout the manuscript. A separate limitation section/paragraph would be helpful for the readers.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper deals with coded distributed computing. I need to note that it is very popular research area now with a vast number of papers. But the authors are right when mention that the majority of papers utilizes standard algebraic codes (such as Reed-Solomon codes). The main problem of such approach is that these codes are designed over finite fields, which is not the case for machine learning tasks. The improvements utilize real or complex valued codes (e.g. Reed-Solomon codes over real or complex fields) but they face with the problems of accuracy (Vandermonde matrices have bad condition numbers). Moreover such approaches work only for some particular functions, e.g. polynomial ones. In this paper the authors propose a framework, which outperforms the Berrut approximate coded computing (BACC), which is the state-of-the-art coded computing scheme for general computing.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main strong point are as follows:
- new framework foundation for coded computing, new loss function and its upper bound by decomposition
- theoretical analysis and guarantees. Under some assumptions the authors find the optimum encoding and decoding functions and characterize the convergence rate for the expected loss in both noise-free and noisy computations.
- Numerical analysis, that shows the new framework to outperforms the Berrut approximate coded computing (BACC), which is the state-of-the-art coded computing scheme for general computing.

Weaknesses:
I list the main weaknesses below:
- «We develop a new foundation for coded computing, based on the theory of learning, rather than the theory of coding». I would not be so categorical. Coding is a method when you add and the utilize the redundancy to deal with errors and erasures. The main advantage of your approach is that you are not using standard finite field codes.
- How the optimal (under some assumptions) encoder and decoder functions in section 4 are related to the functions utilized for numerical experiments (DNNs)? Is it possible to analyse the performance under optimal encoder and decoder and compare it to the results from Section 5?
- You made the comparison to BACC, which is a framework for general computing. Could you please make a comparison for some particular (e.g. polynomial) problems? I just wonder if proposed general approach is competitive with e.g. Lagrange computing and what you should pay for universality. So my claim is that more scenarios and computing problems should be checked to understand the limitations and applicability of your method.
- How flexible is your approach if the system parameters change (the number of worker nodes or the number of stragglers).
- Figure 3 seems to suffer from the lack of statistics (you need more experiments).
- You mentioned Byzantine case. It is known to be a problem for the codes over the real field (in case of finite field the approaches are well-developed). It would be beneficial to briefly explain how you plan to deal with this problem.

Limitations:
Limitations are well described.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
CECVgSZbLW;"REVIEW 
Summary:
The authors explore the use of distributional reinforcement learning within Monte Carlo Tree search. They propose two algorithms CATS and PATS a categorical distribution and particle distribution based approach respectfully. They perform a theoretical analysis of the methods and show analysis of regret. They then evaluate on a synthetic planning tasks and evaluate it in combination with a pre-trained network on the atari benchmark.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
- Thorough theoretical analysis
- The authors address distributional RL applied to planning which is a clear important direction of research

Weaknesses:
- Lack of referencing of existing work and novelty relative to existing work
    - Hayes, C.F., Reymond, M., Roijers, D.M., Howley, E. and Mannion, P., 2023. Monte Carlo tree search algorithms for risk-aware and multi-objective reinforcement learning. *Autonomous Agents and Multi-Agent Systems*, *37*(2), p.26.
- Unjustifiable statement: “For example, CATS is significantly better than other methods in Breakout, Enduro” There is no significance testing performed so this statement cannot be made and in fact the Confidence intervals overlap
- Key results in appendix and lack of empirical results in the main paper
- CATS never outperforms fixed depth MCTS on the synthetic tree task
- Unable to find code despite checklist saying it is provided

- Small issues
    - Figure 2 algorithms alignment off
    - Indication of Atari results in section 5 which are not there
    - Adding bold to best performing method in the Atari table would be useful for readability

Limitations:
- Limitations are not included in the main body of the paper which they should be especially considering there is space. The limitations are also not thoroughly discussed for example
    - “faces challenges in managing computational demand” : this does not say anything meaningful
    - “Our approach’s performance is slightly influenced by hyperparameters”, this can be said for essentially any method
- It seems that the distributional approach has an additional memory cost which if correct should be added to the limitations
- Given CATS and PATS do not massively outperform all baselines on the synthetic task I think the limitations should be where this is addressed and perhaps some insight given into why this is and why performance on Atari is also not particular strong relative to methods such as MENTS.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper propose two algorithms, Categorical Thompson Sampling for MCTS (CATS) and Particle Thompson Sampling for MCTS (PATS). These algorithms extend Distributional Reinforcement Learning (RL) to Monte-Carlo Tree Search (MCTS) by modeling value functions as categorical and particle distributions, respectively to improve the performance of MCTS in highly stochastic settings.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- **Originality:** The integration of Distributional RL into MCTS using categorical and particle distributions is innovative and addresses a critical need in handling stochastic environments (Sections 3.1-3.3).
- **Quality:** The theoretical analysis is rigorous, with well-defined proofs and clear methodology (Sections 4.1 and 4.2).

Weaknesses:
1. **Empirical Validation**: While the paper presents a comprehensive set of experiments demonstrating the efficacy of the proposed methods (CATS and PATS) in synthetic scenarios, there is an evident lack of diversity in the benchmarks used. 

2. **Algorithm Complexity and Overhead**: Both CATS and PATS introduce additional complexity by incorporating distributional approaches and Thompson Sampling into MCTS. The paper does not sufficiently address the computational overhead or the scalability of these methods when applied to environments with larger state or action spaces. This could be crucial for understanding the practical deployment of these algorithms in real-world applications.

Limitations:
### Computational Demands
The authors recognize that the Categorical Thompson Sampling (CATS) distributional Monte Carlo Tree Search (MCTS) involves increased complexity due to the management and updating of probability distributions. This acknowledgment is crucial as it highlights a potential scalability issue, especially in environments where computational resources are limited or real-time responses are required.

### Fixed Precision
The approach used in the Particle Thompson Sampling (PATS) to manage the growth in the number of particles by fixing the float precision is a practical solution to prevent computational overload. However, this method may introduce limitations in the precision and adaptiveness of the model, potentially affecting the accuracy of value estimations in environments with high variability.

### Number of Atoms
The performance sensitivity to the number of atoms indicates a hyperparameter dependency, which could impact the effectiveness and robustness of the model. The authors mention that suboptimal choices in this hyperparameter may affect performance, suggesting a need for careful tuning and validation to optimize the model's accuracy and efficiency.

### Addressing Limitations
While the authors have outlined these limitations, the discussion could be expanded to include more detailed strategies for mitigating these issues, particularly the computational demands and fixed precision aspects. For instance, strategies to optimize computational efficiency or adaptive techniques to dynamically adjust precision based on the context could further strengthen the approach.

### Societal Impact
The paper does not explicitly address the potential negative societal impacts of the research. In the realm of reinforcement learning and AI planning, concerns such as the deployment in sensitive or critical environments, where errors may have significant consequences, should be considered. Discussions around ethical implications, misuse, and long-term effects would be beneficial.

### Suggestions for Improvement
1. **Enhanced Computational Strategies**: The authors could explore methods to reduce computational overhead, such as parallel processing or optimizing algorithmic efficiency, to make the model more practical for real-time applications.
   
2. **Dynamic Precision Adjustment**: Introducing mechanisms to adjust the precision of particle distributions dynamically based on the observed variability in the environment could help maintain balance between computational efficiency and model accuracy.

Overall, the authors should be commended for their upfront discussion of the limitations, but there is room for deeper analysis and additional strategies to address these limitations comprehensively.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces distributional return estimates to MCTS-based planning. For this the authors borrow from work on distributional Q-Learning and show how to adapt the MCTS value back-up and action selection steps to compute and utilise these distributions. They formulate two approaches based on different distribution representations (quantile and particle based) for which they provide some theoretical convergence analysis as well as first experimental results.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
The paper combines two well-established ideas in MCTS and distributional value approximation resulting in a new algorithm with better theoretical guarantees. The overall approach and implementation of this combination makes sense and should at least in theory overcome limitations in stochastic environments.

Though I was unable to verify all proofs in detail, the theoretical analysis seems to make sense and establish the advantages of the proposed methods.

Weaknesses:
Despite the soundness of the overall proposed method, I found the paper very hard to follow and felt details were missing due to an overall lack of focus. Contributing to this were the following issues:

1. Empirical Evaluation
Experiments are limited to a toy domain and results on the Atari benchmark reported in the appendix.
The toy domain is a tabular environment that is being generated randomly and contains stochasticity in both the final reward and transitions.
For an illustrating example this makes it hard to judge the combined effect on the overall return distribution to be approximated. 

How the combinations of branching factor and depths that were plotted were chosen is unclear to me. Beyond this I am not sure how meaningful these plots are. In the right most plot it appears as if the PATS approximates the root value almost correctly in under 100 simulations - at which point it could not even have tried all k = 200 actions available to it.

Also CATS appears to be doing consistently worse than some of the other methods despite having the same theoretical properties as PATS.

For the Atari baseline the authors make use of Q-networks and point to a related paper. However, the exact implementation details and hyperparameters are not discussed making it hard to reproduce this work based on the paper alone.

While stochasiticity and the exploration challenges this causes form one of the main motivations for this paper, no further ablations how the proposed methods improve here are presented.

2. Content division
The author devote a significant amount of space to the summarisation of MCTS and distributional RL. While the theoretical analysis is arguably the strongest part of the presented work only the main theorems are found in the main body of the paper with very little contextualization.

3. Overall presentation
There are several presentation issues in overall formatting, grammar and spelling. The former includes, but is not limited to overlapping lines, inconsistent / in-text section headers and wrong section references.

Limitations:
The discussion of limitations is restricted to a short paragraph in the appendix listing generic points such as increased computational demand and sensitivity to hyperparameter choices. However, no further investigation or explanation as to their severity is provided.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Categorical Thompson Sampling for MCTS (CATS) and Particle Thompson Sampling for MCTS (PATS) algorithms, which incorporate distributional reinforcement learning into Monte Carlo Tree Search (MCTS) to handle value estimation in stochastic settings. By modeling value functions as categorical and particle-based distributions and applying Thompson Sampling for action selection, the proposed algorithms aim to improve the robustness and accuracy of value estimates. The paper proves the theoretical effectiveness of these methods by achieving a non-asymptotic problem-dependent upper bound on simple regret of $O(n^{−1})$.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea is interesting and original and the non-asymptotic problem-dependent upper bound on simple regret of $O(n^{−1})$ significantly advances the state-of-the-art from the previous $O(n^{−1/2})$.

Weaknesses:
1- While using distributional RL in MCTS to do Thompson sampling is interesting, it introduces much computation complexity hindering the applicability of the proposed algorithms.

2- The numerical experiments for the stochastic environments that are the main motivation of this work are done on a toy problem.


Minor comments

1- The presentation of the paper can be improved, specifically the parentheses () citation style can be confused with equations reference. 

2- Line 42, the authors mention V node for the first time without properly defining what is a V node.

Limitations:
1- The added high computational complexity from maintaining a distribution for each node in the MCTS.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
Ur9f4hNIpN;"REVIEW 
Summary:
This work presents a transformer architecture inspired by predictor-corrector methods for solving ODE problems. It adopts the Adams-Bashforth-Moulton method and utilizes an exponential moving average (EMA) method to compose the predictor, discussing two correctors (the EMA-based and the simple backward Euler method). Without the background of ODE, the proposed method could be seen as a special case of a transformer with cross-layer connections. Consider that the residual connection only links two layers, while the proposed method links multiple layers in a high-order way.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This work provides theoretical results to enhance the connections between residual connections and the ODE solver, providing an interesting perspective on viewing cross-layer connections within neural networks.

Using EMA as a high-order predictor is a simple and flexible solution, and the experiments demonstrate its effectiveness.

Weaknesses:
The experiments are insufficient and out-of-date. As the main contribution is the new transformer architecture, scaling law style experiments and testing on modern LLMs' benchmarks are recommended. I saw that Appendix D has some results, but it's better to have a formal experiment.

Limitations:
Agree with the limitation section.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper takes inspiration from established high-order approaches in numerical analysis for solving differential equations to improve the architectural design of Transformers. Prior work has shown that residual networks can be seen as discrete approximations of Ordinary Differential Equations (ODE) and explored methods to improve the quality of the solution. Specifically, this paper introduces a predictor-corrector learning framework to minimize approximation errors and an exponential moving average-based coefficient learning method. These advancements are used within Transformer architectures that are evaluated on several tasks such as translation, summarization, language modeling and language understand tasks, improving over standard and previous ODE-based Transformers.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well motivated and clearly described for the most part. It provides sufficient background to make the reading self contained and explain what is the novel contribution of the work. It's also well situated and compared to prior work on ODE Transformers as it discusses prior first order and high-order methods with a single step. 
- Builds on top of predictor-corrector methods from numerical analysis and extending them to improve their stability when training ODE Transformers. The key novelty lies in the selection of the predictor and corrector methods, and the proposal to use exponential moving average method to learn the coefficients in high-order methods instead of using constant values. 
- The proposed method uses a high-order method as predictor and a multi-step method as corrector and aims to provide a better approximation to the implicit ODE problem in Transformers than previous studies. 
- Presents empirical results on a variety of tasks are better than the original and previous ODE Transformers. This makes the work of interest to the researchers working in ODE Transformers.

Weaknesses:
- W1. There is lack of emphasis in the experiments on the computational overhead introduced by the high-order predictor and multi-step methods. Achieving better quality is not sufficient for adoption in practical settings. I'd suggest quantifying the training and inference cost compared to standard and ODE Transformers.
- W2. In terms of parameter efficiency, the proposed model achieves better performance with 1/3 of parameters only on one of the examined datasets. It's not clear if this is by chance or if the result holds on other settings. It would be useful to report performance with a smaller model size on the rest of the datasets to better establish the underlying claim. 
- W3. The impact to research communities beyond the ones focusing on ODE transformers is somewhat limited because the results are with relatively small model sizes. Showcasing that the results hold on larger architectures such as 7B Mistral would make the results more convincing. 
- W4. The improvement of the proposed multi-step high-order method compared alternative predictor-corrector methods is small (Table 10) and the benefit compared to simpler first-order methods from the predictor-only paradigm in a controlled setting is not provided (similar to the ablation in Table 10).

Limitations:
The limitation section covers the shortcomings when the proposed methods are applied to different model designs but lacks discussion about the experiment shortcomings related to model size, computational cost, and comparison with simpler methods in a controlled setting.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an approach to improve the performance of Transformer models for conditional natural language generation (machine translation and summarization).   The authors introduce a predictor-corrector framework, inspired by numerical methods for
solving ordinary differential equations (ODEs), to enhance the accuracy and
stability of the  models. The proposed model is evaluated against standard Transformer models on multiple benchmark datasets. The presented results show improvements in prediction accuracy and efficiency when compared to standard models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The application of the predictor-corrector paradigm to Transformer models is innovative and offers a new perspective, extending the ODE transformer proposed in (Li et al., 31).

The integration of high-order ODE solutions into the Transformer architecture is a novel contribution. The authors use EMA (Exponential
Moving Average) coefficient learning to enhance training stability.

The experimental results look comprehensive, with evaluations on multiple benchmark datasets demonstrating the efficacy of the proposed model.

The paper provides a detailed description of the methodology.

The paper is well-organized and clearly written, with each section leading to the next.

Weaknesses:
The paper does not compare its results with state-of-the-art  models, which could have provided a more comprehensive evaluation of its effectiveness.  For instance for WMT14 En to FR, we can reach 43 in BLEU, instead of 41 for Attention is all you need.  Moreover, the difference with ODE Transformer is very limited. 


The theoretical justification for the predictor-corrector framework could be more detailed, particularly in explaining how it relates to and improves upon existing methods. 

The computational overhead induced by the proposed method is only lightly discussed in the Appendix and would require more in depth discussion.

Limitations:
The authors should discuss the potential computational overhead introduced by the predictor-corrector framework.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents advancements in Transformer architecture to minimize errors in approximating solutions to Ordinary Differential Equations (ODEs). 

The contributions are:
- Introducing a learning paradigm with a high-order predictor and multistep corrector to reduce truncation errors.
- Proposing an exponential moving average-based method to enhance the predictor's learning ability and stability by replacing constant coefficients with dynamic ones.
- Demonstrating superior performance across various benchmarks, including machine translation, abstractive summarization, language modeling, and natural language understanding, achieving notable improvements in BLEU scores and parameter efficiency.

The work shows improvements in translation tasks and highlights the general applicability of the proposed methods across different natural language processing domains.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper introduces a new predictor-corrector framework within Transformer architectures, which is a fresh and innovative approach to addressing errors in approximating solutions to ODEs. The integration of a high-order predictor and multistep corrector, combined with an exponential moving average (EMA) coefficient learning method, represents a creative combination of established numerical analysis techniques with modern neural network architectures. 

The paper successfully applies the method across various natural language processing tasks. The paper is well-structured and clearly written, making complex concepts accessible.

Weaknesses:
The complexity of implementing these methods might be a barrier for practical adoption. The paper could benefit from providing more detailed guidelines or code to facilitate easier implementation and replication of the results.

The experiments primarily focus on natural language processing tasks. While the results are impressive, it remains unclear how well the proposed methods generalize to other domains, such as time series forecasting. Including preliminary results or discussions on the potential applicability to these other areas could strengthen the paper.

Limitations:
Provide a detailed analysis of the computational complexity and resource requirements of the proposed methods. This should include a comparison with standard Transformer models and an explanation of any trade-offs between performance improvements and computational costs.

Discuss the scalability of the proposed methods in more detail. Explain any challenges encountered when scaling up to larger datasets or models and how these were addressed. Provide insights into potential limitations in terms of scalability and how future work could overcome these challenges.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

";1
LLuSjg59an;"REVIEW 
Summary:
This study primarily investigates where in-context learning occurs within GPT-style models. Specifically, it explores the stage at which a model transitions from functioning as an in-context learner to a task-specific model. By applying layer masking to the instruction and in-context examples in machine translation and coding tasks, the authors observe performance changes to understand the internal mechanisms of in-context learning. They find that certain critical layers within the model are crucial for in-context learning.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
The experimental design is well-conceived and includes an analysis that elucidates the internal mechanisms of in-context learning.

Weaknesses:
1. The practical utility of the findings is questionable. I doubt whether the proposed improvements to inference efficiency in Section 5 can be applied in practice. For example, critical layers differ across tasks, and even within the same task, they can vary between subtasks (e.g., en→fr vs. fr→en). Identifying critical layers typically requires significant cost.

2. The experiments are entirely based on multi-head Attention, which limits the applicability of these findings. Most current models use other attention methods, such as the grouped-query attention used in Llama-2. The findings in this paper may not apply to new attention variants, which already consider partial context attention.

Limitations:
--

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tries to locate where LLMs handle in-context learning (ICL) and interprets how LLMs perform ICL internally. The authors propose a layer-wise attention masking strategy and conclude that LLMs perform ICL in bottom layers.

Soundness:
1: poor

Presentation:
3: good

Contribution:
3: good

Strengths:
The motivation of this paper is clear, and the research question sounds interesting.

Weaknesses:
However, I have several concerns about the work.
1. Layer-wise masking. The authors only release first j bottom layers and conclude that bottom layers are important. This is problematic. If the authors mask layers with a reversed order, I suppose the conclusion would change to that top layers are important. The reason is that pretrained LLMs solve task with multiple modules. And different layers may share similar functions for solving that task. Simply masking several layers or layers with specific order cannot prove whether these layers are important/useless. It’s similar to the glove game: you have 3 gloves with 2 left gloves and 1 right glove. Dropping 1 left glove randomly cannot prove this dropped one is useless, or conclude that the right one is the only necessary one. 
2. Token masking. The three token masking strategies are not comprehensive. There is no discussion about the individual examples. Either dropping all of them or keeping all of them cannot explain how ICL works, since many existing works show that ICL examples affect the performance much.

Overall, I think this paper tries to explore an important research question, and I would like to encourage the authors to address my two concerns.

Limitations:
See weakness.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In-context learning has emerged as an important paradigm in LLMs. In this paper, the authors attempt to characterize where models learn to “recognize” an in-context task. 

To do this, in-context portions (examples and/or instructions) are masked out after certain layers and are not included to generate model predictions.  An advantage of this style of masking is improved cost. Their experiments give clear results, indicating that for each of the selected tasks (machine translation, code generation), there exists a cutoff layer beyond which masking is acceptable without hindering ability to recognize the task. The most clear utility of this masking is computational savings. The authors also find evidence of 3-phase in-context learning, with the last phase accounting for little to no performance improvements.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
* This is an important area of research. The authors carve out a strong motivation, which is enhanced by clearly defined and executed experiments. 
* The paper is very well written, which makes it an interesting read. 
* The experiments are extremely thorough, some of them being: ex v/s Inst masking, MT v/s code, attention to context v/s input, # prompts analysis, attention heads study etc. I can tell that these took a lot of effort and will be of great importance to the community.

Weaknesses:
I have some concerns about the definitions of “task recognition” [more in the Questions], as well as explanations on some sections (like Sec 4.3 and 6.2). It would be important to iron these out.

Limitations:
NA

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates where in-context learning occurs within large language models, focusing specifically on machine translation and code generation tasks. The authors introduce a ""layer-from context-masking"" technique to identify at which layer an LLM transitions from task recognition to task execution during ICL. They apply this method to several models including GPT-Neo2.7B, BLOOM3B, LLaMA7B (base and chat), and StarCoder2 (3B and 7B).

The key finding is that models do not need to maintain attention over the entire context throughout all layers to perform the task. Instead, there appears to be a ""task recognition point"" after which attention to the context is no longer necessary. This point varies between LLMs but generally occurs in the middle layers. The authors characterize this as a three-phase process: initial processing where masking has little effect, a critical phase where masking greatly impacts performance, and a final phase where additional context processing yields minimal gains.

The paper also explores the roles of instructions versus examples, the differences between instruction-tuned and non-instruction-tuned models, and whether these phenomena generalize across different tasks. 
The authors highlight the potential practical implications of their findings, particularly for inference efficiency. They estimate that up to 45% computational savings could be achieved by eliminating unnecessary context processing in later layers.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The authors introduce a ""layer-from context-masking"" technique to probe the internal workings of large language models during in-context learning tasks. This method offers a fresh perspective on how these models process and utilize contextual information, providing insights that were not previously available. 
- The identification of a ""task recognition point"" and the characterization of a three-phase process in task recognition and execution are important.
- The finding that attention to context can be removed after certain layers without significant performance degradation has direct practical applications for improving inference efficiency in LLMs, potentially leading to substantial computational savings.

Weaknesses:
- One notable weakness is the limited exploration of why different models exhibit varying behaviors in terms of their ""task recognition point"" and critical layers. For instance, the authors observe that GPT-Neo has more severe critical layers compared to other models but do not provide a thorough analysis of potential reasons for this difference. A more in-depth investigation into the architectural differences, training data, or other factors that might contribute to these variations would significantly enhance the paper's insights and generalizability.
- The paper doesn't adequately address potential confounding factors that could influence the results. For example, the impact of different tokenization schemes across models or the potential effects of the specific prompt format used are not discussed in depth.
- The paper's focus on machine translation and code generation tasks, while valuable, raises questions about the broader applicability of the findings. The authors could strengthen their work by discussing potential limitations in generalizing these results to other types of tasks or by providing a theoretical framework that explains why these findings might (or might not) extend to other domains of language model application.
- The paper uses models of different sizes (2.7B to 7B parameters) but doesn't systematically analyze how model size affects the location of the ""task recognition point"" or the distribution of critical layers. A more structured comparison across model sizes could reveal important trends.

Limitations:
In the NeurIPS Paper Checklist (after appendix), they mentioned that they have discussed the limitations in the conclusion section (Line#592). However, I didn't find any such thing.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
WpEaUIBIWH;"REVIEW 
Summary:
The paper addresses unsupervised anomaly detection by proposing a method named UniCAD. The authors aim to enhance anomaly detection performance by establishing a theoretical connection between representation learning, clustering, and anomaly detection. They introduce a unified framework that jointly optimizes these three components, using a probabilistic mixture model and a Student's-t distribution for robust representation learning and clustering. The framework also includes an anomaly-aware data likelihood objective, which reduces the impact of anomalous data on the learning process. Additionally, the authors propose a gravity-inspired anomaly scoring method that leverages relationships between samples and clusters.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Modeling the connection between representation learning, clustering, and anomaly detection is highly relevant. This paper effectively demonstrates how these three tasks are interrelated and can be jointly optimized to improve anomaly detection performance.

2. The paper is well-written, presenting its hypothesis and method clearly. 

3. The results are impressive and demonstrate the effectiveness of UniCAD.

Weaknesses:
1. The ablation study on the hyperparameters $k$ and $l$ is insufficient. The authors only present results from a single dataset, satimage-2, where their method achieves an almost perfect score. It would be more informative to perform ablation studies across all 30 datasets or at least a subset where the model also shows lower performance. This broader analysis would demonstrate how these hyperparameters affect the average ranking of the method, similar to the results reported in the paper's table.

2. The authors introduce a $g(\Theta)$ term to prevent shortcut solutions, mentioning it in Equation 15. However, they do not discuss its importance or impact on performance after its introduction. Key questions remain unanswered, such as how the $g(\Theta)$ term affects the model's performance, what happens if it is removed, and how the autoencoder is implemented. These details are crucial, as the regularization term may significantly influence the results.

Limitations:
The authors mention some of the limitations, but they do not address the potential negative impact of the work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes UniCAD, a theoretically unified framework for representation learning, clustering, and anomaly detection. This paper first introduces the mixture of Student-t distribution $p(x|\Theta, \Phi)$ with degree of freedom $\nu=1$ based on a representation learner $f_\Theta$ using NN. Then, this paper combines with an anomaly indicator $\delta$ for maximum likelihood estimation. Parameters $(\Theta, \Phi)$ are optimized by EM algorithm and SGD. In addition, when detecting anomalies, an improved score is used with reference to gravity. The UniCAD achieved good performance on experiments with various datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is well written and easy to follow.
- Good experimental results.

Weaknesses:
- We have several questions about the proposed method and experiments. Please see Qustions.
- The comparison with DeepSVDD and DIF is excellent, but I think the paper also needs to be compared with other Deep anomaly detection methods. For example, DROCC [1].

[1] Goyal, Sachin, et al. ""DROCC: Deep robust one-class classification. ""International conference on machine learning. PMLR, 2020.

Limitations:
- Hyper-parameter sensitivity seems to be one limitation.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel probabilistic mixture model for unsupervised anomaly detection (UAD) that unifies representation learning, clustering, and anomaly detection into a single theoretical framework. The proposed UniCAD model addresses the lack of a unified approach in existing methods, which often consider these components separately or in pairs. The experimental results show that UniCAD consistently outperformed other methods in terms of AUC-ROC and AUC-PR. The model’s iterative optimization process using EM was also highlighted as effective and convergent.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- This paper introduces a novel integration of a probabilistic mixture model that unifies representation learning, clustering, and anomaly detection into a single theoretical framework.
- The proposed approach is well-motivated (Fig. 1) and supported by a robust theoretical foundation that maximizes anomaly-aware data likelihood, ensuring the model effectively leverages the interplay between representation learning, clustering, and anomaly detection.
- The paper is well-written, offering clear and comprehensive explanations of the proposed method, including detailed theoretical derivations and intuitive motivations for the design choices. The methodology section is particularly well-structured, logically outlining the steps and equations involved in the proposed model.
- The comprehensive evaluation design underscores the robustness of the proposed method.

Weaknesses:
- The connection between force analysis and anomaly detection, particularly between Equations 7 and 8 in Section 3.2.1, could benefit from further justification. While the analogy is interesting, it may not be immediately intuitive for all readers.
- The iterative optimization process may pose scalability issues for large datasets. An in-depth analysis and discussion of this would further strengthen the quality of this research.
- Although the model maps data to a low-dimensional representation space, the effectiveness of this mapping for very high-dimensional datasets could be explored further.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose UniCAD to jointly model representation learning,
clustering and anomaly detection.  The main objective is maximizing
the product of anomaly indicator (1 is normal, 0 is anomaly) and the
joint probability of instance x_i and cluster c_k given parameters for
representation learning theta and clustering phi.  The joint
probability is decomposed into the prior of c_k and likelihood of
p(x_i|c_k), which is modeled by a Student's-t distribution on the
distance between representation z_i and mean mu_k with covariance
Sigma_k.  p(x_i) is the marginal over c_k.  Anomaly indicator delta is
zero for p(x_i) in the lowest l percentage.  The anomaly score is
1/p(x_i).

Compared to Newton's law of Universal Gravitation, the anomaly score
function has similar components, except for the unit vector r_ik
(which indicates the directions of forces, beyond the
magnitudes). Hence, they incorporated r_ik into their anomaly score
function.

For updating the clustering parameter phi (mixture weights, means,
covariance), they use EM.  In the E-step, they estimate the posterior
p(c_k|x_i).  In the M-step, they estimate phi.  For updating
representation parameters theta, they use gradient descent to minimize
negative log likelihood of instances, together with a reconstruction
loss via an autoencoder to prevent shortcut solutions.

For empirical comparisons, they use 30 tabular data sets and 17
existing algorithms.  The proposed approach generally outperforms the
others in terms of average rank in AUCROC.  The vector version of
anomaly score function is ranked higher than the scalar version.  On
computation time, UniCAD is in the middle among 5 algorithms.  Ablation
studies indicate the contributions of the different components.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main contribution is combining representation learning,
clustering, and anomaly detection in a unified single probabilistic
formulation, which is interesting.

The empirical results indicate that UniCAD compares favorably against
17 existing techniques on 30 tabular datasets.  Compared to four
existing algorithms, computation is not the most intensive.

The paper is generally well written.

Weaknesses:
The clustering part is similar to a typical Gaussian mixture model for clustering via EM, except for t-distribution instead of Gaussian and the scaling factor.

Two neural-network-based approaches were compared.  As UniCAD utilizes
representation learning, comparing with more approaches that utilize
representation learning would be significant.  Approaches without
representation learning have an inherent disadvantage.

Some parts could be clarified--see Questions.

Limitations:
Limitations of the proposed approach do not seem to be mentioned.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
KgGhxmQFFy;"REVIEW 
Summary:
introduce a molecule tokenizer based on Codebooks which gets integrated into UniMoT, a unified molecule-text LLM

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
strong performance on a variety of benchmarks; outperforms/on pair with 3D-MoLM

Weaknesses:
- Table 1: include CLAMP [1] as well as standard molecular fingerprints (incl. in said reference); further KV-PLM linear probing for ToxCast results in 66.3 AUROC compared to 55.03 in your paper

[1] Seidl, P., Vall, A., Hochreiter, S., & Klambauer, G. (2023, July). Enhancing activity prediction models in drug discovery with the ability to understand human language. In International Conference on Machine Learning (pp. 30458-30490). PMLR.

Limitations:
-

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propsed Uni-MoT, a unified structure to align molecules with texts with a VQ tokenizer and the Q-Former in BLIP-2. By treating molecules as new word tokens in the codebook, Uni-MoT aligns the discrete token representation for molecules and texts, while also following the autoregressive manner of LLMs. The training of Uni-MoT follows four main stages, Causal Q-Former Pretraining, Molecule Tokenizer Pretraining, Unified Molecule-Text Pretraining, Task-Specific Instruction Tuning. The experiments demonstrate that Uni-MoT can achieve better performance compared to the selected baselines.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The performance of Uni-MoT is overall good and better than the baseline models.
2. Uni-MoT provides a alternative solution for aliging molecules with texts.

Weaknesses:
1. Although the authors claim that Uni-MoT follows a different structure as shown in Figure 1 c, it turns out that it still follows the BLIP-2 [1] structure, which has been widely used to align 2D molecule graph [2] and 3D molecule information [3]. Thus, **the technical contribution and novelty of this paper are extremely limited**. Especially when VQ-VAE [4] is also a well-developed structure adopted in computer vision. This paper is more like simply swapping the input of the Q-former in BLIP-2. 
2. The experiments are only conducted on a single serie of LLM, Llama-2, **which is not sufficient to demonstrate the model agnosticsm of Uni-MoT**. In fact, LLMs like Mistral [5] and Meditron [6] might possibly achieve a better performance. Meanwhile, the selection of Llama-2 is also not convincing, as Llama-2 is not specially pre-trained on chemistry or biomedical corpora. 
3. **The seletion of the datasets is also worth discussion**. The result on ChEBI-20 is presented in Appendix, while the main experiments are conducted on PubChem. I am wondering why not also conduct the remaining experiments on ChEBI-20, as the data scale of ChEBI-20 is much larger than PubChem. At the same time, the reverse task, text-based molecule generation, on ChEBI-20 and PubChem is surprisingly not presented.
4. **The comparison with the baselines is not fair enough**. For example, MolCA adopts Galactica-1.3B as its backbone model, which is much smaller than Llama-2-7B. Thus, the proposed method can not demonstrate its superiority compared to previous methods. Notably, since the authors mentioned MolCA and 3D-MoLM, it is necessary to discuss the possible affects of the modalities. Furthermore, several SOTA baseline models like BioT5 [7] and BioT5+ [8] are not discussed.
5. **The ablation study is also not sufficient without providing the naive fine-tuning performance of Llama-2**. Besides, as Uni-MoT incorporates the VQ tokenizer, it is also important to discuss the size of the codebook.
6. During the pre-training stages, it should ensure that the **data leakage** is avoided. Considering the pre-training dataset adopted has overlaps with the fine-tuning dataset [7, 8], the performance gain could possibly come from the data leakage.
7. **Some claims and definitions are confusing**. e.g. In Line 44, ""a unified molecule-text LLM"", I do not agree with the claim of an ""LLM"". The ""LLM"" is still the Llama-2. It should be like ""structure"" or something.

#### References
[1 ]Li, J., Li, D., Savarese, S., & Hoi, S. (2023, July). Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning (pp. 19730-19742). PMLR.
[2] Liu, Z., Li, S., Luo, Y., Fei, H., Cao, Y., Kawaguchi, K., ... & Chua, T. S. (2023). Molca: Molecular graph-language modeling with cross-modal projector and uni-modal adapter. arXiv preprint arXiv:2310.12798.
[3] Li, S., Liu, Z., Luo, Y., Wang, X., He, X., Kawaguchi, K., ... & Tian, Q. (2024). Towards 3D Molecule-Text Interpretation in Language Models. arXiv preprint arXiv:2401.13923.
[4] Yu, J., Li, X., Koh, J. Y., Zhang, H., Pang, R., Qin, J., ... & Wu, Y. (2021). Vector-quantized image modeling with improved vqgan. arXiv preprint arXiv:2110.04627.
[5] Jiang, A. Q., Sablayrolles, A., Mensch, A., Bamford, C., Chaplot, D. S., Casas, D. D. L., ... & Sayed, W. E. (2023). Mistral 7B. arXiv preprint arXiv:2310.06825.
[6] Chen, Z., Cano, A. H., Romanou, A., Bonnet, A., Matoba, K., Salvi, F., ... & Bosselut, A. (2023). Meditron-70b: Scaling medical pretraining for large language models. arXiv preprint arXiv:2311.16079.
[7] Pei, Q., Zhang, W., Zhu, J., Wu, K., Gao, K., Wu, L., ... & Yan, R. (2023). Biot5: Enriching cross-modal integration in biology with chemical knowledge and natural language associations. arXiv preprint arXiv:2310.07276.
[8] Pei, Q., Wu, L., Gao, K., Liang, X., Fang, Y., Zhu, J., ... & Yan, R. (2024). BioT5+: Towards Generalized Biological Understanding with IUPAC Integration and Multi-task Tuning. arXiv preprint arXiv:2402.17810.

Limitations:
Yes. They have discussed the limitations as not enough tasks, data scarcity, and real scenarios.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work presents a new molecule LLM that uses a pretrained tokenizer to replace the projection layer. The tokenizer consists a Q-Former and a VQ module, which are trained with consistency loss. The model is evaluated on molecular understanding and generation tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The authors provided a novel framework to align text representation in LLMs and the molecules. 
- The authors conducted experiments on many datasets.

Weaknesses:
- The claim that adapter-based methods cannot do text-to-molecule generation tasks in not accurate (Sec.1). They can always adapt text-encoder to a pre-trained SMILES or graph generator. This may make this work not well motivated.
- In the tokenizer (Fig.2), it seems that multiple alignment methods are required to train the model, including: (1)the molecule and text contrastive in Q-Former, (2), the aligning MSE loss, (3) the SMILES decoder reconstruction. It's not clear about the design reasons, and if all of them are required.
- Given the learnable query has a fixed size, it may not perform well for larger molecules.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose to use a vector-quantized tokenizer that incorporates a Q-Former to connect pre-trained molecule encoder, SMILES encoder, and SMILES decoder so that a language model can integrate molecule and text modalities. Based on the proposed tokenizer, the authors introduce a four-stage training strategy to train UniMoT, a unified molecule-text LLM proposed in this submission. The performance of the UniMoT is evaluated empirically with 7 tasks in the areas of molecule comprehension and molecule generation. Some ablation studies are also conducted.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is well-organized and well-written.
- Using a vector-quantized tokenizer and a Q-Former to connect different modalities could be somewhat novel.
- The proposed UniMoT outperforms baselines in most cases and reach comparable performances in others.
- The authors provide many implement details which increases the reproducibility.

Weaknesses:
- All the components are borrowed from existing works. Besides the pre-trained molecule encoder, SMILES encoder and decoder, both vector quantization and Q-Former are proposed in previous works [1][2]. The Q-Former part of this paper (Appendix A) is very similar to Q-Former's original paper. Even Figure 4 in this paper is very similar to Figure 2 in Q-Former's original paper [2].
- When generating molecules the proposed method relies heavily on a pre-trained decoder. In the decoder's original paper, the reported validity is 99.9 and no guarantee is provided that the generated SMILES string will be always valid [3]. The 100% validity reported in this paper could be attributed to overfitting.
- Some hyperparameter choices are not well justified and studied. For instance, the molecule codebook size is set to 2048, but there is no explanation why 2048 is chosen. How the molecule codebook size affects the performance is also not studied.
- There are some existing works about molecule tokenizers [4][5], the paper lacks the comparison of the performance using different tokenizers.
- The robustness of the model is not studied. Each molecule has many synonyms, how the proposed method performs given different synonyms of the same molecule is desired to know.

[1] Van Den Oord, Aaron, and Oriol Vinyals. ""Neural discrete representation learning."" Advances in neural information processing systems 30 (2017).

[2] Li, Junnan, et al. ""Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models."" International conference on machine learning. PMLR, 2023.

[3] Irwin, R., Dimitriadis, S., He, J., Bjerrum, E.J., 2021. Chemformer: A Pre-Trained Transformer for Computational Chemistry. Mach. Learn. Sci. Technol.

[4] Li, Xinhao, and Denis Fourches. ""SMILES pair encoding: a data-driven substructure tokenization algorithm for deep learning."" Journal of chemical information and modeling 61.4 (2021): 1560-1569.

[5] Schwaller, Philippe, et al. ""Molecular transformer: a model for uncertainty-calibrated chemical reaction prediction."" ACS central science 5.9 (2019): 1572-1583.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
SFxAjB7UXx;"REVIEW 
Summary:
This paper introduces the *modular norm*, which is a norm designed to be adapted to neural network (NN) optimization. Specifically, an optimization process involving a gradient computed according to this norm scales with the size of the NN to be optimized. This paper provides the algorithm of computation of the modular norm, along with a small set of training experiments.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
# Originality

Creating a norm making the optimization process scale with the size of the trained NN is original.

# Clarity

Overall, the general idea is easy to understand.

# Significance

Finding the scaling laws linking the hyperparameters (learning rates, initialization scale, penalty, etc.) to the architecture of a NN is a major field of research in NN optimization.

# Quality

Lines 87--92, the authors state an interestng motivation of their work:

> In machine learning, we we would ideally like [...] that meets these requirements: the *modular norm*.

Overall, the motivation and the idea of the modular norm are appealing.

Weaknesses:
# Clarity

A major issue of the paper is the absence of list of contributions. In other words, the authors do not make any general claim about their results. The list of contributions *must* appear somewhere (ideally in the beginning), so that one would have a formal basis to evaluate the paper (are the claims significant enough? are they well justified? etc.). 

Section 4 (experiments): one would expect a description of the implementation of the modular norm in an optimizer. As such, I do not understand what is computed (mass? sensitivity? norm?), when (at each step? epoch? ...) what is used in the optimizer and how.

Major typographical issue make the main text hard to read: some notations overlap with each other (line 148, lines 162--163, etc.).

# Significance

No list of contributions is provided, so it is difficult to evaluate the significance of the paper.
Besides, the set of experiments is very limited.

# Quality

The contribution of this paper is unclear for the reasons aforementioned. 

Moreover, given the context, the motivation and the related works, one would expect a comparision with similar ""scaling"" techniques (muP and NTK parameterizations, for instance. It is well-known that standard training (with the usual parameterization) diverges as the number of ""blocks"" or the ""width"" tends to infinity (Fig. 4), so it is not surprising that the proposed method performs better in this situation.

Limitations:
Lack of experimental validation, lack of clear list of contributions.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors are tackling the difficult problem of try to scale the parameters of a network so that different sizes of network have similar optimisation properties.  This would greatly aid in hyper-parameter tuning.  They do this by introducing a new norm which is defined for the whole network.  They show experimental results providing evidence that their approach is successful.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This is a hard problem that is worthy of study.  The approach is mathematically rigorous in so far as it introduces a new norm.  The norm is designed to capture features of the network that are important in scaling and this is backed up by experimental results.

Weaknesses:
The approach feels a bit ad hoc, particularly the introduction of ""masses"".  It feels a little bit like the problem of adjusting hyper-parameters has been pushed onto determining masses for modules.

Limitations:
I see no issues here.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose the “modular norm”, a norm for deep learning that can be simply recursively composed, allowing for easily recursively computing (and hence controlling) the Lipschitz constant of the network and loss gradient.
They propose how to scale the gradient updates by the modular norm, and empirically demonstrate the effectiveness of such ‘normed optimization’ at achieving invariance of the optimal hyperparameters to scale.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The paper proposes a very interesting idea: an architecture-agnostic norm for neural networks that gracefully scales in width and depth.
This seems like a very interesting take on what is currently being achieved through convoluted scaling of initialisations, learning rates, and residual branches following rules derived from asymptotic properties of these rules.
This paper has challenged how I think about sharpness and curvature of the loss landscape in a deep learning context, and the asymptotic properties thereof. I'll admit, I'm still digesting the take-aways, but I feel fairly confident many in NeurIPS community would benefit from reading this paper.

Lastly, the experimental results look on learning rate transfer are fairly robust, and look very promising.

Weaknesses:
1. The use of the term ‘feature learning’ seems somewhat distinct in this paper from the way it's been used in the Tensor Programs (TP) literature. To the best of my understanding, in TP4, it refers to the lack of or presence of change in how each layer processes its inputs in the infinite width limit.  In Proposition 3, the amount of “feature learning” seems to refer to a *bound* on how much the way a layer processes its inputs can change. I think it would have been great if the authors defined or front-loaded what they mean by ‘feature learning’ and ‘proportion of feature learning’ before Proposition 3. E.g., lines 128-131 were difficult to parse on a first read-through.
	- Similarly, the controlling the ‘proportion of feature learning’ claims are teased again without clarifying what is meant by feature learning in lines 171-173. I think these lines could be cut to be honest.
2. I think the presentation could use a little bit of work. A lot of it is very good already. In particular, the notation, definitions and propositions read well, and the discussion of limitations and future work in section 5 was very useful and interesting. That being, I think section 2, which was setting up the motivation for what follows, left me a bit confused on my first read-through. I would have hoped to give concrete suggestions on how I'd like to see it changed, but that is a non-trivial task, so I'll point to what I think is confusing at the moment:

The authors mention two motivations for the modular norm in the abstract: a) graceful scaling in width and depth, and b) ease of constructing a Lipschitz-continuous network (in the modular norm). The elaboration on and setup of these two motivations than gets slightly jumbled, in my opinion. The introduction primarily focuses on a) – the graceful scaling.  Then, section 2, to the best understanding, gives an argument for why achieving Lipschitz continuity with a tight constant that's invariant to the scaling dimension might result in the step-size being invariant across scale. As I understand it, this is a fairly loose argument, that's not being corroborated with further formalism or arguments later on.

I think, when first reading section 2.1, I would have appreciated:

1. Having a better sense of direction:
	1. Making it clear that the link between graceful scaling of step-size and Lipschitz continuity is a loose motivating argument, that's absolutely useful, but will not be formalised later on.
	2. Making it clearer that the goal of the following sections will be to present a modular norm in which it will be easy to specify networks with a Lipschitz-continuity that's invariant to a scaling dimension.
	
Then, I think the relationship before what section 2.3 lays out — achieving norm $\eta$ updates in modular norm — and section 2.1 could also have been made much clearer.

3. Some smaller points:
- On lines 81-83, it seems like bulletpoints (ii) and (iii) could be merged. They refer to the same desirable property, and one is just setting up the other.
- The sharpness bound in equation 2.1 could use a citation (or link to appendix derivation) for pedagogical purposes.
- It would have been great to define (or cite a reference for) what a matrix norm induced by input and output space norms is. I wasn't familiar with this term.
- The authors claim that equation (2.2) holds quite tightly. Do the authors have a reference?

Limitations:
1. If I'm understanding things correctly, Proposition 3 does not guarantee the _presencence of feature learning_ in the modular norm, the same muP guarantees a change in the way a layer processes its inputs by at least $\Omega(1)$ as the width goes to infinity?
2. Only transfer of the step-size and mass allocation across scale is considered, and not of other training hyper-parameters (e.g. learning rate schedule).

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a new normalization strategy for deep models rooted in the introduction of a new framework and on feature learning considerations. The authors provide a few experimental examples as motivation, and then start introducing their framework. They formally define what a module is and its norm and then present a few results on module composition and relation to gradient/hessian-dependent quantities. The authors showcase that, on some toy experiments, their ""modula"" package yields improvements over vanilla SGD, closing the gap to Adam.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The paper is well-written, presentation is formal, notation is pleasant. Illustrations are very well done and in general the whole work is very thought-through. I also like the idea: not modifying the optimizer or the architecture, but normalizing stuff before it is fed into the optimizer. This is very flexible and I hope the authors can scale this up on bigger models.

Weaknesses:
There are 2 weaknesses I think are hurting the paper, but both are solvable.

1) While the paper reads well, I found a lot of distance between the discussion of SGD in formula 2.1 and the results of Proposition 5. In between, there is little discussion of SGD. When reading, I felt a bit lost into the definitions and formalism - seemed the connection was just used as motivation but one has to to through 4 pages to get back at it. I think the authors should put proposition 5 sooner as a main result, and actually as a motivation for introducing the modular norm.

2) This is solvable but hurts the contribution a lot: experiments. Resnet experiments are ok - but where we really need the thing to shine is transformers since there SGD has a huge gap. I think the current status is lacking a bigger transformer architecture: the context length used is 128 for GPT - very small. I know that if you have resource limitations, this can be demanding, but I am afraid your idea would be lost in the literature if you don't provide further evidence. If I have to make a suggestion: pythia 160M (https://github.com/EleutherAI/pythia/blob/main/models/160M/pythia-160m.yml) is a good model: probably takes < 0.5 day on a single GPU (train e.g. on slimpajama). If you feel very confident, try 24 layers. I think you only need to run this with SGD lr = [1e-3, 1e-2, 1e-1, 1], use the standard Adam parameters reported in the link above, and then run module on SGD. I would keep the context to 1k or 2k. I think if this works many people will pay attention.

I am happy to raise to accept if you can provide some experimental evidence in the rebuttal.

Limitations:
Experiments, see above.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
nkwPiBSw1f;"REVIEW 
Summary:
This paper focuses on personalization and test-time adaptation in federated learning of foundation models. The authors propose two solutions for this setting and show its performance on two splits of FLAN dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper is generally easy to read.
- The constructed experimental setup is genenrally reasonable.

Weaknesses:
- The motivation and significance of the setting is not sufficiently strong. The training method in this paper is not new in personalized FL [1,2]. The test-time adaptation method is straightforward and does not convey new insight. Therefore, I do not clearly see the significance of considering personalization and test-time adaptation at the same time. 

- Limited performance gain. Despite that the paper focuses on personalization and test-time adaptation at the same time, it proposes two methods, one performs better at personalization while the other performs better at test-time adaptation. This is really confusing. If the authors can not find one solution that fits both two metrics, then why bother to consider these two issues in one paper?

- Limited performance gain. From the two main tables in the paper, the performance of FedLoRA is stable across two metrics, which always performs the second best.
 
[1] Exploiting shared representations for personalized federated learning

[2] Perada: Parameter-efficient and generalizable federated learning personalization with guarantees

Limitations:
yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors use two set of adapters to personalize a model with federated learning. The idea is to use FL to learn the global adapter whereas each device has a local adapter to personalize the model for each client.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well written and easy to understand 
- The overall approach is sound
- THe authors evaluated their approach on a number of NLP tasks

Weaknesses:
Overall this is an interesting paper. However these are some areas to potentially improve: 

- Firstly, the overall premise: the authors say that this approach is used to learn from distribution shifts between training and test datasets. However, this is the whole reason we use federeated learning. Typically we have models trained on public deatasets (e.g., imagenet, wikipedia, etc), but these don't work on real-world applications because the domain is slightly different. FL was proposed so we can get a real-world (and real-time) signal about in-domain distributions in a privacy-preserving. As a result, we actually do want to learn this ""test"" distribution during FL. It is unclear why in the FL scenario, the ""test"" distribution is not similar to the train distribution. Overall, the paper would benefit for a much stronger motivation. 

- There are a lot of works that use federated learning to learn a global model and then personalize this model for each client (e.g., https://arxiv.org/abs/2103.00710). As a result, the overall approach of training a global model and then having extra client-side personalization is not entirely novel. Furhtermore, there have been many works that train adapters instead of full models in order to save on communication and on-device compute. 

- At section 3.1 the authors say there are two datasets belonging to two different distributions (Ps vs Pt): why does this happen within a device ? How do you detect that the distribution has shifted ? How does this happen in practice ?

- The evaluation has been done in a very artificial scenario where each of the 8 devices had a completely different NLP task (section 5.1). This is quite extreme and it is likely to favour the results that take advantage of personalization. However, this extreme is unlikely to happen in the real FL deployments: typically we want to build a global model from a set of non-IID users but not to an extreme where each user has a different task. Furthemore, we attempt to train large-enough models that can tolerate some distribution shift (as long as it has been seen in the data).

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel dual-personalizing adapter to tackle the test-time distribution shift for federated foundation models (FedFM). FedFM is a new research domain to enhance foundation models by leveraging many fine-tuning tasks on many protected datasets for end users. The solution is essentially to tackle the trade-off of personalisation and generalisation of the client-specific models in FedFM.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. FedFM is a new research domain that has been regarded as an important pathway to enhance foundation models by leveraging decentralized data. Tackling the tradeoff between client-specific personalisation and generalisation is a key challenge of the FedFM research domain. 
2. The proposed method is a new idea with original design. Moreover, this paper is the first work to discuss the test-time generalization challenge on FedFM.  
3. The proposed method is technique sounds. The design of the method is simple yet effective. It is very easy to follow. 
4. In terms of clarity, the paper’s contents are well organized and clearly presented with easy-to-understand figures and well-described details. 
5. The appendix with experiment details and the source codes are provided to support the reproducibility of this work.

Weaknesses:
1. The important weight of the global adapter and personalised adapter is a key factor of the proposed dual-personalising adapter mechanism in FedFM. More insightful discussion is expected to analyse the selection of the important weights. 
2. The proposed instance-wise trade-off mechanism essentially relies on the similarity between the test instance and training dataset. The current solution is straightforward and reasonable, however it could be elaborately designed.
3. It would be better if a large-scale experiment could be conducted to further evaluate the proposed method. However, considering the lack of computation resources, this paper’s experiment is sufficient enough as an initial exploration and discussion of this research direction.

Limitations:
N/A

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
Federated Foundation Models (FedFM) is an emerging research domain to study collaboratively fine-tuning the pre-trained foundation models. This paper studied a test-time distribution shift problem on FedFM by proposing a new dual-personalising adapter.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1) The proposed method is novel. The targeting problem is new while a new setting is created in the experimental study.

2) The paper presents high-quality content and novelty design. The claimed points are well supported by theoretical discussion and experimental analysis. The appendix and source codes provide sufficient details of the experiment.

3) The clarity of this paper is excellent.

4) The targeting problem is significant to the emerging domain of FedFM.

Weaknesses:
1) According to Eq 1, the P_all is all potential distributions. However, in real applications, the clients are usually insufficient to represent all possible distributions. A discussion is required to clarify this assumption. 

2) The implementation framework heavily relies on the LoRA, thus the contribution is reduced.

3) The experiment setting assumes each client owns one type of dataset or task. What’s the difference with multi-task learning? Should you compare it with some multi-task learning baseline methods?

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
zuWgB7GerW;"REVIEW 
Summary:
This paper introduces Accordion Networks (AccNets), a novel neural network structure composed of multiple shallow networks. The authors propose a generalization bound for AccNets that leverages the F1-norms and Lipschitz constants of the subnetworks, demonstrating that these networks can break the curse of dimensionality by efficiently learning compositions of Sobolev functions. The paper also provides theoretical insights and empirical validation, showcasing the superior performance of AccNets in learning complex compositional tasks compared to shallow networks and kernel methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The introduction of Accordion Networks (AccNets) as a novel neural network structure is a creative and original contribution. The paper provides a thorough theoretical analysis supported by empirical evidence, ensuring the soundness of its claims. The ability of AccNets to break the curse of dimensionality by learning compositional functions efficiently addresses a fundamental challenge in high-dimensional learning tasks.

Weaknesses:
1. The practical implementation of the proposed regularization methods might be challenging, particularly the first one requiring infinite width. 

2. The paper mentions the difficulty in optimizing Lipschitz constants, which could be a limitation in practical applications.

3.  Additional experiments on more diverse real-world datasets could further demonstrate the robustness and generalizability of AccNets.

4. Although the author has discussed the differences between DNN and AccNet, there is still not enough information for me to be sure in which settings to use AccNet and in which settings to use DNN. More clear differences and applicable conditions, especially the shortcomings of each need to be pointed out.

Limitations:
See weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present a generalization bound for deep neural networks that describes how depth enables models to learn functions that are compositions of Sobolev functions. To do this, they both prove a generalization bound for compositions of accordion networks (densely connected networks with a low-rank weight structure) and for compositions of Sobolev functions. They then present a sample efficiency result for different kinds of regularization on accordion networks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
I really liked this paper and would like to see it accepted to NeurIPS. It addresses an important question: how does depth change generalization bounds for deep neural networks? To my knowledge, not many papers so far have addressed this question and I found the findings presented here very interesting and well embedded within prior methodology.

I also found the paper very well written. I found it easy to follow along despite the highly technical nature of the results (note that I did not check the proofs in particular detail). I especially appreciated the remarks explaining different potential extensions and limitations.

Finally, the theory appears to be able to explain certain empirical phenomena (in networks trained under realistic paradigms) at least qualitatively (though note that I had a few questions I will mention under weaknesses and questions). This indicates to me that it is a promising way for thinking about generalization in deep neural networks.

Weaknesses:
1. I would like to see a more thorough comparison with shallow networks and generalization bounds, as this comparison is a central argument for the usefulness of the presented theory. While it is clear how the findings for the shallow network are a special case of the findings on the deep networks (as presented in Thm. 1), it remains a bit unclear to me how the theory can explain improved generalization in deep compared to shallow networks. The authors certainly present different several pieces of evidence on this: both Fig. 1 and Fig. 3 demonstrate that shallow networks exhibit worse scaling. I also appreciated the theoretical explanation of a particular contrast in l. 256-261. However, I think it would be really useful to provide a general theoretical explanation for this difference and test it empirically: would it be possible to extend the theoretical comparison in l. 256-261 to the general experimental setup studied in the figures --- and if so, would this theoretical comparison predict the conditions under which deep networks have the strongest advantages over shallow networks (or perhaps the conditions under which they don't perform that much better)? Not only would this serve as a useful validation of the theory, I think it would also provide a more extensive intuition for the authors' findings.

2. I appreciated the fact that the authors compare their findings with related work wherever this becomes relevant. However, I think a (potentially brief) section comparing the results here to other theoretical investigations of depth in deep networks (perhaps using different approaches) would be useful. 

3. The linked codebase does not contain the notebooks indicated in the README as far as I can tell and therefore currently can't be used to directly reproduce the findings.

4. I believe the figures would still benefit from error bars or some other indication of the overall statistical error in the findings. I agree that the main contribution of this paper is theoretical, but since the experiments test the empirical validity of the theory, I believe it is nevertheless important to get a sense for the overall deviation in these findings (e.g. across model seeds). If the authors are concerned about a lack of clarity, they could leave the bars out of the main figures but add supplementary figures with error bars. Moreover, some of the lines in Fig. 1 do contain error bars and it would be good to clarify what these error bars represent.

Limitations:
The authors adequately discuss the limitations of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce accordion networks (AccNets), which are compositions of multiple shallow networks. By leveraging prior workthat computes norm-based generalization bounds for shallow two-layer networks, the authors bound the complexity of a deep AccNet (as measured by its F1 norm) but the sum of the complexities of the individual shallow networks. They empirically observe that the rates predicted on real-world data are roughly representative of the trained networks, and are indeed much better than those for kernels trained on the same tasks. They put forth a nontrivial scaling law for the excess risk: $N^{-\mathrm{min}(1/2, \nu_g/d_{in}, \nu_h/d_{mid})}$ for an Acc Net compared to $\mathcal L \sim N^{-\mathrm{min}(1/2, \nu_g/d_{in}, \nu_h/d_{in})}$ for a kernel in terms of the dimensionalities $d$ and Sobolev constants $\nu$ of the respective spaces and functions. From this, the authors obtain predictions of several phases, that they put forth experiments to verify.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper tackles a very important open question in the theory of deep learning, for which not much progress has been made. By creatively leveraging results for shallow network in composition, the authors arrive at a nontrivial bound for deep nets. The empirics are a very compelling and welcome part of the paper. The phase diagrams illustrate the nontrivial predictivity of the theory, especially at the level of the rates. This may have important implications for scaling laws. Modulo minor revisions in discussion and exposition, the whole paper is quite readable for a relatively broad audience.

Weaknesses:
I am not sure how compelling the phase plots in Figure 2 are. The bounds in general are extremely loose, however the comparison of the rates in Figure 2c and Figure 3 is very promising. In general, however, it is the experience of the reviewer that measuring a rate is an extremely finicky business. It is therefore important to add a section in the appendix explicitly stating how the rates were obtained and measured. I also strongly encourage the authors to make the code for all figures public. 

Because they are used very early on throughout the paper, it is the opinion of the reviewer that the notions of F1 distance and Sobolev norm should be defined earlier on in the paper. Without this, it seems like the audience will be constrained to the set of learning theorists familiar with these terms. However, if these terms are defined early on, the paper becomes remarkably accessible to a much broader audience.

Limitations:
Given the theoretical nature of this work, it is unlikely to have major social implications.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
jKzLukkKZO;"REVIEW 
Summary:
The paper ""Learning to Control the Smoothness of GCN Features"" investigates the impact of activation functions, specifically ReLU and leaky ReLU, on the smoothness of node features in Graph Convolutional Networks (GCNs). It provides a geometric characterization of these effects, showing how altering the input's projection onto eigenspace M can control the smoothness of output features. The study introduces a Smoothness Control Term (SCT) to modulate the smoothness of node features, aiming to improve node classification tasks in both homophilic and heterophilic graphs. Experimental results validate the efficacy of SCT, demonstrating significant improvements in node classification accuracy for several GCN-style models .

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
## Originality
- The paper introduces a novel approach to control the smoothness of Graph Convolutional Networks (GCNs) features, which is a significant departure from traditional methods. It builds upon and extends the work of Oono & Suzuki and Cai & Wang by integrating geometric insights with the message-passing process in GCNs.

## Quality
- The paper provides a robust theoretical framework, including geometric characterizations and proofs, that underpin the proposed methods.
- Extensive experiments validate the theoretical claims, showing significant improvements in node classification accuracy. Detailed descriptions of the experimental setup, including datasets and hyperparameter tuning, enhance the reproducibility of the results.

## Clarity
- The paper is well-structured, with clear sections that logically flow from introduction to theoretical analysis, experimental validation, and conclusions.
- The paper provides a comprehensive review of related work, situating its contributions within the broader context of graph neural networks research.

## Significance
- The ability to control the smoothness of GCN features addresses a to me interesting and important challenge in graph neural networks, with potential applications in various domains such as social network analysis, biological networks, and recommendation systems.
- The proposed SCT shows improvements in real-world datasets.
- The insights gained from this work could inform future research on activation functions and feature smoothness in other types of neural networks.

Weaknesses:
## Weaknesses
- The removal of white space in the paper makes it hard to read. The authors should really try and make the paper easier to read visually by not condensing as much math in the main text as possible. Not only is this arguably in violation of the guidelines, but it also illustrates that the authors need to distinguish clearer what the main contributions are and which parts in the main text can go to an appendix.
- While the geometric insights provided are valuable, the complexity of the mathematical formulations is challenging for readers not well-versed in advanced geometry and spectral graph theory. Simplifying explanations or providing more intuitive examples could enhance accessibility. Moreover, I feel like the math could be made more intuitive by giving verbal explanations before the theorems. The cramming of the paper and not highlighting enough what the main contributions should be improved.
- Although the paper compares SCT with a few baseline models, it would benefit from a broader comparison with additional state-of-the-art methods in GCNs and GNNs to provide a more comprehensive evaluation of its effectiveness.
- The experiments are primarily conducted on benchmark datasets. Incorporating more real-world applications and diverse datasets would demonstrate the practical relevance and versatility of the proposed method.
- The drop in accuracy for deeper models (16 or 32 layers) is noted but not deeply analyzed. A more thorough investigation into the causes of this performance degradation, beyond mentioning vanishing gradients, could offer insights into potential improvements. What happens if you use techniques that combat eg oversmoothing?
- While the paper mentions computational efficiency, a more detailed discussion on the computational overhead introduced by SCT, including potential trade-offs between accuracy and efficiency, would be beneficial.

Limitations:
## Limitations

The authors have acknowledged several limitations of their work, including the over-smoothing issue in deep GCNs and the dependence on specific activation functions, but they could improve by providing more detailed discussions on computational efficiency and potential negative societal impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the challenge of balancing smooth and non-smooth features in graph convolutional networks (GCNs) for node classification. Building on previous work that highlighted the correlation between feature smoothness and classification accuracy, the authors propose a novel method to control the smoothness of node features through a geometric approach and an augmented message-passing process. Their strategy involves establishing a geometric relationship between input and output vectors of activation functions like ReLU and Leaky-ReLU, and integrating a learnable term in the graph convolutional layers to modulate feature smoothness. The paper provides an empirical study to showcase the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper offers a novel geometric insight into the effects of different activation functions, specifically ReLU and Leaky-ReLU, on smoothing in GCNs. Furthermore, it introduces an innovative method to enhance the message-passing framework for GCNs (and similar networks) to better control smoothing.

Weaknesses:
* The empirical results section is very weak and a major revision is needed in order to be able to judge the merit of the proposed method. The most significant weaknesses are:
    * The paper does not compare to established baselines from other competing methods. For instance, one could take a look at the tables in https://arxiv.org/abs/2202.04579 or https://arxiv.org/abs/2210.00513 and add the results to the comparison in this paper (i.e., for cornell, texas, wisconsin, squirrel, and so on). 
    * The paper reports different (mostly weaker) performance for the baseline models such as GCN and GCNII. Again, this should be fixed and established results can be taken from the tables in the two papers mentioned above.
    * The improvement using the proposed SCT is very marginal in many experiments, and in particular the performance still drops (or improves very little) for higher number of layers. It is somewhat expected that increasing the number of layers and solving oversmoothing does not help much in homophilic tasks, i.e., the tasks considered in Table 1. However, if the method works it should lead to significant improvements on heterophilic tasks, even for increasing number of layers. Therefore, please show the results for the datasets in Table 2 in the same way as you show them in Table 1, i.e., for increasing number of layers.
    * It is claimed in Table 1 that the drop in performance of GCN for increasing number of layers is due to vanishing gradients and not due to oversmoothing. This claim has to be justified empirically. 
    * Figure 4 in the appendix showing gradient norms is not meaningful. The y-axis should be displayed on a logarithmic scale. Vanishing gradients occur for gradients approaching zero (exponentially fast). It is not clear if this is the case here, since it is plotted in linear scale.
    
* The structure and readability of this paper should be improved. For instance, it would be helpful for readers who are not very familiar with the graph-learning field, to introduce the concept of GNNs and GCNs in the introduction. It would be advisable to move the technical aspects from section 1 to section 2. In fact, none of the definitions presented at the beginning of the introduction are needed anywhere else in the remainder of the introduction section. Thus, this could be moved to section 2.

Limitations:
* The method necessitates a pre-processing step to compute the eigenbasis in equation (5). This may not scale efficiently for very large graphs.
* The method does not show significant improvements. Notably, there are more effective models and methods available for mitigating oversmoothing, which have not been cited or empirically compared. Examples include https://arxiv.org/abs/2202.04579, https://arxiv.org/abs/2210.00513, https://arxiv.org/abs/2206.05437, https://arxiv.org/abs/2110.14446, https://arxiv.org/abs/2206.10991,
https://arxiv.org/abs/2006.11468.
* While the theoretical justification of the approach is intriguing, the insights are somewhat limited. For example, there are no provided estimates for choosing the parameter alpha, which instead has to be learned through gradient descent.
* The empirical results are not convincing.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies how GCN smoothes node features in terms of unnormalized and normalized smoothness. The results show that adjusting projection can alter the normalized smoothness to any desired level. Based on this, the paper proposes a new method SCT to let GCN learn node features with a desired smoothness to enhance node classification and verifies it effectiveness in practice.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	Understanding the effect of nonlinearities in GNNs is an important yet underexplored problem due to the complexity of nonlinearities. The paper offers a new perspective on it.

2.	Oversmoothing is a known issue, while it is also known that some amount of smoothness is desired for graph learning. How to find the ideal amount of smoothness among node features is an important but nontrivial problem.

3.	The proposed method SCT is principled and seems effective.

Weaknesses:
1.	While I can see that normalized smoothness has its own merit, this notion could be better motivated and connected to the literature. 

   a.	Given analysis in [27, 4] is asymptotic, I would not say that “over-smoothing – characterized by the distance of features to eigenspace M or the Dirichlet energy – is a misnomer”, as it is too strong of a claim to make. Those results essentially say that “very deep GCNs are bad due to oversmoothing” which has their limitations but can be well justified by the distance of features to eigenspace M or the Dirichlet energy.

   b.	My understanding is that normalized smoothness could be more connected to the non-asymptotic notion of oversmoothing studied in [32], which is defined based on the Bayes error of classification (the distance to the decision boundary) and hence taken the magnitude of features into account.

   c.	Based on the above, the argument presented in the paper can be strengthened in the following way: the motivation of normalized smoothness should be based on a discussion on how the magnitude (and hence normalized smoothness) is more related to a non-asymptotic notion of oversmoothing, which is directly related to the classification performance of finite-depth/shallow GNNs. Based on this, the results present in this paper is more practically relevant than the previous asymptotic result.

I would suggest the authors modify the relevant text in the introduction and analysis accordingly.


2.	The analysis only applies for GCNs, while whether the analysis or the proposed method can be extended to more complexed GNNs such as GATs or graph transformers is unclear. 

3.	The analysis only applies for ReLU and LeakyReLU, which reads a bit specific. I wonder if the results can be generalized to a general family of nonlinearities.

4.  For the experiments, there is a lack of baseline comparisons except the basic backbone architecture. For example, I wonder how it would compare to APPNP, which is proposed to balance the need to explore larger neighborhood and locality and the implicit goal is also to produce node features with the ""right"" amount of smoothness.

4.	Another presentation suggestion I have for the authors is that one should minimize the use of in-text math and bold or italic fonts for highlighting (such as line 167-173).  Math is hard to read in-text and when too many texts are highlighted, the paper becomes ever harder to read because everything seems to be emphasized and it kind of messes up with its original purpose.

Limitations:
n/a

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper first shows that in GCN, the output of ReLU or LeakyReLU lies on a sphere whose input is characterized by components parallel and perpendicular to $\mathcal{M}$, the space spanned by eigenvectors for the maximum eigenvalue of a graph. As a corollary, this paper shows that these activation functions do not increase the component of the feature vector perpendicular to $\mathcal{M}$. Furthermore, this paper defines the normalized smoothness and evaluates how its range varies with the activation functions. Based on this discussion, this paper proposes an SCT that learns the parallel components of the feature vectors. The proposed method is applied to GCN, GCNII, and EGNN, and verifies its effectiveness by applying it to node prediction tasks with various heterophily.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The theorems presented in the theoretical analysis (Propositions 3.2 and 3.3) enable a unified treatment of ReLU and Leaky ReLU.
- Numerical experiments show that the proposed method is effective for various data sets and models, showing the versatility of the proposed method.

Weaknesses:
- I need help understanding the explanation in Section 3.3. More specifically, it is difficult to understand that the *independence* of the inequality means that the upper bound of the inequality does not depend on the value of $\boldsymbol{Z}_{\mathcal{M}}$. I suggest writing it explicitly. 
- P7, L.265: It seems strange that although SCT changes its architecture depending on whether the underlying GNN is GCT or GCNII, it has the same name. I suggest naming SCT for GCT and SCT for GCNII differently.

Limitations:
The authors discuss in the conclusion section that the proposed method's limitation is that it assumes the model's oversmoothing. However, I do not think this is a limitation because if the model does not cause oversmoothing, there is no need to use the proposed method. Rather, I recommend evaluating whether SCT has bad effects when the model does not cause oversmoothing.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper deals with (Over-)smoothing is Graph Neural Networks. While it was previously known that GCN-type GNNs oversmooth, this paper reexamines the case for GCN-type architectures with Relu-type activation functions in terms of *normalized* smoothness. The authors show that the convergence behaviour of GCNs can be split into two parts, a ""smooth"" part and a ""non-smooth"" part and that by manipulating the smooth part, one can influence the normalized smoothness of the signal. From these theoretical insights, the authors propose a new system that uses a learnable parameter that modulates the smooth part, making the model able to learn the most beneficial normalized smoothness for the problem at hand.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper furthers the understanding of oversmoothing in GNNs
- The proposed approach is well-founded in theory

Weaknesses:
- The experiments are unconvincing. 
    - There is a slight improvement to be found in models with SCT, but this is not too surprising, as these models also have more parameters, and mostly improvements are quite slim.
    - You choose GCN, GCNII and EGNN, two of which have built-in skip connections, that are known to help with oversmoothing. This also coincides with the models that cope quite well with a larger number of layers. The vanilla GCN takes a huge hit in all benchmarks apart from ogbn-arxiv. So learning the normalized smoothness of features does not actually seem to help in the case of GCN. 
    - The other two models don't suffer from oversmoothing to begin with

Limitations:
The limitations are not well-discussed. 
The only limitation the authors claim for this work is, that: ""without this condition [that oversmoothing happens], SCT cannot ensure performance guarantees."" There are no performance guarantees given for SCT. This is the only limitation discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies how ReLU and Leaky ReLU affect the smoothness of node features in graph convolution layers. The authors demonstrate that adjusting the input projection onto eigenspace $\mathcal{M}$ of the node feature matrix can achieve any desired normalized smoothness. Additionally, they propose a Smoothness Control Term (SCT) to enhance node classification in Graph Convolutional Networks, validated on both homophilic and heterophilic graphs.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. From a geometric perspective, the authors prove that how ReLU and Leaky ReLU affect the smoothness of node features in graph convolution layers.
2. The experimental results validate the theory proposed by the authors.

Weaknesses:
Equation 5 implies that using SCT requires performing eigendecomposition. This paper avoids the high time complexity associated with eigendecomposition, especially when the number of nodes in a graph is very large.

Limitations:
N/A

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Y9aRCjuhnV;"REVIEW 
Summary:
This paper proposes a variant of Multi-Armed Bandits (MAB) named EdNetRMABs that model the interdependencies between learning contents to meet the real-world education scenarios. Subsequently, the authors introduce EduQate, an interdependency-aware Q-learning algorithm to optimize content recommendation given the EdNetRMABs. The paper demonstrates the theoretical and empirical effectiveness of EduQate with the experimental results of synthetic and real-world data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work uses RMABs in education to model the learning process with interdependent educational content as the different knowledge concepts have relevance.
2. The paper introduces EduQate employs Q-learning to make decisions on arm selection that do not require knowledge of the transition matrix to compute an optimal policy and provides related theoretical analysis.

Weaknesses:
1. As this work considers interdependency awareness in content recommendation in educational scenarios, the generated group in the experiment is not clear. The authors need to clarify how to capture relevance between the selected exercises of each topic in different datasets. Furthermore, does the different number of topics influence the experimental results?
2. The definition of state space representing a student's knowledge states a binary value. However, in real-world scenarios, the knowledge states are multi-level. A fine-grained knowledge state estimation result is essential for the content recommendation in adaptive learning.
3. The paper lacks the details of EdNetRMABs. Some illustrations would help to understand.

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces **EduQate**, a system that generates adaptive educational curricula using restless multi-armed bandits (RMABs). This method aims to efficiently achieve mastery across multiple interdependent educational contents. Unlike traditional methods that assume learning contents are independent, EduQate acknowledges and leverages the interdependencies between different educational concepts.

The paper addresses key challenges in modeling and optimizing the learning process in educational environments where the learning of different topics is interconnected. By considering these interdependencies, the proposed EduQate system ensures more accurate and effective personalized learning.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
**Originality** The paper introduces the concept of EdNetRMABs, a model that considers the interdependencies among learning contents in educational settings. This approach contrasts with existing methods that typically assume independence among different educational topics. By employing the Whittle index and Q-learning to develop the EduQate algorithm, the paper further demonstrates originality in adapting and integrating these techniques to address the specific challenges of personalized education.

**Quality** The authors provide clear definitions and a well-structured EdNetRMABs model. The optimality guarantees of the EduQate algorithm are well-explained and supported through rigorous proofs. Additionally, the empirical results are compelling, demonstrating the effectiveness of the proposed method over benchmark strategies using both synthetic and real data.

**Clarity** The paper is clearly written and well-organized, making complex concepts easier to understand. The authors systematically introduce the problem, the proposed solution, and both theoretical and empirical validations. The use of figures helps to elucidate the model and results. However, the clarity could be further enhanced by including more detailed explanations of the algorithm's implementation and practical applications.

**Significance** The significance of the paper lies in its potential impact on the field of educational technology. By addressing the interdependencies among learning contents, EduQate offers a more accurate and efficient approach to personalized education.

Weaknesses:
1. **Clarity of Explanations**: Due to the limited length, many explanations in the main text are not very clear. For instance, the survival and design of the student model and the content and information of the datasets are not thoroughly explained. For example, there is a dataset mentioned later that lacks similarity content, which should have been noted when introducing the dataset.

2. **Inconsistency in Notation**: In Section 4.1, ""Analysis of EduQate,"" the variable k in the second line has a different font from the k mentioned later. Additionally, the content related to k mentioned earlier is too far from this section, making it difficult for readers to understand and potentially leading to misunderstandings.

3. **Simplified Modeling**: The model uses only one pseudo-state, where if any content within a group is learned, all unlearned content in the group is marked as pseudo-state. This approach seems overly simplified. For example, in Case 1, if only one piece of content in a group is learned, all other content is marked as pseudo-state. In Case 2, if many pieces of content in a group are learned, the remaining content is also marked as pseudo-state. Although both cases result in a pseudo-state, their actual significance differs, and it seems the authors did not consider this distinction.

4. **Bidirectional Relationships**: The relationships between knowledge points in the paper are bidirectional, with only grouping relationships. However, in practice, many relationships between knowledge points within the same group are unidirectional. Recommending subsequent courses without recommending prerequisite courses first can lead to students needing to self-study prerequisite courses when learning subsequent courses, increasing their learning burden. The modeling in the paper does not seem to account for such unidirectional structures, instead using groups to link knowledge points, which could lead to this reverse learning issue.

5. **User Experience**: The discussion on user experience for educators and students is insufficient. Including qualitative and quantitative feedback from pilot implementations would be useful. The authors should conduct user research to gather insights on the intuitiveness and user-friendliness of the system for the target users. This could include surveys, interviews, and usability testing, focusing on ease of use, effectiveness, and areas needing improvement.

6. **Scalability of the EduQate Algorithm**: Although the paper mentions the efficiency of the EduQate algorithm, it lacks an in-depth exploration of its scalability in large-scale educational environments. Detailed performance benchmarking of the algorithm, including analysis of computation time and resource utilization, would be beneficial. The authors should provide a comprehensive performance analysis of the algorithm as data scale and complexity increase, including potential optimization techniques for handling large datasets.

Limitations:
The authors have explicitly acknowledged the limitations of their work, providing explanations for these constraints and discussing their impact on the research. The paper does not pose any negative social impact, as it aims to improve educational practices through a more personalized approach to learning.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes EduQate, an innovative framework that leverages EdNetRMABs to achieve interdependence among knowledge points. By using Q-learning, EduQate implements optimal strategies for personalized learning, offering optimality guarantees without needing explicit knowledge of transition functions governing student learning states. This approach dynamically adapts educational content to individual progress, optimizing learning experiences and outcomes. The effectiveness of EduQate is validated using three real-world datasets, demonstrating its capability to identify and implement the most effective teaching strategies.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Overall, this article provides valuable insights and has a good overall quality. It has a certain impact on the AI education domain. The proposed approach, EduQate, adapts the Q-learning method of the multi-armed bandit model and introduces active and passive arms to establish correlations between knowledge points, which demonstrates a level of innovation. The article ensures the validity of the approach at the theoretical level, making it more persuasive. In terms of narrative, the article maintains logical coherence and is comprehensible. The discussed topics are indeed important in the field of education.

Weaknesses:
The formatting of the article is messy, and the images are not very clear. The layout on the sixth page is problematic, with formulas and text mixed together, making it confusing. The images on the seventh page and some of the appendix images are blurred. I suggest using a different format to redraw them.

There are too few baselines in the experimental section, and there are very few validation experiments. Merely presenting outstanding performance in one experimental metric is insufficient to demonstrate the superiority of your method. It would be beneficial to include some reinforcement learning baselines for comparison. Other reinforcement learning methods, such as the Bayesian network approach mentioned in the citations, can also accomplish the task. Therefore, the necessity of your method is unclear in the paper.

Your method does not consider the specific circumstances of practical problems. It raises fairness concerns. Ensuring that all knowledge points are fairly selected in a specific educational context is a crucial issue. Although the article presents a good framework, I believe it is not feasible to use it.

Limitations:
As mentioned above, there are many factors to consider in the specific field of education. These factors include fairness, mapping each question's knowledge points to the ARMs mentioned above, and the presence of cold-start problems, among others. Time is also a crucial factor to consider during real-time usage, although the paper does not mention specific algorithmic time information in practice.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a solution to generate personalized learning curricula in educational settings, focusing on the challenge of accounting for interdependencies between learning topics. It argues that existing approaches, often based on the Restless Multi-Armed Bandit (RMAB) framework, fall short by assuming independence of learning content which is unrealistic in educational settings. To address this the paper introduces a new model Restless Multi-armed Bandits for Education (EdNetRMABS) enabling the capture of relationships between independent learning items. Building upon this model, the authors propose a new algorithm called EduQate, which leverages Q-learning and the Whittle index to compute an interdependency-aware teacher policy for recommending educational content. Notably, EduQate doesn't require prior knowledge of the transition matrix, unlike traditional Whittle index methods.

The authors provide a theoretical analysis demonstrating the optimality of EduQate for the case of recommending a single item at each time step (k=1). While finding the optimal solution for recommending multiple items (k>1) is proven to be NP-hard, a heuristic greedy algorithm is proposed to find solutions.

Through experiments on synthetic and real-world datasets (Junyi and OLI), the paper demonstrates the superiority of EduQate over baseline policies, including Threshold Whittle (TW), WIQL, Myopic, and Random. EduQate consistently achieves higher intervention benefits and average rewards across all datasets. Further analysis reveals the effectiveness of the replay buffer in EduQate, mitigating the ""cold-start problem"" common in reinforcement learning applications.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well organized and very well written with comprehensive explanations of the model, algorithm and the experimental setup.
2. Addressing the crucial issue of interdependency in learning content is a significant contribution. EdNetRMABs offer a realistic model for educational settings, moving beyond the simplifying assumption of independence prevalent in traditional RMAB approaches.
3. The paper presents a rigorous theoretical analysis of EduQate, proving its optimality for the k=1 case and providing complexity bounds for the k>1 case.
4. The authors address the cold-start problem by incorporating experience replay and perform ablations to show its effectiveness. This is a practical enhancement relevant for real-world applications.

Overall, this paper presents a strong contribution to the field of adaptive learning by introducing a novel and effective approach for generating personalized curricula that account for interdependencies in learning content.  The theoretical analysis, empirical results, and focus on practical considerations make this work both insightful and impactful.

Weaknesses:
The assumption of fully observable knowledge states is a significant limitation. Future work should explore extending the model and algorithm to handle partial observability, a more realistic scenario in education.
While the complexity analysis is provided, further investigation on the scalability of EduQate to larger datasets and more complex networks would strengthen its practical applicability.
The current work primarily focuses on maximizing long-term rewards. Further analysis on balancing exploration and exploitation in EdNetRMABs could offer valuable insights for curriculum design.

Limitations:
1. The experiments are based on simulated students and existing datasets. Real-world classrooms involve numerous factors not accounted for in the model, such as student motivation, engagement or diverse learning styles. This may limit the practical utility of this method.
2. The paper does not address the interpretability of EduQate's recommendations.
3. The assumption that the student states are fully observable is a major limitation of this work. This can result in overconfident recommendations.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a way to extend RMAB with Q-learning to accommodate the fact that some items/arms belong to a group. RMAB can be considered as a weakened version of contextual bandit CB but also a strong version of CB since it considers state transitions (explicitly defined on arms).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This is a very solid paper with quite a few reasons to accept it:
1. The contribution of education simulators. Can the authors comment on whether they will release their simulators publicly for other education or bandit communities to use in the future? Currently, there is a lack of high-quality education simulators that are based on real-world data. This could be a great contribution. I hope the authors make an effort to make such resources public.
2. The technical contribution of introducing a pseudo-action that extends RMAB and WIQL.
3. Clean writing and presentation.

MAB has been used in many different real-world settings and is the main algorithm behind many learning platforms. Any innovation in this space will have a huge impact on students around the world. Unfortunately, this area is very niche and requires high technical sophistication. Unless someone can convince me that this paper's algorithm has already been published elsewhere or is fully derivative of some other work, I think it's a great paper to present at NeurIPS.

I might also be a bit concerned if the authors decide not to share their code/algorithm/simulators to encourage more future research in this direction.

Weaknesses:
1. Can add some more ablation studies.
2. Some clarifications (see question section).

Limitations:
The authors discussed limitations in a section.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
7s53dAJlwz;"REVIEW 
Summary:
The paper addresses the problems of multi-view consistency and geometric detail in image-to-3D generation. The proposed method, LAM3D, adopts a two-stage approach for training. In the first stage, the authors train a plane encoder and decoder to compress point clouds into a latent tri-plane representation. In the second stage, to align image and point cloud features within the same feature space, they train the diffusion model. During inference, the image is converted into a latent tri-plane by passing through the trained U-Nets from the second stage, and the features are decoded by the plane decoder trained in the first stage. A final mesh is extracted using the marching cubes algorithm from the reconstructed tri-plane. Experiments demonstrate the effectiveness of the proposed method in 3D generation from a single image, both quantitatively and qualitatively. Additionally, the authors provide an extensive ablation study on different design components.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
(1) The task of single-view image to 3D generation is extremely challenging and well-motivated.

(2) The proposed method addresses the limitations of existing baselines, which only use images for training and fail to effectively reflect multi-view consistency and geometric detail. To solve the problems, the authors use point clouds as a 3D prior and converting them into tri-planes to facilitate training. Additionally, the method is carefully designed to align the images and tri-planes within the same feature space.

(3) The results show that the proposed method achieves SOTA performance on single image-to-3D generation with a short inference time compared to existing baselines.

(4) A numerous ablation studies on design choice is conducted to achieve optimal performance.

Weaknesses:
(1) As mentioned in the limitation, LAM3D cannot generate a textured mesh, while baselines (One-2-3-45, LRM, and CRM) can. In generating 3D content, the accurate geometry of the mesh is important, but the corresponding texture is also a crucial component. It is unfortunate that this aspect cannot be generated. Although it would take more time, generating multi-view images based on the input image using a multi-view diffusion model like Zero-1-to-3 and then unprojecting them to the generated mesh could be a solution.

(2) Recently, many papers related to single image-to-3D generation have been published, and there are many papers like [1, 2] that were published 2 months before the NeurIPS 2024 submission deadline. Among these, I guess Michelangelo [1] has the best quality in geometry generation, so comparing this paper would make the argument more compelling.

[1] Zhao et al., Michelangelo: Conditional 3D Shape Generation based on Shape-Image-Text Aligned Latent Representation, NeurIPS 2023

[2] Vikram Voleti et al., SV3D: Novel Multi-view Synthesis and 3D Generation from a Single Image using Latent Video Diffusion, arXiv 2024

Limitations:
The authors mention that the limitation of the current pipeline cannot generate texture, as discussed in Section 5.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces a new 3D generation framework, LAM3D, which uses point cloud data and image-point-cloud feature alignment method to improve the geometry of the generation results. Authors use triplanes as representation, combining image feature as well as point cloud feature. For point cloud compression, they use point cloud transformer to encode the point cloud to triplane. For better generation, they encode the point cloud triplane to latent triplane, and trained three diffusion model to generate latent triplane. The whole method can generate untextured mesh within 6 seconds, showing better geometry results comparing baselines.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well-written with good soundness. Logic is clear and motivation is reasonable. 
2. Qualitative and quantitative experiment are carried out in detail and are executed with precision. Ablation study was done with a high degree of accuracy, confirming the significance of parallel diffusion and latent triplane.
3. To handle the difficulty of image triplane and point cloud triplane alignment, LAM3D use a plain encoder to encode the initial triplane to latent triplane, this may give the community another insight in 3D representation and 2D image alignment.

Weaknesses:
1. There are already some work on Shape-Image-Aligned generation work such as 3DGen[1] and Michelangelo[2], authors should compare with these work and point out what are the advantages of latent triplane and three parallel diffusion model.
2. Parallel diffusion design may lose some 3D information. 

[1] Gupta, Anchit, et al. ""3dgen: Triplane latent diffusion for textured mesh generation."" arXiv preprint arXiv:2303.05371 (2023).

[2] Zhao, Zibo, et al. ""Michelangelo: Conditional 3d shape generation based on shape-image-text aligned latent representation."" Advances in Neural Information Processing Systems 36 (2024).

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces the Large Image and Point Cloud Alignment Model (LAM3D), a novel framework that enhances 3D mesh reconstruction from single images by utilizing point cloud data. LAM3D integrates a point-cloud-based network for generating precise latent tri-planes, followed by an Image-Point-Cloud Feature Alignment technique that enriches image features with robust 3D information. This approach allows for the production of high-fidelity 3D meshes while mitigating geometric distortions commonly associated with single-image inputs. The method demonstrates its effectiveness across various datasets, achieving state-of-the-art performance in high-fidelity 3D mesh reconstruction.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper presents a method combining point cloud data with image features to reconstruct 3D meshes, which is a promissing solution to the problem of geometric inaccuracies in single-image 3D reconstructions.
2. The experimental results are robust, showing clear improvements over existing methods.

Weaknesses:
1. The LAM3D framework's omission of texture mapping is a significant limitation, particularly when compared against methods like One-2-3-45 and CRM that integrate both geometric and texture reconstructions. This absence not only restricts the visual and practical applicability of the generated meshes but also raises concerns about the fairness of comparisons made in the paper. The inclusion of texture details is pivotal for realistic 3D reconstructions, and its absence in LAM3D suggests a crucial area for potential improvement. Furthermore, the comparisons with models that handle both geometry and texture may not provide a balanced view. It's more appropriate to compare LAM3D against models such as ""Slice3D: Multi-Slice, Occlusion-Revealing, Single View 3D Reconstruction"" (CVPR, 2024) and ""D^2IM-Net: Learning Detail Disentangled Implicit Fields from Single Images"" (CVPR, 2021), which are more aligned in terms of focusing primarily on geometric details.
2. LAM3D requires point cloud data during training, its applicability is notably restricted to specific categories or types of objects. This limitation indicates a constrained generalization ability, which may impede its deployment in diverse real-world scenarios that demand robust 3D reconstruction capabilities across varied object environments.
3.  The use of independent diffusion processes for each tri-plane is a novel approach, but the paper does not provide a comparative analysis of this method against more integrated approaches.

Limitations:
While LAM3D effectively enhances mesh reconstruction quality by leveraging 3D point cloud data, its current implementation omits texture mapping. This omission not only limits the visual realism and practical utility of the generated 3D meshes but also makes direct comparisons with methods like One-2-3-45 and CRM, which include textural details, somewhat unfair.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a two-stage 3D reconstruction method that first uses a transformer-based 3D point cloud feature extractor to initialize hierarchical latent triplanes (XY, XZ, YZ) and reconstructs the 3D mech. Next, it presents an image-point cloud feature alignment approach that leverages initial latent triplanes to align them with the image-based features using independent plane diffusion models to produce better detail and reduced geometric distortion reconstructions.

The approach achieves state-of-the-art high-fidelity 3D mesh reconstructions from a single image in just 6 seconds, and experiments on various datasets demonstrate its effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
•	The concept of utilizing the proposed image-point cloud feature alignment approach using an independent plane diffusion model seems distinctive. 
•	The proposed method of 3D reconstructions shows reduced geometric distortion reconstructions compared to the currently mentioned methods.
•	Achieves SOTA results on the mentioned 3D object-based dataset.

Weaknesses:
•	Comparison of the model capacity with the current SOTA methods might be added as the proposed methods seems heavy that uses three diffusion, three UNet and two transformer-based architecture.
•	Also, the paper might want to compare the inference time with the existing methods as it specifies that it only requires 6 secs for inference.
•	Details of the module and inference stage missing (see section correctness).
•	The method is evaluated using only a single dataset. 
•	Lack of reproducibility

The paper is not well-written. The figure is supposed to give an overview of the pipeline, however, if just reading the figure it is not clear what the input is. The paper refers to input point clouds multiple times, which makes it furthermore confusing.  Or maybe the point clouds are also part of the input as it is referred to as prior point clouds in the paper. If so the performance gain might also be due to the additional multi-modality inputs which is not a fair comparison with the other baseline methods.

Typo:

•	Line 622: We follow LRM [14] ‘and uses’ a 12-layer transformer to directly align image feature to point cloud seems mistaken.

Limitations:
The authors adequately addressed the limitations.

Overall, the idea of aligning the 3D point features induced triplane representation and image-based features utilizing independent plane diffusion models seems interesting and can produce less geometric distortion reconstruction. However, the proposed method is evaluated only on a single dataset and misses the related work, mentioned module details, and model capacity comparison that must be addressed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
biAqUbAuG7;"REVIEW 
Summary:
Due to problems with momentum and a non-stationary target, the authors propose resetting the value of ‘t’ in Adam, which is used to determine bias correction on the momentum terms, when updating the target. This approach is validated on PPO and DQN in Atari and Craftax.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Performance benefits over two key algorithms (PPO and DQN) when used with Adam.
- Simple to implement idea.

Weaknesses:
- In terms of new insight, this paper offers very little that hasn’t been said in prior work (Bengio et al., Asadi et al.). Similarly, analysis in the paper is based around the update size. However, simply resetting would also bound the update size, so I feel like there is some lacking analysis or evidence to explain why maintaining momentum estimates is a reasonable approach. 
- Since the idea is not analyzed in depth, I would say there is limited empirical evidence (in terms of evaluations over environments and algorithms), especially compared to prior work.

Limitations:
Satisfactory.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
One of the main challenges of reinforcement learning is its inherent nonstationary nature. Such non-stationarity can cause learning difficulties. The tools currently available for deep reinforcement learning are largely borrowed from deep learning, such as the Adam optimizer, which this paper focuses on. The authors show that Adam, under nonstationarity, can have large updates leading to learning difficulties. Thus, they propose a simple modification to Adam in which they reset the time parameter at every epoch in PPO or every target update in DQN. This slight modification seems to significantly improve performance, agreeing with the observations by several works that proposed similar modifications to Adam [1,2,3] and showed gains in performance.
\
\
[1] Emmanuel Bengio, Joelle Pineau, and Doina Precup. Correcting momentum in temporal difference learning. arXiv preprint arXiv:2106.03955, 2021.
\
[2] Kavosh Asadi, Rasool Fakoor, and Shoham Sabach. Resetting the optimizer in deep rl: An empirical study. arXiv preprint arXiv:2306.17833, 2023.
\
[3] Shibhansh Dohare, Qingfeng Lan, and A Rupam Mahmood. Overcoming policy collapse in deep reinforcement learning. In Sixteenth European Workshop on Reinforcement Learning, 2023.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors present a novel and simple approach to solving a big problem. This is especially promising since it will allow quick wide adoption in the future. The authors introduced the problem carefully and talked about it in such a clear way that guides the reader step by step, and they also provide some theoretical justification for why their method would work from first principles. The evaluation benchmarks seem extensive, suggesting that the methods can be applicable to a wide range of problems.

Weaknesses:
- The main weakness of the paper is its limited comparison to other methods, particularly the methods that are closely related to Adam-Rel (e.g. [1,3]). Additionally, the only competitor is Adam-MR [2] which almost always gives worse performance, contradicting previous works. There is no similar theoretical analysis on Adam-MR explaining the poor performance or some empirical evidence convincing the reader why it fails.
- Writing can be improved, especially when explaining parts of Adam. This can be done using more proper mathematical terms such as “estimators” or “correcting the biasedness of an estimator,” etc.
- The title is written in such a way that suggests your method is applicable to a wide range of RL methods, but it seems that this is not the case, and you only focus on PPO and DQN. I suggest a change that reflects your contributions without overstating them. 
- No pseudocode is given for Adam-Rel with DQN. It’s inferred from the context that a time parameter reset is done before each target function update, but the pseudocode needs to be there to confirm this for the reader.
- Dohare et al. (2023) showed that policy collapse in PPO with MuJoCo is more pronounced than DQN in Atari. It might be because the observations are bounded in Atari but not in MuJoCo, which increases the non-stationarity. The authors didn’t consider any environments other than pixel-based ones with bounded observation ranges. I think it’s necessary also to show some results on MuJoCo environemnts (or similar environments) to confirm that the results are qualitatively similar in those settings.
\
\
\
**Minor issues:**
- “It uses a learned critic trained by a TD loss to estimate the value function, and a clipped actor update of the form.” -> This is not accurate. PPO doesn’t use a learned critic. The critic is learned as well.
- “over which the above update is calculated” -> You wrote an objective in Eq. 1, not an update
- “Adam, the most popular optimizer that  uses momentum” -> It doesn’t use the momentum you defined in the background. You might want to clear this up for the reader.
\
\
[1] Emmanuel Bengio, Joelle Pineau, and Doina Precup. Correcting momentum in temporal difference learning. arXiv preprint arXiv:2106.03955, 2021.
\
[2] Kavosh Asadi, Rasool Fakoor, and Shoham Sabach. Resetting the optimizer in deep rl: An empirical study. arXiv preprint arXiv:2306.17833, 2023.
\
[3] Shibhansh Dohare, Qingfeng Lan, and A Rupam Mahmood. Overcoming policy collapse in deep reinforcement learning. In Sixteenth European Workshop on Reinforcement Learning, 2023.

Limitations:
The authors adequately discussed the limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a simple approach to address the issue of large updates commonly encountered with Adam optimizers in deep learning applications. The authors focus on a specific scenario prevalent in deep reinforcement learning: the updating of target networks. Instead of the conventional approach of resetting both the timestep and the momentum variables in the Adam optimizer, the authors propose only resetting the timestep. This modification leads to enhanced learning performance, as demonstrated through evaluations conducted in both the Atari and Craftax environments, utilizing a range of on-policy and off-policy algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is generally clear, particularly benefiting from a detailed explanation of the Adam optimizer’s mechanics, which enhances accessibility for those who may be unfamiliar with the specifics. The simplicity of the proposed modification—only resetting the time step—is a significant advantage, allowing easy implementation and thus potentially reducing errors in implementation. The experiments are thoughtfully designed, aligning closely with the research questions and conducted in the Atari and Craftax environments, which are known for their complexity. Additionally, the selection of baselines, including the standard and a modified version of Adam, demonstrates the proposed approach’s effectiveness against established methods.

Weaknesses:
The research question addressed by the authors, focusing solely on large gradient updates due to changes in the target network, seems somewhat narrow. This focus may overlook the broader applicability and efficacy of the proposed solution across various reinforcement learning scenarios. Notably, many algorithms have achieved better stability using softer target updates, like Polyak averaging, which the authors acknowledge [1,2,3]. It would be beneficial to see how the proposed optimizer modification compares in environments where such techniques are traditionally employed.

Moreover, the approach of resetting parameters in an optimizer, though straightforward, could be considered a drastic and potentially inelegant solution. This method might introduce other issues, such as affecting the convergence behavior of the optimizer. While I currently lack an alternative suggestion, exploring methods that adjust the scale of parameter updates dynamically, rather than resetting them, might offer a more refined solution.

[1] Lillicrap, Timothy P., et al. ""Continuous control with deep reinforcement learning."" arXiv preprint arXiv:1509.02971(2015).

[2] Kaplanis, Christos, Murray Shanahan, and Claudia Clopath. ""Continual reinforcement learning with complex synapses."" International Conference on Machine Learning. PMLR, 2018.

[3] Schwarzer, Max, et al. ""Bigger, better, faster: Human-level atari with human-level efficiency."" International Conference on Machine Learning. PMLR, 2023.

Limitations:
None. The authors have presented the concerned limitations of their work in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the effect of non-stationarity on the Adam optimizer. It shows that the standard use of the Adam optimizer can lead to large updates, which can cause sub-optimal performance. To address this issue, Adam-Rel is introduced, which resets Adam's timestep parameter to zero when the target network changes. Finally, experiments show that Adam-Rel provides performance improvements over Adam.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper is generally well-written. The paper studies an important problem of large updates caused by Adam in non-stationary settings like reinforcement learning. Explicitly studying basic components of modern deep-RL, like Adam, in non-stationary settings is an important direction of research. The proposed solution, Adam-Rel, is simple and easy to implement.

Weaknesses:
The same problem of large updates by Adam in non-stationary problems has been studied before (Lyle et al., 2023; Dohare et al., 2023). They both use the solution of setting $\beta_1 = \beta_2$. The authors seem aware of this proposed solution because the work by Dohare et al. (2023) is discussed in the paper. However, Adam-rel is not compared with Adam with $\beta_1 = \beta_2$. A comparison with Adam with $\beta_1 = \beta_2$ is needed to accept this paper. Even if Adam with $\beta_1 = \beta_2$ performs better, this paper can be accepted, as it studies the problem in much more detail than any prior work.

I'm willing to change my score if this comparison is added to the paper.


Lyle et al. Understanding plasticity in neural networks. ICML, 2023.
Dohare et la. (2023). Overcoming policy collapse in Deep RL. EWRL 2023.

Limitations:
The authors adequately discuss the limitations.

-----------------
UPDATE: I increased my score as the authors provide comparison with Dohare et al. (2023).

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
iRHxp1ibFj;"REVIEW 
Summary:
The paper introduces a novel image-level supervision method for semantic segmentation, utilizing approximate targets for the relative sizes of segments in training images. These targets, represented as categorical distributions for the expected average prediction over pixels, are integrated using a zero-avoiding variant of KL divergence as the training loss. This approach achieves quality comparable to full pixel-level supervision but is significantly less costly, requiring only rough estimates of the areas occupied by each class.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Using object size as a form of supervision is both innovative and interesting.
2. The proposed method is straightforward and easy to understand.

Weaknesses:
1. The title of the paper is misleading. It claims that approximate size targets are sufficient, but the work also uses image labels for supervision.
2. The most important comparison in Figure 1 is between 'Tag' and 'Size target,' as this validates the significance of using target size supervision. To clearly demonstrate that 'Size target' is superior to 'Tag' under identical conditions, it would be better to use the same architecture for both comparisons.
3. Labeling the size of objects can be challenging for humans and may introduce significant noise, especially for tiny objects. Although the authors demonstrate impressive accuracy with up to 8% size target errors, this remains a stringent annotation standard, particularly for small objects. For instance, as seen in Table 1, the mean relative error (mRE) often exceeds 10% during human annotation in the Pascal VOC dataset. Moreover, estimating target sizes in Pascal VOC is relatively easy since objects are typically large and centered. However, labeling images in more complex datasets, such as COCO, might result in a higher mRE.
4. In Table 1, the authors should also report the speed of tag annotation to highlight the cost of estimating target sizes.
5. The proposed method is straightforward and impressive for its end-to-end training, especially considering that existing weakly supervised semantic segmentation (WSSS) methods typically use CAM and two-step training. However, as shown in Table 2, while the proposed method achieves comparable accuracy to state-of-the-art WSSS methods, it relies on additional supervision and a high annotation standard (8% mRE). Moreover, Table 2 indicates that the accuracy with only tag supervision is close to that of fully supervised methods, suggesting that tag supervision alone may be sufficient for segmentation.

Limitations:
The authors do not discuss the limitations and broader impact of their method, which necessitates a dedicated discussion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new weakly supervised semantic segmentation task. This task uses pixel-level categorical distribution as the label in the training stage. KL divergence is used as the training loss. Experiments on three public segmentation datasets show the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The proposed task is interesting. It provides the community another choice for segmentation with less annotation effort.

2.The proposed KL divergence loss is effective, demonstrated by experiments on three public datasets. It achieves performance comparable to methods using more expensive labels, like the box supervised one.

3.The proposed method is robust to size target error, which makes it more practical.

4.The writing is fluent and easy to follow.

Weaknesses:
1.Labeling effort on complex images. Images from PASCAL VOC (like Figure 1) are easy to annotate. It contains few classes and the background is generally clean. The density of target objects is low, and hence it’s also suitable for the proposed grid-based size target annotation way.

However, in practice, scenes are much more complex, with more classes, more crowded objects, and complex backgrounds. The authors are recommended to show the annotation effort on those images, like images from Cityscapes and ADE20K. I think when the scenes become more complex, the labeling effort will increase significantly. The labeling effort of size target will be much more than the tag way, since tagging will be less influenced in such cases.

2.Model performance on complex images. Similarly, it’s recommended to evaluate the model’s performance with the proposed loss on these complex datasets. This will give a more comprehensive understanding of the proposed method.

Limitations:
No negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper titled ""Approximate Size Targets Are Sufficient for Accurate Semantic Segmentation"" proposes a novel method of semantic segmentation that leverages approximate size targets instead of full pixel-level supervision. The method involves using categorical distributions to represent the expected average prediction over image pixels, utilizing the zero-avoiding variant of KL divergence as a training loss. The approach aims to reduce annotation costs while maintaining segmentation accuracy comparable to full supervision.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Originality: The use of approximate size targets as a form of weak supervision for semantic segmentation is novel and creative.
2. Quality: The experimental results are comprehensive and demonstrate the effectiveness of the proposed method across different datasets and segmentation architectures.
3. Significance: The approach has significant implications for reducing annotation costs in semantic segmentation tasks, making it highly relevant to practical applications.

Weaknesses:
1. Simplicity of Method：While the proposed method is innovative, it seems relatively simple. There might be opportunities to enhance its contributions with further development or by integrating additional techniques.
2. Limited Scope of Evaluation: While the paper evaluates the method on several datasets, it would benefit from a broader range of scenarios, including more diverse and complex images.

Limitations:
The authors have addressed the limitations related to annotation errors and have demonstrated the robustness of their method to these errors. However, it would be beneficial to discuss potential limitations in more detail, such as the scalability of the method to larger and more diverse datasets, and any assumptions made about the nature of the size target annotations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel image-level supervision method for semantic segmentation using approximate segment size targets. It utilizes categorical distributions for expected average predictions, reducing annotation cost and complexity. The authors propose a zero-avoiding KL divergence as a training loss, compatible with any segmentation architecture, and demonstrate significant robustness to size target errors, improving generalization. The method achieves state-of-the-art performance on multiple datasets with standard segmentation models like ResNet101. Additionally, it requires minimal extra information and no architectural changes, making it a practical and effective solution for weakly-supervised semantic segmentation in real-world applications.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper introduces a novel form of image-level supervision for semantic segmentation using approximate segment size targets. This approach is original in its use of categorical distributions for expected average predictions, providing a fresh perspective on weakly-supervised segmentation methods.

2. The quality of the research is high, with comprehensive experiments conducted on multiple datasets. The use of a zero-avoiding variant of KL divergence as a training loss is well-justified and demonstrates robustness to size target errors. The empirical results show that the method achieves state-of-the-art performance using standard segmentation models.

Weaknesses:
1. The paper claims robustness to size target errors but provides limited detailed analysis on this aspect. Including more experiments to quantify and analyze how different levels of size target errors impact performance would provide a clearer understanding of the method's robustness.

2. Lack of related work. The paper’s logical flow and organization need improvement. 

3. The paper lacks comprehensive comparisons with the latest models, such as ""SFC: Shared Feature Calibration in Weakly Supervised Semantic Segmentation (AAAI24)"".

Limitations:
1. Fig and Figure are inconsistent in Line 24

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
avsZ9OlR60;"REVIEW 
Summary:
The paper proposes the Equivariant Transformer Flow (ET-Flow) to generate high-quality molecule conformations. The authors use rotational alignment, stochastic sampling, and chirality correction to improve the flow matching framework for this task. Additionally, the paper modifies the TorchMD-NET equivariant transformer architecture to parameterize the target vector field. Experiments on the GEOM dataset show that  ET-Flow can perform well on the conformation prediction task. Furthermore, conformations generated by ET-Flow can also be used to predict ensemble properties with great performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Very clear writing.
2. The method is simple and easy to understand.

Weaknesses:
1. Lack of experiments. The experiments on the GEOM dataset are not enough to show the empirical performance of the framework. Some further experiments on large-scale datasets with more data and larger system size are necessary. I suggest the authors do experiments on OC20/OC22(Open Catalyst 2020/2022) datasets to test the performance of the ET-Flow framework.
2. The novelty of this work is not enough. I think the idea and method of this paper is very similar to [1], but in this work, the authors use a transformer-based model. Additionally, this paper proposes several tricks such as rotational alignment, stochastic sampling, and chirality correction. So the authors should clarify the novelty of this work.

[1]. Klein, Leon, Andreas Krämer, and Frank Noé. ""Equivariant flow matching."" Advances in Neural Information Processing Systems 36 (2024).

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes the Equivariant Transformer Flow (ET-Flow) which predicts low-energy molecular conformations given the molecular graphs. Unlike existing methods that rely on large transformer-based models for conformed fields or complex internal geometry calculations, ET-Flow leverages flow matching with equivariance and harmonic prior which directly operates on all-atom coordinates with minimal assumptions. The extensive experimental results illustrate that ET-Flow achieves state-of-the-art performance in molecular conformer generation benchmarks with fewer parameters and faster inference times, outperforming or matching previous methods while maintaining high accuracy and efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed approach represents a significant innovation by leveraging flow matching with equivariance and harmonic prior in order to simplify the conformer generation process while maintaining high accuracy. 

1. ET-Flow achieves SOTA model performance on molecular conformer generation benchmarks, outperforming or matching existing methods with much fewer parameters and faster inference times without sacrificing accuracy. The high accuracy and efficiency make it a promising candidate for practical applications. 

1. The paper provides comprehensive experimental results, comparing ET-Flow with several leading approaches across various datasets. The inclusion of various evaluation metrics strengthens the validity of the findings.

Weaknesses:
1. ET-Flow achieves SOTA or near-SOTA performance on the benchmarks. However, my concern is that TorchMD-Net is already a strong model with a well-established architecture that leverages equivariant transformers for molecular modeling. Given this, it can be challenging to attribute the performance improvements of ET-Flow to the flow matching approach. Ablation studies focusing on the flow matching component or results from simpler architectures might be useful. 

1. The need for a post hoc chirality correction step suggests that the model does not inherently handle stereochemistry (baseline models like MCF do not require such an explicit correction). Such a weakness may result in issues in practical applications. 

1. ET-Flow primarily combines TorchMD-Net with the flow matching technique. While both components are robust and effective, the novelty of the approach is limited as it largely builds on existing methodologies. This integration, although leading to performance improvements, does not significantly advance the state-of-the-art in terms of methodological innovation.

Limitations:
The authors have discussed the limitations of recall performance and additional chirality correction steps. No potential negative societal impact is involved.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper describes an equivariant flow matching model for conformer generation.  The stated contributions are accurate - the model performance is state-of-the-art and largely due to good engineering.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
The paper is well written and the evaluations follow other published work.  Informative ablation studies are performed.  The evaluation of ensemble property averages is particularly appreciated.

I appreciate the authors highlight the modifications required for stable training.

Weaknesses:
Very little evaluation of out-of-distribution performance is considered (a small evaluation is in the appendix).  The ensemble property averages evaluation seems to be done on molecules drawn from the training data (if this is not the case, it needs to be more clear).  The paper would be stronger if generalization performance was more comprehensively evaluated.

Although efficiency is one of the main claims, there's no evaluation of inference time.

I think ""DRUGS"" is used as a short-hand for GEOM-DRUGS - this is confusing, use consistent nomenclature throughout.

Limitations:
Some recent work has pointed to the fact that improved recall/precision on these GEOM benchmarks does not necessarily result in better performance on downstream tasks that use conformers.  At least some discussion of this limitation would be appreciated.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
2BOb4SvDFr;"REVIEW 
Summary:
The paper proposes to use a new distance, Min-Max-Jump, which is the minimum largest distance on any path between two points, to be used in k-means clustering to learn clusters.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
The distance can overcomes some demerits of the convex (""spherical"" in the paper) clusters.

Weaknesses:
The distance in the paper is, in fact, related to single linkage clustering that assign give a pair of points a distance at which the pair is joint to one cluster. This need to be analyzed to relate to previous work as well as to compute pairwise distances efficiently. 

Theoretical property of the distance is poor. The paper should review many other density-based distance functions to put this work into the correct context. 

There would be a lot of problems using this distance as many of pairs of nodes would share the same distance. There is no analysis on the  metric property of this distance. 

On evaluation, the methods need to compare to single-linkage clustering (SSL) at the very least as all the advantages of using this distance with k-means are available in SLL in its simplest form.

Presentation-wise, it is hardly up the standard. There are methods/algorithms/concepts that are mentioned as ""a is like b with a difference"" without a formal definition. This mixes up definitions and properties.

Limitations:
The paper uses a new distance without theoretical justifications. It learns nonconvex clusters by using a nonconvex clustering-based distance (without explicitly mentioning it), which is hardly a novelty.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new metric, min-max jump distance. Effectively, say we are given a complete graph with vertex set $\Omega$ and edge weights $d(x,y)$ denoting the distance between $x$ and $y$, where $d$ is a metric. Then $MMJ(x,y|\Omega')$ is the minimum, over all paths between $x$ and $y$, of the maximum weight edge between $x$ and $y$ on the subgraph induced on the vertices $\Omega' \subseteq \Omega$. Explained nicely in the paper, if you started at vertex $x$ and wanted to get to $y$ and $d(\cdot,\cdot)$ denoted the distance required to ``jump'' from one vertex to another, what is the minimum distance you need to be able to jump to somehow traverse from $x$ to $y$?

This is a nice intuitive metric, and has strong connections to the minimum spanning tree. In fact, I suspect there is more literature to draw from minimum spanning tree research that could yield conclusions about MMJ. The MST is also nice in clustering since oddly shaped clusters (non-convex, for instance) can have small MSTs. This is the idea of MMJ: use it as a metric for K-means so that it can identify non-spherical clusters.

The paper proves some notable theory about the properties of MMJ. Mostly, they show how: 1) When adding a new vertex $p$ to a set $\Omega$, $p$'s MMJ within the context of $\Omega+p$ can be computed knowing all pairwise MMJ's within $\Omega$ within the context of $\Omega$. This effectively adds a new point and evaluates it within the complete, updated context. 2) Given the MMJs for this additional point $p$ within the context of $\Omega +p$, expand the MMJ context of all other pairs in $\Omega$ to the context of $\Omega+p$.

This can then be used in a very dynamic programming-like manner to start with just two points, add new points $p$ and find the context of $p$ and all other points, and then update all known existing MMJs to the new context. This is their algorithm 1, requiring $O(n^3)$ time. They also use properties of the MST to bypass unnecessary calculation to yield algorithm 2, which takes only $O(n^2)$ time (to find the MST).

They then evaluate the performance of algorithms using the MMJ measure on irregular shaped clusters to verify that MMJ helps identify these. This makes sense, since they likely have small MSTs, but not small average/sum/max/etc distances within clusters. Notably, they show how MMJ improves K-means.

Soundness:
4: excellent

Presentation:
1: poor

Contribution:
3: good

Strengths:
I think MMJ is a very cool metric with nice properties and intuition, particularly that related to the MST (I wish the authors had spent more time discussing this!). Their findings are nice and relatively simple to understand (in spite of the presentation). They show that it helps K-means expand to more complicated cluster shapes, and overall it is a very nice, NeurIPS-worthy result. However, as I will explain in the weaknesses section, I do not think this paper is in an acceptable state for NeurIPS.

Weaknesses:
There are a few notable downsides. In terms of the result, I'm not entirely convinced of its novelty. How much of this is actually a re-iteration of MST-based methods already understood? Is this really better than other MST-based algorithms on irregular clusters (think single linkage)? I know that there is a lot of literature that explores irregular shaped clusters, but I am not an expert in this area and so I cannot place this work in the context of existing results. I wish the authors would explain that. Though even if these algorithms aren't entirely better than state of the art, the novelty of the nice formulation of MMJ is certainly appreciated.

However, the biggest flaw in the paper is the writing quality. There are places where the paper is nice and concise, but most of the time it just lacks exposition to understand the higher level of things or adequate details to fully understand what is happening. Formal proofs are contained in the paper, but the jumps in some of the proofs are too large. Theorems and proofs are placed back to back with no high level explanation. Algorithms are written and pseudocode with only the briefest justifications, and no thorough explanations. This is not an acceptable paper for NeurIPS, and I think these issues are too extensive to simply ask the authors to revise. Though if other commenters disagree, I am amenable to changing my opinion.

And one of the disappointing things about this is how natural this work is and how much it lends itself to nice intuitive explanations and visual depictions! For instance, you could do some very nice visualizations of Algorithm 1, where you depict a matrix and show which indices have been calculated to what context $\Omega_n$ at each time point. This clarifies the different purpose of the two loops.

I hope to see this paper submitted again later in a more cleaned up state!

Limitations:
None notable

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents the Min-Max-Jump (MMJ) distance concept and two calculation methods, focusing on path optimization in data analysis and clustering. The contributions include introducing MMJ distance, proposing efficient calculation methods, discussing its properties and applications, and offering a user-friendly approach for practical implementation. Overall, the paper introduces a new distance metric for path optimization and data analysis, providing useful tools and insights for various applications in the field.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
S1: The paper demonstrates strength through its meticulous use of theorems and proofs, enhancing the credibility and robustness of the research findings.

S2: Clear visualization of results in the paper aids in effectively conveying complex information to the readers, improving understanding and interpretation.

S3: Extensive literature citations throughout the paper showcase a strong foundation of existing knowledge and research, adding depth and scholarly rigor to the study's presentation.

Weaknesses:
W1: The paper's writing style deviates from academic norms, indicating a need for improvement in writing proficiency.

W2: The extremely brief Introduction lacks a detailed definition of the problem, its significance, and challenges. Moreover, it lacks citation support for the points presented. While Section 2.1 mentions methods like k-NN, UMAP, HDBSCAN, it fails to provide corresponding references, lacking essential academic backing.

W3: The overall structure of the paper lacks clarity, as it introduces different distance metrics in Section 2.1 but introduces a new distance measurement approach in Section 2.4, leading to disjointed logic.

W4: The presentation of various distance metrics in Section 2.1 appears disorganized and lacks coherence.

W5: The extensive definition provided towards the end of Section 6.3 disrupts the logical flow of the paper, suggesting a need to adjust the paper's structural coherence.

Limitations:
The paper does not discuss limitations. The authors seem to perceive their work as solely testing models with datasets without considering the shortcomings of the algorithms themselves.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Different distance metrics have been introduced in the literature for data analysis. In this paper the authors consider the min-max-jump distance and apply it in the context two applications, namely, k-means clustering and as an internal clustering evaluation index. They also present two algorithms for computing the min-max-jump distance.

Experimental comparisons reveal that min-max-jump based k-means clustering is better than standard k-means clustering. Also, the min-max-jump distance is shown to be a better internal clustering evaluation index.

This referee feels that this work is rather incremental. Also, experiments have been conducted only on very small datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors demonstrate the efficacy of the min-max-jump distance on two different applications.

Experimental results have also been supplied to support their assertions.

Weaknesses:
The work done is incremental with very limited novelty.
Extensive experiments are called for.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
XkMCKoHNCD;"REVIEW 
Summary:
The authors present an analysis of logistic regression on sentence embeddings as a way to predict the speaker of a particular line of dialogue in the Big Bang Theory. Specifically, they fit a PCA model to embeddings obtained from a sentence transformer and then use each PCA dimension as a linear feature. The authors present some qualitative analysis of the most predictive PCA dimensions and some quantitative analysis of the classification accuracy. In addition, they present a brief analysis of the ability of GPT-4 to directly classify lines of dialogue and compare to a limited user study.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The authors have identified an interesting and important debate in the AI community — more tests which help researchers discriminate between mere stochastic parroting and true generalization are certainly needed! In addition, the authors are very thorough in their description of the methods involved and their qualitative analysis of the PCA features is extensive. I also appreciate the thought the authors have given to the limitations of their study and the need for further work.

Weaknesses:
First and foremost, I feel that this paper needs to be much clearer and more focused in its research question. The introduction indicates that the objective of the study is to determine the extent to which the apparent ability of large language models like GPT-4 to generalize to novel tasks is actually attributable to their ability to parrot data from their training. However, a good part of the analysis appears dedicated to the specifics of The Big Bang Theory and the features of its dialogue. Section 3.1, for instance, extensively interrogates the PCA dimensions obtained from the sentence embeddings in a way that feels very specific to the particular dataset. Similarly, the conclusion raises claims that the ability for logistic regression to predict the speaker with reasonable accuracy is due to stereotyping in the characterization of the show. These claims are potentially warranted given the experimental evidence (though a more detailed and statistically-motivated analysis would be necessary to make such claims with certainty), but feel as though they belong in a different paper (a potentially quite interesting paper for a different venue, I should add). The connection between these results and the initial framing of LLM evaluation remain, unfortunately, somewhat murky. This is not to say there is no possible link between dialogue speaker prediction and LLM abilities! I encourage the authors to think about this problem more and articulate the specific claim they hope to interrogate.

On that note, and assuming that the main motivation is indeed to study large language models, I feel that the analysis could be strengthened. First, it would be helpful to justify some of the specific decisions made as part of evaluation. For instance, why were the Big Bang Theory and Friends selected over other possible dialogue datasets? Why was dialogue speaker prediction always studied between exactly two characters? Why were these specific characters selected? Do the characters have a similar amount of lines, or are there other statistical biases in the dataset that might affect the results?  When proposing a novel task, it’s important to make the assumptions and decisions that went into the task selection clear.

With regards to human evaluation, I encourage the authors to widen their study. That is to say, a user study which consists of only two participants (both of whom are related to one of the authors) makes it difficult to ascertain the reliability of the results. Indeed, I would suggest a study consisting of a larger number participants (ideally participants who do not have any externally motivating factors like relationships to the authors) so that a more general measure of human ability can be obtained. Further, I think it could actually be preferable for the participants to not have prior experience with the television show. This would make the test more an examination of the ability for participants to generalize their knowledge of personality traits to a novel situation instead of their ability to recall information (which is, ostensibly, closer to the desired research question in LLMs).

Despite these critiques, I hope that the authors continue to refine their research question, justification, and methodology. There are interesting questions to study here!

Limitations:
I feel that the authors have been very up front with the limitations of their work and have situated it in the context of broader impacts.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors are focused on whether or not LLMs can be thought of as stochastic parrots or contain ""Sparks of AGI"". They look into what kind of data is recoverable from internal LLM representations. Specifically, the authors investigate to what extent the task of identifying TV personalities (e.g. Penny vs Sheldon) based on their dialogue lines is solvable using various methods. The authors compare a classifier based on PCA components extracted from existing LLM embeddings, GPT-4 zero shot performance, and human expert judgments. They find that all methods show fairly good performance, with human experts showing best results, followed by GPT-4, followed by the classifier. The authors also present a brief qualitative analysis, interpreting the more prevalent axes of variation in the embeddings identified using PCA.

Soundness:
1: poor

Presentation:
3: good

Contribution:
1: poor

Strengths:
The authors tackle a very ambitious and important problem. The writing is clear throughout, and the authors provide extensive background for the methods they use.

Weaknesses:
I need to preface this by saying that I hope that my negative review does not discourage the authors from further pursuing the topic. I feel bad for having to reject this paper as it has some good ideas behind it and has an intention of researching a highly important problem. I hope that in next iterations, their work can be improved and expanded. At present, unfortunately, it does not match publication standards. I will try to explain why, and give pointers on how to potentially fix it in the future.

The biggest flaw of the paper is the experiment design. The authors never clearly define what exactly it means to be a ""stochastic parrot"" as opposed to ""general intelligence"". The authors also don't explain how their experiments would help to decide one way or another. So the results we have are impossible to interpret. It would help to go back to the original question and work through the argumentation more clearly. If the internal LLM representations have information about TV personalities, does it make them more or less of a stochastic parrot and why.

Otherwise, the experiments give a very exploratory impression. For example the authors run PCA on sentence embeddings computed on their dataset and interpret the components. But it's unclear why and how this would help to answer the main question the paper attempts to answer.

Additionally, the paper's methods are extremely well-known, but unfortunately, the authors don't refer to relevant literature. The work highly overlaps with the topic of linear and nonlinear probes, as well as with the general theme of transfer learning. In essence, what the authors did can be described as adding a ""classification head"" to a pre-existing LLM. This is a very well-known technique.

If we want to gain new insights into what the models are doing, it is usually more interesting to look into the computations in intermediate layers of the model, rather than the last embedding layer. It is also often desirable to look at causal probes (rather than just a classifier).

Lastly, there are certain writing choices that deviate from common ""conventions"" in academic publishing. For example, oftentimes the authors go into excessive detail on well-known methods (explaining how PCA works and what a covariance matrix is). I highly suggest that the authors look at existing successful papers that use similar methods and copy their approach when it comes to decisions on what to explain in the main text, what to put into the appendix, and what to omit. The general rule of thumb is that newly introduced and important ideas should be at least briefly given in the main text, with extra details given in the appendix. Extremely well-known and established methods such as Principal Component Analysis don't need a full explanation, and a simple reference to the original source is enough.

I really hope that the authors don't get discouraged and try to refine and improve their research in the future. The first starting point would be to more clearly define the problem, and to study in depth the existing literature on linear probes and probing in general, and on investigating what the internal LLM representations contain. One potential starting point is the paper ""Evaluating the World Model Implicit in a Generative Model"", Vafa et al. 2024 and related works.

Limitations:
The authors acknowledge some of the limitations of their study.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper’s main contribution is to apply a logistic regression on the principal components of the LLM embeddings for classifying TV series characters based on their dialog lines. The main finding is the logistic regression approach does worse than GPT-4 in predicting TA characters, but is comparable to human evaluations with two annotators.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
The paper focus on an interesting angle of using language model features for predicting the belongings of dialog lines of characters of TV shows.

Weaknesses:
The methodology of using logistic regression over PCA of language model embeddings is not novel, and there's no rigorous quantitative evaluations of the method beyond qualitative examples. The connection of the method and task to the broad discussion around ""spark of AGI"" and ""Stochastic Parrots"" is farfetched.

Limitations:
The paper claims that ""the contribution of the paper is primarily methodological, and their study is limited to a qualitative study of two very specific datasets."" However, the method they adopt is a fundamental ML technique, which lacks novelty.

Rating:
1: Very Strong Reject: For instance, a paper with incorrect statements, improper (e.g., offensive) language, unaddressed ethical considerations, incorrect results and/or flawed methodology (e.g., training using a test set).

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper aims to prove LLMs work as ""stochastic parrots"" (Bender et al) rather than ""sparks of agi"" (Bubeck et al). To prove this claim, the paper presents an experiment where a task can be solved by training a linear model (logistic regression) on top of PCA of the LLM output. The authors then claim, based on the linear model experiments, that the LLM doesn't exhibit any sparks of agi due to the ability of (nearly) solving the task using linear models.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The authors show a simple linear model trained on the output of an LLM for a given task is good enough to solve it, compared to using a GPT4 model, raising questions on the supposed intelligence often ascribed to the model.

Weaknesses:
While I generally agree that LLM's are closer to ""stochastic parrots"" than ""sparks of agi"", the claim that it can be proved using the proposed PCA experiments is weak to me. 

- Firstly, the embeddings are essentially the output of the LLM in question (all-MiniLM-L6-v2) - I would call it outputs rather than embeddings, as embeddings just indicate input word embeddings to the model, which clearly here isn't the case. 
- Secondly, the outputs itself being feature rich to be used for classification is unsurprising. It is expected the principal components of this embedding would be useful in predicting the properties of the task (as shown in the projection of PCA plots). This just shows the underlying model (SentenceBERT here) is good at extracting rich semantic and syntactic features from the input sentence (probing literature essentially proves that [1]).
- Lastly, the experiment also shows the representations extracted from the sentence embedding model is sufficient for the task. For a harder task, if the linear probe on all-MiniLM-L6-v2 was not good with respect to GPT4, that would also not conclusively prove the ability of GPT4 is due to any sparks, rather it can be explained that GPT4's own embedding features are richer. That is, a linear probe trained on GPT4 embeddings for a harder task would also likely mimic its own performance. (this is theoretical, as neither the author or anyone other than OpenAI have access to their embeddings)

[1] https://aclanthology.org/D19-1250/

Limitations:
There are no explicit limitation section, however the last paragraph of conclusion discusses it.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
apI1GltwSx;"REVIEW 
Summary:
This work studies batch-to-global dataset distillation, optimizing the synthetic dataset by matching the statistical information of the synthetic batches to that of the full real dataset. Previous batch-to-global methods lacked diversity because each batch had the same optimization objective, leading to redundant information being learned across different batches. Based on this, the paper proposes an early-late training method. First, the real data is divided into lowest, medium, or highest probability patches based on a pretrained model, and these patches are sampled to initialize the synthetic dataset. During training, within-class samples are divided into smaller sub-batches, which are gradually concatenated for batch-to-global training.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. Previous batch-to-global methods indeed faced the problem of synthetic datasets receiving the same supervision signal, leading to redundant information being learned. This paper attempts to propose a new solution to this issue.

2. Extensive experiments demonstrate the good efficacy of the proposed method.

Weaknesses:
##  The main problem of this work is the writing.

>*It dedicates too much space to introducing previous work.*

The introduction describes previous methods in too much detail, leading to redundancy with the content in the related work section. The related work section also spends too much space summarizing and describing previous methods.

>*The technical part is confused*

The proposed method appears straightforward, but the authors describe the entire process almost entirely in text, lacking mathematical descriptions and definitions, which makes it somewhat difficult to understand. I suggest the authors dedicate more space to explaining the Concatenation Training and Training Procedure, incorporating some formulas to clearly demonstrate how the training is conducted.

>*It is doubtful whether the proposed method can effectively solve the issues present in previous approaches.*

Although the synthetic dataset is further divided within classes and different initializations are used, the supervision signal for each sub-batch seems to still be the same global signal as in other batch-to-global methods. This means that each sub-batch is still optimized in the same direction, potentially resulting in redundant information being learned. I suggest that the authors try to provide a more sound explanation.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Recent advancements in dataset distillation have led to two main approaches: batch-to-batch and batch-to-global matching. While the former excels in small datasets, the latter, though popular for large datasets, faces a diversity challenge due to independent optimization. Authers propose an EarlyLate training scheme that enhances diversity in batch-to-global matching by partitioning IPC samples into subsets and optimizing them locally. Experiments show significant improvements over previous methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1) The technical approach is solid and robust, demonstrating a high level of technical competence.
2) The performance metrics presented are highly competitive, showcasing the method's effectiveness in comparison to existing benchmarks.

Weaknesses:
1) The motivation for the research is unclear, lacking an explicit articulation of the unifying challenges faced by current state-of-the-art works.
2) The resolution of the figures is inadequate, impeding clear interpretation of the results.
3) There is inconsistency in the styling of table borders and captions, with captions for Table 1 and 2 placed in different positions compared to subsequent tables, some above and some below the table.
4) The experimental settings are not uniformly aligned, and efforts should be made to cover all datasets and settings consistently across all experiments to ensure comparability and rigor.

Limitations:
see weaknesses

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a simple but novel approach to enhance image diversity in dataset distillation. previous methods face challenges in balancing computational efficiency and diversity in synthetic images. The proposed EarlyLate training scheme addresses these issues by partitioning predefined IPC samples into smaller subtasks and using local optimizations for each subset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The EarlyLate training scheme effectively enhances the diversity of synthetic images. This is a very simple but novel approach to increase the diversity of distilled datasets. I believe it can provide some inspiration for future work.
2. The method, or the training scheme reduces the computational load compared to batch-to-global matching methods.
3. The experiments are comprehensive, including performance, cross-architecture generalization, ablation, and application. These experiments verify the method's superiority.

Weaknesses:
1. Compared to previous methods, the work in this paper is incremental.
2. The motivation and the advantages of the ""Selection Criteria"" in the initialization approach are not clear. And I am confused about how to rank, which is presented in Fig 5, could the authors explain it here?
3. There are a lot of hyperparameters involved. How should these hyperparameters be tuned? Are there any principled approaches?
4. I want to know the impact of the initialization method. In the ablation study, only CDA+init is shown. More advanced methods with init and whether EarlyLate uses init are not presented.
5. The performance of other sota methods on MobileNet-v2 is not presented in Table 1. Is the proposed method still better than other sota methods on MobileNet-v2?
6. Training tricks like random crop play a significant role in methods such as SRe2L. I would like to know to what extent the method proposed in this paper relies on such tricks.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes an EarlyLate curriculum learner, which distills the easiest samples first and gradually add harder samples. Based on batch-to-global distillation algorithms, the proposed method consistently enhances the distillation performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The writing is clear.
- The proposed curriculum learning scheduler seems effective, which is also an interesting point to analyze.
- Good distillation performance.

Weaknesses:
Limited contribution and potential overclaiming: 

1. In section 3, the initialization with real samples is common in DD, the data selection is proposed by RDED, and only the training scheduler is proposed in this paper. I suggest that, at least, add some diversity analysis and comparison of the distilled data.
2. Though the paper is titled with ""diversity-driven"", the method part lacks justification of the relation between ""diversity"" and the proposed scheduler. It seems that only the real initialization contributes to the diversity.

Limitations:
The authors have adequately discussed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
I6tRENM5Ya;"REVIEW 
Summary:
The paper proposes a new framework that revisits SHGL from a spectral clustering perspective, incorporating rank and dual consistency constraints. This approach uses a rank-constrained spectral clustering method to refine the affinity matrix and remove noise, while also integrating node-level and cluster-level consistency constraints to better capture and use invariant and clustering information. Theoretical analysis and experimental results show that this new method significantly improves performance in downstream tasks compared to existing methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This paper provides a theoretical investigation of previous SHGL methods from the perspective of spectral clustering.
2. This paper proposes a new framework to capture the cluster-level graph invariant representations.
3. Experiments demonstrate the effectiveness of the proposed methods.

Weaknesses:
Unfortunately, this paper is not easy to follow due to the missing parts in terms of motivation and logic.

1. In the introduction part, the authors mention three challenges abruptly from Lines 44 to 48. However, it is unclear why they are challenging to address. For example, why it is difficult to understand previous SHGL methods from the clustering perspective?  It is also unclear the logical relationships between the three challenges.

2. In Section 2, the authors introduce some notations about the heterogeneous graph. However, it is unclear what is the objective of the problem.

3. In Section 2.1, it is unclear why authors need to bridge the gap between SHGL methods and graph-cut algorithm, and what is its benefits.

4.  In Section 2.2, it is unclear why the rank constraint can mitigate noisy connections.

5. When reading Section 2.2, the reviewer gets lost in the lemma and equation details. It is better if the authors can highlight the contributions and leave all technical derivations to the appendix.

6. The logical relationship between Sections 2.2 and 2.3 is unclear. Why both the low-rank constraints and dual consistency constraints should be imposed, and what is the relationship between these two constraints?

7.  In the experiments, it is unclear why two homogenous graph datasets were used for performance evaluation, given the objective of this paper is for heterogeneous graph learning.

Limitations:
This paper does not mention the limitations of this work or the potential negative societal impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work deals with the problem of self-supervised representation learning in heterogeneous graphs. First, a spectral-clustering based objective is presented to unify the objectives of existing methods. Second, a novel self-supervised method is proposed that tries to capture both the cluster information and node-level information in the learnt representation. Experiments show the effectiveness of the proposed method over existing methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method is novel and follows a principled design.
2. Presented theoretical analysis is helpful in deeper understanding of existing and proposed method.
3. Experiemental section is comprehensive with adequate baselines, datasets, and ablations.

Weaknesses:
1. Presented theorems are often unclear or imprecise. For example

   (a) In Theorem 2.6, authors use the term model complexity abruptly without any definition or reference. I could not understand the relation between the infimum and supremum of model complexity with generalization ability (which also needs to be rigorously defined).

   (b) In Theorem 2.2, what exactly are the previous meta-path-based and adaptive-graph-based SHGL method mentioned in the theorem statement? What is the expression for the regularization term for the corresponding previous methods?

   (c) There does not seem to be a clear delineation between what already exists in the literature and what is the new contribution of the paper. E.g., does Theorem 2.3 directly follow from existing results in the literature, or does this require custom analysis?

   (d) Theorem 2.5 is not precise enough. When referring to ""proposed method"", it would be more precise to refer to specific objective function or algorithm block. Also ""equivalent to performing spectral clustering based on the affinity matrix ...."". Authors are recommended to replace the language description with precise objective function.



2. The writing is often unclear and could be greatly improved. Several terminologies (which might not be standard in the literature) are never explained/defined. For example:

   (a) Section 1 Line 42 "".. it is feasible to analyse .."", it is not clear what exactly is feasible to analyse (although after Section 2.1, it can be understood).

   (b) In abstract and introduction, it is not clear what is invariant information is

   (c) Sentence in line 216-218 is unclear.

   (d) How does H^T H = I imply statistical independence?

Limitations:
Limitations and impacts are adequately discussed in the Appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a theory-backed method for Self-Supervised Heterogeneous Graph Learning (SHGL) based on spectral clustering and incorporates rank constraint and node/cluster consistency regularizers to generate better embeddings. In specific, the authors start by showing that existing algorithms divide the representations into a certain number of clusters that are much larger than the number of real classes. Then, an objective function with rank constraint is proposed to reduce the noise of message-passing and consistency terms are employed to improve downstream task performance. Experiments on the real datasets show that the proposed method outperforms other baselines.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	The paper analyzes the issue of the exiting methods that are based on meta-path-based graph or the adaptive graph structure theoretically. Furthermore, a rank and consistency constrained approach is proposed to tackle know issues of SHGL and it is shown to have less complexity theoretically.
2.	Experimental results show the superiority of the proposed method consistently over a variety of datasets.

Weaknesses:
1.	Notations are not always clearly defined. For example, what are the dimensions of the mapping $g_{\phi}$ and $p_{\phi}$?
2.	The derivation of the first loss term is not clear to me. Why is there an entropy constraint term in (13)? How do we translate Eq. (8) to Eq. (13) and what is the correspondence of the terms? Line 183 states that ‘fitting eigenvectors F by (13)’, but it seems that F does not appear (13). Or is it that (13) is only for solving the last term in (8)? Clarity can be further improved upon the objective function, especially the term $\cal{L}_{sp}$.
3.	Why is $Y$ a probability matrix? Is there any guarantee on that? Only orthogonality is proved for the matrix. Do we need a simplex constraint on the columns of $Y$?

Limitations:
The authors are encouraged to discuss the limitations in the main paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Overall, this paper makes the first attempt to theoretically revisit previous SHGL methods from the spectral clustering perspective in a unified manner. Specifically, this paper revisits SHGL from the spectral clustering and introducing a novel framework enhanced by rank and dual consistency constraints. Specifically, the proposed framework incorporates a rank-constrained spectral clustering method that refines the affinity matrix to exclude noise effectively. Additionally, the proposed method integrates node-level and cluster-level consistency constraints that concurrently capture invariant and clustering information to facilitate learning in downstream tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. I really appreciate the idea that revisiting previous SHGL methos from the perspective of spectral clustering, which is interesting and may inspire some researchers in the graph learning as well as the self-supervised learning communities.
2. Theoretical analysis verifies the effectiveness of the proposed method, which divides the learned representations into distinct partitions based on the number of classes. Furthermore, the proposed method exhibits enhanced generalization ability than previous SHGL methods.
3. Extensive experiments on both heterogeneous and homogeneous graphs demonstrate the effectiveness of the proposed method. Visualization and case studies further verify the claims in this paper.
4. The proposed rank-constrained spectral clustering is novel and interesting, and it seems like can also be used to the graph structure learning in the homogeneous graph.

Weaknesses:
1. In the Introduction, the authors claim that previous methods conduct message-passing relying on meta-path-based graphs and adaptive graph structures, which inevitably include noise. Are there any real examples to better illustrate the noise in meta-path-path based graph as well as the adaptive graph structures?
2. In the dual consistency constraints, the proposed method designs the node-level consistency constraint to capture the invariant information between the node representations and the heterogeneous representations. How about replacing the node-level consistency constraint with other common loss, such as InfoNCE [1]?
[1] Aaron van den Oord et al., Representation learning with contrastive predictive coding.
3. The authors design the rank-constrained spectral clustering to learn the affinity matrix for nodes belonging to the same node type. Although the visualization verifies the effectiveness of the affinity matrix, it would be better for the authors to add some ablations studies to further verify it, such as replacing the affinity matrix with a self-attention mechanism or simple cosine similarity.
4. What are the specific processes for different downstream tasks (e.g., node classification and node clustering)? It seems like training the proposed method first, and then freezing parameters of the model and applying outputted representations for downstream tasks.
5. The proposed method is designed for the heterogeneous graph while it can be implemented on both the homogeneous graph datasets and heterogeneous graph datasets. Is it just to replace the heterogeneous graph encoder with the graph convolutional layer? How to generate two different views for the dual consistency constraints?
6. It would be better to add some recent works about the self-supervised heterogeneous graph learning in the related work, especially those published in the past two years.
7. The paper needs further proofreading. For example, 
- In Eq. (17) I know $q_i$ is the i-th projected representation. What’s the $\hat{\mathbf{q}}_{\mathbf{y}_i}$ actually means?
- The definitions of some symbols need to be further determined, such as \mathbf{I} in Definition 2.1. I guess it might represent the identity matrix. 
- Some mistakes in Table 3, the Freebase dataset should be removed from the Table.
- The caption of Table 7 should be fixed.

Limitations:
Yes, the authors have discussed the limitations and potential negative societal impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
7ug4oSmN7l;"REVIEW 
Summary:
This paper presents a novel neural network-based CARP solver that uses a direction-aware attention model to incorporate directionality into the embedding process. It then applies supervised reinforcement learning for subsequent fine-tuning.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. A learning-based CARP solver is proposed.  
2. The performance of the proposed solver on large-scale data is discussed.

Weaknesses:
1. The comparison algorithms were published five years ago, and there is no discussion of existing methods aimed at big data.  
2. The experiments only tested the self-constructed dataset and did not evaluate on public datasets.

Limitations:
The amount of data required for algorithm training needs to be discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a learning-based method to address the Capacitated Arc Routing Problem (CARP). It involves breaking undirected edges into directed arcs and utilizing a graph attention network to build a Direction-aware Attention Model. In the training process, supervised learning is used to create the initial policy, followed by reinforcement learning based on policy gradients using Proximal Policy Optimization (PPO) to refine strategies. Lastly, dynamic programming is applied to optimize depot placements for path enhancement. Experimental outcomes show notable benefits of this algorithm in evaluation criteria.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
In general, the paper exhibits a well-organized structure with detailed experimental outcomes showcase through graphs and tables, facilitating readers in comprehending and visualizing the results effortlessly. The dataset employed comprises real-world scenarios, thereby boosting its practical relevance.

Weaknesses:
Converting the graph G from arcs to nodes represents a common approach in many heuristics for addressing CARP. This process adds complexity to the problem and increases its scale. The proposed method appears to lack enough novelty, with most components bearing resemblance to neural models designed for CVRP.

Limitations:
It appears that the paper focuses on an unlimited number of vehicles. How would the approach adapt to a specific set of vehicles?

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors skillfully address challenges posed by non-Euclidean graphs, traversal direction, and capacity constraints with their novel NN-based solver in solving capacitated arc routing problem. The introduction of the direction-aware attention model and a supervised reinforcement learning scheme is particularly commendable. These innovations significantly narrow the gap with advanced metaheuristics, achieving superior efficiency and competitive decision quality.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The manuscript employs numerous innovative methods to solve the capacitated arc routing problem, achieving impressive results. 
2. It also shows promising performance in generalizing to larger problem instances.
3. The combination of supervised and reinforcement learning is quite interesting. Using supervised learning for pre-training followed by fine-tuning with reinforcement learning is a noteworthy approach.
4. The qualitative comparisons in real street scenes presented in Figure 4 are particularly interesting.

Weaknesses:
1. It's better to redraw the first part of Figure 1 to enhance its aesthetic quality.
2. The baseline is not very recent. After S2V-DQN and S2V-DQN, there are still some excellent works that can be used to address the CARP problem.
3. Some writing errors have been identified, such as in line 2 of Algorithm 1. Please review the entire manuscript to check.
4. The completeness of the manuscript still requires supplementation and refinement.

Limitations:
The approach of decomposing undirected edges into directed ones introduces additional decision elements, which complicates the problem. It's better that the authors can find a more efficient graph processing method.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new learning-based constructive heuristic for capacitated arc routing problems. In contrast to node routing problems such as the TSP and VRP, arc routing problems received comparably little attentition. To address the specific
challenges in the capacitated arc routing problems, the authors propose a Neural Network-based approach that uses a graph attention
model considering arc directionality, a reinforcement learning approach with supervised pre-training and PPO-based fine-tuning. In
order to improve solutions obtained by an RL-based construction approach, they propose a beam search approach for path optimization which, after turning the set of routes into a giant tour, splits the tour into routes by adding returns to the depot. A set of experiments
shows that the proposed approach consistently yields better results than traditional hand-crafted constructive heuristics, and that their solutions almost match the quality of a time-consuming memetic algorithm that is only capable of solving small instance in a reaonable amount of time.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors propose one of the first learning-based approaches for the capacitated arc routing problem (CARP). Their approach, in particular their graph embedding, explicitly addresses one of the challenges of learning-based construction algorithms for this problem by explicitly replacing the undirected edges by directed arcs. This idea is original and turns out to be helpful to create a well-performing heuristic. 

The online performance of the approach surpasses hand-crafted constructive heuristics both in terms of runtime and efficiency. While for small instances, other metaheuristic approaches are better, it can be assumed that for large-scale instances, the proposed approach surpasses the state-of-the art of heuristic approaches. This is a significant result, since for node routing problems such as the CVRP, researcher have been struggling for years to design learning-based heuristics that achieve a performace that is comparable to hand-crafted heuristics. It should be mentioned, though, that in general, arc routing problems receive much less attention than node routing problems in the literature.

The paper comprises several insightful and, as far as I can tell, reasonably designed experiments, in particular showing the generalization capability to larger instances. 

The paper provides both code and instances.

Weaknesses:
The presentation of the CARP routing problem, the solutions approaches and related work lacks clarity in many places.
As an example, in the abstract, we find that the CARP consists in finding ""the minimum-cost tour""hat covers all required edges on a graph, while within capacity constraints"". This is a a bit misleading description since we look for a set of routes instead of a tour.

The paper distinguishes ""heuristics"" and ""metaheuristics"", while clearly metaheuristics are a type of heuristics. Actually, what the authors appear to have in mind is ""constructive heuristics"" which sequentially construct a solution by adding edges to form routes. I suggest to formulate more precisely here.

Similarly, it would enhance the understanding of the paper to introduce the notion of ""route-first, cluster second"" and the related
notion of a ""giant tour"" which is commonplace in routing applications, to characterize respective existing work. It would even facilitate the
presented path optimization which actually turns the presented approach into a route-first, cluster second approach. 


The computational results are convincing, but the discussion should emphasize that a fair comparison can only be made between their
approach wihout path optimization and the other constructive heuristics. It would indeed be interesting to see how the far the path
optimization is able to improve the results of the other constructive heuristics.

The claims ""NN-based approaches tend to lag behind advanced metaheuristics"" (abstract) and ""NN-based methods usually lags far
behind the traditional ones in solving CARP"", ""they still lag significantly behind traditional methods"" are not valid. Actually, (Rahmamoorty et. al 2024) (reference 20) report that on average, they improve upon the memetic algorithm by 11% on average.

When it comes to the evaluation of the path scanning approaches in the experiments, it is unclear how they are parameterized. From reading the paper (Aarakaki 2019) one sees that the parameter alpha and the number of iteration have a considerable impact both on solution time and solution quality, and (albeit on different instances), the average gaps for the path scanning approaches to the optimal (and to the memetic algrithm) reported in (Aarakaki 2019) are smaller than those found in the submission.

The description of the path improvement is not very clear; in particular the definition of the state used in the Dynamic Programming
algorithm. Is it a path? Is it the length of a path? Also, the statement ""f(*) denotes a state featuring dynamic programming"" is hard
to decipher.

Training time is not discussed at all.

Limitations:
Limitations are mostly addressed in a reasonable way. I suggest to add the following aspects:

I think that for small instances, the approach by (Rahmamoorty et. al 2024) may surpass the results reported here, which should be mentioned. 

Also, you should at least briefly mention the training time, since this makes it easier to assess the trade-off between offline effort and online performace.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
jM9atrvUii;"REVIEW 
Summary:
The authors introduce a family of Gaussian process based regression models for protein variant effect prediction. The ""composite"" kernel introduced makes use of structural information (i.e. closeness in 3d space) as well as pre-trained sequence and/or structure models like ESM2 and ProteinMPNN (via embeddings and/or amino acid preference distributions). The model shows promising predictive performance on the ProteinGym benchmark, including w.r.t. uncertainty metrics like expected calibration error (ECE).

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The authors contribute to a problem class that is of considerable interest to a fairly large slice of the NeurIPS community, inasmuch as it brings together a compelling application of ML to proteins, touches on transfer learning and representation learning, investigates uncertainty quantification in a difficult problem setting, and leverages classical methods like Gaussian processes
- By using the (ever-larger) ProteinGym benchmark and including quite a few ablations, the authors provide a relatively comprehensive empirical evaluation of their method.
- The performance of the proposed method appears to be pretty good (at least for the regime where you only extrapolate out a few mutations), and could presumably improve as other sequence and/or structure models are plugged-in to the same general construction

Weaknesses:
- The description and discussion of the Kermut kernel is not very easy to follow and could be considerably improved. 
  - For example, much of the discussion on lines 128-144 seems a bit besides the point, since the authors do not in the end develop a model that is linear in one-hot-features. 
  - The authors should do a much better job of foreshadowing/signposting that their construction ""for single mutant variants"" is but a stepping stone to a multi-mutant construction; otherwise the reader is liable to get confused about what's going on. 
  -  Many of the choices that lead to the final kernel construction are either not motivated or only briefly discussed. Why not use, for example, a product kernel $k_{\rm struct} \times k_{\rm seq}$? Some of these alternative choices should be discussed and, ideally, included in ablations. 
  - What does this mean? [Line 167] ""preventing the comparison of different mutations at the same site"" 
- The paper is missing a discussion of the computational complexity of computing the kernel w.r.t. the sequence length, the number of mutations from the wildtype, etc.
- The discussion of prior work is inadequate, especially w.r.t. work on sequence kernels and previous applications of GPs to protein modeling. Granted there is only so much space in the main text, but the reader deserves a more detailed discussion. (Perhaps some of the discussion can be relegated to the appendix.) To name just a few examples, there is a lot of work on sequence kernels (see e.g. [A] and references therein) and it is negligent to ignore this body of work. Also the method comparison to mGPfusion [24] is inadequate. The authors did not invent sequence kernels or pioneer their application to protein modeling (e.g. [B, C, D] for more recent work) and should be much more liberal and informative in their discussion of the literature. Granted the relevant literature can be scattered (arxiv, biorxiv, etc.), but the readers deserve (much) better. Discussion of relevant work is more than a required chore: if well done it adds significant value to the reader and the literature as a whole.

References:
- [A] ""Biological Sequence Kernels with Guaranteed Flexibility,"" Alan Nawzad Amin, Eli Nathan Weinstein, Debora Susan Marks, 2023.
- [B] Moss, Henry, et al. ""Boss: Bayesian optimization over string spaces."" Advances in neural information processing systems 33 (2020): 15476-15486.
- [C] Parkinson, Jonathan, and Wei Wang. ""Scalable Gaussian process regression enables accurate prediction of protein and small molecule properties with uncertainty quantitation."" arXiv preprint arXiv:2302.03294 (2023).
- [D] Greenman, Kevin P., Ava P. Amini, and Kevin K. Yang. ""Benchmarking uncertainty quantification for protein engineering."" bioRxiv (2023): 2023-04.

Limitations:
To my mind the major limitation of this work is that reliance on the ProteinGym benchmark means that there is little signal on how well this class of models would perform in more challenging problem settings where the model is asked to extrapolate out many mutations away from the wild type. There are increasingly many public datasets that make this kind of evaluation possible in principle, and I encourage the authors to apply their method to more challenging scenarios.

For example:
- Chinery, Lewis, et al. ""Baselining the Buzz. Trastuzumab-HER2 Affinity, and Beyond!."" bioRxiv (2024): 2024-03.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors suggest a method to predict the effect of mutations given sparse data. Their method is based on identifying the similarity of different sites on a protein by embeddings from large language models and structure. They show that their method performs state of the art mutation effect prediction. They also show that their method is only slightly overconfident.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The method performs substantially better than state of the art prediction.

The method combines a variety of methods to build a prior on the effect of mutations.

The authors include a codebase that makes this method easy to use for practitioners.

Weaknesses:
Epistasis is only included through sequence embeddings; in particular, the impact of structure is purely linear.

Despite not suggesting any radically new technique, this is a practical method that cleverly and effectively uses available tools. For this reason however, I think it is reasonable to expect that the authors try hard to build as strong a method as possible. In particular, this paper is missing a more thorough investigation of model choices -- what if the Hellinger kernel is replaced with something else? what if a different kernel is used to compare embeddings? what if equation 2 is replaced with a kernel with a term for every combination the 3 kernels? what if the kernel is meta-learned on a subset ProteinGym and applied to the rest? The paper would be substantially strengthened by applying the methodology of Duvenaud, David. 2014. “Automatic Model Construction with Gaussian Processes.” University of Cambridge.

Limitations:
Partially addressed. I would like a longer discussion about epistasis mentioned in the weaknesses section.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a kernel regression model to predict protein mutational effects. The model includes kernel functions crafted for the task. In specific, it includes a kernel that measures sequence similarity based on ESM-2 features, a local structure similarity kernel based on ProteinMPNN probability, and other kernels that impose priors such as spatial correlation between mutation sites.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Kernels are carefully designed. Using ESM-2 and ProteinMPNN features to construct kernel functions is well-motivated. The effect of each kernel is well-justified via ablation studies (Table 2).
- Uncertainty quantification capability of Gaussian process is valuable for making decisions in wet labs, which has been often neglected in previous work on protein variant effect prediction. This work provides such quantification and further insight into it.
- The model is much faster than purely neural network-based methods which require at least one forward pass per mutation. Kermut is efficient because the sequence and structure features are computed only once for a protein sequence.
- Kermut achieves significant better performance than baselines. It is a good demonstration of making use of pretrained neural network features with statistical methods when training data is not that much and interpretability is desirable.

Weaknesses:
- Current formulation of Kermut does not provide transferability to different protein sequences, while previous zero-shot prediction methods are capable of predicting variant effects without prior experimental data on the same sequence.

Limitations:
See weakness section.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
cO1llRY2Br;"REVIEW 
Summary:
This paper focuses on model editing at a low cost. Evidence suggests that modules carrying knowledge in a Transformer module are primarily the MLP blocks. Therefore, the authors propose a method, namely iReVa, to initialize and retrofit key-value pairs into MLP blocks in a Transformer for explicitly inserting new knowledge. Specifically, they insert new neurons in the MLP blocks for each piece of knowledge. Each neuron is initialized with the embedded key and value derived from the input-output pair, respectively. To prevent dramatic change to the irrelevant knowledge, iReVa further retrofits the key and value by fine-tuning with multiple objectives. Compared to the existing methods such as MEND, ROME, MEMIT, and MELO, iReVa reveals better interpretability and stronger capacity for carrying traceable edits. The experiments on zsRE-10K and PARAREL-10K datasets reveal that iReVa has superior performance regarding edit success, generalization, and specificity. Further edit withdrawal test indicates that iReVa can explicitly manipulate the activation of neurons and easily withdraw the edits.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	This paper focuses on modeling editing, which has significant applications in the era of LLMs. It can be applied to alleviate the hallucination issue of LMs and resolve the out-of-date as well as missing knowledge in an LM.
2.	This paper introduces a novel editing method with key-value adaptors for traceable model editing. The proposed method makes sense to me. The initialization with embedded key and value derived from the input-output pair can easily make precise edits to the model. Further retrofitting refines the adaptors to satisfy the task.
3.	For experiments, the author has comprehensively shown the superiority of their method in the perspectives of edit success, generalization, and specificity. And more analyses reveal the generalization of iReVa. Particularly, the edit withdrawal test in Section 6.2 is well-designed, which shows the effect of traceable edits and could provide a potential solution for dynamic knowledge maintenance for LMs.
4.	Overall, this paper is well-written and easy to follow.

Weaknesses:
1.	The discussions on the limitations and broader societal impacts of iReVa are not included in the paper. I have some questions about the application scope of the proposed method. Please see the questions below.

Questions
1.	Could iReVa lead to a dramatically increasing number of parameters? Let’s see if there are millions of knowledge for editing, how can you potentially insert all the knowledge into LMs with iReVa? 
2.	After you change a piece of knowledge, can the reasoning still be conducted for the edited knowledge? For example, if we have edited the president of America, could some reasoning questions like ``Who is the wife of the president of America” also be resolved with the new knowledge?
3.	Typo: ``evident’’ in line 6 should be ``evidence’’. Please check.

Limitations:
No, the author should discuss the limitations of the proposed method such as the application scope, the potential risks, and future improvement to indicate how robust the results are to violate the
assumption. I would like the author to add such information during the rebuttal.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses the tracable sequential model editing challenge by plugging in additional model components to a transformer MLP blocks. The proposed approach adds additional model components for each edit, allowing for traceability for each edit.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
•	The results indicate that it is a strong approach compared to relevant literature and its performance is relatively stable when scaling it to thousands of edits.
•	The approach allows for ""separability of each edit"" which in turn allows for additional operations such as edit updation or deletion as showcased in the Edit withdrawal experiment. 
•	The edit withdrawal experiment is both unique and intriguing, as the concept of removing edits appears to be a novel area of exploration.

Weaknesses:
•	The overall approach does not appear to be novel, as it closely resembles T-Patcher. Both iReVa and T-Patcher rely on inserting neurons for editing and using neuron activations to control when to run the patch/adopter. Furthermore, analysis of the editing approach across different layers reveals the same pattern as discussed in the T-Patcher paper which involves adding additional model components in the final layer for optimal results.

•	Experiments with T-Patcher are missing from the comparisons to the existing methods section. Given its similarity, T-Patcher should be included for comparison.

•	Although T-Patcher performs editing in batches, it still uses a single patch of neurons for each edit, making its editing similarly traceable. Thus, the paper's claim of a ""stronger capacity for carrying traceable edits"" seems unfounded.

•	The Edit Withdrawal Test section is hard to understand. How exactly was the experiment conducted? Were all edits removed or only a limited set? Detailed experimentations for this section are needed as it is the only use case of traceability explored in the paper.

•	Editing techniques that rely on code books with playback vectors e.g. GRACE would allow for edits to be removed. The authors should make it clear that the withdrawal test is not possible for the editing techniques that they have chosen for comparison.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces iReVa, a novel method for model editing that explicitly initializes and retrofits key-value pairs into MLP blocks of transformer models to perform CRUD (Create, Read, Update, Delete) operations on LMs. iReVa aims to update knowledge in LMs without damaging irrelevant knowledge, offering better interpretability and traceability compared to existing methods. The method is validated through experiments on GPT series models, showing significant improvements in edit success and generalization without affecting specificity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Provision of the first attempt at conducting knowledge withdrawal tests for model editing methods.

The paper includes a comprehensive analysis of iReVa's performance, including knowledge withdrawal tests and generalization tests.

iReVa's approach to model editing is innovative, focusing on retrofitting key-value adaptors into MLP blocks for traceable model editing

Weaknesses:
This paper could benefit from a more detailed comparison with other model editing methods, especially those focusing on lifelong learning and continual editing [1][2].

It does not discuss the computational efficiency of iReVa in terms of inference time or memory, which is crucial for real-world applications.

The reliance on the hypothesis that factual knowledge is stored in MLP blocks may be limiting [3], and the authors could explore the broader implications of this assumption.

The method's applicability to other types of tasks, such as erasing hallucinations, is not validated.

There is a noticeable absence of experimental validation on other recent and updated models such as GPT-J (used by ROME etc.), LLaMA.

The technical novelty of iReVa is somewhat limited, as it builds upon existing concepts like MEMIT [4] and key-value memory structures in MLPs [2].

The absence of a strategy for selecting the adaptor layer may hinder the method's rapid migration and application to various language models。

Equation 3 requires clarification, why 'i' and 'o' in Equation 3 are both passed through SELF_ATTEN again?

References

[1] Aging with GRACE: Lifelong Model Editing with Discrete Key-Value Adaptors, Hartvigsen et al,
Neurips 2023.

[2] Transformer-Patcher: One Mistake worth One Neuron, Huang et al, ICLR 2023.

[3] What does the Knowledge Neuron Thesis Have to do with Knowledge? Niu et al, ICLR 2024

[4] Mass-Editing Memory in a Transformer, Meng et al, ICLR 2023.

Limitations:
No

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel method called iReVa for knowledge editing. iReVa initializes and retrofits key-value pairs into MLP blocks to create a new mapping of knowledge without affecting related information. Compared to existing methods, iReVa offers better interpretability and a stronger ability to make traceable edits.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The proposed methods demonstrate great performance compared to other baselines under the batch editing scenarios.

Weaknesses:
1. The color in the figure is not obvious to discriminate between the original knowledge neurons and new knowledge neurons.
2. The computation of the proposed method is similar to T-Patcher, I'm curious about the difference between them. The proposed methods are designed to tackle the batch edit, but it seems it still needs to add one neuron for each example.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
uRnTYPkF3V;"REVIEW 
Summary:
This paper studies the problem of sequential probability assignment under logarithmic loss with context and assumes that both the contexts and labels are generated adversarially. The main contribution is a complete characterization of the minimax regret via a new complexity measure named ""contextual Shartkov sum."" In particular, this allows the authors to recover and improve several previously known bounds with simpler proofs. The authors further demonstrate that such minimax regret can be achieved via a contextual normalized maximum likelihood predictor.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
Sequential probability assignment is a fundamental problem that finds many applications, such as universal compression, portfolio optimization, interactive decision making, and even the famous next-token prediction in LLMs. This is a clear significant result, which provides the first precise minimax regret of adversarial sequential probability assignment under log-loss and provides an explicit algorithm achieving such regrets. Given the importance and foundational nature of the problem, I would expect the paper to have a broad impact on the community.

Although most of the proof ideas, such as the switching order of inf and sup, follow from prior literature, the truly novel technique that bypassed the difficulties faced by prior results is Lemma A.3. This allows the authors to characterize the minimax value directly (leveraging properties of log-loss) without resorting to the offset Rademacher complexity as in [RS15]. I also find the optimization problem in line 206 to be of independent interest. Although the paper is fairly preliminary (see point 4 in the Weakness section), it would serve as a stepping stone for important future research.

Overall, this is a significant result that would inspire follow-up research. Therefore, I recommend a ""Strong Accept.""

Weaknesses:
I do not see any significant weaknesses in the paper. However, I would like to outline a few minor remarks as follows:

1. I believe the characterization in the current paper is similar in spirit to that of [BHS23], who provides a general reduction from the smoothed adversarial to the fixed design regret that also leverages the minimax switches. (Although I believe the techniques developed in the current paper could also recover this result.)

2. I think Algorithm 1 can be fit into the relaxation-based algorithmic framework introduced by [RSS12]. (This does not mean the current algorithm is not novel.)

3. It would be good if the authors could provide a short argument to explain why the P^* in line 507 attains the optimal of line 506. (This is clear to experts but is not immediately obvious for non-expert readers.)

4. Section 3.2 could have been developed in much more detail, for example, by providing some (simple) examples to demonstrate the non-triviality. See also the Question section for more concrete examples.

[BHS23] Bhatt, A., Haghtalab, N., and Shetty, A. (2023). Smoothed analysis of sequential probability assignment. NeurIPS 2023.

[RSS12] Rakhlin, S., Shamir, O., & Sridharan, K. (2012). Relax and randomize: From value to algorithms. NeurIPS 2012.

Limitations:
N/A

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors explore the fundamental problem of sequential probability assignments, specifically focusing on log-loss online learning with an arbitrary, possibly non-parametric hypothesis class. They introduce a new complexity measure and demonstrate that the worst-case contextual Shtarkov sum equals minimal regret. With this, they derive a minimal optimal strategy, called cNML, which extends beyond binary labels.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper successfully generalizes classical results in sequential probability assignment to contexts that involve multiary labels and sequential hypothesis classes. 
Another strength of this paper is its robust characterization of minimax regret and optimal prediction strategies holds for arbitrary hypothesis classes. This is particularly good as it relaxes the assumptions often required in earlier works.

Weaknesses:
The paper does not include experimental results to support the theoretical findings.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this work, the authors consider the problem of sequential probability assignment, also known as online learning with the log-loss.  While earlier results have shown that in the context-free or transductive settings, the Shtarkov complexity is minimax optimal (and attained with normalized maximum likelihood), the contextual analogue was bounded only in terms of sequential covering numbers.  The authors propose a new complexity measure, the contextual Shtarkov sum, that is precisely equal to the minimax regret.  The authors then bound this complexity in terms of the sequential covering numbers in a way that improves marginally on prior work.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper discusses an important problem (that of sequential probability assignment) and provides a new bound on the minimax regret.  This new bound depends on a new notion of complexity that acts as the analogue of the sequential Rademacher complexity for log loss.  The paper is presented well and explains its technical details clearly.

Weaknesses:
The paper's contributions are somewhat incremental.  The Shtarkov complexity is well-known for case of transductive learning and the introduction of contexts uses techniques introduced and used in many earlier works on sequential Rademacher complexity.  This is not helped by the fact that the Shtarkov complexity itself is almost tautalogically equal to the minimax regret.  This weakness would be somewhat mitigated if the new complexity measure led to improved bounds on the minimax regret, but the claimed improvement over BFR20 is only at the level of a constant as, asymptotically as $\alpha \downarrow 0$, the upper bound still scales like $\inf_\alpha \{ 2 T \alpha + \mathcal H_\infty(\mathcal F, \alpha, T) \}$.  Furthermore, on the question of depth of technical contribution, it is not clear to me what is really novel.

Limitations:
The discussion of limitations is adequate, although mention of the computational challenges of their algorithm even in the case of convex classes (due to the necessity of optimizing over the space of trees, which is exponentially large) would be good to include.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies minimax regret in the online learning setup. The paper extends prior works to nonbinary labels and proves improved bounds on the regret. The analysis is through studying the Shtarkov sum that characterizes the minimax regret.  The authors develop a data-dependent variant of the contextual Shtarkov sum and introduce a variant of the Normalized Maximum Likelihood. Moreover, the paper takes an information-theoretic approach and studies sequential $\ell_\infty$ entropy terms to give upper bounds on regret.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper is solid and presents a nice mathematical framework to prove bounds on the regret in online learning. The frameworks allows one to study a broader class of online learning problems including non binary labels. In addition it makes the proof a bit nicer with slightly improved bounds on the regret. Overall I liked the technical contributions of the paper as a framework to study various online learning problems.

Weaknesses:
On the negative, side it seems that the novelty of the work is somewhat limited, given the cited prior works. It looks like that the authors extend the Sharkov sum and its analysis to the contextual case.

The presentation of the paper could be better. It starts with elementary definitions and explanations of the problem but jumps to the technical parts without a smooth transition. 

Moreover, the seemed that the paper is dry and lacks giving enough motivation and explanation.  For instance, the authors introduce multiary trees without explaining the high-level picture. It is not clear why and how this concept is used in the proofs. As another example, it is not clear at the beginning why the proposed algorithm (cNML) is useful.  Perhaps a discussion on this matter helps the reader kept interested in the paper.

Limitations:
Nothing significant.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
2TktDpGqNM;"REVIEW 
Summary:
The authors introduce a new metric AUGRC for evaluating classifiers under the Selective Classification framework, whereas the classifier
has an option to reject low-confidence predictions. The authors introduce desirable properties for evaluating the Selection Classifiers, and show that the new metric has all those properties, unlike any of the currently used metrics. Experiments show that AUGRC produces significanly different rankings of Confidence Scoring Functions compared with currently used metrics.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The authors address relevant question and make useful contribution. The paper is well written, clear and easy to read. The experimental analyses are, to my judgement, sound. The code is provided.

Weaknesses:
Fig. 4: the risk vs. coverage curves for AURC vs. AUGRC do not look that different. Sure, there is non-monotonicity in AURC, but that is
in the low-coverage region, which presumably is not of practical interest. The rest looks monotonic and fairly similar. It is also true
that the two metrics suggest different CSFs, but again, they aren't that different. To be clear, I still think AUGRC is favorable, just
saying that the difference appears modest.

I think the key conclusion, which is the recommendation to adopt AUGRC, is valid, however the language in the section is a bit too strong (there is ""substantial"" twice and ""significant limitations""). I recommend to tone-it-down.

Limitations:
There is no limitations section. I don't have a good idea regarding limitations of this manuscript.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents 5 requirements for multi-threshold metric for Selective Classification and a novel metric to evaluate selective classifiers called AUGRC. The proposed metric satisfies 5 requirements that are not met by current approaches. The proposed metric changes the rankings on 5 out of 6 datasets considered by the authors.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper's main strengths are:


1. the paper aims to tackle a long-standing concern in abstaining classifiers literature, i.e. how to evaluate with a single measure these classifiers
2. the theoretical derivations seem sound
3. the contribution is well framed within current literature

Weaknesses:
The main concerns of the paper are:

1. The empirical evaluation can be improved
2. The interpretability of the proposed measure is not that straightforward
3. The paper presentation can be improved. 

Regarding the empirical evaluation, I have a few remarks.
* there are some contradictory lines, which should be double-checked: for instance, in lines 268-269, it is not clear to me why you claim that AURC erroneously favors DG-RES over MCD-PE, as DG-RES's performance seems to be better than MCD-PE (accuracy-wise and ranking-wise). Similarly, it is not clear to me why in Figure 4 the authors state DG-RES is favored ""despite a lower classification performance and ranking quality"", while in Figure 4.a is reported a higher accuracy and higher $AUROC_f$ for DG-RES;
* second, the authors never specify whether they correct for multiple outcomes testing (e.g. using Bonferroni correction). I think since the authors are performing multiple pairwise ranking tests, they also have to account for this. 

Regarding the interpretability requirement, the authors claim (correctly) that the AUROC can be interpreted as ""the probability of a positive sample having a higher score than a negative one"". I did not fully grasp what is the straigthforward interpretation of the AUGRC score in this context (lines 206-208).

Regarding paper presentation, the paper heavily relies on acronyms. I would personally reconsider this choice to improve the overall readability.

Limitations:
The authors do not directly discuss limitations of their proposed approach in the main paper. For instance, I think a brief discussion regarding the computational time required to compute AURGC should be included in the main paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose 5 requirements that should be satisfied by selective classification (SC) metrics such that they can be successfully used to rank SC models for a task. They then propose a new metric, called AUGRC, which is shown to satisfy all 5 requirements. Finally, the authors show empirically that their method performs better rankings thank previous metrics.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The paper is well-motivated, clearly written, and has an excellent coverage of previous works.
2. The 5 requirements were well-chosen and a metric that satisfies them is likely to be a good choice for SC ranking.
3. The metric is simple, yet effective, and the maths seems sound.
4. Results are convincing and statistically analysed; the toy dataset was didactic.
5. There's enough information in the main body of the paper to fully understand the experiments and corresponding results, i.e. it's not necessary to read the appendix, even if further details are available there.

Weaknesses:
1. Limited discussion of future works.
2. NLL and Brier Score are listed among the multi-threshold metrics, but there's no thresholding involved in their evaluation. In fact, it should be possible to use them as risk measures, which would cover more for than just 0/1 loss, especially where probabilities are important for decision making.

Limitations:
Yes.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper tackles the problem of Selective Classification (SC). The authors show problem with the existing metrics used in evaluating SC and propose to use the Area under the Generalized Risk Coverage curve (AUGRC). Empirical results provide useful insights about the effect of using this metric and shows how this changes  the relative ordering of state-of-the art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The problem tacked is very important, especially for the safe deployment of ML models which requires the knowledge of when not to trust the model.
- The proposed metric is well-motivated.
- The experiments are extensive.
- It is important for the community to know about this work as this changes the perception of SC and the best method to use.

Weaknesses:
- It is important to add the details of the methods in the main paper. For example, DeepGamblers (DG) is referred to as DG in the main paper and this abbreviation is only explained in the appendix. Similarly for other baselines.
- Although the experiments are extensive, it is good to include the recent state-of the art SC methods such as Feng et al (2023) to understand which method is the best to use.

References:
Feng et al: Towards Better Selective Classification

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
AM1znjeo9l;"REVIEW 
Summary:
The paper studies the effect of rescaling symmetry in SGD and shows SGD tends to favor solutions with balanced gradient noises. The authors then derive an exact solution of the stationary distribution of a toy model trained by SGD.  The derived solution shed lights on problems observed in deep learning such as fluctuation inversion and edge of stability.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper contributes to the understanding of SGD properties. The noise balance theorem is novel and important. The analytical solution as well as the interpretation is interesting and insightful.

Weaknesses:
The results of the paper are interesting and important, the writing needs refinement to improve clarity and precision. The conditions under which the results hold is sometimes omitted, leading to confusion. The language should also be made more precise.

Minor points:
1.	The first paragraph in related works appears to overstate the novelty of the results. Specifically, our result is the first to derive an exact solution to the stationary distribution of SGD without any approximation. (Line 55-56) This is a strong claim, but it seems inaccurate. There are previous results showing exact solution of stationary distribution of SGD (e. g. Liu Ziyin 2021). Corollary I.1 in arXiv:2306.04251 (2023) also states the stationary distribution on a deep learning setup similar to the D=1 model discussed in this paper. Also, the solution given in the paper is for a specific model. These should be made clear.
2.	It seems that eq. 15 takes D=1, which has not been stated and thus is confusing.
3.	It is unclear why the left figure of Fig. 5 has only two theory lines instead of three.

Major points:
1.	The related works on symmetry and SGD dynamics are insufficient. There are a few related works that are missing, e. g. arXiv:2309.16932 (2023).
2.	The paper has not discussed convergence to the stationary distribution. The authors seem to assume convergence to stationary distribution and use interchangeably the SGD properties and the stationary solution properties (e. g. line 97-98). However, the properties of SGD can be very different from the properties of stationary solutions unless convergence to the stationary solutions is guaranteed. The authors should clarify this.
3.	The authors fail to discuss uniqueness of the stationary solutions. For example, it is unclear to me why eq (3) is a necessary and efficient condition for stationarity. Eq (3) is a critical result in the paper, and it would be better to make it a theorem or corollary. However, since eq (2) cannot be interpreted as a deterministic ODE. The unique condition for a stationary distribution should be justified, especially considering that C1 and C2 are not constant but depend on u and w.
4.	The equivalence of SGD bias and weight decay is not rigorous. (line 155-158) The C0 term is not constant but depends on u and w, while the weight decay rate is constant.

Limitations:
The authors have listed limitations at the end. The major limitations are the simplicity of the model and lack of experiments on deep neural networks.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
For ReLU networks trained by gradient flows, it is classical that a type of Minkowski inner product between the coefficients of consecutive layers is preserved. The authors demonstrate a monotonicity of the same quantity for stochastic gradient descent in continuous time. They use this to study the invariant distribution of parameters trained by (continuous time) SGD.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The topic is well-chosen and the results - if correct - are very interesting.

Weaknesses:
* In its current form, I find the article a bit unpolished and the results not easy to access. Many questions remained unanswered when I tried reading the article (see questions).

* Important quantities are defined throughout the plain text. I understand that reading as a reviewer under time pressure is different from normal reading, but for instance in Theorem 3.1, I would have hoped for a more self-contained statement on relations and properties $L, C, \ell$ and the distribution of $x$ have to satisfy. As far as I can tell, the statement is fairly general and not specific to machine learning.

* I have serious doubts about Theorem 3.1. It is derived in Appendix A from Itô's Lemma without the diffusion term. This is valid *in expectation over $\theta$*, but not pointwise in $\theta$. Pointwise in $\theta$, there should be white noise in the 'time derivatives', i.e. the ODE identity should be written as an SDE. In the proof, equations (27) and (28) appear to be wrong.

* The authors do not pay any attention to whether solutions to the evolution equations exist (or are unique). Problems with regularity can sometimes be alleviated if the distribution in $x$ is sufficiently regular, but I would appreciate a short discussion.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tries to analyze the specific features that carry the noise of SGD (through a continuous model). The authors show that there is a certain 'law of balance' across the layers when some invariance is assumed. Going further, they derive a toy model to push their study, showing that there is an analytic stationary solution to it. They finally propose a phenomenology related to the role of the noise of SGD when analyzing this precise stationary distribution.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea that a conservation law for the gradient flow implies an asymptotic balancedness condition for the stochastic flow is a good and striking idea.

The one-dimensional examples that are given in the text are very pleasant to follow and they are good exercices to display the ability of the stochastic flow to diverge from the gradient flow.

The example given in Eq.(13) is thoroughly analyzed.

Weaknesses:
The paper present the following weaknesses:

- The law of balance is an interesting phenomenon, yet considering it with a closer look, it seems that not much can be said generally and that one has to understand it case by case. In one dimension, sure, it is possible to conclude that balancedness will occur at exponential speed, yet in dimension more than $2$, it seems impossible to predict it surely.

- I have to say that I was a bit bothered by the general overselling of the paper : 
     - As said before the law of balance is truly valid asymptotically in one-dimension
     - The stationary distribution that the authors claim to be the first to derive is for a very specific model, which is not standard and does not resemble a diagonal network! 
     - The fact that the stationary distribution can be computed is also very inherent to $1d$ calculation and is simply a recognition of a Pearson diffusion that already made in way in ML (at least in https://arxiv.org/pdf/2402.01382 and https://arxiv.org/pdf/2407.02322).

Minor typos/flaws:

- l.41: Fokker Planck is not inherently high-dimensional
- l.44: Go to the line for new paragraph 
- l.165: The law of balance is not strictly applicable here since $\ell$ is not scale invariant because of the regularization.
- Section **4.1 Depth - 0**: I think that $\Delta > 0$ is not currently the ""most practical example"" since it corresponds to a underparametrized model.

Limitations:
As said before, all conclusion are drawn for models that live intrinsically in one dimension.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
YZWvf58dBS;"REVIEW 
Summary:
This paper introduces the TransLLM framework to transform English-centric chat LLMs to non-English languages, addressing the challenges of transferring advanced abilities without supervised data and preventing catastrophic forgetting of original knowledge. Key contributions include using the Translation Chain-of-Thought (TCOT) to divide the transfer process into sub-tasks, employing Low-Rank Adaptation (LoRA) and Recovery Knowledge Distillation (KD) to maintain original LLM parameters and recover knowledge. TransLLM's effectiveness is demonstrated through experiments transforming LLaMA-2-chat-7B to Thai, where it outperformed strong baselines and ChatGPT in multi-turn conversations and safety benchmarks, highlighting significant improvements in helpfulness and safety without extensive supervised data. This framework offers a solid foundation for developing safe and useful non-English LLMs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The introduction of TransLLM, which combines Translation Chain-of-Thought (TCOT) and recovery knowledge distillation, provides an effective method for transforming English-centric LLMs to non-English languages. This approach addresses both the transfer of advanced abilities and the prevention of catastrophic forgetting.
2. The method shows notable improvements in rejecting harmful queries and maintaining human preference alignment, as evidenced by outperforming GPT-4 and ChatGPT on the safety benchmark AdvBench, highlighting the robustness of the model in safety-critical applications.

Weaknesses:
1. The experiments are primarily conducted on transforming LLaMA-2-chat-7B to Thai, which may limit the generalizability of the findings to other non-English languages and other models. Further validations on other models (not necessarily bigger than the current one) or other size of Llama-2 would have strengthened the paper.
2. While the proposed method exhibits promising results in MT-bench and AlpacaEval, it is not tested on other traditional NLU benchmarks, just like MMLU in English. Incorporating more diverse evaluation benchmarks would provide a more comprehensive assessment of the model's performance across various language tasks.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a transfer pipeline, TransLLM, which uses a translation chain-of-thought (TCOT) to adapt English-centric large language models (LLMs) to low-resource languages. This pipeline consists of pre-training and supervised fine-tuning (SFT) phases. During the pre-training stage, the authors select monolingual data in the target language and translation parallel data for language modeling. This pipeline also uses an external translation model to construct TCOT dialogue data, which is used in the SFT phase. In the SFT phase, to avoid the catastrophic forgetting problem caused by continual learning, the authors adopted the LoRA PEFT method and further proposed the recovery Knowledge Distillation (KD) method. Specfically, the recovery KD method mixes responses generated by the original model during the SFT-based transfer phase. The effectiveness of this pipeline was demonstrated by successfully transferring the Llama-2-7B model from English to Thai. Compared to other baseline models based on open-source LLMs, the transferred model exhibited great multi-turn dialogue capabilities and safety on the Thai MT-bench and AdvBench benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. **Originality**: Compared to other machine translation-based transfer methods in the community, the proposed pipeline introduces the concept of Translation chain-of-thought and specifically addresses the catastrophic forgetting problem that might arise from SFT-based transfer methods, proposing effective solutions.
2. **Evaluation Protocol**: I appreciate the efforts made by the authors in the experimental section to ensure the effectiveness of evaluations for low-resource language (Thai). The authors employed professionals for evaluation and also validated the agreement between GPT-4's automatic evaluation and human evaluation on the MT-bench. Additionally, they used human translation in constructing some of the test data.
3. **Significance**: Currently, training data and resources for LLMs are predominantly English-centric. The proposed method helps build strong chat LLMs for low-resource languages and minority groups.

Weaknesses:
1. **Flexibility of Methodology**: Although the proposed method reduces the dependency on instruction-following data in the target language, it still relies on parallel corpora and external translation models for data construction (Translation pre-training data, TCOT data). If high-quality parallel corpora or models are not available, the proposed method might be infeasible. For example, even commercial translation systems cannot support translations for some endangered languages or dialects.
2. **Scope**: While the method proposed in this paper might be extendable to other low-resource languages, the authors only validated it on Thai, which lacks empirical evidence for the generality of proposed pipeline. I also noticed that the authors emphasized ""non-English"" in the title, but a broader range of non-English languages still requires exploration.
3. **Effectiveness**: The authors tested the method on Llama-2-7B. Experiments on other model series and larger models could further support the effectiveness of proposed pipeline.
4. **Quality of MT data**: The authors used Google Translate for translating TCOT and AdvBench data. Although this is a commonly used commercial system, it would be better to supplement the evaluation and report on the quality of these data translations to explore their impact. In the absence of reference translations, quality estimation methods like CometKiwi [1] and TransQuest [2] could be used.
5. **Typos**:
    - Line 45: instruct tuning -> instruction tuning

References

[1] Ricardo Rei, Marcos Treviso, Nuno M. Guerreiro, Chrysoula Zerva, Ana C Farinha, Christine Maroti, José G. C. de Souza, Taisiya Glushkova, Duarte Alves, Luisa Coheur, Alon Lavie, and André F. T. Martins. 2022. CometKiwi: IST-Unbabel 2022 Submission for the Quality Estimation Shared Task. In Proceedings of the Seventh Conference on Machine Translation (WMT), pages 634–645, Abu Dhabi, United Arab Emirates (Hybrid). Association for Computational Linguistics.

[2] Tharindu Ranasinghe, Constantin Orasan, and Ruslan Mitkov. 2020. TransQuest: Translation Quality Estimation with Cross-lingual Transformers. In Proceedings of the 28th International Conference on Computational Linguistics, pages 5070–5081, Barcelona, Spain (Online). International Committee on Computational Linguistics.

Limitations:
The authors discussed some limitations after the conclusion.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this work, the authors present a method for transforming a chat-based English LLM to Non-English (Thai is the only language experimented with) based on a series of steps that teach the LLM to take in a non-English query and respond in non-English for that query. The methods presented is referred to as TransLLM pipeline which comprises of extending the based model vocab and finetuning with LoRA in multiple stages -- comprising of target language pretraining (on monolingual target language data), translation pretraining and transfer finetuning. The experiments are done on LLaMA2-Chat-7B with Thai as the target language and show the model performs well on both translation into and out of Thai and obtains good performance wrt baselines on MT-Bench & Alpaca-Eval.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. The paper presents a simple and scalable solution pipeline to adapt chat LLMs to new languages.

Weaknesses:
1. The experiments are done on only one LLM and on only one language (Thai). This severely constrains the extend to which the results could be verified.
2. The novelty of the proposed method is very thin and very limited analysis is done to motivate that novelty.

Limitations:
Limitations have been adequately addressed.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on a scenario as transforming a English-centric **chat** large language model to a non-English chat large language model (or not just en-centric). The authors want to address the catastrophic forgetting problem where further tuning on the original En-chat LLM without reusing their original SFT data will hurt their original chat abilities mainly in English. In terms of this, they introduce several techniques, including translation chain-of-thought, low-rank adaptation, as well as recovery KD, and they claim that with only single-turn Thai data, they can successfully transform English LLama2-chat-7B to Thai, while the performance in MTBench and some others remain competitive.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors propose a pipeline to tune the chat English language model to a non-english language model. The pipeline looks interesting and the performance looks good in MTBench, Th-translated MTBench, etc.

Weaknesses:
1. To be honest, I am not sure whether this paper has good starting point. They focus on a scenario where you want to further fine-tune the chat model in English to a chat model in other languages. Could you provide more concrete application scenarios in your introduction? I am not convinced because typically, in practice, people will use all the data available to tune a base model, and usually, this renders the best downstream task performance, or at least, the robustness defined as the average downstream task performance of a particular tuned chat LLM. Why cannot we follow this paradigm?
2. I suggest the authors to further polish the paper abstract. I am getting confused several times, for example, in your statement ``Transforming English-centric LLMs to non-English has been identified as an effective and resource-efficient method'', why can you simply say this is a resource-efficient method? As far as I know, if you just use knowledge distillation by distilling the data synthesized from a stronger model in your target languages, you might still need to use stronger model, usually those closed-source models, e.g., ChatGPT, this is still expensive, right? Besides, as even the knowledge distillation, in the SFT stage, you have to mix all the data to do the training as well. So it is still expensive.

Limitations:
See the weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
7XkwzaPMvX;"REVIEW 
Summary:
This paper addresses the manipulation of explanations in AI-assisted decision-making, presenting a comprehensive study that explores how human behavior models can be used to adjust explanations provided by AI systems. The aim is to understand if these manipulations can nudge decision-makers towards specific outcomes, which might be either beneficial or malicious.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper's focus on quantitatively modeling human behavior to manipulate AI explanations is highly novel and impactful. This approach not only extends the current understanding of AI-human interactions but also opens new avenues for both enhancing and securing AI-assisted decision-making systems.

2.  The experiments conducted across various decision-making tasks provide a robust validation of the proposed models. The inclusion of both adversarial and benign manipulations allows for a balanced view of the potential impacts of this technology.

Weaknesses:
1. The scope of the experiments is restricted to tasks such as census and recidivism prediction, which may not adequately represent the complexities and stakes of decision-making environments in sectors like healthcare or finance. Expanding the range of tasks to include high-stakes decision-making could improve the generalizability of the results.
2. The behavior model does not account for the inherent variability and noise in human decision-making, potentially oversimplifying the complexities of real-world human-AI interactions.
2. The data and code are not immediately available for replication and further study, which could hinder the verification of the results and the advancement of the research.

Limitations:
1. Score-based explanations may not be adequate. Incorporating Large Language Models could introduce more versatile and comprehensive explanations.
2. Include more diverse datasets, possibly extending to more complex data types like images or videos, to test the robustness of the behavior models across different AI applications.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes to train a computational model to predict how humans would respond to model predictions and their explanations to make the final decision. Using this model, the authors then demonstrate that it could be used to manipulate explanations for both good and bad purposes, specifically to steer human predictions toward the label that is likely to be correct, or make human decisions intentionally biased. Furthermore, the humans have very little idea that the explanations have been manipulated in both cases.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper focuses on an important topic: the role of explanations in human decision making. 

The presentation is generally clear and the writing structure is good. 

Extensive experiments are conducted to demonstrate the main arguments of the paper.

Weaknesses:
1. I noticed that the user study payment is only \\$1.2 base pay with a potential bonus of \\$1.0 (Sec. 4.2). The study consists of a tutorial, 5 predictions without AI assistance, 15 predictions with AI prediction and explanations, and an exit survey. Given that this study is deployed to US-based participants, the compensation is extremely meager: even at the federal minimum wage of \\$7.25 per hour, the \\$1.2 base pay would be equivalent to 10 min of work, which, given the user interface of Fig. A.1 and A.2, is extremely unlikely to be enough for all the tasks. Thus, I have serious concern about the ethics of the study, despite its IRB approval, and thus decide to request additional ethics reviewers for this paper. 

2. The use of computational model for human behavior is not novel, and the applications to human decision manipulation seems quite straightforward. 

3. Furthermore, it would be helpful have some additional analysis on the learned computation model itself, maybe with the help of various interpretability tools. For example, when is the human prediction most likely influenced by the provided explanation, and in what way? These quantitative and qualitative insights could be helpful to understand human behaviors better. 

4. For the ""benign"" use case of improving human model prediction, the authors looked at cases ""when the AI model decision is likely incorrect"" (Line 285). How is this ""likely incorrect"" determined? And if we know when the AI prediction is likely incorrect, why can't we simply fix/patch the AI model directly? The authors demonstrated that the human performance is better after explanation manipulation, compared to the original explanation in Fig. 3, but is the human prediction better than the ""fixed AI model"" performance?

Limitations:
Yes.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel method that manipulates human decision-making by manipulating AI explanations in human-AI interaction scenarios. By utilizing human behavior models and minimizing the cross-entropy function between human and AI agreement with constraint to generating the same AI recommendation outcomes, the authors investigate this method for adversarial and benign purposes. The results show that the manipulation in AI explanations significantly negatively affects human decisions in the four tasks with adversarial purpose and enhances human decision accuracy, over-reliances, and diminishes under-reliances. The paper also discusses how the human perception to the AI explanations varies with the manipulation. Overall, it provides a novel and practical way to intervene in human decision-making in human-AI interactions.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper formulates the AI explanation manipulation problems, as optimization problems to minimize human-AI decision disagreement, with constraint to keep the original AI outcomes. This can indeed be applied in both positive or negative ways (in respective to adversarial and benign purposes in the paper). The authors discuss both sides of the methods and provide an implication for society about such methods.

- The paper selects multiple AI explanation baselines and collects human data on multiple tasks, enhancing the robustness of the manipulation method and broadening potential social impacts in various scenarios. The paper also collects human perception, providing an extra view from human participants in subjective feelings.

Weaknesses:
- Only particular features are selected in each task. This may weaken the effectiveness of the manipulation. For example, in the census task, the paper selects 'gender' as the dimension of manipulation; while 'degree of education' or 'ages' can possibly be biased and effect human decision-making. Thus, the authors may need to address such representativeness (or probably other dimensions do not exhibit significant effects in manipulations).

Limitations:
- The limitation of this paper as discussed, is only certain features in each task are selected to manipulate. It would reveal more about the societal impact if comprehensive features are considered. Moreover, the consideration of interactions between features is also helpful, though this does require a larger dataset.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors show that by modeling a human decision maker they can manipulate the provided information in ways that reliable influence their decisions towards even non-benign outcomes.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
# originality

Manipulating Mturk works is a well studied area of research, but this is a unique approach and highlights the limitations of interpretability research using similar techniques.

# quality

The results are statistically significant and done across a range of tasks which strengthens their claims. 

# clarity

Most of the plots are readable and I think I was mostly able to understand the results after reading a few time.

# significance

This is an interesting result, presenting an example of ""attacking"" these score based systems via manipulating the interpretably metrics is an important result for the XAI literature.

Weaknesses:
# originality

Manipulating workers on digital platforms is a well studied area.

# quality

Reading the plots (fig 3) some of these results look to be supporting the null-hypothesis (all interventions have the same effect). Having a table giving the numbers and more details on the statistical tests would make the quality of the results much easier to judge. The results also are weak, in part this is due to a small sample size.

# clarity

If found the plots difficult to read either both being too small and the lack of numbers makes analyzing them tricky. Could the authors include the results as a table in the appendix with the error ranges clearly laid out so we don't have to eyeball tiny error bars.

I also had to refer to the appendix to understand the experimental procedure, making it clearer what exactly is happening in the experiments would greatly aid the readability of this paper.

 # significance

Looking at figure B.2 it looks like the manipulation is mostly making the difference much more extreme and noticeable, this suggests these results could simply be due to the differences in the visual presentation between interventions.

Limitations:
This is an area where ethics should be given additional scrutiny, but I do not see anything alarming in the paper, and the authors take the correct tone in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
F8wKoSFSaA;"REVIEW 
Summary:
In this paper, a unified single-loop zeroth-order gradient descent extragradient ascent (ZO-GDEGA) algorithm to solve the nonconvex-concave minimax problem faster and more robustly. The theoretical analysis is provided to guarantee an overall complexity of $O(\epsilon^{-6})$. The experimental results on the data poisoning attack task and the AUC maximization task are shown to validate the practical performance of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. In this paper, the authors propose a zeroth-order algorithm that achieves lower complexity under the nonconvex-concave condition.
2. The first convergence analysis of stochastic zeroth-order algorithm under the nonconvex-concave condition is provided.
3. For nonconvex strongly-concave problems, the complexity with respect to the condition number is improved.

Weaknesses:
1. In section 2 related work, the complexity of first-order minimax algorithms is not discussed. To my understanding, the error of the zeroth-order gradient can be bounded by parameters $\mu_1$ and $\mu_2$ that are set as small as $O(\epsilon)$. Therefore, the analysis should not change a lot based on the analysis of the first-order counterpart, which probably undermines the novelty and contribution.
2. In ref [18] (Huang et al 2022), variance reduction is used to improve the complexity with respect to $\epsilon$. The contribution of this paper will be further stronger if this part is also considered.
3. The figures in the experiments section is too small and some curves are covered by the legends. I understand the space is limited but maybe some sections could be rearranged to the Appendix.

Limitations:
I did not see any negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
They design a new unified ZO gradient descent extragradient ascent (ZO-GDEGA) algorithm, which reduces the overall complexity to find an ε-stationary point of the function ψ for nonconvex-concave (NC-C) problems. ZO-GDEGA is the first ZO algorithm with complexity guarantees to solve stochastic NC-C problems.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The theoretical section is very thorough and comprehensive.

Weaknesses:
(1)The paper lacks detailed link of code implementation.
(2)While the paper effectively addresses the NC-C problem from a theoretical perspective, it should also provide more experimental details and demonstrate that the NC-C problem exists within the experimental models.

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies zeroth-order methods for nonconvex-(strongly)-concave minimax optimization. The achieved rates improve previous results and tolerate much larger choice of the smoothing parameters. The proposed methods also perform well for some empirical tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Minimax optimization is an important problem that has many applications in machine learning and related areas. In many settings, gradients are hard to estimate or impossible to obtain, which motivates the study of zeroth-order methods. Moreover, nonconvex-concave minimax is itself a class of nonconvex nonsmooth optimization, which is considered as challenge problems in the related literature. The paper proposes new algorithms with improved convergence results compared with previous work for this challenge class of the problem.

Weaknesses:
1. I think a discussion on lower bounds can improve the understanding and position of this work. For example, lower bounds on zeroth-order methods justify the dependence on the dimension is inevitable without additional assumptions [1]. This, together with lower bounds for minimax optimization, e.g., [2], provide lower bounds for the considered problem class, which suggest the foundamental limits of this problem and whether the complexity can be further improved.

2. A discussion of the best known upper bounds for first-order methods could also help. As far as I know, the best rate for first-order nonconvex-concave minimax optimization is also $\epsilon^{-6}$ [3]. The authors can do some additional literature review and check whether my statement is correct. As the complexity of zeroth-order methods is usually d times that of first-order methods, this suggests the results in this paper match the state-of-the-arts. For nonconvex-strongly-concave minimax optimization, [3] achieves a rate of $\kappa\epsilon^{-4}$, which suggests that the upper bounds in this paper can possibly be improved.

3. Have the authors considered to use two-point estimators to construct zeroth-order gradient estiimators? In some cases, two-point estimators give better rates [4]. The paper mentions that for the NC-SC case there is no need to use $z_t$ to update $x_{t+1}$ but $y_t$ instead. However, there is $z_t$ involved in the update of $y_t$. Are there any typos in the statement of the algorithm? Also, I suggest to add dimension in the complexity stated in the abstract. Otherwise, this could be misleading.

I did not have time to carefully check every step in the proof. If the results are correct, I think they make enough contributions to the related literature. Therefore, I will keep a low confidence score.

References

[1] Optimal rates for zero-order convex optimization: The power of two function evaluations. IEEE Transactions on Information Theory, 2015.

[2] The complexity of nonconvex-strongly-concave minimax optimization. UAI, 2021.

[3] SAPD+: An Accelerated Stochastic Method for Nonconvex-Concave Minimax Problems. NeurIPS, 2022.

[4] An optimal algorithm for bandit and zero-order convex optimization with two-point feedback. JMLR, 2017.

Limitations:
See weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes zeroth-order method called ZO-GDEGA to find a near-stationary point for nonconvex-concave minimax optimization, with complexity guarantee. The proposed method is also extended to stochastic setting, being the first work on ZO method on stochastic NC-C problem. The method has weaker requirement on ZO gradient estimate, thus also has better dependency on condition number in the special case of NC-SC. Numerical results on data poisoning attack and AUC maximization show the proposed method is comparable and usually slightly better than compared baselines.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work designs a unified single-loop ZO method for NC-C minimax, with better complexity and more robust allowance on ZO gradient estimate. The proposed idea of continuous-time dynamics to assist with updates of dual variable and its related analysis is novel. Overall complexity is proposed, with solid and rigorous proof, under weak assumptions such as not requiring lipschitz continuity, more tolerant ZO gradient, which also results in good complexity in the NC-SC special case.

2. The proposed method can be extended to stochastic setting, with first-ever complexity in this case.

Weaknesses:
1. Complexity (deterministic NC-C): The work claims the $O(d\epsilon^{-6})$ complexity of the proposed method is a 'reduced' complexity, however to the best of my knowledge, this is not the best-known complexity of ZO method on NC-C minimax. Even for single-loop methods, the existing method in [43] shown by table 1 has a better $O(d\epsilon^{-4})$ complexity. Intuitively, only a complexity as good as this existing $O(d\epsilon^{-4})$ is near-optimal, because first-order methods on NC-C minimax have $O(\epsilon^{-4})$ complexity. Although assumption-wise, as this work claims, [43] has the extra assumption of 'decreasing regular parameter sequence', and also requires more accurate ZO gradient on the primal variable, but in my opinion, the extra assumption above is not too restrictive, and requiring more accurate ZO gradient would only cost a constant multiple of queries thus would not affect overall complexity. Therefore, the $O(d\epsilon^{-6})$ complexity in this work is not good enough, and is in fact worse than certain existing single-loop ZO methods.

2. Complexity (stochastic NC-C): I acknowledge this is the first-ever complexity of ZO method on stochastic NC-C minimax. However, the 
$O(d_x \epsilon^{-6} + d_y \epsilon^{-8})$ dependence is not near-optimal, since first-order methods on such problem just have $O( \epsilon^{-6})$ complexity. ** Update: typo fixed, from $O(d \epsilon^{-6})$ to $O(\epsilon^{-6})$.

3. Listed numerical results show the proposed method is similar and generally only slightly better than compared baselines. The difference is not significant in both experiments.

4. Overall, both theoretical complexities and numerical results does not surpass existing methods, thus the contribution may not be strong enough for NeurIPS standard.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, the authors establish a unified framework of zeroth-order optimization for nonconvex-concave minimax optimization problems in both deterministic and stochastic settings. This framework is based on the gradient descent-extragradient ascent algorithm. They claim that their algorithms require weaker assumptions on the zeroth-order estimator, while achieve competitive iteration complexity compared with the existing work. Besides, they provide some numerical experiments to verify the effectiveness of the proposed algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Main advantages can refer to the Summary. Besides, this paper has a good organization, which makes the readers reading easily. Numerous experiments including abundant baselines and datasets are provided to illustrate the effectiveness of the proposed algorithm.

Weaknesses:
In the stochastic setting, the authors make the bounded variance assumption on the zeroth-order estimator in Assumption 3. It seems that the assumption makes the proof very similar to first-order method. It is true that the zeroth-order methods are used when computing derivative are not possible, but does this assumption rules out the most challenging technical part in the proof?
This paper has proposed zeroth-order method that is motivated by application scenes when the objective function is not smooth enough to enable one to compute the gradient. But there is no specific examples provided that such method plays a prominent role while the first-order method is not applicable. Since there is a definite trade-off when zeroth-order method are used compared to first-order, the smooth parameter is another factor that should be considered in using zeroth-order method. So I think providing a concrete application that zeroth-order method is inevitable will help convincing the importance of the algorithm in application.

Limitations:
This paper is mostly a theoretical work, no negative societal impact may result in.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
azkuhJBZXi;"REVIEW 
Summary:
This paper develops a structured and generalized reasoning framework, CreDes, for long-range reasoning in LLMs. In the framework, the Causal Relationship Enhancement (CRE) is used to guarantee the solid causal rightness between each step of reasoning and state transition, and the Dual-End Searching (DES) approach is proposed to seek solutions by simultaneously starting from both the initial and goal states on the causal probability tree, to improve the efficiency.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper is well-structured and clearly states the problem they studied. It considers the long-range reasoning of LLMs from two aspects: the correctness from one-step reasoning (OSR) to state transition, and the efficiency of the solving process.
2. This paper transits the long-range reasoning problem of LLMs into the construction of causal probability trees from the initial and goal states and uses Dual-End Searching to improve efficiency. This is a reasonable and interesting thought.
3. The experimental results are SOTA in long-range reasoning tasks in terms of both accuracy and time efficiency.

Weaknesses:
1. The main concern is the understanding of ATE. This paper frequently uses ATE as part of the loss function and thinks the lower ATE can guarantee the solid causal rightness between each step of reasoning and state transition. However, ATE is used to measure the causal influence level between variables from the observational data, and causality does not mean rightness.
2. The DES section is not clear enough. It is suggested that more explanation be provided for the reason for the ATE as part of the loss. For example, if “B is the number of unfolded layers where the current leaf is located Ni”, what does E(A|do(B)) and E(A) mean in Formula (5)?
3. This paper needs to supplement the usage scenarios of methods, specifically in which scenario to use CreDes, in which scenario to use Cre alone, and whether Des is used separately.

Limitations:
None.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces CreDes, a framework to improve the long-range reasoning capabilities of LLMs, consisting of two main components: Causal Relationship Enhancement (CRE) and Dual-End Searching (DES). CRE is developed to reduce causal hallucinations in LLMs by strengthening the causal relationships between reasoning steps and state transitions; it uses structural causal modeling and optimizes the Average Treatment Effect (ATE) during training. DES breaks down long-range reasoning tasks into shorter segments by simultaneously searching from both the initial and goal states on a causal probability tree. The authors evaluate CreDes on Blocksworld, GSM8K, and Hanoi Tower puzzles, showing improvements in both accuracy and efficiency compared to existing methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- CreDes demonstrates significant improvements over existing methods, especially for complex tasks requiring many reasoning steps.
- The use of causal modeling concepts like ATE provides a solid theoretical foundation for the proposed approach.
- The method shows effectiveness across different types of reasoning tasks (e.g., spatial reasoning, math problems).
- CreDes enables simultaneous multi-step reasoning, potentially reducing computation time compared to sequential methods.

Weaknesses:
Major concerns: 

- The generalizability and scalability need better justification. The paper primarily tests the CreDes framework on Blocksworld, Hanoi Tower, and some mathematical reasoning tasks (GSM8K). These are relatively structured, rule-based problems that may not represent the full spectrum of reasoning challenges. In addition, the proposed method cannot be well scaled to long-range reasoning; for example, in Table 1, performance drops significantly for Blocksworld tasks beyond 8 steps, with success rates falling from 0.68 to 0.34 for 12-step problems using Llama-2-7B + CreDes. Table 3 shows even steeper declines for Hanoi Tower, with success rates dropping from 0.27 at 9 steps to just 0.07 at 13 steps for Llama-2-7B + CreDes. Notably, the authors explicitly acknowledge this limitation in Section 4.6, stating: ""The DES approach, while effective for moderate-length tasks, struggles with very long reasoning steps, leading to a decline in performance.""

- The presentation of this paper could be improved.

  -- In the problem definition, there is no explanation of the difference between training without common instructions and with common instructions.

  -- There is no detailed discussion of the differences between correlation and causation in Sec 3.2. I am confused about whether the correlation of two variables has anything to do with their distributions.

  -- While efficiency gains are mentioned, the added complexity of CRE and DES likely introduces some computational overhead, which could be further discussed.

  -- There is no analysis of the impact of the choices of hyperparameters on the methods, particularly in the CRE component.

- The proposed method lacks comparison to more recent state-of-the-art methods. The paper compares CreDes mainly to older baselines: Reasoning via Planning (RAP), Chain of Thought (CoT), and Reflexion of Thoughts (RoT). However, it doesn't evaluate against more recent advances in LLM reasoning, such as Tree of Thoughts (ToT) extensions in line 42, and the paper doesn't mention or compare to other recent works such as [a] and [b], which also address multi-step reasoning challenges. As a result, the technical contribution is not entirely clear.

[a] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, & Xinyun Chen (2024). Large Language Models as Optimizers. In The Twelfth International Conference on Learning Representations.

[b] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, & Zhiting Hu (2023). Reasoning with Language Model is Planning with World Model. In The 2023 Conference on Empirical Methods in Natural Language Processing.

Minor concerns: 

- Experiments are primarily conducted with 7B parameter models, leaving questions about scalability to larger models. How does the performance of CreDes scale with increasing model size (e.g., to 10B+ parameters)? The computational overhead may limit the framework’s scalability and applicability in real-world scenarios with limited resources.

- The approach achieves significantly lower accuracy in tasks with very strict ordering constraints, such as the Hanoi Tower problem.

- Since Blocksworld involves random steps, an analysis of the robustness of the performance may be needed.

 - More analysis/discussion on the sequential ordering of steps may be helpful. Notably, the ATE cannot recognize casual logic.

 - Some editorial issues, e.g., Line 110

Limitations:
The authors have discussed the limitations, and it is adequate to me. I do not see any potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The integration of Causal Relationship Enhancement (CRE) and Dual-End Searching (DES) mechanisms presents a novel solution to addressing causal hallucinations and large search spaces in long-range reasoning tasks. The CRE mechanism’s use of Structural Causal Modeling (SCM) and Average Treatment Effect (ATE) is  ensure causality between reasoning steps. Extensive testing on datasets such as Blocksworld, GSM8K, and Hanoi Tower demonstrates the effectiveness of the CreDes framework.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The idea seems novel and it test on well-known reasoning datasets.

Weaknesses:
The method presented in this paper evaluates ATE on LLMs, but this approach's validity hinges on the assumption that LLMs can perfectly represent the real-world environment. The very reason we criticize LLMs for their reasoning issues is because their inferences are not accurate. Estimating ATE might only bring the prediction results closer to Y while maximizing the influence of the intervention factor on Y. However, it does not necessarily mean that the intervention factor is the true cause of Y. In other words, since there is no alignment with the causal relationships in real-world scenarios, the implementation of this method does not prove that the reasoning is causally sound.

The method lacks deeper thinking. The authors just apply the concept of ATE to the Chain-of-Thought (CoT) without thorough analysis. This oversight leads to a misalignment between the experimental results and the motivation of the paper. Suppose LLMs are not a good s simulations of the real world. In that case,  performing interventions on LLMs (whether they align with the real world or their identifiability) requires sound theoretical analysis and experimental validation. The current paper lacks a deep discussion on this matter.

Limitations:
See weakness.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to improve LLMs in dealing with long-reason reasoning problems, especially the challenges of causal hallucination (inconsistency between one-step reasoning and corresponding state transition) and large search space. To tackle the first challenge, average causal effect of the one-step reasoning (treatment) on the state transition (outcome) is added to the loss function of the LLM; and for the second challenge, a dual-end (i.e. bi-directional) search approach is taken to improve efficiency. Experiments are conducted to demonstrate the effectiveness of the proposed method and its superiority over the compared existing methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. An interesting idea of formalizing the problem from the perspective of causal effect and incorporating causal effect into the loss function.
2. The adoption of a dual-end search approach for improving efficiency.
3. The motivation of the paper is well presented in general.

Weaknesses:
1. The soundness of the proposed CRE method (for dealing with the challenge of causal hallucination) is in doubt.

(a) It's not clear why the method aims to $\textbf{minimize} the absolute value of the average treatment effect (ATE) of the one-step reasoning on state transition. Assuming that the ATE can be accurately estimated, what we want here would be to maximize the ATE that can be achieved by the LLM, i.e. when the one-step reasoning is correct done will likely lead to a correct state transition.

(b) It's not clear how an unbiased estimation of the ATE can be obtained, and what assumptions are made in terms of ATE estimation.

(c) The definition or understanding of ATE is incorrect. In particular, formula (2) is wrong, and formula (5) is incorrect too. 

2. The presentation/technical quality requires improvement, including the presentation of related work. Please find below some examples:

(a) In Lines 42 to 44, it is said that the existing methods such as CoT are limited in task decomposition, but Lines 78-80 state that they can breakdown queries into manageable steps.

(b) Section 3.1 is titled as ""Problem Definition"", but it rather looks like a section on experiment setting.

(c) Lines 145-146 state that Fig. 1 shows ""we leave the reasoning path selection to be controlled by the cross-entropy loss"", but I cannot see this indicated in Fig. 2.

(d) Line 159: do(.) is an operator, specifically the do operator, rather do-calculus, although do-calculus uses this operator.

(e) Lines 159-160: the statement on the do(.) operator or do-calculus is incorrect, since an do operation on the treatment X would lead to the change of the outcome Y, especially if X is a cause of Y.

Limitations:
The authors have presented some discussions on the limitations of the proposed method. It would be better if the assumptions made could be presented more clearly and what the practical implications would be if the assumptions are violated.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new framework, CreDes, designed to enhance causal reasoning in large language models (LLMs) and solve complex, long-range reasoning problems. The framework integrates two main innovations: the Causal Relationship Enhancement (CRE) mechanism, which applies cause-effect interventions to maintain causal accuracy across reasoning steps, and the Dual-End Searching (DES) method, which approaches problem-solving by initiating searches from both the initial and goal states to efficiently navigate large search spaces. The efficacy of CreDes is demonstrated through rigorous testing on challenging datasets like Blocksworld and Hanoi Tower, where it outperforms existing state-of-the-art models in both accuracy and efficiency.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Novel approach: The paper addresses essential limitations in LLMs' reasoning capabilities for long-range tasks in a causal perspective.
2. Comprehensive evaluation: The authors test their method on multiple datasets and compare against several baselines and shows improvements in both accuracy and time efficiency.

Weaknesses:
1. Limited model sizes: The experiments are primarily conducted on 7B parameter models, which may not reflect performance on larger state-of-the-art LLMs.
2. Lack of error analysis: The paper doesn't provide a detailed analysis of the types of errors made by the model or how they differ from baseline methods.
3. Dataset validity and construction: More details is needed for the use of a custom-made Hanoi Tower dataset which potentially limiting the reproducibility and generalizability of the results.
4. Computational efficiency and scalability: As mentioned in the Limitation, the paper lacks a detailed discussion of the computational requirements and scalability of the CreDes framework.
5. Generalization to less structured tasks: The framework's effectiveness is primarily demonstrated on highly structured tasks but it's unclear about its applicability to more dynamic or open-ended reasoning scenarios.
6. Lack of statistical significance: The paper doesn't report error bars or statistical significance for its experimental results.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
hQfcrTBHeD;"REVIEW 
Summary:
This paper describes an online experiment seeking to measure how much power online search provides have in terms of impacting what content people consume. In short, the study attempts to measure the causal effect of small ranking rearrangements on click-rate for a population of web users. The authors present experimental data that can be used to estimate the expected impact that operators of search engines, recommender systems, and other ranking technologies might have on viewership of items when they make small changes to their rankings.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
In terms of originality, quality, and clarity:
- Originality is high. The authors review (in reasonably terse fashion) a number of studies that have sought to understand the impact of ranking items in search on attention (i.e., clicks and  visual attention that items receive). The study is grounded in this past work and notes its major contribution is to begin studying this effect experimentally.
- Quality of the study is high overall. See some concerns / questions below, but overall I would consider this to be a very convincing set of results.
- Clarity is very high throughout. Experimental details are crisply described.

In terms of significance
- Reasonable dataset (for this kind of study) with 57,000 ""real"" queries from 80 participants
- Known caveat that getting this data without direct access to search operator datasets is prohibitively expensive, hence why this is novel.

Weaknesses:
Two weaknesses (with respect to venue fit and potential of the current draft) stood out: 

First, a minor note: this kind of experimental work might be somewhat unusual at NeurIPS as it doesn't engage with the modelling side of ranking. Personally, I do not think this should be a blocking reason -- I think many in the community would like to see future works that incorporate this kind of experimental approach -- but it felt like an important piece of context to note.

Second, some readers may concerned with perceived issues with ecological validity. To some extent these are unavoidable in any experimental study like this -- there will always be some set of ecological validity concerns, they just trade off with each other.

- While this choice is reasonable, it may impact the kinds of queries used: ""The study participants are trusted individuals of different age groups and backgrounds, recruited by reaching out personally or via email.""
- It might be helpful (esp. given the work may impact policy discussions) to know more about the domain / type of queries, but very reasonable privacy choice to avoid sharing any information about that.
- Very minor: It's (by choice of technology companies) unclear if the kinds of perturbations studied here map to the kinds of a/b tests or experiments that are frequently rolled out by those companies. I don't think this is something the paper needs to address explicitly, but is also worth noting.

Limitations:
Limitations of the methods are reasonably discussed throughout.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors describe performative power, a pre-existing proposed measure of platform market power, and give an approach to measuring performative power using a browser extension. The browser extension perform random modifications to the search results page of results from target search engines, and measure click behavior. The modifications and user clicks are logged, providing sufficient statistics to compute a variant of the performative power metric. They deploy the extension to 85 users, resulting in about 57k clicks, and produce performative power calculations based on these interactions. The authors then provide some discussion of the results.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
I feel there are two main strengths to the paper
1. The authors make the case for measuring performative power using a browser extension
2. The authors created a browser extension to measure performative power. They appear to have been somewhat careful, discussing issues with hiding the DOM until rewriting is complete, discussing some latency numbers that are not bad (around 40ms, certainly enough to impact user behavior, but not enough to be wildly visible). They discuss privacy implications of their storage and logging of user search events, and have taken a reasonable approach with this, logging only intervention ids and click positions.

Weaknesses:
W1: supporting ongoing work. I didn't see a reference to the source code for the browser extension, or any discussion of it being available for further work -- this seems like a surprising omission, though perhaps I didn't read the right section?
W2: on novelty, I have a hard time characterizing exactly where the main contribution of the paper lies. The development of the performative power measure, and the argument for its relevance in regulation, comes from prior work. The understanding of the impact of position on click likelihood is also quite heavily studied, so the headline numbers the authors show have reasonable support in the literature already. The authors make a small modification to the definition to incorporate changes to the page beyond the organic results, but this does not seem to be the key point. The observation that PP can be computed from a browser extension seems fairly straightforward, not a centerpiece for the argument of novelty -- such approaches have been taken for modifying search results and measuring interactions in the past. The paper could arguably make the case for taking a straightforward idea (browser extension for performative power) and exploring the many thorny problems in designing and deploying this measurement, but my second primary concern below is that these issues remain largely outside the scope of the paper. Overall, I feel that there isn't a clear case to be made for the dimension of novelty.
W3: platform power. The authors paint a picture of developing a causal understanding of the ""power"" present in the platform to differentially route user attention across resources. My concern is that some interventions are sustainable (ie, the platform could actually implement such a modification) while others are not. As an extreme and somewhat laughable example, consider the intervention that replaced every search result with a link to the CEO's gofundme account. This would change clicks dramatically, resulting in high performative power, but nobody would argue that the platforms could sustainably deploy this intervention. Instead, the goal is to consider power the platform has to alter the *ongoing* distribution of user attention to online resources, so making this measure robust requires significant attention to the issue of reasonable interventions. The related literature the authors cite (cascade, eye-tracking, MLIR) is quite careful in these areas. It is not clear that placing a pure navigational result at position 3 instead of position 1 is sustainable, and the issue is not so simple: users habituate to platform behavior, and will respond differently when the platform stops behaving as expected, both in determining which results to consider and hence where to click, and in bigger ways (ie, changing providers). Correspondingly, platforms themselves rely on user feedback, which would be adulterated by these types of interventions, with unclear implications. For a proposal intended to be a ""blue print for how performative power can be used to guide digital market investigations,"" I think it's a significant omission not to consider these issues.

Limitations:
I feel that the limitations I discuss above should be covered in more detail by the authors. For ethics review, the authors indicate that this type of study does not require IRB approval at their institution, so I take this at face value.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper titled ""An engine not a camera: Measuring performative power of online search"" presents a study on the performative power of online search engines, specifically focusing on how they can influence web traffic through the arrangement of search results. The authors designed and executed an experiment to quantify the ability of search engines to steer user clicks by rearranging search results, a concept known as performative power.

The main contributions of the paper are as follows:

**Conceptual Framework**: The authors build upon the recent definition of performative power to develop a framework for understanding the influence of digital platforms, particularly search engines, on user behavior.

**Experiment Design and Quantitative Findings:**  

The study involved tens of thousands of clicks collected over a five-month period from more than 80 participants, providing robust quantitative insights into the causal effects of search result arrangements. The paper reports significant quantitative findings, such as the observation that consistently downranking the first search result by one position causes an average reduction in clicks of 42% on Google search, and even more for Bing.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality:**

The paper introduces a novel concept, the performative power of online search engines, which is a significant contribution to the field of digital platform policy and regulation.

**Quality:**

The analysis is thorough, with robust quantitative findings supported by extensive data collection over a five-month period from a diverse participant base.

**Clarity:**

The paper is well-structured, with clear definitions and explanations of key concepts such as performative power and the methodology used for the study.

**Significance**

The researchers presents that search engines are active ""engines"" that can significantly shape and influence the information landscape, with important sociopolitical implications.

Weaknesses:
1. **Limited Sample Size and Diversity.** The paper could benefit from a larger and more diverse sample to ensure the results are representative of different user behaviors across various demographics.
2. **Lack of case studies**. While the paper provides clear quantitative results, additional qualitative analysis or case studies could help interpret why certain patterns emerge, offering deeper insights into user decision-making processes.
3. **Lack of discussion on interventions**. The researchers provide a compelling framework for measuring this performative power, but the paper does not extensively address potential countermeasures or interventions that could help mitigate the negative aspects of search engines' performative power.

Limitations:
The author has already discussed the limitations of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors conduct a user study on the performative power of search engines, i.e., how much search engine providers can affect the information seen by the end user by tweaking the algorithmic ordering of results. In the specific context and assumptions formulated in this paper, this essentially amounts to measuring click-through rate differences across arrangements of search results.
The authors ran a live RCT experiment by injecting different arrangements directly on the result page with the help of a browser extension of rheir design. Results show that there are notable differences in CTR and therefore commercial search engines have a large performative power.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
The paper is very well written, with all methodology and assumptions being laid out clearly. Moreover the experiment design and scale seem sufficient for the task at hand according to my understanding. 
The results are made more solid and generalizable thanks to the use of different providers. Also this work is more complete than some previous user studies of this kind and the performative power angle is original and relevant.

Weaknesses:
I have two major concerns :
- I am not sure this paper should appear in NeurIPS. It does contin any neural system, nor does it indicate how to create one from the results. While these results could in general be useful to an ML practitioner working with search engines, this could be said of many other research outcomes from different fields, that ypicaltly don't appear in NeurIPS proceedings.
- The paper does not relate enough to previous work on position bias and other types of biases in searh results. While I believe new, updated user studies are always valuable, I think the authors should compare their results with those obtained by past studies.

Limitations:
Limitations are correctly adressed and very clearly laid out.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
0Lr9HQijA1;"REVIEW 
Summary:
The paper provides a unified view on various imprecise-label learning frameworks, such as semi-supervised-, partial-label-, or noise-label learning, through the lens of the expectation-maximization algorithm. In addition to unifying these existing setups, EM naturally allows treating combinations of the above setups. Experiments show that the proposed method compares favourably with existing methods specialized to just one setup.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The proposed method generalizes to a wide range of settings, and performs on par or better with more specialized algorithms in most of the evaluations.

Weaknesses:
* Neither abstract nor introduction make it clear that the paper is concerned purely with multi-class classification with deterministic labels y=f(x)

* This stands in contrast to the critique in l. 70 regarding competing, `[...] they usually require additional assumptions 69 and approximations on the imprecise information for learnablility`: Assuming deterministic labels is a _very_ helpful simplification for noisy labels, as, together with some upper-bound on the noise rate, it restores identifiability of the model

* A serious problem with the writing of the paper is that, for an attempt at introducing a probabilistic model that can then be used in EM, it does not actually write down the actual probabilisitic models it considers:
In this sentence (l. 158), the paper is extremely vague: 
> If I represents partial labels, then P(Y |I) would 158 have non-zero value over the candidate labels, and be 0 elsewhere. When I represents a set of noisy 159 labels, P(Y |I) would represent the distribution of the true labels, given the noisy labels. When I 160 does not contain any information, i.e., unlabeled data, Y can take any value.

for partial labels, is the underlying assumption $P(Y|I) = 1(Y in I) / |I|$, i.e., uniform distribution over all candidates? How about for unlabeled data?
for noisy labels, you either need to assume a fixed noise model, or $P(Y|I,\theta)$ where $\theta$ are the parameters of the noise model. 
l. 170: `Note that P (X; θ) is omitted from Eq. (5) since P (X) 170 does not rely on θ.` Why? Is this a new assumption? Earlier in the paper, $\theta$ was introduced as the modelling parameter of the generic joint distribution `Let P (X, I; θ) represent a parametric form for the joint distribution of X and I`. The footnote claims `he actual parameters θ may apply only to some component such as P (Y |X; θ) of the overall distribution` but to me, ""may"" here means that in some situations, such a restriction is possible, whereas I guess the intended meaning is that the model is _always_ supposed to be $P(Y|X,\theta)$?
l. 174: `For independent instances setting` again, this reads as if the previous section had situated the paper in the independent instance setting, whereas this is the first time the topic comes up

> The property of the second term log P (I|X, Y ; θ) is dependent on the nature of imprecise label I. If I contains information about the true labels Y , such as the actual labels or the label candidates, it can be reduced to P (I|Y ), i.e., the probability of I is no longer dependent on X or θ and thus can be ignored from Eq. (5).

This is not correct, just because I contains information about Y, the probability cannot be simplified.

Equation (6) seems to appear out of nowhere: Even following the derivations in C.2, $\mathcal{A}_s$ and $\mathcal{A}_w$ just appear out of thin air in these equations. 
l. 223: `Things become more complicated here since the noisy labels $\hat{Y}$ do not directly reveal the true information about $Y$` I'm not sure what this sentence is trying to say.  $\hat{Y}$ should have some information about $Y$, otherwise, learning is impossible; of course, it doesn't have the full information, but neither do partial labels, so I don't see how this changes the situation compared to the preceeding paragraphs.


The (unreferenced in the main paper) section D.7 in the appendix claims
> Since the settings studied in this work has loss functions derived 943 as close-form from Eq. (5), the time complexity can be viewed as O(1). Thus our method in general present faster runtime without complex design such as contrastive loss.

This argument doesn't make any sense. Solving a system of $n$ linear equations can be written in closed form, yet this is not an O(1) operation.

Overall, after reading the paper, I have almost no idea what the proposed method actually does. Equations (6), (7), (8) contain data augmentations, which I suspect may be important for attaining the performance reported in the paper(?), yet they are not really discussed as part of the proposed method. There are numerous inaccuracies, gaps, and I think even some errors (not fundamental, I think, but in the way the writing describes the math). It is certainly possible that I am misunderstanding something here, but as I see it right now, the paper is not in a shape in which it should be published.


Typos/grammar:
l. 87: our proposed method _generalise_ and subsumes
l. 148: we consider all possible _labeling_ along

Limitations:
The authors adequately concede in section 5 that the scalability of the algorithm hasn't been verified yet, sentences like
> While there exist earlier attempts on generalized or EM solutions [...] thus presenting limited 70 scalability on practical settings.

(l. 68-71) seem to suggest that the presented algorithm has this not-yet-verified scalability.

Rating:
3: reject, not good enough

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces imprecise label learning (ILL), a framework for the unification of learning with various imprecise label configurations, such as partial label learning, semi-supervised learning, noisy label learning, and a mixture of these settings. They propose an EM based method with closed-form learning objectives to handle the problem.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
● The problem of learning with imprecise labels is with great importance, and the idea of unification of different settings is interesting, impressive, and instructive.
● The paper is well-written. 
● While some details are omitted in the main text, the comprehensive appendix furnishes ample information. Such thorough work is highly appreciated.

Weaknesses:
Given the extensive content, the main text of the paper omits certain details, which may not be immediately straightforward to follow for some parts, e.g., section 3.2.

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a framework that unifies various imprecise label configurations, with an EM modeling for imprecise label information. The framework is demonstrated can be adapted to partial label learning, semi-supervised learning, and noisy label learning, and the combinations of all above. The experiments results show that the framework surpasses existing methods on various settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
-	This paper proposed a unified framework that can unify various imprecise label learning settings, reducing the need for separate designs and solutions for each type of label imprecision.
-	Promising performance is achieved in individual settings and mixture settings with the unified framework.
-	The proposed method is highly versatile and can be applied to the setting of a mixture of imprecise labels with robust performance.
-	The framework demonstrates scalability on larger and more complex datasets.

Weaknesses:
- The implementation of EM over all possible labelings may increase the computation time.
- More related works need to be discussed. The author considers the ground-truth or Bayes label distribution as the latent variables and leverages variational inference for estimating. I am not should this strategy is novel enough in the field of variational inference. I suggest the author add more related works to highlight the novelty of their technique.
- The author should give an explanation of why utilizing an EM framework optimizes the variational lower bound. What is its advantage?

Limitations:
-	Computational efficiency of EM regarding the weakness above. It would be better that the author provide (theoretical or empirical) analyses regarding the computation of the algorithm.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The article addresses the challenge of learning with imprecise labels in machine learning tasks, such as noisy or partial labels. Traditional methods often struggle with multiple forms of label imprecision. The authors introduce a novel framework named Imprecise Label Learning (ILL) that serves as a unified approach to handle various imprecise label scenarios. ILL employs the expectation-maximization (EM) technique, viewing precise labels as latent variables and focusing on the entire potential label distribution. The framework demonstrates adaptability to different learning setups, including partial label learning and noisy label learning.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The article is well-written, with clear and concise language that effectively conveys the main ideas and contributions of the research. Also, comprehensive derivation of loss functions for the three imprecise annotations configurations derived from equation 5 are given which ensure clarity and thorough understanding for the readers.
2. The article offers a comprehensive solution to the prevalent challenge of imprecise annotations, enhancing the adaptability and applicability of machine learning models.
3. The inclusion of experimental results across multiple settings provides empirical evidence of the framework's robustness and superior performance.

Weaknesses:
1. The article's innovation is limited, as the approach of considering ground-truth labels or Bayes label distribution as latent variables and using variational inference for approximation in weakly supervised learning is already a common method[1-2], which suggests that the presented techniques may not be as novel as claimed.

   [1] Xu, N., Qiao, C., Geng, X., & Zhang, M. L. (2021). Instance-dependent partial label learning. Advances in Neural Information Processing Systems, 34, 27119-27130. 

   [2] Yao, Y., Liu, T., Gong, M., Han, B., Niu, G., & Zhang, K. (2021). Instance-dependent label-noise learning under a structural causal model. Advances in Neural Information Processing Systems, 34, 4409-4420.

2. Some important baselines should be compared, such as [1,2] in SSL.

   [1] Nguyen, Khanh-Binh, and Joon-Sung Yang. ""Boosting Semi-Supervised Learning by bridging high and low-confidence predictions."" *Proceedings of the IEEE/CVF International Conference on Computer Vision*. 2023.

   [2] Schmutz, Hugo, Olivier Humbert, and Pierre-Alexandre Mattei. ""Don’t fear the unlabelled: safe semi-supervised learning via debiasing."" *The Eleventh International Conference on Learning Representations*. 2022.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
I3kIEjoON6;"REVIEW 
Summary:
This paper presents a framework for crystal structure generation, focusing on polymorphs. The framework utilizes matrix representation of crystals and various generative models used in vision tasks, with specially designed similarity metrics and loss function. It is tested on (1) modification of given structures and (2) generation from scratch within the dataset, as well as finding new structures in the Ta–W–B system.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This work develops domain-specific representation, metric, and loss, so that generative models that have proven useful in image generation can apply to crystals. The topic is timely and important.

Weaknesses:
- In the demonstrated use cases, the generation is conditioned on elements, space group, etc., but not materials properties of interest. These show limited usefulness in materials discovery and design.
- The matrix representation does not take physical constraints into account, e.g., space group determines symmetries in the lattice parameters. Besides, related previous works, e.g., [UniMat](https://openreview.net/forum?id=wm4WlHoXpC), should be discussed.
- The clarity and rigorousness need to be improved (see Questions). The mathematical notations are not unified, e.g., $x$ vs $X$.

Limitations:
Discussed in the Conclusion.
Besides, Sec. 9 contains a GitHub link that could break anonymity.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors studied the use of diffusion and flow matching approaches for the generation of crystalline materials. The authors trained UNet models on polymorphs in the AFLOW database (which has a series of DFT-computed properties for these materials) using either simple R3 regression, diffusion/flow matching. The authors then presented inference results on similarity to training structures (Section 3.4, which shows these methods can reproduce training structures to different extent), and showed that a subset of the modified generated crystals (with Ta, W, B) can have a small non-zero formation energy.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
*Originality*: The authors attempted to study the problem of crystal structure generation with no invariances/equivariances other than periodic translation invariance. 
*Quality*: The authors attempted to use DFT to validate some inference results.
*Significance*: Crystal structure generation (especially synthesizable ones) is an important problem. It seems that training from uniform noise distribution works better for CFM than training on Gaussian noise, contrary to the established results in the field.

Weaknesses:
*Originality*. The manuscript lacks originality. The diffusion/flow matching techniques are well-established in inorganic crystal structures (e.g. CDVAE cited here, DiffCSP/FlowMM that's not here). Sure, using a network architecture not designed for materials/crystals and using no invariances/equivalences is new, but it deviates from standard practices in the field without sufficient justification. I believe the implementations shown in the paper are a great exercise for practitioners interested in the field, but unfortunately, I do not see it as a NeurIPS paper.

*Quality*. The manuscript is _very_ bare-boned, making a comprehensive technical critique challenging without appearing disproportionately critical. 
- On the ML side, there are numerous large fallacies/mistakes (e.g. no consideration of bonds between atoms at all, Sec. 3.3 there is no description of the PBC loss, the generation does not consider the unit cell, no generation with atom types, and there is no investigation of any experiments observed e.g. why is uniform noise better for CFM, the result in Table 1 appears to evaluate overfitting rather than novel generation, the list can go on). 
- On the chemistry/validation side, there are again numerous problems (why would formation energy be given during the generation process, what functional did you use in DFT, there are no comparisons against existing structures and hence cannot be claimed as novel, etc.) 
- There is no comparison against _any_ known methodologies. 
- The results overall, are very weak both in ML and in chemistry (e.g. Table 3 shows most if not all materials generated have extremely large positive formation energies despite the simple elemental composition; the remaining few negative ones are at the brink of instability, in any case they likely would not be synthesizable).  

*Clarity*. The manuscript suffers from poor presentation, starting with a promotional-style title that lacks scientific descriptiveness. I unfortunately do not understand the novelty of the paper in comparison to existing methods. The paper consistently fails to provide essential explanations across both machine learning and chemical methodologies. 
- On the ML side, there are numerous things poorly presented (e.g. Figure 1 is just periodic translation invariance and in a typical manuscript would be summarized in one sentence). 
- On the chemistry side, things are greatly exaggerated (e.g. computationally making a few materials with negative formation energy can be done by undergraduate students and certainly does not warrant descriptions such as 'This significant outcome underscores the remarkable potential of our framework in uncovering thermodynamically stable materials)

Limitations:
Partially.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper addresses the inverse problem of generating crystal structures based on given properties, thereby avoiding the need for extensive computational resources typically required in traditional methods. The authors utilized the AFLOW materials database, selecting unstable and stable series of structures for two specific tasks: modifying structures to achieve stability and conditional structure generation.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors experimented with various generative model approaches and evaluated two tasks in crystal generation. Additionally, they integrated the VASP software for application testing and successfully identified four previously undiscovered stable structures through conditional generation.

Weaknesses:
1. From the perspective of model application, although the authors used the AFLOW database for their study, they did not compare the data range and coverage with other significant databases like the Materials Project. This omission leaves a gap in understanding how the generative models perform across different datasets and whether the results are consistent and generalizable. Comparing the performance of the same generative models on different databases could provide valuable insights into the robustness and applicability of their approach.

2. There is a partial break of anonymity in the GitHub link on Page 9 in this paper.

Limitations:
The authors proposed two major directions: conditional generation and conditional modification. There is room for improvement in both the experimental results and the data used for conditional modification. For example, they could consider recognizing unit cells with translational and rotational transformations and introducing more ways to assess generation results.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper deals with an important application of generative models for science: generation of crystalline structures. However, I have some serious concerns. First, scope. While comparing different methods for the same objective is informative, I am not so sure what is the purpose here. Having so many different methods certainly dilutes the main message in a short paper format like neurips. Second, approach. From what I understand, there are questionable design problems with the technical approach. Third, results. The authors spend most of their space explaining various methods such that there is little room left for explaning the impact of their results, or comparing their results with existing approaches. Finally, presentation. Figure 1 is kind of trivial or at least very simple and I am not sure it is worth a separate figure. The overall typesetting looks not too professional.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The methodology selection is broad and hopefully the audience can benefit from a mini-benchmark of different generative approaches. The overall model architecture is distinctive from what I have seen in the literature.

Weaknesses:
I have a good number of questions on the technical approaches. More specifically, the model takes a specially formulated data structure that does not seem to be obviously invariant or equivariant under permutation, translation, rotation, which is concerning. For example, change the selection or ordering of the unit cell vectors and everything will change in an uncontrolled way.

The conditioning approach seems to be to provide desired properties as inputs to the generative approaches. I am not sure this always make sense. For example, if one desires a certain space group, there is no enforcing compliance with the space group. One can always easily check it. It is perhaps more suitable to enforce space group compliance using a guidance-based conditioning approach.

Limitations:
There is insufficient discussion about the limitations given my concerns shown above.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
5FHzrRGOKR;"REVIEW 
Summary:
The paper introduces Federated Behavioural Planes (FBPs), a method designed to track FL clients' behavior by means of examining the their representations in two behavioural planes with the aid of a server-owned dataset. The Error Behavior Plane (EBP) and Counterfactual Behavioural Plane (CBP) correspond to two 2-D space where each client model's prediction errors and counterfactural distribution are examined. Combining these two representations a new aggregation rule is proposed to fend off malicious clients.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is overall well written. The idea presented is clear and easy to follow.
- The method of characterizing clients' behavior is kind of novel.

Weaknesses:
- Creating the two plains require the plaintext of all local models, which raises major privacy concerns.
- From Eqs. 6 and 7, I suppose generating the counterfacturals must be very costly.
- Insufficient experimental evidence to support the claim. For example, fig. 3 does not show distinct client behavior clearly. I also expect to see more results such as the behavioural scores of different clients and how sensitive the FBP is to the intensity of attacks.

Limitations:
- A framework overview is missing, which makes it hard to understand how the planes are created throughout the FL iterations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel method called Federated Behavioural Planes (FBPs) for analyzing, visualizing, and explaining the dynamics of Federated Learning (FL) systems. FBPs are consist of Error Behavioural Plane (EBP), reflecting the model’s predictive performance, and Counterfactual Behavioural Plane (CBP), reflecting the decision-making processes. Using insights from FBPs, the paper also proposes a new robust aggregation technique called Federated Behavioural Shields (FBS) to enhance security in FL systems.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ Novel approach: The Federated Behavioural Planes (FBPs) introduce a new way to analyze and visualize client behaviors in Federated Learning, addressing a gap in existing literature.
+ Theoretical foundation: The authors provide a theoretical framework for understanding FL dynamics, grounding their practical approach in solid mathematical concepts.
+ Explanatory power: FBPs allow for visual identification of client clusters and trajectories, enhancing interpretability of FL systems.

Weaknesses:
- The paper heavily relies on visual representations (FBPs) to explain client behavior, which makes it infeasible to scale to large-scale FL systems where the convergence might take more rounds and client selection takes place in each round. These factors will largely make the trajectory on the planes hard to keep track of.

&nbsp;

- The computational overhead is too large. 
  + In each training round, the counterfactuals are required to be computed for the locally updated model from every client. And the differences between the counterfactuals from each pair of clients are also computed. For cross-silo FL settings, this is already a large computational overhead. For cross-device settings, I do not think this process is affordable especially for high dimensional data like image data.
  + Besides, since the paper focuses primarily on detecting anomalies and enhancing security from attackers, the main battlefield of this method should be more on the cross-device setting where the number of clients is large. To this end, the prohibitive computational overhead mitigates the motivation of this work.

&nbsp;

- The method completely relies on a validation set on the server, which may be hard to obtain without introducing any privacy concerns. Even a validation set is available to use, under what circumstances will the validation set be equally fair to all clients so that it is not far from any client’s local data distribution. And if the plane shows anomalies for some client, how to distinguish the reasons from having an unfair validation set and client is the anomaly/attacker?

&nbsp;

- The datasets and the federated settings in the experiments are too simple. And the compared robust aggregation methods are not so recent.

Limitations:
Most limitations of the proposed method (validation set, computational overhead) have been addressed. Please refer to Weaknesses for other limitations of this work. There is no potential societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel method called Federated Behavioural Planes (FBPs) to analyze, visualize, and explain the dynamics of client behavior in Federated Learning (FL) systems. The primary contributions of the paper are as follows:

* 1\. Introduction of Federated Behavioural Planes (FBPs): FBPs consist of two planes:
     * 1.1\. Error Behavioural Plane (EBP): This plane analyzes the predictive performance of client models by visualizing the errors they produce.

     * 1.2\. Counterfactual Behavioural Plane (CBP): This plane examines the decision-making processes of client models through counterfactual explanations, highlighting how decision boundaries are formed.
* 2\. Visualization and Analysis: FBPs provide informative trajectories that describe the evolving states of clients, enabling the identification of clusters of clients with similar behaviors. This helps in understanding both beneficial and detrimental behaviors in FL systems.

* 3\. Federated Behavioural Shields (FBS): Based on the patterns identified by FBPs, the authors propose a robust aggregation technique named Federated Behavioural Shields. This technique enhances security by detecting malicious or noisy client models and surpasses the efficacy of existing state-of-the-art FL defense mechanisms.

* 4\. Experimental Validation: The paper demonstrates through experiments that FBPs can effectively track client behavior, identify client clusters, and improve the security and performance of FL systems. The proposed FBS method outperforms other robust aggregation methods in defending against various types of attacks.

Overall, the paper offers a comprehensive approach to enhance understanding, trust, and control over federated learning systems by introducing a novel method to analyze and secure client behaviors.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper ""Federated Behavioural Planes: Explaining the Evolution of Client Behaviour in Federated Learning"" demonstrates several strengths across different dimensions:

* 1\. Originality: The paper introduces a novel approach, Federated Behavioural Planes (FBPs), to analyze and visualize client behavior dynamics in Federated Learning systems. This method offers a unique perspective on understanding client behavior evolution in FL, which is a relatively unexplored area in the existing literature. The combination of predictive performance analysis and decision-making process evaluation through FBPs showcases originality in addressing the challenges of client behavior in FL systems.
* 2\. Quality: The paper maintains a high standard of quality in terms of methodology, experimental design, and theoretical framework. The introduction of FBPs as a tool to explain the dynamics of FL systems reflects a well-thought-out approach to addressing the evolving behavior of clients in federated learning environments. The robust aggregation mechanism proposed, Federated Behavioural Shields, demonstrates a quality solution to enhance security in FL systems.
* 3\. Clarity: The paper is well-written and structured, making it easy for readers to follow the concepts presented. The clarity in explaining the Federated Behavioural Planes framework, the experimental results, and the implications of the proposed method enhances the overall understanding of the research. The use of figures and explanations aids in visualizing complex concepts related to client behavior in FL systems.
* 4\. Significance: The paper's contribution to the research area of Federated Learning is significant. By introducing FBPs and Federated Behavioural Shields, the paper addresses a key challenge in FL systems - understanding and controlling client behavior. The insights provided by FBPs and the improved security offered by Federated Behavioural Shields have the potential to enhance the reliability and control over FL systems, making a valuable contribution to the field.

Overall, the paper makes a substantial contribution to the field of federated learning. Its originality lies in the novel problem formulation and creative combination of existing ideas. The quality of the research is demonstrated through rigorous methodology and comprehensive experiments. The clarity of the presentation ensures that the contributions are accessible to a broad audience. The significance of the work is underscored by its potential impact on improving the security and efficiency of federated learning systems.

Weaknesses:
**Complexity of Methods:**
 - **Computational Overhead**: The concurrent training of counterfactual generators with the main predictive models introduces significant computational overhead. This could be particularly burdensome in real-world federated learning settings where resources are limited. The paper could benefit from a more detailed analysis of the computational costs and potential optimization strategies to mitigate this overhead.
    - **Actionable Insight**: Consider providing a detailed comparison of the computational requirements of the proposed method with baseline methods. Explore possible optimizations or approximations that could reduce the overhead without significantly compromising the performance.

**Real-World Applicability:**
- **Scalability Concerns**: While the experiments are comprehensive, they are conducted on relatively small datasets and a limited number of clients. This raises concerns about the scalability of the proposed methods to larger, real-world federated learning scenarios with many clients and more complex data distributions.
    - **Actionable Insight**: Include a discussion on the scalability of FBPs and FBS. Consider performing a scalability analysis, even if only theoretical, to predict the performance and feasibility of the methods in larger settings. Additionally, simulations or theoretical models could provide insights into expected behavior in large-scale deployments.

**Generalization Across Different Models:**
- **Model-Specific Limitations**: The proposed method may be tailored to specific types of models (e.g., neural networks) and may not generalize well to other types of models used in federated learning (e.g., decision trees, support vector machines).
    - **Actionable Insight**: Discuss the applicability of FBPs and FBS to different types of models. Providing a broader range of experiments that include different model architectures could strengthen the paper. If certain models are not compatible, explain the limitations and potential modifications required for broader applicability.

**Evaluation Metrics:**
- **Limited Evaluation Metrics**: The evaluation primarily focuses on standard metrics like accuracy and robustness against attacks. While these are important, they might not capture all aspects of the system's performance, such as the impact on communication efficiency, latency, and energy consumption.
    - **Actionable Insight**: Introduce additional evaluation metrics that capture the holistic performance of the system, including communication overhead, latency, and energy consumption. This would provide a more comprehensive assessment of the practicality of the proposed methods in real-world applications.

**Interpretability and Usability:**
- **Interpretability for Non-Experts:** The paper, while clear in its technical explanations, may still be challenging for practitioners who are not experts in federated learning or explainable AI. Enhancing the interpretability and usability of the methods for a broader audience could be beneficial.
    - **Actionable Insight:** Provide more intuitive explanations and visualizations of the key concepts and methods. Including case studies or practical examples demonstrating the application of FBPs and FBS in real-world scenarios could make the methods more accessible and easier to understand for non-experts.

**Real-World Validation:**
- **Lack of Real-World Validation**: The experiments are conducted in controlled settings, which may not fully represent the challenges and variability encountered in real-world federated learning deployments.

    - **Actionable Insight**: Discuss potential real-world applications and the expected challenges. If possible, provide preliminary results or insights from deploying the methods in a real-world scenario. Alternatively, outline a detailed plan for future real-world validation studies.

**Conclusion:**
While the paper presents significant contributions to federated learning, addressing these weaknesses could enhance its impact and practicality. By focusing on computational efficiency, scalability, broader model applicability, comprehensive evaluation metrics, interpretability, and real-world validation, the work can move closer to its stated goals and become more robust and applicable to a wider range of scenarios.

Limitations:
**Assessment of Limitations and Potential Negative Societal Impact:**

Based on the provided content and the NeurIPS checklist guidelines on limitations and broader societal impacts, here is an assessment of how well the authors have addressed these aspects and suggestions for improvement:

**Addressing Limitations:**

- **Identified Limitations:** The paper acknowledges some limitations, such as the computational overhead introduced by the concurrent training of counterfactual generators and the potential scalability issues in larger federated learning deployments.

- **Suggestions for Improvement:**
    - **Computational Efficiency:** Provide a more detailed discussion on the computational requirements and potential optimization strategies. This could include specific techniques to reduce overhead, such as model pruning, quantization, or distributed optimization methods.

    - **Scalability Analysis:** Include more comprehensive scalability experiments or simulations that predict the method's performance in larger, real-world settings. Discuss how the method can be adapted or optimized for large-scale deployments.

**Potential Negative Societal Impact:**
- **Security and Privacy:** The primary focus of the paper is on enhancing security and privacy in federated learning, which is a positive societal impact. However, potential negative impacts, such as the misuse of federated learning systems or unintended biases in the models, should be considered.

- **Suggestions for Improvement:**
    - **Bias and Fairness:** Discuss the potential for unintended biases in federated learning models and how the proposed methods could mitigate or exacerbate these biases. Provide suggestions for ensuring fairness in federated learning deployments.
    - **Misuse of Technology:** Address the potential for misuse of federated learning systems, such as using the technology for surveillance or other harmful purposes. Discuss safeguards and ethical considerations to prevent misuse.

**Constructive Suggestions for Improvement:**
* 1\. **Detailed Discussion on Limitations:**

    - Expand the discussion on identified limitations, providing more details on computational efficiency and scalability. Include theoretical analysis or empirical evidence supporting the claims and potential solutions.
* 2\. **Bias and Fairness:**

    - Add a section discussing the potential for biases in federated learning models. Explain how the proposed methods could impact fairness and provide guidelines or best practices for ensuring equitable outcomes.
* 3\. **Ethical Considerations and Misuse:**

    - Address potential misuse of federated learning technology. Discuss ethical considerations and propose safeguards to prevent the harmful application of the technology. Highlight the importance of transparency and accountability in federated learning deployments.
* 4\. **Societal Impact Statement:**

    - Include a comprehensive societal impact statement that covers both positive and negative aspects. This should highlight the benefits of enhanced security and privacy, as well as address the potential risks and ethical concerns.

**Conclusion:**
While the paper makes significant contributions to enhancing security and privacy in federated learning, it could benefit from a more thorough discussion of limitations and potential negative societal impacts. By addressing these areas, the authors can provide a more balanced view of their work and ensure that it is applied responsibly and ethically.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
2QvCOFw058;"REVIEW 
Summary:
This paper introduces a custom Guass-Newton method dubbed AGN to solve general over-parametrized matrix sensing problems, and demonstrated that 1) This new method is a descent method under benign assumptions and 2) it achieves Q-linear convergence under restrictive RIP assumptions.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Introduced a new algorithm beyond GD based ones to solve this hard non-convex problem with some theoretical guarantees, which is nice. The framework of the problem is relatively relaxed, and is not tailored to very specific instances of matrix sensing.

2. Showcases that AGN will not be trapped inside hessian points theoretically.

3. The authors proves the quick convergence of this algorithm under small RIP constant.

4. The structure of this paper is easy to follow, addresses appropriate prior works, and offers appropriate level of details. Overall, this paper is very well-written and a pleasure to read.

Weaknesses:
1. I still find computational cost and difficulty of obtaining a good AGN update to be hard to understand. It is well-known that GN methods perform better than GD in terms of landscape and convergence rate, while at the cost of computation. Therefore the readers need to know where this tradeoff stands in this scenario.’

2. Section 5 offers the analysis under a very restrictive setting, which is an extremely small RIP constants. Such constants are hard to find in real life, and even GD methods perform very well under this setting, some achieving linear convergence when not too far from the ground truth. The readers need to know under this easy setting, how does AGN shine?

3. Theorem 2 offers the convergence results in terms of function value, but this is a tricky choice because we don’t know whether the distance between $X_t$ and ground truth also shrinks at a linear rate, which is what we are after. Since in Theorem 2 a small RIP constant is used, I imagine the difference in function value can be transformed into matrix distances with a minor twist, i still find it intriguing why the authors made this choice.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on the optimization of overparameterized, non-convex low rank matrix sensing (LRMS)—an essential component in contemporary statistics and machine learning. 
This paper introduces an approximated Gaussian-Newton (AGN) method for tackling the non-convex LRMS problem. Notably, AGN incurs a computational cost comparable to gradient descent per iteration but converges much faster without being slowed down by saddle points. This paper proves that, despite the non-convexity of the objective function, AGN achieves Q-linear convergence from random initialization to the global optimal solution. Moreover, under certain conditions on the sensing operator, AGN demonstrates super-linear convergence rate. The global Q-linear convergence of
AGN represents a substantial enhancement over the convergence of the existing methods for the overparameterized non-convex LRMS.


Problem:

There are some severe typos that will affect the readability of this paper.

(1) Line 134, it should be $J(x_t) = \phi'(x_t)$ instead of $J(x_t) = \psi'(x_t)$.
(2) Line 155, $\mathcal{B}(X,X) = \mathcal{A} (PXX^\top Q)$, so, what is $\mathcal{B}(\Delta, X_t)$ in line Eq.(7)?
(3) Why does Eq. (7) relate to Gauss-Newton? I known $\phi(x) = \mathcal{B}(X,X) - b$, but what is its Jacobian? what is $J\Delta$? 

Due to these problems, I think this paper is hard to read.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper focuses on the optimization of overparameterized, non-convex low rank matrix sensing (LRMS)—an essential component in contemporary statistics and machine learning. 
This paper introduces an approximated Gaussian-Newton (AGN) method for tackling the non-convex LRMS problem. Notably, AGN incurs a computational cost comparable to gradient descent per iteration but converges much faster without being slowed down by saddle points. This paper proves that, despite the non-convexity of the objective function, AGN achieves Q-linear convergence from random initialization to the global optimal solution. Moreover, under certain conditions on the sensing operator, AGN demonstrates super-linear convergence rate. The global Q-linear convergence of
AGN represents a substantial enhancement over the convergence of the existing methods for the overparameterized non-convex LRMS.

Weaknesses:
No

Limitations:
No

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this submission, the authors proposed an approximated Gaussian-Newton (AGN) method for overparameterized non-convex low-rank matrix sensing problem. The authors presented the corresponding theoretical analysis and partially explained the reason the proposed AGN method achieves fast convergence rates.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
In this submission, the authors proposed a new AGN method The main idea is natural and the authors provide both theoretical analysis as well as some numerical experiments that provide empirical results consistent with the theoretical results. The main structure of this submission is clear and the presentation is of high quality.

Weaknesses:
There are major two drawbacks of this paper.

1. The authors should put all the empirical results from the numerical experiments into one single section. The authors should proved more numerical experiments to compared the proposed method with other state-of-art algorithms and put all the figures and tables in one numerical experiment section.

2. The authors didn't present or prove the superlinear convergence rate of the proposed AGN algorithm.

Limitations:
The authors has presented the limitations in the last section.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
444LAH3MhG;"REVIEW 
Summary:
In this paper, the authors aim to address the problem of active fine-tuning learning. The difference with active learning is that active fine-tuning models are pre-trained rather than trained from scratch. Therefore, the characteristics of the pre-trained model are important for this problem. While the uncertain methods of active learning disturb the quality of the pre-trained model, ActiveFT in active fine-tuning learning ensures stable training by selecting the most representative sample solution. The authors' idea lies in refining the boundaries by selecting samples through uncertainty based on representative samples. Their active fine-tuning framework, BiLAF, selects central and boundary samples to learn diversity and uncertainty. Besides, it has novel improvements, including unsupervised denoising and an iterative strategy for boundary sample selection. Finally, this paper conducts extensive experiments and ablation studies.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The authors provide a new active fine-tuning framework that combines the advantages of traditional uncertainty methods with the state-of-the-art ActiveFT method. They also provide extensive data experiments and analyses to illustrate the effectiveness of the proposed method.

Weaknesses:
Boundary sample selection is not discussed for fine-grained categorization, a case with less category differentiation. Besides, what is the gap between active fine-tuning learning and the upper performance of fine-tuning using all samples?

Limitations:
For downstream task fine-tuning, which in many cases is application-specific, one of the problems I often encounter is how to determine the data collection conditions as best as possible in order to minimize costs. Therefore, the biggest problem I encountered was not having enough samples for classification.
Therefore, I suggest the authors to further improve the results for tasks with higher annotation costs, such as detection and segmentation tasks or domains requiring expert knowledge annotation such as remote sensing and medicine.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes an active fine-tuning method that considers both diversity and uncertainty. This method selects uncertain samples through an unsupervised denoising approach and boundary score evaluation. The efficiency and effectiveness of this method, which involves selecting central and boundary samples, have been validated through multiple experiments.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The writing is generally clear, and the content is relatively comprehensive.
2. The method considers both diversity and uncertainty, which is a good idea.
3. The experiments are quite thorough.

Weaknesses:
1. The main diagram, Figure 2, lacks detail and clarity, and its meaning overlaps with the schematic in Figure 1. Figure 2 should focus on explaining how the methods ""Boundary Score Calculation"" and ""Iterative Selection and Removal"" are implemented (in detail) rather than simply outlining the overall process.
2. Is there consistency between the decision boundaries of the K pseudo-classes and the decision boundaries of the true task categories? Are the decision boundaries of the unsupervised method consistent with the decision boundaries when the pre-trained model is fine-tuned on downstream tasks? (Different types and capabilities of models have different decision boundaries.) It seems these issues were neither considered nor analyzed.
3. The theoretical foundation is weak. Is it necessary to introduce data on class decision boundaries? Is there a theoretical basis for applying ""diversity and uncertainty"" sampling to active fine-tuning methods? Is there a theoretical basis for the design and implementation methods?
4. Exploration of the number of uncertain sample points: How is K determined? Having too many such sample points presents two problems: incorrectly labeled samples and disruption of learning the general features of the categories.
5. ""but neglects the contributions of samples to local boundaries."" 
Is the statement in the abstract deviating from the original meaning? Should it be ""but neglects the contributions of local boundary samples"" instead of ""but neglects the contributions of samples to local boundaries""?

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors introduce a novel Bi-Level Active Finetuning Framework (BiLAF) designed to address the limitations of existing active learning methods in the context of the pretraining-finetuning paradigm. The framework aims to optimize sample selection for finetuning models within a limited annotation budget. The authors propose an innovative unsupervised denoising technique to eliminate noisy samples and use a newly designed boundary score metric for iterative boundary sample selection. Extensive experiments demonstrate that BiLAF outperforms existing methods across various datasets and tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The bi-level framework that combines core sample selection with boundary sample selection effectively addresses limitations in existing active learning methods.
- The novel unsupervised denoising technique effectively eliminates noisy samples, improving the reliability of sample selection.
-  Extensive experiments on multiple datasets and tasks consistently show BiLAF outperforms state-of-the-art methods.
- The paper is well-organized with a clear explanation of the motivation, proposed method, and experimental results.

Weaknesses:
Activate Finetuning is advantageous for model fine-tuning in scenarios with limited data, and the motivation behind the proposed BiLAF is clear. However, I have concerns regarding the generalizability of this method to different data sizes. As shown in Table I, there is a significant performance drop in the CIFAR10 dataset under the 0.5% setting. This decline might be attributed to the small size of CIFAR10 images and the low 0.5% ratio, making it challenging to select and denoise boundary samples. It raises the question of whether BiLAF requires a certain threshold of fine-tuning data to be effective, which warrants further investigation by the authors. Additionally, the CVPR 2024 paper (see reference below) reports on the performance of CIFAR10 under 0.1% and 0.2% settings, which is highly relevant. I believe that Activate Finetuning would be more meaningful in scenarios with extremely limited data (e.g., few-shot fine-tuning), where changing the random seed can lead to significant accuracy fluctuations. Therefore, I recommend the authors include experiments and analyses on such scenarios. Furthermore, comparative experiments with other settings in the paper should be included to substantiate the method's effectiveness.
Moreover, when the amount of fine-tuning data increases, BiLAF's performance becomes comparable to the baseline ActiveFT. Hence, it would be insightful for the authors to discuss whether BiLAF remains effective with larger amounts of fine-tuning data, such as 20% of the ImageNet 1k dataset. This additional discussion would enhance the paper's comprehensiveness.

Xu, Wenshuai, et al. ""ActiveDC: Distribution Calibration for Active Finetuning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.

Limitations:
As described in the Weakness section, there might be some critical aspects that are not verified enough. The authors are encouraged to show additional results to support their claims.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a Bi-Level Active Finetuning Framework (BiLAF) for optimizing sample selection in the pretraining-finetuning paradigm. BiLAF combines global diversity and local decision uncertainty through two stages: core sample selection and boundary sample selection. Without requiring labels, the method successfully identifies pseudo-class centers, employs a tailored denoising technique, and iteratively selects boundary samples. Experimental results demonstrate that BiLAF consistently outperforms existing baseline methods across various vision tasks, demonstrating its superior efficacy in improving model performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-structured, with clear organization into subsections that introduce the different stages of the method. The use of an algorithm to summarize the entire process is effective.
2. The approach not only focuses on the centers of each class but also pays attention to the boundary samples between classes, effectively balancing global diversity and local decision uncertainty when selecting samples for annotation.
3. The iterative selection and removal strategy, along with the opponent penalty, effectively prevents the aggregation of multiple samples near the same pseudo-class boundary that faces the same opposing pseudo-class center.

Weaknesses:
1. There is ambiguity in the use of symbols in the method description, leading to unclear explanations.
2. The authors claim that the method is effective for imbalanced datasets but do not provide a thorough theoretical explanation. Additionally, experimental results show that the improvement over ActiveFT on the CIFAR100LT dataset with a 15% annotation ratio is only 0.3%, whereas the improvement is much more significant on balanced datasets. This contradicts the authors' claim. The authors explain that ""the default denoising removal ratio parameters might remove minority samples in long-tail distributions,"" which highlights a potential issue with their algorithm design under long-tail conditions, conflicting with their earlier statement.
3. The ablation study shows that the inclusion of the opponent penalty contributes little to the performance improvement and even causes performance degradation under lower budgets. The necessity and design of the opponent penalty need to be reconsidered.

Limitations:
The authors acknowledge the issue that in long-tail scenarios, the denoising methods might remove outliers that are key samples. However, what the authors do not mention is that the numerous hyperparameters in their algorithm can lead to difficulties in achieving optimal performance and even cause instability in different scenarios.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces BiLAF, a novel approach for selecting boundary examples alongside core samples to enhance the fine-tuning of pre-trained models for downstream tasks. Specifically, the boundary selection strategy leverages the distinction between intra- and inter-class distances within the pre-trained feature space and incorporates an opponent penalty to promote diversity across different boundaries. Experiments and ablation studies demonstrate the effectiveness of the proposed BiLAF in achieving state-of-the-art results.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written and easy to follow. 
- In addition to classification tasks, the experiments include different scenarios such as detection and segmentation. In these settings, BiLAF demonstrates the state-of-the-art results. 
- Ablation studies and execution time are included in the analysis to help provide a deeper understanding of the proposed BiLAF.

Weaknesses:
- BiLAF seems to heavily rely on pre-trained features, as all of its data selection processes are based on them. BiLAF's reliance on pre-trained features for data selection raises concerns due to potential discrepancies between pre-training and fine-tuning tasks. Note that the features from pre-trained classifiers are commonly used in traditional active learning because the labeled and unlabeled datasets are usually from the same task. However, this might not hold true in general pre-training and fine-tuning paradigms. Moreover, Line 120 mentions that “By leveraging pre-trained models, data samples are mapped to robust feature representations that elucidate the relationships among samples, their intra-class counterparts, and inter-class samples from diverse classes.” It remains unanswered and uncertain to what extent these pre-trained features are robust enough to effectively apply BiLAF. 

- In the typical pre-training and fine-tuning paradigm, full fine-tuning may not always be the optimal approach. For instance, when we only have limited/few-shot data, techniques like linear probing or even nearest-neighbor classifiers can yield better results. Furthermore, the choice of fine-tuning method can also be influenced by the similarity between pre-training and downstream tasks [1]. However, the paper lacks a discussion of this aspect and solely focuses on full fine-tuning, neglecting the potential benefits of alternative methods. Specifically, does BiLAF remain necessary for selecting high-quality data points, if we can apply more suitable fine-tuning methods? Or would simple random sampling suffice? 

[1] Head2Toe: Utilizing Intermediate Representations for Better Transfer Learning, ICML 2022.

Limitations:
In the appendix, the paper acknowledges a limitation in the proposed method, as it is based on general features. However, as I indicated in the weakness section, the paper lacks a discussion/study to investigate the severity of this limitation and its potential impact on the performance.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
7VNvM9SnRE;"REVIEW 
Summary:
This paper considers the Adaptive Optimal Assortment (AOA) problem a.k.a. Utility Maximization with Subset Choices. The goal of this problem is to find the optimal profit-maximizing subset of size up to m (Top-m-Objective) or its weighted variant (Wtd-Top-m-Objective). Given a selected subset, the feedback follows the Plackett-Luce (PL) choice model that returns an item from the subset or a ""no-choice"" option. The probability of choosing each item is proportional to their underlying score/utility values.

The paper proposes a new algorithm, AOA-RB, that is claimed to be practical, efficient, and optimal. Compared to previous works, this algorithm does not require sampling the same subset repeatedly nor assumes a strongest default item. Later, the authors extend this algorithm with adaptive pivots that further improves performance.

The theoretical analysis shows that AOA-RB obtains regret guarantees that build on a novel ""Rank-Breaking"" parameter estimation technique for the discrete choice model.

The performance of AOA-RB is further demonstrated in numerical experiments using synthetic datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Problem Statement
- Clear presentation and motivation. Easy-to-follow section.

Algorithm
- Clear strengths are the relaxation of previous assumptions, e.g., repeated sampling of the same subset or the assumption of a strong default item
- The algorithm is well-presented and easy-to-follow.
- The adaptive pivot extension of the AOA-RB is a clear improvement that provides significant improvements

Theoretical Analysis
- The new concentration lemmas in Section 3.2. are claimed to be novel by the authors.
- Regret guarantees are provided for both objectives. The main strength is Theorem 6 which analyses the regret of the adaptive pivot version of the algorithm and shows a regret bound that does not blow to $\infty$ in corner cases.

Experiments
- The numerical experiments section further demonstrates the performance improvement of AOA-RB over the state-of-the-art MNL-UCB algorithm. It highlights especially the benefits of the adaptive pivots.

Weaknesses:
Introduction, Related Works, and Contribution
- Certain claims are not supported, e.g., Line 21 ""Studies have shown that it is often easier..."" but it lacks citation which studies the authors refer to.
- I found some citations to be misplaced or non-supportive of the claims it is used for, e.g., [11] is used in Line 62 as a reference for Battling Bandits while it is a survey of dueling bandits. Similarly, citations [45, 46] are used for dueling bandits while they are only two examples from the literature. It would be great if authors could use consistent citations, e.g., surveys when they refer to broader literature and individual publications when specifics are important.
- Table 1 is provided for the comparison of regret guarantees but the authors do not describe it. It would be great if they could comment on the differences between the algorithms.

Problem Setting
- Limitations are not mentioned in the problem statement. For example, how restrictive is the Plackett-Luce model, and whether the approach could be extended to other models? I see that it is mentioned in Remark 1 but could be commented on in Section 2 as well.
- Both Top-m and Wtd-Top-m consider the (weighted) utility optimization problem. However, for most of the applications used as motivation, e.g., assortment optimization and recommender systems, the utility of the user which dictates the selected feedback, and the utility/profit of the subset selection (platform) are misaligned. Could the authors comment on how to formulate these problems in their setting?

Algorithm
- The $argmax_{S\subseteq [K], |S|\leq m}$ optimization is non-trivial and could be computationally expensive for large values for $K$.
- The authors claim that AOA-RB is practical, efficient, and optimal. While the theoretical analysis supports the last two claims, I struggle to find the intuition behind the algorithm. Could the authors elaborate further on this point?

Experiments
- Numerical experiments demonstrate performance only in synthetic data. Given the clear application and motivation of the paper, I would like to see experiments that reflect these problems.
- I recommend the authors to use larger figures. Axes and titles are hardly visible in the printed version.
- Only one baseline is considered. It would be appreciated if the authors could include the other algorithms mentioned in Table 1 for numerical comparison besides the theoretical one.

While the paper is easy to read and follow even for readers not familiar with all the works in the area, the inconsistent citations and unsupported claims have to be addressed before the paper would reach publication standards.

Limitations:
Limitations are mentioned in the paper, however, it is often not directly connected, e.g., the assumption of the PL model is only addressed in Remark 1. I would suggest the authors address limitations more clearly when they appear for easier readability.
The work is mainly theoretical without any immediate direct societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors consider the online MNL assortment optimization problem, where the goal is to learn MNL parameters while suggesting assortments, with the goal of either learning the top-m highest utility items or learning the maximum revenue set with m items. They use a UCB-based approach on pairwise win rates to get a UCB for utilities, which can then be fed into a traditional assortment optimization algorithm. The authors show this approach achieves asymptotically optimal regret and does not require assumptions used by previous approached. The basic algorithm relies on comparisons between each item and the no-choice option, but they also introduce a more sophisticated adaptive pivot approach that works better when the no-choice option is rarely selected. In experiments on synthetic data, their assortment optimization approach performs significantly better than the previous state-of-the-art.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem studied is natural and important. 
2. The presentation is generally clear.
3. The technical quality seems good, although I cannot attest to the correctness of all the proofs in the appendix.
4. The UCB approach on pairwise win rates is clever and appears original.

Weaknesses:
1. The algorithms and proofs could use some additional description/intuition. Some of the steps in the proofs take rather large leaps.

Limitations:
I think the limitations of the paper were adequately stated

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the problem of active online assortment optimization problem with preference feedback, which has been extensively studied. The paper argues that the previous studies have some unrealistic assumptions such as: there is a ‘strong reference’ which is always included in the choice sets; the same assortments can be repeatedly selected. Without these assumptions, they propose some efficient algorithms for the problem of regret minimization in assortment selection with Plackett Luce (PL) based user choices.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper proves the regret bounds of the proposed online learning algorithms. The regret bounds are proved based on some concentration guarantee for estimating the score parameters of the PL model using ‘Pairwise Rank-Breaking’.

Weaknesses:
1.  I cannot fully understand the motivation of the paper. The paper says that two major drawbacks of the previous studies include: the existing algorithms assume that the ``no-choice’’ option is stronger than the other choices, and they may query the same set of items for multiple times. It seems that the focus of the paper is to address these drawbacks. However, I think that these ``drawbacks’’ may not be real. First, it is natural that most of the customers will not choose any product, so it is very reasonable to assume that no-choice option is stronger. Second, in the typical assortment optimization scenario where customers arrive online one by one, showing the same set of items to different customers for multiple times absolutely will not cause any problem.  So I think that addressing these ``drawbacks’’ has very limited value.
2.  The regret bounds proposed by the paper is actually K\sqrt{T}\log T. It seems that this regret bound is weaker than those of the previous studies such as [2] (at least by log factors on T). The authors may argue that their bounds are better when \theta_{max}\rightarrow \infty, but this depends on the assumptions made on specific application scenarios, which is questionable as explained in my last comment.
3.  The experiments are conducted using some specific values of \theta and hence are not very convincing. I think that more experiments on more applications are necessary to demonstrate the superiority of the paper.

Limitations:
see the above

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the problem of active assortment optimization in MNL model.

In the problem of assortment optimization, we have  a large universe of products i=1,2,\dots N, each of which generates a given revenue r_i for the seller.  In MNL model  each product i has a value \theta_i to the customers and when customers are offered a subset of products they choose each item (including the no-choice option) with a probability proportional to their value. We also assume there is a no-choice option with revenue 0. The seller’s objective is to identify the assortment of products which generates maximum expected revenue. 

In the active version of the problems, the values of items  \theta_1, \theta_2,\dots , \theta_N, are not known to the seller. Thus, the seller  shows a  subset of items from the universe to the customers at  rounds 1,2, \dots ,T  and estimates \theta_i s  based on the observations. After approximating these values, the seller may solve the problem in static setting and find the optimal assortment.  This strategy is known as exploration and exploitation. 

In active assortment optimization, the objective is to minimize the regret of the algorithm which is defined as the summation over rounds t=1,2,\dots T the difference of the expected revenue in each round from the optimal revenue.

Prior works for instance [2] provided an algorithm for this problem by estimating at each round a high probability upper bound for the values \theta_i,  and then solve the static problem using the upper-bounds. In [2] the authors assume that \theta_0 (the value assigned to no-choice option and thus its probability ) is the highest among all items. 

The submitted manuscript claims that they provide an algorithm with a similar regret bound to [2] which does not have the restriction of assuming the no-choice option has the highest value. Their suggested approach is similar to that of [2] (finding high probability upper bounds for the parameters) but it is hard to follow all details of obtaining the upper bound and how it removes the restriction imposed on the value of the no-choice option. 

The result, if true, is interesting but I found the paper hard to read and got lost in section 3.1. I think that the paper will benefit greatly from rewriting and improving the presentation. 

I will detail my confusions as follows: 

- In Equation (3) on line 173 there is a variable x which is not defined up to this point. I understand that x appears to bound the probability of error in Lemma 1. But you have to introduce it before you use it the first time. 
- Between line 176 and 177 what is the + sign on the denominator of the equation? you use this notation again in another equation between lines 252 and 253.
- In equation 3 you show an upper bound on \hat{p_ijt} which then turns to a bound on \theta_i s. But in Lemma 1 you have shown a different upper bound for \theta_i. Can you explain the connection of these two bounds. 

A few minor typos:

Lemma 1. atleast-> at least
^ucb is sometimes with roman font and sometimes normal font. 




[2] Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Mnl-bandit: A dynamic learning approach to assortment selection. Operations Research, 67(5):1453–1485, 2019.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
- The problem of active assortment optimization is a fundamental problem in revenue management 
- The result is interesting if correct, as it removes an important restriction from prior algorithms.

Weaknesses:
- The results are poorly presented and it is hard to follow the paper. The paper lacks an explanation of main intuitions . 
- The technique seems to be similar to [2] as both papers obtain high probability upper bounds for the parameters and then solve it in an static setting. An intuitive explanation of how the given different upper bound is obtained, why it is correct, and how it removed the restriction on no-choice option is not provided.

Limitations:
limitations are not discussed but there are several future directions that have been discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
zm1LcgRpHm;"REVIEW 
Summary:
This paper introduces a new method for time-series representation learning that enhances the modeling of non-adjacent segment dependencies. Specifically, the proposed method segments, shuffles in a learned manner and stitches the shuffled segments to combine with original time series. The proposed method is model-agnostic without adding significant parameter overhead and shows performance improvement across multiple classification and forecasting base models.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method permutes the original segments to better capture inter-relations between distant segments. It is model-agnostic and introduces minimal parameter overhead to the original model.

2. Extensive experiments on various base models for both classification and forecasting tasks demonstrate the effectiveness of the proposed method.

Weaknesses:
1. It it not clear how the sorting process, specifically the calculation of permutation $\sigma$ from $P$, is made differentiable.

2. The compared forecasting baselines such as Informer are no longer state-of-the-art methods. Adding more recent baselines such as Time-LLM, GPT4TS, DLinear, PatchTST would provide a clearer understanding of the proposed method's comparative benefits.

3. The basic assumption for S3 is that modeling non-adjacent dependencies is important. However, the paper lacks detailed case studies that demonstrate the specific types of non-adjacent dependencies effectively captured by S3, which are not addressed by existing models. Additionally, there is no case study to validate that the learned shuffling weights accurately represent these segment dependencies.

Limitations:
The paper mentions potential expansions into tasks like imputation and anomaly detection. Further details on limitations from the reviewer are discussed in Weaknesses and Questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a plug-and-play mechanism called Segment, Shuffle, and Stitch (S3) designed to enhance time-series representation learning in existing models. S3 operates by dividing the original sequence into non-overlapping segments and shuffling them in a learned manner that is optimal for the given task. It then reattaches the shuffled segments and performs a learned weighted sum with the original input to capture both the newly shuffled sequence and the original sequence. This proposed model can enhance the performance of specific models in classification and prediction tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is easily comprehensible and straightforward.

Sufficient experiments are conducted to confirm the effectiveness of the method.

Weaknesses:
Lack of comparative methods:
In fact, the proposed method seems to share the same spirit as data augmentation methods in the time series field[1-4]. Why hasn't any data augmentation method been compared?


Selection of baseline models:
The selected baseline model, Informer, seems somewhat outdated. Why not choose a more recent model, e.g., iTransformer[5] or PatchTST[6]?


Dataset for prediction task:
The author conducted experiments on three ETT datasets, but for prediction tasks, more datasets should be considered, e.g., traffic, electricity, and weather.


Time-Series Representation Claim:
 As the author pointed out, more tasks should be considered for time series representation learning.


[1]FRAUG: FREQUENCY DOMAIN AUGMENTATION FOR TIME SERIES FORECASTING  [2]Time Series Data Augmentation for Deep Learning: A Survey  [3]SimPSI: A Simple Strategy to Preserve Spectral Information in Time Series Data Augmentation [4]TOWARDS DIVERSE AND COHERENT AUGMENTATION FOR TIME-SERIES FORECASTING [5]ITRANSFORMER: INVERTED TRANSFORMERS ARE EFFECTIVE FOR TIME SERIES FORECASTING [6]A TIME SERIES IS WORTH 64 WORDS: LONG-TERM FORECASTING WITH TRANSFORMERS

Limitations:
See Weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new neural network design element which segments, shuffles, and stitches time series for improved representation learning. They evaluate their methods on forecasting and classification tasks, and show that S3 benefits some widely used baselines.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. To the best of my knowledge, the idea is novel, and fundamentally challenges and changes how to learn representations for time series data
2. The paper is well written and easy to follow
3. Experiments are well-designed, and results are promising

Weaknesses:
I have not found any major weaknesses in the methodology or experimental design. However,  I think that the paper might benefit from showing what the S3 module is actually learning. For example, the authors can include the segmented, shuffled, and stitched time series on a particular dataset as an example, along with the weighted time series (used as input to the model), and the original time series. This might provide some intuition as to how this design element improves predictive performance. 

I think there's always scope to improve experimental design. TS2Vec is a excellent choice for classification, but not for forecasting. I would recommend that the authors use methods such as PatchTST (transformer-based) or iTransformer, TimesNet (CNN-based), N-BEATs or N-HITS (MLP-based) etc. for time series forecasting. For classification, it would also be good to compare with fully supervised methods such as ResNet1D (see [1]). 

### References
[1] Ismail Fawaz, Hassan, et al. ""Deep learning for time series classification: a review."" Data mining and knowledge discovery 33.4 (2019): 917-963.

Limitations:
The authors have a very brief description of limitations of their study.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper paper introduces a new approach called Segment, Shuffle, and Stitch (S3) to enhance time-series representation learning. The method involves segmenting the time-series into non-overlapping parts, shuffling them optimally, and stitching them back together along with the original sequence.

Key contributions include:

- Proposing the S3 mechanism to improve time-series representation learning by dynamically reordering segments.
- Demonstrating that S3 can be integrated with existing neural architectures like CNNs and Transformers, resulting in significant performance improvements.
- Showing through extensive experiments that S3 enhances performance in time-series classification and forecasting tasks, with improvements up to 68%.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Code is available, making reproducing this paper easier.
- Paper is clear.
- Results appear good, when considered on the set of baselines and dataset picked by the authors.

Weaknesses:
- Tables 1 and 2 focus on the ETT datasets, which are only a (highly intra-correlated) subset of the common forecasting datasets: Electricity, Traffic, Weather, Illness...
- I see no mention of CoST in the results tables, despite being cited in the paper. This is usually a very strong baseline for contrastive approaches. Including it would certainly paint a more complete picture of the results landscape. On a related note this also applies to e.g. more recent transformer baselines. Informer is relevant, but also very far from state of the art.
- Error bars would help one better contextualize the results.
- The lack of an ablation study makes understanding the reason this works more complicated.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a simple but effective differentiable module that performs pre-processing to input multivariate time-series before being fed into any differentiable model for arbitrary task. The pre-processing involves segmenting, shuffling the segments and stiching them together. The novelty include making this seemingly discrete operations into a differentiable module. This simple idea yields significant improvement in performance of different kinds of models over variety of datasets.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The method is simple and easy to add to most deep learning models
2. The technical details are well-motivated and explained
3. The method also improves training efficiency and convergence time along with performance with very little increate in model complexity
4. Experimental results across different tasks are strong

Weaknesses:
1. Visualization and any qualitative study on the shuffling and segments generalted by S3 would greatly benefit the readers.
2. How well does it optimize transformer based models, especially those that already do segmentation like PatchTST since the attention module captures the relations all pairs of segments already?
3. Does the representations due to S3 generalize to multiple tasks at a time or do we need to retrain for each task?

Limitations:
1. Lack of understanding on the segment permutations generated and why they are better for the model performance atleast qualitatively

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
VrVx83BkQX;"REVIEW 
Summary:
The paper studies constrained policy optimization for the language model alignment problem. The authors propose a stepwise alignment method that involves two separate steps for fine-tuning a language model: first with a reward and second with constraints. Several advantages of the proposed method are illustrated compared to existing methods, such as simplicity, efficiency, and flexibility. In theory, the authors prove that the reward optimality gap and constraint violation are bounded, assuming linear reward/constraint functions. In the experiment, the authors demonstrate a practical implementation of the proposed method and show better performance than several existing algorithms.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The authors characterize the optimization properties of the safe RLHF, such as strong duality. Although these properties are from the CMDP literature, they are particularly useful for understanding safe RLHF problems.

- The authors exploit the constrained RLHF problem structure to show that an optimal policy can be obtained in two steps. This is a useful property, as it allows us to improve existing models with safety constraints by using standard unconstrained RLHF algorithms (e.g., DPO, KTO).

- The authors also provide a theory of optimality for the proposed method. Although the linear function approximation assumption is restrictive, this appears to be the first theoretical characterization of safe RLHF.

- Despite the proposed algorithm being ideal, the authors provide a practical implementation and test its performance in several variations. Better practical performance is demonstrated through comparison with existing safe RLHF methods.

Weaknesses:
- The motivation relies on the existence of a safety model that can be constrained. However, in practice, how to set a constraint on the safety model is not discussed. This can be challenging since a safety model is often inaccurate, and the safety threshold is unknown.

- The multiplicative structure of the optimal policy in Theorem 1 assumes an optimal Lagrange multiplier. However, the analysis of the optimal Lagrange multiplier is not provided.

- The proposed stepwise method uses any Lagrange multiplier, raising a question: if an approximation of the optimal Lagrange multiplier is used, should we expect to achieve a similar near-optimal policy?

- The provided analysis implicitly assumes that an offline dataset can be represented by reward/safety models. However, it is challenging to verify the quality of offline data in practice. What happens if the models can't be represented by the data?

- The provided theory assumes linear reward and constraint functions, which can be restrictive in practice.

- The choice of an optimal Lagrange multiplier is heuristic in implementation, which is not characterized in theory.

Limitations:
No

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents the SACPO, a method that optimizes LM policies by sequentially aligning them to maximize helpfulness and harmlessness in either order. By selecting appropriate hyperparameters, the method enables balancing these criteria according to contextual needs. The authors leverage DPO and KTO in various experimental settings to demonstrate the effectiveness of their method, outperforming the prior SafeRLHF approach. SACPO is backed up with strong theoretical validations.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
S1. SACPO is grounded in strong theoretical foundations, ensuring that the final policy is as effective as if it were optimized simultaneously for both objectives (Theorem 1). The use of the $\delta$-uncertainty quantifier to statistically bound errors in estimation adds a layer of reliability and predictability. By establishing that the error between the estimated and true functions is statistically bounded, SACPO provides a robust framework reliable within known limits. This is useful for safety guarantees of LM harmfulness.

S2. While the authors primarily focus on a single safety function to constrain LM harmfulness and do not empirically analyze scenarios with multiple safety constraints, they provide a theoretical framework that outlines how SACPO can be extended to accommodate multiple safety constraints. This facilitates further extensions of this work.

S3. Not explicitly mentioned by the authors, but after both optimization stages, if the necessity arises to further fine-tune the language model in either direction of helpfulness or harmfulness, then this option is available. For instance, if further data becomes available, the model is deployed in different contexts, or new requirements for helpfulness or harmfulness arise which can be incorporated into the prompt, then the optimization can continue in one of the directions to ensure that it remains effective and relevant.

S4. While the authors do not explicitly address further optimization after the initial stages, SACPO has the potential to facilitate further fine-tuning of the language model as needed. For example, if new data becomes available, the model is deployed in varying contexts, or evolving requirements for helpfulness or harmlessness emerge, the model can be further optimized. 

S5. SACPO supports combining multiple algorithms for maximizing helpfulness and minimizing harmfulness. While they currently utilize only DPO and KTO, more powerful optimization techniques could be adopted as they are developed. This flexibility enhances the potential to further improve the effectiveness of SACPO.

S6. The flexibility granted by selecting an appropriate Lagrangian multiplier $\lambda$, KL penalty $\beta$, as well as the mixing ratio $q$ for P-SACPO enables effective balancing of helpfulness and harmlessness to meet different contextual needs and specific requirements. Fixing $\lambda$ eliminates the need for iterative adjustments and thereby adds stability by avoiding the oscillations and instability encountered with dynamically optimizing primal-dual methods.

Weaknesses:
W1. The extent of the evaluation is not clearly defined, particularly in terms of the number and variety of prompts used for testing. This makes it challenging to assess the generalizability of SACPO.

W2. I don’t think the evaluation of helpfulness and harmfulness should uniquely be separated to different prompts and responses. Surely, the prompts used in SafeRLHF [1] are specifically selected to ‘trigger’ harmful responses from LMs with higher likelihood, and this is important to detect. However, the goal, ultimately, is to have the model generate responses that are simultaneously helpful and harmless. Therefore, it makes more sense to evaluate these two criteria in parallel on the same prompts and responses. Otherwise, the policy might learn to simply detect prompts that contain ‘triggering’ clauses, and proceed to output very safe but less helpful answers. Contrarily, if the prompt appears safe, the policy can freely generate a maximally helpful answer, while having very low potential of being ranked as harmful.

W3. The description of the training protocol involving the PKU-SafeRLHF dataset in the optimization of the base SFT policy using DPO/KTO methods lacks clarity. Specifically, it is not clear whether the same data from this dataset is employed across both optimization stages for helpfulness and harmfulness. Additionally, the duration of the training process is not mentioned.

W4. Although used in SafeRLHF [1], I don’t think that solely relying on LLMs to evaluate helpfulness and harmfulness on question-answering problems is a reliable or objective method. Note, that SafeRLHF also incorporated other methods of evaluation. 

W5. The method introduces several new hyperparameters that necessitate tuning or heuristic selection when trained for other contexts. The choice of optimization sequence, the KL penalty scaler $\beta$, the Lagrangian multiplier $\lambda$, and the mixing ratio $q$ complicate the setup.

[1] Dai, Josef, et al. ""Safe rlhf: Safe reinforcement learning from human feedback."" *arXiv preprint arXiv:2310.12773* (2023).

Limitations:
The authors have briefly addressed the limitations of their work. I have pointed out further limitations in the weaknesses section of this review. My main concerns are W2 and W4.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
From the perspective of safe reinforcement learning, the author formulates human value alignment as an optimization problem of the LM policy to maximize reward under a safety constraint, and then proposes an algorithm, Stepwise Alignment for Constrained Policy Optimization (SACPO).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The author introduces SACPO, an algorithm that effectively enhances the safety of LLMs. The theoretical derivations are solid, and the algorithm's effectiveness is demonstrated through extensive experimental settings.

Weaknesses:
1. Although the author included some descriptions of related work in the Preliminaries section, I did not find a dedicated Related Work section in the main paper. This omission significantly impairs readability. It is unclear why the author chose to exclude this section from the main paper. Given that the paper explores the safe alignment of llms from the perspective of safe reinforcement learning, two highly relevant areas of related work would be Safe Reinforcement Learning and Safety Alignment of LLMs.
2. Considering the point 1, I am curious about the relationship between the proposed SACPO algorithm and traditional safe reinforcement learning algorithms. I noticed that the author describes SACPO's two phases in Algorithm 1 as reward alignment and safety alignment. In traditional safe reinforcement learning, such as in the Constrained Update Projection Approach to Safe Policy Optimization, the algorithm's update logic is similarly divided into Reward Improvement and Projection (safety satisfaction). What are the connections between the proposed algorithm and traditional reinforcement learning algorithms, or the challenges and difficulties in extending traditional reinforcement learning algorithms to the LLMs setting? From a reviewer’s perspective, these points are worth including in the main text.
3. I am also working on LLMs Safety Alignment and appreciate the motivation behind this work. I observed that in the experimental section, the author aligns SACPO with DPO(H) and DPO(S), where the latter two are trained separately using the Helpful and Harmless dimensions from Beavertails. Since Beavertails' preference annotations are decoupled—helpfulness is annotated without considering safety—DPO models trained separately are naturally deficient in the corresponding dimensions. I would be interested to see a comparison between DPO models trained with a trade-off between helpfulness and harmlessness and SACPO. For example, how do they perform on datasets like PKU-SafeRLHF and PKU-SafeRLHF-single-dimension?
>https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF  
https://huggingface.co/datasets/PKU-Alignment/PKU-SafeRLHF-single-dimension

Limitations:
see above.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new alignment algorithm SACPO to improve the both helpfulness and safety (harmlessness) of language model. SACPO separates the two objectives into two alignment steps. The second step of optimization is equivalent to be an optimization with the policy from first step as the reference policy. Meanwhile, each step can be implemented with reward-free alignment methods (e.g., DPO, KTO). The authors compare the proposed method with SFT base model and show it can achieve higher win rate in terms of harmlessness and helpfulness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Overall, the paper is clearly written and easy to follow.
- The two steps optimization of SACPO can be achieved by reward-free alignment algorithm, which reduces the requirement of computation source and dataset (e.g., it also works for dataset which is not constructed by preference data). 
- The empirical results show that proposed method exceeds the baseline on helpfulness and harmlessness.

Weaknesses:
- Although the authors use P-SACPO as a remedy, it is still questionable that the proposed method cannot get the correct $\lambda^*$. For example, how to step a conservative starting point $\bar\lambda$, how to set the linear interpolation coefficient $q$.
- As a general alignment algorithm, the authors should test the performance on other datasets or different models.

Limitations:
The authors have mentioned several limitations of this work in Sec.8. One another limitation is that the proposed method only works for the KL divergence regularizer ($D_{KL}[\pi(\cdot|x)\|\pi_{ref}(\cdot|x)]$) as discussed in Remark 1.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses the challenge of fine-tuning a language model (LM) policy to maximize reward while adhering to safety constraints. Building on the concept of Safe RLHF, which introduces a constrained safe RL paradigm for aligning LLMs, the authors propose a novel approach: Stepwise Alignment for Constrained Policy Optimization (SACPO). Unlike the traditional method that simultaneously balances reward and safety optimization, SACPO adopts a stepwise approach, first aligning the LLM for reward and then for safety, or vice versa. They also present a practical variant called P-SACPO, which leverages model merging techniques. Empirical results demonstrate the superiority of SACPO over baseline methods such as Supervised fine tuning (SFT) and Safe RLHF.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Merites of the proposed method (SACPO): (1) simple, stable, and computationally efficient compared to Safe RLHF; (2) compatible with different alignment algorithms (DPO, KTO, and IPO) and datasets; (3) it has solid theoretical grounding.

Weaknesses:
Experiments on additional datasets could strengthen the paper's findings.

Limitations:
In the conclusion, the authors acknowledge the limitations of their work and the potential societal impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
