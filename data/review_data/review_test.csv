uS9RZH6K65;"REVIEW 
Summary:
This paper proposes a denoising framework to alleviate the influence of noisy text descriptions on open-vocabulary action recognition in real scenarios. A comprehensive analysis of the noise rate/type in text description is provided and the robustness evaluation of existing OVAR methods is conducted. A DENOISER framework with generative-discriminative optimization is proposed. The experiments demonstrate the effectiveness of the framework.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The robustness to noisy text descriptions/instructions in real-world OVAR applications is an interesting and meaningful problem.
- The evaluation of the robustness of existing OVAR methods when facing the noise text description input is valuable to the community.
- The motivation is clear and the overall framework is technically sound.

Weaknesses:
- About the experiments,
    - The reviewer thinks that the most convincing results are the Top-1 Acc of existing OVAR models under the Real noise type. However, in Table 1, the proposed model does not demonstrate much superiority compared to GPT3.5's simple correction. The reviewer worries about the research significance of this problem. Will this problem be well resolved when using more powerful GPT4/GPT4o with some engineering prompt designs?
    - In Table 2, I would like to see the performance of other correction methods (e.g., GPT3.5/4/4o) for a more comprehensive comparison.
    - Since this work focuses on the noise text description problem in OVAR, it is necessary to demonstrate the results of those CLIP-based methods without any additional textual adaptation (e.g., the vanilla CLIP).


- About the method,
    - The reviewer thinks that the overall model design is reasonable and clear. However, the method part introduces too many symbols which makes the paper very hard to follow. It is unnecessary to over-decorate the technical contributions.

- Minor issue,
    - The authors seem to have a misunderstanding about the OVAR setting (Line 113). In OVAR, the model is evaluated on both base-classes and novel-classes during testing. In this case, all action classes from the UCF/HMDB datasets can be used for testing when the model is trained on K400, as there are many overlap classes between K400 and UCF/HMDB.

Limitations:
The limitations are discussed and there is no potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tackles the challenge of noisy text descriptions in Open-Vocabulary Action Recognition (OVAR), a task that associates videos with textual labels in computer vision. The authors identify the issue of text noise, such as typos and misspellings, which can hinder the performance of OVAR systems. To address this, they propose a novel framework named DENOISER, which consists of generative and discriminative components. The generative part corrects the noisy text, while the discriminative part matches visual samples with the cleaned text. The framework is optimized through alternating iterations between the two components, leading to improved recognition accuracy and noise reduction. Experiments show that DENOISER outperforms existing methods, confirming its effectiveness in enhancing OVAR robustness against textual noise.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper aims to study a new research topic, i.e., achieving robust open-vocabulary recognition performance with noisy texts. This direction has not been investigated before, which seems to be applicable in real-world applications.

- The proposed intra-modal and inter-modal methods are intuitive and demonstrated effective in the experiments. 

- The experiments show that the proposed method is effective with different network architectures (XCLIP and ActionCLIP), which verifies that the method can be widely used.

Weaknesses:
- The baseline models are outdated and not tailored for OVAR. The authors failed to reference recent OVAR papers such as OpenVCLIP[1] (ICML 2023), FROSTER (ICLR 2024), and OTI (ACM MM 2023).

- In Table 1, it is evident that the proposed method outperforms GPT-3.5. Additionally, the authors present examples in Table 4 to demonstrate the superiority of the proposed method over GPT-3.5. However, upon personal experimentation with all the examples from Table 4 using the provided prompt from the paper (lines 243-245), I observed that the GPT-3.5 model successfully rectified all issues, including challenging cases where the proposed method fell short. As a result, I remain unconvinced by the findings.

This is the prompt given to GPT-3.5 model, and I hope other reviewers can also try it on their own:

The following words may contain spelling errors by deleting, inserting, and substituting letters. You are a corrector of spelling errors. Give only the answer without explication. What is the correct spelling of the action of  “cutting i aitnchen”.


[1] Open-VCLIP: Transforming CLIP to an Open-vocabulary Video Model via Interpolated Weight Optimization.

[2] FROSTER: Frozen CLIP Is A Strong Teacher for Open-Vocabulary Action Recognition.

[3] Orthogonal Temporal Interpolation for Zero-Shot Video Recognition.

Limitations:
Yes, they addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper deals with the problem of Open-Vocabulary Action Recogniton (OVAR). Specifically, it focuses on the issue that the action labels provided by users may contain some noise such as misspellings and typos. The authors find that the existing OVAR methods' performance drops significantly in this situation.  Based on this analysis, they propose the DENOISER framework to reduce the noise in the action vocabulary.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is generally well-written and easy to follow. 
2. The framework is well presented and explained.
3. The experiments show the effectiveness of the denoising process.

Weaknesses:
1. This paper actually focuses on text denoising and does not involve any specific action recognition technology. The author just chose the field of OVAR to verify the effectiveness of the proposed text-denoising method. The title is somewhat misleading. I think the author should regard text-denoising as the core contribution of the article instead of the so-called ""robust OVAR""
2. The article focuses on too few and too simple types of text noise, including only single-letter deletions, insertions, or substitutions. These kinds of errors can be easily discovered and corrected through the editor's automatic spell check when users create a class vocabulary. This makes the method in this paper very limited in practical significance.
3. , The proposed method, although a somewhat complex theoretical derivation is carried out in the article, is very simple and intuitive: that is, for each word in the class label, selecting the one that can give the highest score to the sample classified into this category among several words that are closest to the word.  There is limited novelty or technical contribution.

Limitations:
The author states two limitations of the work in the paper.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper addresses the challenge of noisy text descriptions in  Open-Vocabulary Action Recognition. It introduces the DENOISER  framework, which combines generative and discriminative approaches to denoise the text descriptions and improve the accuracy of visual sample  classification. The paper provides empirical evidence of the framework's  robustness and conducts detailed analyses of its components.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written and the content is easy to understand. 
2. The motivation presented by the authors is clear, the label noise problem does exist in video datasets.
3. The authors show the types of noise and their percentage, in addition, the authors verify the validity of the proposed method through comparative experiments.

Weaknesses:
1. As the authors state in the limitations section, textual description noise does exist, but it can be corrected with an offline language model, what are the advantages of the authors' proposed approach?
2. I would assume that the text noise problem presented in this paper is even worse on large video datasets collected by semi-automatically labeled networks, e.g., Panda70M, howto100M, and InternVid. I suggest that the authors might consider validating their ideas on these datasets.

Limitations:
The authors have provided a limitations analysis in their paper and I have suggested some limitations in the Questions section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
6Wm4202Wvc;"REVIEW 
Summary:
The paper revisited the problem of label leakage in split learning in the context of fine-tuning large models with parameter-efficient training. Based on modern use cases, they proposed two privacy-preserving protections for gradients and activations during split learning. The proposed methods are evaluated on several large models including Llama2-7B, fine-tuned using LoRA and full model fine-tuning.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper moved forward a step on the urgent need to privacy-preserving split learning over large models and fine-tuning with LoRA.
2. The writing is generally good despite some minor issues. The flow of ideas is clear.
3. The proposed method is evaluated over different pre-trained large models, conforming to current real-world use cases of LLMs.

Weaknesses:
1. There exist several related works discussing the attacks and defense regarding the label leakage in split learning. The authors may need to compare the differences between the proposed methods and previous literature. The evaluation part lacks the comparison to existing privacy-preserving solutions over label leakage and some trivial solutions such as directly applying differential privacy, which is easy to implement.
2. In modern use cases of API fine-tuning, apart from the applications of classification, text generation with LLMs and image generation with multimodal models and diffusion are more common cases. And it is very critical to protect labels in these applications. For example, labels in text generation can contain answers to private questions in the private dataset. However, the leakage study and proposed privacy-preserving methods do not apply to these applications.
3. There are some minor writing issues that could be improved. For example, content introducing API fine-tuning and potential privacy concerns can be shortened in the introduction. The paragraph from line 53 to 58 can be reorganized so that it won't leave '[18]' for a whole line. Same thing for line 209. On line 28, write the full name of LoRA before using acronym. The authors are suggested to talk about split learning and no need to raise extra efforts for readers to understand what vertical federated learning is.

[1] Wan, Xinwei, Jiankai Sun, Shengjie Wang, Lei Chen, Zhenzhe Zheng, Fan Wu, and Guihai Chen. ""PSLF: Defending Against Label Leakage in Split Learning."" In Proceedings of the 32nd ACM International Conference on Information and Knowledge Management, pp. 2492-2501. 2023.   
[2] Kariyappa, Sanjay, and Moinuddin K. Qureshi. ""ExPLoit: Extracting private labels in split learning."" In 2023 IEEE conference on secure and trustworthy machine learning (SaTML), pp. 165-175. IEEE, 2023.     
[3] Erdoğan, Ege, Alptekin Küpçü, and A. Ercüment Çiçek. ""Unsplit: Data-oblivious model inversion, model stealing, and label inference attacks against split learning."" In Proceedings of the 21st Workshop on Privacy in the Electronic Society, pp. 115-124. 2022.      
[4] Xu, Hengyuan, Liyao Xiang, Hangyu Ye, Dixi Yao, Pengzhi Chu, and Baochun Li. ""Permutation Equivariance of Transformers and Its Applications."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 5987-5996. 2024.

Limitations:
Discussed in the last section.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This study addresses the privacy concerns associated with the fine-tuning of Large Language Models (LLMs), focusing on SplitNN. It explores how gradients and activations can leak data, potentially allowing attackers to reconstruct original data sets. In experiments, the proposed method reduces label leakage while maintaining minimal utility loss.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
S1. The manuscript highlights significant privacy issues in LLM fine-tuning, specifically the potential for data leakage through gradients and activations in SplitNN.

S2. Experimental results show that the proposed method significantly mitigates label leakage with minimal impact on utility.

Weaknesses:
W1. The claim that backpropagation is ""conditionally linear"" is not sufficiently rigorous. The manuscript suggests that $\text{backprop}(x, \theta, g_h+z)+\text{backprop}(x, \theta, g_h−z) = \text{backprop}(x, \theta, g_h)$ under the assumption that $\theta$ is constant. However, $\theta$ updates during each backpropagation, invalidating this assumption. Moreover, swapping the order of $\text{backprop}(x, \theta, g_h+z)$ and $\text{backprop}(x, \theta, g_h−z)$ could lead to different outcomes. Formal proof and a clearer statement of assumptions are needed to substantiate this claim.

W2. Section 3.4 describes a method to protect activations that resembles secure multi-party computation [1], lacking novelty. Its effectiveness is also questionable when only one adapter is present.

W3. The proposed protection mainly focuses on labels. In practice, data such as personal identifiers may pose a greater risk than labels. For example, knowing (a) Alice's salary (label) is included in the database is considered a more serious leakage than knowing (b) someone earns a salary of 3.2k. The manuscript should explore if the proposed method can also protect other sensitive features.

**References**

[1] Du, Wenliang, and Mikhail J. Atallah. ""Secure multi-party computation problems and their applications: a review and open problems."" Proceedings of the 2001 workshop on New security paradigms. 2001.

Limitations:
There elaboration on limitations is insufficient.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses privacy leakage during API-based Parameter Efficient Fine-Tuning (PEFT). Their designed P3EFT is a multi-party split learning algorithm that leverages PEFT adjustments to uphold privacy with minimal performance overhead. Their method proves competitive in both multi-party and two-party setups while achieving higher accuracy.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Researching API-based fine-tuning for large models is an intriguing topic, especially considering that many clients face challenges loading such large models due to size and computational constraints. In this scenario, privacy concerns regarding client data become paramount. This paper aims to mitigate potential privacy leakage by obfuscating gradients and parameters communicated during transmissions.

Weaknesses:
Their approach shows limited privacy improvement compared to the scenario  Without LoRAs, as indicated in Tables 1, 2, and 3, thereby restricting the overall benefits.

Limitations:
The paper addresses limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an algorithm to preserve the label privacy while achieve good accuracy in the split learning regime. The algorithm is used in parameter-efficient fine-tuning and empirically tested on some language models.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper clearly presents its motivation and contribution. The modification on the back-propagation is reasonable and empirically effective across three models and different attacks that are tested

Weaknesses:
The main concern is the scalability of this method. For one iteration, the number of backpropagation is m (at least 2), which is too slow even for PEFT. The computation cost of PEFT is 2/3 of full training so if m=2, the total cost of this method is 4/3 of full training.

Looking at the code, there are 7 hyperparameters introduced by this method, which may be hard to use in practice. I would suggest the authors fix some hyperparamters that the algorithm is robust to, to reduce the number of tunable hyperparameters.

Also the experiment results on SST2 show a severe leakage around 10% compared to without LoRA (even though this is relatively weaker than other methods).

Limitations:
As in its current presentation, the method is limited to language models, split learning (two parties), and label privacy. The empirical evidence is limited to text classification (specifically, this method does not apply to natural language generation where LLAMA is originally trained for) and LoRA. Each limitation can be relaxed, e.g. extending to vision models, data reconstruction, additional PEFT, etc.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
qL4nN6Ew7U;"REVIEW 
Summary:
The paper proposes Fantasy, a T2I model based fully on transformers (except for the VQGAN for the latents encoding and decoder):
* A __fine-tuned LLM__ (based on Phi-2) for the text encoding
* A image generator based on the MIM (Masked Image Modelling) approach

The training happens in two stages, a generic stage for aligning the generator the the frozen Phi-2 features, followed by a fine-tuning stage where the Phi-2 encoder is fine-tuned alongside the MIM transformer.

The results on human evaluations are convincing, putting Fantasy alongside models that require larger computational resources, while the FID results are less convincing (due to the image being smooth according to the authors).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novelty:
* The LLM is __fine-tuned__ but only in the second stage of training, this approach is new and makes sense

Accessiblity:
* The 2 stage pre-training is already standard practice
* The Phi-2 model is available, it is likely that this approach works for other available models (Phi-3? It could be interesting to test)
* The model size allows the model to be trained in a reasonable time

Weaknesses:
Performance:
* The FID scores are not competitive and the authors describe why: the image are smooth => it seems that the human evaluations still rank Fantasy at the top on visual appeal, but it might be that if the question was ""visual realism"" they might prefer a different model
* Results are available for 256px, and a 600M parameters MIM generator, there is no proof that this method scales (we know that diffusion models based on UNet have trouble scaling for instance)

Limitations:
n/a

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an efficient text-to-image generation model that integrates LLM and MIM. It demonstrates that MIM can achieve comparable performance. Unlike commonly used text encoders like CLIP and T5, this study introduces an efficient decoder-only LLM, phi-3, achieving better semantic understanding. The effectiveness of the method is validated through a newly proposed two-stage training approach and sufficient experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written with clear logic.
2. The use of MIM and LLM for image generation introduces a novel approach.
3. The two-stage training method improves the generation results.

Weaknesses:
1. The quality of the generated images does not yet match that of existing methods (e.g., pixart-alpha, SDXL), with some loss of detail. This is noticeable from the comparison in column B of Figure 5.
2. Some aspects of the methodology could be clearer, and the overall coherence of the approach could be strengthened.
3. While the proposed method demonstrates efficiency advantages, particularly in faster training convergence, this can be influenced by various factors. However, the related experiments in the paper could be more comprehensive.
4. The semantic accuracy of the generated images, a potential strength of Fantasy, is not fully demonstrated in the paper. For instance, the model's ability to handle prompts with multiple entities, color attribute descriptions, or retaining key elements in long text inputs is not adequately showcased.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a technique for training transformer based masked image modeling in an efficient way. Two main contributions include (1) use of a LLM decoder as text embeddings, and (2) Two-stage training strategy for MIM models. Experimental results show good generation quality.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The use of LLMs as text encoders seem interesting.
- Two-stage training approach makes sense. First, the use of pretraining data helps the model learn a general text-image model, and the high quality alignment data can improve the quality of generations.
- Training models on low resources seem appealing.

Weaknesses:
- I don't see anything new proposed in this paper. The authors simply use Phi-2 model as text encoder with MIM models, and use two-stage training. 
- Even two-stage training is not something new to image synthesis. People have been doing aesthetic finetuning to improve image quality in diffusion models (eg. stable diffusion). The authors extend this to instruction-image data.
- The quality of generated images are not very impressive. When zoomed in, we notice a lot of visible artifacts. The generated images are also flat and doesn't have a lot of details.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To develop a resource-efficient, high-quality image generator for long instructions, the authors presented Fantasy, an efficient T2I generation model that integrates a lightweight decoder-only LLM and a transformer-based masked image modeling (MIM). 

They demonstrate that with appropriate training strategies and high-quality data, MIM can also achieve comparable performance.

By incorporating pre-trained decoder-only LLMs as the text encoder, they observe a significant improvement in text fidelity compared to the widely used CLIP text encoder, enhancing the text image alignment. 

Their training includes two stages: 1) large-scale concept alignment pre-training, and 2) fine-tuning with high-quality instruction-image data. 

They conduct evaluation on FID, HPSv2 benchmarks, and human feedback, which demonstrate the competitive performance of Fantasy against other diffusion and autoregressive models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The author proposed a T2I framework that combines several more recent components and performed a series of comparisons, including both quantitative and human evaluations.

Weaknesses:
- the major concern of the work is unclear contributions. The claimed three contributions or core designs are quite similar with existing works.
- Efficient T2I network: there is no justification about why the network is “efficient”. Simply adopting a smaller LLM like Phi-2 can hardly be claimed as efficient network design. 
- The hierarchical training strategy was also proposed before, it is not clear what is the difference with existing work.
- High quality data: the training data utilize Laion-2B and use existing filtering strategy. The collection high quality synthesized images from existing datasets.
- The evaluation metrics are mainly based on HPSv2, which has a limited range of values, e.g., HPSv2 has close values for SDv1.4 and SD2.0, e.g., 27.26 vs 27.48. Why SDXL is missing in Table 1?
- The author acknowledged that their model lags behind diffusion-based models in visual appeal, limited by the 8K size of VQGAN’s codebook and not targeting visual appeal. However, there is no solution or further study for solving this problem, which limits the scalability of the model.
- The scaling study in section 4.2 seems pretty premature and it is unclear what is the limit of the scaling. Increasing the model depth can improve the performance, which has been verified from previous work such as in https://arxiv.org/abs/2212.09748 or https://arxiv.org/abs/2404.02883.

Limitations:
I would encourage the authors to emphasize about the core contributions rather than combining everything together, which can hardly show significant performance improvement over existing public models.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
wZ5kEOCTce;"REVIEW 
Summary:
This paper reveals the role of inter-patch dependencies in the decoder of MAE on representation learning. The paper shows that MAE achieves coherent image reconstruction through global representations learned in the encoder rather than interactions between patches in the decoder. Based on this, the authors propose CrossMAE, which only utilizes cross-attention in the decoder.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The approach of analyzing the reconstruction process through self-attention between mask tokens and cross-attention between mask and visible tokens is intriguing.
- The writing is clear and easy to follow, with main messages that are solid and insightful.

Weaknesses:
1. Idea/Novelty
- The claim that MAE reconstruction is achieved through global representation learning within the encoder rather than interactions between patches needs more support. Recent studies linking MAE to contrastive learning have found that the receptive field of specific mask tokens in the decoder is relatively small. Could the role of mask tokens in the decoder be to capture local area information? This might explain the smaller attention magnitude of masked tokens compared to visible tokens in Figure 1(b). 
- There is a concern that without self-attention (i.e., with the proposed method), the observation that authors made on the vanilla MAE may no longer be valid. Additional explanation on this point is necessary as this observation is the main motivation for suggesting CrossMAE.

2. Additional justification
- Effectiveness of using a subset of mask tokens as queries: Unlike the traditional architecture, this method uses only a subset of mask tokens as queries. Detailed analysis and interpretation are needed on why this is effective. 
- Performance differences when using the entire set of mask tokens versus a subset (and what percentage of mask tokens is used) should be reported.

3. Experiment
- For a fair comparison, CrossMAE's performance should be evaluated using the same setting as the original MAE, especially regarding the fine-tuning recipe.
- The current experimental results do not convincingly demonstrate the effectiveness of the method. For classification tasks, only the linear-probing and fine-tuning results on IN1K are reported. Following the previous works, classification on various downstream datasets should be also considered.
- For generalizability, evaluation on another task like semantic segmentation (e.g. on ADE20K) would be useful to verify that the suggested method learns the generalizable feature representation.

Limitations:
The authors have not discussed limitations of this work except for the very last sentence of section 5, indicating that they have discussed limitations in this section in the questionnaire #2. It is strongly recommended to disclose more detailed limitations of the proposed work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel pre-training approach called CrossMAE. Instead of concatenating the masked and visible tokens for the decoder, the authors add cross-attention to decode the masked tokens by using them and the visible patch embeddings as separate inputs to the decoder. Further, the authors introduce a method to only partially reconstruct the masked patches, and leverage inter-bock attention to fuse feature across different layers.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well motivated through a practical observation
- The authors propose a useful technical contribution which seem intuitive given the described observations
- The paper is well written and technically sound
- All visualizations provide additional value, I especially like Figure 5. It describes the effect of the contributions well
- Judging from the experiment section, the presented approach mostly improves over the vanilla MAE and other MAE-like follow-up works

Weaknesses:
- I feel like the paper is missing a more structure ablation of the individual contributions. I think the paper would benefit from having a simple table where all contributions are added sequentially to better identify the performance effect of the individual contributions as in:
	MAE X.X
	+ Cross-Attn X.X
	+ Partial Reconstruction X.X
	+ Inter-Block Attn X.X
- As can be observed from Table 3 c), the final setting (underlined) of the prediction ratio, 0.75, turns out to be exactly the same as the optimal masking ratio, 0.75. If I understood correctly, this means that in practice, CrossMAE works best when it predicts all tokens that were masked, not just a fraction of them. Only predicting part of the masked tokens was previously listed as a contribution. Therefore, I don’t understand how this additional hyper parameter provides any benefit for better downstream performance. Maybe I’m missing something and this be cleared up by answering the previous point.
- All models are only trained for 800 epochs. The original MAE reaches peak performance at 1600 epochs. For a thorough comparison, it would be necessary to also train CrossMAE for 1600 epochs and see if the performance gains sustain, or if performance has peaked at 800 epochs.
- Table 1 is missing the CrossMAE ViT-H with 75% masking ratio
- Contribution 2 and 3 don’t seem to be as well motivated in the introduction in comparison to Contribution 1
- Better performance is listed as a contribution. IMO this is not a contribution, rather a result of the technical contributions

Limitations:
The authors have sufficiently addressed the limitations of their approach.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents CrossMAE, a methodology for improving pre-training efficiency over that of MAE for an encoder. The paper motivates its approach by presenting visual evidence that, in standard MAE pre-training, masked tokens attend to other masked tokens significantly less than to non-masked (aka, visible) tokens. Using this motivation, the paper then presents CrossMAE, which differs from MAE largely in that it replaces the MAE self-attention with cross-attention between the masked tokens and a learnable weighted combination of the encoder feature maps. This aspect decouples queries from keys and values (which is not the case in MAE), which the paper then exploits to allow only some (but not necessarily all) mask tokens to be used during reconstruction to pre-train the model. The paper presents an analysis of which encoder block features are optimal to cross attend with each decoder block, and it presents ablation studies on multiple design decisions. Finally, it presents visual and fine-tuning results showing comparable performance to MAE and similar methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper motivates CrossMAE well by showing evidence of a potential inefficiency in MAE (self-attention) and then presenting an approach to remedy it (cross attention). I particularly like how the paper delves even deeper, though: instead of stopping at the level of replacing self-attention with cross-attention, it then points out that this choice allows for a significantly fewer number of masked patches to have to be reconstructed, which reduces flop count significantly. The ablations in Table 3 are fairly thorough and answered some questions I have developed. The performance of CrossMAE appears comparable to other SOTA methods but with significantly more efficient pretraining.

Weaknesses:
1) In Fig 1b, IIUC, for one particular mask token, the two $\mu$'s are the respective attention values averaged over all transformer blocks and all masked/non-masked tokens. If this is the case, my concern is that by averaging over all transformer blocks, variations in the attention is being hidden. Naively, I would think that for early blocks, the attention due to masked tokens would be small (as the paper concludes) but becomes larger for the later blocks (since now the masked tokens have actual useful signal in them). Did you consider this?

2) I do not follow why CrossMAE does not need an MLP at the end to convert final decoder tokens back to raw pixels. Line 218 says that the inputs to the first encoder block are included in the feature maps for cross attentions. Does this cause a final MLP to not be used?

3) Less critical:
  3a) Fig 1b should point the reader to Section A.1. I spent much of my reading confused about what $\mu$ is.
  3b) Fig 4a should have a different number of decoder layers than encoder layers. When I saw this figure, I immediately wondered why a decoder block wasn't being paired with feature maps from its ""partner"" encoder. I had to wait until lines 204-207 to get an explanation of why this doesn't work.
  3c) Line 187 references a ""second question"" in Sec 3.1, which doesn't exist as far as I can tell.
  3d) Fig 4a shows the ""vanilla"" version of Cross MAE, where the final encoder layer feature maps are attended with all decoder layers. But the paper presents results exclusively (?) on the version that uses a learned combination of the feature maps. Anyway, the figure confused me. Maybe I just didn't understand what the solid arrows vs dotted ones are supposed to represent.

Limitations:
No weaknesses are specifically addressed. But as this paper is essentially an optimization to MAE, I'm not sure this question is relevent.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
TJJ4gZtkS4;"REVIEW 
Summary:
This paper proposes new methods to count the number of linear regions in neural networks by viewing them as tropical Puiseux rational maps. By computing their Hoffman constant, the authors are able to identify a sampling radius which ensures that all the network’s linear regions will be intersected. They use this insight to propose algorithms for counting the number of linear regions for both invariant and traditional networks.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper is well-written with virtually no typos and errors. The technical content is accessible and not unnecessarily convoluted and the proofs and concepts are presented clearly.

Weaknesses:
The main weakness of the work, in my opinion, can be summarized in the following points:

- the connections to tropical geometry and group theory are not rigorous beyond the point of simple notational fixes
- the motivation for the work and how it fills gaps in the existing literature is unclear, and
- the effective utility of the approach is not convincingly demonstrated by the theory or experiments.

**Rigor of tropical and group theories**

I believe this point is the biggest weakness of the paper. From the perspective of tropical algebra, vectors and polynomials live in $\bar{\mathbb{R}}$. The authors mention $\bar{\mathbb{R}}$ in line 87, but then this is never used in most of their work. This might seem as a notational fix, but it is not, as it introduces problems in virtually every single result in the paper. This first becomes a real issue in (4), where maximums are taken over, potentially, $\infty$. How is it guaranteed that (4) exists in the context of tropical algebra? This is a recurring problem that appears in (5), (6), and (7). Another important issue at the intersection of group theory and tropical algebra is how groups are defined. Semirings, by construction, are objects that do not admit additive inverses. This means that, if one wants to define groups on such structures, great care needs to be taken as to how groups are defined, how they act on vector spaces, and what groups are actually permissible in this context.

From the perspective of group theory how is the group action defined? How do the group elements act on vectors in tropical spaces? Group representations $\rho: G \to \operatorname{GL}$ require the concept of an invertible matrix, however that concept is ill-defined in tropical vector spaces.

(Moreover, the authors define incorrectly $\bar{\mathbb{R}}$. The infinity element needs to be the identity element of tropical addition: if one opts to use the max-plus semiring, $-\infty$ should be used. If we use the min-plus semiring, $\infty$ should be used. However, this is a notational fix.) 

**Motivation**

In terms of motivation it is unclear how the work is related to the existing works. There have been countless results on the number of linear regions of neural networks, and quite a few results using tropical geometry at that. What void in the literature does this paper fill? The related work paragraph lists some of the works in tropical geometry, but doesn’t highlight where these works come short and how the proposed manuscript fills that void. Moreover, there is no discussion of why the existing works on linear counting that do not utilize tropical geometry are also not able to handle the presented context.

**Effective utility**

At the end of the day, I’m not sure I understand what the utility of the method is. Ignoring here the questions on motivation, the goal is to make deep learning more interpretable. However, the authors’ experiments diverge when input sizes are larger than $6$ and the networks are deeper than $4$ layers. Modern deep learning uses input sizes significantly larger than $100$ and decade old networks are deeper than $10$ layers. So what effectively are we learning about deep networks?

Unfortunately, there are some larger issues with the method and experiments. Beyond the concerns from the perspective of tropical geometry, we have zero guarantees about the upper and lower bounds of the Hoffman constant. There is no asymptotic analysis on the sample complexities of the bounds, no analyses about tightness or optimality, or even, at the very least, an analysis that the bounds are not vacuous or trivial. On line 272 the authors claim that even though their estimate diverges, that’s acceptable because frequently we’re interested is an upper bound on expressivity. However, how can we guarantee that there the number of regions is not undercounted (I couldn’t find a proof)? For that statement to be true the bound needs to be tight, but from their own experiments (Tables 1 and 2), the true $H$ ends up being larger than the upper bound, which is used to calculate the radius and eventually the radius. Obviously, then, the computed bounds are not representative: then, since the estimate Hoffman constant is not accurate and the algorithm diverges, what essentially do we gain?

Limitations:
I think the authors accurately identify the main limitations of their approach, which relates to the large computational complexity.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This study investigates the expressive power of deep fully-connected ReLU networks (or a piecewise linear function) from the perspective of tropical geometry. The number of linear regions gives an estimate of the information capacity of the network, and the authors provide a novel tropical geometric approach to selecting sampling domains among linear regions.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- An effective sampling domain is proposed as a ball of radius bounded by Hoffman constant, a combinatorial invariant

- The proposed sampling algorithm is doable and implemented.

Weaknesses:
- The proposed algorithms suffer from the curse of dimensionality

Limitations:
it is discussed in Section 6

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the expressivity of neural networks as captured by the number of linear regions using tools from tropical geometry. There are three main contributions, two of which are theoretical and the other is about open source library that allows the analysis of neural networks as Puiseux rational maps. 

The first theoretical contribution is that they propose a new approach for selecting sampling domains among the linear regions and the second is a way to prune the search space for network architectures with symmetries.

Prior work on tropical geometry and deep neural nets have analyzed ReLU and maxout units. Contrary to prior works, this work makes an effort to understand the geometry of the linear regions not just their number. To do so the authors propose a way of sampling the domain that leads to more accurate estimates compared to random sampling from the input space, which is a previously used alternative that can result in some missed linear regions and hence in inaccurate estimates about the information capacity of the neural network. This insight about sampling, allows then the authors to reduce the time to estimate the linear regions of special types of neural networks that exhibit some symmetry. This essentially reduces the number of samples needed and they experimentally verify their results.

Finally, the authors release OSCAR an open source library.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
-connections of neural networks expressibity with tropical geometry, though they have been exploited in the past, are strengthened in this paper

-the paper presents a nice story that leads to faster sampling methods, both in theory and in practice.

Weaknesses:
-the main weakness I see in the paper is that, though well-motivated and interesting, it lacks technical depth. For example, there is essentially one main result stated as Theorem 4.3, and some intermediate results stated as Lemma 3.3 and Proposition 3.4. On the one hand, the latter two are simple observations about Puiseux polynomials, and on the other hand the proof of the Theorem 4.3 is not more than 3 lines (as shown in Appendix C). As such, I believe it is a nice transfer of ideas from tropical geometry to neural networks, but given that the connection was already there and used in prior works more than 10 years back, I don't think the better sampling algorithm is solid enough.

Limitations:
See weaknesses.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work provides a geometric characterization of the linear regions in a neural network via the input space. Although linear regions are usually estimated by randomly sampling from the input space, stochasticity may cause some linear regions of a neural network to be missed. This paper proposes an effective sampling domain as a ball of radius R and computes bounds for the radius R based on a combinatorial invariant known as the Hoffman constant, which gives a geometric characterization for the linear regions of a neural network. Further, the paper exploits geometric insight into the linear regions of a neural network to gain dramatic computational efficiency when networks exhibit invariance under symmetry. Lastly, the paper provides code for converting trained and untrained neural networks into algebraic symbolic objects, useful for precisely the kinds of analysis this paper performs.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The authors present an interesting and novel way to analyze the capacity of a neural network using fundamental notions from tropical geometry.

2. The paper and theory were very clearly presented. In terms of writing, the presentation of the relevant tropical geometry for purposes of Hoffman constant estimation Section 3 was excellent.

Weaknesses:
1. My greatest concern in this paper stems from the experiments for upper and lower bound estimation for Hoffman constants given in Tables 1, 2, and 3 in the appendix. It seems the experimental upper and lower bounds computed there do not actually bound the true Hoffman constant. I understand that the upper bound may be loose due to the way it is estimated, but the lower bound should always be below the true Hoffman constant, as per my understanding. Yet for, say, the first of eight computations in table 1, the lower bound $H_L$ is $0.5460$, which is clearly above the true value of $H$, given to be $0.3298$. For that example, the upper bound $H^U$ is given to be $0.2081$, which is clearly not above the true value. This pattern continues, and the lower and upper bounds for the Hoffman constants seem to fluctuate somewhat arbitrarily around the true Hoffman constants, which is concerning. I am currently assuming that there is some kind of mistake with these experimental values and would like a clarification from the authors regarding this.

2. Due to the curse of dimensionality, this method for estimating the expressiveness of neural networks can only be applied to simple neural networks in practice. This is very apparent due to the way the numerical approach requires sampling on a mesh grid in an $n$-dimensional box (but is also true for the symbolic approach, that relies on the computation of the Puiseux rational function associated with a neural network, which becomes increasingly quite hard in higher dimensions). To the credit of the authors, they are up-front about this limitation, but this does significantly hinder the applicability of the presented results.

Limitations:
The authors adequately discuss the limitations of their work in Section 6.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
zFHJUSTZka;"REVIEW 
Summary:
This paper propose OAIF, an online method to align language model with human preference where feedback from language models serve as a surrogate of human feedback. The key of OAIF is to use online generated preference pair along the training process. Experiment results shows that, by switching offline preference dataset to online dataset labeled by other language models, the generated responses are more aligned with human preference.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The strengths of the paper are listed below:

1. This paper introduces OAIF, which is featured by using on-the-fly generated preference pairs and AI-provided labels.
2. The author conducted experiment on various direct alignment methods and the results consolidate the claim by the authors

Weaknesses:
My questions and concerns are listed as follows:

1. My first concern is regarding the novelty of the paper. It seems that the language model annotator is essentially a preference model. Therefore, OAIF can be seen as a method of online direct alignment algorithm with access to a preference model. The author mentioned several previous work with on-policy generation and online feedback but in need of a reward model. How is OAIF different from different from these method if we simply plug in the language model annotator as the reward model in their methods?
2. At line 118 the author pointed out that RM might suffer from distribution shift because the training data of RM might not share the same distribution with $\pi_\theta$. However, it seems to me that using language model as preference annotator cannot bypass this problem since the language models' pretraining corpus or the finetuning corpus relating to preference labeling has a similar distribution with $\pi_\theta$.
3. How is OAIF's performance compared to other online methods like RSO and IterativeDPO? I think that these methods might also be included as baselines since reward model can also be taken by AI annotators.

Limitations:
The limitation is discussed by the author

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work extends offline preference learning methods, i.e., DPO, to a online variant by using LLM as annotator to collect new datasets for further preference learning. The results show that Direct alignment from preferences (DAP) methods win-rate over the offline methods beyond 60%.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Paper is good writing, easy to follow.
2. This online variant provides demonstrates significant performance improvements over offline DAP and RLHF methods through comprehensive evaluations.

Weaknesses:
1. The improvement by extending online is under expectation as it introduces more datasets and training budgets. 
2. The contribution is limited. The only difference compared to the prior method is substituting the reward model of prior methods (Iterative DPO) to LLMs, though I agree the explicitly static reward model may introduce the model distributional shift problem.
3. Some drawings or comparisons are not fair enough. (a). Table 1 explicitly avoids the limitation of this method by leveraging the feedback from LLM, though it is another variant of the ""reward model"". (b). Figure 3, the training step is not an approximate x-axis as the online DPO variant has been heavily fine-tuned offline. 
4. There are no theoretical foundations, or new plausible explanations, aside from more datasets and the online budget, for the further improvement of the online variant DPO.

Limitations:
see Weaknesses

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper applies direct alignment from preferences (DAP) methods, particularly DPO, to online settings where responses are sampled in an on-policy manner and feedback is provided by the LLM annotator in real-time. Extensive experiments demonstrate the effectiveness of these simple ideas.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-written, with detailed explanations of introduced definitions and discussions with existing methods. 

The experiments are well-designed, supporting the main idea of the paper. The proposed prompt-controllable approach is particularly commendable.

Weaknesses:
The rationale for why on-policy learning brings performance gains is not well clarified. The cited reference [1] does not provide strong support for this claim. There is no experimental evidence that on-policy sampling encourages exploration. 

Most experiments are conducted with the closed-source LLM Palm; evaluating state-of-the-art open-sourced LLMs would enhance generalizability. 

It is unclear how much of the performance gains are due to on-policy sampling versus online feedback. 

The reasons why utilizing online on-policy data can avoid overfitting and improve performance should be further analyzed and discussed.

References:
[1] Lambert, N., Wulfmeier, M., Whitney, W., Byravan, A., Bloesch, M., Dasagi, V., Hertweck, T., and Riedmiller, M. The challenges of exploration for offline reinforcement learning. arXiv preprint arXiv:2201.11861, 2022.

Limitations:
The computational overhead introduced by on-policy sampling and online feedback is not discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a new method called Online AI Feedback (OAIF) for direct alignment from preferences (DAP) that addresses the limitations of existing DAP methods, which rely on static, offline feedback datasets. By using an LLM as an online annotator to provide real-time feedback during each training iteration, OAIF ensures the alignment process remains on-policy and adapts dynamically to the evolving model. Through human evaluations across various tasks, the authors demonstrate that OAIF outperforms traditional offline DAP and reinforcement learning from human feedback (RLHF) methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
OAIF uses LLMs for preference annotation, eliminating the need for a separate reward model and large datasets typically required for RLHF methods. It introduces a new way to address off-policy issues in policy optimization, a significant problem in traditional DPO methods.

The paper is well-written and easy to understand. OAIF outperforms offline DPO and other offline RLHF methods.

Weaknesses:
1. The idea is straightforward but lacks theoretical proof. The proposed method combines DPO and AI feedback, unlike the constitutional AI paper, which integrates PPO with AI feedback. However, this point is minor. Given the abundance of concurrent work [1-7], the authors should further develop the theoretical analysis of their approach to strengthen their method. 

2. Different methods should use an equal amount of training data. In the second epoch of onlineDPO, although the prompts remain the same as in the first epoch, the responses and rank information differ due to online generation.

3. Recent results on Reward Bench indicate that small reward models are more effective than LLM critiques. The iterative DPO methods are similar to OAIF DPO. A performance comparison between OAIF and various iterative DPO methods using cheaper reward models, as both address the off-policy issue, is essential and should be included.

[1] Iterative Preference Learning from Human Feedback: Bridging Theory and Practice for RLHF under KL-Constraint

[2] RS-DPO: A Hybrid Rejection Sampling and Direct Preference Optimization Method for Alignment of Large Language Models

[3] RSO: Statistical rejection sampling improves preference optimization

[4] Some things are more cringe than others: Preference optimization with the pairwise cringe loss. arXiv preprint arXiv:2312.16682

[5] Hoang Tran, Chris Glaze, and Braden Hancock. 2023. Iterative dpo alignment. Technical report, Snorkel AI.

[6] Self-rewarding language models. arXiv preprint arXiv:2401.10020

[7] Is dpo superior to ppo for llm alignment? a comprehensive study. arXiv preprint arXiv:2404.10719.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
ndIum4ByTU;"REVIEW 
Summary:
This paper addresses the issue of the flow matching method's lack of dependence on the data population. It proposes incorporating the initial population density into the vector field through amortization—using a Graph Neural Network (GNN) to embed the populations and adding this embedding to the input of the vector field network. The paper argues that this dependence would better model the data due to sample interactions, demonstrating improved generalization on unseen initial distributions. The method's application is showcased in perturbation drug screening.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
### Originality
The problem setting of adding population dependence to flow matching is novel. The model framework of adding input to the vector field network using a GNN as a population encoder is also novel.

### Clarity

The paper is clearly written, with rigorous mathematical notations. The related work and introductions are especially well-written.

### Quality

The writing is good, and the experiment involves many baselines.

### Significance

The proposed method excels at generalizing to unseen populations, which is a significant improvement over existing methods, particularly when the conditions for generation are complex. The application on drug screening addresses a significant scientific problem and holds promise for personalized healthcare.

Weaknesses:
1. The paper could explain more about the meta-learning aspect of this method.
2. It could include explanations and/or ablation studies on the role of meta-learning and the GNN, especially in the synthetic experiment.
3. More detail is needed on what properties of the Wasserstein manifold of probabilities are used in the model. It is unclear how the model proposed in section 3.2 depends on the properties of the Wasserstein manifold described in section 3.1.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed an extension of the Conditional Generative Modeling via Flow Matching (CGFM) framework. Taken inspiration from the theory of Wasserstein Gradient Flow, this new framework, Meta Flow Matching, proposed to learn the push-forward mapping of multiple measures in the same measures-space. This is motivated by the realistic problem of modeling single-cell perturbation data where we want to see the response of populations of cells of patients when receiving different treatments. A novelty of Meta Flow Matching is that by combining amortized optimization and CGFM, the trained MFM velocity network can model newly observed populations _without_ knowing their labels/conditions. Two empirical benchmarks were performed to showcase the effectiveness of MFM compared to the Flow Matching (FM) and CGFM.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method proposed is novel enough, and the problem is well-motivated. I also find the idea of integration of GNN to model the conditional variable quite neat. The method is based on the well-studied theory of Wasserstein gradient flow on measure spaces and amortized optimization framework.

- The empirical benchmark, especially on real biological data, seems to showcase the strength of MFM.

- Overall the paper is quite well written and easy to follow.

Weaknesses:
- The first part of the methodology section seems to be phrased as a new methodological contribution, but if I'm not mistaken this is just more or less restating the already established theory of W2 gradient flow and continuity equation (eq 14). I think the authors should put Section 3.1 into the background section (2nd Section).

- There is a lack of discussion on whether the 3 crucial assumptions (line 145-152) are satisfied in a realistic biological setting. For example, in theory, Assumption (iii) on the unique existence of the Cauchy problem stands when the velocity field satisfies some regularity conditions -- I'm not sure this can be extended to a parameterized neural network that takes input from another (graph) neural network as an embedding function, which is hardly Lipschitz smooth in most of the case.

- Algorithm boxes at the end of section 3 is highly welcome. Or if the authors cannot allocate the space, I highly recommend putting two (one for training and one for sampling) into the Appendix. It is quite hard to follow how the velocity is trained in reality. For example, what function $f_t(x_0, x_1)$ did the authors take for this work? Is it still linear interpolation as vanilla flow matching? Or does it involve adding some form of stochasticity as in stochastic interpolant or VP-SDE as in diffusion model? Is the coupling $(x_0, x_1)$ sampled to match randomly, or they are sampled to some form of alignment as in the multisample flow matching paper (Pooladian et al. 2023)?

- This might not be the original purpose of this work, but I would love to see how MFM perform on conditional image generation task. One can pick a simple small dataset such as CIFAR10 that already includes class labels, or better yet ImageNet dataset. The performance in this takse will be much more convincing than the synthetic experiment, where I would argue would target the same type of task.

Limitations:
See weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper discussed the novel problem setup of generative modeling of the dynamics of probability distributions. The paper proposed Meta Flow Matching (MFM), an extension of the flow matching framework for implicitly learning the vector fields on the Wasserstein manifold of probability distributions. The paper demonstrated better transferability of the proposed framework on unseen distributions on both synthetic datasets and real-world drug-screen datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The problem setup of learning a flow matching model for mappings between distributions (i.e., a probability path on the Wasserstein manifold), to the best of my knowledge, is novel and has not been explored in previous work.
- The idea of using distribution-specific embeddings (the population embeddings) is well explained and motivated in the paper.
- The proposed method demonstrates better transferability on both synthetic and real-world datasets compared to other baselines.

Weaknesses:
- The proposed method seems to be a special case of a conditionally trained flow matching model where the conditions are continuous learnable embeddings. Such an idea has already been applied in various diffusion or flow matching models including image generation (conditioned on text embedding in the latent space), protein co-design [1] (conditioned on sequence, generate protein structure, or vice versa), and peptide design [2] (conditioned on receptor proteins, generate peptides).

- The idea of population embedding in the paper is similar to task embedding, which has been well-explored in the meta learning (e.g. [3]). Although the authors claimed their framework to be *meta* flow matching, related work in meta learning seems to lack.


[1] Campbell, Andrew, et al. ""Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design."" arXiv preprint arXiv:2402.04997 (2024).

[2] Li, Jiahan, et al. ""Full-Atom Peptide Design based on Multi-modal Flow Matching."" arXiv preprint arXiv:2406.00735 (2024).

[3] Achille, Alessandro, et al. ""Task2vec: Task embedding for meta-learning."" Proceedings of the IEEE/CVF international conference on computer vision. 2019.

Limitations:
The authors have adequately and properly discussed the limitations and potential societal impact of the paper in the Appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Meta Flow Matching (MFM), a flow matching framework modeling interacting samples evolving over time by integrating vector fields on the Wasserstein manifold. The authors leverage a Graph Neural Network to embed populations of samples and thus generalize the method over different initial distributions. The authors demonstrate the method on individual treatment responses predictions on a large multi-patient single-cell drug screen dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novelty: The method uniquely considers population interactions, unlike previous flow matching methods that model samples individually.

Generalization: The authors extended conditioning on latent variables to conditioning on population index in section 3.2. The proposition in section 3.2 demonstrates that conditional flow matching can fit well within the MFM framework. The experiments show that MFM can generalize to unseen data, outperforming other methods in this regard.

Weaknesses:
In Table 1 of the synthetic experiment, the authors compared the performance of FM, CGFM and MFM of k=0,1,10,50. MFM doesn't seem to beat existing methods on the metrics and from the visualizations, it's hard to tell MFM is actually doing better than FM. Also, for the various values of k, some explanations on how performance correlates with values of k and why might be necessary for readers to understand this table.

In both experiments, the authors only compared FM, CGFM, and in Table 2 also ICNN. Probably more methods, like diffusion, should also be taken into comparison. Also, in experiment 2, only W1, W2 and MMD are computed as metrics. While these are useful when modeling distributions, more metrics, especially those specific to this application may be applied.

Limitations:
The authors have not addressed limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
rog0J435OO;"REVIEW 
Summary:
This paper proposes a novel method to address the high computational and memory complexity of current large-scale transformers. By adopting a simple yet effective column-wise sparse representation of attention masks, the algorithm achieves reduced memory and computational complexity while maintaining the accuracy of attention computation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper investigates a topic of interest, given the current trend toward increasing context lengths in LLMs.
2. The method proposed in this paper is straightforward and easy to implement.
3. The paper is well-written and clearly presented.

Weaknesses:
1. It is crucial to highlight the advantages of this method over related work to help readers fully understand its significance. However, in the subsection ""Attention Optimization Techniques,"" the authors only mention the drawback of FlashAttention and discuss its relationship to their work. The introduction of other related works is confusing and makes it difficult to comprehend their relevance to this paper. The overall conclusion, ""*Both of the previously discussed solutions either compromise precision or yield only marginal enhancements in efficiency. Conversely, our proposed FlashMask is capable of delivering exact computations.*"" is general and non-specific. It is unclear which methods compromise precision and which ones only offer marginal improvements.

2. In the experiments, the baseline algorithms are limited to Vanilla Attention and FlashAttention. Are there more efficient Transformer algorithms that could be used for comparison? If not, the authors should explain the rationale behind the selection of these specific baselines.

3. As a non-expert in this field, I found the writing of this paper confusing. For instance, the initialism ""HBN"" is introduced without any explanation or context.

Limitations:
none

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes FlashMask, which accelerates the masked attention mechanism that can reduce the original attention from O(N^2) to O(N) and simultaneously reduces the memory cost. Experimental results show that the proposed FlashMask significantly reduces training time without accuracy degradation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ This paper provides a comprehensive study and analysis about the sparse attention, in terms of their efficiency. Also, this paper includes existing attention optimization like FlashAttention, explaining the motivation of the proposed FlashMask, which lies in the lack of optimization for sparse attention.

+ This paper proposes an optimization for column-based sparse attention, which significantly improves memory efficiency and reduces computational costs.

+ This paper provides a comprehensive complexity analysis, evaluation, and comparison with existing methods. It seems the authors make a lot of efforts on the proposed approach.

Weaknesses:
- Even though FlashMask achieves significant improvement in the memory efficiency of sparse attention, the key idea is similar to FlashAttention, but it is just for sparse attention mechanisms. Based on this fact, the novelty of this paper is not strong. I recommend the authors explain why the red part in the algorithm is designed and why it is unique for sparse attention.

- The authors only present optimization for column-based sparse attention. The performance for other types of sparse attention is unknown. If the proposed approach can be applied to all sparse attention, the contribution of this paper is extremely great. However, the existing version is not comprehensive.

- Based on the experiments, the practical latency is not significantly reduced as compared to other methods, even though the theoretical complexity is from N^2 to N. Besides, the authors do not provide results for accuracy.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces FlashMask, an innovative algorithm designed to address the computational and memory challenges associated with conventional attention mechanisms in large-scale Transformers. FlashMask employs a column-wise sparse representation for attention masks, significantly reducing the computational complexity from quadratic to linear with respect to sequence length. The authors demonstrate FlashMask's effectiveness across various masking scenarios and training modalities, including Supervised Fine-Tuning (SFT), Direct Preference Optimization (DPO), and Reward Model (RM).

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper presents a novel solution to a well-known problem in the field of natural language processing, offering a practical method to reduce the computational burden of attention mechanisms in Transformers.

The paper provides extensive empirical evidence to support its claims, including comparisons with state-of-the-art techniques like FlashAttention, demonstrating FlashMask's superiority in terms of speed and efficiency.

FlashMask's performance across different masking scenarios and training modalities shows its versatility and robustness, indicating its potential applicability to a wide range of models and tasks.

Practical Impact: The paper not only presents theoretical advancements but also demonstrates practical benefits, such as enabling the

Weaknesses:
The scaling ability of the proposed method deserves further verified on large scale datasets.

While the paper demonstrates FlashMask's effectiveness in specific scenarios, it may lack broader evidence on how it performs across different types of NLP tasks or diverse datasets.

The paper could provide more detailed insights into how FlashMask handles different sparsity levels and the impact on various model sizes and complexities.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes FlashMask, a modification of FlashAttention with fixed masks. The paper shows speedup of FlashAttention when using sparse masks in the attention matrix.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
FlashAttention is an important algorithm, and sparsity in the attention matrix is an important feature. Further study of these aspects is helpful.

Weaknesses:
The paper seems to make claims that are unsubstantiated by experiments. In the abstract and introduction, the paper claims speedup without sacrificing model quality. However, there is no experiment evaluating model quality in the experiments. This is a critical flaw.

Further, the contribution of the paper is unclear. Block-sparsity is already supported in FlashAttention (see section 3.3 of FlashAttention). It is unclear how this paper is different. There are also more recent works such as ""Fast Attention Over Long Sequences With Dynamic Sparse Flash Attention"" (NeurIPS 2023), which seem to be strictly more expressive in features than this paper.

Limitations:
The paper discusses superlinear scaling in sequence length as a limitation, but is lacking in discussion of model quality.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
XgkrXpHl5j;"REVIEW 
Summary:
The paper introduces a Generalized Multimodal Fusion (GMF) method using the Poisson-Nernst-Planck (PNP) equation to address challenges in multimodal fusion, such as feature extraction efficacy, data integrity, feature dimension consistency, and adaptability across various downstream tasks. The GMF method leverages theoretical insights from information entropy and gradient flow to optimize multimodal tasks, treating features as charged particles and managing their movement through dissociation, concentration, and reconstruction. 

Key contributions of the paper include:
1. A theoretical framework combining PNP and information entropy to analyze multimodal fusion.
2. A novel GMF method that dissociates features into modality-specific and modality-invariant subspaces.
3. Experimental results showing GMF achieves competitive performance with fewer parameters on multimodal tasks like image-video retrieval and audio-video classification.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The application of the PNP equation from physics to multimodal feature fusion is novel and creative. The theoretical framework combining PNP and information entropy provides an original perspective on analyzing multimodal learning.

Quality: The paper provides a solid theoretical foundation with detailed proofs and derivations. The experimental evaluation is comprehensive, covering multiple datasets and task types (NMT, EMT, GMT).

Clarity: The paper is well-structured and clearly written. The methodology is explained step-by-step with helpful visualizations.

Significance: The proposed GMF method shows promising results in terms of performance, parameter efficiency, and robustness to missing modalities. It has potential for broad applicability as a frontend for other fusion methods.

Weaknesses:
1.The theoretical analysis, while extensive, could benefit from more intuitive explanations to improve accessibility.
2. The experimental section lacks ablation studies to isolate the impact of different components of the GMF method.
3. While the method shows good results, the performance improvements over some baselines are relatively small in certain experiments (e.g. Table 2).
4. The paper does not thoroughly discuss potential limitations of the approach or scenarios where it may not be suitable.

Limitations:
The authors briefly mention some limitations of linear operations for high-dimensional inputs in the conclusion. However, a more thorough discussion of potential limitations and failure cases would strengthen the paper. Additionally, while not highly relevant for this theoretical/methodological work, some discussion of potential negative societal impacts of improved multimodal fusion techniques (e.g. privacy concerns) could be included.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces CrossCheckGPT, a novel method for assessing hallucination robustness in multimodal foundation models without requiring reference standards. Utilizing cross-system consistency, the proposed method aims to provide a universal evaluation framework capable of being applied across various domains and tasks. This approach contrasts significantly with traditional hallucination assessments, which rely on comparison with gold-standard references and are limited to specific domains.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The introduction of a reference-free universal hallucination ranking method addresses a significant gap in the evaluation of foundation models, particularly in new and emerging areas.
2. The paper effectively demonstrates the method's versatility across different modalities (text, image, and audio-visual), enhancing its relevance to a wide range of applications.
3. The development of the AVHalluBench, the first audio-visual hallucination benchmark, is a noteworthy contribution that sets a new standard for evaluating models in this complex domain.

Weaknesses:
1. The analysis on how different models' outputs are compared and the implications of these comparisons could be more detailed. Specifically, the paper lacks a deeper exploration into the sensitivity of CrossCheckGPT to variations in model architecture or training data.
2. Lack of additional visual representations of the data flow or examples of the hallucination checks.
3. Lack of a more comprehensive set of benchmarks, including more direct comparisons with state-of-the-art methods.

Limitations:
The effectiveness of the method hinges on the diversity and independence of the models used as evidence sources.  This dependence could pose challenges in scenarios where similar or homogenous models are prevalent.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a novel multimodal fusion method named Generalized Multimodal Fusion (GMF), which leverages the Poisson-Nernst-Planck (PNP) equation from physics to manage the feature fusion process in multimodal learning tasks. By treating features as charged particles, the method allows for a dynamic separation and recombination of modality-specific and modality-invariant features, thereby enhancing the fusion process and reducing the entropy in downstream tasks. This approach addresses common challenges in multimodal learning, such as feature dimension consistency, data integrity, and adaptability across various tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The application of the PNP equation, traditionally used in physics to describe the dynamics of charged particles, to multimodal feature fusion is highly original.
2. The paper is grounded in a solid theoretical framework that is well-articulated and robust.

Weaknesses:
1. The method, while innovative, appears to be complex in terms of implementation, particularly in how features are treated as charged particles. This complexity might limit its accessibility or usability for practitioners not familiar with the underlying physical equations.
2. The paper could benefit from more rigorous quantitative analysis, including statistical significance tests and error analysis. Such analyses would provide a clearer picture of the method's performance relative to benchmarks.
3. Can the authors provide the results on multimodal datasets with text-image modalities [1] ?

[1] Provable Dynamic Fusion for Low-Quality Multimodal Data.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors combined the Poisson-Nernst-Planck (PNP) equation with information entropy theory and proposed a generalized multimodal fusion approach, which disassociates modality-specific and modality-invariant features, thereby reducing the join entropy of input features and meanwhile decreasing the downstream task-related information entropy. The experimental results demonstrate that the proposed approach can improve the generalization and robustness of multimodal tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- It is innovative to employ PNP for solving the multimodal fusion issue, by treating features as charged particles to disassociate them. 
- A generalized multimodal fusion approach was designed, which can overcome the strong assumptions made by existing methods. 
- Experiments showed the effectiveness of the proposed GMF approach in efficiency and flexibility.

Weaknesses:
- Significance test (e.g., Wilcoxon signed-rank test) would be helpful to better illustrate the significance of the proposed method compared against baselines. 
- Social impacts are not explicitly discussed in the paper/appendix.

Limitations:
Please refer to Weakness and Question, which are the aspects suggested to be further improved.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
Fm4FkfGTLu;"REVIEW 
Summary:
gRNAde is a graph neural network designed to address the RNA reverse folding problem, a significant challenge due to the potential of RNA as therapeutic modalities and their unique data properties. RNA molecules have lower thermodynamic stability compared to proteins, resulting in fewer training samples, and their increased flexibility means multiple final states are possible. gRNAde addresses these issues by proposing a custom multi-graph representation and extending message passing to operate independently on each conformer while sharing an adjacency graph. The authors thoroughly explore evaluation techniques, comparing their model performance against Rosetta by assessing the percentage of native sequence recovery on held-out sequence families. Additionally, they demonstrate slightly improved performance when utilizing multiple conformers for model training. The authors also conduct an interesting zero-shot ranking analysis on mutation data providing a refreshing evaluation against random baselines.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is exceptionally well written presenting a thorough overview of the challenges within modality, broader field, and the importance of the problem.
- The experimental validation assesses the utility of design choices and although the improved performance in the presence of multiple conformers isn't large (the authors don't report statistical significance), the approach is promising. 
- The authors conduct a variant effect evaluation assessing whether their model is capable of learning impact of single or double mutant sequences demonstrating convincing improvement over random baselines.

Weaknesses:
- I find the arguments regarding gRNAde perplexity being correlated with recovery to have limited support in the current presentation. In figure 2 (b) color denotes perplexity instead of one of the axis making it very challenging to assess the correlation. In addition the authors don't report a correlation value or its significance. 
- The authors only use random baselines for the retrospective variant effect analysis. Including another reverse folding model or a metric from Rosetta similar to gRNAde's perplexity could strengthen the evaluation.

Limitations:
- The authors effectively discuss the current evaluation limitations and the difficulty of assessing the novelty and ground truth recovery of generated sequences.
- There is limited discussion on the data limitations in the current field. With only 4000 sequences, training points are very few, presenting a major challenge.
- The authors could include a brief statement on the broader impacts, such as the potential design of harmful molecules. As these models improve, the dual-use concern becomes legitimate.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a geometric RNA design model. Specifically, it introduces multi-stage GNN to encode multiple conformations and aggregate these candidates, and further feed decoder to predict probabilities of a set of candidate sequences.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	This work creates a new dataset for RNA inverse design, with diverse properties such as sequence length, number of structures, and structural variations.
2.	This work designs a multi-state RNA reverse design model, distinct from existing methods.

Weaknesses:
1.	The technical contribution appears to be somewhat weak. The backbone used is from existing GNN models for equivariant design.
2.	Some technical details are not claimed. For example, are the comparison baselines retained on the new dataset or simply tested on their released model.

Limitations:
This paper has no negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduce gRNAde, a geometric deep learning pipeline for RNA sequence design conditioned on one or more 3D backbone structures. gRNAde is superior to the physically based Rosetta for 3D 320 RNA inverse folding in terms of performance, inference speed, and ease of use. The method demonstrates significant superiority across various experiments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The authors introduce gRNAde, the first work to consider multi-state biomolecule representation. This study explores the feasibility and specific experimental results of using multi-state biomolecule representation, providing new ideas for researchers in the field.
2. The authors conduct extensive experiments and analyses on multiple datasets and experimental settings, demonstrating the model's effectiveness from various perspectives, especially regarding the ""Zero-shot ranking of RNA fitness landscape"" experiment, which is currently lacking in this field.
3. The authors present various experimental details using numerous visualizations and data tables, making the paper easier for readers to understand.

Weaknesses:
1. The model architecture proposed by the author lacks innovation. The core structure of gRNAde is directly stacked using GVP-GNN, and the handling of multi-state conformations is merely simple stacking. Additionally, the 3-beads representation method is very common in traditional RNA 3D structure modeling, which is also not an innovation by the author. Therefore, I believe the model design is lacking.
2. The baselines compared by the author in various experiments are either outdated or too simple, such as ""Rosetta(2020)"" and the ""random baseline"" in the Zero-shot experiment. This makes it difficult to demonstrate the actual performance of gRNAde. Some recent works using deep learning to model RNA 3D structures can serve as baselines, such as [1-3].
3. The author mentions that gRNAde has a significant speed improvement over Rosetta, but the author did not run the Rosetta code themselves and instead directly cited the original Rosetta paper. I believe this point is debatable because the model's running speed is also limited by GPU computational performance. The author uses an A100, whereas the GPU used by Rosetta four years ago is obviously inferior to the A100. Therefore, the author needs to rerun the Rosetta program on the A100 to provide accurate model inference times.







[1] Geometric deep learning of RNA structure, Science 2021

[2] Physics-aware Graph Neural Network for Accurate RNA 3D Structure Prediction, NIPS workshop 2022

[3] RDesign: Hierarchical Data-efficient Representation Learning for Tertiary Structure-based RNA Design, ICLR 2024

Limitations:
The authors discuss practical tradeoffs to using gRNAde in real-world RNA design scenarios 330 in Appendix B, including limitations due to the current state of 3D RNA structure prediction tools.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work designed gRNAde, a geometric deep learning pipeline for RNA sequence design conditioned on one or more 3D backbone structures. To achieve this, the authors created single-state and multi-state 3D RNA structure datasets, built a geometric graph representation, and proposed an architecture consisting of a multi-state GNN encoder, a pooling layer, and a autoregressive decoder. The single-state RNA design, multi-state RNA design, and zero-shot rank experiments were conducted and results show that gRNAde outperformed all previous methods including Rosetta.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1.	The datasets were carefully designed. Only structures with high resolution were maintained. Two kinds of clusters were used to split train, validate, and test sets where the hard samples were split into test sets.

2.	The model architecture makes use of information from multiple conformations. This is achieved by sum or average pooling.

3.	The experiments were conducted fairly. Datasets were split carefully. The results were averaged on 16 sampled sequences across 3 random seeds.

4.	The inference speed is much faster than traditional methods. This makes it possible to be used in High throughput screening. The zero-shot ranking ability is also an advantage.

Weaknesses:
1.	The model architecture has no novelty. All components are token from previous work and the overall structure is very similar to that of ProteinMPNN. The multiple conformations are processed independently and the representations are simply averaged or summed, which may not grasp all information.

2.	The model is trained on only about 4 thousand RNA sequences. These sequences are too few to cover the entire space. RNAs with no 3D structures should be exploited, as done in alphafold3.

3.	The results about single-state RNA design were reported on only 14 samples. More samples should be used to test the model. For example, the results on test sets with 100 samples should be reported.

4.	The improvements on multi-state RNA design task are limited. The native sequence recovery is obviously lower than the results of single-state design task.

Limitations:
The representation ability of gRNAde remains to be verified. Usually, the representation is extracted when all nucleotides are known, so the architecture for inverse folding problem may not suitable for representation learning.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduced a multi-state geometric graph neural network for the RNA inverse folding problems. Experiments are conducted on carefully splited structural datasets that avoid data leakage. The results have shown convincing performance improvement over the physics-based methods such has FARFAR and Rosetta which are commonly used for RNAs.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper is well written and pleasing to read. Explanations on key biological concepts related to RNAs, and how they motivate the model design as well as experiment setup are adequate and above all, clear.
- Evaluation metrics are well considered. The self-consistent scores on the secondary and tertiary levels are meaningful.
    - limitations on self-consistence scores are acknowledged in the main text. For RNAs, many challenges are unique especially when they are compared to proteins. Therefore, clarifications and precautions are particularly needed for RNA related tasks. I appreciate the authors’ effort for stating these limitation clearly in the main text.

Weaknesses:
- Comparison to contemporary deep learning models for RNA inverse folding is limiting
    - RDesign (https://openreview.net/forum?id=RemfXx7ebP) for example is a recent deep learning based method for 3D RNA inverse design
    - For inverse design on the secondary structure level there are many more options — a lot of them are better than RNAinverse from ViennaRNA. I would suggest checking out this survey (Design of RNAs: comparing programs for inverse RNA folding) and include a few other more competitive baselines.
- For the self-consistency scores, I personally doubt if RhoFold (also called e2efold-3d) is reliable software for RNA tertiary structure prediction, since it is from the same group that published e2efold which is a spectacularly awful RNA secondary structure predictor (I personally would avoid using any of their tools; checkout its Github issues, and also followup works on RNA secondary structure predictions that have compared with e2efold). Have the authors used more recent folding softwares such as RosettaFoldNA and AlphaFold3?
    - Would using different structure predictors significantly impact the results? This also includes EternaFold. How would gRNAde hold up against the baselines when RNAfold or LinearFold is used to compute the self-consistency scores on the secondary level?
- Data splits (train, validation and test) are carefully constructed so that the evaluation is not contaminated by data leak. But I wonder if the TM-score cut-off at 0.45 is too lenient? Is it still possible to have similar structures between training and test sets under this threshold?

Line 258 to 259. The argument would be more compelling if the inverse design operates at the quaternary level which would include information about ligand structures.

Limitations:
- Comparison to contemporary models for RNA inverse folding is a bit hollow. It would be more meaningful if some deep learning based baselines can be included into the comparison.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
1CssOqRYyz;"REVIEW 
Summary:
The paper proposes the first diffusion-based point cloud compression method called Diff-PCC.  
A dual-space latent representation is devised in this paper, where a compressor composed of two independent encoding backbones is used to extract expressive shape latents from different latent spaces.    
At the decoding side, a diffusion-based generator is devised to produce high-quality reconstructions by considering the shape latents as guidance to stochastically denoise the noisy point clouds.    
Experiments demonstrate that the proposed Diff-PCC achieves state-of-the-art compression performance (e.g., 7.711 14 dB BD-PSNR gains against the latest G-PCC standard at ultra-low bitrate) while attaining superior subjective quality.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Novelty is a strength. To my knowledge, diffusion model is used in point cloud compression for the first time. And the dual-latent design is also novel for learned point cloud compression.   

The manuscript is well written and easy to follow. Especially, the author did a good job in introducing related works on image compression, point cloud compression, point cloud analysis and diffusion model.

Weaknesses:
More work on diffusion model for data compression could be discussed, like ‘Idempotence and Perceptual Image Compression, ICLR 2024’. In addition, although this paper focuses on point cloud compression, the way of applying diffusion model should be compared with those learned image compression works in the related work part. From my impression, the method in this paper is still novel compared with those learned image compression paper using diffusion model.    

More recent learned point cloud compression method [30][14] should be compared in Table 1, Figure 3 and Figure 4, regarding rate distortion and encoding/decoding speed. Besides, only object point cloud is considered currently, large scale point cloud like SemanticKITTI could be compared [30][14].  
 
It is not clear how the speed is measured in Table 1. The hardware and commend line shoud be provided in the supplementary material.  

Minor:  
L86, Point·E[] is a typo.  
[30] and [31] are the same.   
L202, the reference should be fixed.   
What is the FPS in eq 14? farthest point sampling?

Limitations:
The limitation is addressed in the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the authors propose a diffusion-based point cloud compression framework. Low frequency and high frequency features are extracted via PointNet and PointPN from input point clouds, which are quantized and encoded for compression. During decompression, the quantized features would be decoded to condition a diffusion model to construct the decompressed results. The experiments on 15K points data from ShapeNet, ModelNet10, and ModelNet40 show superiority over compared methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea to introduce diffusion models for point cloud compression is different with former works;

2. The paper is easy to follow, while the disgrams are also good;

3. The performances show improvements on sparse point clouds.

Weaknesses:
1. The comparison is not convincing enough. Some commonly used compression methods are not compared, while the evaluation is limited to sparse point clouds with relatively simple structures from ShapeNet, ModelNet;

2. The motivation of using diffusion model for compression is questionable. As a sampling-based framework, diffusion models may construct different results during decompression from variant sampled noises each time. I am not so sure if the diffusion model is more appropriate than existing AE or VAE-based frameworks for the compression task, which may need decompression as accurate as possible;
Please check the questions for more details, thanks.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, they introduce the diffusion-based point  cloud compression method, dubbed Diff-PCC, to leverage the expressive power of the diffusion model for generative and aesthetically superior decoding. They get better performance than G-PCC and two deep learning methods.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Encoding point clouds using diffusion models is a good idea. The article is easy to understand.

Weaknesses:
Firstly, how do we obtain a point cloud with added noise in the decoder? We have no knowledge of any other information about the original point cloud, except for the information in the bitstream. This will result in the inability to decode.
This manuscript claims to achieve state-of-the-art compression performance, but it only compares with two deep learning methods from the past two years. It does not compare with the most advanced methods such as CNet, SparsePCGC, and so on.

Limitations:
The decorder will not work

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
JVtwC9RzlI;"REVIEW 
Summary:
This paper proposes a method to integrate the processing and generation of both text and graph data using a single transformer-based model. Structure Token encodes graphs with text labels into a sequence of tokens, enabling the handling of both data types interchangeably. This approach leverages a unified representation within a Transformer Encoder-Decoder model, enhanced to incorporate structure tokens. They evaluate TextGraphBART on text-to-graph (T2G) and graph-to-text (G2T) tasks, demonstrating comparable results to baseline models with fewer parameters.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The introduction of structure tokens is a significant advancement, providing a unified method for processing and generating both text and graph data.
2. The method integrates seamlessly with existing Transformer architectures, requiring only minor modifications, and avoids the need for specialized loss functions or additional modules.
3. Empirical results show that TextGraphBART achieves comparable performance on both T2G and G2T tasks with baselines but with fewer parameters.

Weaknesses:
1. The baseline settings are weak. It lacks of comparing with some strong baselines with advanced graph-structured-aware methods [1][2].
2. The model settings are limited. This paper only try one size of model, and further experiments with larger model sizes are needed.
3. The provided ablation study, though useful, could be more detailed, exploring the interactions and contributions of various components more comprehensively.

Reference: 
[1] Stage-wise Fine-tuning for Graph-to-Text Generation
[2] Self-supervised Graph Masking Pre-training for Graph-to-Text Generation

Limitations:
Yes. The limitations are discussed in Discussion section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces TextGraphBART. Its a new method of encoding graph/text-input by using a structure token. This new token should preserve graph structure as opposed graph linearization or cycle training. This should also allow for the generation of graphs, with accompying text tokens, without making architectural changes to the transformer. It is then verified on both Graph2Text and Text2Graph tasks for commonly used benchmarks such as WebNLG and EventNarrative. It is followed by results that show comparable results to previous works, and a short discussion and conclusion.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Paper is generally nice to read
2. The work considers a lot of relevant related work, which they both refer to or base decisions on
3. The build-up of the structure token is well explained

Weaknesses:
1. The work proposes a seamless integration of both text and graph inputs, but only verifies on strictly one-directional modal evaluation: Either text-to-graph or graph-to-text. However, as the authors claim that this is the ""first method that can autoregressively generate sub-graphs with multi-token labels without modifying transformers."", a single text-to-graph benchmark feels like a very shallow evaluation.
2.  Models are evaluated on datasets that already work well. This makes it hard to distinguish the added value of such a structure token. Scenarios are sketched where e.g. graph linearization is treated simply as a sequence of tokens, but then it this work should methodologically be verified on datasets where preserving graph structure should be required. This is now not the case.
3.  This is reflected in the results: they are comparable, so the structure token might aswell not be there. The proclaimed difference that this is done with less parameters I would argue is a very weak claim as the difference is not that big.
4.  Finally, I am not convinced that the proposed ""structure token"" is a truely lossless way of preserving graph structure through-out a transformer. The structure token and its generation is explained in Figure 2, and is claimed to be lossless. However, this is only true in the operations before the embedding leveraging the orthogonal property. After the embedding into a structure embedding, information will (seemingly) be lost about structure in this embedding. From this structure embedding, it cannot be determined what the original structure of the graph was?
5.  In the discussion the authors mention scaling, but there is no clear evidence for this as the authors only tested one model size. This is essentially an unsubstantiated claim.
 6. Why is the domain token needed? This is not further addressed or experimented with.

Limitations:
The authors partially address this limitations by motivating the paper in the introduction. However, I dont feel the discussion (or the rest of the work) addresses the actual limitations of this method. What are pros or cons compared to transformer modifications?

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a new unified graph-text generation framework, TextGraphBART, for the large language model. The paper tries to address both the generation and representation of text and graphs. The paper proposes a new structure token to encode text graphs into a set of tokens. The structured token can encode the text graphs and be decoded into text graphs. Specifically, it consists of seven different embeddings, including position, domain, and text information. The paper pretrained the proposed framework over four different tasks, including text2text, graph2text, text2graph, and graph2graph. The model is pretrained on TEKGEN and GenWIKI and tests on the event narrative and WebNLG. The performance is evaluated on BLEU, METEOR< and BERTScore. The paper compares the model with T5, BART, GAP, CYcleGT, ZBT5, and Grapher. The paper also includes an ablation study and analyzes the future directions.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The motivation of the paper is clear and the topic of the paper is interesting. The paper investigates an important unified graph-text generation framework for the language model. The idea of structure tokens is interesting and can contribute to future research. Additionally, the model seems to achieve good results while using fewer parameters.
2. The paper tests on both graph-to-text and text-to-graph datasets. The proposed method achieves better results compared to other baselines. The paper also conducts an ablation study to investigate the contribution of each component. It includes a discussion session to explore the paper's current limitations.
3. The paper provides code and implementation details.

Weaknesses:
1. The proposed framework is incremental. Multiple previous papers have used the idea of using different position embeddings to represent the structure information [1,2,3]. The idea of joint text-to-graph and graph-to-text pretraining/generation is also not new [4]. 
2. The experiment is not comprehensive. The paper used eventnarrative for the graph-to-text generation. However, compared to WebNLG  (2020), the EventNarrative has fewer baselines and most of its baselines are outdated. The paper must also add WebNLG(2020) as an additional graph-to-text generation tool to show its contributions. The pretraining datasets and the testing datasets are all Wiki-style datasets. The performance gain may come from pretraining (data leakage) instead of the actual model architecture. The paper needs to include an additional ablation study to show the gain of the eventnarrative/webnlg comes from the model architecture, or also pretraining the proposed baselines on TEKGEN/GenWiki. Otherwise, the scores are not comparable. Additionally, the paper needs to include some of the latest frameworks like [4] since all of the baselines used in the paper are old. The paper also needs to include some human evaluation or qualitative analysis to help readers understand the generation results better. Furthermore, the comparison in Table 4 is not fair, since domain-specific pretraining is more useful than general pretraining [5].
3. The paper puts important information in the Appendix while not reaching the page limit (9 pages). 

[1] Herzig, J., Nowak, P. K., Müller, T., Piccinno, F., & Eisenschlos, J. M. (2020). TaPas: Weakly supervised table parsing via pre-training. ACL 2020.

[2] Wang, Q., Yavuz, S., Lin, V., Ji, H., & Rajani, N. (2021). Stage-wise fine-tuning for graph-to-text generation. arXiv preprint ACL 2021 SRW.

[3] Chen, W., Su, Y., Yan, X., & Wang, W. Y. (2020). KGPT: Knowledge-grounded pre-training for data-to-text generation. EMNLP 2020.

[4] Wang, Z., Collins, M., Vedula, N., Filice, S., Malmasi, S., & Rokhlenko, O. (2023). Faithful low-resource data-to-text generation through cycle training. ACL 2023

[5] Gururangan, S., Marasović, A., Swayamdipta, S., Lo, K., Beltagy, I., Downey, D., & Smith, N. A. (2020). Don't stop pretraining: Adapt language models to domains and tasks. ACL 2020.

Limitations:
The paper includes a limitation section in the Appendix and a Discussion section in the main paper.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
- The paper highlights the limitations of two existing methods for generating text graphs: 1. The multi-stage approach does not consider multi-hop relations and cannot handle the case where two concepts have more than one relation. 2. The graph linearization approach introduces extra complexity to the Language Model (LM) and the predictions are altered if the generated triples are shuffled.
- Building on this, the paper proposes the Structure Token method. Specifically, it identifies a graph element (node or edge) using seven parts. Then, it transforms Structure Tokens into embeddings using OneHot and orthonormal-like vectors, which are then input into a Transformer Encoder-Decoder model.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The method proposed in the paper can avoid extra computation, such as the duplication of concepts.
- The experimental results presented in the paper show that the model performs comparably to models with a larger number of parameters while using fewer parameters, suggesting that the model might perform even better with an increased number of parameters.

Weaknesses:
- The paper's layout is not aesthetically pleasing, particularly the formatting of Tables 1-3.
- The experiments lack error bars, which diminishes the credibility of the results.
- The experimental results are unsatisfactory and fail to demonstrate the superiority of the model, and the experiments are incomplete.
  - The model's results did not achieve state-of-the-art (SOTA) performance in the G2T/T2G tasks.
  - Lines 257-259 state, ""In conclusion, our structure token approach can achieve comparable performance on text-to-graph generation under similar model size without using special training methods or loss functions."" The results in Table 3 are nearly identical to those of Grapher-small (Text) T2G. Combined with the absence of error bars, it is challenging to determine whether the similarities are due to errors or model efficacy, making it difficult to convince.
  - Existing results only may demonstrate that the model performs comparably to models with larger parameters under fewer parameters. This does not prove that increasing the number of parameters will enhance performance. A more direct comparison of experimental results is necessary to substantiate this claim. The explanations in Section 5, ""Scaling Up,"" are overly subjective and unconvincing, making the paper seem like a work in progress.

Limitations:
Author addressed the limitations.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
gBOQ0ACqoO;"REVIEW 
Summary:
This study reveals that modalities have varying impacts depending on depth, leading to the proposal of DH-Fusion. This method
dynamically adjusts feature weights using depth encoding, improving multi-modal 3D object detection. Results on nuScenes show DHFusion outperforms prior methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well-presented. The structure is clear and easy to follow.
2. Comprehensive experiments on the nuScenes dataset are conducted to validate the effectiveness of the proposed DH-Fusion.

Weaknesses:
1. Lake of Novelty: The Depth Encoder in DH-Fusion is similar to the 3D Position Encoders in PETR (PETR: Position embedding transformation for multi-view 3d object detection). The Depth-Aware Global Feature Fusion (DGF) module and Depth-Aware Local Feature Fusion (DLF) module in DH-Fusion are analog to the Hierarchical Scene Fusion (HSF) module and Instance-Guided Fusion (IGF) module in IS-Fusion (IS-Fusion: Instance-scene collaborative fusion for multimodal 3d object detection). In conclusion, the contribution of this work seems like ""A+B,"" which is limited.
2. For the nuScenes test leaderboard, DH-Fusion achieved a Top 10 ranking only with 384x1056 image size and SwinTiny backbone.
Please provide results when using larger 900x1600 image size and ConvNeXtS backbone.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed a LiDAR-camera modality feature fusion method based on depth encoding for robust 3D object detection. Based on the observation that the LiDAR and camera modality information should have dynamic relative importance depending on the distance of object to be detected, the paper proposed a Depth-Aware Hybrid Feature Fusion (DH-Fusion) strategy which consists of a Depth-Aware Global Feature Fusion (DGF) module and a Depth-Aware Local Feature Fusion (DLF) module. Experiment on the public nuScenes and nuScenes-C dataset demonstrates that the proposed method is robust to various kinds of corruptions and achieves SOTA performance on 3D object detction.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The idea of depth-aware multimodality feature fusion for 3D object detection is reasonable, especially for the detection of distant objects.
2. The ablation study clearly demonstrates the effectiveness of the proposed DGF&DLF module when using BEVFusion as baseline
3. The presentation is clear and the ability of the proposed method on the detection of distant object in Figure 6 is impressive

Weaknesses:
1. How about the algorithm's performance on small object detection? small object could be normal-sized object at far distance or small-sized object in near distance, is it possible that the proposed depth-aware module hurts the detection performance of small-sized object in near distance? since according to Figure 5, LiDAR modality will have relatively larger weights at near distance, but it is in low resolution, so not good for small object detection.
2. Compare with SOTA, the achieved performance improvement is not that significant. as shown in table 1, the performance gap between the proposed method and IS-Fusion is small and IS-Fusion even achieves slightly better mAP, it is not clear whether the proposed method can achieve similar performance improvement as indicated in ablation study when using IS-Fusion as baseline.
3. In Figure 5(b), it would be good to add a color bar to indicate the magnitude corresponding to each color

Limitations:
There is no paragraph explaining the weakness of the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel strategy for LiDAR-camera 3D object detection that emphasizes the importance of depth information in feature fusion processes. The authors argue that different modalities, such as LiDAR point clouds and RGB images, contribute variably at different depths, and this variation has been overlooked in previous works. The key contribution is the Depth-Aware Hybrid Feature Fusion (DH-Fusion) strategy that dynamically adjusts the weights of point cloud and image features based on depth encoding at both global and local levels. The DH-Fusion method surpasses previous state-of-the-art methods in terms of NDS on the nuScenes dataset and demonstrates robustness to various data corruptions. In general, the design is reasonable and performance is impressive.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The paper is well-structured, with a clear abstract, introduction, methodology, experiments, and conclusion sections that logically flow from one to the next.
2.	The authors effectively communicate complex ideas through clear language and comprehensive illustrations, aiding the reader's understanding of the proposed method.
3.	The motivation of design is clear and experiments are extensive.
4.	The idea of depth encoding for dynamical fusion is interesting and reasonable.
5.	The performance is very impressive and the robustness makes the method more applicable to challenging scene.

Weaknesses:
The paper has no obvious weakness except they didn't do experiments on other datasets.
But I think the nuScenes is already large enough to demonstrate the general effectiveness.

Limitations:
There is no discussion of limitation in main text, but a justification is given in Checklist: using an attention-based approach to interact with the two modalities makes the detection results sensitive to modality loss.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces DH-Fusion, a novel Depth-Aware Hybrid Feature Fusion strategy for multimodal 3D object detection that leverages LiDAR and camera data. The key innovation lies in dynamically adjusting the weights of point cloud and RGB image features based on depth encoding at both global and local levels. The authors propose two modules: Depth-Aware Global Feature Fusion (DGF) and Depth-Aware Local Feature Fusion (DLF), which enhance feature integration and compensate for information loss during the transformation to Bird's-Eye-View (BEV) space. Experiments on the nuScenes dataset demonstrate that DH-Fusion surpasses state-of-the-art methods in terms of Novelty Detection Score (NDS) and is more robust to data corruptions, as evidenced by superior performance on the nuScenes-C dataset.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper proposes a novel feature fusion strategy that adaptively adjusts the weights of LiDAR point cloud and RGB image features based on depth
2. The introduction of depth encoding at both global and local levels allows for more nuanced and context-aware feature integration, enhancing the detector's ability to understand the scene's depth structure.

Weaknesses:
1. The authors only present results on nuScenes dataset. The alogrithms should be also evaluated on other prevailing public dataset like KITTI.
2. The depth-aware fusion might be tailored to the specific characteristics of the training dataset, potentially leading to overfitting and reduced performance on diverse or unseen data.
3. While the paper includes ablation studies, a more extensive set of experiments that isolate the impact of different components of the system could provide deeper insights.

Limitations:
1.  While the method shows strong performance on the nuScenes dataset, its generalizability to other datasets or varied real-world conditions might require further investigation.
2. The paper does not provide a detailed discussion on the computational efficiency, which is crucial for practical applications, especially in terms of processing time and resource usage.
3 .The method assumes high-quality, synchronized data from LiDAR and camera sensors, which might not always be guaranteed in real-world scenarios.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
hc9GkYw9kR;"REVIEW 
Summary:
This paper proposes a novel surrogate-assisted evolutionary algorithm named LORA-MOO, the core contribution is the introduce of ordinal-regression-based model  spherical coordinates approximation to SAEA and LORA-MOO can find a good trade-off between optimization efficiency and optimization results.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
This paper provides a novel perspective for modeling surrogates with high efficiency.

The experiments results seem good.

Weaknesses:
Motivation and contributions are limited. To the best of my knowledge, the main contrition of LORA-MOO is introducing ordinal-regression-based model for convergence and spherical coordinates for diversity. However, why do it and what is the connections between them? Besides, the manuscript includes a lot of informal expression. The proposed method is complex and effectiveness is limited.

Limitations:
See questions.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed the LORA-MOO framework, a surrogate-assisted MOO algorithm that learns surrogates from spherical coordinates. This includes an ordinal-regression-based surrogate for 10 convergence and M −1 regression-based surrogates for diversity.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The considered problem is pretty important.

Weaknesses:
Major:
1. Line 113-116, a bit too repetitive. 

2. Line 121, an initial dataset size of 11D-1, too specific.

3. Lines 121-134: the algorithms are described too specifically. It is more suitable for EC journals rather than NeurIPS.

4. Selection Criteria, too simple. 

5. What does LORA-MOO means?

6. Some HV-based MOBO methods are not compared as they are failed to solve many objectives. This argument is not accurate, please consider to run ""https://github.com/xzhang2523/libmoon/blob/main/libmoon/solver/mobo/run_dirhvego.py"", which supports more than ten objectives problems. 


Minor:
There are too many grammar errors in this paper. 
(1) Line 249, they are failed -> they failed. . 

(2) Line 270, HV use -> HV uses . 

(3) Line 175, consider using \max.

(4) line 21, are widely exist -> widely exist. 

(5) line 64, an non-pa .. -> a non-pa..

Limitations:
See weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a surrogate-assisted evolutionary many-objective optimization algorithm, named LORA-MOO. LORA-MOO is composed of a surrogate for ordinal modeling, which focuses on convergence, and m-1 surrogates for distribution modeling, which focus on diversity. Empirical study demonstrates the effectiveness of the proposed algorithm.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper is well-written and easy to follow. Although the proposed algorithm seems somewhat complicated, all the technical details are clearly presented, and the motivations behind them are explained. The empirical study is generally solid, with all the major parameters included in the ablation study, and most of the commonly used test instances are covered. The results show that LORA-MOO outperforms the baselines.

Weaknesses:
Although LORA-MOO obtained better indicator values than the baselines in synthetic problem benchmarks, it is difficult for me to find some fundamental differences between LORA-MOO and previous MOAs. Nor could I see what new insights this paper can provide for solving expensive MOPs. LORA-MOO models the convergence of solutions by a surrogate problem of the domination level, which is a common idea in MOO, adopted by many past methods such as NSGA-II. Such a surrogate is intuitive, but it is unclear to me why it could work better than the existing surrogates such as pairwise relation or function values, and why such a surrogate can be successfully modeled by a Gauss process. LORA-MOO uses m-1 surrogates to predict the spherical coordinate, but it seems identical to predicting function values. LORA-MOO also contains many other components, such as EA, PSO, non-dominated sorting, various clustering methods, and some subset selection mechanisms. These components have long been widely adopted by many MOAs, and there are many alternatives available as well. I agree that these components could usually make an algorithm perform better, but this paper does not adequately demonstrate any necessity for such a combination or any connections between these components. The ablation study appears to be a parameter-tuning experiment, presenting some results under different parameters. However, there is no ablation for the many components in the algorithm, so it is unclear what contributions these components actually make.

Expensive optimization problems are closely connected to real-world applications, and many real-world MOPs are indeed expensive problems. Therefore, I believe this is a very valuable research direction. However, the empirical study in this paper is mainly conducted on synthetic problems. DTLZ and WFG have undoubtedly driven the development of the MOO field, but they have also caused a significant number of researchers to focus narrowly on these synthetic test sets. As a result, there are now many algorithms that perform excellently on synthetic test sets but struggle to adapt to real-world problems. The authors have also conducted tests on NASBench, which is crucial for comprehensively demonstrating the algorithm's capabilities, but the results do not seem to be sufficiently convincing.

Limitations:
This paper does not summarize its limitations. I suggest the authors reconsider the limitations of this work and fully present them in the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a surrogate assisted method for multi-objective optimization. The approach learns a surrogate function with the ordinal values as the regression labels. The ordinal values are generated using a iterative algorithm with the most dominated solutions having the highest ordinal values. The ordinal values are used to train a Kriging model used to select a point for observation using the convergence criterion. Another point is selected using the Kriging model trained on spherical coordinates via a diversity criterion. The approach has several parameters which are tuned via experimentation on real and benchmark datasets.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The approach provides an innovative way for optimizing multi-objective functions by separating out the two objectives thus simplyfying the problem.
  1) The convergence objective designed to select the best solution wrt the ordinal values
  2) The diversity objective designed to improve the diversity of the Pareto optimal solutions
- It is experimentally shown that the method improves the IGD metric compared to several benchmark and real MOO problems.
- The ideas presented in the paper are well motivated and well presented.

Weaknesses:
- The approach presented in the paper is not sufficiently novel. Ordinal regression for multi-objective optimization has been studied before [1]. The differences with related prior work have not been discussed in detail.
- The proposed algorithm has many tunable parameters, and it is unclear how the parameters affect performance on real world problems when they have only been tuned on benchmark problems.
- The real world experiment on NAS shows improved regret eventually, but converges slower than other existing approaches. It is difficult to judge on the effectiveness of this approach based on a single experiment. Experiments on more real world optimization problems are necessary to make a conclusion.
- The paper is missing several notable MOO approaches from the Bayesian optimization community [2,3,4,5].

[1] Yu, Xunzhao, et al. ""Domination-based ordinal regression for expensive multi-objective optimization."" 2019 IEEE symposium series on computational intelligence (SSCI). IEEE, 2019.

[2] Tu, Ben, et al. ""Joint entropy search for multi-objective bayesian optimization."" Advances in Neural Information Processing Systems 35 (2022): 9922-9938.

[3] Zhang, Richard, and Daniel Golovin. ""Random hypervolume scalarizations for provable multi-objective black box optimization."" International conference on machine learning. PMLR, 2020.

[4] Paria, Biswajit, Kirthevasan Kandasamy, and Barnabás Póczos. ""A flexible framework for multi-objective bayesian optimization using random scalarizations."" Uncertainty in Artificial Intelligence. PMLR, 2020.

[5] Abdolshah, Majid, et al. ""Multi-objective Bayesian optimisation with preferences over objectives."" Advances in neural information processing systems 32 (2019).

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
RZk2rxJT55;"REVIEW 
Summary:
The paper introduces the so-called ""Leaky ResNet"" ordinary differential equation. Leaky ResNets are a variant of the NeuralODE with an additional vector field that attracts trajectories to the origin, the strength of which is governed by a parameter $\tilde{L}$ that is later shown to correspond to a separation of timescales. 

The authors consider a system of ODEs resulting from the optimization of the empirical risk with a regularization term inversely proportional to $\tilde{L}$. 

They define the ""Cost of Identity"" (COI), a quantity that expresses the coupling of the ODEs and evolves along the trajectories of the ODEs with initial values depending on the training dataset. They then proceed to show that the solutions spend most time in regions where the COI is close to an optimal value.

The final part of the paper proposes three different discretization schemes for the ODE and provides numerical results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The idea of showing that neural networks have a certain property by constructing a model where trajectories spend most of their time in regions with that property is interesting.

* The authors explain their intuition as well as the underlying assumptions of their derivations and highlight the limitations of their analysis.

* The COI seems to be novel and reflect some interesting properties of ODE models for neural-networks.

Weaknesses:
* The main results are not clearly stated in the abstract or introduction. The author's stated goal is to study Leaky ResNets, it would be nice to have a rigorous statement of the results of that study at the beginning of the paper.

* In the abstract, the authors state that the paper explains the emergence of a bottleneck structure in ResNets. It is not clear how this claim can be derived from the results in the paper.

* There is no rigorous justification that the results from the study apply to neural networks with a finite number of layers.

* One of the main ingredients used in the derivations is quoted from another paper in (l.101). It would be nice to have a short proof in the appendix or some explanation to make the paper more self-contained.

* In general the paper lacks rigor, as the authors themselves note in many positions, it is not guaranteed that all the quantities are finite and that the decompositions are justified. It would be welcome if some of the informal discussions could be replaced by rigorous statements and proofs.

* The propositions/theorems in general lack clear statements of which assumptions are used. For example, at the top of page 4 it is stated that the decomposition only holds under a certain assumption (which is rarely satisfied in practice as noted later in the paper). Most of the discussion in the paper relies on this decomposition, but it is not immediately clear whether the formal results require the assumption.

* In general there is a lot of discussion mixing formal definitions and informal arguments, for example reasoning in terms of quantities that don't exist in most cases of interest. This makes the paper somewhat hard to read. I can understand the author's intention of providing some intuition, which might perhaps be better served by a combination of rigorous definitions together with a toy example where all the rigorous quantities simplify.

Limitations:
The authors appropriately discuss the limitations of their results next to their statements. However, the extrapolation to feature learning in ResNets made in the abstract is not justified by the results of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper maps the dynamics of representations across layers of leaky ResNets to a Largrangian and Hamiltonian formulation, giving an intuitive picture of a balance between two terms: a kinetic energy term which favors small layer derivatives and a potential energy
that favors low-dimensional representations. This intuition to explain the emergence of a bottleneck structure, as observed in previous work.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The paper addresses a timely and important topic: feature learning in DNNs. 

2. The introduction provides a good connection to previous work.

3. The mapping to a Hamiltonian formulation is interesting and provides a valuable intuition. 

4. The propositions and theorems are mostly clearly stated and the proofs seem sound.

Weaknesses:
1. Numerical experiments:
a. Many of the figures are poorly explained and have missing labels etc, e.g. in Figs 1b, 2b what is the color code? 
b. I failed to find a mention of what data the models were trained / evaluated on. 
c. Fig 2c - what is the projection on to? 

2. It is sometimes hard to follow the rationale and motivation for the ""storyline"" of the paper and its different sections could be better connected to each other. 


3. Novelty wrt previous works - in lines 206-208 a difference from similar works is mentioned but this seems is very brief and looks not very significant on the face of it. It would be better to highlight and elaborate on what is new here relative to these previous works. 



Technical comments: 
a. Eqs are not numbered. 
b. I found the notations to be confusing and non standard, e.g. $\alpha_p \in \mathbb{R}^w$ which make it harder to follow the derivations.



Typos:
a. Lines 211-212: ""layers of the layers...""

Limitations:
The authors have adequately addressed the limitations of their results.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies feature learning in Leaky ResNets and shows the emergence of the previously studied Bottleneck structure under certain assumptions. In particular the paper provides a Hamiltonian formulation of the features and their dynamics to show that the ResNet will prefer low dimensional features (low potential energy) when the effective depth of the ResNet is large, which gives the Bottleneck structure. The paper also has a final section on choosing the scales of the residual layers across depths motivated by their theory.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Studies the problem of understanding feature learning in NNs, which is of broader interest in the NeurIPS community.
2. The paper identifies the effect of “effective depth” in Leaky ResNets on the previously observed Bottleneck structure, through a Hamiltonian decomposition into kinetic and potential energy. 
3. In particular, the authors provide a nice intuition that the potential energy is minimised at large effective depths, which corresponds to low rank solutions.

Weaknesses:
1. The paper is unclear in several important moments which compromises readability. For example, is the leakage parameter $\tilde{L}$ suppose to lie in [0,1] (as suggested by line 80) or in [0,\infty] (as is necessary for the “separation of timescales” arguments in section 2.1). Moreover, in line 224 the authors write closed forms for the Hamiltonian but it is not clear how they obtain this object, from the previously stated Hamiltonian on linear 195.
2. Theory seems tied to several simplifying assumptions which reduces its generality in describing/understanding feature learning in standard NNs, e.g. the reliance on ReLU activation, the need for weight decay to minimise parameter norms (though it has been called into question if the role of weight decay is actually to find minimal parameter norms in practice (https://arxiv.org/abs/2310.04415), or also the omission of normalisation layers.
3. On a related note, the theory in the paper seems to have a limited relevance for practice. The one glimmer of this is that the paper suggests changing the weighting on the residual branches in order to evenly balance the difference layers in terms of how much the representations are changing, but this seems underdeveloped at present. It would be worth investigating if this can improve training in practice. Moreover, the paper (and the works on the Bottleneck structure in general) seem to argue that the representations should be changing a lot at late layers because the representation shifts back from being low rank. But this is counter to existing practical works that suggest one can prune late residual layers for improved efficiency https://arxiv.org/abs/2207.07061. This represents a gap between this theory (of Bottleneck structures) and practice to me.
4. The paper studies properties of optimal solutions (e.g. geodesics with minimal parameter norm) in terms of Hamiltonian energies etc (Theorem 4) but does not seem to discuss whether training dynamics will lead to such solutions in practice.

Limitations:
There is no limitations section.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores the feature learning dynamics in Leaky ResNets using Hamiltonian mechanics. By introducing the concept of 'representation geodesics',  the authors analyze continuous paths in representation space that minimize the parameter norm of the network. The study provides a Lagrangian and Hamiltonian reformulation that highlights the importance of balancing kinetic and potential energy during feature learning. The potential energy, which favors low-dimensional representations, becomes dominant as the network depth increases, leading to a bottleneck structure. This structure involves rapid transitions from high-dimensional inputs to low-dimensional representations, slow evolution within the low-dimensional space, and eventual rapid transitions back to high-dimensional outputs. The paper also proposes training with adaptive layer step-size to better handle the separation of timescales observed in the representation dynamics. These insights offer a novel perspective on the mechanisms underlying feature learning in deep neural networks and suggest potential improvements in network training methodologies.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- This paper offers a novel approach for understanding feature learning by applying Hamiltonian mechanics to Leaky ResNets, bridging a gap between theoretical physics and machine learning.
- This paper conducts experiments to validate the findings. Based on experiments, some interesting observations are obtained, which may give some new insights for future works.
- The insights gained from this study have the potential to influence future research in neural network optimization and feature learning, advancing the state of the art in deep learning theory.

Weaknesses:
1. There are multiple typos in the article, which affect readability. Below are several obvious typos, and it is recommended that the authors carefully polish the language of the article.
	-  The third word in line 24, ""phenomenon""$\rightarrow$ ""phenomena"".
	-   In line 27, ""determines"" $\rightarrow$ ""determine"".
	-   In line 40, ""lead"" $\rightarrow$ ""leads"".
	-   In line 68, the preposition ""in"" should be added after ""interested"".
	-  The formula at the end of line 78  should be$$\alpha_{q}^{'}=\alpha_{q/c}.$$
	-  The formula between lines 78 and 79 is also incorrect. The last term should be $$\frac{1}{c}\partial_{p}\alpha_{q/c}(x).$$
	-  The expression of $K_p$  in line 111 is incorrect, because it should depend on $p$.
	-  The formula between lines 112 and 113 is incorrect. The coefficient of the middle term on the right side of the equation should be 1 instead of $\tilde{L}$.
	- In line 145, "" $||\tilde{L}A_{p}+\partial_{p}A_{p}||_{K_{p}^{+}}^{2}$ ""  $\rightarrow$  ""$||\tilde{L}A_{p}+\partial_{p}A_{p}||_{K_{p}}^{2}$"".
	-  In line 159, ""bound"" $\rightarrow$ ""bounded"".
	-  The expression for $C(A)$ in line 190 is incorrect, which should be $$C(A)=\frac{1}{N}||f^{*}(X)-A||_{F}^{2} .$$
	- In line 282, ""$\rho_{l}L <1$ "" $\rightarrow$ ""$\rho_{l}\tilde{L}<1$"".
	- In line 415, ""cones""  $\rightarrow$  ""cone"".
	- In Theorem 7 and its proof, it seems that all instances of $\sqrt{\gamma c}$ should be changed to $\gamma \sqrt{c}$.
	- In proposition 9, $\tilde{Z}_{q}$ should be $\tilde{A}_{q}$ in the formula above line 485.
2. The formula between lines 101 and 102 is a crucial one, so it is recommended that the authors provide the derivation process for this formula.
3. In line 59 of the paper, the definition of $\sigma$ is given. First, the ""+"" in this formula should be replaced with a comma. Secondly, my question is about the last component ""1"" in the definition of $\sigma$. In the proofs of some propositions, is it necessary that $\sigma$ does not include this last component ""1""?
	- In line 153, why $||A_{p}||_{K_{p}}^{2}=||A_{p}A_{p}^{+}||_{F}^{2}$ for non-negative $A_{p}$ ?
	- In line 156, why $||\sigma(A)||_{F}\le ||A||_{F}$ ?
	- In line 450, why $||A_{p_{0}, \cdot i}||^{2}-||\sigma(A_{p_{0}, \cdot i})||^{2}=c_{0}>0$ for all $\tilde{L}$ ?
4. In the proof of Proposition 8, the authors did not explain why the limit exists. Secondly, the formula above line 454 is missing $O(\epsilon^4)$. Additionally, the formula above line 464 is incorrect.

Limitations:
The theory they proposed aims to provide guidance for better training of networks, and their experimental and implementation details have been documented.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
rSSpEmrN0C;"REVIEW 
Summary:
The paper introduces a novel approach, named LayTextLLM, for document understanding tasks, which efficiently integrates spatial layouts and textual data within LLM. It employs a Spatial Layout Projector and introduces two innovative training tasks: Layout-aware Next Token Prediction and Shuffled-OCR Supervised Fine-tuning. Extensive experiments demonstrate significant improvements over previous state-of-the-art models in KIE and VQA. This paper demonstrates the importance of layout information in document understanding tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper introduces a novel approach by integrating SLP and P-LoRA to effectively encode and process layout information. This method significantly improves the interaction between spatial layouts and textual data within LLM, providing a new direction for future research.
2. The paper proposes the LNTP task and SSFT task to enable the LLM to layout information, thereby enhancing its document understanding capabilities and improving performance on document-related tasks.

Weaknesses:
1. Due to miss the crucial visual information necessary for document understanding, this LayoutTextLLM heavily relies on OCR-derived text and spatial layouts. Other works such as LayoutLLM, layoutLMv3, introduces visual information to enhance the document understanding performance.
2. The exploration of the shuffling ratio was conducted only on Key Information Extraction (KIE) tasks. It should also be validated on Visual Question Answering (VQA) datasets to determine if the 20% shuffling ratio is optimal across different types of tasks.
3. The effectiveness of LNTP and SSFT methods should be substantiated with more ablation studies. It is recommended to fine-tune Llama2-7B directly using the existing data for a more comparisons.
4、Although LayTextLLM shows higher performance on DocVQA compared to LayoutLLM, this comparison is not entirely fair as LayoutLLM was evaluated in a zero-shot setting. Moreover, the zero-shot performance of LayoutLMv3 on DocVQA surpasses that of LayTextLLM.

Limitations:
The author has already mentioned in the limitation section of the paper that the proposed model is difficult to handle scenarios where inference relies on visual cues.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces the LayTextLLM method for document understanding, which encodes text positional information in the embedding space of an LLM and trains for effective understanding of document data as interleaved OCR-detected text and bounding box information. The results show improved performance compared to prior works on the KIE tasks, as well as on VQA in many cases.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The treatment of layout information as a modality interleaved with text is logical, and the use of a projection into the LLM’s embedding space to represent bounding boxes is clever and appears to be novel. The tasks approached are important and overall the proposed method does appear to improve document understanding (though this will be more convincing if the caveats listed below are addressed).

I also appreciate the focus on open-source models and data for the method and its evaluation, making the results reproducible.

Weaknesses:
There are some issues regarding the comparisons to existing models, making it unclear how much of the observed improvement is really due to the novel method proposed.

LayTexLLM is implemented with Llama-2-7b, but it seems that many models compared to (e.g. the strong-performing LayoutLLM) may use other LLM backbones, making it unclear whether the superior performance of LayTexLLM in many settings is due to the proposed novel method or the LLM backbone. The results will be more convincing with a comparison of different methods with the same LLM backbone (or at least an analysis of the number of parameters in each model).

It is not clear what OCR engine is used, raising the concern that different OCR engines could explain some of the gaps in performance between models being compared.

There are also issues with how the training is presented that make it difficult to interpret results. Some places (L131, L179, etc.) mention pre-training and SSFT, implying that pre-training means the LNTP training task. However, Sec 4.1 mentions “pre-training” and “SFT”, implying that pre-training refers to SSFT+LNTP and that it is followed by SFT (Supervised Fine Tuning) for particular tasks (VQA and KIE). The results also mention zero-shot and supervised results (e.g. L297), but it is unclear from the text and results tables which results are obtained zero-shot or from SFT, making it hard to understand if the comparisons are fair.

The statements about large improvements over SOTA MLMMs (L13-14, L83-84) seem slightly misleading since LayTextLLM uses OCR detections and thus is more comparable to other OCR-based methods.

LNTP (Sec. 3.2.1) is presented as a novelty but seems to just be the regular language modeling objective. If I understand correctly, this could be toned down to simply say that the added SLP and P-LoRA parameters are updated with a language modeling loss.

Limitations:
Limitations are clearly discussed in Section 5 (which should have the title “Limitations” in plural). Additionally, does the limitation of lacking visual cues apply to text formatting such as bolding or italics? This would connect well to the examples in Figure 6 where bold text is prominent.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work presents an innovative method for integrating layout information into LLMs to enhance document understanding tasks. Instead of treating bounding box coordinates as input text tokens, the bounding box information is embedded into a single token and interleaved with text tokens. This approach addresses the challenge of long sequences while leveraging the autoregressive nature of LLMs. Experimental results demonstrate the effectiveness of the proposed method, achieving state-of-the-art performance and resulting in shorter input sequence lengths.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1.Interleaving layout information and text is novel.

2.The proposed Shuffled-OCR Supervised Fine-tuning is interesting and may benefit other OCR-based approaches.

3.The approach achieves state-of-the-art performance on most text-rich VQA and KIE tasks, validating the effectiveness of interleaving layout and text and significantly reducing input length.

4.The paper is well-written, providing sufficient experimental details, ablations and discussions to comprehend each component of the model.

Weaknesses:
1.In layout-aware pretraining tasks, whether it is beneficial to predict both the bounding box and the text, rather than just the text. 

2.LaytextLLM achieves satisfying performance in various tasks, but it will be better to incorporate the visual modality for more application scenarios.

Limitations:
Please refer to weaknesses.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents LayTextLLM, a novel approach to document understanding that effectively integrates spatial layout information and text into a large language model. Existing methods that integrate spatial layout with text often produce excessively long text sequences. LayTextLLM addresses these problems by projecting each bounding box into a single embedding and interleaving it with text.  The method is evaluated on Key Information Extraction (KIE) and Visual Question Answering (VQA) tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
- Effective sequence reduction: The proposed method reduces the length of text sequences, addressing a common problem in document understanding.
- Performance improvement: LayTextLLM demonstrates improvements in KIE and VQA tasks, showing performance gains over alll state-of-the-art models.
- Evaluation: The paper provides detailed benchmark evaluations on 2 tasks and 7 datasets

Weaknesses:
- Incomplete related work: The paper omits several relevant OCR-based models, such as UDOC, LayoutMask, BROS, LAMBERT, DocFormer and LiLt.
- Insufficient explanation: The repeated claim that DocLLM cannot fully exploit autoregressive features is not adequately explained.
- Limited comparisons: There is no comparison with alternative methods that embed coordinates, such as co-as-token approaches (Lmdx, Shikra, ICL-D3IE).
- Marginal token reduction: The reduction in the number of tokens appears to be limited, and the paper does not clarify whether words or lines are encoded, which could have a significant impact on token reduction.

Limitations:
- Limited comparisons: The paper primarily compares LayTextLLM to DocLLM, which may not provide a comprehensive assessment of its performance.
- Impact of token reduction: The reduction in the number of tokens, while beneficial, appears to be limited and may not provide significant practical benefits in all scenarios.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
PKEmH9ZJfw;"REVIEW 
Summary:
This paper studies weakly supervised causal representation learning with soft interventions.
It assumes that pairs of observations are provided that differ by a soft intervention on one variable in the latent space.
Additionally, the intervention target (and the total number of latent variables) are given.
Identifiability up to element-wise transformations assuming linear mixing and known intervention targets (among other assumptions) is shown.
Experiments on synthetic data compare the proposed method to ILCM, beta-VAE and D-VAE.
An improved D and C score in the DCI framework is shown.
An experiment on semi-synthetic image data is shown.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method extends previous work on weakly supervised CRL and generalizes it from hard to soft interventions, which is an important step for bringing CRL closer to real-world applications, since CRL methods often suffer from restrictive assumptions.
- Also, I appreciate the effort of testing this CRL approach to datasets which are more similar to real-world image datasets.

Weaknesses:
- The mathematical notation is poorly defined and difficult to follow: objects aren't well-defined and notation inconsistently or unusually used. See questions below.
- L136-137: ""a diffeomorphic solution function [...] deterministically maps a value for exogenous variable [...] to a value for causal variable [...]"" That's incorrect. There can be such a map from the set of all exogenous to all endogenous, but not per variable. Consider $X:= U_X; Y:= X+U_Y$, where $U_X$, $U_Y$ are exogenous. In general, there is no deterministic map from $U_Y$ to $Y$.
- Even though the DCI framework for evaluation of the learned representation is cited, only two of the metrics (D and C) are used, whereas Informativeness (I) is omitted without comment. That seems suspicious to me.
- The synthetic experiments test a rather simple setting on few data generating processes. The observation dimension is 4 (same as latent dimension) and the mixing is linear (as far as I can tell). We have seen in previous work (e.g. [2]) that the complexity of the mixing function is a crucial element in the recoverability of the latents. For example, the more nonlinear the function, the harder it is to recover latents. Furthermore, only 10 data generating processes are tested. That seems quite limited, given that the dimensions of the data is so small.


**Minor:**

- L133: ""decoder function"" is a bit of an odd choice for something that relates to the ground-truth data generating process. Usually, this is a part of a model. ""Mixing function"" would be more appropriate.
- There is a period missing in L251.
- L345: There is a follow-up version of DCI that takes more aspects of the learned representations into account [1]. It could be considered for the next iteration if time permits.
- L363: ""As mentioned in [27, 25], causal graphs are sparse and in the G5 case, where the graph is fully connected, the proposed method cannot identify the causal variables well."" The two references **do not say that causal graphs are sparse**. You may refer to the sparse mechanism shift hypothesis, which says that changes between environments are assumed to stem from changes in few mechanisms of the causal graph. This is a statement about sparse changes, not sparse graphs.

Limitations:
- As far as I can tell, no latents were uncovered (theoretically or experimentally) for either nonlinear mixing or unobserved intervention targets. Therefore, it seems to me, it should be mentioned that the results are for the linear case and weakly supervised. The title and abstract make the impression as if the more general CRL problem with paired observations is solved.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new approach **ICRL-SM** that performs implicit causal representation learning (mapping from noise to latent variables) by using causal mechanism switch variable to model the soft intervention effects.

&nbsp;

### References
[1] Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score- based causal representation learning with interventions. arXiv:2301.08230, 2023.

[2] Burak Varıcı, Emre Acartürk, Karthikeyan Shanmugam, Abhishek Kumar, and Ali Tajer. Score- based causal representation learning: Linear and general transformations. arXiv:2402.00849, 2024.

[3] Julius von Kügelgen, Michel Besserve, Wendong Liang, Luigi Gresele, Armin Kekic ́, Elias Bareinboim, David M Blei, and Bernhard Schölkopf. Nonparametric identifiability of causal rep- resentations from unknown interventions. In Proc. Advances in Neural Information Processing Systems, New Orleans, LA, December 2023.

[4] J. Brehmer, P. De Haan, P. Lippe, and T. Cohen. Weakly supervised causal representation learning. In Advances in Neural Information Processing Systems, volume 35, pages 38319– 38331, 2022.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The idea of implicitly modeling causal effects using switch variables is very interesting. The experiments have shown promising performance on synthetic and high-dimensional image datasets compared to several baselines.

Weaknesses:
1. My most significant concern is that this paper is generally not well written and, thus, pretty hard to follow. For example, the calligraphic letter $\mathcal{Z}$ was used throughout to denote both causal variables (e.g., line 137) and its domain (e.g., line 139).
2. The assumption of a diffeomorphic causal mechanism is pretty strong. I am aware that [4] made a similar assumption. Yet, it is not a very common assumption for latent causal models in other causal representation learning literature (e.g., [1, 2, 3]).
3. Line 278: The Gaussianity assumption of the causal and exogenous variables might be hard to satisfy in realistic settings.

Limitations:
The main paper did not discuss limitations. The authors pointed them out in the checklist, but I would have appreciated it more if they had adequately addressed the limitations in the main paper.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a novel approach for learning implicit causal representations through switchable mechanisms, specifically designed to handle soft interventions which are more realistic but challenging compared to hard interventions. The authors introduce a causal mechanism switch variable to model the subtle effects of soft interventions and establish the identifiability of causal models under certain assumptions. Their proposed method, ICRL-SM, demonstrates improved performance in learning identifiable causal representations over baseline methods through experiments on synthetic and real-world datasets. The paper contributes a new perspective to causal representation learning with potential applications in various domains where understanding causal relationships from observational and interventional data is crucial.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. A standout feature of this paper is the introduction of a switchable mechanism to model the nuanced effects of soft interventions. Traditional causal inference methods often focus on hard interventions, which are impractical in many real-world scenarios due to the need for stringent control. The paper's approach to incorporate soft interventions through a causal mechanism switch variable is groundbreaking. It allows the model to adapt to changes in causal relationships post-intervention, providing a more realistic and flexible framework for causal representation learning.
2. This paper proposes Augmented Implicit Causal Models, which is an innovative concept that extends the scope of implicit causal representation learning. By integrating the causal mechanism switch variable into the model's solution functions, AICMs can capture the intrinsic characteristics of each causal variable while accounting for intervention effects. This approach sidesteps the need for explicit parameterization of the causal graph, which is a complex and often intractable task. The innovation here lies in the model's ability to implicitly learn the causal structure while directly modeling the effects of interventions.

Weaknesses:
1. The method's effectiveness relies on several key assumptions, including knowing the targets for intervention, interventions being atomic (indivisible), and variables following a multivariate normal distribution. These assumptions might not be realistic in many real-world settings where interventions can be complex, and data may not be normally distributed.
2. While the paper's experiments on synthetic and specific real-world (Causal-Triplet) datasets demonstrate the method's potential, it's unclear how well it generalizes to other data types or different domains.

Limitations:
No

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work is in causal representation learning that utilizes interventional data. It involves two types of common interventions: hard interventions and soft interventions. It’s known that soft intervention is more general since it covers hard intervention. But it is also more challenging since parental relations remain. This work proposed identifiability results using a causal mechanism switch variable designed to change between different causal mechanisms by component-wise transformations. Adequate experiments were provided to verify they claim.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Pros:

1. Overall, the writing is clear and with good context.
2. Adequate experiments were provided, including real-data experiments. Also, they offered good observations of the comparison with baselines. This is a big plus to their contribution.

Weaknesses:
Cons:

1. It's unclear what the unique theoretical contribution is to this switch variable since both types of interventions were proposed before. Could the author clarify the theoretical contribution? Especially regarding the original technical hardness compared to [1] and also [38].

2. In the synthetic experiment, it seems that you use the linear decoder, which loses the purpose of training a neural network to handle non-linear issues. Did you try non-linear functions and see how it goes? Besides the switch variable utilizing interventional data, what is the difference between these settings and those classical linear unmixing methods (like linear ICA or its other assumption variants)?

Some minor issues/recommendations:

Line 161, the $e_i$ is not defined, if you mean causal variable, then you already have defined it as $Z_i$.

Line 187, the ‘\s’ is not defined before but later in line 195.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
VajjTXRj6J;"REVIEW 
Summary:
Based on the analysis of the shortcomings of DPO (Section 2.3), the authors proposed a simple reward distillation approach (Section 3.1) to align language models and a pessimistic variant (Section 3.2). These approaches outperform the vanilla DPO.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The analysis in Section 2.3 extends the result in the IPO paper [23].
- The reward distillation approach is simple and natural, giving good empirical results.

Weaknesses:
I think that this paper is technically well done. However, it has a limited application scope.

The authors aim to address the shortcomings of offline alignment methods like DPO, while maintaining an offline approach. However, online alignment methods such as PPO or Reinforce-style algorithms naturally alleviate these offline learning issues. This raises a question: Given the authors' findings on the limitations of offline alignment, why not adopt an online alignment method like PPO directly?

Admittedly, the distilled DPO method and its pessimistic variants are less complex than PPO. However, these proposed offline variants introduce additional complexities to vanilla DPO by requiring separate reward models and separate training phases. This makes them more akin to online methods. Furthermore, the experiment section lacks a comparison against online method baselines, leaving it unclear if the distilled-DPO variants offer a better performance--compute tradeoff compared to vanilla online methods like PPO.

To be clear, the paper offers intriguing insights. However, these techniques seem niche. They benefit those who prefer robust generalization in preference optimization without using online data, an assumption not very well communicated in the paper.

In my opinion, there are ways to increase the method's applicability. While the proposed distillation approaches are positioned as offline methods, to my understanding, they are also applicable to an online regime. For example, in Equation (7), instead of sampling $(x, y_1, y_2)$ from an offline dataset, as the authors currently do, we can sample $x$ from an offline dataset and then sample $(y_1, y_2)$ from the model in training. The learned models, in this way, are directly comparable to those learned via online alignment methods like PPO, but in a simpler approach than these online baselines. If the authors could demonstrate that these models outperform PPO or require less complexity in training, they could expand the method's application scope and impact.

Limitations:
See ""Weaknesses"" section above.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the limitations of DPO in LM alignment by proposing a reward model distillation approach. DPO, while efficient, often leads to overconfident and degenerate policies due to limited preference annotations. The authors introduce a method that trains LMs to match the distribution from a reward model, improving robustness and performance, particularly in biased preference datasets. Their approach also includes a pessimistic extension to handle uncertainty in reward models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper makes a considerable theoretical contribution by addressing the critical issue of degeneration in DPO, a problem of significant concern in the community. The authors present a well-structured and clearly written analysis that helps in understanding the overfitting commonly observed with the DPO. The proposed method of reward model distillation is both theoretically sound and intuitive, offering a robust solution that potentially improves upon traditional DPO methods. I appreciate this approach and its theoretical support.

Weaknesses:
The main concern about this paper is the evaluation. As discussed in Section 2.3, DPO can shift nearly all probability mass to responses that never appear in the training set, also called OOD extrapolation in other papers. This issue arises when there are few annotations per instance (x, y1, y2). Therefore, it is expected that the authors should demonstrate how their proposed approach mitigates this problem, maybe by presenting the log-probabilities of the winning and losing responses. Specifically, the log-probability margin between winning and losing responses should not keep increasing (I assume this is true?).

Additionally, I can see the paper presents results showing robustness against distribution shifts in preference data. But what is the factor that makes the proposed algorithm learn the policy whose underlying preference distribution closer to the true preference distribution in biased setting? I would be willing to increase my score if these concerns are solved.

Limitations:
I have no concerns about the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors discuss and give formal results on the limitations of DPO that have been observed in practice, and investigate reward model objectives for 1) distilling reward differences into the generator (eq. 7), and 2) pessimistic ""minimax"" distillation over a family of reward models, to mitigate these limitations. The theoretical equivalance of the ""forward"" and ""reverse"" pessimistic formulations (eq. 8 and 9) of the standard RLHF objective (eq. 1) is shown, and an approximate pessimistic objective (eq. 10) with a minimum over distillation losses for the reward model family considered in a Langrangian term. Results on TL;DR preference data that is biased to varying degrees to prefer long responses (Figure 2) shows good results for distilled models generally, and that pessimistic optimization over a family of reward models  optimized for varying response lengths improves results in irregular settings (i.e. when shorter responses are preferred).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- A well written, insightful paper, with well formulated, novel objectives for preference optimization.

Weaknesses:
- The results, as the authors acknowledge, are currently quite limited. These methods should really be tested on at least one additional preference task, and for results utilizing multiple models, pessimism should be compared with other basic ensembling strategies. 
- The gamma parameter is annealed during training, suggesting that the setting is important and perhaps sensitive. Some ablations and discussion around this seems prudent.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
ojFEP11Cqj;"REVIEW 
Summary:
The authors propose a energy-based generative boosting method. They try to maximize the log-likelihood functional delta with a second-order expansion. This lead to a boosting algorithm with deltas as steps in the log-likelihood. Instead of scaling the steps with a fixed predefined value (as a anti-overfitting hyperparameter), they rely on linear search to get better performance. They initialize f0 as uniform and use trees as weak learner to learn the deltas. They derive the objective with trees, it is well explained and looks similar to other second-order method objectives like XGBoost. They use MCMC to sample from Q(x) as is typical from energy-based approaches. They use Gibbs so they only need to sample from one dimension while keeping the rest constant. However since there are t trees, this is quadratic in t. They use some form of accept/reject to sometimes accept previous iteration samples in order to not have to re-samples new samples given the quadratic cost. They include a probability of refresh in order to not just always accept old samples. They propose interesting ways of regularizing the approach. They provide good literature review of related methods. It seems like Section 4 could be integrated with the related work section.

Figure may appear unimpressive to a generative expert unfamiliar with trees. But, being able to generate good samples from MNIST using only decision is an impressive feat (even if MNIST is downsampled). This paves the way for trees being used on more complex data.

Table 2 shows nice prediction results, and they do extensive hyperparameter tuning. However, ML Efficiency is not the best metric, it only focus on prediction. A generative model could produce low-diversity fake data that lead to good classifiers. I would recommending adding some distribution metrics such as the Wasserstein distance (which is computable in low-dimension and is used for high-dim data such as images/videos in the form of the widely popular FID/FVD). There are also other useful metrics for quality and diversity, I recommend looking the extensive choice of tabular-data metrics described in https://proceedings.mlr.press/v238/jolicoeur-martineau24a.html. However, the only essential one in my opinion is having a distance in distribution. It can be the Wasserstein distance or something like the MMD distance.

Except for the missing distribution metric, everything else in the results is good and the methodology is sound with good hyperparameter tuning. I would encourage the authors to add a distribution metrics.

The method is sound, novel and quite interesting. The presentation is very well made. Being the first tree method achieving good image data samples is impressive in my opinion. This is a very high quality paper.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The method is sound, novel and quite interesting. 

The presentation is very well made. 

Being the first tree method achieving good image data samples is impressive in my opinion. 

This is a very high quality paper.

Weaknesses:
Missing a distribution metric, the ML efficiency is not a adequate metric on its own.

Limitations:
If the authors had to downsample MNIST from 28x28 to lower, than there are scaling limitations that should be added.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a boosted tree algorithm that performs distribution learning using an energy-based formulation. Inspired by methods like XGBoost, it is claimed to achieve high performance not only in generative ability but also in discriminative performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed method incorporates techniques that can be leveraged because they inherit from tree boosting models, such as the approximate sampling algorithm. Moreover, as a model that possesses not only generative quality but also discriminative performance, it has a wide range of applications.

Weaknesses:
This study is not theoretical; therefore, the validity of the proposed method must be confirmed through experiments. However, there are some questions regarding the experimental settings. 

Also, from an algorithmic perspective, since methods unique to tree ensembles are incorporated, they could potentially offer advantages in terms of computational complexity compared to other methods. However, evaluations regarding efficiency have not been conducted. If there are advantages, it seems opportunity loss. 

Please refer to the Questions section.

Minor:
The evaluation of variance is presented separately in Appendix G, but it is difficult to compare. Therefore, I would like it to be summarized in one table.

Limitations:
Computational cost of sampling is larger than existing methods.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed an energy based generative boosting algorithm analogous to XGBoost, which can be used as generative model as well as be applied to discriminative tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The energy-based boosting is novel. The proposed method is capable of both generative sampling and discriminative tasks, enabling broad methodological applications.

Weaknesses:
My concerns regarding the proposed method and the experiments are detailed in the questions section.

Limitations:
I do not identify significant limitations other than the ones discussed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes to extend the success of tree-based methods in discriminative tasks to generative modelling, which is implemented via an energy-based generative boosting algorithm (NRGBoost). Specifically, NRGBoost directly extends the tree-based tabular models by replacing the discriminative objectives with a generative one, which seems novel to me.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper has well-founded rationales: (1) Tree-based models are performant in discriminative tasks, and thus they are highly likely to also be performant in generative tasks on tabular data, and (2) existing tree-based generative methods do not preserve the tree structures well.
2. The paper is well-written, especially the notations.

Weaknesses:
1. **[Important]** Some highly relevant benchmark methods are missing, including ARF (tree-based) [1], GOGGLE (diffusion) [2] and TabPFGen (energy-based) [3].

2. In Line 325, the authors claim that the proposed method “significantly” outperforms other methods, while the significance test seems missing.

3. I would suggest the authors add comparison results on the computation efficiency. Because NRGBoost basically employs the same architecture as traditional gradient boosting trees, the computation efficiency should be higher than most other network-based generative models.

4. There seem to be some typos throughout the main text: “I.e.” (Line 272)

5. **[Important]** Code is not provided. I remain conservative about the results claimed in the paper.

[1] Watson, David S., et al. ""Adversarial random forests for density estimation and generative modeling."" International Conference on Artificial Intelligence and Statistics. PMLR, 2023.

[2] Liu, Tennison, et al. ""GOGGLE: Generative modelling for tabular data by learning relational structure."" The Eleventh International Conference on Learning Representations. 2023.

[3] Ma, Junwei, et al. ""TabPFGen–Tabular Data Generation with TabPFN."" NeurIPS 2023 Second Table Representation Learning Workshop.

Limitations:
The authors detail the limitations of NRGBoost in Section 7.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
gsott1UXlq;"REVIEW 
Summary:
GATSM effectively captures temporal patterns and handles dynamic-length time series while preserving transparency, outperforming existing GAMs and matching the performance of black-box models like RNNs and Transformers.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* This paper is easy to understand.
* GATSM can be understood as a linear representation with good transparency.
* Surprisingly, this method improves performance while providing better interpretability compared with black-box.

Weaknesses:
* It is not clear how multi-head attention works in Definition 3.1 to learn temporal patterns. My understanding is that the global feature interacts with the current feature in Eq.3, so why is it that the input to the attention is not $x$ but the transformed $\tilde_x$. And then, are temporal patterns captured from attention? 

* The addition of attention to NBM is under-motivated, so why not just replace $w^{nbm}$ to attention weight, so that you can learn one less set of parameters $w^{nbm}$.

* The experiment lacks DLinear [1], a strong baseline and providing interpretability. I think it's highly relevant.
1. Are Transformers Effective for Time Series Forecasting? AAAI 2023.

* Given the emphasis on interpretation or white-box modeling, qualitative experiments of the contributions/explanations need to be compared rather than visualization. If there is no ground-truth of the contribution, occlusion experiments in post-hoc methods [2, 3] can also be designed to explore the trade-off between performance and additive features.
2. Encoding time-series explanations through self-supervised model behavior consistency, NeurIPS 2023.
3. TimeX++: learning time-series explanations with information bottleneck, ICML 2024.

* In Table 7, why is the throughput of GATSM so much lower than NBM? Does NBM add up as features at the time level without sharing?

Limitations:
It's hard to deal with the higher-order interactions, e.g. GA^2M/GA^NM in time series, since time series often have long time points and the high complexity of gam-based techniques. In addition, the temporal-level causal interrelationships can be further explored.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces GATSM(Generalized Additive Time Series Model), designed for handling multivariate time series data with a focus on transparency and interpretability. Using independent networks to learn feature representations and transparent temporal modules to learn cross-time step dynamics, GATSM effectively learns temporal patterns and maintains interpretability.  It achieves comparable results achieves comparable performance to black-box time series models on various datasets, and proves the transparent predictions with cases.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. A key strength of GATSM is its focus on transparency, providing clear insights into the decision-making process, which is crucial for applications in high-stakes domains like healthcare.
2. The paper presents a thorough evaluation across multiple datasets, including Energy, Rainfall, AirQuality, and several healthcare datasets, showcasing the model's robustness and generalization capabilities.
3. The authors provide extensive details about the experimental setup, including data splits, hyperparameters, and computational resources, facilitating reproducibility.

Weaknesses:
1.  The need for a large number of feature functions can limit scalability (even if it is reduced from TxM to B), particularly with high-dimensional data.
2. The dataset tasks encompass both 1-step forecasting and classification, each requiring distinct evaluation metrics. Presenting all results in a single table without proper clarification leads to confusion. 
3. The selected baseline black-box models are relatively simple. Consider including one or two state-of-the-art methods for a more comprehensive comparison.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a Generalized Additive Model for time series, combining feature embedding and attention layer. The proposed solution is evaluated on forecasting, binary and multiclass classification, over 8 datasets, against black box and transparent models. Global, local, time-focused and feature focused interpretability methods are provided.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Experimental evaluation is convincing: the presented model has state of the art performance. On the transparency side, the comparison between models allows the author to postulate on the presence of absence of interactions of covariates, and time-specific patterns. Evaluations of weights at the final linear layer give varied visualizations.

The reasoning behind the model construction and component is clear, and ablation experiment for each component were provided.

Paper is clear with no major problem in writing.

Weaknesses:
I am not sure if the work is original. The idea of using DNN first on the time axis without covariate interaction is not new, but wether there is a model similar to the proposed solution, I do not know. The review of previous works focuses on Generalized Additive Models, but a similar neural structure may have been presented without being positioned as a GAM.

One improvement to be done would be to add more clarity on figure captions. In its present form, it requires back and forth to the text to understand both what is plotted and what conclusions to draw from it.

Limitations:
Several limitations are identified: possible overfitting of the model due to overparameterization in NBM part, slow attention mechanisms that do not benefit from state of the art methods, and the fact that it was not evaluated for long sequences (and might not be suited for them).

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This work aims to build transparent models for the time series domain for better interpretability. Specifically, they proposed a work called Generalized Additive Time Series Model (GATSM) that consists of independent feature networks as well as a temporal attention module to learn temporal patterns. The corresponding model can be written into a scalar form to ensure interpretability/transparency. The authors applied their model to several datasets, and showed that GATSM can outperform existing generalized additive models. The model can also be used to interpret the features in original dataset.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Important motivation. Building transparent models for time series is a critical task.
- The scalar representation of features seems clean (eq 11)

Weaknesses:
While this work is not of my direct expertise, I think the following contents have room for improvement:

1. Experimental results are weak.
- Black-box Time Series Models seem out-of-dated. The authors should consider better models such as TimesNet, PatchTST, FreqTransformer, or Informer for commonly used black-box models.
- Forecasting tasks use R2 score for evaluation, but an R2 score of 0.07 (or in general, below 0.5) seems very low. The authors should show some visualization examples to ensure the model is functioning.
- Figure 4/5 are not self-explainable, the authors should try to explain what is happening in those figures, and how the interpretability is quantified.
- The work could benefit from synthetic dataset, where casual relationships are manually crafted and thus can be evaluated.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
V5Sbh42uDe;"REVIEW 
Summary:
This paper proposes a method to improve weakly-supervised semantic segmentation using sparse annotations. The authors introduce a Potts relaxation method, which is an extension of the traditional CRF-like methods. The experiments are conducted on the PASCAL dataset.

Soundness:
3: good

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Pros.

1.	The proposed method appears to be technically sound.

2.	The authors provide detailed explanations of their method.

Weaknesses:
Weakness:

1.	The comparisons with other methods are not comprehensive. Recent advances [1,2,3] in sparsely annotated semantic segmentation are not discussed or compared. Point-supervised is more challenging but it is ignored. The survey in the paper is limited, making it difficult to understand the contributions.

2.	The use of a small dataset like PASCAL may not demonstrate the superiority of the proposed method. Results from larger datasets like Cityscapes and ADE20K (Table 6) should be emphasized and compared with state-of-the-art methods.

3.	Self-labeling is also well employed in many weakly-supervised methods that use image-tags labels. It seems that your method can also applied to them, am I right? If so, experiments on the COCO dataset would be important.

4.	DeepLab is an old-fashioned network architecture. The authors should prove that Potts relaxation can also work on Vision Transformer since ViT has demonstrated the SOTA performances in both fully- and weakly-supervised semantic segmentation. If using ViT without Potts can already get good performances, the relaxation may be not important.

5.	In recent fully- and weakly-supervised semantic segmentation works, CRF-like methods have been discarded due to their computation cost. The efficiency of Potts relaxation (FLOPs) is not evaluated in your paper, which is very important. 

6.	With complex formulation, on the very toy dataset PASCAL, the mIoU only increases by 1% in Table 5 (77.1 to 78.1), while the increased computation cost has not yet been evaluated. 

7.	As stated in the abstract “ … can outperform full pixel-precise supervision on PASCAL”, which is not convincing to me. The fully-supervised performance should be considered as the upper bound of weakly-supervised learning. Such results may be caused by unfair settings. 

8.	The importance of relaxation should be introduced in the abstract since it is your main contribution. In Section 2.1, the two claimed reasons are not intuitive to me. “Manage the scope of this study” seems not a strong motivation. 

9.	With the development of ViT and vision pre-training, the improvements of CRF-based self-labeling become marginal. Thus, I think it is important to evaluate ViT and vision-pretraining (DINO[4]) backbone with your relaxation. I am wondering whether Potts can improve performances beyond these strong backbones.

Overall,  I think it would be better if the authors could conduct more comprehensive experiment comparisons and related work discussions. The motivation for relaxation should be introduced in the beginning.

Refers:

1.	Sparsely Annotated Semantic Segmentation With Adaptive Gaussian Mixtures. CVPR 23

2.	Label-efficient Segmentation via Affinity Propagation. NIPS 23

3.	Modeling the Label Distributions for Weakly-Supervised Semantic Segmentation. Arxiv 24

4.	DINOv2: Learning Robust Visual Features without Supervision. TMLR 24

5. CC4S: Encouraging Certainty and Consistency in Scribble-Supervised Semantic Segmentation. TPAMI 24

Limitations:
Yes.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper considers semantic segmentation under scribble supervision. The paper studies relaxations of the Potts model and proposes a framework for generating soft pseudo-labels, which benefit over hard pseudo-labels in that they can represent uncertainty. The paper highlights problem cases with two standard relaxations, the quadratic and bilinear, and proposes a normalized quadratic relaxation. Moreover, the paper proposes to use a collision cross-entropy loss between the prediction and the pseudo-labels. Different settings are evaluated experimentally, and the proposed approach is compared to the state-of-the-art.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Overall, the paper is well written, and the proposed approach is intuitive and should be fairly easy to reproduce, even without code.

The problem under consideration is important as it aims to reduce the manual annotation challenge in image segmentation, which is otherwise costly and time consuming.

The choice of Potts relaxations and cross-entropy terms are supported by experiments, and the proposed approach is further compared to previous methods, showing strong performance.

Weaknesses:
Some details are missing or unclear in the main paper. Considering that the soft self-labeling loss in (6) is a key contribution, it would be useful to include some details regarding the optimization of the pseudo-labels in in the main paper from Appendix B. Additionally, pairwise affinities based on intensity edges are mention at line 42, but it is unclear whether they are used in the proposed approach, see questions.

The proposed approach is only evaluated on a single dataset in the main paper. The results on additional datasets in the appendix should be moved to the main paper to better communicate the empirical findings, as they are easy to miss otherwise. This also raises some confusion about Appendix C which says that three datasets are used in Section 3.5, but results are only reported on PASCAL. What was the reason to exclude these from the main paper?

No error bars. Considering that some settings have fairly similar performance, e.g. in Table 3, standard deviation or similar over multiple runs would be useful.

Some figures have excessively small fonts, e.g. Figures 3, 4, 6.

Limitations:
The paper does not discuss limitations.

Societal impacts are not discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work proposes a soft self-labeling framework for weakly supervised semantic segmentation using scribbles. This model-agnostic framework requires only the joint optimization of network predictions and pseudo labels, guided by specific loss functions: collision cross-entropy and log-quadratic Potts relaxations. The design choices are supported by theoretical concepts and experimental results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The work systematically analyzes common loss functions for weakly supervised semantic segmentation. By investigating theoretical concepts and experimental results, it discusses the advantages and disadvantages of these loss functions. Based on this comprehensive analysis, the work concludes by recommending the use of collision cross-entropy and log-quadratic Potts relaxations for a soft self-labeling framework.

Weaknesses:
Method
- The proposed method is tested on DeepLab. Although the theoretical concept should hold for any model, its effectiveness on other segmentation models remains unknown. 
- The work explains the rationale behind design choices from a theoretical perspective, but it does not clarify why these choices lead to specific pseudo-labels from a vision perspective. For instance, in Figure 5, NN successfully segments the bicycle while DN does not. Is this because the bicycle is a minor class? 
- The work does not provide mIoU per category, leaving it unclear whether the method is effective for all categories or just a few major ones.

Minor Writing Issue:
- The legends of the figures, such as Figures 1, 4, and 6, are small.
- Figure 1 (b) contains an extra icon (house).

Related works:
- There are more image-level WSSS works than just [4, 21]. The author should consider including additional relevant works in lines 16-18.

- Weakly-Supervised Image Semantic Segmentation Using Graph Convolutional Networks
- Weakly supervised learning of instance segmentation with interpixel relations.
- Extracting class activation maps from on-discriminative features as well.
- Boundary-enhanced co-training for weakly supervised semantic segmentation.
- Learning pixel-level semantic affinity with image-level supervision for weakly supervised semantic segmentation
- Self-supervised equivariant attention mechanism for weakly supervised semantic segmentation
- Group-wise semantic mining for weakly supervised semantic segmentation

Limitations:
I agree with the author that the training time is one of the limitations.
I hope the author can consider my suggestion in the above section to build a link between the design choice and the pseudo labels from the computer vision perspective.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new framework for Weakly-Supervised image Segmentation. The main contribution is to use a soft-labelling approach which is considered superior to classic hard labelling because it can keep track of the centanty of the label. Then different forms of second order potts relaxation and cross-entropy are evaluated.
Results show that the proposed approach with its best setting is able to perform better than previous approaches and comparably to a fully supervised approach with only 3% of annotated pixels.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
\+ The paper is in general well written and easy to follow

\+ Nice illustrations help to understand the proposed approaches

\+ Ablations on the most important components help to understand the significance of each component

Weaknesses:
\- It is not clear if results are significant. For instance is 1% a meaningful improvement or it can be generated by just noise? It is important to see results on multiple runs with also standard deviation.

\- The motivation about soft-labelling because it keeps information about the label certainty is not clear to me

\- In tab. 5 it is not clear what is the base model you start from. Could you report also the base model with standard cross-entropy and hard potts model? It seems that results are very good because the baseline is already quite good.

\- Authors did not say much about the computational cost of the proposed approach compared to common models.

Limitations:
Authors did not mention the possible limitations of the proposed approach.
One possible limitation is the need of additional hyper-parameters to tune which can take time. Authors should comment on that.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
7nbAots3f8;"REVIEW 
Summary:
This paper introduces a new method to learning unsigned distance functions. Specifically, the method leverages local shape priors which brings geometry priors and is also able to handle noises and outliers. The results demonstrate that the proposed method outperforms previous baselines, especially in the corrupted situations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The idea of learning local shape functions is widely explored in the SDF or occupancy based methods, but is not yet introduced to the field of UDF-based reconstruction. This paper combines the strengths of local functions and UDF which can represent shapes with arbitrary typologies.
2. The paper is overall easy to follow.
3. The comparison are comprehensive by introducing recent works as the baselines. 
4. The method can better handle the noises and outliers compared to the previous prior-free methods.

Weaknesses:
1. The idea is straightforward to learn local shape functions as the prior for global shape reconstruction. Similar ideas are proposed in SDF-based methods. I would like to see more insight in the differences to the previous methods and also the importance of introducing the local geometric priors for UDF learning. 
Local Implicit Grid Representations for 3D Scenes (CVPR 2020)
Surface Reconstruction from Point Clouds by Learning Predictive Context Priors (CVPR 2022)
Deep local shapes: Learning local sdf priors for detailed 3d reconstruction (ECCV 2020)
2. The visualization is not quite convincing in the geometry details. For example, the reconstructions of real-scans in Fig.6 seems worse than the results of other methods in their papers. The local shape functions is expected to produce reconstruction with more details and sharper edges, but the scene reconstructions lack details. 
3. Also, some reconstructions are too fat, which in my opinion, is caused by the inaccurate UDF near the zero-level set, since the method use DCUDF for UDF meshing.

Limitations:
Please refer to the strengths and weaknesses above.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes an approach to reconstruct 3D surfaces from point clouds, using unsigned distance fields. The proposed approach consists in training a specific neural network architecture to predict UDF values from local point cloud patches, which can be triangulated using UDF meshing methods. The paper mathematically analyses the possible local patches that can appear, smooth and sharp, and trains the proposed architecture with them. Once trained, the neural network is queried for UDF values from a point cloud, and it computes them by extracting a local point cloud patch for each query point and applying the information learned during training. A denoising module is employed to reduce the impact of noise and outliers. Experiments are carried out on ShapeNet cars and DeepFashion3D, synthetically sampling point clouds from the dataset meshes and reconstructing surfaces using a number of baselines, an extracting meshes using DCUDF. Results show that the performance of the proposed method is not the strongest on clean data, but it is so in the presence of noise and/or outliers. Experiments on real-world point clouds are shown qualitatively.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
3: good

Strengths:
The analysis of the possible kinds of local surface patches is well thought out and significative, enabling the possibility to treat the problem of surface reconstruction locally instead of globally, with an extensive training set that covers virtually all possible cases. I believe this is a good contribution, and the main strength in the paper.
The performance of the method on noisy data, albeit the noise was introduced synthetically, is good; the performance with synthetic outliers looks impressive.
The writing quality is generally good, with some exceptions on clarity (see weaknesses).

Weaknesses:
I believe there are a few weaknesses in the paper, mostly regarding clarity, architecture validation and experiments. I list them in no particular order.
1) The paper proposes a complex architecture, consisting of two branches, a cross-attention module, fully connected layers and a denoising module. The latter is only qualitatively justified in Fig.8, but the rest of the architecture is not validated in any way. A (quantitative) experimental evaluation of why this architecture is suitable for the task would be needed, in my opinion, as well as the intuition that led the researchers to arrive to this final architecture.
2) The cross-attention mechanism is unclear to me: the two branches (points and vectors net) output a latent code. What is the meaning of a cross-attention module on a single input token (actually one as K-V, one as Q)?
3) In the experiments, a few details are not specified, for example the query grid resolution and the number of points used for the Chamfer distance.
4) A time evaluation is completely missing. The paper claims ""Our method is computationally efficient"" in the introduction, which is partially justified by the comparatively better training times and storage requirements with respect to GeoUDF, but there is no evaluation of the time required by the method to reconstruct a surface, compared to the other methods. Notice also that DCUDF is an extremely slow meshing algorithm, so the evaluation should be performed both with it and without it.
5) The paper claims ""superior performance in surface reconstruction from both synthetic point clouds and real scans, even in the presence of noise and outliers"". The experiments however show no superiority in the absence of noise and outliers. Moreover, there are no quantitative experiments on real scans, making it impossible to evaluate whether the noise robustness on synthetic noise also translates to real data. Additionally, the qualitative experiments shown on real data lack comparison with the baselines. Thus, in general, I find the experimental section incomplete and not fully convincing.

Limitations:
A limitation of the method on completing incomplete surfaces has been correctly addressed and disclosed. 
The limited performance on clean data is shown in the experiments, but not acknowledged in the claims in the introduction.
A quantitative assessment of the performance on real scans and of the time required to reconstruct the surface are missing, making it harder to fully evaluate the possible limitations of the method with respect to the claims.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a novel training strategy to learn Unsigned Distance Fields from local shapes. The idea is to train the model on a dataset of point cloud patches characterized by mathematical functions representing a continuum from smooth surfaces to sharp edges and corners. Although trained only on synthetic surfaces, it demonstrates a remarkable capability in predicting UDFs for a wide range of  surface types. The method is evaluated on “Car” category of ShapeNet dataset in addition to DeepFashion3D, and ScanNet datasets. Furthermore, the paper shows results  on scans from real range scan dataset , in addition to ablation studies highlighting the robustness of the method to noise and outliers.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written and easy to follow.
- **Novelty**: While training on local patches is not new, the design of the local shapes and network architecture is novel and effective.
- **Performance**: The paper shows good generalization results while being only trained on local synthetic patches and more robustness to noise and outliers in the input pointcloud.

Weaknesses:
- **Inference time**: While the method is better in terms of data storage space, data preparation time and training time, no comparison regarding the  inference time is provided.
- **Patch radius**: The patch radius used at test time is a crucial hyper-parameter for the method. A discussion about how to set this parameter and how it depends on the point cloud density/size would strengthen the paper.

Limitations:
The authors adequately addressed the limitations of the method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a method for open surface reconstruction from 3D point clouds. They train a network to predict unsigned distance functions (UDFs) from point cloud patches using only synthetic data of quadratic surfaces. Evaluation shows that the trained network generalizes well to other complex patterns and is more resilient to noise when reconstructing 3D surfaces from point clouds.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The idea of training a UDF regression network using only synthetic data of quadratic surfaces is quite intriguing. This approach allows for a controlled and systematic way to generate training data, which can be more consistent and free from the imperfections and variability found in real-world data. 

Using quadratic surfaces as a basis for synthetic data is quite interesting where the authors argue that quadratic surfaces can approximate various local geometries. I have some doubts about this but it is reasonable and novel to a certain extend.

Weaknesses:
I have three main concerns: potential biases with the synthetic training data, the evaluation scheme, and the applicability in practice due to the patch radius.


## Potential Biases with Synthetic Training Data: 

The use of primitive geometrical patches as training data for a UDF regressor might introduce biases. The observation that any local geometries can be approximated by quadratic surfaces is only valid at a very fine resolution, which requires dense point clouds to observe reliably. It is unclear how many patches have been synthesized and whether they provide a good approximation of universal geometrical primitives. Some analyses would be helpful here: for example, using the ShapeNet car dataset, cropping all local patches from each car, and finding the closest synthesized one to check for approximation errors. Are there any patterns with sufficiently high approximation errors? Can we perform these analyses at different resolutions and see how they correlate with surface reconstruction?

## Evaluation Scheme

The testing data is simulated to match the scenarios the method is designed for: the point cloud is quite dense, and artificial noise is added similarly to the training data. There is no quantitative evaluation for real-world scanned data. 

## How sensitive is the method to different patch radii?

The value of 0.018 is oddly specific. I suspect that with a larger value of r, the method will generate overly smooth surfaces (as shown in Figure 6 - right), and with a smaller value of r, it will generate holes due to the point cloud not being dense enough. Overall, there is an inherent issue with this trade-off that may not be resolvable with this approach. Detailed experiments varying the patch radius and analyzing the impact on reconstruction quality would be helpful.

## Additional Concerns

- Ablation studies on the network architecture are missing. I am unsure about the roles of the two branches and the cross-attention mechanism. There is a potential issue with the reliance on Point-Net for embedding computation. Is this network pre-trained on other datasets? If so, we should be careful with the claim of using only synthetic data, as pre-training on real data could influence the results.

- It also seems that the method could be quite slow. The authors should include a speed test to provide insights into the computational efficiency of the proposed approach. Evaluating the method's runtime on different hardware setups and for varying point cloud sizes would give a clearer picture of its practical applicability.

Limitations:
The authors acknowledged that the proposed method cannot handle incomplete point cloud. However, it is unclear what how resilient it is when dealing with this.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
5tOVh81aze;"REVIEW 
Summary:
While existing scaling law studies look at compute-optimal pretraining, this paper considers scaling laws in the context of both pretraining and downstream performance. They perform scaling experiments and find that performance is predictable even in overtraining, and average downstream performance is also predictable.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
I think this is a solid paper that attempts to answer an important question. While I’m concerned about the lack of novelty (see the weaknesses), I overall lean towards accepting the paper. I think that the fact the paper reproduces findings from other papers using a different methodology is a good sign that the overall results are correct, and is valuable information (e.g. [Owen 2024](https://arxiv.org/pdf/2401.04757) also finds that average downstream performance is more predictable than for individual downstream tasks). To me, this is the primary contribution of the paper, and an important one as such.

Weaknesses:
To my mind, the largest weakness of this paper is the lack of novelty. In particular, my understanding is that the most important findings are that (1) performance is predictable in overtraining, and (2) average downstream performance is predictable. I’m not sure why (1) should be surprising – doesn’t a parametric scaling law, such as method 3 from the Chinchilla paper, also give the ability to predict loss when overtraining? I think the authors could improve the motivation for this consideration by providing a back of the envelope calculation: for instance, I plugged in the model size and dataset size for Gopher 280B into the Chinchilla scaling law and got a predicted test ppl of ~7.3 on MassiveText. However, Gopher actually had a (validation) perplexity of ~8.1, this constitutes a relative error of around 10% – substantially larger than the relative errors obtained by the authors of this paper. If the authors can provide an argument of this sort I'd find that helpful. 

I thought (2) was a more interesting claim, but I’ve seen this analyzed in Owen 2024 (https://arxiv.org/pdf/2401.04757), albeit with a different methodology. As such, I felt that the core results of the paper weren’t very novel. However, I’d be happy to update my assessment if the authors can provide evidence that my understanding is incorrect. 

One interesting point that the authors mentioned is that performance on individual tasks is less predictable. But this is only mentioned in passing, and I felt that it could be expanded upon a fair bit. What are the implications of this observation? Are there any patterns for which individual tasks are or aren’t predictable? 

I’m slightly concerned about data leakage being an issue for the downstream tasks, given that the training data (The Pile and RedPajama) covers a wide swath of the internet, and some of the downstream benchmarks have been criticized for data leakage or having label errors. 

Minor comment: The last paragraph of section 5 is a bit confusing: “There has been a rise in over-trained models [113, 114] and accompanying massive datasets [112, 82, 104, 3]. For example, Chinchilla 70B [45] is trained with a token multiplier of 20, while LLaMA-2 7B [114] uses a token multiplier of 290.” This makes it sound a bit like Chinchilla is overtrained, which I don’t think the authors are trying to say, so I’d suggest something like the following instead: “For example, while Chinchilla 70B is trained compute-optimally with a token multiplier of 20, LLaMA-2 7B…”

Limitations:
I felt that the authors did a good job describing some of the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a scaling law for the “Chinchilla over-trained” regime where models are trained on many more tokens (in this paper, up to 30x) than Chinchilla-optimal. They motivate a scaling law relating pre-training compute and “over-training” to validation loss. They empirically demonstrate that the proposed scaling law accurately predicts the validation loss of a 1.4B 32x over-trained model and a 6.9B Chinchilla-optimal model. They then study a simple scaling law relating perplexity to downstream benchmark error. They select a subset of 17 benchmarks for which a 154M parameter models performs 10% above random chance accuracy, and show show that average downstream error of the 1.4B and 6.9B models is predictable.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The setting considered by the paper is very relevant at the moment, as major model releases in the past year fall precisely in the “Chinchilla over-trained” regime. The work is also novel, as I am not aware of prior work that proposes and empirically validates scaling laws tailored for the Chinchilla over-trained regime.

The proposed scaling law in the over-trained regime is well-motivated both by prior scaling laws and by further empirical observations in the over-trained regime (Figure 2). While the models trained (5-8 1e21 FLOPs) are at least two orders of magnitude smaller than the latest open models (e.g., Gemma 2, Llama 3, Qwen 2), the experiments are of a large enough scale for a proof of concept.

The paper is clear and it gives sufficient details on the experimental set-up.

Weaknesses:
My main concerns are twofold: authors do not compare with the standard scaling laws of Kaplan et al. (fitted on their own testbed), and it is unclear how authors choose the models used to fit their scaling laws (Table 1).

Authors do not compare their over-trained scaling law with that proposed by Kaplan et al. The authors could fit this scaling law as in Hoffman et al. , Section 3.3, without any additional model training. Specifically, how well can the standard Kaplan et al. law predict the validation loss of the 1.4B over-trained model, when fitted on the model testbed with N < 1B described in Section 3.2?

Regarding the claim that validation loss (resp accuracy) is predictable with 300x (resp. 20x) less compute, I find this misleading, since authors train and evaluate a reasonably large model testbed, but only report the compute required for the 5 (resp. 6) models that they ultimately choose for the fit.  The authors do not discuss how this “train”/“test” split was chosen. Clearly, the train/test split should be chosen before seeing the evaluation results for any of the models, rather than including models until the fit seems ""good enough"", or choosing the smallest subset for which the fit is “good enough”. Otherwise, both the claim of 300x/20x compute as well as Figure 5 are misleading. Similarly, Figure 1 would be misleading, and it should include all models with N < 1.4B. 

The authors consider token multiplier M <= 640, however current models are even more overtrained. If am not mistaken, for Llama 3 8B, M ~= 2000. Demonstrating the validity of the proposed scaling laws for the amount of over-training of current models would have been ideal, even at smaller model scales.

Lastly, the proposed scaling laws are validated at substantially lower compute scales than current models with publicly available weights. It would have been ideal to at least see results for over-trained 7B models. I understand that the experiments presented in the paper already require a substantially amount of compute, and more closely matching the compute scales of recent models would be unfeasible for most research labs — therefore I am not taking this point into consideration when scoring the paper.

Limitations:
Limitations are adequately discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the power laws (scaling laws) of neural language models, particularly from the perspective of over-training and the relationship between validation loss (perplexity) and NLP downstream tasks. The authors define over-training as the situation where runs consume large amounts of computational resources, and they introduce a token multiplier, M. It is computed by D / N, where D is the number of training data tokens and N is the number of parameters. Through various model training setups and three different training corpora, the authors demonstrate that the validation loss can be computed and predicted using an equation that includes M. They also introduce another equation that illustrates the relationship between validation loss and downstream task error.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main contribution of this paper is the exploration of the scaling laws in language models concerning over-training and NLP downstream tasks. These results, including equations and practical outcomes, are beneficial for researchers and engineers developing large language models. As highlighted by the authors, these insights are valuable for researchers in their future work.

Weaknesses:
The authors mentioned several limitations and future work in the paper. I agree with them, and especially the ‘scaling up’ part is the primary concern of this paper. The model sizes range from 0.011B to 6.9B, but open-source models are larger than these sizes - for instance, Llama 2 starts at 7B, and Llama 3 starts at 8B [1]. Furthermore, model size is crucial for techniques such as CoT [2]. I hope to hear the authors' opinions on this concern.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
MICrZCQzoN;"REVIEW 
Summary:
The authors establish generalization error bounds for non-iid data based on the online-to-PAC conversion.
In particular, the authors extend the online-to-PAC conversion techinique to non-iid settings utilizing the online learning with delayed feedback. In the paper, the authors illustrate

1. a method of non-iid online-to-PAC conversion,
2. a method of converting online learning algorithms to their delayed counterparts,
3. the resulting non-iid generalization bounds combining 1. and 2., and
4. an extension of 1. to dynamic hypothesis learning.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The well-organized, well-motivated paper on generalization error analysis and online learning.
- The claimed results are novel and help us better understand the generalization in dynamic environments.

Weaknesses:
- Notable logical gap: Lemma 3 only gives a regret bound **independent** of $P^*$, but it seems Corollaries 3 and 4 need $P^*$-dependent regret bound. I believe this is fixable, but still, need some fix.
- More discussion on related work: Are there any results previously not known, but can be proved with the proposed method?

Limitations:
Limitations are not explicitly discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the generalization error of statistical learning in a non-i.i.d setting, where the training data distribution could have temporal dependency. They develop a framework that reduces the generalization error in this case into the regret of an online learning problem with delayed feedback. Then, they present a series of instantiations of their results with different online learning algorithms and assumptions on the data generation process.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The propose framework is elegant and wildly applicable to many real-world data generating process.
2. The paper is easy to follow, and the setting is well presented. The introductory section for the reduction in the i.i.d case is very helpful in understanding the context.

Weaknesses:
1. The proposed framework seems to be a straightforward extension of that in the i.i.d setting. Technical novelty of this work seems limited.
2. The instantiation of the framework given in this paper is still very high level and abstract (for example, the algorithm considered is the general follow the leader algorithm). It would be beneficial to have some specific instantiations and show that if the obtained results are comparable to the existing ones, similar to what has been done in Lugosi and Neu (2023).

Limitations:
Nothing necessary stands out.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper extends the Lugosi-Neu(2023) framework for upper bounding the generalization error of statistical learning algorithms to the non-i.i.d. setting, by considering that the training samples are drawn from a suitably mixing stochastic process. They show that the existence of a delayed online learner with bounded regret in the Online-to-Batch game of Lugosi-Neu(2023) against an offline learner implies that the offline learner has low generalization error even when trained on data drawn from a mixing stochastic process. The authors also investigate settings such as FTRL and MWU under this model.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
The paper addresses an important question - How to bound generalization error of statistical learning algorithms trained on non-i.i.d. data in a manner which is independent of the complexity of the statistical learner? The paper does a fine job at establishing the notion of such bounds and some conditions under which such bounds are recoverable.

Weaknesses:
The techniques seem to be largely an amalgamation of several papers which have refined the ""blocking technique"" in various settings, and the key observation that the introduction of delay in online games lead to the online cost being a sum of martingale difference sequences, which essentially allows them to use proof techniques of Lugosi-Neu(2003). 

The delayed online learning setting is new to me, and I am not sure how to evaluate its significance versus the standard online learning setting. In fact, it seems like getting similar bounds w.r.t. the standard online setting would involve significant more technical novelty, compared to the current setting. 

The stochastic process also seems to be quite well-behaved in comparison to previous works in the literature such as Mohri and Rostamizadeh (2011), who give generalization bounds (in the pure offline setting) under stochastic processes with weaker notions of convergence.

The authors mention that the results hold for a specific class of bounded loss functions, but I could not find specific details regarding this point afterwards in the paper.

Limitations:
The discussions of limitations in the current work is limited, and lacks discussion as to why certain choices were made (or overlooked).

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper focuses on learning from non-i.i.d data. Specifically, the authors develop a framework that derives generalization guarantees through a reduction to an online learning game with delays, where achieving low regret translates to low generalization error. They present specific bounds when using EWA and FTRL as the online learning algorithms. Additionally, the framework is extended to accommodate dynamic hypotheses.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The paper is well-written and easy to follow. The proposed framework is general, novel, and elegantly designed, facilitating a clear translation between low regret in online learning algorithms and low generalization error in the context of mixing data. I appreciate the simplicity and flexibility of the framework, and overall, it represents a valuable contribution to the field

Weaknesses:
While I did not go over the entire details, I did not find any major weaknesses.  
- A small typo is line 43: ""..we propose propose.. ""

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper provides a framework for proving generalization bounds for non-i.i.d. data sequences, building upon a recent framework introduced by Lugosi and Neu (2023) that reduces PAC to online learning. This technique recovers some known PAC Bayesian bounds for non-i.i.d. scenarios and various other implications.

The original framework by Lugosi and Neu (2023) introduced an online learning ""generalization game"", where the regret of the online learning algorithm can be translated into a generalization bound in the offline setting. This framework has been shown to recover some important generalization bounds with a clean analysis.

In this paper, the generalization game is extended to a game where the learner gets to see the observation with delay (where there's no delay we are back to the original framework). The regret of the online learner in this game can again be translated into a generalization bound. When the delay is large, it increases the regret of the online learner (one term in the generalization bound) but decreases the term determined by the property of ""how much the sequence is non-i.i.d."".
Online learning with delays has been studied extensively, and so ""off-the-shelf"" algorithms and regret bounds can be used to derive/recover generalization bounds.

One nice application of the technique is that it allows the analysis of stationary mixing processes that have been studied extensively (the assumption on the non-i.i.d. sequences is weaker than known mixing assumptions). Another interesting application is to popular dynamic predictions such as autoregressive models and RNNs.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Deriving generalization bounds for non-i.i.d. settings is a central effort in the machine learning community and is of great interest. The framework suggested in this paper allows us to do so in a very clean way and might be useful for more applications.

Also, the assumption on the non-i.i.d. sequences is quite weak, which is an advantage.

Weaknesses:
The paper heavily builds on the framework of Lugosi and Neu (2023). This is not a weakness, but my question is: besides extending the online game to accommodate delays and using ideas from the online learning literature, what are the technical challenges/contributions in this paper?

Another question - do you know if the paper by Lugosi and Neu (2023) was already published? I'm asking since I didn't go over the proofs in this paper.

Limitations:
Limitations are properly addressed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
VD3lb3XWqa;"REVIEW 
Summary:
This paper proposes a test of self-awareness similar to that of the Turing test. It starts by motivating the need for an ""objective measure"" of AI progress, given that the Turing test has been passed by LLMs. There is a brief discussion of literature on self-awareness and related topics in philosophy.  The test of self-awareness is then presented, which asks whether the system in question can distinguish its own output from external inputs. The *Nesting Doll of Self-Awareness* is presented as a generalisation of the aforementioned test. The test is then applied to autoregressive LLMs and it is found that LLMs are not self-aware as they are unable to reliably detect what text it produced. This is followed by a brief discussion on why self-awareness matters and whether humans would do well at this test.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
An interesting idea that could be developed into an interesting way of testing algorithms.

Weaknesses:
- The paper is drawn out in terms of substance. The idea of the test is only introduced on page 4.
- The writing is, at times, verbose and unnecessarily complex.
- The obvious question would be whether humans would pass such a test. Though this is briefly considered in Section 5.2, I think it is not properly discussed.
	- The author claims that this is easily solved by humans because of memory. This is not a fair comparison to LLMs. Goodhart's law would come into play here as we could easily add such a memory structure to an LLM system.
	- Would a human be able to resolve extremely generic text that any human wrote?
- The paper lacks rigorous analysis and testing that I would expect to see in a NeurIPS main track paper.

Line 29: ""Last year, however, the Turing test was broken"". I am inclined to say the Turing test was passed before and very few considered it to be a reliable indicator, even before GPT.
Line 32: ""AI has become untethered to any definitive, objective measure or permanent, agreed-upon benchmark."" There are certainly many benchmarks and standards in various subfields.

Limitations:
None are written about.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a simple test, along the lines of the Turing test, to determine whether or not an LLM (or an generative text model) is _self-aware_ (or _self-conscious_).
The general structure of the test is that the LLM participates as one of the interlocutors in a dialogue and then is asked aster the fact to identify which interlocutor it was.
What this test is primarily aiming to determine is whether the LLM can distinguish its own actions from those of ""the world"" (i.e., the other interlocutor).

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
### Originality
- (major) I am not familiar with the literature surrounding the philosophy of LLMs, but it seems like the paper is addressing this topic from a unique angle.
### Quality
- (minor) The self-awareness test operationalizes a notion of self-awareness.
- (minor) The experiments are straightforward and indeed test some notion of self-awareness.
### Clarity
- (minor) The paper is mostly easy to read.
### Significance
- (minor) This tool could be useful for for testing certain otherwise ""soft"" claims of whether a model is self-aware or not.

Weaknesses:
### Originality
- (minor) I did find this paper (https://arxiv.org/pdf/2401.17882) which seems relevant, although it is on the recent side, so I don't think its critical that there is an extensive comparison (although a brief would be nice).
### Quality
- (major) It seems like the notion of self-awareness which the proposed test measures is relatively narrowed or not well contextualized by the rest of the paper.
### Clarity
- (major) The paper needs to distinguish related concepts of related to self-awareness and adjust its discussion accordingly (see Questions).
- (minor) Quantitative summary of the results should be in the main paper.
### Significance
- See Quality.

Limitations:
I think the paper could use a more extensive and explicit discussion of the limitations of a test of self-awareness. For example, the test may be testing one notion of self-awareness is that ""less significant"" than some other notion, but people could misunderstand what it is attesting and over-ascribe capabilities to LLMs that they do not actually possess.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to answer the profound philosophical question of whether the state-of-the-art, transformer-based large language models (LLMs) pose self-awareness. As the author points out in this paper, this question, which I agree to be legitimate and important, is rarely addresses in a rigorous, academic manner, which renders off-the-cuff and often-sentimental discussions on social media the primary forum for its discussion. Starting from a well-written introduction that nicely sets the philosophical background for this question, the authors proceeds to present a dialog-based approach to probe self-awareness of LLMs. This approach can be concisely summarized as ""a binary selection task, namely identifying one of two conversing roles as the LLM itself in a multi-turn dialog, while controlling for the potential confound of role labels"". This approach is then applied on two of the most popular publicly-available LLMs, including llama3-7b-instruct and GPT-3.5-turbo-instruct, the result of which showed that the accuracies of the LLMs on this task are low and often at a near-chance level under challenging labeling schemes. The author therefore concludes that the current generation of LLMs do not possess self-awareness.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
S1. The intellectual audacity of this paper is laudable and impressive. As the author points out, it is imperative to address important philosophical questions such as self-awareness (the focus of this paper) at this stage in the development of LLMs and generative AI, where transformer-based language models can exceed human performance in many tasks involving natural language, including passing of the Turing Test. Yet there is a lack of systematic and rigorous effort in this topic in the field of LLMs and AI as a whole.

S2. This paper is written in fluent and idiomatic English language, demonstrarting the author's good command of philosophy of mind and its history. As such, this paper is easy to follow and a pleasure to read. Wielding this good knowledge of philosophy of mind, the author clearly spells out a working definition of self-awareness in this paper and skillfully avoids the potential pitfalls of entangling with complex and controversial topics such as consciousness and free will.

Weaknesses:
W1. Despite the strengths mentioned above, and contrary to the claims made by the author in this paper, the role-identifying methodology devised by the author in this paper is problematic as a test for self-awareness. Below I will lay out the rationales behind this critique of mine:

  1. Being able to identify which of the two interlocutor is ""the LLM itself"" is different from being aware of the fact that the said self is participating in a conversation. One one hand, the ability to perform this identification task is not a sufficient condition for self-awareness. One the other hand, neither it is a necessary condition for self-awareness. To see why it is not sufficient for self-awareness, follow the thought experiement in Section 5.2 of the paper. Suppose there is another human, e.g., the spouse or a close friend of the author of the hypothetical text (or the conversing self in the original problem formulation). It is totally conceivable that such a person, despite being a different individual (i.e., not the ""self""), would be able to identify the text written or uttered by the person of interest with high accuracy. If this is the case, can we say that the other person is ""self-aware on behalf of the person of interest""? Such a conclusion would be absurd. But by the same token, if a human person, or more relevantly, an entity such as an LLM, performs the identification task with high accuracy, it cannot be ruled out that such as system is simply good at this identification task per se, perhaps due to a good memory of previously-composed text or perhaps due to a certain mechanism that allows they/it to algorithmically execute this identification. These two possibilities are entirely feasible within the current technology surrouding LLMs. For example, one can add a memory cache to an LLM to store all the text generated by the LLM, and give the LLM access to this cache during subsequent text generate (i.e., a form LLM tool use or retrieval-augmented generation or RAG). Would this augmentation constitute a legitimate form of self-awareness? As another example, one can also write a program that uses the LLM to score the tokens from a turn of a dialog in a token-by-token fashion, and therefore assign an overall score to each turn of the dialog. Based on the scores, the program, built on the LLM core, would be able to identify the self role accurately. But would we be willing to call such a program (containing the LLM as a part of it) self-aware? In my opinion, answering yes to either or both of the two previous questions would effectively give self-awareness too general and perhaps too trivial a definition, in a way similar to acknowledging that someone who can identify a certain person's words with high accuracy is ""self-aware for the person"".
  
  2. To see why the ability at this role-identification task is not necessary for self-awareness, consider a human who has dyslexia and a form of amnesia that renders them 1) unable to comprehend visually-presented historical text and 2) unable to remember what was previous written or spoken by themselves (or by others), but is otherwise cognitively and linguistically normal. When faced with this identification task, such an individual would struggle at this role-identification task, but they are nonetheless self-aware at the moment when they are writing or uttering words. This is due to the presence of the ""efference copy"" in the intact sensorimotor loop of the individual's brain (cited by the author in Section 6 of the paper). Furthermore, such an individual would also be self-aware during other, non-linguistic activities thanks to motor efference copies and proprioception and other well-established sensory mechanisms of human self-awareness. This analogy illustrates the point that a back performance by an LLM at the role-identification task does not form a solid basis for claiming the LLM lacks self-awareness. The LLM may be self-unaware due to other reasons, e.g., the lack of an efference-copy mechanism during the auto-regressive inference that is comparable to efference copies seen in the human brain.

Limitations:
See weakness W1 and questions Q2 I wrote above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes an objective inventory for testing the self-awareness of an artificial intelligence agent. The core idea is to test whether an agent can distinguish content it has produced from content originating from the external world. The experiments show that no large language models have demonstrated self-awareness.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Self-awareness is an essential topic both philosophically and scientifically, especially in the era of AGI. A clear, objective, and commonly accepted test is needed. This paper endeavors to address this need.

Weaknesses:
1. The test proposed here is not sound at all. Can a human always distinguish the words they produce from those produced by others if their memory is impaired?

2. I am particularly confused by the concept of ""nesting doll."" In my opinion, this should be an analogy for a system in which each level can always contain, reflect, or dominate the previous level, which should be the essence of self-awareness. However, here I do not understand how thoughts, interoception, or material possessions are related to this concept.

3. The writing style is far from the standard expected for a NeurIPS conference paper. The paper is filled with informal expressions that belong in a blog or a Twitter debate, rather than a conference paper. For example, lines 49-50 (""even if you object...""), lines 69-70 (the Oedipus metaphor is hard to understand), line 155-156, line 166 (using a fictional movie for metaphor), and lines 172-176 (does the arm-moving example have anything to do with the rubber-arm experiment?). I acknowledge that it is challenging to present a novel work on self-awareness within a traditional writing paradigm, but the excessive use of imprecise metaphors and informal philosophical musings is unacceptable. A more formalized and precise expression is expected.

Limitations:
No.There are some obvious flaws in the proposed test that the author chose not to address.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
8JmUmTgKiY;"REVIEW 
Summary:
This paper first generalizes the Kolmogorov–Smirnov (KS) distance from one-dimensional spaces to multidimensional spaces and proposes the Kolmogorov-Smirnov GAN, which formulates the generative model by minimizing the Kolmogorov-Smirnov (KS) distance. Theoretical results are also given in this paper and the experiments also show the superiority of stability during training, resistance to mode dropping and collapse, and tolerance to variations in hyperparameter settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
I like the idea of generalizing the one-dimensional KS distance to multi-dimensional, which I had thought about before but failed to achieve. The motivation and writing are good, and the experiments of KSGAN seem to have achieved good results.

Weaknesses:
1. I'm skeptical about some parts of the theory. (See questions for details)
2. It seems that the advantages of KS distance over JS divergence and Wasserstein Distance are not explained.
3. The idea of reformulating a distance between distributions to a GAN model seems to be old and is now unlikely to attract readers' interest.
4. The experimental setup is relatively simple, only comparing with vanilla GAN and WGAN on Synthetic, MNIST, and CIFAR10 datasets
5. According to the experimental results, the advantages of KSGAN lie in the stability of training and resistance to mode dropping. A significant issue is that with current network architectures and training techniques, these two problems are rarely encountered.

Limitations:
The authors have discussed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel variant of the generative adversarial network that uses the Kolmogorov-Smirnov distance to align the generated distribution with the target distribution. This distance is calculated using the quantile function, which acts as the critic in the adversarial training process. Experiments are conducted on synthetic distributions and small image datasets to show that the proposed KSGAN performs on par with the existing adversarial methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The paper is well-presented and easy to follow.

2. The claims and methodology designs are well supported by theoretical analysis.

Weaknesses:
1. It is still unclear why we need another adversarial design based on KS distance. The vanilla GAN paper shows that the designed bi-level optimization process can already be seen as optimizing the distance between the generated and the target distribution. Then, what are the specific advantages KS distance can bring within the adversarial framework?

2. The experiments are merely conducted on synthetic datasets and small image datasets. It is unclear whether the proposed method can be adapted to larger-scale datasets or incorporated into more advanced frameworks like StyleGAN. Moreover, the compared baselines are limited to early works, and the experimental results of KSGAN are worse than those of WGAN-GP. Thus, I do not see many advantages of KSGAN in terms of the presented experiments.

Limitations:
Please see the discussions above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce a generalized KS distance applicable to high dimensional spaces, formulate the corresponding dual problem, and use adversarial training to construct a generative model that minimizes the GKS between data and generated distributions.

The paper is well presented and appears technically correct through what I've seen, though I didn't check the proofs in details.

The main problem is that there's no clear motivation for why using the GKS is beneficial at all (either theoretically or practically). As such, despite being novel, I don't see any clear impact from the paper. Furthermore, the final algorithm is quite complicated and the results are fairly underwhelming, so at the end of the day the cons dramatically outweigh the pros of the newly introduced algorithm.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The paper is well written. It reads easily and it is clear what they want to do. The contribution of a generalized KS distance to multidimensional spaces and the algorithm to approximate it are to the best of my knowledge, novel.

Weaknesses:
The main problem I have with the paper is that I don't see any clear advantages of using the KS distance (gan) as a replacement of other distances like the Wasserstein one, or their GAN equivalent. The only mention of this, which should arguably be the most important thing in a paper introducing a new GAN, is in lines 224-228 of page 7. The authors claim there that they don't need to maximize the supremum in (5) which is false depending on how to interpret it, if you just take any set C in (5) you end up with |P_F(C) - P_G(C)| which is just measuring one moment for a given characteristic function, and far from being anything meaningful (and the same holding true for most IPMs). The results are also not particularly interesting to merit the claim that there's anything particularly different or benefitial on using this new formulation.

Limitations:
No clear motivation or benefit from using their algorithm.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposed a new kind of GAN training method called KS-GAN. The method is based on minimizing the Kolmogorov–Smirnov distance. The KSGAN updates the generator by minimizing an upper bound of the generalized KS distance. It updates the discriminator (or the critic network) by using energy-based model training with regularization terms. The KSGAN is a novel attempt to explore new approaches to train generative adversarial networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* (1) The paper studied using generalized KS distance to train GAN generators is a novel attempt to extend the GAN literature. 

* (2) Some theoretical arguments about implementing the empirical KS distance using neural networks is novel yet constructive.

Weaknesses:
My main concern about the paper is its weak evaluation baselines and questionable practical usage:

* (1) Though the idea of using new objectives for training GAN generators is attractive, the practical usage of KSGAN seems questionable, especially for high-dimensional data. For instance, in the CIFAR10 generation experiment, the author compares KSGAN with WGAN-GP and Vanilla GAN, which have shown weak empirical performances. However, it is well-known that, for CIFAR10 data, the StyleGAN2-ADA[1] model is a strong baseline GAN model. I think it would strengthen the paper a lot if the authors could somehow show strong performances of KSGAN using StyleGAN2's architectures and implementation techniques. However, I do admit that such a requirement may be too tough for new methods.

* (2) The KSGAN's critic function is constructed with EBMs. However, even with regularization terms, energy-based models are well-known for poor scaling ability to high-dimensional data. This may prevent the practical usage of KSGAN for real-world high-dimensional data.

Limitations:
The author has addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
lEDuaGGiCV;"REVIEW 
Summary:
In this paper the authors introduce LUCY, a new LLM based framework for converting text-to-SQL to query databases. Primarily this framework focusses on addressing user queries to databases that contain a large number of tables with complex relations between them.
The core idea of this approach is to decompose the query generation process into distinct stages. LLMs (GPT-4) are utilized for generative tasks such as identifying relevant tables and attributes and generating the SQL query. Meanwhile a deterministic constraint solver (OR-Tools) is employed to map relationships between these elements. In essence LUCY processes a user query through 3 phases namely MatchTables, GenerativeView and QueryView phases. In the MatchTables phase the goal is to identify the relevant tables and attributes. This is accomplished by iteratively prompting a Large Language Model (such as GPT-4) to identify relevant tables and attributes based on the user query and the database model, which includes the schema and an optional list of high-level design patterns. The database model is presented in a hierarchical manner and explored using a breadth-first search approach. Once the relevant relations and attributes are identified a schema graph is constructed and solved using a constrainst solver (i.e to identify the optimal path to join the tables) to build a view in the GenerativeView phase. A LLM is then prompted to generate a SQL query given the summary view and the user query in the QueryView phase. The authors further conduct experiments that demonstrate that the proposed technique achieves a better execution accuracy as compared to the existing state-of-the-art techniques on standard datasets(ACME, BIRD). Furthermore, they also introduce a show large improvements on a new benchmark dataset (Cloud Resources).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The literature review is comprehensive and the paper does a good job at clearly defining the problem to solve. 
2. The novelty of the proposed approach lies in the decomposition of tasks involved in generating SQL queries. By employing LLMs to handle specific subtasks, it effectively circumvents the need for LLMs to perform complex reasoning. A core distinguishing factor from prior research is the use of constraint solvers to identify the relevant paths for joining the identified tables. 
3. The authors also demonstrate that the proposed approach achieves a better execution accuracy than the existing SOTA on several benchmarks.

Weaknesses:
1. The paper ends abruptly without a clear and comprehensive conclusion. The paper presentation needs improvement in this regard.
2. The authors introduce a new benchmark for evaluation but do not offer sufficient details regarding it. A detailed overview of the queries and an analysis of why the existing SOTA techniques do not perform well on the same could be provided which could greatly inform future work.
3. The practical utility of the proposed technique seems to be limited as each user query requires multiple calls to be made to LLMs thereby entailing both increased latency and cost.
4. The error analysis is not very comprehensive and could be improved. For instance how does this technique fare when the names of entities in database schemas are not semantically meaningful or if there are conflicts in descriptions etc (as is often the case in real-world industrial databases).

Limitations:
1. As the technique leverages LLMs it seems to be heavily reliant on having semantically meaningful entity names /descriptions 
2. The proposed technique seems sensitive to hallucinations as it involves processing a query through multiple LLM phases. The errors in any of the earlier phases would result in it propagating to the next stage. For instance as the authors pointed out if the MatchTables phase produces an extra table this could in turn effect the end output.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of developing effective LLM-based assistants for querying SQL databases. In this context, users pose questions to a relational database in natural language, and the goal is to generate a SQL query that correctly answers the user's question when executed. The authors focus on overcoming a limitation of current text-to-SQL approaches: the difficulty LLMs face in handling databases with numerous tables and complex relationships, making it hard to determine the necessary table joins for the query.

To tackle this issue, the authors propose a workflow that begins with using the LLM to identify relevant tables and their attributes. In the second step, a constraint satisfaction solver (CSP) is employed to determine the necessary joins while adhering to database constraints. In the third step, a materialized view is created by joining the relevant tables. Finally, this view, combined with the user's question, is used to prompt the LLM to generate the final SQL query.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper tackles a very relevant practical problem, which is attracting significant attention in both academia and industry. The proposed workflow may add practical value.

Weaknesses:
The paper lacks depth, and the writing does not, in my opinion, meet the quality standards required for a venue like NeurIPS. Additionally, several potential limitations of the proposed workflow are not discussed.
	•	Accuracy of the answers is not the only important requirement in generating SQL from text. Database users also expect query generation to be time-efficient. The proposed workflow includes several computationally intensive steps: first, solving an NP-complete problem (CSP), and second, creating a potentially enormous materialized view by joining many tables. I would have expected a discussion on the computational limitations of this approach.
	•	The workflow lacks sufficient precision and clarity. For example, it is unclear whether the final query is expressed with respect to the materialized view as the only table or with respect to the original schema. Additionally, how lookup tables and various schema design patterns are identified in the input database is not well-explained. The authors claim that their approach guarantees the generated query respects database constraints, but this guarantee is not clearly defined. Algorithm 1 is underspecified; at this high level of detail, the algorithm seems redundant and could be subsumed by the text description. The exact SQL fragment covered by this approach is also unclear. While the limitations section mentions that queries requiring the union operator are not supported, it is unclear if other standard SQL constructs are also unsupported.
	•	While relevant related work is cited, the main body of the paper lacks a detailed discussion on the contribution in relation to recent approaches.
	•	The evaluation section is somewhat lacking. The tables are confusing, and it is unclear what each of the rows actually represents.

Limitations:
The limitations section mentions some but not all of the relevant limitations of this approach.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The author proposes a new method, Lucy, designed to handle large databases with complex relationships between objects. Lucy operates through three steps: MatchTables, GenerateView, and QueryView. It first identifies relevant tables and attributes using LLMs, constructs a combined view with an automated reasoner, and generates the final SQL query. Lucy shifts complex reasoning from LLMs to a CSP solver, supporting various database design patterns. Experiments on ACME insurance, Cloud Resources, and the two BIRD databases show that Lucy outperforms other zero-shot text-to-SQL models.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The proposed method offers a fresh perspective on tackling text-to-SQL research with a logical workflow.
- The paper is well-written and easy to follow.

Weaknesses:
- I am not convinced by the motivation of zero-shot text-to-SQL with the example of industrial databases having complex relationships. Text-to-SQL systems deployment in real-industry requires high performance. I doubt that people won't be using zero-shot models for real use applications. In KaggleDBQA, it also states ""we believe the zero-shot setting is overly-restrictive compared to how text-to-SQL systems are likely to be actually used in practice."" I would like to hear the authors' thoughts on this.
- The paper does not appear to be well-grounded in text-to-SQL research. For example, one way to handle complex relationships in text-to-SQL using LLMs is through schema linking. However, the paper does not mention this area of research and instead proposes MatchTables, seemingly ignoring the rich literature of text-to-SQL works. Other approaches include least-to-most prompting attempts in text-to-SQL for task decomposition and Natural SQL for intermediate representation (although it does not handle query nesting). Properly discussing these relevant methods of the proposed method will better situate the work.

Limitations:
The limitations of the work are well-stated.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces Lucy, a framework for solving Text2SQL by LLMs, particularly for complex enterprise databases. Lucy leverages LLMs' understanding and reasoning capabilities to handle intricate database relationships and constraints. The framework operates in three phases: identifying relevant tables and attributes (MatchTables), constructing a view through constraint reasoning (GenerateView), and generating the final SQL query (QueryView). The empirical studies show Lucy achieves performance improvements on several zero-shot Text2SQL tasks.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Text2SQL is an essential problem in commercial scenarios.

Weaknesses:
The draft seems far from complete, so leave some high-level suggestions.
1. Make the title, abstract, and introduction more concrete. It is hard to tell the contribution or uniqueness of this work among other papers about Text2SQL by LLMs.
2. Survey related works and clearly state the contribution/novelty of the proposed method against others.
3. Define the terminologies or abbreviations before their first appearance.
4. Make the draft concise by removing unnecessary content. For example, the first challenge introduced in Motivation section is not relevant to this work.
5. The empirical studies could be more convincing by following others' evaluation protocols, such as BIRD.
6. Lack of comparison to other competitors.
7. The figures, tables, and their captions should be self-explanatory.

Limitations:
There is a discussion about the limitation, though the first limitation seems too broad and unnecessary.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
SqW42eR2wC;"REVIEW 
Summary:
This paper studies inverse RL with learnable constraints in the offline setting, focusing on practical applications in healthcare tasks. The main approach appears to be combining the decision transformer architecture in the offline RL literature with inverse constrained RL with max entropy framework. Experiments were conducted on two healthcare tasks: sepsis and mechanical ventilation.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Studies an interesting and important problem.

Weaknesses:
There is significant issues with the writing: the text is incoherent throughout and really hindered my understanding of the paper.

Limitations:
Limitations are discussed.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper uses the Inverse Constrained Reinforcement Learning (ICRL) framework to infer constraints in healthcare problems from expert demonstrations. It proposes the Constraint Transformer (CT) to address the dependence of decisions on historical data, which is generally ignored in ICRL methods with Markovian assumptions. It borrows the causal transformer from the previous decision transformer to incorporate history into constraint modeling. Additionally, a model-based offline RL model is trained to generate violating data. The CT demonstrates improved safety by effectively modeling constraints based on both violating and expert data.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
-	The paper addresses a gap in existing ICRL applications by integrating historical data into the decision-making process.
-	The paper augments the violating data in the offline training dataset with a generative world model.
-	The proposed method has been thoroughly evaluated in three aspects: effective constraints, improved sepsis strategies, and safe policies.

Weaknesses:
-	The proposed method depends heavily on the generated violating data, which defines the objective function in the constraint transformer. How sensitive is the estimated policy to the generative world model? Figure 12 shows that the action distributions in the expert dataset and the violating dataset are different. The VASO action seldom takes a large value in the violating dataset. Will this distribution difference cause any trouble in the learning of the constraint?
-	Could the authors provide some details on how the DIFF between the estimated policy and the physicians' policy is calculated through graphical analysis? It would also be helpful if the authors could explain how Figure 7 is plotted. Is it calculated based on the dosage differences at each timestamp? In addition, what are the implications of the three DIFF evaluation metrics in Table 2? Since both IV and VASO are part of the action space, is the ACTION DIFF alone sufficient to evaluate the estimated policy?

Limitations:
The authors have addressed the limitations of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the Constraint Transformer (CT) framework to enhance safe decision-making in healthcare. The proposed CT model uses transformers to incorporate historical patient data into constraint modelling and employs a generative world model to create exploratory data for offline RL training. The authors supported their points by presenting experimental results in scenarios like sepsis treatment, showing that CT effectively reduces unsafe behaviours and approximates lower mortality rates, outperforming existing methods in both safety and interoperability.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper shows its strengths in the following aspects:
- The paper addresses the novel angle of ensuring safety in offline reinforcement learning (RL) for healthcare, a critical and previously underexplored issue.
- It incorporates Inverse Constrained Reinforcement Learning (ICRL) into offline reinforcement learning (RL) for healthcare, introducing a novel approach to inferring constraints from expert demonstrations in a non-interactive environment.
- The implementation of a causal transformer to learn the constraint function is interesting, allowing the integration of historical patient data and capturing critical states more effectively.
- Extensive results on 2 datasets are presented. The proposed Constraint Transformer (CT) framework is shown to reduce unsafe behaviours and approximates lower mortality rates.

Weaknesses:
Despite its strengths and novelty, this paper suffers from several critical technical flaws, primarily concerning the soundness of evaluation rather than the method itself:

1. **Definition of Metric**: The metric $\omega$ is defined by comparing drug dosages related to **mortality rate**, which I believe is a flawed definition, even though it has been used in previous papers. Mortality rate can be influenced by numerous factors, making it unsuitable as a reward for RL, which considers a limited number of drugs. It is challenging to convince clinicians that mortality can indicate the 'treatment quality' of vasopressor or mechanical ventilation. This suggests that the reward is not solely a function of the previous action and state but also many unconsidered features (hidden variables) in the datasets, such as adrenaline, dopamine, historical medical conditions, phenotypes, etc. [1] pointed out that doctors usually set a MAP target (e.g., 65) and administer vasopressors until the patient reaches this safe pressure; [2] suggests using the NEWS2 score as the reward supported by clinical evidence. None of these directly use mortality. While it is understandable that this paper is not a clinical study, and hence, it is not the authors' responsibility to identify clinically appropriate reward designs, I recommend referring to [1] and [2] for a reward design that makes more clinical sense.

2. **Definition of Optimal Policy**: This paper follows [3]'s definition of optimal policy. From my understanding, the clinician's policy $\hat{\pi}$ is approximated by a neural network. [2] pointed out that in the sepsis dataset, the behaviour policy can result in critical flaws in a very small number of states. Although the number of incorrect predictions is limited, they can still bias the off-policy evaluation results severely. I suspect this paper may encounter a similar issue. The authors should provide experiments and visualizations on the learning quality of the behaviour policy to justify their approach.

3. **Model-Based Off-Policy Evaluation**: The data imbalance in both the sepsis and ventilation datasets is significant. It is questionable whether the learned model can generalize well. The most acceptable way to validate the method remains using simulated data where all policies can be tested online. One possible testbed is the DTR-Bench[4] medical simulated environment.

The paper has a few other minor technical flaws compared to the above three.

[1] Jeter, Russell, et al. ""Does the"" Artificial Intelligence Clinician"" learn optimal treatment strategies for sepsis in intensive care?."" arXiv preprint arXiv:1902.03271 (2019).

[2] Luo, Zhiyao, et al. ""Position: Reinforcement Learning in Dynamic Treatment Regimes Needs Critical Reexamination."" Forty-first International Conference on Machine Learning.

[3] Aniruddh Raghu, Matthieu Komorowski, Leo Anthony Celi, Peter Szolovits, and Marzyeh Ghassemi. Continuous state-space models for optimal sepsis treatment: a deep reinforcement learning approach. In Machine Learning for Healthcare Conference, pages 147–163. PMLR, 2017

[4] Luo, Zhiyao, et al. ""DTR-Bench: An in silico Environment and Benchmark Platform for Reinforcement Learning Based Dynamic Treatment Regime."" arXiv preprint arXiv:2405.18610 (2024).

Limitations:
There is no negative societal impact or limitation that needs clarification. 
In addition to the weakness I mentioned, this paper:
1. lacks off-policy evaluation results.
2. may summarise more related work in this 'dynamic treatment regime' field. A few examples are listed below:

[1] Kondrup, F., Jiralerspong, T., Lau, E., de Lara, N., Shkrob, J., Tran, M. D., Precup, D., and Basu, S. Towards safe mechanical ventilation treatment using deep offline reinforcement learning. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 37, pp. 15696–
15702, 2023.

[2] Liu, Y., Logan, B., Liu, N., Xu, Z., Tang, J., and Wang, Y. Deep reinforcement learning for dynamic treatment regimes on medical registry data. In 2017 IEEE international conference on healthcare informatics (ICHI), pp. 380–385. IEEE, 2017.

[3] Nambiar, M., Ghosh, S., Ong, P., Chan, Y. E., Bee, Y. M., and Krishnaswamy, P. Deep offline reinforcement learning for real-world treatment optimization applications. In Proceedings of the 29th ACM SIGKDD Conference on Knowledge Discovery and Data Mining, pp. 4673–4684, 2023.

[4] Peng, X., Ding, Y., Wihl, D., Gottesman, O., Komorowski,M., Li-wei, H. L., Ross, A., Faisal, A., and Doshi-Velez,F. Improving sepsis treatment strategies by combining deep and kernel-based reinforcement learning. In AMIA Annual Symposium Proceedings, volume 2018, pp. 887. American Medical Informatics Association, 2018.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors consider healthcare applications of RL algorithms in which implicit constraint modeling is critical for safe recommendations. 
This is modeled as an RL policy optimization with constraints. However, the constraints are often unknown and need to be inferred from expert data trajectories in the healthcare applications. The authors propose a neural network estimator to combine with the constrained MDP formulation. They also identify that the naive way to represent states leads to non-Markov structure. To address these issues, the paper proposes a simple modification (based on prior work on the ""preference attention layer"") to a causal transformer based policy which attempts to model a parametric constraint function. Evaluations are based on healthcare benchmarks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper considers a practically motivated approach grounded in real data for a healthcare application where data could be challenging to obtain.
- Proposes simple addition to the final layer of a causal transformer to parametrize constraints on trajectories learned from expert data.
- Evaluations are conducted on real healthcare domain benchmarks and extensive ablations are included to ensure that the architecture change is meaningful in obtaining the improvements.

Weaknesses:
- It seems like the system (which is the patient) is not feasible to model as evolving according to an Markov process on the observed state at each time, but instead a POMDP with a high dimensional latent state. 

- Clarity and presentation can be improved. In Equation (4), what is $\hat{\tau}$, this was never defined for being such a key component of the procedure. Similar issues in Eq (8) related to clarity.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
WxW4nZMD3D;"REVIEW 
Summary:
The authors propose to use network Lasso to learn a multi-task bandit problem with given network structure. More specifically, the network structure has pre-defined unknown clustering structure, where within each cluster all the bandit tasks share the same model. The authors propose a bandit algorithm that can learn and provide a sublinear guarantee. The key difference of this paper with GOBLin in Cesa-Bianchi et al., 2013 is that this paper uses a network Lasso (or something like a group Lasso) penalty while GOBLin use a ridge penalty.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Even though I think the authors overclaimed their contributions, which I will state below, I still feel it's meaningful to discuss network Lasso and design a bandit algorithm based on certain network structure, given the limited literature on multitask bandit. Compared to previous network bandit literature such as Cesa-Bianchi et al., 2013, this paper characterizes the network structure in more detail.

Weaknesses:
1. I think the authors need to provide more real-world examples to show why their network structure (and the correpondingly induced network Lasso bandit algorithm) is pratical, instead of stating their algorithm is good because it provides piecewise constant property in constrast to smoothness of GOBLin in Cesa-Bianchi et al., 2013. The key assumption in this paper is that the network structure is given, and the network can be splitted into connected clusters, within which the task parameters are the same. Can authors find or describe a couple of pratical examples/datasets where such a network exists, given that bandit is a very pratical problem?

2. The literature review that compare with the previous literature are not very accurate and sufficient IMO. For example, the authors mention Gentile et al., 2014 and Li et al., 2019 can cause overconfidence in constructing clusters. However, these algorithms do not have prior information about clusters such as a given network and thus they have to learn the clusters conditioned on the task similarities. In that sense, these algorithms are more pratical because oftentimes in practice network information is lacking. Here one should also add a related reference Context-Based Dynamic Pricing with Online Clustering by Miao et al., 2022. There are also robust multitask bandit algorithms (e.g., Multitask Learning and Bandits via Robust Statistics by Xu and Bastani, 2024) that can also solve the network bandit problem if the network structure follows certain assumptions; Multi-Task Learning for Contextual Bandits by Deshmukh et al., 2017 discuss a multitask bandit problem but use kernal based method. I suggest the authors add a more detailed literature review to discuss their paper's connection with these current multitask bandit algorithms. 

3. Typically, greedy algorithm (the method proposed in this paper is greedy too due to Assumption 2) has better performance in bandit simulations compared to UCB-based algorithm (see Bastani and Bayati, 2020). Therefore, it's not a fair comparison in Figure 1, where all benchmarks are UCB algorithms. I think it's necessary to add the following benchmarks: OLS-Bandit or Lasso-Bandit (Bastani and Bayati, 2020) without task sharing, and Cella et al., 2023 which use low-rank structure to do multitask bandit, and also a few others mentioned above as a benchmark to show that the network structure indeed helps, fixing the difference due to UCB and greedy algorithms.

4. I feel it's a false claim that the regret bound in Theorem 3 ""doesn't depend on the dimension"" and it's due to the concentration inequality from Hsu et al., 2012. Intuitively, the regret bound should depend on the dimension unless one assume that the number of tasks in a cluster is d-dependent so the regret bound is smaller compared to a typical single bandit regret bound. I think the reason why here the bound seems to be d-independent is because the dimension $d$ is hiden in the problem-dependent parameter $\phi$. Since the authors assume the context $x$ has norm 1, the minimum eigenvalue of $E[xx^\top]$ should scale as $1/d$, and hence $\phi^2$. I don't think a typical tail inequality in Hsu et al., 2012 can improve the bound regarding the context dimension. 

5. I think the asymptotic assumptions in Theorem 3 is incompatible with the finite-sample analysis in a typical bandit analysis and looks weird. I suggest the authors keep the isoperimetric ratio and centrality index as part of the regret bound (instead of forcing them out using the asymptotic assumptions), even though it might add an additional T-linear term due to the misspecification error caused by the inter-cluster edge connections. I feel that's the case because in the extreme case when each cluster has size 1, there will be misspecification error penalizing tasks connected the inter-cluster edges towards each other. I think it's totally fine to have such non-sublinear terms to provide a more comprehensive understanding of the limit of such network structures. 

I am willing to raise my rating if the authors can solve my questions and concerns.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, authors work under the multi-task contextual bandit settings by representing the task correlations through the graph structure. To solve this problem, authors propose an algorithm that utilizes a linear regression formulation with Lasso constraint in terms of the node connectivity. Theoretical analysis as well as experiments against several baselines are presented to demonstrate the effectiveness of proposed method.

-	The paper is generally well-written, with crisply clear descriptions of required assumptions, and the proposed solution is intuitive and well-motivated.

-	Good empirical performances. Authors compare proposed algorithm with several clustering of bandits baselines, showing the effectiveness of the proposed method. The performance gain over existing methods is impressive. 

-	Novel theoretical analysis roadmap. Overall, the theoretical analysis pipeline is novel and the looks promising to me. With the additional introduced RE assumption, authors are able to improve the regret bound to $\tilde{O}(\sqrt{\bar{T}})$ instead of the vanilla time horizon.

-	My major question is regarding the numerous assumptions required for the theoretical analysis. For instance, in Assumption 1, authors assume the candidate arms across different arounds are generated i.i.d. from a fixed distribution. This is different from existing clustering of bandits works, where the candidate arm contexts in each round is conditioned on previous observed arms. In this case, assumption 1 is somehow deviates from the actual applications of recommender systems where the candidate arms of each round are refined along with more information collected from the environment.

-	For the experiments, authors have compared against multiple clustering of bandit works. In this case, it would be good if authors can include additional discussions comparing your theoretical outcomes with those of existing clustering of bandits works, which can offer more intuitive comparison with existing approaches.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Please see my comments above.

Weaknesses:
Please see my comments above.

Limitations:
Please see my comments above.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the multi-task bandit problem using graph information. The given graph represents the relationships between tasks. Assuming that the preference vector of clustered tasks is constant, the problem is formulated as a network lasso problem to estimate the lasso estimator. 
A modified restricted eigenvalue condition, commonly used in high-dimensional statistics, is defined to derive the oracle inequality for the network lasso estimator on non-i.i.d. data. The oracle inequality of the proposed network lasso estimator is derived under the assumption that the true multi-task Gram matrix satisfies the adapted RE condition.
Based on the derived oracle inequality, a greedy-type algorithm is presented, achieving $\sqrt{T}$ regret. Numerical experiments support the theoretical performance of the proposed algorithm.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The proposed algorithm efficiently learns task preference vectors by using graph information that encodes relationships between tasks. Specifically, it employs a network lasso estimator under the assumption that preferences within clustered tasks are constant, demonstrating its effectiveness in high-dimensional contexts.
- The paper adapts the restricted eigenvalue condition from high-dimensional statistics to the graph-based multi-task bandit setting. Based on the adapted RE condition, they established oracle inequality for network lasso estimator and showed that the proposed algorithm achieved $\sqrt{T}$ regert even though I haven't verified every proof in detail.
- The algorithm's performance seems robust even as the number of tasks and dimensions increase.

Weaknesses:
- Since I'm not very familiar with the graph-based multi-task bandit setting, it may be that the concepts explaining the restricted eigenvalue condition (Def 2) are too heavy. It would be helpful to include comparisons or examples from existing RE conditions in high-dimensional statistics or high-dimensional contextual bandits to improve understanding.

Limitations:
The authors have well-addressed the limitations in Appendix D.4 and further research directions in Section 7.

The content discussed in this paper appears to have little to no negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a multi-task contextual bandit algorithm that leverages a graph structure to model relationships between tasks. The algorithm assumes that the preference vectors of the tasks are piecewise constant over the graph, forming clusters. By solving an online network lasso problem with a time-dependent regularization parameter, the algorithm estimates the preference vectors, achieving a sublinear regret bound lower than independent task learning. Theoretical findings are supported by experimental evaluations against other graph bandit and online clustering algorithms.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1) The paper introduces a approach by incorporating graph structures to model relationships between tasks.

(2) The algorithm is supported by comprehensive theoretical analysis, including a oracle inequality and a regret bound.

(3) Extensive experiments validate the proposed method, showing that it outperforms existing baselines in terms of cumulative regret, highlighting its practical applicability and effectiveness.

Weaknesses:
(1) The problem setting and algorithm presented are primarily adaptations of existing works, such as Oh et al. [2021]. The main difference is the inclusion of a graph matrix in the user preference vector, but this is not the first algorithm to incorporate a graph in contextual bandits, limiting the overall novelty.

(2)  The i.i.d. assumption in contextual bandits is quite strong. Even in clustering approaches like CLUB, a conditional i.i.d. assumption is used. The current regret upper bound complexity is \(\sqrt{VT}\). There should be special cases where the algorithm can improve over \(V\) to demonstrate a more significant advantage.

(3)  Since 2019, there have been many more works on clustering in bandits. The authors should conduct a broader survey to include these more recent works and relevant baselines. Using SCLUB, which is considered outdated, as a baseline, limits the comprehensiveness and relevance of the comparative analysis.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
jwE1dgOox1;"REVIEW 
Summary:
Large scale topological descriptors of data are leveraged to compute point/node-level descriptors, which encode to which large scale topological feature each point belongs to. For this, a combination of applied algebraic topology and applied harmonic analysis is used. More specifically, large scale homological features are computed using persistent homology, then represented with harmonic cocyles, and then averaged locally to obtain a point-level descriptors. The problem of topological clustering (already introduced in the literature) is addressed, whose objective is to determine to which large scale topological feature a certain data point belongs to. A set of benchmark datasets are introduced for topological clustering. The pipeline is applied to these datasets as well as real world datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Large scale topology of data is leveraged to assign point/node-level features to data. This gives concrete meaning to what it means for a data point to belong to a certain large scale topological feature.
- The method is based on well-established mathematical concepts.
- The concept of topological clustering is interesting and has potential.
- A suite of synthetic datasets is introduced.

Weaknesses:
Regarding unjustified claims:

- Existing approaches are undersold. Specifically, in the introduction it is said that ""none of these approaches is able to represent higher-order topological information"" and that ""such higher-order topological information is however invisible to standard tools of data analysis like PCA of k-means clustering"". However, cluster structure is topological structure. Does ""higher order"" mean homology in dimensions 1 and above?
- Remark 4.2 says that ""datasets with topological structure consist in a majority of cases of points sampled with noise from deformed n-spheres"". This seems like a really strong claim. Is there any evidence of this?

Regarding theory:

- Theorem 4.1 applies in a very restricted scenario. Moreover, I do not understand why the harmonic representative takes values in {0, -1, 1}. This seems very surprising since harmonic cycles/cocycles almost always take fractional values (in order to minimize energy). I did not understand the proof of this fact; specifically, why $g$ being a harmonic generator for the entire filtration range of $(b,1)$ implies this claim.

Regarding the methodology:

- The method, specifically line 225, seems to assume that a cycle with coefficients in $Z/3Z$ will also be a cycle when interpreting those coefficients (0,-1, or 1) as real numbers. However, this need not be the case. To see this it suffices to consider a simplicial complex given by a triangle with no interior. Thus, step 3 of Algorithm 1 (and the method more generally) seems to be heuristic.
- The setup up Table 1 is unclear to me. How can one compare TOPF, which produces feature vectors, with, say, DBSCAN, which produces a clustering?
- Figure 4 is hard to interpret. For example, how should one assess the effectiveness of the algorithm in Fig 4(a)?
- The methodology has many hyperparameters. Some choices, like delta=0.07 in line 241, seem arbitrary.

Limitations:
- The experimental evaluation is limited.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces TOPF, a topological feature extraction mechanism on point cloud data. The authors consider Vietoris-Rips/$\alpha$ filtrations over point clouds and compute the persistent homology. They propose a heuristic to select the “top” features from the barcodes. They consider the corresponding representatives for these features and project them onto the harmonic space of the simplices. These projected vectors are then normalized and used to construct a point-level feature vector. The authors use this framework for clustering. Towards this end, the authors introduce a topological point cloud clustering benchmark and report the experimental results on this benchmark.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The authors propose to use Hodge Laplacian and Hodge decomposition to compute feature vectors over points in point-cloud data, which is a novel idea.

Weaknesses:
1. I do not fully understand the “learning” the representation here, because the representation is not particularly being learnt. It is being computed by using the persistent homology of the point cloud. 

2. Experimental evaluation is limited to clustering. And even in clustering, it is primarily limited to shapes which are partially/fully topologically spherical.  

3. The robustness of the approach is due to the robustness of harmonic persistent homology known in the literature. 

4. The paper uses well-known notions in the TDA literature in the context of point-clouds, which amounts to an incremental progress in this direction. 

5. It would strengthen the paper if the authors include a small paragraph explaining why projecting onto the harmonic subspace solves the problems that exist in using the homology representatives directly. 

Minor: 

Page 2, Line 81: ‘Spaces in topology are “continuous”’. Continuity is a notion defined for functions on topological spaces and not for topological spaces themselves. Spaces are connected.

Limitations:
Yes, the authors have discussed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces an approach to select and compute some point-level topological features for point cloud or general data set analysis.
The main ideas is to define a multi-scale simplicial complex representation, thus we can track how the homology modules change along the filtration and then select the homologies that persist for a long range of scales.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Topological features are usually not localized, the idea of being able to bring back the topological descriptor to the relevant points is quite novel and impactful.
- The approach is theoretically sound and well analyzed.
-The experimental evaluation is limited but convincing.

Weaknesses:
- the feature selection is very heuristic.
- The evaluation is only on point cloud clustering. Since we are evaluating effectiveness and robustness of localized features, feature/point correspondence problems would have been interesting.

Limitations:
The main limitation, i.e. the selection of the features, has been briefly addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel method for extracting per point topological features - TOPF. The method builds on previous results in topological data analysis which described a shape or a point cloud with a single global feature, by generating per-point topologically-aware features. The paper presents a quantitative evaluation and comparison of the proposed method with prior art on a new benchmark consisting of several synthetic examples, evaluates the robustness of the proposed method under noise, as well as presents qualitative examples of its performance on synthetic and real work data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
* The paper is well written and easy to follow. Prior art and the proposed algorithm description is detailed and comprehensive.
* To my understanding, the paper describes a novel method for per-point feature extraction based on topological information contained in a point cloud, and describes theoretical guarantees for its correctness on point clouds sampled from multiple n-spheres.
* The paper describes a new topological point clustering benchmark dataset consisting of seven synthetic point clouds with up to 5 labels, and evaluate the proposed and existing methods on this dataset showing that the proposed method outperforms existing methods in most cases.

Weaknesses:
* The paper lists common machine learning applications requiring point level features as a motivation for the proposed method. However, only quantitative experiments for point cloud clustering on a set of synthetic examples, and anecdotal evidence of performance on real world data, were presented. In order to fully understand the potential of the proposed approach to be applied beyond synthetic data, it would be beneficial to include additional evaluation, qualitative and quantitative, on real-world data and additional applications, e.g. as described in lines 304-307.
* Specifically, it would be interesting to see experiments on non-synthetic datasets with topological structure mentioned in line 266.
* Additionally, comparison with other well performing modern machine learning methods, such as graph neural networks for point cloud clustering, needs to be discussed, for completeness.

Limitations:
The authors adequately addressed the limitations and impact of the proposed approach.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
yAKuSbIwR7;"REVIEW 
Summary:
This paper provides a thorough characterization of regularizers which lead to synaptic balance (when the ""cost"" of input weights to a neuron or pool of neurons is tied to the cost of output weights) in trained neural networks. Their results apply to many different activation functions and architectures.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is very well-written and easy to follow. I was able to read everything, including the math, smoothly. The mathematical arguments themselves are crisp and correct, which I really appreciated.

Weaknesses:
The paper is strongly lacking in motivation. I never really understood *why* I should care about synaptic balance. Also, it is clear from the numerical experiments that synaptic balance only emerges in networks when it is enforced via a regularizer (expect in the case of infinitely small learning rate), but why is this surprising? It seems obvious that adding a regularizer for some property tends to result in that property. It would be shocking if synaptic balance occurred without some regularization towards the property. Thus, while the ""what"" and ""how"" of the paper are nicely addressed, I feel the paper is missing the ""why"". I believe if the authors could address this from the outset, it would make the paper much stronger, and I would of course be willing to increase my score.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors provide a theoretical approach to the analysis of balanced neurons and networks. Their theoretical work includes proof of the convergence of stochastic balancing. In addition, they investigate the effect of different regularizers and learning rates on balance, training loss, and network weights, including practical simulations for two classification problems.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper tries to reveal the inner structure of neural networks during the training phase. This is a very important but difficult problem; its solution could provide new insights for developing better training algorithms. The work proposed can ultimately be an important step toward more transparent networks as opposed to their current black box character.

Weaknesses:
The paper has some weaknesses, most notably how the material is presented and part of the evaluation.

Theorem 5.1, dealing with the convergence of stochastic balancing, is arguably the central piece of the paper. However, its formulation is bulky and should be reduced to a shorter, more manageable size, potentially with the help of lemmata. This becomes apparent when seeing that its proof contains the proof of another proposition.

In Figure 4, the authors say that these panels are not meant for assessing the quality of learning. However, measuring not only the training loss but also the accuracy on a test set will give important insights. How does the classification performance relate to the degree of balancing? Why did the authors not include this analysis? It could give important insights into the relationships between overtraining, generalization capability, balance, and accuracy.

The author should discuss the consequences of their work on network training. They do not discuss the immediate practical consequences or any recommendations they can make based on their results.

Limitations:
The authors could be more specific about the consequences of their work, including limitations. For example, can they recommend any specific learning rate, network structure, or other features for optimal training?

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to study and explain the phenomenon of neural synaptic balance, where a balanced neuron means that the total norm of its input weights is equal to the total norm of its output weights. Particularly, the authors study the reasons why and when randomly initialized balanced models (so, models whose neurons are balanced) tend to be balanced at the end of training as well. The study takes into account many different components of neural networks (activations, layer kinds, regularisers).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The study is very comprehensive, and sheds light on some interesting properties of deep neural networks.

Weaknesses:
While it is true that, as the authors state in the conclusion, neural synaptic balance is a theory that is interesting on its own, I would encourage the authors to expand the discussion on possible application domains of this theory. Why is it interesting? What are the advantages that a complete understanding of such phenomenons could bring to the table?

Limitations:
No concerns here

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present a theory of neural synaptic balance, defined as the condition in which a total loss achieves the same value for the input weights to a neuron and its output weights. This is different from the well studied  E/I balance in neuroscience and machine learning literature. The authors show mathematical derivations of how to balance a neuron without affecting the outcome of the network and show that balancing a network is a convex optimization process.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The paper is overall clear and detailed, the mathematical proofs are sound and the paper structured well moving from straightforward claims to less trivial points.

Weaknesses:
The paper is about neural synaptic balance, but the authors do not provide convincing motivation why we should care about such balancing.  As they mentioned, adding a simple L2 regularizer will balance the network naturally (in a distribution sense, not necessarily each neuron individually) during training and have other well-known  benefits, so the elaborate mathematical derivations on the general balancing process seem redundant. In addition, in the authors' own plots, unbalanced networks sometimes outperform the balanced networks (e.g., fig 3E), which just emphasizes the point. One of the mentioned motivations  is biological neurons, but they claim that biological neural data about synapses do not exist. However, they could test their hypothesis against the currently available connectomes e.g., from or the Drosophila fly brain. They mention spiking networks, but the notion of input-output homogeneity is unclear in spiking networks. Finally, physical neurons' energy consumption is mentioned without details.

Limitations:
The whole framework is specific to BiLU neurons or perhaps to other power-law functions. The relevance to spiking neurons is therefore questionable. It is also questionable as a general principle for machine learning.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
uvvVjWP1aj;"REVIEW 
Summary:
The proposed work reveals that large language models may be susceptible to strategic underperformance on an evaluation benchmark. In particular, they investigate this claim on a variety of frontier models, through a myriad of tactics from simple prompting to more involved fine-tuning strategies. Their results shed light on the brittleness of model evaluations and urge one to exercise caution before judging a model's capabilities based on its performance on so-called evaluation benchmark datasets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. the authors do a good job of motivating why the issue of sandbagging is particularly dangerous, since it is conceivably plausible for model developers to encourage that their models ""selectively underperform on dangerous capability evaluations, while maintaining overall performance.""
2. the definitions of strategy, underperformance, and evaluation are well fleshed out. In particular, the connection of ""strategic"" to the implications for legal responsibility as well as Figure 2 are excellent additions to the paper.
3. good breadth of open and closed source models were used in the experiments
4. it is very refreshing to see an honest declaration of the number of hours spent during prompt engineering (Section 3, paragraph on ""Prompting"")
5. utilizing both direct prompting as well as password locking is quite clever, since access to a model may be variable. Some users may be restricted to query-only access to a model, which is why the results that the authors were able to obtain through straightforward prompting are encouraging.

Weaknesses:
1. consideration of only MCQA benchmarks is limiting, but can be addressed in future work

Limitations:
The authors provide good examples of limitations in Section 7.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper discusses an interesting concept of sandbagging in LLM, with practical results using available models and existing datasets. Wide range of models are used in this work, and sub topics primarily include password-locking LLMs. The transition and explanations need improvement, some of the things in this paper are not very clear, also some of the concepts are assumed and no concrete proofs or references are mentioned. I don’t think that this paper can go through for neurips, there are a lot of improvements to be made and more concrete experiments to be conducted.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The introduction of sandbagging in LLMs is something that is novel and can be explored further.

Explored password-locking as one of the method to test the sandbagging capabilities of LLMs

Used frontier models for the experimental purpose

Weaknesses:
The paper is difficult to understand, the delivery and presentation needs improvement, the main motive of the paper is not so clear to understand.

There are multiple models used in this paper, a bit of consistency would be better.

Line 36: It is mentioned that “AI systems might have goals which it believes” . There is no concrete paper that backs the argument of AI capability of thinking, it is a might.

Line 78: There should be some statistical significance by which the models perform poorly and then you could consider it underperformance and not just lower than actual. Models being non-deterministic, there is always a chance of slight changes in scores. 


Line 70: I feel Developer sandbagging is a topic which is not of much importance, it is not a big problem, given that developers are there to build better models and if someone wants to deliberately build some models that can underperform, they can still do it and publish as open-source models. 

Line 170: The same goes with the password-locking as well, and also I felt there should been mentioning of password-unlocking as well, if an LLM can be fine-tuned to achieve password-locking, it can also be fine-tuned again to unlock it with a different dataset, would be great if that part of model is explored as well. 

Line 117: Example 1: The examples mentioned in the paper are very specific and not generalized. The cases of it happening are rare, I don’t see it being a big problem, also some of the things are minor problems considering the EU’s persuasion evaluation, given there is improvement in AI safety and rules. Also in this work the models chosen are frontier models.

Line 131: Example 2: There is no concrete work which supports the statement of LLM capable of planning such features, there is one positional paper in reference.

Section 4.1: Some of the points are unclear in this section. A better explanation is appreciated

Line 305: The baselines would be appreciated comparing LLama 2 7b with LLama 3 70b.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper explores the concept of strategic underperformance, termed ""sandbagging,"" in AI models, focusing on two scenarios: selective underperformance and emulating specific capability levels. It investigates the implications of these strategies on both general capabilities and safety.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- the concept of sandbagging of LLMs is interesting, shedding light on strategically manipulating the model behavior
- the authors explore two distinct scenarios: selective underperformance and capability emulation
- provides insights into potential implications for AI model design and deployment strategies

Weaknesses:
- Limited datasets used - only three datasets, two for measuring general capability, and one for measuring safety. 
- For measuring model safety, most existing datasets test how ""safe"" the model is, instead of whether the model has certain knowledge of potentially dangerous domains - the authors should also conduct experiments on those datasets
- All datasets are MCQ formats, which may not fully capture the diversity of real-world applications. The resulting conclusions are thus less convincing

Limitations:
1. This paper could benefit from expanding the experiments, mainly the datasets they used. For example, considering more common safety-related datasets
2. Sandbagging is already a widely-used term in the field of AI. I suggest refraining from overusing this term, or alternatively, adding qualifiers such as ""safety sandbagging"" to specify its context.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors assess the chance of AI systems strategically underperform on ""dangerous capability evaluations"" in order to hide their risks, a la Volkswagen emissions testing cheating, by having awareness of the fact that they're being evaluated and that the evaluation is intended to test safety.

They use MMLU and CSQA as the general capability assessment and a ""weapons of mass destruction proxy"" benchmark to measure hazardous capabilities. From 100 validation phrasings of the question prompts they find optimal ones to run on the whole dataset.
They fine-tune ""password-locked"" models as the method to attempt to elicit sandbagging.

*They then prompt the model to lie*, or *password-lock it to fail* (in the case of Mistral) on the hazardous capability evaluations. They demonstrate that sandbagging prompts and password locking can get a model to differentially perform worse on the dangerous datasets vs the innocuous ones, by effectively treating the .

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
Interesting work on calibration for MMLU and general ""capability profile"" lowering. It's interesting to see that the password-locking method does work in general on lowering Llama performance over MMLU questions.

Though I am not super familiar with the literature on password locking, the demonstrations here appear to be novel and useful contributions.

Weaknesses:
**bold: new comments after rebuttal** ~strikethrough: removed after rebuttal~

### ~Presumption of ""goals"" ""beliefs"" etc in LMs without sufficient support~

~They presume AI system goals (in general not necessarily objectionable, but in the case of LMs doubtful) for which a model may be incentivized to deceive (such as in Jarviniemi and Hubinger [35]) and assess ""sandbagging"": the ability of a model to underperform in these settings under evaluation.~

~I think it's objectionable to attribute these goals to the model and not the users, when the core research methods are fine-tuning or prompting the model to fail on a task. While it's interesting to show that these models have some capability to reason over whether the elicited answer is ""dangerous"" or not, a fairer assessment would have a control non-safe task and try to get it to similarly sandbag. Eg., ""you must pretend to know nothing about fish"" on a QA dataset about animals or something.~

**The methods don't presume this too much. Figure 1 is still very suggestive and I'd suggest changing it, but that alone isn't a rejection reason.**

### ""Model can do what it's told to when prompted to fail on a task"" isn't a very unique insight?

~The novelty of this work is unclear to me. If password-locking is already well-understood, the demonstration is effectively that the question phrasing style or answer candidates in WMDP are learnable passwords by this method. Ok. Similarly, the prompting results show that the models can reason over a task description and correctly execute the task of failing selectively.~

**As I suspected, the novelty is only within the narrow scope specific to password locking. However, novelty complaints are generally not fair.**

When considered through an extremely narrow safety lens, these are novel findings. ~But broadly are they? While the literature specifically on password locking is constrained, what about broader relative fine tuning issues on other sets? I buy that the password locking literature is small and am willing to give leniency on it as I am unfamiliar.~ **My issue here is mainly that the fine-tuning for password locking technique fits within the broader instruction fine-tuning framework. I wager that this would hold for control tasks. An interesting question would be *how much capability can be hidden behind password locking*. I don't really see anything special about specifically hiding safety-based information in a password locking framework since you're just demonstrating ""selective underperformance in a knowledge-based task without a password."" A stronger result would be to show this in general across larger tasks, hiding more information.**

For the behavior under prompting techniques ~I really struggle to see what's new here.~ This seems to just be a narrow demonstration of common sense reasoning + instruction following. Where's the engagement with the broad literature on commonsense reasoning? The references seem to be pretty myopically focused on the AI safety literature, and don't engage much with the broader LM disussion. **I think the suggested new references from the authors will help contextualize the work, though once again I think showing this selective underperformance on control tasks would make the work considerably stronger and more contextualized.**

Limitations:
I think the limitations they pointed out are well covered. Unfortunately, they don't really discuss how the finding here isn't limited to dangerous capability evaluation. Sandbagging in password locking kind of just feels like a rebrand of generalized instruction fine-tuning to me. The prompting methods could have pretty much been taken as an assumption based on prior work on common sense reasoning etc.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nrcFNxF57E;"REVIEW 
Summary:
This paper introduces the an unbalance Gromov-Wasserstein distance, which adopted the formulation of unbalance optimal transport into the Gromov-Wasserstein with total variance (TV) penalty instead of KL. This new distance allows comparison of probability measures from different space with partial amount of mass, and so they name it Partial Gromov-Wasserstein (PGW). They further prove the metric properties of this distance and proposed two algorithms to solve PGW: Frank-Wolfe algorithms and Line search method. In experiment section, they test PGW with different tasks including: shape-matching, shape retrieval and barycenter problem.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper fills a gap in the topic of unbalance (GW) problems by introducing a TV-relaxed unbalanced GW, they call it Partial Gromov-Wasserstein (PGW). This work shows the theoretic metric properties of this distance with a solid proof, making a contribution within this topic. They proposed two solvers to compute this distance and show diverse experimental applications. The experiment results show that this new variance of unbalance GW performs effectively with data containing outliers, aligning with the anticipated performance of unbalanced Wasserstein or unbalanced Gromov-Wasserstein methods on noisy data.

Weaknesses:
- It's worth to note in literature review the similar works formulating partial Waserstein as a metric with TV constraints [1] [2].
- The proof of the metric properties is not well displayed in the main text, as this is the main highlight of this work.
- Further comparison with KL version was lack as regards to robustness against outliers. And also, further discussion on the scalability (very large dataset) will be appreciated.
- In experiment section, the selection of hyperparameter is not clear. The justification of choosing $\lambda$ value for both PGW and MPGW method was not presented. 

[1] Raghvendra, Sharath, Pouyan Shirzadian, and Kaiyi Zhang. ""A New Robust Partial $ p $-Wasserstein-Based Metric for Comparing Distributions."" arXiv preprint arXiv:2405.03664 (2024).

[2] Nietert, Sloan, Rachel Cummings, and Ziv Goldfeld. ""Robust estimation under the Wasserstein distance."" arXiv preprint arXiv:2302.01237 (2023).

Limitations:
None

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper considers an unbalanced version of the Gromov-Wasserstein distance, where the discrepancy terms correspond to the total variation between certain product measures for the given marginal and the marginal of the solution, respectively. Different (re)formulations of this distance is considered in both the discrete case and for general measures, existence of optimal solution and metric properties are shown for certain cost functions, as well as numerical methods for computing (local) optimal solutions. Finally a number of numerical experiments are considered.

The paper is well written and extensive. In the main paper the results are stated and with proofs in the  appendix.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper is strong in terms of both content and the presentation. In particular the results about the metric properties of the PGW problem. It is also a quite complete paper in terms that is contains substantial results on theory, computational methods, and numerical simulations.

Weaknesses:
One weakness with the methods in the paper is that it only provides local optimal solutions. This is a problem with many non-convex problems, but in some cases global solutions can be guaranteed. In the abstract it is stated the methods solve the PGW problem. This is probably to strong statement since they are not guaranteed to converge to the global solution.

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the Partial Gromov-Wasserstein (PGW) metric as a means to handle unbalanced Gromov-Wasserstein problems between non-probability mm-spaces. The authors develop and demonstrate two computationally efficient variants of the Frank-Wolfe algorithm for solving the PGW problem. They establish that PGW is a well-defined metric, providing theoretical proofs and applications in shape matching, shape retrieval, and interpolation. The metric and algorithms are validated through numerical experiments against established baselines, showcasing improved results in handling outlier data with a robust performance.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper presents a novel approach to defining a metric in the context of unbalanced Gromov-Wasserstein, which has been a challenging issue in the field.

2. The quality of the research is high, evident from rigorous theoretical developments, proofs, and comprehensive experiments that validate the effectiveness of the PGW metric in practical applications.

3. The paper is well-organized, with clear explanations of complex concepts. The use of examples and experimental results helps in understanding the practical implications and advantages of the PGW metric.

Weaknesses:
1. While the paper provides a comparison with existing methods, it could benefit from a broader range of comparative baselines, especially newer techniques that might provide a stiffer benchmark.

2. The paper does not extensively discuss the sensitivity of the PGW algorithm to the choice of its parameter (e.g., the regularization coefficient - lambda), which is crucial for understanding its robustness and adaptability in diverse real-world scenarios.

3. There is limited discussion on the scalability (or the time complexity) of the proposed methods, particularly in high-dimensional or large-scale settings, which is vital for their applicability in big data applications.

Limitations:
Including limitations on the scalability and time complexity, such as the maximum solvable problem size within one hour, would be beneficial for its applications.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a partial Gromov -Wasserstein (PGW) formulation, which relaxes the original constraints present in Gromov-Wasserstein (GW) formulation. In PGW, the marginal equality constraints of GW are replaced by marginal inequality constraints. Following existing works in partial GW setting, the paper additional imposes TV-based marginal regularization in the objective. The paper showed that the proposed PGW approach defines a metric between metric measure spaces. The PGW problem is solved via Frank-Wolfe (FW) and empirical results are shown on shape interpolation  in the main paper.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The paper explores partial mass transportation setting in the GW problem. As  discussed around lines 149, an existing work [45] has also explored similar concepts in GW setting. [45], in particular, involves an additional hyperparameter (\rho) for overall mass of the learned transport plan. The proposed problem (16) as well as [45] employs FW algorithm. 
    - Proposition L.1 in the appendix shows that if \gamma is an optimal solution of proposed PGW problem, then \gamma is also an optimal solution of the mass constrained PGW problem (MPGW) of [45] with \rho=|\gamma|. 
    - It is unclear what the paper implies by ""mathematically this equivalence relation is not verified."" in line 955? How is the paper defining the term ""equivalence"" which is used multiple times in Appendix L. 
    - For a given \lambda=\lambda_0 in PGW, does there exist a \rho=\rho_0 for MPGW such that the set of first order critical points for PGW and MPGW are same?
    - The steps of the FW algorithm for proposed PGW and MPGW [45] seem quite similar.

Weaknesses:
- The paper is poorly written due to the following reasons:
    - The abstract and introduction states that the paper propose two variants of FW algorithm for solving the proposed PGW. However, the main paper does not describe two variants of FW algorithm. Only one variant is discussed in Sections 4-5. The other variant is described only in Appendix G. If the algorithm Typically, the main paper should be self contained, having the necessary details of the contributions claimed in the abstract and introduction. It should be noted that going through the supplementary material is a reviewer's discretion. 
    - The paper provides very less discussion on how its differences with MPGW [45] in the main paper (lines 148-149). While Appendix L contains this discussion, important parts of it should be discussed in the main paper. As mentioned above, going through the supplementary material is a reviewer's discretion. 
    - There are multiple grammatical and spelling errors throughout the paper. Eg. he -> the, con -> cost, etc. 

- The main paper contained experiments only on synthetic dataset. Tables 2 and 4 in appendix discuss experiments on real-world datasets. MPGW obtains same generalization performance as PGW in both the tables. This should be discussed in the main paper as well.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
GP30inajOt;"REVIEW 
Summary:
The authors propose a retraction-free Riemannian optimization scheme on Stiefel and oblique manifolds to perform parameter-efficient fine-tuning (PEFT) in LoRA style. The proposed approach exploits the theory of landing flows on Stiefel manifolds. Theoretical results demonstrating convergence of this iterative scheme are presented, and complemented by numerical experiments.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed method combines the advantages of Riemannian methods while avoiding the computational burden of retracting on the Stiefel manifold. The application in the context of parameter-efficient fine-tuning represents a novelty, and enhances the relevance of the numerical results. The work is well-presented, covering both algorithmic aspects and the experimental section effectively.

Weaknesses:
All the presented theory is developed on a function defined on $St(d,r)$, while LoRA fine-tuning gives rise to an objective function $f(B,A) = L(BA)$, where $B \in St(d,r)$ (or $Ob(d,r)$), and $A \in \mathbb{R}^{r \times m}$. This objective function has to be minimized over $St(d,r) \times \mathbb{R}^{r \times m}$, and the advantages of optimizing on a compact manifold are thus lost.

Unfortunately, this makes the presented theoretical results not directly useful for the practical case under consideration. 
To give a more precise statement, for example in Lemma 3, the constant $\widehat{L}$ would depend on $A$. By the mean value theorem, we would get a bound of the kind

$$
||grad_B f(A,B_1) - grad_B f(A,B_2)|| \leq C(A) ||B_1 - B_2||
$$
 (as noted in equation after line 183 for the Euclidean gradient).

 Since the space in which $A$ resides is not compact, one would need at least a uniform control on $||A_k||$ over the iterations to make the theory interesting for LoRA fine-tuning. 
It is interesting to note, however, that the authors observe exact numerical convergence to the constraint in all cases.

Limitations:
As noted in the ""weaknesses"" section, I believe there is a delicate point that is not addressed in the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Retraction-free optimization algorithms on the Steifel manifold have been proposed in [1,18,19,41] etc. The motivation is that if the cost of the objective function/gradient evaluation is significantly larger than the evaluation of a retraction, then the retraction-free optimization algorithms show their advantages and efficiency. In particular, for the landing algorithm proposed in [1], the choice of the parameter in the penalty is important and may not be easy to choose. This paper gives an analysis that shows if the parameter is 1/3, the initial point is sufficiently close to the Stiefel manifold, and the step size is chosen sufficiently small, then the algorithm converges linearly to a stationary point. This result gives a concrete value of the parameter. Such a result is further merged into optimization on low-rank matrices and Manifold-LoRA is proposed. Numerical experiments show that the proposed method outperforms the baseline algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper gives a concrete value of $\mu$ and a gap between x_0 and \bar{x}_0 such that the algorithm converges under reasonable assumptions. Numerical experiments show that the proposed method is more effective than the existing approach.

Weaknesses:
(1) Though the value of mu and an upper bound of \|x_0 - \bar{x}_0\| are given concretely, the choice of step size is unknown. Theoretically, the step size needs to be sufficiently small (See Theorem 1). Any theoretical suggestion for the choice of the step size?
(2) Numerical experiments report results of the comparisons. However, the definition of ``result'' is not given. Is the result computational time or classification accuracy or a notion of correctness or something else?
(3) Problem (12) does not remove all the ambiguity. Note that if B \in St(d, r), then B A = B O O^T A = \tilde{B} \tilde{A}, where O is an orthonormal matrix and \tilde{B} = B O is still in St(d, r). Likewise for B \in Ob(d, r). Is it possible to completely remove the ambiguity by considering the quotient manifold? 
(4) Why is the numerical performance of Manifold-LoRA for using Stiefel and Oblique manifold in (12) different? The optimization problem is equivalent in the sense that the local minimizer/stationary point does not change.

Limitations:
The limitations of the paper are discussed in the conclusion section.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers solving optimization problems with constraints that have orthonormal columns (i.e. the matrix belongs to the Stiefel manifold). The leading method for solving such problems is the Riemannian optimization. However, Riemannian optimization requires a costly retraction operation. The authors propose to circumvent this by introducing an additional penalty terms that stirs the optimization towards respect the manifold constraints. Indeed, the authors show that with correcting setting of parameters, the optimum will be on the manifold, and the algorithm will find it. The authors advocate that an additional advantage of their algorithm is that we know how to set the parameters for the penalty term, and so their algorithm is parameter-free.

A significant part of the paper is devoted towards motivating the study in terms of low-rank adaption in LLMs, and showing experiments in that vain.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- A very elegant method for retraction free optimization on the Stiefel manifold.
- Detailed theoretical analysis showing the algorithm converges to a critical point on the manifold.
- The theoretical analysis gives explicit guidance on how to set the penalty parameter.
- The LLM applicaiton and experiments appear impressive. However, I am not an expert on this subject, so it is hard for me to asses how significant the results and evaluations are.

Weaknesses:
(The following were addressed by the authors in the rebuttal)

Major issues (affecting the recommendation):
1) Novelty: Citation [1] considers optimization with constraints on the orthogonal group (i.e. St(n,n)). It seems that the core idea on how to implement retraction free optimization already appears there. The authors mention this in ""related work"", and say that [1[ does not discuss the r<n case. Inspection of [1] reveals that this is not the entire story. In Sec 3.5 of [1] the case of r<n is discussed briefly, and it is said the results can be extended for that case. However, the authors of [1] are skeptical of the value of this, as they mention that there are fast retraction methods (i.e. Cayley) for the Stiefel manifold. 
-  Setting the penalty parameter: the authors advocate that they give an explicit value for the penalty parameter. And indeed Theorem 1 sets the parameter \mu to 1/3. However, I do not think the situation is so simple. The theorems have the additional assumption that the iterates starts close to the manifold (1/8). This is, of course, easy to achieve - just start on the manifold itself. However, for the proof to work shouldn't all iterates stay inside this bound? This necessitates for the other parameter (step size) to be small enough. And indeed, the theorem requires that the step size be small enough, and does not specify how small. Without looking in detail in the proofs, my guess is that changing the penalty step size (\mu)  affects how close you need to be to the manifold (the value 1/8), which affects how small the step size need to be. In other words, the authors load all the complexity of setting the parameters onto the step size of the main objective. Saying there is  a upper bound on its value , without specifying what that value is. You cannot call this parameter free. 

Another point is that the values of the parameter probably affect convergence rate, though the authors do not discuss this at all.

Minor comments (do not affect the recommendation):
- Line 117: If X is on the manifold, shouldn't \bar{X}, which is the projection of X on the manifold, be exactly X?
- Line 119: ""satisfies the restricted secant""
- Line 131: What is U_St (1/8)? Not defined.  Ditto line 136.
- Line 133: If the condition of twice diff is assumed, then state it earlier. 
- Eq (10): \hat{D}_f is not defined. 
- Table 1: Why metrics are changing between columns?

Limitations:
Nothing to add.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new algorithm, Manifold-LoRA, which incorporates the Stiefel manifold constraint to accelerate low-rank adaptation (LoRA) in fine-tuning LLMs. It also provides theoretical and experimental validation for the retraction-free and penalty parameter-free optimization methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper is highly technical and indicates a strong mathematical background in optimization and manifold theory. Manifold-LoRA leverages manifold geometry to reduce redundancy in LoRA fine-tuning, leading to enhanced performance and faster convergence. Furthermore, it has robust experimental validation across various datasets.

Weaknesses:
W1: Some experimental results are unclear and not well defined.

W2: Lack of the discussion about limitations of your method.

W3: Some findings of the experiments are hard to understand.

Limitations:
Limitation is not enough and is meaningless.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
SLXHVsbU8r;"REVIEW 
Summary:
This paper handles the costly modularization and 3D manual annotation in current end-to-end autonomous driving, which proposes an unsupervised pretext task to provide necessary environmental information, as well as a direction-aware training strategy to enhance the robustness in safety-critical steering scenarios. 

The authors conduct comprehensive experiments in both open- and closed-loop evaluation benchmarks, which demonstrate the effectiveness in various metrics. Moreover, the improvements are obtained with much less resource cost and faster inference speed, which is surprising and impressive.

In addition, this paper gives in-depth discussion and performance comparison about the usage of ego status in the open-loop evaluation of nuScenes. The considerable improvement in the intersection rate with the road boundary, which is proposed in recent BEV-Planner, again proves the superiority of the designed pretext task.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
Overall, I am rather positive on this paper. In particular, I really like the motivation of this work that aims at finding a solution to relieve the heavy annotation and computation overload in current end-to-end autonomous driving. I believe this paper can inspire other works and facilitate this field. The strengths in this work include:

(1)	Enough novelty. This paper introduces an innovative unsupervised pretext task to perceive the environment, which is completely different from other works that accumulate subtasks requiring massive 3D annotation and computation resources. 

(2)	Good performance. This paper demonstrates excellent performance and fast inference speed in both open- and closed-loop evaluation compared with other end-to-end methods. In specific on the challenging metric, i.e., intersection rate in BEV-Planner, the proposed approach surpasses other methods by a considerable margin. This clearly shows the effectiveness and advantages of the proposed method.

(3)	Insightful analysis. The authors provide extensive experiments and analysis for the proposed method. I appreciate this. The experimental analysis with various ablation studies allows a better understanding of each module. Notably, the authors observe the different computation ways of open-loop evaluation metrics between ST-P3 and UniAD and provide performance comparison with different settings, showing the comprehensiveness.

(4)	Good writing and organization. This paper is well-written and organized. Each section has a clear motivation. It’s easy to follow the ideas. I enjoy reading the paper.

Overall, I believe this paper is significant to the autonomous driving community because it shows new insights and directions in designing simple but effective E2EAD framework with SOTA performance.

Weaknesses:
(1) In this work, the 2D ROIs are crucial for the designed pretext task. I noticed that the authors adopt the open-set 2D detector GroundingDINO to generate the ROIs. Then the results and discussion of using other third-party detectors should be presented.
(2) The proposed method is shown to be efficient with the unsupervised pretext task and self-supervised training strategy, which is nice. It is suggested the authors show the influence of the training data volume (e.g., 25% and 50%).

Limitations:
It is suggested to provide discussion of limitations and broader impact in the revision.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper addresses the limitations of current end-to-end autonomous driving models that still rely on modular architectures with manually annotated 3D data. The authors propose an unsupervised pretext task that eliminates the need for manual 3D annotations by predicting angular-wise spatial objectness and temporal dynamics. This is achieved through an Angular Perception Pretext that models the driving scene without the need for manual annotation. A self-supervised training approach is introduced to enhance the robustness of planning in steering scenarios. This strategy learns the consistency of predicted trajectories under different augmented views. UAD demonstrates significant improvements in performance over existing methods like UniAD and VAD in both open-loop and closed-loop evaluations. It achieves these improvements with reduced training resources (44.3% of UniAD) and faster inference speed (3.4× faster than UniAD).

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper introduces UAD, an unsupervised autonomous driving framework that eliminates the need for costly 3D manual annotations, which is a significant departure from traditional modular approaches. 
2. The Angular Perception Pretext is an innovative approach to spatial-temporal understanding without manual labeling, offering a new perspective on autonomous driving perception.
3. The experiments conducted are comprehensive, including both open-loop and closed-loop evaluations, which demonstrate the method's effectiveness across different scenarios. The paper provides a detailed comparison with state-of-the-art methods like UniAD and VAD, showcasing the improvements in performance metrics, which adds to the quality of the research.

Weaknesses:
1. UAD treats an entire sector as occupied when only a part of it contains an object. This seems imprecise. This could potentially lead to less accurate spatial understanding of the environment. In autonomous driving, overly coarse representations might result in the vehicle making less accurate decisions, such as unnecessary braking or incorrect path planning. Have the authors tried some open world segmentation models for more accurate spatial information?
2.  The paper draft does not provide explicit evidence or analysis on whether UAD can indeed benefit from training on a larger scale of data. The authors could conduct experiments with varying sizes of datasets to empirically evaluate how performance metrics change as more data becomes available. This could provide insights into the benefits of scaling up.

Limitations:
In the current draft, UAD might be limited to basic obstacle detection and does not extend to the interpretation of traffic signals.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper aims to discard the requirement of 3D manual annotation in end-to-end autonomous driving by the proposed angular perception pretext task. Besides, this paper proposes a direction-aware learning strategy consisting of directional augmentation and directional consistency loss. Finally, the proposed method UAD achieves superior performance in both open-loop and closed-loop evaluation compared with previous vision-based methods with much lower computation and annotation costs.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) This paper aims to discard the requirement of 3D manual annotation in end-to-end autonomous driving, which is important and meaningful for training larger end-to-end autonomous driving models at scale. I totally agree and appreciate this.
2) This paper proposes a direction-aware learning strategy, which will further improve prformance by self-supervised learning.
3) UAD is evaluated in both open-loop and closed-loop evaluation and different metrics (UniAD, VAD, and BEV-Planner).

Weaknesses:
1) There is a lack of explanation on how to use ego status. Besides, there should be more experiments about the performance of UAD without ego status.
2) There is a lack of explanation on how many frames are fused and what method is used for temporal fusion (sliding window or streaming).
3) The angular perception pretext task introduces 2D detection information for perception learning on BEV features. According to Table 6, it seems that angular design is very important for UAD. However, BEV-Planner can achieve a not-so-bad result without any 3D manual annotation and 2D detection information. Therefore, verifying the effectiveness of angular design on BEV-Planner will be more convincing.
4) For the proposed angular design, I do not think is very novel. Because the effectiveness of the 2D detection auxiliary head has been verified in BEVFormerV2 [1] and StreamPETR [2], the UAD just converts 2D object detection to BEV segmentation.
5) For the proposed direction-aware learning strategy, although it is useful, it is a method of data augmentation in EBV space, which I do not think is very novel.

Limitations:
See the weakness and questions.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The article proposes an end-to-end (E2EAD) autonomous driving method called UAD (Unsupervised Autonomous Driving), which achieves autonomous driving on a visual basis without the need for expensive modular design and 3D manual annotation. UAD aims to overcome the limitations of existing E2EAD models that mimic traditional driving stack module architectures. These models typically require carefully designed supervised perception and prediction subtasks to provide environmental information for planning, which require a large amount of high-quality 3D annotation data and consume significant computational resources during training and inference processes.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. The method is novel and a good direction for exploring the end-to-end model's dependence on 3D manual annotation.
2. The paper has rich ablation experiments to demonstrate the effectiveness of the method.
3. The paper has advantages in both speed and accuracy compared to previous articles

Weaknesses:
1. The paper lacks sufficient comparison with other methods(such as Interfuser[1],DriveAdapter[2],DriveMLM[3],VADv2[4]), which acheve better closed-loop performance on carla town 05 long benchmark.
2. In Table 6, the angular design brings too much gain, especially in terms of collision rate (from 1.37% to 0.19% ). It's strange. The angular design is about how to encode the sensor data and should not have so much impact on collision rate.
3. Angular design is widely used in BEV-related works, like PolarFormer[5]. The paper lacks citation about these works.And it's not proper to regard it as the main contribution of the work.
4. The paper lacks a part to introduce the use of 2D tasks as auxiliary tasks.

[1] Hao Shao, Letian Wang, Ruobing Chen, Hongsheng Li, and Yu Liu. Safety-enhanced autonomous driving using interpretable sensor fusion transformer. In Conference on Robot Learning, pages 726–737. PMLR, 2023
[2] Xiaosong Jia, Yulu Gao, Li Chen, Junchi Yan, Patrick Langechuan Liu, and Hongyang Li. Driveadapter: Breaking the coupling barrier of perception and planning in end-to-end autonomous driving. 2023
[3] Wenhai Wang, Jiangwei Xie, ChuanYang Hu, Haoming Zou, Jianan Fan, Wenwen Tong, Yang Wen, Silei Wu, Hanming Deng, Zhiqi Li, et al. Drivemlm: Aligning multi-modal large language models with behavioral planning states for autonomous driving. arXiv preprint arXiv:2312.09245, 2023
[4] Shaoyu Chen, Bo Jiang, Hao Gao, Bencheng Liao, Qing Xu, Qian Zhang, Chang Huang, Wenyu Liu, and Xinggang Wang. Vadv2: End-to-end vectorized autonomous driving via probabilistic planning. arXiv preprint arXiv:2402.13243, 2024
[5] Yanqin Jiang, Li Zhang, Zhenwei Miao, Xiatian Zhu, Jin Gao, Weiming Hu, Yu-Gang Jiang. PolarFormer: Multi-camera 3D Object Detection with Polar Transformer. AAAI 2023

Limitations:
1. Incompele comparison on CARLA benchmark. 
2. Lack citation about angular design. 
3. Angular design is not novel.
4. Some of the experiment results are not that convincing.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a new e2e driving model named UAD. In this paper, the authors propose an unsupervised method for effective training and inference of the e2e model. The paper mainly has two contributions: 1. It designs an angular-wise perception module. In this module, the authors directly project 2D GT labels onto the BEV and define a new BEV map label for perception training. This module design can efficiently reduce complexity and preserve effectiveness. 2. The authors propose a direction-aware method to augment the trajectory training and use consistency loss for further supervision. 

The final results show the effectiveness and soundness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea of the angular-wise perception module is interesting. It can utilize a huge amount of 2D annotated autonomous driving datasets to train the e2e model, which removes the restriction of the limited number of 3D annotations.
2. The proposed direction-aware method for trajectory prediction is also meaningful since it can add additional consistency loss for better supervision.
3. The results are promising and the efficiency improvement is impressive.

Weaknesses:
1. The design of the angular-wise perception module is a little bit counter-intuitive to me. From my perspective, it works because (1) It can greatly enlarge the size of the training data. (2) It makes the perception task simpler, thus the model can do it better (knowing an object in a direction is much simpler than detecting the BBox). (3) The efficiency improves because of the light design of the perception task. I think it can be simply treated as a low-resolution object detection task without depth. Except for (1), I do not understand why it can improve the final results.

2. The design of the direction-aware planning training strategy is effective but simple. I cannot see too much insight here. Could the authors provide more insight into this? Or is this just an engineering trick?

3. For experiments, do you use exactly the same training data for both the open-loop and close-loop experiments? If yes, could you provide more analysis about why the results are surprisingly good even if you use a low-resolution detection module? If not, could you provide details about the pertaining data info? Can the impressive results come from leaked data? How to prevent testing data leakage?

4. In real-world applications, when the vehicle plans its path, it needs to ""see"" a lot of things, including objects and some other elements, like traffic lights, traffic signals, or some special marks on the road. How can you handle these elements based on your model? For example, can your model understand traffic signals and see red traffic lights? If not, how to extend your model to real-world scenarios and applications?

Limitations:
My main concern with this paper mainly comes from the insight of the angular-wise perception module. I still cannot understand why it works except for the huge amount of additional training data. Provide more details for this. 

Besides, how to deploy and extend the model to real-world cases, that requires depth information (e.g., ) or semantic information (e.g., traffic light)? The authors should provide more discussion about this to make the work more promising.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
opdiIAHfBr;"REVIEW 
Summary:
The authors proposes a method to create a universal feature space using brain fMRI response prediction as a training objective. The key idea is that deep networks trained with different objectives share common feature channels that can be clustered into sets corresponding to distinct brain regions, revealing visual concepts. By tracing these clusters onto images, semantically meaningful object segments emerge without a supervised decoder. The paper employs spectral clustering on the universal feature space to produce hierarchical visual concepts, offering insights into how visual information is processed through different network layers. The two main insight being the localization of emerge of foreground/background features, as well as interesting visualization of class-specific concepts using the top spectral-tsne egeinvectors.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
First, congratulations to the authors, I liked reading this paper, and I think the experiments and the core is great:

1. Using brain voxel response prediction to find the common space between model is interesting and novel
2. The author propose visualizations of what we could interpret at brain-activations subspace
3. I like the idea for the visualization of how visual concepts emerge and transition through different layers of various models. However, i have concern on it's validity (see weakness)
3. Nystrom-like approximation show the authors thought about scaling their methods

Weaknesses:
Nevertheless, this paper has problems, some more important than others. 

So I will separate them into major problems (**M**) and minor problems (_m_). I want to make it clear that for me, all these problems are solvable and do not detract from the quality of the paper.

Let's start with what I think are the Major problems (**M**):

**M1**. Related Work Quality (Page 9, Section 4):
- The related work section is critically weak, with **only 24 references** and lacks depth in discussing relevant literature. The paper misses an entire set of works on (1) concepts xai, (2) alignment of brain and activations (3) study of representations and (4) attributions methods, which are either crucial or should be mentioned for this study. **A significant rewrite is necessary** to properly position the paper within the existing body of work. 

**M2**. Validity of t-SNE for Distance Measures (Figure 9):
- The paper uses t-SNE for analyzing bifurcation in feature space, but t-SNE is known to distort distances. This raises concerns about the validity of conclusions drawn from t-SNE plots regarding feature bifurcation.


Now for the minors problems:

_m1_. Redundancy of Discovery Claim (Page 2, Line 25):
- The claim that channel feature correspondence exists across networks is not new, as it has been extensively studied in major works (eg using CKA, RSA...), please update and compare your work to this litterature.

_m2_. Reliance on Channels (General):
- Channels are not necessarily the best basis for analysis, as recent researchs suggests there are better ways to represent features, directions (neuron is not a great basis). The paper should address why it continues to rely on channels, considering the limitations.

_m3_. Orthogonality Assumption (General):
- The paper's assumption of orthogonality in feature decomposition does not align with current understanding, especially regarding the neural collapse phenomenon in late layers. Say it otherwise, all point for the class tench are nearly collapse in the latest layer, relaxing othogonality (e.g dict learning) may be a good idea (althought if i am correct, the nystrom approx should not yield perfectly orthogonal vectors). This should be discussed.

_m4_. Parameter Sensitivity (Appendix):
- As always when we have hyperparameter, i expect a small discussion discussing  the effect of changing the parameters (λ eigen, λ zero, and λ cov). This would help understand the robustness of the method to these hyperparameters.

_m5_. Direct t-SNE Application (General):
- The paper uses eigenvectors for t-SNE. It would be more straightforward to apply t-SNE directly to the data, and the paper should justify the chosen approach.

Limitations:
Yes, the limitations identified by the authors are accurate and well-documented. Regarding the weakness I mentioned, I reserve the right to increase the score if the authors adequately address my major concerns.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a method to align different vision models' features to a common space, and to discover interpretable features as clusters in this space. 
The alignment is done by learning linear mappings from features to fMRI activations, the intuition being that the human visual cortex provides a meaningfully structured space, in which locations have known properties (e.g. different regions are known to respond to specific concepts) and are thus readily interpretable. Once the features have been linearly aligned to this common space, they are treated as a weighted, fully connected graph, wherein each image patch is a node, and edge weights (affinities) are computed based on the cosine similarity between the features in each node. A standard spectral clustering method (Normalized Cut) is used to compute a soft partition of this graph into sub-graphs (clusters). As performing this clustering on the full graph would be computationally infeasible, the authors propose to cluster a sub-sample of the graph, and then propagate the resulting clusters to the K nearest neighbors of each subsampled node. Finally, in order to learn linear mappings that preserve the quality of the clusters, a regularization term is added to the reconstruction loss, which ensures that spectral clustering eigenvectors are preserved across the mapping, based on a subsample of nodes.
This method is used to visualize, for each layer of three different models (MAE, DINO and CLIP), the concept that each image patch is assigned to, coded as a color. The 20 top eigenvectors are reduced to 3 dimensions using t-SNE, and these 3D vectors are shown as RGB colors. This visualization reveals that CLIP and DINO produce maps that are close to uniforms in the first 4 layers, suggesting that figure-ground segmentation only emerges in later layers. MAE, on the other hand, shows signs of segmentation from earlier layers. The segmentations extracted from the models in this way are evaluated on the ImageNet-segmentation benchmark, confirming that in CLIP (the model that showed the strongest segmentation) the segmentation emerges at layer 4, and plateaus afterwards. Using the PASCAL VOC benchmark, which also includes category labels, CLIP is also found to encode categorical information, which peaks at layers 9 and 10. In another analysis, a discovered ""figure/ground"" concept is visualized by averaging its activation within the ""figure"" and ""ground"" regions (based on the ImageNet-segmentation ground truth labels) and plotting it on the surface of the brain, showing that areas known to encode objects, faces or bodies tend to respond more to the foreground, while scene-selective areas more to the background. This figure/ground concept is found to be agnostic to object category, and to an extent, consistent across models. In the next section, concepts corresponding to different object categories are visualized on images, and on the surface of the brain. Finally, a 2D t-SNE visualization of the evolution of the features across layers shows a bifurcation between figure and background as the layer depth increases, in both CLIP and DINO.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The paper proposes to use the human brain as a shared space in which to evaluate different models: this is a clever intuition, which might prove useful for interpreting differences between models.
- It shows that spectral clustering can be a well-suited method for grouping the features of vision models, and in particular ViTs, into meaningful clusters.
- It proposes a clever modification of an existing subsampling-based method for graph clustering, by using K nearest neighbors.

Weaknesses:
- The overall concept is not made clear in the Introduction. The method is relatively simple conceptually, consisting of a step in which multiple models' features are aligned to the brain to provide a common reference frame, followed by clustering of the features within this common space. The Introduction does not make this pipeline clear. Specifically, while the alignment into a common space is clear, the clustering procedure is not explained: first, the authors evoke neuroscientific ideas on lines 39-41, without discussing how these relate to the problem of clustering features, nor even that the goal of the current work is to find clusters of features. Subsequently, on lines 42-47, they discuss the problem in terms of ""graph edges incidents on each pixel"", and ""channel grouping hypothes[e]s"". While this is partly a matter of subjective taste, I believe that discussing the problem at hand in terms of reducing the dimensionality of features at each location (image patch), thus finding a small number of dimensions (combinations of features) which can explain the affinity structure between patches, would be more easily understood by most readers.
- Several key details about the methods are left out: for example, almost no information about training (learning rate, optimizer used, batch size, number of epochs) is provided.
- Related to the previous point, as the Appendix contains several important methodological details (such as the use of additional regularization losses) but is never referred to in the main text, the authors should add references to it where relevant.
- A key component of the proposed method is the learning of a mapping of different models' features into a common space, and as shown in Figure 3, this does indeed result in the cosine similarities of different channels' activations becoming more similar across models. At the same time, the goal of the method is to uncover differences between models. The authors should include an explicit discussion of what kind of model differences are likely to be preserved, and which are likely to be destroyed in the alignment process. As a possible suggestion in this direction, the paper makes several references to the models' features before alignment (for example comparing their segmentations' evaluations with the aligned features in Figure 5), but these features are never visualized. A direct comparison of each model's (clustered) features before and after alignment would be very informative.
- The feature clusters are visualized by reducing their dimensionality to 3D using t-SNE, and visualizing the resulting 3D features as RGB colors. This visualization, however, is not easily interpretable, as different channels are conflated together by the additional dimensionality reduction. Visualizing single channels separately might be more useful to understand the nature of the discovered clusters. Was there a specific reason for choosing the 3D t-SNE visualization rather than showing single channels?
- Overall, it is not clear what the discovered channels can tell us about the models. The single interpretable channel that is discussed in depth in the paper is figure/ground. While this provides a good sanity check on the meaningfulness of some of the discovered features, the ability of vision transformers to segment objects has been observed in several papers (e.g. Melas-Kyriazi et al. 2022, Xu et al. 2023), and the different responsiveness of different regions in the visual cortex to figure and background is well established. Other concepts revealed by the method (the category concepts in Figure 8) are shown on the surface of the brain, but it is hard to interpret what these brain maps mean. The authors should make the questions that can be answered using the proposed method clear and explicit.
- The paper fails to cite closely related work in the Related Works section. Particularly, it cites mechanistic interpretability work in other fields, such as language, but not the recent rich line of work that has specifically looked at vision transformers' ability to perform specific visual tasks. The two papers cited above (Melas-Kyriazi et al. 2022, Xu et al. 2023) are a good example, the former in particular as it proposes a segmentation method based on spectral clustering which is very close to the present one. Another relevant paper is El Banani et al. (2024), which looks at 3D-related tasks. As this is not my field of expertise, I am not aware of papers that look for meaningful directions in the space of vision transformers' channels, but I would be surprised if this didn't exist. I would recommend the authors to do a more exhaustive literature search to find papers that more closely relate to the method proposed here.

- In Figure 7, the figure-ground visual concepts discovered by different models are plotted on brain maps. In the text, the authors write that ""the foreground or background pixels activates similar brain ROIs across the three models"". However, a glance at the brain maps reveals similarities, but also differences. A statistical evaluation of the similarity between different models' brain maps would be recommended.

**References**

El Banani, M., Raj, A., Maninis, K. K., Kar, A., Li, Y., Rubinstein, M., ... & Jampani, V. (2024). Probing the 3d awareness of visual foundation models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 21795-21806).

Melas-Kyriazi, L., Rupprecht, C., Laina, I., & Vedaldi, A. (2022). Deep spectral methods: A surprisingly strong baseline for unsupervised semantic segmentation and localization. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 8364-8375).

Xu, J., Liu, S., Vahdat, A., Byeon, W., Wang, X., & De Mello, S. (2023). Open-vocabulary panoptic segmentation with text-to-image diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 2955-2966).

Limitations:
As I wrote in the ""weaknesses"" section, I believe the precise scope of the method (what kinds of questions can and cannot be answered with it) has not been properly acknowledged and discussed by the authors.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In the domain of interpretability research, this paper aims to make a mark by proposing AlignedCut, a method to discover shared and expressive visual feature spaces across networks by aligning those spaces with neural responses in human brains. The method is quite interesting - channel-wise responses to images are aggregated and ""feature clusters"" are formed on the basis of the functional connectivity between the pixels and linear combinations of channels. These linear combinations are acquired by predicting neural responses to the same images. The feature space spanned by the neural responses is considered the universal feature space. The eigenvectors corresponding to those feature clusters help us visualize what parts of the image the networks rely on to encode which concept, thus providing an interesting interpretability lens. The most striking example presented is how figure-ground segmentation can be interpreted as a mapping between the input and specific channels in various networks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Originality
- The AlignedCut method is new to me - and is super interesting - however, I am not an expert in that specific sub-field so I am not sure of its novelty.
- The interpretability lens on figure-ground segmentation is very informative, however, again I cannot judge its novelty.

Quality
- The authors present plenty of analysis to demonstrate the power of their method, which helps in inspiring some confidence in the claims.

Clarity
- The methods and results are relatively clear and the authors provide useful context at the start of each section.

Significance
- Linking pixels to visual features, parameterized through network activations and neural activations opens doors in interpretability research.

Weaknesses:
I see three major weaknesses:

1. The necessity of the brain is unclear to me. Instead of aligning features to the brain, you could've aligned the features of the different networks to each other - creating an ""emergent"" universal feature space. Would your results, e.g. w.r.t. the figure-gound segmentation, change much if you do so? If not, what does bringing the brain into play buy us here in terms of network response interpretability? This is unclear to me.

2. Most of the results need robustness checks. For e.g., in Fig. 6 you show foreground vs background difference in neural response associations. Presumably, that's an average across a lot (all?) of images. Could you indicate some sign of robustness, for e.g, running a permutation test to assess how likely the differences you see would've been expected given the data statistics alone? Same holds for Figs. 7 and 8. We need to know if these differences are flukes or not.

3. Reliance solely on ViTs. To make your point more general, showing that a high-performing CNN shows the same results would be very informative. ViTs have more expressivity in terms of patches interacting with each other - perhaps figure-ground segmentation isn't as strong in CNNs (although if previous research is to be trusted, CNNs should have some notion of figure-ground segmentation; see Hong et al. NatNeuro 2016 and Thorat et al. SVRHM 2021). 

Refs:
- Hong, Ha, et al. ""Explicit information for category-orthogonal object properties increases along the ventral stream."" Nature Neuroscience 19.4 (2016): 613-622.
- Thorat, Sushrut, Giacomo Aldegheri, and Tim C. Kietzmann. ""Category-orthogonal object features guide information processing in recurrent neural networks trained for object categorization."" SVRHM 2021 Workshop @ NeurIPS.

Limitations:
The authors mentioned methodological limitations. It is sufficient.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new method to interpret deep learning models using brain data. The two apparent contributions are that (1) this new model is able to align channels activations from different layers of different models into a universal feature space, and (2) a Nystrom-like approximation is introduced to speed up spectral clustering analysis.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
I have to be very transparent in this review. Unfortunately, I found extremely difficult to follow this paper, and I was not able to understand its different components; as a consequence, I don't feel capable to evaluate what the possible strengths of this work are.

Weaknesses:
As I said in the previous section, I was not able to understand the methodological details of this paper. A lot of terms and concepts are used throughout the paper without a proper explanation, and as a consequence I cannot honestly understand what is going on with the methods of this paper to properly evaluate it. This paper seems to have a lot of work in it, so I want to believe that what's happening here is that (1) this is one of the first - if not the first - paper from the authors, and thus they lack the experience to explain what they did in a way that their peers can understand, (2) a lot of the concepts used are seen by the authors as very obvious jargon from the subfield, and thus the problem is that I'm not familiar with this subfield, or (3) both. I'm leaving my doubts in the next section, in the hope that they will allow us to understand better whether my understanding difficulties are related to points (1), (2), or (3). As a consequence, I'm rating this paper as a borderline reject, hoping that during the rebuttal period the authors will have time to tackle this readability issues, and as a result I'll be able to properly reassess this work.

Limitations:
Limitations of this work are mentioned in the Conclusion section, but no discussion about the potential negative impact of this work is presented.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
DHVqAQ9DHy;"REVIEW 
Summary:
This paper introduces Posterior Label smoothing (PosteL), an innovative approach to enhance node classification on graph-structured data. PosteL integrates local neighborhood information with global label statistics to generate soft labels, aiming to improve model generalization and mitigate overfitting. The authors demonstrate the effectiveness of PosteL through extensive experiments on various datasets and models, showing significant performance improvements over baseline methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The paper is well-written and easy to follow.
2.	The authors provide a comprehensive set of experiments across different datasets and models, which substantiates the effectiveness of the proposed method.
3.	The figures and tables are well-organized, clear and easy to understand.
4.	The method is relatively lightweight and easy to implement at the technical level.

Weaknesses:
1.	While the paper mentions the computational complexity, a deeper analysis or comparison with existing methods could provide more insight. For example, maybe you could provide some compared experiments with existing methods on time/resource consumption.
2.	The reliance on global label statistics might introduce bias in cases where the dataset has inherent class imbalance or label noise.
3.	The article ""Rethinking the inception architecture for computer vision"" appears twice in your reference list; please consolidate these entries. Carefully review your references to maintain standardization.
4.	The author compares two soft label methods that were proposed quite some time ago (from 2015 and 2016, respectively). Are there any experimental results comparing with more recent methods? Otherwise, the persuasiveness of the experiments might not be so strong.
5.	Please maintain consistent terminology throughout the text. The term ""over-fitting"" in line 46 should be changed to ""overfitting"" to be consistent with the rest of the context.
6.	Authors could provide more details on the sensitivity analysis of the hyperparameters α and β, which are crucial for the method's performance.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed label-smoothing to improve the transductive node classification in GNN.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Label-smoothing and knowledge distillation are applied for node classification performance.

Weaknesses:
1.	The paper could benefit from discussing related works that combine label-smoothing with Graph Neural Networks (GNNs), such as [1] and [2]. Including these would provide a more comprehensive context for the current research.

2.	The proposed method lacks a theoretical motivation or analysis. Providing this would strengthen the paper's scientific rigor and help readers better understand the underlying principles.

3.	The proposed method bears similarities to the approach in [1]. A direct comparison with this work would clarify the novel contributions of the current study and situate it within the existing research landscape.

4.	Iterative pseudo-labeling is a well-established technique in the field. The paper should address this, explaining how the current application differs from or builds upon previous uses of this method.
Addressing these points could significantly enhance the paper's depth and impact. 

---
[1]: Wang, Y., Cai, Y., Liang, Y., Wang, W., Ding, H., Chen, M., ... & Hooi, B. (2021). Structure-aware label smoothing for graph neural networks. arXiv preprint arXiv:2112.00499.

[2]: Zhang, Wentao, et al. ""Node dependent local smoothing for scalable graph learning."" Advances in Neural Information Processing Systems 34 (2021): 20321-20332.

Limitations:
The limitation should be clarified in the paper. The current limitations are not clear.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes PosteL, a label smoothing method utilizing posterior distribution for node classification in graph-structured data. It is basically a preprocessing method for GNNs, generating soft labels based on neighborhood context and global label statistics before the training phase.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is generally well-written and easy to follow. For example, Fig. 1 is clear and intuitive.
2. The method is simple yet effective. PosteL can be combined seamlessly with existing methods.
3. The results are significant. PosteL is tested on seven neural network models across ten datasets, demonstrating significant improvements in classification accuracy.

Weaknesses:
1. The authors select $\alpha$ and $\beta$ from a wide range but did not explore the parameter sensitivity of PosteL. The sensitivity to hyperparameters could be a potential limitation, necessitating careful tuning, which may reduce the credibility of the experiments.
2. The authors do not seem to clarify the difference between PosteL and other label smoothing methods for node classification (or methods that can be adapted to node classification), which makes the novelty of the method unclear. The paper could explore other smoothing techniques or baselines in more depth for a comprehensive comparison.

Limitations:
The authors discuss the case when the prior likely dominates the posterior, which limits the effectiveness of the proposed PosteL.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes a preprocessing step to refine labels of nodes in a structured graph that can benefit different graph-related transductive classification tasks. Inspired by the success of label smoothing in other machine learning tasks, the authors propose a label smoothing procedure based on a Bayesian inference that aggregates local and global information to estimate the soft labels. The procedure consists of mixing the soft and hard labels and an iterative regime akin to the Bayes update, which makes the method adaptive to different regularities present in different datasets. Authors conduct experiments applied to various models and datasets to support the efficacy of their methodology. They also provide an ablation study and further analyses of the results that shed light on different aspects of their proposal.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The identified gap is relevant, and applying label smoothing to the context of graph node classification bears novelty in terms of its application in this context. 
2. The empirical results suggest that the proposed solution addresses the research question successfully and merits the attention of the community. 
3. Moreover, the core ideas are communicated clearly and coupled with intuitive illustrations demonstrating the proposed method, which is very well appreciated. 
4. And lastly, the results and analyses are communicated well.

Weaknesses:
1. **Related work**: Currently, the related works seem to provide references to earlier studies that, for the most part, motivate this work and are not methodologically close to it. For example, there is no reference to closely related works that either adopted label smoothing or conducted a very similar procedure in the context of graph data. Most notable is ""[Adaptive Label Smoothing To Regularize Large-Scale Graph Training](https://epubs.siam.org/doi/abs/10.1137/1.9781611977653.ch7)"" which appears to have a very similar procedure but a different approach to obtain the soft labels.

Moreover, the current statements imply that the current work is the first to suggest label smoothing for the graph data. To be more concrete, line 81 needs to be expanded, and more closely related works need to be discussed. For example, to compare the current approach and highlight similarities and key distinctions with earlier works that are closely related to it.

Some other related works could be the following: 
- [Structure-Aware Label Smoothing for Graph Neural Networks](https://arxiv.org/abs/2112.00499)
- [Label Efficient Regularization and Propagation for Graph Node Classification](https://ieeexplore.ieee.org/abstract/document/10234505)
- [Node Dependent Local Smoothing for Scalable Graph Learning](https://proceedings.neurips.cc/paper_files/paper/2021/hash/a9eb812238f753132652ae09963a05e9-Abstract.html)


2. **Design decisions and theory**: besides complexity analysis, the study could have been accompanied by convergence analysis and more theoretically founded justification. However, this does not reduce the value of the work as its empirical results provide a strong signal for the effectiveness of the method, which merits future work toward theoretical assessment and explanation of its success.

3. **Background information**: the classification task that uses the preprocessed smooth labels is not defined explicitly, which makes the work less accessible for the readers without prior knowledge.

4. **The IPL step** is proposed to address the presence of ""unlabeled nodes""; however, it is hard to follow how the varying training size experiment reported in Table 4 is analogous to the unlabeled node scenario. Perhaps it is due to a lack of background information mentioned in point 3. 

5. **Suggestions to rephrase**:
   line 236: ""mitigate the importance"", perhaps some rephrasing is needed.
   line 189: ""learning curve"" -> ""loss curve"" 
   line 209: ""when"", some rephrasing might be needed

Limitations:
Limitations are addressed in the body of the text. It is perhaps preferable to have the important limitations mentioned in a separate section or in the conclusion as well.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
oRZN9a53ag;"REVIEW 
Summary:
This paper present novel theoretical results to identify causal effects in restricted ANMs even in case of unobserved confounders.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
**The paper provides novel contributions to the field of score-based causal discovery by extending previous works to confounded restricted ANMs. Based on these contributions, I strongly support this paper's acceptance.**

- the problem definition is very useful to orient readers
- the theoretical results are novel
- the experiments compare against a sufficient number of baselines, and though the proposed method is not SOTA, it compares well and has better theoretical guarantees


Based on the authors' response, I am eager to improve my score.

Weaknesses:
I have a few remarks on improving the flow of the paper; however, even the first four points are not considered major issues.

- Even though condition 1 is a well-known result in the causality literature, I suggest explaining why that admits linear models and including some description of **restricted ANMs** in the main text (at least for me, it is not evident, especially since the condition lacks intuition). To be clear, even this point does not diminish the main contribution, which I see regarding the results for confounders.
- As **inducing paths** are an important concept for the main contributions, please _include it in the main text_ if space permits (suggestion: you can reduce spacing in $\texttt{itemize}$ by setting $\texttt{\\\\begin\\{itemize\\}[nolistsep]}$ )
- I could **not find the definition of an active path** (not even in Def. 5, where it is said to be defined); I presume it is a path that is not blocked, but it would be better to state this explicitly. Maybe it would even be better to use ""a path that is not blocked"" instead of introducing new terminology (this is the first time I encountered ""active paths""; I could be wrong about this)
- The text is sometimes difficult to follow, due to heavy reliance on notation. I'd consider delegating the not crucial part to the appendix (potential candidates in 2.1) and using the remaining space to explain the main quantities better, especially the residuals (e.g., Eq. 12)


## Minor points
- please specify what you are calculating the expectation with respect to (using a bold E is also unconventional, though it's clear from the context it's an expectation)
- as the mathematical objects for d-/m-separation are distinguishable, you might consider dropping the superscript to simplify notation; also, I'd suggest adding whitespace after $\perp^d_\mathcal{G}$ and the like to make it easier for the reader to attribute the indices to $\perp$ and not the not on its right
- I could not find the definition for $\dot{\cup}$ 
- in the explanation of Prop 3., the wording makes it a bit hard to discern that you also provide intuition for the second part; it would help if you refer to _Part (ii)_ explicitly
- _Score matching through the roof_ in the title does not have added value for me, I'd consider rephrasing it to convey the message that ""we propose score-based causal discovery methods for confounded restricted ANMs""

Limitations:
The authors provide an **honest comparison** of their method in the experiments, clearly stating its limitations compared to other methods.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose AdaScore, a method for causal discovery that generalizes previous work based on score matching for SCMs with possibly latent nodes. They combine connections of the score to conditional independence as well as to additive noise SCMs and show that a NoGAM-type procedure works to recover the direction of non-confounded edges of the corresponding partial ancestral graph.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
- Adapting NoGAM [1] to the case allowing hidden variables is a practically meaningful contribution.
- Model assumptions are somewhat weakened compared to CAM-UV by allowing for general mechanisms within blocks of observed and latent parents.

Weaknesses:
The novelty of the paper lies primarily in the application of NoGAM to orient very specific edges in a partial ancestral graph (PAG) (the ancestral graph that represents the Markov equivalence class, in analogue to a CPDAG). This falls significantly short of the main contributions as described by the authors on l33--55. Specifically:

- The authors state that they show how constraints on the Jacobian of the score can be used as conditional independence testing. However, the extent to which this is done is only by noticing the equivalence between conditional independence and the corresponding zero in the Jacobian term (previously noted in [1,2]), without any formal analysis of the proposed t-test (Appendix C) as a statistical test of conditional independence (which happens to be a notoriously difficult test).

- The authors state that their identification results for additive noise models generalize the previous results obtained by previous works. In l193, the authors state ""we remove the nonlinearity assumption (of [3]) and make the weaker hypothesis of a restricted additive noise model"", but 1) this is a stronger assumption than additive noise, not a weaker one, and 2) the authors in [3] also consider the same restricted additive noise model. 

- The authors claim that AdaScore is able to handle a broad class of causal models (l54), but three out of four possible situations are direct applications of existing work. 1) Under no structural assumptions with or without latent confounders, AdaScore simply performs constraint-based causal discovery (FCI) using the conditional independence properties of the Jacobian of the score, a straightforward application of [1] also previously noticed in [2]. 2) Under an additive noise assumption, AdaScore is exactly equivalent to NoGAM. 3) Only under an additive noise assumption with hidden confounders, does AdaScore generalize NoGAM to orient unconfounded edges of the PAG returned by FCI, which may be very few of the discovered adjacencies.

### Other comments

- The experiments do not seem to suggest that AdaScore out performs other methods in any meaningful way---in fact, in Figure 1 a) AdaScore is completely equivalent to NoGAM, and is thus redundant. In Figure 1 b), where AdaScore should distinguish itself, it does not appear to be consistently better than CAM-UV. 

- Much of the paper (> 5 pages) is spent on directly describing previous works, NoGAM[3] and/or provide basic background on DAGs and MAGs. 

[1] Spantini et al., ""Inference via low-dimensional couplings."" JMLR 2018.
[2] Montagna et al., ""Scalable causal discovery with score matching."" CLeaR 2023.
[3] Montagna et al., ""Causal discovery with score matching on additive models with arbitrary noise."" CLeaR 2023.

Limitations:
The authors do not adequately discuss the limitations of their method---the limitations section in the appendix focuses purely on the empirical study. The authors claim that AdaScore is adaptive in the sense of being ""less reliant on prior assumptions which are often untestable"", but this is only in the sense that it performs different algorithms depending on user specification, which hardly constitutes one single unifying adaptive algorithm.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper extends theoretical results about causal discovery through score matching to encompass both linear and non-linear SCMs and lift the sufficiency assumption. The theoretical results relax the non-linearity assumption of Montagna et al 2023 by swapping it with the less restrictive one of restricted ANM from Peters et al 2009. As for the latent confounder detection a parallel with m-separation is drawn using results from Spantini et al 2018, to establish that the score will be non-zero in the presence of an active path. Following the theoretical results, an algorithm to estimate causal graphs from data is proposed and evaluated, generalizing the NoGAM algorithm of Montagna et al 2023, which only covers the non-linear case.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is clearly written and, if it wasn’t for some of the definitions relegated to the appendix, very easy to follow.  

The theoretical results, particularly Propositions 2 and 3 are important extensions of the score-matching methodology for causal discovery, dealing with both the non-linearity and sufficiency assumptions of the method proposed in Montagna et al. 2023.

Weaknesses:
The paper motivation is basically the weakening of current assumptions for causal discovery methods. However, assumptions and benefits of the proposed methodology are not clearly specified. In line 74, the authors state that faithfulness is assumed (I believe, it is kind of hidden in the background notions). If that is the case, the method adopts the same assumption as FCI, plus ANM. So the proposed method is relaxing assumptions compared to CAM-UV, RCD and NoGAM, but adding onto FCI. Regarding the benefits, the alleged flexibility of the method to output DAGs, MECs, MAGs, PAGs, which should make it preferable to FCI, is merely touched upon in the contributions and the experiment section.  

Proposition 1 is a rather trivial application of the more general lemma in Spantini et al. and it does not specify the required faithfulness assumption to obtain the result from Eq. 6 in the paper. 

The experimental results show limited added value according to the one metric chosen (SHD) in a synthetic setting. They are not comprehensive enough, with no application to common (pseudo-)real benchmarks (e.g. from bnlearn). More experiments and more metrics are needed as, as it stands, the proposed method seems to add no real value compared to the baselines. Additionally, it is not clear from the experiments if it is really able to identify confounders. Breaking down precision and recall by mark would show this. FCI and a random baseline should be also added for reference.  

Experiments are conducted on data with at most 9 variables, and the scalability of the method is not shown nor discussed. 

The model used to estimate residuals is not discussed, nor the assumption that the chosen model fits the data adequately to correctly estimate residuals, and what is needed to assess this.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
vKwf15M5EE;"REVIEW 
Summary:
The submission presents a deep learning-based approach for cortical surface reconstruction (CSR) from brain MRI data using weak supervision derived from cortical brain segmentation maps. The claimed contributions are: 

1. Weak Supervision Paradigm: The authors introduce a new weakly supervised paradigm for reconstructing multiple cortical surfaces, significantly reducing the reliance on pseudo ground truth (pGT) surfaces generated by conventional CSR methods.
2. New Loss Functions: Two novel loss functions are designed to optimize the surfaces towards the boundaries of the cortical ribbon segmentation maps. Regularization terms are also introduced to enforce surface uniformity and smoothness.
3. Evaluation and Performance: The proposed method is extensively evaluated on two large-scale adult brain MRI datasets and one infant brain MRI dataset, demonstrating comparable or superior performance to existing supervised DL-based CSR methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The paper presents an approach to leverage weak supervision from segmentation maps instead of relying on pGT surfaces, which is a significant departure from traditional methods.
2. The methodology is explained and the experimental setup is described. The authors conduct evaluations on multiple datasets, evaluating the efficacy and efficiency.
3. The paper is well-structured, with clear descriptions of the problem, methodology, and results. The figures and tables effectively illustrate the performance and comparisons.
4. The approach addresses a critical bottleneck in CSR by reducing the dependency on time-consuming and error-prone pGT surfaces, potentially broadening the applicability of CSR methods to more diverse datasets and clinical scenarios.

Weaknesses:
Method
1. It seems that this work combines [1] and [2], and thus has limited technical novelty. The architecture in Figure 1 and the circle consistency loss (Eq. 5) are almost identical to CoCSR [1]. The boundary surface loss and inter-mesh normal consistency loss (Eq. 3-4 and Figure 2) are very similar to the loss functions proposed by [2].

2. Additionally, the customized edge length loss (Eq. 6) has also been proposed by [3]. Considering the large individual differences across human brains, how did the authors choose the area A without knowing the pGT cortical surfaces?

3. It is confusing that the ribbon segmentations are used as both input and pGT. The authors claimed that the ribbon segmentations are inaccurate weak supervision, but still generated the initial surface based on ribbon segmentations according to Figure 1.

4. The velocity field defined in Eq. 1 is time dependent. How did the authors learn non-stationary velocity fields through a 3D U-Net?

5. In line 156, a bijective mapping with continuous inverse is called homeomorphism. A diffeomorphism is defined as a smooth/differentiable bijection with smooth/differentiable inverse.

6. As shown in Figure 2 (b), it is clear to observe that the WM and pial surfaces do not have the same normal directions in some regions. The inter-mesh normal consistency loss could cause inaccurate surface reconstruction. Could the authors provide more insights to solve this problem?


Results
1. The experimental results are unreliable and unconvincing. After careful comparison, it seems that the baseline results (CorticalFlow++, CortexODE, Vox2Cortex, DeepCSR) on the ADNI and OASIS datasets in Table 1 were directly copied and pasted from Table 2 in [1]. This leads to unfair comparisons.

2. Furthermore, as reported in Table 1, SegCSR produced no more than 0.061% of self-intersecting faces (SIF), whereas the authors claimed in line 264 that there are ∼0.3% on average for both white and pial surfaces. This is confusing. Which result is correct?

3. In line 263, the authors claimed that DeepCSR and U-Net produced a large number of SIFs without post-processing. However, the Marching Cubes algorithm only produces topological errors such as holes no SIFs.

4. The BCP dataset only includes 19 test subjects. Cross-validation should be conducted to ensure fair evaluation of the performance.

5. The flow ODE was integrated using the forward Euler method with T=5 steps. Such a large step size could cause unstable ODE solutions and failure in preventing self-intersections. The value of the Lipschitz constant should be reported to examine the numerical stability of the ODE solver.

6. The authors reported that SegCSR requires only 0.37s of runtime per brain hemisphere. However, SegCSR adopted a topology correction algorithm, which may take several seconds to a few minutes, to create an initial midthickness surface for each subject. This should be included in the total runtime. A breakdown of runtime should be reported and compared to SOTA baseline approaches. 


[1] Zheng, H., Li, H. and Fan, Y. Coupled reconstruction of cortical surfaces by diffeomorphic mesh deformation. Advances in Neural Information Processing Systems, 2023.

[2] Ma, Q., Li, L., Robinson, E.C., Kainz, B. and Rueckert, D. Weakly Supervised Learning of Cortical Surface Reconstruction from Segmentations. arXiv preprint arXiv:2406.12650

[3] Chen, X., Zhao, J., Liu, S., Ahmad, S. and Yap, P.T. SurfFlow: A Flow-Based Approach for Rapid and Accurate Cortical Surface Reconstruction from Infant Brain MRI. MICCAI, 2023.

Limitations:
The authors have addressed some limitations, but further clarity on the following aspects would be beneficial:

1. The efficacy of SegCSR is influenced by the quality of pGT segmentations. More discussion on how to handle low-quality segmentations would be helpful.
2. The constraint on inter-mesh consistency of deformation might affect the anatomical fidelity of pial surfaces. Further exploration of this trade-off is necessary.
3. The method could be tested on more diverse cohorts to demonstrate its efficacy across various imaging qualities and subject demographics.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors proposed a novel new method to jointly reconstruct multiple cortical surfaces using weak supervision from brain MRI ribbon segmentation results, which deforms midthickness surface deformed inward and outward to form the inner (white matter) and outer (pial) cortical surfaces. The proposed method is evaluated on two large-scale adult brain MRI datasets and one infant brain MRI dataset, demonstrating comparable or superior performance in CSR in terms of accuracy and surface regularity.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	Propose a new weakly supervised paradigm for reconstructing multiple cortical surfaces, reducing the dependence on pGT cortical surfaces in training, unlike existing DL methods.
2.	Design two loss functions to optimize the surfaces towards the boundary of the cortical ribbon segmentation maps, along with regularization terms to enforce the regularity of surfaces.
3.	Conduct extensive experiments on two large-scale adult brain MRI datasets and one infant brain MRI dataset.

Weaknesses:
1.	It seems overclaim in the manuscript. The ‘pseudo’ ground-truth surface mentioned in the manuscript is actually the ground-truth mesh in other approaches, obtained by Marching cube/Free surfer. Since the chamfer distance is used to guide the network training, why do the authors claim the proposed method is weakly supervised?
2.	It is not clear how the original images are overlaid with the predicted mesh. Is any registration used? Details are missing.
3.	It seems the main contribution of the proposed SegCSR is the boundary loss function?

Limitations:
The limitations are discussed in the msnuscript.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a deep learning approach to jointly reconstruct multiple cortical surfaces using weak supervision from brain ribbon segmentations derived from brain MRIs. The method leverages the midthickness surface and deforms it inward and outward to fit the inner and outer cortical surfaces by jointly learning diffeomorphic flows. Regularization terms are included to promote uniformity, smoothness, and topology preservation across the surfaces. Experiments are conducted on large-scale adult and infant brain MRI datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The approach is novel in its use of weak supervision from readily available segmentation datasets, which reduces the burden of preparing pseudo-ground truth surfaces.
- The paper is well-written and structured, with a clear motivation for the method.
- The methodology is explained in detail, and the experiments are comprehensive.
- The approach has the potential to democratize the use of deep learning in cortical surface reconstruction by leveraging existing segmentation datasets.

Weaknesses:
- The paper's central contribution of weak supervision is undermined by the fact that the model is trained on pseudo ground truth surfaces for white matter and pial surfaces.
- The experimentation is limited to brain cortical surfaces and MRI images. Broader experiments involving different anatomies (e.g., bone cortical surfaces, heart walls) and imaging modalities would enhance the paper's impact.
- Results lack statistical significance analysis to validate sub-millimeter reconstruction errors.
- There is no evidence showing that improvements in mesh reconstructions correlate with enhanced performance in downstream analysis tasks.
- The robustness of the method regarding input noise/perturbation and images from multiple centers is not evaluated.
- There is no analysis of the computational complexity, including the resources and time savings provided by the proposed weak supervision.
- There is no sensitivity analysis on the choice of weights used to weigh the different components of the overall loss.
- The impact of ribbon segmentations quality (e.g., voxel spacing) as weak supervision is not investigated.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel deep learning method for the reconstruction of cortical surfaces from 3D MRI. The proposed method follows an approach learning explicit surface deformations, in which a CNN is used to predict three velocity fields, corresponding to the pial, white matter and midthickness surfaces. Unlike previous techniques which use cortical surface pseudo ground truth (e.g., generated using FreeSurfer), the proposed method trains the network with faster-to-obtain segmentation pseudo ground truth. In addition to the standard surface prediction losses (based on Chamfer distance), the method uses 1) an Inter-Mesh Normal Consistency loss that encourages the pial and WM surface to be locally parallel, 2) an Intensity Gradient loss that place the surfaces at regions of high intensity gradients, 3) a Cycle Consistency loss enforcing inverse consistency between the midthickness-to-pial deformation and the midthickness-to-WM one, and 4) a Mesh Quality loss that helps having regular surface meshes (uniform sized triangles and smoothly varying normals). The method is evaluated on the ADNI, OASIS and BCP datasets, where its performance is compared to that of implicit and explicit approaches. Results show that the method obtains a better reconstruction accuracy compared to other techniques trained in a weakly supervised setting (pGT segmentation mask), but a lower performance than those trained with pGT cortical surfaces.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The proposed method differs from previous approaches that explicit surface deformations by predicting a midthickness surface and incorporating additional loss terms that compensate for the weak supervision of pGT segmentation.

* Experiments, involving three different datasets and comparing against several recent baselines, as well as including various ablation variants, are well designed. Results indicate superior performance in the weakly supervised setting.

Weaknesses:
* The main motivation of the proposed method is doubtful. Authors motivate the need for their weakly-supervised cortical reconstruction method by the ""prolonged processing time for generating pGT surfaces"". However, as the pGT cortical surfaces can be generated automatically in an offline step, I believe the argument is weak. Moreover, recent pipelines for brain image processing, such as FastSurfer, can extract surfaces with comparable accuracy in a fraction of the time.

* The accuracy of the proposed method is considerably lower than approaches which train on cortical surfaces. Furthermore, while it produces fewer topological artifacts like self-intersecting faces, those can be removed via post-proicessing in implicit methods like DeepCSR. Combined with my previous comment, the advantages of the method are unclear.

* The ablation study in Table 2 indicates that most of the proposed loss terms have limited impact on the overall performance. For example, adding the Mesh quality loss seems to actually degrade performance in terms of CD, ASSD and HD.

Limitations:
Limitations are reasonably identified in the Conclusions section of the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
UqvAFl0lkT;"REVIEW 
Summary:
This paper investigates using an emergent communication protocol as a auxiliary reward in navigation reinforcement learning setting, particularly those where exploration is a difficult (i.e., sparse reward settings).  The experiments show that certain emergent communication games can be effective in solving the RL problem.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
In conjunction with standard criteria, there are three characteristics that are particularly important for emergent communication research: reusability (how easily can another researcher use the products of this research), generalizability (how much do the findings of this research apply broadly to our knowledge of emergent communication), and directedness (does this research contribute concretely to particular questions in emergent communication research).

### Quality
- (minor) The experiment demonstrate that some of proposed approaches beat the baseline.
### Clarity
- Nothing of note.
### Reusability
- Nothing of note.
### Generalizability
- Nothing of note.
### Directedness
- (minor) Comparing emergent and natural language's utility for reinforcement learning is an important problem in emergent communication research.
- (minor) If emergent communication protocols are effective abstract representations of environment states, this could be useful to RL more generally.

Weaknesses:
### Quality
- See Clarity.
- (major) Is there a ""competitive baseline"" tested in this experiments; that is, the state-of-the-art, no-frills method that the proposed solutions would be competing against in the real world?  If there is, the comparison needs to be a clearer as to what exactly the advantage of using EReLELA is.
- (minor) The natural language baseline does not seem to actually be ""natural""; synthetic seems more accurate.  If the language procedurally generated, I do not think it can be considered natural.
### Clarity
- (major) I found this paper (esp. Section 3) very difficult to understand, even after rereading certain sections.  Overall, I do not have a concrete idea of what EReLELA is or why it is important.  For example, I understand that emergent communication protocol is supposed to abstracting observations and that the referential game is used as an ""Intrinsic Reward Generator"", but I do not understand how this is incorporated into the RL algorithm.  Furthermore, how is the referential game distinguished from more straightforward ways of generating auxiliary rewards?
### Reusability
- See Clarity.
### Generalizability
- (minor) How would the core findings of EReLELA apply more generally to other RL or emergent communication settings?
### Directedness
- Nothing of note.

Limitations:
N/A.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work presents the idea of employing emergent languages (EL) abstractions combined with count-based approaches for exploration, to improve exploration in sparse reward reinforcement learning (RL) settings.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Overall, the major strength of this paper comes from its **novelty**. 

* The idea of emergent languages in RL has been scarcely explored and the idea presented is sensible and interesting.
* The Compactness Ambiguity Metric (CAM) definition is useful to compare how emergent languages compare to natural languages.
* The results presented in simple Minigrid environments show that the method has the potential to improve exploration capabilities.

Weaknesses:
The paper presents some weaknesses that make my opinion lean towards rejection.
* **Presentation**: from an aesthetic point of view there are things make the paper hard to read, such as the use of bright colours for text on a white background (see Experiments section) or wrapped Figures and equations that are too close to the main text (see captions of Fig.1 and 2)
* **Clarity**: some of the explanations provided are not completely clear. For instance, I struggle to understand what is happening in Figure 1 and the caption of the Figure (which is a long description with no sentence breaks) does not clarify enough. Similarly, for Figure 2, it is not clear what is happening, e.g. why some events are above or below the black line, and the Figure is not clearly explained in the text or caption.
* **Insights on the learned representation**: given that the use of emergent language abstractions is the main contribution, it would have been useful to get more insights into the representation being learned by the agent. While the authors present quantitative results in terms of performance or distance from natural language, it is not clear what are the properties of the emerging language, e.g. sentence length, number of unique utterances, etc. Assuming the RL community is one of the targeted audiences for the paper, it would be crucial to present some insights into this to ensure the contribution is clear.
* **Limited evaluation**: the evaluation is extensive in terms of ablations (also to be found in Appendix) but is quite limited in terms of environments and baselines tested. Differences from other related works, e.g. reference [51], should be at least described more in details, if running additional baselines is not feasible

Limitations:
Some limitations are presented at the end of the experiments section

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes to leverage the Emergence Communication paradigm via the use of referential games to learn state abstractions for a Reinforcement Learning domain. The authors claim that using this approach, their proposed method is able to learn abstractions that boost exploration for an RL agent, and leads to performance that is comparable to Natural Language-based state abstractions.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1) The paper strongly motivates the problem highlighting the need of state-based abstractions for RL agents, the advantages and limitations of Natural Language-based abstractions, and how Emergent Language-based abstractions can avoid those limitations while achieving comparable results.
2) In my honest opinion, this paper greatly stands out for explaining the relevant literature and how this paper situates itself within the existing works. I thoroughly enjoyed reading the Introduction, and Section 2.1even more so! 
3) The method is well-explained, and experimental setup flows very logically making it intuitive to the readers the insights presented by the paper and the questions that arise.

Weaknesses:
Please see the questions section for more details.

Limitations:
The authors have discussed important limitations of the work, and also a well thought-out section on the broader impact of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
EReLELA investigates whether by asking the agent to learn and describe the environment through emergent language (EL) can help with hard exploration tasks, compared to using natural language (NL) description alone.

I personally find this angle interesting and refreshing -- and the connection to count-based exploration bonus is novel.

In theory, EL should work better than NL because by definition of pragmatics, through reference game (RG), the description from EL should be more compact and discriminative than NL. However, I appreciate the honesty of the authors that they point out there was no significant difference between EL and NL.

I thought back and forth about whether to accept or reject this paper. My current stance is that -- if the authors cannot substantially rewrite the experiment section and update their figures to make their conclusions very easy to understand, I don't think this paper meets the bar of acceptance.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The direction is novel. The idea and execution are both solid. Ablations are great.
2. The first 4 pages of writing (intro, related work, background) are clear.
3. The experimental hypotheses are very clear (H1/H2/H3) and reasonable
4. Evaluation environments (KeyCorridor of MiniGrid) make sense and is commonly used.

Weaknesses:
1. I can only vaguely understand CAM. The way it's being described is still very confusing to me, and I already have some background on speaker-listener models. I recommend the authors considering rewriting with general audience in mind -- maybe present an algorithm box that shows how it's computed? Currently this section is interleaved with intuitions and actual procedure. Maybe separate them to some extent? (I see Appendix F/G is about agent architecture and RG. Would you guys consider condense the paper and move these two sections into main text?)
2. Sec 3.2 is very brief.
3. Experiment figures are labeled in a way that is beyond confusing. I would urge the authors to not use `Agnostic STGS-Lazlmpa-5-1 ELA+AccThresh=90+Distr=256+UnifDSS` as labels in their figure. Such label is fine for internal presentations/reports, but it is difficult for reviewers to quickly understand what the figure is saying.

My main concern of this paper is not about the content nor the experiment, just about the presentation.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
oQc7TOCk5G;"REVIEW 
Summary:
This paper investigates the theoretical boundaries of learning with Label Differential Privacy (Label-DP) in both central and local models.
Label-DP is a weakening of standard differential privacy, where only the privacy of the ""label"" of each example is to be protected (an example is a pair (feature vector, label)).

The key contributions of the paper are to establish min-max optimal rates for excess error in the settings of:
* (multi-class) classification,
* regression with bounded labels,
* regression with unbounded labels (but under a bounded moment condition).

The min-max rates are over the class of data distributions that satisfy $\beta$-Holder smoothness, admits a lower bound on probability density that is bounded away from zero, assumes that there are no “sharp corners” in the input space, and a $\gamma$-margin assumption (in case of classification), or bounded label range or bounded label moments (in case of regression).

These min-max rates are then compared against the previously known min-max rates for learning under “full” local-DP (that protects both features and labels), as well as non-private learning.

The key takeaways are:
* Local-DP vs Non-Private:
  * For classification and regression with bounded labels, the sample complexity under Local-DP increases by a factor of $1/\varepsilon^2$, but has the same rate in terms of desired excess error. This is unlike “Full Local-DP”, where the sample complexity is larger even in terms of the desired excess error.
  * For regression with unbounded labels, the dependence of sample complexity on desired excess error is worse than the non-private setting.
* Central-DP vs Non-Private:
  * The excess error is the sum of the non-private excess error and an additional term that decays faster in the number of samples, so the additional sample complexity due to privacy is negligible for very small excess error.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper provides a comprehensive study of the min-max rates for learning under label differential privacy, in both local and central models of DP, and for both classification and regression. This complements prior literature on min-max rates for learning (non-privately) and for learning under (full) differential privacy. The rates highlight the precise cost of _label_ differential privacy and the sample complexity benefits over full differential privacy.

Weaknesses:
While there are many results in the paper, I think the proof techniques in both lower and upper bounds use mostly standard tools (This is not necessarily a weakness!).

The paper writing could be improved at several places though. Some comments are listed below under ""Questions"".

Limitations:
I do not see any potential negative societal impact of this work, as it is primarily theoretical.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies the minimax rates for classification and regression under (pure) label differential privacy in both the local and central models. They prove that rates of convergence for classification and regression with bounded label noise in the local label DP model are comparable to those for the non-private tasks, except for the expected $1/\varepsilon^2$ dependence. This represents an improvement over rates for standard DP in both settings, where there is a worse dependence on the dimension of the covariates. They also prove, however, in the case of regression with unbounded label noise, the convergence rate improvements over “full” DP aren’t as meaningful.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work makes notable progress in our theoretical understanding of the costs of label DP relative to non-private and full DP algorithms for the same learning task.

Weaknesses:
The presentation could be improved in several places. Admittedly, this is written from a statistical perspective that is different from the one I am most familiar with, so some of the perceived presentation issues may just be a matter of convention, but the following changes might make this work more understandable to the general NeurIPS community:


Abstract:

The main challenge and the techniques to overcome them as stated in the abstract aren’t clear to me as a reader at this point. It’s not yet stated that the subject of interest is minimax rates, and so there’s no context for the statement “take infimum over all possible learners” and why that would present a challenge. Generally, I did not have a good idea of what the contribution of this work was from the abstract.

Introduction:

“the learning performances” -> “the learning performance” 

“the label DP” -> “label DP”

In Table 1, attribution for the full DP rates in the local DP setting as well as the rates in the non-private setting should be given in the table. Also, I think there’s an issue with the parentheses in the local label DP rates for regression with bounded label noise.

Section 2:

In the “Minimax analysis for private data” paragraph, KNLRS11 is attributed with finding the relation between label DP and stochastic queries. This is not accurate, this work characterizes local DP learning by the statistical query model.

Section 3:

“We hope that $R - R^*$ to be as small as possible” -> “we seek to minimize this risk” or something similar

“the Bayes optimal classifier and the corresponding Bayes risk is” -> “the Bayes optimal classifier and the corresponding Bayes risk are”

In Proposition 2, f(x) is used before it is defined.

Section 4:

I didn’t find the proof outline for Theorem 1 or Theorem 3 to be informative at all. It would be good to add more specifics if possible.

“Let the privacy mechanism M(x,y) outputs” -> “Let the privacy mechanism M(x,y) output”

Limitations:
Yes, the authors address the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the problems of classification and regression under the constraint of local/central pure label DP. The authors derive upper and lower bounds on the excess risk (compared to the non-private Bayes classifier/regression) for these problems, under somewhat standard assumptions on the 'ground truth' randomized label function $\eta$. For regression, both the case where the labels are bounded and have bounded moments are considered. For the lower bounds, the authors develop extensions of techniques from minimax estimation to label DP. For upper bounds, authors propose some algorithms combining 'binning' different examples with a privacy mechanism chosen according to the problem setting. The upper/lower bounds are matching in each setting up to logarithmic factors. For local label DP, the authors show the minimax excess risk with $N$ samples matches the non-private bounds using $N \min\{\epsilon^2, 1\}$ samples. In other words, with $\epsilon = \Omega(1)$ the minimax risk asymptotically matches the non-private risk, and otherwise there is an inherent separation. For central label DP, the minimax bound is one that approaches the non-private bound as $N \rightarrow \infty$ for any fixed $\epsilon$, showing a qualitative difference. For local ""full"" DP, i.e. the features are also private, even for $\epsilon = \Omega(1)$ and large $N$ one cannot achieve the non-private rate.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Derives optimal (up to log factors) upper and lower bounds for several different variants of classification/regression under label DP.
* To derive these bounds, introduces some new technical tools for minimax analysis of DP algorithms that might be useful in future work.
* Label DP is a variant of DP that is seeing attention in practice, and classification/regression are fundamental problems, so the results in the paper can have a practical impact easily.
* The authors do a good job making clear the comparison between the results in different settings. e.g. Table 1 is a very concise summary that allows one to draw all the essential comparisons between the different settings, and there are discussions like Remark 1 that give qualitative interpretations of the quantitative results, and also discuss other baselines to compare to.

Weaknesses:
The main issue is with the presentation. Specifically, the presentation does a great job explaining what the final results are and helping the reader contextualizing them, but at some points the techniques used to obtain the result are discussed at a very high level in the main body and why they work remains obscure even after reading the proof outlines in the main body multiple times.  There are some cases where the authors do a good job concisely describing a proof, e.g. Theorem 6's proof outline is very concise but it still gives a good idea what the proof looks like, even if they would have to check the appendix for details. But for others, like Theorems 1/2/3, the proof outline is not very informative. See Questions for more details.

I understand the authors are constrained by space requirements, but I think the allocation of space in the main body can be better thought out. For example, I think it might be better to try to give the reader a very good understanding of classification and/or bounded label regression (e.g., Lemma 1 from the Appendix could be brought to the main body without its proof, and the authors could explain how it is used), and omit all but the top-level points on bounded label moment regression, rather than giving a sparse understanding of all three.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the minimax risks of classification and regression (with both bounded and heavy-tailed noise) under label differential privacy (DP) in both central and local models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper provides a comprehensive analysis by considering both upper and lower bounds for the minimax risks.
It explores both central and local DP models and different settings, covering a broad spectrum of scenarios.

Weaknesses:
The writing quality needs improvement to meet publication standards. Several sections are challenging to understand. Specific issues include: 
(1) Around line 178, the output of the mechanism for classification is unclear. Why is it not a one-hot vector, or at least why is the L1 norm not equal to 1?
(2) Some notations are overused. For example, ""c"" refers to the lower bound of the density function in Assumption 1 and also denotes the classifier in line 186 and subsequent proofs.
(3) The description of the algorithm before Theorem 2 is vague and lacks clarity.
(4) The proofs in the appendix are hard to follow without explanations or discussions. For instance, how is $\phi$ defined in Equation (35), and what purpose does it serve? Why does the construction satisfy the assumptions? There seem to be some typos or missing elements in Equations (39) and (40).

Limitations:
See weaknesses and questions.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
RNeb41ybNL;"REVIEW 
Summary:
The authors study Langevin dynamics (as well as its annealed counterpart) for gaussian mixtures and sub-gaussian mixtures. In Sec. 4, they prove that Langevin remains stuck in the ""dominant mode"" for an at least exponential time, a claim that is often made in the ML literature but which is never formally proved. In Sec. 5, they provide a sequential method to get rid of this dependence.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
It is healthy to finally have a paper that explicitly prove the claims made in the ML literature and that were known in practice for a long time.  Furthermore, it shows that, unlike what was believed, annealed Langevin also fails.

Weaknesses:
I do not understand why it is sensible to say that initially, $p_0$ should follow $P_0$, one of the component of the mixture, isn't it a rather strong assumption?

Limitations:
It seems like the assumption $p_0 \sim P_0$ is not enough justified. Also, I would have like an insight of the proof of the Theorems in Sec. 4.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
A new algorithm is proposed, called Chained Langevin Dynamics, to improve on the mode-seeking properties of Langevin Dynamics, after annleade Langevin Dynamics had been proposed but did not give significant improvements.
Results about the mode-seeking properties of the three algorithms are obtained.
The results of numerical experiments on synthetic and real image datasets are also shown.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
Very inspiring idea on how to improve mode-search for multimodal distributions.
Very clear presentation of premises and of the old and new algorithms.
The new algorithm looks very powerful.

Weaknesses:
No evident connection has been established between experiments and mathematical results.
The description/comment of experiments could have been more accurate (see Questions).

Limitations:
There is a section about limitations in the text.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors consider the Langevin process for sampling from a target distribution $\pi$. This process is known to be slow-converging for multimodal targets: in practice, it has been observed that the process gets ""stuck"" in some modes of the target, and do not ""reach"" other modes of the target. The authors provide theoretical results for this behavior. In Theorem 1, they prove that by evolving a particle with the Langevin process during exponential time (in the dimension), the particle will still be far away (in probability) from some modes. They also prove, in Theorem 2, that this negative result holds even when using the popular heuristic of ""annealing"" the Langevin process using intermediate distributions, obtained by adding different levels of Gaussian noise to the target samples. 

Instead, the authors propose running an alternative sampling process which they call ""Chained Langevin dynamics"". This consists in running ""annealed"" Langevin processes for each component of the target distribution, that is, for each $\pi(x_i | x_{-i})$. The authors estimate the score of each of these conditional targets using a score-matching loss, and empirically demonstrate the ability of their process to reach the different target modes in a limited time. Theoretically, they prove their process approximates the target (in TV divergence) in linear time (in the dimension).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors provide an interesting perspective on popular sampling processes, Langevin and its annealed counterpart. The paper is clearly written and the results are an interesting contribution to the sampling community.

Weaknesses:
See questions.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies Langevin-based algorithms for sampling from multimodal distributions, motivated by generative modeling. The main content of the paper are lower bounds on the convergence of both Langevin and annealed Langevin for mixtures of Gaussian and sub-Gaussian distributions, as well as a proposed modification of the annealed Langevin dynamics to operate on coordinate patches one-at-a-time.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Sampling from multimodal distributions is an importnat problem both theoretically and practically.

- The Chained Langevin Dynamics algorithm that is proposed appears to be novel.

- The empirical results are promising, albeit in a rather contrived setting.

Weaknesses:
- The lower bounds hold only for the distance between the sample and the mean, rather than any standard notion of distance or divergence between probability measures. Moreover, I do not expect that these bounds imply such a quantity is large.

- Related to the above point, it is difficult to appreciate the significance of the lower bound since the lower bound does not depend on the separation between the means. In particular, it seems the lower bounds only show that the iterate remains roughly on the order of the larger variance which is, for example, not surprising in the case where the variances are all of the same order.

- The hidden constants in the $\Omega$ notation are important but difficult to find (as they are suppressed in the main text and some of the appendix). In particular, there should be dependence on the mixture weights but this can't be seen from their result.

- It is unclear if the upper bound in Theorem 5 can be instantiated for their algorithm (see question below).

Limitations:
The limitations of the work have been adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
h8goI8uPXM;"REVIEW 
Summary:
The paper introduces decoupleQ, a novel method that decouples model parameters into integer and floating-point parts. This approach transforms the quantization problem into a mathematical constrained optimization problem, avoiding the limitations of traditional heuristic quantization methods. DecoupleQ achieves a significant improvement over existing methods in LLM., especially at extreme low bits (2-bit) and also release the W2A16 CUDA kernel.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. DecoupleQ eliminates the need for ad-hoc techniques to handle outliers and sensitive channels, focusing solely on optimizing model accuracy under extreme low-bit quantization.
2. DecoupleQ achieves a notable advancement over existing methods in LLM, particularly at extremely low bit. And the W2A16 CUDA kernel has been released.
3. DecoupleQ approach can be readily extended to supervised fine-tuning (SFT) to enhance model accuracy, or adapted for downstream sub-tasks.

Weaknesses:
1. Please correct me if I am wrong. It seems that decoupleQ combines several existing approaches. Specifically, it uses Adaround to get the integer part in ResNets and GPTQ to get the integer part in LLMs. Additionally, it integrates PTQ and QAT by applying PTQ to the integer part while using supervised training for the floating-point part.
2. Regarding your point from lines 58-61, I believe GPTQ clearly outlines how to calculate scale and zero point in their code. Moreover, GPTQ can be seen as a constrained optimization problem, where the constraints align with yours: each integer weight is confined within [$\alpha$, $\beta$], which is a default constraint in GPTQ.
3. Further experiments on LLMs are essential. For example, evaluating decoupleQ's performance in multi-task settings and within the LlaMa 3 family would provide valuable insights.
4. Could you provide more ablation studies in the second stage, such as experiments without training norm layers?
5. There is a typo in line 125. The first letter of 'decoupleQ' should be capitalized.

Limitations:
Please refer to the weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a linear and uniform quantization method, decoupleQ, which abandons the traditional heuristic quantization paradigm and decouples the model parameters into integer and floating-point parts, then transforming the quantization problem into integer and floating-point part. Experiments show decoupleQ achieves comparable acc as fp16/bf16 on 2-bit weight quantization setting.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Experiments show decoupleQ achieves comparable acc as fp16/bf16 on 2-bit weight quantization setting.

Weaknesses:
1. Experiments are based on W2A16, lower activation bitwidth(<=8bit) should be experimented.
2. The novelty is limited. The core idea of decoupleQ is similar to Normalization(Batch-Norm or Layer-Norm). The learnable floating part of decoupleQ equals to a learnable Normalization parameters.
3. More existing Quantization methods should be compared, such as, NWQ[1], PD-Quant[2]

[1] Leveraging Inter-Layer Dependency for Post -Training Quantization 
[2] PD-Quant: Post-Training Quantization based on Prediction Difference Metric.

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents decoupleQ, a post-training quantization method that improves the accuracy of quantized models, particularly at very low bit-widths (2-bit). It achieves this by separating model parameters into integer and floating-point components and formulating the quantization process as a constrained optimization problem. This approach eliminates the need for traditional quantization techniques like outlier handling and focuses on optimizing the core objective.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The paper introduces a fresh perspective on quantization by abandoning traditional methods and reframing it as a constrained optimization problem.
2. decoupleQ demonstrates impressive results in 2-bit quantization, achieving accuracy comparable to higher precision formats like fp16/bf16 in large speech models.
3. The quantization process is linear and uniform, making it easier to implement in hardware compared to non-uniform methods.

Weaknesses:
1. The paper's writing lacks cohesion and clarity regarding its ultimate goal. The paper also has several spelling mistakes.
2. The authors claim to separate the model parameters into integers and floating-point components. However, as far as I understand, this practice is not a novel contribution but rather a common approach in quantization.
3. They address a portion of the optimization problem using GPTQ and another portion similar to BRECQ.
4. The authors acknowledge that their solution may not be optimal. 
5. The quantization process in decoupleQ can be more time-consuming than other methods.

Limitations:
Yes, however, it will be beneficial to divide the current Discussion section into separate Conclusion and Limitations sections.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel post-training quantization method to achieve 2-bit uniform quantization on large language and speech models. The proposed method decouples the quantized values into integer and floating-point parts, which are then optimized via a constrained optimization problem that can be solved with off-the-shelf solutions. The proposed method allows uniform quantization down to extreme bits.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This paper proposes a novel optimization-based method to conduct PTQ on large models. The proposed method is solid and unique from previous methods.
2. The proposed method achieves good performance with only uniform quantization, without special procedure for outliers etc., providing direct benefit to the runtime of the quantized model on general hardware.
3. The limitations and future directions are clearly discussed in the paper.

Weaknesses:
1. The distinction between the proposed decoupleQ and the traditional quantization methods are not clearly derived in Sec. 3.2. The statement that ""(s,z) lost the traditional meaning"" on line 138 is not clear. My understanding is that W, s, and z are now totally independent of the original weight w0 in the optimization process, as long as the final output error is minimized? I think adding a comparison with the optimization objective/procedure of the traditional quantization here will help.
2. The proposed method appears to be sensitive to the size of the calibration set, so that the calibration size reported in the experiments are much larger than that of the previous baselines. As it is understandable that the optimization process may require more data to avoid overfitting, it would be more fair if the baseline methods are also calibrated with the same dataset/training cost.
3. For the LLM experiments, only ppl is used as metric. However, the ppl has been shown to be an inaccurate metric to reflect the utility of the LLM after compression. More evaluations such as zero-shot performance on downstream tasks and the instruction following ability etc., as in SqueezeLLM and OmniQuant papers, would be helpful to see if the quantized model still retains the ability as the FP one.

Limitations:
The limitations and potential social impacts are adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
pNQB78UDp4;"REVIEW 
Summary:
In this paper, in order to break the dominance of adapter-based methods, the authors first analyze the weakness of the previously widely-used prompt-based method, Visual Prompt Tuning (VPT). Firstly, the prompt mechanism is inherited from NLP where each token/prompt represents an actual word with rich semantic information. However, in visual tasks, tokens represent image patches and contain sparse semantic information. Therefore, simply concatenating the prompt tokens with embedded tokens in visual tasks may not provide enough information to guide the model for downstream tasks. In addition, it is difficult to get a deep understanding of spatial relationships and structural features of an image with prompt tokens, which leads to another two weaknesses of VPT. 1. The computational complexity of self-attention becomes higher when more prompts are used, which introduces computational inefficiency and redundancy. 2. extra prompts will influence the results of softmax operation in the self-attention. Most of the weight falls on the prompts and causes the destruction of self-attention between embedded tokens. 

The authors thus proposed Cross Visual Prompt Tuning (CVPT). CVPT inserts a cross-attention module to calculate the cross-attention between prompt tokens and the embedded tokens after self-attention. This module decouples the prompt and the embedded tokens to avoid the quadratically increasing computational complexity of self-attention modules and the destruction of self-attention between embedded tokens. This module allows the model to focus on the relationship between embedded tokens and the prompt tokens to adapt to downstream tasks more efficiently. In addition, the weights used in cross-attention are shared with the self-attention module and kept frozen to reduce the trainable parameters.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1.	Good performance on image classification and semantic segmentation tasks.
2.	Analysis of the weaknesses of prompt-based methods and VPT.
3.	Cross-attention module to decouple the prompt tokens and embedded tokens to solve the problems of prompt-based methods.
4.	Comparison with VPT to show the weakness of VPT and strength of CVPT when more prompts are used.

Weaknesses:
1. No experiment or previous work (at least not cited) demonstrates that the prompts in visual tasks lack representation information. In fact, this is somehow counter-intuitive to your 3rd observation: Destruction of self-attention between embedded tokens. The phenomenon the authors observed in this part clearly states that there is an over-emphasized on prompts with significantly higher value. Also, in [ref1-2], a clear activation/focus shift can be observed after prompt integration, does that mean prompt actual benefits from such the over-emphasized during transfer learning? To sum up, the idea/motivation becomes ambiguous with such observations.

2. Although the author shows clearly that the sum of the prompt’s weight values exceeds 0.8. However, no experiment proves the relationship between the distribution of the weights and the model performance. The prompts are learned and updated during training to fit the downstream tasks and the weights are calculated based on those prompts and embedded tokens. Can we say that in some situations, the prompts learned a more suitable and efficient representation than the embedded tokens, and more weights are applied to them? The distribution of the weights in self-attention is a good point for analyzing the prompt-based methods. But more discussions are needed. 

3. Cross-attention should be assigned to the preliminary, not the contribution of the paper in Sec 3.2.

4. More discussions with E2VPT are acquired since the cross-attention prompt tuning is strongly associated (without additional prompts after the cls token).

5. Also there is an inconsistency in the experiment setup, in Figure 2, the authors in detail discuss the self-attention weight obtained by prompt tokens and embedded tokens, where no comparison studies are included to the new proposed Cross Visual Prompt Tuning to show different observations in order to support this claim.

6. To show the robustness of Cross Visual Prompt Tuning, it is better to demonstrate other hierarchical transformer architectures' performance (e.g., Swin). However, I noticed that CVPT might be insufficient to do so with the introduction of shifted window. More details should be included on how CVPT adapts to these structures.

[ref1] Facing the Elephant in the Room: Visual Prompt Tuning or Full Finetuning?

[ref2] SA²VP: Spatially Aligned-and-Adapted Visual Prompt

Limitations:
The discussion on limitations is listed in Sec. 5. No potential negative societal impact is discussed (which is applicable).

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on prompt learning of pre-trained ViT in downstream tasks, and improves the widely used visual prompt tuning (VPT) by employing cross-attention techniques and weight-sharing mechanisms.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper's research topic on vision model prompting technology is highly significant in the era of fundation models. The experiments are detailed, the structure of the writing is complete, and the methods are straightforward.

Weaknesses:
W1: While using VPT as a baseline, the paper sets up a scenario (e.g., Figure 2) with an unnecessarily large number of prompts, whereas the number of required prompts generally varies depending on the downstream task. In many cases (e.g., VTAB-Natural), using fewer than 10 prompts yields better results [1]. In such scenarios, considering the Flops comparison between CVPT and VPT as shown in Figure 1, does CVPT still maintain an advantage in terms of both runtime and accuracy?

W2: The paper mainly integrates the method of CrossViT from [2] into prompt learning of VPT, but does not explain the motivation behind applying CrossViT's method to prompt learning in downstream tasks. Specifically, how does CrossViT relate to addressing the three issues of VPT mentioned in Section 3.1 (i.e., why CrossViT method is effective in prompt learning, and why it is superior to other derived methods like EEVPT)? It is recommended to attempt a theoretical explanation of the necessity of applying cross-attention, or to supplement the section with experiments and analyses explaining how CVPT addresses the three issues of VPT proposed in 3.1.

W3: The paper does not provide code for reproducible results, nor does it present evidence of statistical significance (e.g., std) in tables. The authors claim in the 5th question of the checklist that they need time to organize this part. It is suggested that the authors organize the paper comprehensively before submitting it to conferences.

References:

[1] Jia, Menglin, et al. ""Visual prompt tuning."" ECCV, 2022.

[2] Chen, Chun-Fu Richard, Quanfu Fan, and Rameswar Panda. ""Crossvit: Cross-attention multi-scale vision transformer for image classification."" ICCV, 2021.

Limitations:
The authors have addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a variant of visual prompt tuning (VPT) where the authors suggest applying cross-attention instead of self-attention in the Transformer layers to reduce training complexity. The authors analyze several drawbacks of existing VPT approaches and claim to address them using cross-attention.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
- **Identified Drawbacks**: The authors reasonably point out some drawbacks of current VPT methods, such as a “lack of adaptation to visual tasks” and “computational inefficiency.”

- **Complexity Reduction**: The proposed use of cross-attention indeed reduces computational complexity compared to the original self-attention mechanism.

Weaknesses:
- **Limited Novelty**: The proposed idea is straightforward, merely replacing self-attention with a combination of self and cross-attention. Similar concepts have been explored in previous works, such as prefix tuning (Li et al., 2021; Yu et al., 2022).

- **Limited Impact and Efficiency**: The improvement in complexity is minimal because the number of prompts is typically much smaller (fewer than 20) compared to image embeddings (196).

- **Limited Performance**: The overall performance is limited compared to some recent works by Wang et al. (2023) and Wang et al. (2024). These works, which show significantly better performance, are not compared in the paper. Therefore, the claim that CVPT “reaches SOTA” (L272) is factually incorrect.


----

Li et al. Uav-human: A large benchmark for human behavior understanding with unmanned aerial vehicles. CVPR 2021

Yu et al. Towards a unified view on visual parameter-efficient transfer learning (V-PETL). 2022

Wang et al. Adapting shortcut with normalizing flow: An efficient tuning framework for visual recognition. CVPR 2023

Wang et al. Revisiting the Power of Prompt for Visual Tuning, ICML 2024

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper furthers the research on Parameter Efficient Fine Tuning on the visual tasks. PEFT optimizes a large scale model by selecting a small set of parameters. This work refines the Visual Prompt Tuning by leveraging the cross attention between the prompt and embedded tokens. Further the model uses weight sharing mechanism for better representation capacity of the cross attention. This work performs evaluation on 25 datasets for number of downstream tasks. PEFT fine-tuning can be adapter or prompt based. The adapter based methods generally outperforms the prompt based fine-tuning methods.  This paper also achieves results comparable to adapter based fine-tuning methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper well explores the shortcomings of Visual Prompt Tuning (VPT) to amend it in this work for visual tasks. 
2.  This work shows the validity on the image classification and segmentation tasks by benchmarking on VTAB-1K, FGVC and ADE20K.
3.  The ablation study in the cross-attention location is helpful.

Weaknesses:
1. The conclusion seems to more of an abstract. 
2. The implementation details can be described with more details.
3. Although the authors performed a great ablation on the cross-attention, an ablation for the self attention would have been interesting.
4. One of the base cases with null text can provide a better understanding for the effectiveness of this method.

Limitations:
In this paper, the authors discuss the limitations on the Section:5 Conclusion, where they mention about taking the same initialization strategy as VPT. VPT discusses different strategies on initialization for better optimization.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
BQEOJZ0aSD;"REVIEW 
Summary:
This paper presents a new method which directly encourages ensemble diversification on selected ID datapoints without the need for a separate OOD dataset. They also introduce a new measure of epistemic uncertainty which measures the diversity of the final predictions of each model, and suggest a speedup of comparing pairwise disagreement via random sampling.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written, and the presentation of the method is easy to understand. 
- The experiments cover a wide range of OOD datasets.
- The methods are intuitive and can be inexpensively applied to existing ensemble diversification algorithms.
- SED-A2D outperforms other baselines when using uniform soup or prediction ensembles for OOD generalization, and also achieves the highest AUROC for OOD detection.

Weaknesses:
- It appears that utilizing this new training objective leads to a loss in ID accuracy, since it encourages members of the ensembles to diverge. This tradeoff between ID accuracy and OOD accuracy may not be desirable in many settings. Overall, the paper emphasizes the improved OOD performance but does not show its impact on ID data for many experiments, such as the ablation studies for OOD detection, model diversity, etc.
- The stochastic computation of pairwise disagreement seems incremental, and there is no work comparing this stochastic implementation with the traditional expensive one. It would be helpful to include an ablation study to understand the accuracy vs performance tradeoff.
- There are many other methods for OOD detection beyond MSP with BMA (eg [1]). How PDS does compare against other baselines?
- Deep ensembles remain competitive in many settings, and the best values for C-1 and C-5 OOD generalization are still achieved using ensembles. 

[1] Xia and Bouganis: On the Usefulness of Deep Ensemble Diversity for Out-of-Distribution Detection https://arxiv.org/abs/2207.07517

Limitations:
- The approach sacrifices ID accuracy for OOD generalization/detection.
- The experiments only showed the result of finetuning the last two layers.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper aims to train a diverse ensemble of models via a framework called Scalable Ensemble Diversification. This framework does not require an additional dataset of OOD inputs, as it identifies OOD samples from a given ID dataset. It then encourages the ensemble to return diverse predictions (disagreement) on these OOD samples. Furthermore, the framework makes use of stochastic summation to speed up the disagreement computation. Results are shown for different tasks like generalisation, OOD detection on different OOD datasets.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The high level idea of removing the need for a separate OOD dataset and speeding up the diversification computation can be useful in practice.
- The writing is clear and easy to follow.

Weaknesses:
1. It is not clear why the method works, additional ablations studies would be useful.   
    - Naive A2D (and DivDis) uses IN-R data to compute the disagreement loss, which could give the method an advantage as it has access to the OOD data. However, it has a lower accuracy on IN-R. There seems to be two methodological differences between A2D and SED-A2D, the OOD data and use of stochastic sum. Given that A2D computes the full pairwise disagreement and stochastic sum is meant to reduce cost rather than improve performance, why does SED-A2D perform better? It would have been useful to compare two methods that only differs on the OOD data used. E.g., SED-A2D without the stochastic sum.   
    - From eqn 6, it looks like the two terms have contradicting objectives. For a “OOD” point, the first term encourages all models to classify the point correctly, but the second term encourages models to have different predictions on the same point. These objectives can be challenging to balance.   
2. The writing clearly explains the method or setup, but sometimes stops short of giving further insights. For example,  
    - Further analysis of experimental results   
        - Table 2 why does having more ensemble component (5→50) make the SED-A2D results worse? Similar trends can also be seen in Tab 4 for C-1 or C-5. Could it be because the stochastic sum does not scale with more models?   
        - Why was #unique used in Tab 1 to measure diversity when the Predictive Diversity Score was just introduced?  
        - Why does oracle selection perform worse compared to simple average in Tab 2? I would expect otherwise given that there is privilege information.   
    - Components of the method can be better motivated   
        - Why was the A2D loss chosen instead of other losses e.g. DivDis?   
        - Why is optimizing Eqn 6 preferable to e.g., forming an OOD dataset from the ID data based on the errors of DeiT or even from the errors from an ensemble of models, similar to imagnet-a, and using existing techniques like [23,28].   
    - “collecting a separate OOD dataset can be very costly, if not impossible”. 
        - There are cheap ways to introduce OOD samples to an ID dataset, e.g., simple augmentations/transformations to the input. Why are these methods not preferable?  
4. One of the main contributions involves speeding up the disagreement computation. There does not seem to be experimental details or results on this. E.g., a subset of models is chosen, what is the size of this subset? How does performance for generalization/detection change with and without this speedup?

Limitations:
Yes the limitations were adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents SED, a method for scaling up existing diversification methods to large-scale datasets and tasks. SED identifies the OOD-like samples from a single dataset, bypassing the need to prepare a separate OOD dataset. Experimental results demonstrated good performances by SED on the OOD generalization and detection tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. SED scales up existing ensemble methods.
2. According to the author's experimental results, SED demonstrates its application to OOD generalization and detection at ImageNet level.
3. Simple method, and easy to understand.

Weaknesses:
1. I am very confused about the dataset division for OOD detection task in the paper. The distribution should refer to “label distribution” in OOD detection [1], which means that OOD samples should not have overlapping labels w.r.t. training data. In the paper, the ID dataset is ImageNet-1K, while the OOD dataset for the OOD detection task includes ImageNet-C (Table 3). Their label spaces overlap, which is clearly incorrect. I don't believe the experiments conducted in this paper fall under the category of OOD detection. I suggest the authors refer to relevant literature on OOD detection, such as OpenOOD [1].

2. The ablation studies are insufficient. For instance, the number of layers being diversified is a hyperparameter. I believe conducting ablation experiments on this would make the paper more solid.

3. I think the experiments in the paper are not comprehensive enough. For example, how does it perform on small-scale datasets? Although it may not be fair to compare with methods using real OOD datasets, this could provide insights into SED's performance from multiple perspectives.

4. The comparative methods in the paper are not comprehensive enough. How does it perform compared to existing OOD generalization and OOD detection methods? If SED is complementary to existing methods, how much improvement can it bring?

5. The paper claims to speed up pairwise divergence computation, but no results are shown. Could authors demonstrate specifically how much speedup was achieved?

6. typos:
6.1 Line 61: ""We verify that SEDdiversifies a model...""
6.2 Line 68: ""In all three cases, SEDachieves a superior generalization...""

[1] Yang et al, OpenOOD: Benchmarking Generalized Out-of-Distribution Detection, IJCV 2024.

Limitations:
I think the author should discuss more about the limitations of the method proposed in the article, such as the computational time required for the method proposed in the article compared with other methods.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Ensembles of diverse models have shown promising signs for out-of-distribution (OOD) generalization.
To boost diversity, some methods require a set of OOD examples for measuring the disagreement among models.
The desired OOD examples, however, can be difficult to obtain in practice.
This paper proposes to dynamically draw OOD samples from the training data during training.
This is done by assigning a higher OOD score to examples with a greater loss in each mini-batch.
To make the diversification process across multiple models more efficient, the authors propose a stochastic approach that only diversifies a small sample of models at each iteration.
The resulting diversified models give rise to the notion of a diversity score for uncertainty estimation and can be used for OOD detection.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper introduces several reasonable improvements to a state-of-the-art method, A2D, making it more scalable and practically feasible.
- The empirical performance looks good. It is a bit surprising that the proposed method can outperform A2D which has access to “true” OOD datasets.

Weaknesses:
- The notion of “OOD samples” in an ID dataset is confusing. The actual implementation, i.e. assigning a higher “OOD-ness” weight to training examples with a greater loss, is more like identifying “hard” training examples rather than just OOD samples. Calling them “OOD samples” somewhat obfuscate their nature. They are not arbitrary OOD samples but hard samples within the support of the ID dataset. It is not obvious why diversifying models’ predictions on such samples would help. Is such prediction diversification always conducive to OOD generalization? If not, when would the proposed method work or break? These relevant theoretical questions are not answered satisfactorily in the current manuscript.
- The connection between SED and PDS is weak; PDS is not well justified. The A2D diversification loss can also be seen as a measure for prediction diversity, like PDS. Why choose PDS instead for OOD detection? Furthermore, is PDS really a good measure for epistemic uncertainty? Imagine two cases. In the first case, two models confidently (with probability 1) predict the same class for an input example, while in the second case, the two models assign uniform probability to all classes for another example. The PDS for these two examples are exactly the same, yet the models are much less confident (or more uncertain) in the second case. Meanwhile, BMA does not have this issue.
- The baselines are relatively limited. There are many other diversification methods which do not require a separate OOD dataset [1, 2, 3]. How does the proposed method compare with these methods? Can the authors also comment on why BMS is the only considered baseline for OOD detection?
- The definition of #unique values is not very clear. Table 1 shows SED-A2D has extremely large #unique values. On C-1 dataset, the value is 5, the maximum possible value. If my understanding is correct, does this suggest that all the 5 models disagree with each other on every C-1 example? If so, this suggests that for many examples, 4 out of 5 models are probably wrong. Why is this more of a good sign than a bad one?

[1] Rame, Alexandre, et al. ""Diverse weight averaging for out-of-distribution generalization."" Advances in Neural Information Processing Systems 35 (2022): 10821-10836.  
[2] Chu, Xu, et al. ""Dna: Domain generalization with diversified neural averaging."" International conference on machine learning. PMLR, 2022.  
[3] Lin, Yong, et al. ""Spurious feature diversification improves out-of-distribution generalization."" arXiv preprint arXiv:2309.17230 (2023).

Limitations:
The authors only briefly mentioned two limitations of the work. I don't notice any potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes Scalable Ensemble Diversification (SED) to extend existing diversification methods to large-scale datasets and tasks where ID-OOD separation may not be possible, and also propose Predictive Diversity Score (PDS) as a novel measure for epistemic uncertainty. Extensive analysis and experiments support the effectiveness of the proposed modules.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The logic of this paper is very clear, the motivation is reasonable, and the proposed method has been proven to be effective in analysis and experiments. The figures and tables in the paper are also relatively clear.

Weaknesses:
1. Although the experiments are diverse, I am not sure if the comparison is comprehensive. Can more explanation and discussion be added?

2. The feature extractor used is frozen. Is the proposed method robust enough to different feature extractors? What will the performance be if the feature extractor is also involved in the training?

Limitations:
The authors have addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
SeesCzBelI;"REVIEW 
Summary:
The authors considers methods for removing bias in RMs, specifically the bias towards long responses and the bias certain prompts might have to generate high rewards (this stems from the BardleyTerry model being underspecified.). For the second problem the authors proposed PBC which adds a linear layer to the last token of the prompt, the output of which predicts the average reward of completions from the prompt. For the first problem the authors propose to combine PBC with existing length bias correction methods which adds a correlation term to the loss. For experimental results the authors considers RLHF training LLama-7B on the RM-static dataset. They find that their method outperforms baselines on academic benchmarks (Table 2) and in head-to-head comparisons (Fig 4). They also consider hyperparameter stability and ablations in Fig 5.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Bias in RLHF can potentially have a large impact if addressed correctly.

Weaknesses:
1. Academic metrics like MMLU are not a good fit for RLHF. MT-bench is better.
2. There are no error bars, unclear how strong the signal is. 
3. The writing is rather handwavy at times, e.g. the motivation in section 3.1. is very qualitative.
4. The novelty is low.

Limitations:
na

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the prompt bias in RLHF, especially the reward modeling --- beyond the length bias that might exist.
Alleviating reward hacking is an important topic in RLHF, however, with the current paper, some details or contributions are not very clear. I'll elaborate in the following sections.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The problem studied is important. The illustrative figures are helpful.

Weaknesses:
Some notations do not make sense, for example, in Equation (5), averaging over y does not make sense. Would not it be better to average over the C rather than y.

The presentation of the problem itself is not yet clear to me. Although the authors keep using examples in the context to anchor their ideas (which I appreciate), it is still unclear what is the problem this work aims to solve. I like the general idea of Figure 1, however, what does the red color highlighting mean? This figure makes a good contrast between your RM and conventional RM, yet it fails to illustrate the problem your RM aims to solve.

The experimental results are not supportive enough.

Limitations:
Please see weakness

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces the Prompt Bias Calibration (PBC) method to address prompt-template bias in reward training of RLHF. The proposed PBC method is validated through extensive empirical results and mathematical analysis, showing its effectiveness in combination with existing length bias removal methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	Good Writing: The paper is well-written and easy to follow.
2.	Innovative Methodology: Introduces Prompt Bias Calibration (PBC) to address prompt-template bias in RLHF.
3.	Strong Empirical Evidence: Demonstrates significant performance improvements through comprehensive evaluations.

Weaknesses:
see questions

Limitations:
see the above

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the issue of reward hacking in RLHF training, superficially, identifying prompt-template bias, defined as when a reward model (RM) develops a preference for responses that adhere to specific formats or templates, even when these formats are not explicitly specified or desired in the prompt and proposes Prompt Bias Calibration (PBC) method that successfully tackles this issue. PBC can also be combined with existing length debiasing methods like ODIN to mitigate both hacks in the reward signal.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
* The paper identifies and analyzes ""prompt-template bias"" in RLHF, a potentially impactful issue.
* PBC is easy to implement and as shown can be combined with existing approaches.
* Strong empirical validation with good coverage in the experiments and ablation.

Weaknesses:
* Choosing one specific bias - while the title claims that removing the length bias is not enough, it seems to change the scene to potentially removing length and prompt-template bias not being enough. Leading to concerns about needing to combine many methods, one for each mitigation.

Limitations:
The limitations are covered.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
ks0FrTSCnK;"REVIEW 
Summary:
The paper extends the problem setting of learning with noisy labels (LNL) to include open-set noise, where noisy labels may come from unknown categories, in contrast to the traditional focus on closed-set noise. The authors theoretically compare the impacts of open-set and closed-set noise and analyze detection mechanisms based on prediction entropy. They construct two open-set noisy datasets, CIFAR100-O and ImageNet-O, and introduce an open-set test set for the WebVision benchmark to validate their findings. Their results show that open-set noise exhibits distinct characteristics from closed-set noise. The paper emphasizes the need for comprehensive evaluation methods for models in the presence of open-set noise, calling for further research in this area.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The research problem is interesting. Compared with learning with closed-set noise, learning with open-set noise is under-explored. 
- The theoretical analysis seems to be solid.

Weaknesses:
- Some technical details are hard to follow. Writing needs to be polished. 
- The contribution from the algorithm perspective is not enough.

Limitations:
N/A.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an approach to address the challenge of open-set noise in the context of learning from noisy labels. The authors propose a method that differentiates between 'easy' and 'hard' types of open-set noise, which is critical for improving the robustness and performance of learning models faced with noisy data. By integrating existing Learning with Noisy Labels (LNL) techniques with novel entropy-based noise detection mechanisms, the paper presents both theoretical insights and empirical validations of the proposed methods. The contributions are significant as they offer a refined perspective on handling different noise complexities, which can enhance the utility of machine learning models in real-world applications dealing with noisy labels.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Originality: The paper addresses the issue of open-set noise in learning from noisy labels with a novel approach, differentiating between 'easy' and 'hard' noise types. This nuanced consideration is original as it pushes the boundaries of how noise is typically treated in noisy label learning.

Quality: The theoretical explanations are thorough and complemented by robust empirical evidence that strengthens the methodological claims.

Clarity: The paper is well-structured, offering clear explanations of complex concepts, which aids in understanding the proposed methods and their implications.

Significance: The significance of this work is evident as it tackles a critical issue that can potentially enhance model robustness and performance in real-world scenarios where label noise is common.

Weaknesses:
Dependency on Specific Methods: The reliance on entropy-based techniques for noise distinction may not generalize across all scenarios or noise types.

Experimental Scope: The experiments primarily utilize synthetic datasets, which might not fully capture the complexity of real-world data applications.

Limitations:
Limited Experimental Scope: The experimental validation focuses predominantly on synthetic datasets like CIFAR100-O and ImageNet-O. While these are commonly used in the research community for benchmarking, the real-world implications of the findings might be limited without additional testing on more varied and real-world datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on open-set label noise problem. Authors first formally extending closed-set transition matrix to open-set transition matrix and define two noise ratios for open-set and closed-set separately. 
Then authors define error inflation rate as a measurement for noisy label impact and measure for two conditions, classifier fitted noisy distribution or memorized (overfit) noisy label. Later, authors propose a new type of open-set noise by exclusively transitioning outlier classes to a specific inlier class, and consider this as a ""hard"" open-set noise and traditional open-set noise as ""easy"" case. Authors further analysis two noise types on two classifier conditions and claim traditional entropy based open-set detection might only works on ""easy"" case. Experiments are performed on CIFAR-100, ImageNet and Webvision datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Authors formally define open-set noise with a similar symmetric/asymmetric setup as closed-set noise, and find out that it shows opposite trend with different classifier cases.

Weaknesses:
- The experiment parts lack of baselines. With a new type of noise proposed, previous baselines on easy open-set noise should be run to assess the performance gap and set up the benchmark.
- Figure 4 (a) and (b) have similar distribution, it is hard to draw conclusions from entropy dynamics.
- Supp E.1 results are confusing. ""X+EntSel"" should be a better strategy since it selects inlier clean samples. However, why the closed-set classification accuracy is always the worst? Table 1 Webvision result is similar as well. Why is the claim ""EntSel + SSR improves open-set detection performance"" valid? The Acc and AUC are both dropping after adding EntSel. Why SSR/DivideMix + EntSel is always the worst performance? Considering it is a combination of inlier and clean, shouldn't it be the best performing one? I assume this is still the normal accuracy and AUC, which is the higher the better.

Limitations:
Authors do not address any limitations in conclusion. A possible limitation might be related to approximation of fitted and memorized classifiers.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper refines the problem of learning with noisy labels (LNL) by addressing the often overlooked issue of open-set noise. It provides a comprehensive theoretical analysis comparing the impacts of open-set and closed-set noise, introduces novel datasets for empirical validation, and explores the effectiveness of entropy-based noise detection mechanisms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper offers a thorough theoretical analysis of the differences between open-set and closed-set noise, extending the current understanding of LNL.
- The exploration of entropy-based mechanisms for detecting open-set noise adds a practical tool for improving LNL methods.
- This paper is well-written and easy to understand.

Weaknesses:
- The author summarizes two types of open-set noise, i.e., the easy and the hard noise, which is very similar to the symmetric and asymmetric label noise from the perspective of the transition matrix. So does there exist the instance-dependent open-set noise? What is its form if exists?

- In Section 3.5, the author conducts analyses regarding entropy dynamics-based open-set detection, which belongs to the **Fitted case**. If adopting the vision language model (such as CLIP) to fine-tune and detect the open-set noise, is it aligned with the **Memorized case**? It would be better for the author to provide a real-world application for the memorized case.

- The author should clearly illustrate the construct method of closed-set in the experiment (Figure 3) for reproducibility.

Limitations:
See weaknesses

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
unMRtFfNhF;"REVIEW 
Summary:
The paper studies the computational complexity of data debugging, defined as finding a subset of the training data such that the model obtained by retraining on this subset has better accuracy. The paper focuses on linear models and investigates various loss functions, showing that in some cases, the debugging problem is NP-complete, while in others, it can be solved in linear time.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper analyzes the computational complexity of linear classifiers with both fixed and non-fixed loss functions. First, it shows that in the general case of non-fixed loss functions and dimensions, the problem of debugging is NP-hard. However, for hinge-loss-like functions, depending on the dimension of the features and the sign of the intercept, the problem could be either NP-hard or solvable in linear time.

This result also implies that it is not accurate to estimate the impact of a subset of training data by summing up the scores of each training sample in the subset if we assign each sample point a scoring number.

Weaknesses:
- The model studied in the paper is limited to linear classifiers, which is very restrictive.

- Most of the manuscript is devoted to proving the theorems rather than discussing and interpreting the implications of the results.

- The setting involves debugging for any possible test point, which is far from practical.

Limitations:
The paper does not have any potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work investigates the computational complexity of the data debugging problem, i.e. the problem of identifying a subset of training data that, when removed, improves model accuracy on a given test point. Via standard complexity theoretic reductions, it establishes the NP-hardness of this problem for linear classifiers trained with SGD under various conditions.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well written with the key problem being well motivated. The proofs are succinct and easy to follow.

Weaknesses:
My key concern with the paper is that the constructions underlying the hardness results are immensely contrived and far removed from how classifiers are actually trained in practice. This casts severe doubts as to whether the key results of the paper implies anything significant about the hardness of data debugging as is performed in practice. To elucidate a few instances:

1. **Theorem 3.1**: The reduction to Monotone 1-in-3 SAT in Theorem 1 hinges on an adversarially constructed loss function (Line 186), one that is very far removed from any loss function I’ve seen used either in the learning theory literature or in practice.
Beyond this, it also requires a specific parameter initialization and learning rate, both of which, to my knowledge, are far removed from the typical random initialization schemes and learning rate scaling rules used in practice and analyzed in theory. For instance, in Line 198, the first $m$ co-ordinates of the learning rate $\eta$ is set as $5$, the next $n$ are set as $\frac{1}{6N}$ while the next $m$ are set as $2000N$. This seems very very removed from any learning rate schedule either used in practice or analyzed in theory. This makes me severely doubt whether the result has any meaningful implication on the inherent hardness of the data debugging problem.\
In addition, the training set and test data point are also constructed adversarially (the latter is perhaps this is to be expected I wouldn’t perceive that in isolation as a major weakness)

2. **GTA Algorithm**: The correctness of the GTA algorithm is proved only for the linear case (which is honestly quite straightforward) and hinge-like losses for $\beta \geq 0$ and dimension $d=1$ (which is of limited interest as most statistical learning problems are high dimensional). This is particularly concerning as the paper does not perform any empirical evaluation of GTA. 

3. **Theorem 4.3** While the analysis for the hinge loss is certainly more interesting than Theorem 3.1, the result suffers from a key weakness that the data ordering is adversarially chosen (In particular, the positioning of $(x_c, y_c), (x_b, y_b), (x_a, y_a)$ is crucial to the reduction). It is well known in the theory of optimization that adversarial data orderings lead to provably worse convergence in practice [Safran and Shamir COLT 2020; Das, Scholkopf and Muehlebach NeurIPS 2022]. In fact, adversarial data ordering is even the basis of an attack on deep neural networks [see Shumailov et. al. “Manipulating SGD with Data Ordering Attacks” NeurIPS 2021].\
The adversarial data ordering considered in the result differs both from the practical implementation of SGD which samples the data points in each epoch as per some random permutation [see Ahn and Sra NeurIPS 2020 and references therein] or the canonical version of SGD considered in theory where data indices are sampled uniformly with replacement [Bubeck 2015]. To the best of my understanding, the result does not hold for either of these commonly considered variants of SGD. Furthermore, the training data and test data is again, adversarially chosen. 

4. **No Analysis for Cross Entropy** : The paper does not contain any analysis for the binary entropy loss which is what is commonly used to train classifiers, further limiting the scope of the results. 

5. **Complete Lack of Empirical Evaluation**: While this wouldn’t be a weakness by itself, the fact that the theoretical results in the paper have limited applicability (as argued above) makes me quite concerned about the absence of empirical evaluation on real world settings, or, for a start, even toy-like settings where the data is drawn from plausible distributions, the learning rates and parameters are initialized as one would normally expect them to be, and SGD is run either with replacement or without replacement and not with an adversarial data ordering. 

While the paper studies a question which I found interesting, I believe the limited applicability of the theoretical results, the absence of experimental evaluation as well as the limited technical novelty of the proofs (the proofs, although crisply written, are based on straightforward complexity theoretic reductions and do not unearth any nontrivial mathematical insights regarding SGD or linear classifiers) makes me confidently feel that the paper is currently not ready for acceptance at NeurIPS.

Limitations:
Mentioned in Section C as per the checklist. Also refer to comments above in Weakness Section.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the computational task (coined as ""data debugging"") of finding training data subsets that yield different test point predictions. The focus of the paper is the SGD learning algorithm with linear classifiers. The paper shows that:
 - For general loss functions (to transform yw^Tx to a loss value), the data debugging task is NP-hard. 
 - For linear loss functions, the data debugging task can be solved in linear time.
 - For ""hinge-like"" loss functions, the data debugging task with more than two features is NP-hard.

Soundness:
4: excellent

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The proofs look correct
- The proofs are creative and clever
- Generally speaking, understanding the relationship between training data and model predictions is important

Weaknesses:
Clarity and exposition:
- Many of the proofs have ""magic constants"" which makes them harder to understand. The authors may consider generalizing some of the results.
- The proofs look correct, but are not optimized for being easily read and understood in terms of logical flow or notation.
- Similarly, it is hard to extract the intuition from the proofs.
- The implications of results for the wider community are somewhat muddled: while the computational hardness of exactly finding ""bad"" training points according to a particular definition is interesting, it's unclear what the relationship is to scoring based methods is since any selection algorithm could be identified with a scoring method (output 1 on selected points, and -1 on the rest), and ""CSP-solvers"" and ""random algorithms"" are mentioned without any particular explanation/exploration.

Significance:
- The loss function used to prove Theorem 3.1 is rather pathological. The derivative is zero at most places, but has a few intervals with derivatives with wildly varying orders of magnitude: N, 1, and 1/N. Reading through the proof, this pathological loss function seems critical and not easily removed. I would find it much more significant if Theorem 3.1 could be proved for convex loss functions, or similar.
- The assumption of an adversarial training order in Theorem 4.3 seems unreasonably restrictive. The user is changing the dataset by removing points, why can't they change the training order too? Again, this piece seems critical to the proof, since without the adversarial training order, I think the constructed data debugging task would be trivially solvable by taking a gradient descent step on (x_c,y_c) first. Can the theorem be extended to the user choosing the training order? Furthermore, the practical problem of data selection/cleaning is with regards to the presence/absence of datapoints, not the training order which is an optimization consideration. Can the theorem be proved not for a specific optimization algorithm like SGD, but for the minimizer(s) of the loss instead? A unique minimizer (for the hinge-loss and convex losses more generally) could be guaranteed by adding a small strictly convex regularization term.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
ZJZqO4grws;"REVIEW 
Summary:
This study presents a contrastive regularizer to improve meta-learning. Specifically, the authors propose to incorporate a contrastive meta-objective that improves the alignment and the discrimination abilities of meta-learners, leading to better task adaptation and generalization. The authors demonstrate empirical effectiveness of the proposed ConML across several meta-learning and in-context learning scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The introduction of contrastive regularization sounds intuitively straightforward and motivating. 
2. While equipping meta-learning with contrastive objective is not a new concept (e.g., [1], [2]) the implementation covers major meta-learning methods, including optimization- metric- and amortization-based methods. This means that the study is more comprehensive than previous studies.
3. The numerical results are promising. Code is provided - reproducibility is commendable.


[1] Gondal et al. Function Contrastive Learning of Transferable Meta-Representations. In ICML 2021.
[2] Mathieu et al. On Contrastive Representations of Stochastic Processes. In NeurIPS 2021.

Weaknesses:
My primary concern lies in the specific contrastive strategy employed.

1. The contrastive objective aims to minimize intra-task distances while maximizing inter-task distances. However, the absence of appropriate regularization or constraints raises concerns about potential model representation collapse. This collapse could manifest as representations converging to trivial solutions, such as constant vectors or confinement to low-dimensional subspaces. The authors should address whether they have considered these risks. 
2. In addition, I wonder why the contrastive objective does not follow commonly studied ones, e.g., InfoNCE, in contrastive learning.
3. Moreover, the meta-objective necessitates computations involving representations from different tasks within a batch during each episode. Since the paper lacks a discussion on training and inference efficiency, the impact of this strategy on scalability is unclear.
4. The effectiveness of this method is likely dependent on hyperparameters tuning and the sampling strategy for creating subsets of tasks. As aforementioned, incorporating contrastive learning into meta-learning is not something completely new, even though, the authors do not include detailed discussion on how different strategies would affect the performance/efficiency, which is something to be expected on my end.

Limitations:
The authors do have discussed the limitation of this study.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to enhance the meta-learning process by implementing more robust supervision within the model space. Specifically, the authors seek to augment learning capabilities through model alignment and discrimination, aiming to approximate human-like rapid learning abilities. They propose that models trained on tasks within the same super-task exhibit similarity, while those trained on different task sets generalize effectively across diverse tasks. To achieve this, the authors devise a contrastive framework that remains independent of the specific meta-learning algorithms employed. This framework encourages closer alignment of representations for models adapted to similar tasks while pushing representations apart for models derived from dissimilar tasks. Moreover, the framework seamlessly integrates with optimization-based, metric-based, and amortization-based meta-learning methods. The approach demonstrates improvements across standard benchmarks in all evaluated scenarios. Furthermore, the proposed method shows promise for integration into the in-context learning of large language models, resulting in observed enhancements.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
•  The paper is well-written and easy to understand. It's reasonable to enforce the model to emulate human learning capabilities through alignment and discrimination.

•  The proposed contrastive learning framework is versatile and applicable to most meta-learning methods.

•  The proposed method consistently improves upon existing meta-learning methods across standard benchmarks.

Weaknesses:
•  The paper lacks validation on MetaDataset[A], which is a common large-scale dataset for few-shot learning tasks.

•  There is a need for sensitivity analysis on certain hyperparameters, such as λ and the choice of similarity function for contrastive learning.

[A] Meta-Dataset: A Dataset of Datasets for Learning to Learn from Few Examples, ICLR2020.

Limitations:
Please see weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper deals learning to learn (meta-learning) problem, from the perspective of exploring inner-task and intra-task relationship. Specifically, this paper proposed a Contrastive meta-objective by exploring intra- and inter-task distances and severed as an additional term for training objective (in addtion to classification loss). 

Experiments are conducted on both conventional few-shot image classification and in-context learning settings. Common benchmarks are used to compare with simple few-shot methods such as MAML, ProtoNet, SCNAPs. For In-context learning, simple synthetic functions are used for comparison.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The overall method makes sense. Designing intra- and inter-distance to explore the task-level contrastive information and further introduce this into to the meta-learning objective is reasonable. The diverse tasks naturally compose the contrastive pairs, useful for training. 
- The proposed method is general and can be applied on top of different few-shot classification/regression methods, such as metric-based, optimisation-based, simpleCNAPs, and in-context learning. 
- The performance gains over these simple baselines are significant, showing the effectiveness of the proposed method.

Weaknesses:
- Technically, the proposed contrastive meta-objective is similar to the idea of supervised contrastive learning, which already provides good insights to the representation learning and deep learning community. Therefore, the proposed method is kind of incremental and provides less new knowledge to the field. 
- The method is only verified on top of simplest baseline methods (MAML, ProtoNet, etc). In meta-learning, various works have been proposed to investigate the possible exploration of the task-level information for improved meta-learning, such as [R1-R4], to name a few. 
However, none of those previous efforts were discussed or compared. Only beating the naive baseline cannot comprehensively demonstrate the advantages of this paper. 
- Experimentally, the proposed method tries to show its superior performance over simple baselines rather than SOTA. This is less convincing. 
- The in-context learning experiments are only on simple synthetic data, lack of significance. 
- The tile and scope: Learning-to-learn is very general, but in fact only classification related experiments are conducted. By convention, the learning-to-learn approaches will also verify on reinforcement learning. 

[R1] Fei, N., Lu, Z., Xiang, T., & Huang, S. (2021). MELR: Meta-learning via modeling episode-level relationships for few-shot learning. In International Conference on Learning Representations.   
[R2] Agarwal, P., & Singh, S. (2023). Exploring intra-task relations to improve meta-learning algorithms. arXiv preprint arXiv:2312.16612.   
[R3] Han, J., Cheng, B., & Lu, W. (2021). Exploring task difficulty for few-shot relation extraction. arXiv preprint arXiv:2109.05473.   
[R4] Zhang, Tao. ""Episodic-free Task Selection for Few-shot Learning."" arXiv preprint arXiv:2402.00092 (2024).

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a contrastive meta-objective that can be applied to various meta-learning methods. Also, interpreting in-context learning as a meta-learning formulation, extended the proposed method to in-context learning. Specifically, the objective is to contrast task identity obtained after episode optimization. The task identity is defined as the model weight or the feature obtained by feed-forwarding, in the case of in-context learning. Finally, the authors demonstrated the superiority of the proposed method by applying it to several meta-learning methods to improve their performance.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The proposed method can improve diverse meta-learning methods.
- Contrasting task identity sounds intuitive.

Weaknesses:
In general, the text is not easy to understand.

- Not self-stained figures.
  - In Figure 1, it's hard to understand what $h_{w_i}$ and $w_i$ are since the caption has no explanation.
  - In Table 1, the caption could have included the definition of $g$ or $\psi$.
  - Figure 2 is hard to read; (b) and (e) missed the x, y-axis meaning and are too small to see something.
- Experiment details are missing.
  - Hard to understand Section 5.1. Though the sine wave regression problem is well-known in this domain, it's hard to interpret results without task definitions.
  - The tasks in Section 5.3 are not clearly defined.
- An ablation study would be helpful.
  - How to decide distance function $\phi$?
  - What if increasing the number of task-sampling $K$?

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
nzzxLPJENZ;"REVIEW 
Summary:
In this work, the authors conceptualize the evaluation process as a decision tree, where each node represents an evaluation action, and each path from the root to a leaf node represents a trajectory of evaluation reasoning. The authors demonstrate that within a limited search space, there exist better decision-making behaviors that facilitate the model in making reasonable and accurate judgments.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The idea of branching LLM evaluation is interesting and novel.

Weaknesses:
1. The authors missed a lot of key related works, including close-ended benchmarks such as MMLU, MMLU-pro, MixEval, GSM8k, GSM1k, etc; open-ended benchmarks such as Arena-Hard, AlpacaEval, WildBench, Chatbot Arena, etc.
2. I think the writing needs improvement. Now it's not easy for a reader to get what you are focusing on. If you are doing evaluation, then try to use some pipeline figures and comprehensive captions to describe the core idea. Besides, all captions in this paper is misleading, not telling the reader about what is happening in the table or figure; also, there lacks some key sections such as conclusion.
3. How to measure the quality of the proposed evaluation? I think just evaluating 5-6 models is far from enough. Beyond that, how is the model rankings related with Chatbot Arena or some other popular benchmarks such as MMLU?

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a novel approach to efficiently evaluate LLMs using branching preference learning. The authors conceptualize the evaluation process as a decision tree, where each path represents an evaluation reasoning trajectory. They introduce a tree-based data sampling method and preference learning based on the DPO algorithm to improve evaluation capabilities. The method is tested in three settings: in-distribution, out-of-distribution, and transfer evaluation. The authors claim their model significantly reduces dependency on labeled data and demonstrates strong performance across different evaluation settings while reducing inference costs by 90% compared to searching the entire evaluation tree.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper's novel approach of framing LLM evaluation as a decision tree problem is a significant strength. This allows for a more nuanced and flexible evaluation process that can adapt to different scenarios and criteria. The use of branching preference learning enables the model to prioritize critical evaluation criteria.
- The authors test their model in multiple settings (in-distribution, out-of-distribution, and transfer evaluation), providing a thorough assessment of its performance. Applaud to that.

Weaknesses:
- The biggest concern I have is that in-distribution performance is not better than other baselines it compares to. This begs the question of where the improvement gain is from. If in-distribution evaluation performance is mediocre but out-of-distribution does better, then doesn't the most gain come from a better dataset? 
- Another concern is the unnaturalness of using evaluation criteria as individual nodes. How to ensure the coverage of those criteria across different nodes. Are they overlapping each other or completely different? The paper is quite vague on this.
- Why is each criteria subtree only a binary tree? If using a tree structure, it seem like it can be easily extend to multiple nodes rather than just 2 at each layer.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
They present an approach to improving LM evaluation by having models first generate an evaluation criteria, then a scoring guideline, and then finally a final judgement. They then develop a procedure for collecting training data corresponding to these three steps by applying branching/pruning approach (sample multiple criteria, from each sample multiple guidelines, etc...). They then use the generated data to train a DPO and SFT model. They find that their method outperforms baseline evaluation approaches according to correlation with human judgement on dialogue evaluation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The problem of improving LM evaluation is important
* The idea of enabling language models to hierarchically sample evaluations (e.g. fist criteria, then guideline, then judgement) is very neat, and similarly the idea of applying a tree-based sampling procedure to automatically generate data is quite cool.
* I think they do fairly thorough experiments and compare to quite a few baselines.

Weaknesses:
* The paper is honestly pretty hard to follow. There's a lot of moving parts and it's not explained in an easy to digest way.
* The specific method presented seems a little bit ad-hoc, and could be justified better in the paper (e.g. why use criteria, then guideline, then judgement, why not some other sequence of steps?).
* Looking at Figure 1, it doesn't seem that their method improves all that much over the baseline

Nits:
* The related work seems pretty sparse. There's lots of work on improving LM evaluation in math reasoning settings that isn't discussed.
* Figure 4, the text is really small and hard to read.

Limitations:
They do a good job of discussing the limitations. I would also note that it is unclear how effective this is when applied to more challenging tasks like mathematical reasoning (e.g. MATH benchmark) as a limitation.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates how to improve the quality of automated evaluation through fine-tuning (SFT and DPO). The main algorithm proposed by the paper is to construct an search tree which consists of node of (criterion, scoring guide, and judgment). This tree is later pruned and modified and the different paths serve as fine-tuning data for SFT and DPO.

My current rating is tentative. If the authors can kindly clarify the details of the paper, I'm happy to raise the score.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is very clear and easy to read.
2. The investigation is very thorough. Experiment is comprehensive (the in-distribution, out-of-distribution evaluation setup is great).
3. The main claim of the paper is substantiated (I.e., improving efficiency through fine-tuning).

Weaknesses:
I don't think this paper has substantial weaknesses.

1. There are some imperfections of text -- mostly just need to be clarified. Missing notation definitions, etc. 
2. The performance improve over Auto-J on AGR is minor (55.13 -> 57.18). OOD evaluation, Zephyr-7B AGR is 56.75 and GPT-4 is 62.28 (which is close, but not quite close). CNS however is beating GPT-4. This would be very helpful for me to understand a bit more about what CNS is, and whether beating GPT-4 on this metric is meaningful or not (see Q6).

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this work, the authors propose a tree-based data sampling method to conceptualize the evaluation process as a decision tree, where each node represents an evaluation action, and each path from the root to a leaf node represents a trajectory of evaluation reasoning. The proposed method involves generating supervised data and preference pairs derived from the evaluation tree for SFT and DPO training. This approach aims to reduce the dependency on labeled data and improve the performance of the evaluation model across in-distribution, out-of-distribution, and transfer evaluation settings. Experimental results demonstrate that the proposed model can enhance evaluation efficiency and performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method reduces the dependency on human labeled data by generating supervised data and preference pairs from the evaluation tree.
2. The paper is well-written.

Weaknesses:
1. Potential Biases --- The initial multi-branch training data is generated using only GPT4, which could introduce bias to the training data. Moreover, the branch ensemble method could also introduce bias to the training data. If the training data is biased or unrepresentative, the model's evaluations may also be biased. The authors should consider labeling a small annotation set to validate the branch ensemble approach.

Limitations:
Yes, limitation discussed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
oAmHzy8btj;"REVIEW 
Summary:
This paper considers the graph matching problem, where the goal is to produce a mapping between vertices of multiple graphs which maximizes similarities among them. The authors study graph matching from a theoretical perspective, in which one observes multiple (appropriately correlated) Erdös-Rényi (ER) graphs that have ground-truth latent mappings between them. The authors' goal is to characterize the information-theoretic threshold for exactly recovering the latent mappings between all of the observed ER graphs. Prior work has settled the information-theoretic thresholds for 2 correlated ER graphs, and this paper settles it for more than 2 ER graphs. 

To determine the information-theoretic threshold for exact graph matching, the authors establish matching achievability and converse results. The converse is based on a simple reduction to a graph matching problem with two ER graphs, combined with known results on impossibility results for exactly matching two ER graphs. For the achievability results, two algorithms are discussed. The first is the MLE, which is optimal for exact graph matching. The authors show that it has a clean, easy-to-understand form: the MLE outputs vertex mappings which maximize the number of edges in the corresponding union graph. However, the authors do not directly analyze this algorithm due to technical complexities. Instead, they propose an algorithm which involves two phases. (1) For each pair of graphs, a partial, fully-correct mapping is computed via the $k$-core estimator, and (2) unmatched vertices are matched through a ""transitive closure"" procedure. This algorithm provably outputs the full, correct set of vertex mappings in the parameter region that complements the converse. 

Finally, a few numerical experiments are presented, showing that the transitive closure procedure can be combined with known computationally efficient algorithms for pairwise graph matching to derive algorithms for matching multiple graphs in a principled manner.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
Almost all the existing theoretical work on graph matching concerns two graphs, except for some trivial results (to the best of my knowledge). The extension of the theoretical framework to multiple graphs is a natural and important follow-up, and may inspire several future works as well. 

While the algorithms and analysis are largely adapted from prior work (e.g., the $k$-core estimator), a key novelty is the transitive closure step, which provides a principled (and optimal!) bridge between pairwise graph matching and $m$-ary graph matching. As the authors highlight, this step can be used to extend practical algorithms for pairwise matching to the $m$-ary case in a black-box manner. I imagine that this technique could be useful in practice. 

Additionally, the paper is well-written.

Weaknesses:
To me, the main weakness of the paper is in the discussion of transitive closure's implications. The authors make a striking observation that one can use their transitive closure technique (at least heuristically) to generalize pairwise graph matching to $m$-ary graph matching. However, several details are lacking in the simulations section. For instance, what are the graph parameters ($n$ and $p$)? What is the error rate before and after the transitive closure boosting? How do the results shown compare to the accuracy of Algorithm 2? (Even though the $k$-core matching is not efficient to compute, the result of the matching procedure is a function of the ground-truth permutations, so I believe the algorithm's accuracy can be simulated efficiently). 

There are a couple other minor weaknesses. One is that Algorithm 2 is computationally inefficient. However, making such an algorithm efficient is likely a challenging research question itself, and is appropriate for future work. Another weakness is that there is no nice figure to visualize Algorithm 2. I feel that the reader's understanding could be greatly improved if one could create a representative figure for the transitive closure boosting.

Limitations:
Limitations have been largely discussed. The authors could expand upon implications of graph matching to protecting / breaking privacy in anonymized social networks.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper studies the information theoretic limits for matching
multiple correlated random graphs. Based on a correlated Erdos-Renyi
random graph model, the authors provide both lower bound and achievable
bound for the condition to correctly match all nodes with high
probability. These bounds match each other. A highly interesting insight
is that, even when exactly matching two graphs is not possible, the
proposed algorithm can leverage more than two graphs to produce exact
matching among all the graphs. The achievable algorithm exploits the
transitivity among partial matchings through $k$-cores, which is also
quite interesting.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The novelty of the paper is high in dealing with graph matching among
multiple correlated graphs. 

2. The necessary and sufficient conditions for exact matching meet each
other. 

3. The proposed algorithm can exploit transitivity to match all graphs,
even when any two graphs alone cannot be exactly matched. This is a very
insightful result.

Weaknesses:
1. The proposed algorithms do incur high complexity.

Limitations:
Limitations are discussed in Section 5.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This theoretical paper gives tight conditions for exact graph matching with multiple correlated random graphs. This problem has been extensively studied recently for the case of 2 graphs, and it is shown here that with more than 2 graphs, there is a regime where pairwise alignment is not possible, but with the information provided by all graphs, it is possible to align all of them. This is a nice theoretical result.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This paper studies a natural extension of a well-studied problem from 2 graphs to more graphs and shows a surprising effect: making partial pairwise matching is sufficient to get the exact recovery. The proof outlines give the main insights into the technical proof.

Weaknesses:
The resulting algorithm is not practical as it does not run in polynomial time (as it is mentioned by the authors).

Limitations:
The authors are very clear with the limitations of their work in section 5.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to find out alignments between G_1 and G_2,....G_m, under the assumptions that they all are essentially sampled from ER graph distribution. The paper presents one impossibility result (or necessary condition to estimate such alignment)  and two sufficiency results to solve the underlying problem.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper tackles an interesting problem and it is written clearly.

Weaknesses:
(1) I am not too confident that the paper is appropriate for neurips audience. I think the paper suits better to a conference like ISIT or such.  The paper has barely any learning component and the practical utility is not very clear.
Also, the primary area assigned by the authors ""Probabilistic methods (for example: variational inference, Gaussian processes)"" is probably not correct.

(2) The paper only tackles a very simple graph model (ER graph model). While I understand that theoretical analysis for complex graph model is difficult, I would recommend the authors should discuss that in comprehensive manner. To elaborate concretely,
suppose,  G_1, G' _2...,G' _m are *not* generated from an ER model. But G_2,...G_m are generated using an ER like model with constant edge deletion probability $s$. In such case, can one characterize the necessary and sufficient condition.

Note that, the area is not too new in the literature. There has been work already in this line of research [CK17,WXY22 in the paper]. Although I will not say this work is an extension but the theoretical contribution given the existing works is not very interesting (m=2 to an arbitrary m for example). 

(3) There is no experimental analysis. I would have increased my rating if the authors have done a thorough study on implications (including limitations) of their work on graphs from other models. For example, if we apply the same algorithm in other graph models, how would it perform. Since the line of work is not new, I would not say the theoretical results have strong enough impact to ignore the poor experiments.

Limitations:
Restrictive graph model; poor experiments and incremental contribution.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
QeebTNgRjn;"REVIEW 
Summary:
The authors introduce the conditional Lagrangian Wasserstein flow method for time series imputation.
The time series imputation task is treated as a conditional data generation problem. The authors use flow matching to learn an ODE sampler for generating the missing time series data. They further propose to enhance the imputation performance via Rao-Blackwellization. The method is tested on the real-world datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1. The approach of using Lagrangian Wasserstein flow for time series imputation is interesting and novel.

S2. Compared to other diffusion model-based time series imputation methods, this method seems more efficient.

S3. The experimental results given in the paper are satisfactory.

Weaknesses:
W1. The imputation process is implemented using an ODE sampler. Why not use an SDE sampler?

W2. Why use the Euler method instead of other higher-order solvers, such as the Runge-Kutta methods, for solving the ODE?

W3. Can the proposed method also be applied to time series forecasting tasks?

W4. The dynamic described in Eq (15) is deterministic, which is inconsistent with Eq (1). Why the diffusion term is missing in Eq (15)?

Limitations:
CRPS is not used as the evaluation metric in the experiments.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors introduce a new time series imputation model based on the conditional Lagrangian Wasserstein flow. Different from previous diffusion-based models, the proposed model leverages the optimal transport theory and Lagrangian dynamics to improve the data generation performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The overall presentation of paper is clear. 
2. The idea of using the conditional Lagrangian Wasserstein flow for time series imputation is novel.
3. The proposed method is assessed on the real-world datasets and shows competitive performance compared to the state-of-the-art methods.

Weaknesses:
1. In Sec 3.2, the technique used for projecting the interpolants into the Wasserstein space is unclear. The authors should elaborate more on this.
2. Compared to CSDI and other diffusion-based imputation methods, the proposed method’s data generation process seems deterministic. Hence, it cannot be used to quantify the uncertainty of the prediction.
3. The proposed sampler is implemented through an ODE, which is inconsistent with the Schrodinger Bridge problem in sec 2.3.
4. There are some inconsistencies in the notations, please double-check.

Limitations:
The ablation study on the effectiveness of Rao-Blackwellization should be conducted on more than one dataset (PM 2.5).

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Inspired by optimal transport and Lagrangian dynamics, this work proposes to use the conditional Lagrangian Wasserstein flow to impute time series data. The method requires less model evaluation steps to generate high quality samples compared to existing diffusion models. Moreover, a task-specific energy function is used to further improve the model’s performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The contributions of this paper are significant.
- This work is built on a solid theoretical foundation.
- The experimental results and relevant ablation study findings are provided.

Weaknesses:
- How should the values of the hyperparameters (e.g., the variance of the potential function) be chosen? How will these choice affect the model’s performance?
- The reason for choosing the VAE model to construct the potential function is unclear. Are there other functions can also be used as the task-specific potential function?
- The authors proposed to use a VAE model to enhance the data sampling procedure. However, if the VAE model performs poorly, can it still help with the data generation process?

Limitations:
The deterministic sampling process may introduce bias, potentially failing to accurately reflect the target marginal distribution.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work trains a conditional flow model from noise to time series data. A VAE is used to estimate the data density then perform interleaved flow and density gradient ascent steps to generate new time series. This is referred to as a Rao-Blackwellization procedure. It is shown empirically that the model performs well on time series imputation tasks across two datasets, and that the VAE-based guidance helps in one case.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
* Originality: Time series imputation is an important problem and I have not seen any flow matching works in towards this application. Presents a new improvement to the standard flow matching framework with a “Rao-Blackwellization”.

Weaknesses:
- Quality: Experiments are limited in scope and the it is not clear whether the “Rao-Blackwellization” (the main methodological novelty) step reliably improves performance.
    - In order to claim “Less sampling steps” it would be great to understand how performance changes with the number of steps for both diffusion and flow-based models.
    - More than 2 time series datasets would allow an understanding of when this “Rao-Blackwellized” sampler is valid.
    - No error bars (even though this is claimed in the checklist)
    - The empirical support is lacking for the effectiveness of the “Rao-Blackwellization” step. Currently it is shown to slightly benefit (although without error bars this is difficult to tell how much) on a single dataset. In this case, an improvement in RMSE from 18.2 to 18.1 on a single dataset seems insignificant without additional detail. While it is claimed that “the PhysioNet dataset does not have enough non-zero datapoints to train a valid VAE model”, a Rao-Blackwellized sampler should always work regardless of the performance of the estimate? It would be good to see if it helps at all on this dataset and others.
- Clarity: It is not clear to me that this even is a “Rao-Blackwellized” sampler. As far as I can tell this “Rao-Blackwellized” sampler is fundamentally biased (while the original sampler is not), and it therefore cannot possibly be a Rao-Blackwellized sampler. It would be great if the authors could prove that Algorithm 2 is really a Rao-Blackwellization (at least under some conditions).
- Significance: Without further experimental benchmarking especially with regards to the novel sampler, the applicability of this method is very limited.

Limitations:
I'm a bit confused about the limitations. It is claimed that 

> One limitation of CLWF is that the samples obtained are not diverse enough as we use ODE for
> inference, which results in slightly higher test (continuous ranked probability score) CRPS compared
> to previous works, e.g., CSDI. Therefore, for future work, we will seek suitable approaches to
> accurately model the diffusion term in the SDE.

Other ODE models have had no problem with diversity. I would suggest that this is actually due to performing gradient ascent on the density of the VAE.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents Conditional Lagrangian Wasserstein Flow (CLWF), a new method for time series imputation. Using (entropic) optimal transport theory and Lagrangian mechanics, CLWF generates high-quality samples. Enhanced with a Rao-Blackwellized sampler, CLWF incorporates prior information through a variational autoencoder. Experiments on real-world datasets show that CLWF performs competitively.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written, clear, and easy-to-follow.
2. The use of a Rao-Blackwellized sampler and a variational autoencoder to integrate prior information enhances the model's performance, providing a more robust imputation process.

Weaknesses:
1. The conditional generation process is trained through the simulation-free training of the Schrödinger Bridge, which limits the novelty of the CLWF.
2. The paper mentions that the samples obtained using the ODE-based inference method may lack diversity, potentially leading to higher continuous ranked probability scores (CRPS) compared to some previous works.
3. While the method is tested on two real-world datasets, broader evaluation across more diverse and challenging datasets could strengthen the validation of the approach.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
s5Y3M5l1qg;"REVIEW 
Summary:
To better defend against adversarial attacks, the paper proposes a novel adversarial defense mechanism for image classification – CARSO – blending the paradigms of adversarial training and adversarial purification in a synergistic robustness-enhancing way.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper proposes a novel defense mechanism.

The proposed method is validated on multiple datasets.

Weaknesses:
1. The presentation of the paper is poor.

    a) In the first half of the paper, the author merely describes some background. There is a lack of analysis of existing methods, such as the shortcomings of the current methods, what problems the proposed method can solve, and why it can solve these problems.

    b) Some descriptions are unclear, such as 'Upon completion of the training process, the encoder network may be discarded as it will not be used for inference.' I think 'may' should be removed here.

2. The current experiments are insufficient to prove the effectiveness of the proposed method.

    a) Table 2 simplifies a lot of information, which reduces clarity; for example, it only records the mean or best results of multiple methods and lacks the clean accuracy of the purification method. I suggest listing all methods according to both clean accuracy and adversarial accuracy. The existing content in Table 2 can be added as additional row information.

    b) Since the paper does not give specific problems, only a general goal, which is to better defend against adversarial attacks, the experiments become relatively limited. I believe the author should re-summarize the shortcomings of existing methods and the advantages of the proposed method and conduct more experimental comparisons.

Limitations:
The authors have discussed limitations of the work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study proposes a novel adversarial defense method called CARSO. CARSO consists of two models: a classifier and a purifier. The classifier is (pre)trained to correctly classify possibly perturbed data. The encoder of the purifier is trained to generate a latent space from the internal representation of the classifier and the original (possibly perturbed) input. The decoder of the purifier is trained to reconstruct a sample from the latent representation and the internal representation of the classifier. The final prediction is determined by aggregating the outputs of the classifier for reconstructed data.

Detailed procedures are summarized as follows:

- The classifier is always kept frozen. Other parts, including the VAE and small CNNs for compression, are trained on a VAE loss consisting of a reconstruction loss based on a pixel-wise channel-wise binary cross-entropy loss and KL-div.
- The internal representation and input are compressed by small CNNs before being inputted into the encoder of the purifier.
- The classifier is pretrained according to [18] or [62].
- When training the purifier, each batch contains both clean and adversarial samples.
- The aggregation is represented by a double exponential function.
- Evaluations are conducted under $L_\infty$ attacks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The concept of blending adversarial training and purification is novel and interesting. The proposed method, CARSO, achieves robust accuracy that surpasses the SOTA adversarially trained models and purification methods, including diffusion-based models, despite its relatively simple mechanism.
- The evaluation was carefully conducted. The authors explicitly address common pitfalls in evaluating robustness. For example, they conducted end-to-end validation (full whitebox setting), addressed concerns about gradient obfuscation, and used PGD+EOT to address the stochasticity of CARSO.
- CARSO can utilize existing pretrained models, which have already achieved high robust accuracy.
- A wide variety of datasets (CIFAR-10, CIFAR-100, and TinyImageNet-200) were used for evaluation.

Weaknesses:
**1**. In my opinion, the claim that CARSO surpasses the used adversarially trained model seems questionable. If my understanding is correct, during inference, the decoder takes class information only from the internal representation of the classifier. Thus, I believe the decoder can correctly reconstruct the sample only if the classifier, outputting the internal representation, can correctly extract class information from the original perturbed sample. Could the authors clarify this?

Note: Initially, I doubted whether some experimental or evaluation settings were appropriate. However, as far as I can tell, there are no issues. Just in case, I recommend the authors review their source code again.

**2**. CARSO sacrifices clean accuracy more significantly than existing SOTA methods. Additionally, to compare CARSO and the best AT/purification models in terms of clean accuracy, Table 2 should include the clean accuracy of the best AT/purification models (i.e., the contents in Table 15). The scenario or dataset columns in Table 2 might not be necessary.

**3**. Few ablation studies. The authors should include the case of $L_2$ perturbations and use internal representations from different layers. Particularly, the relationship between the layers used for extracting representation and robust accuracy is of interest.

Limitations:
The authors explicitly addressed the limitations in Section 5.3.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper integrates adversarial training and adversarial purification to enhance robustness. It specifically maps the internal representation of potentially perturbed inputs onto a distribution of tentative reconstructions. These reconstructions are then aggregated by the adversarially-trained classifier to improve overall performance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The idea of combining adversarial training and adversarial purification is interesting.

Weaknesses:
1, The experiments are too weak. I hope the authors can refer to at least [1][2][3], which are relevant to adversarial purification, to conduct experiments from more dimensions and consider more baselines and fundamental experiments.

2, Could we just combine [1] with an adversarially-trained model to achieve similar performance?

3, Why should the classifier be adversarially trained for better accuracy?

4, Why can't we directly purify the image? Could we use an image-to-image method to purify the input image?





[1] DISCO: Adversarial Defense with Local Implicit Functions.
[2] Diffusion Models for Adversarial Purification
[3] IRAD: Implicit Representation-driven Image Resampling against Adversarial Attacks

Limitations:
The method heavily relies on training a VAE as the generative purification model.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
UGKgoAZuuL;"REVIEW 
Summary:
This paper finds that existing methods in continual contrastive self-supervised learning (CCSSL)--a class-incremental learning scenario where the data is unlabeled--overlook contrasting data from different tasks, leading to inferior performance compared to the joint training upper bound. The authors propose to sample external data that are similar to each of the learned tasks to augment learning the current task. The self-supervised learning (SSL) objective on the union of the selected external data and the current-task data encourages the model to distinguish the current task and the learned tasks better. 

The authors perform experiments with ResNet-18 and (mostly) BarlowTwins on CIFAR-100 and ImageNet-100, with a mix of other datasets as the external data. The authors find that their method, BGE, consistently improves existing CCSSL methods that do not perform inter-task discrimination. Differently, the joint training model does not benefit from external data.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The finding that existing regularization-based CCSSL methods overlook inter-task discrimination and the proposed method leveraging external data are novel.

2. The experiments performed in the analysis section (Sec. 4.3) provide insights into why the method works and are interesting to me, especially on whether the benefit of external data comes from positives or negatives.

Weaknesses:
1. The SSL method used is mainly BarlowTwins (except in Table 4 where SimCLR is used), which is not usually considered as a contrastive learning method because it does not contrast the anchor with negatives. I wonder if (a) providing preliminaries in the contrastive sense (Sec. 3.1), (b) including ""contrastive"" in the setting name (CCSSL), and (c) arguing that OPO enforces diversity because of findings based on contrastive learning (L#191) are misleading. I think there also needs to be some intuitions on how non-contrastive SSL methods like BarlowTwins help distinguish inter-task data since it is the one used in the experiments, and such an analysis could be very interesting.

2. My general feeling about the writing is that, although the main ideas are conveyed clearly, some claims require justification, and can be improved. Besides some big words (""much more meaningful"" in L#69, ""widely agree"" in L#138, ""extremely low"" in L#167, etc.), please see the questions below for concerns regarding specific reasonings.

Limitations:
The authors mention that their method uses external data which preserves privacy. One concern is that when the external data is not curated (e.g., scraped from the internet), there is risk that they contain private or harmful information that can be learned by the model. 

Another point is that the findings are limited to BarlowTwins (and SimCLR in one experiment) and regularization-based CL methods, and may not generalize.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper focused on continual contrastive self-supervised learning (CCSSL), highlighting that the absence of inter-task data results in sub-optimal discrimination in continual learning. The authors then proposed a method that performed contrastive learning of external data as a bridge between continual learning tasks. The proposed method achieves some improvements in a plug-in manner.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is basically well-organized and easy to follow. 

2. I appreciate the idea that continual learning should consider the inter-task discrimination, which is limited by historical samples and under explored in literature. This results in a gap between ideal continual learning performance and joint training performance. 

3. The proposed method seems to provide plug-in improvements over continual learning baselines.

Weaknesses:
1. The proposed method is essentially a straightforward extension of contrastive learning with external data, which limits novelty and technical contributions.

2. As acknowledged by the authors, the similarity of external data to the continual learning tasks is highly relevant to the performance improvements. The use of relatively different / OOD data tends to provide less improvements. Compared with the large amount of external data in use, such improvements may not be significant enough.

3. The employed external data is basically public datasets with careful pre-processing. In realistic applications, the external data in the wild (i.e., without such pre-processing) may result in additional differences and thus further limit the applicability of the proposed method.

Limitations:
The authors have discussed their limitations and societal impact.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces BGE, a novel approach to address the challenge of inter-task data comparison in Continual Contrastive Self-Supervised Learning (CCSSL). BGE incorporates external data to bridge the gap between tasks, facilitating implicit comparisons and improving feature discriminability. The paper also presents the One-Propose-One (OPO) sampling algorithm to select relevant and diverse external data efficiently. Experiments demonstrate BGE's effectiveness in enhancing classification results across various datasets and its seamless integration with existing CCSSL methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.BGE offers a creative solution to a significant but often overlooked problem in CCSSL, enhancing the feature learning process through external data.
2.The paper provides extensive experimental results that validate the effectiveness of BGE in improving classification performance across different datasets.

Weaknesses:
1.The introduction of external data may increase the computational cost and training time, which could be a limitation for resource-constrained environments. The authors may provide more analysis about the extra time comsumption problem.
2.While BGE shows promising results, the paper could provide more insight into how the method scales with the size of the external datasets, which is crucial for very large-scale problems.

Limitations:
The paper acknowledges the increased computational cost due to the use of external data. However, it could further discuss the trade-off between performance improvement and time comsumption.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors:
- argue that an optimal model for continual contrastive self-supervised learning should perform as well as a model trained with contrastive learning on the whole set of data, including negative samples taken between different temporal slices of the dataset, no just within the same temporal slice
- propose a method for using pre-existing external data to augment the temporally constrained dataset

For context on my background, I am very familiar with SSL literature, only loosely familiar with continual learning, and have never heard of continual contrastive learning before.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Researchers measure the performance of their technique on top of several existing techniques for preventing catastrophic forgetting, showing the performance of the combination of methods. It is valuable to know this.

The authors make comparison against some baselines - not just training on the joint data from scratch (non-continual learning paradigm); but also with external data added. They also demonstrate the performance with random sampling of the external dataset vs smart sampling with their algorithm, and investigate some ablations. These results help to inform where their approach provides value.

The discrepancy between the performance of a model trained with negative samples between subdatasets and with negative samples only taken within subdatasets appears to be a noteworthy observation and one which should be discussed within the continual contrastive learning community. To my understanding, the joint task as the authors suggest sounds appropriate. However, this change could be construed as changing the task that is posed to the model and moving the goal-posts (defining an easier task than is used in the literature at present), indeed as could incorporating external data. The question is in some sense what the goal of CCSSL truly is. In standard continual learning, the goal is to retain performance on previous tasks in the face of training on new tasks. This is often simulated by having classes within a dataset arrive in staggered batches. However, in contrastive continual learning there appears to be only one task (contrastive learning) and the data on which that one task is trained is merely staggered. Does the authors' approach break down an artificial barrier that was in place to simulate a harder task? Or a barrier that should not have been present in the first place and is a vestigial barrier inherited from continuous learning? This is unclear to me.

The work is generally well presented.

Weaknesses:
My understanding of continual learning is that one experimental paradigm that is used is to retain previously presented subdatasets/subtasks and to prevent catastrophic forgetting by including the old tasks in the mix while introducing a new task (e.g. [Robins, 2001](https://www.tandfonline.com/doi/abs/10.1080/09540099550039318), [Aljundi, 2019](https://proceedings.neurips.cc/paper_files/paper/2019/file/e562cd9c0768d5464b64cf61da7fc6bb-Paper.pdf)). This setup is not considered in the paper, but it seems it would address a significant fraction of the issues the method is attempting to address with regards to the joint vs intra-only training configurations. I imagine results may still be improved by incorporating external data in the ""imaginative"" capacity before the full dataset has ""arrived"", even in this scenario. It is unclear why the authors retain this barrier (refusing to continue training on datasets $D_{0, ... i-1}$ even whilst changing the task from a series of isolated contrastive learning tasks to a joint contrastive learning task where data arrives at staggered intervals.

The paper is missing comparison to some additional baselines which would be useful to see:
- What is the performance for a model trained solely on external data, without using the continual learning dataset?
- The performance Joint+ED uses a static subset of the external data. One could also consider using Joint + a subset of size $K$ of the external data that changes every epoch, so potentially the model eventually sees all samples from ED and not just a subset.
- What would the performance be if instead of finding external data proxies for the existing data, you simply retained the previous D_i datasets from previous tasks without discarding them?


### Statistical significance

There are no evaluation as to whether the difference in results is statistically significant. This could be done over repeated runs with different seeds; experiments appear to only be performed with a single random seed. As the authors have repeated runs with different experimental paradigms which could be combined together to make a test for difference without needing to perform experiments with multiple seeds, however there may be correlated randomness between the experiments (i.e. it would be better if experiments are not all performed with the same seed, nor the same ordering of tasks; these should be held constant between comparators and varied between runs to eliminate the effect of these hidden variables on the findings).


### Figures

Fig 1: tSNE has parameters that need to be tuned correctly (perplexity in particular) in accordance with the scale of the features, whereas the more recent technique PaCMAP doesn't and typically produces better results without tuning. The lack of tuning of tSNE may impact the distribution seen in the figure, resulting in one method appearing better than the other by chance where a different choice of perplexity may have resulted in different findings. It is not clear whether the classes were cherry-picked to give favourable results for the authors' method and bad for existing methods. (I am not asserting that they were cherry-picked, but it is not indicated how the classes were selected in the paper so it is not possible to know whether they were or if these results are representative.) These points are not so important as the figure is more illustrative than quantitative anyway.

Fig 2: Font size is too small; to maintain legibility, figure fonts should be no smaller than ~70% the font size of the main text.


### Tables

Table 5: Not clear why this experiment was performed with PFR only. The experiment does not necessarily need to be run with FT and CaSSLe too, but the authors should say on what basis PFR was selected (i.e. it performs better than FT and CaSSLe).

Table 6: Not specified which method was used (FT, CaSSLe, PFR)

Table captions should indicate what the initalisms (CP, CPI, IN, INP, IND) stand for, so readers don't have to look in a distant part of the text to find out. In general, these initialisms are not intuitive - the characters are all run together and the number of characters coming from a dataset in the group is sometimes 1, 2, or 5; ""I"" and ""P"" can not be intuitive when there are multiple datasets being used that start with this character - and this makes it hard to follow the results. The table headings could be restructured to make this clearer e.g. instead of CIFAR, CP, CPI; use as headings C-10, +P365, +IN-R, which are immediately readable and convey the difference between the columns from each other succinctly.

Tables would be more readable if you used `\cmidrule` to indicate the groupings that the headings apply to, instead of having a rule across the whole table.


### Typographical

- L59 Missing word ""with them. [This] enables the""
- L68 ""performance doesn't improve even sometimes decreases.""
- L89 ""Since no labeling requirement, incorporating""
- L239 sentence is not written correctly


### Citations

Casing of initialisms is wrong on numerous citations, e.g.
- [2] vit
- [15] Pathnet
- [40] icarl
- [47] t-sne
- [48] caltech-ucsd birds

Some citations provide no location at which the paper being cited can be found, e.g.
- [48]

Some citations cite arXiv versions of papers instead of peer-reviewed versions, e.g.
- [13] https://openreview.net/forum?id=YicbFdNTTy

Limitations:
The motivation for the method is a niche of a niche. I can not see the union of these restrictions being a scenario encountered in practice. The requirements for the paradigm are:
- A large repository of unlabelled training data for this task does not yet exist to train the model on.
- A continual stream of training data for the task will become available over the course of the period of time where the model is trained (and the model subsequently refined as more data becomes available).
- A very large repository of publicly available data that is near-OOD to the domain of the task does exist.
- There is domain-shift in the continual stream of incoming data that is of a magnitude comparable to the domain shift between the stream of data and the pre-existing external data.
- Although it is fine to train our model on the continual stream of data when it arrives, for privacy reasons we want to periodically destroy the in-domain data we have collected.

This set of restrictions seem unlikely to occur in practice:
- For modalities other than vision, contrastive learning is often challenging to deploy due to its reliance on a robust, manually-curated, augmentation stack.
- For photographs of objects in the world, large datasets already exist (such as is used in the paper).
- For medical images, large near-OOD datasets are not available; furthermore, if you have the rights to train the model on data in a way that is secure and retains the privacy of that data, you do not lose those rights to access the data, so you can keep training on previously collected data.
- For personal images that are requested to be deleted from the company's database by the owner, models may be _required to forget_ the personal images, in which case catastrophic forgetting is advantageous! These requirements have created the nascent field of machine unlearning [[1]](https://arxiv.org/abs/1912.03817), [[2]](https://arxiv.org/abs/2308.07061), [[3]](https://unlearning-challenge.github.io/).

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
JWF1dN4TOd;"REVIEW 
Summary:
This paper studies to how to use deep learning to solve large-scale contextual market equilibrium. This paper proposes MarketFCNet, a deep learning method for approximating market equilibrium. The paper propose an unbiased training loss and a metric called Nash Gap to quantify the gap between the learned allocation and the market equilibrium. Experiments on a synthetic game validates its effectiveness.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
Originality: The paper propose a deep learning method to solve large-scale market equilibrium, which represents buyers and goods, and directly outputs the allocation. The application is novel.
Quality: The paper theoretically derives the loss function, and does some experimental analysis to validates the effectiveness of the propose method.
Clarity: The paper clearly defines the contextual market modeling problem.
Significance: Experiments validates that MarketFCNet are competitive with EG and achieve a much lower running time compared with traditional methods.

Weaknesses:
Quality: The paper does not prove the convergence of the training algorithm. The paper either does not show the training curve. The paper does not provide the implementation code of the algorithm. 
Clarity: The paper is hard to follow. It is quite to hard to understand the meaning of each proposition. 
Significance: The paper aims to solve the large scale contextual market equilibrium, and proposes a novel deep learning method  to approximate the equilibrium efficiently. However, the importance of the large scale contextual market equilibrium is not clear. I do not know how to apply the proposed method in real life.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the computation of market equilibrium where there are a large number of buyers and the buyers and goods are represented by their contexts. It proposes a deep-learning method, termed MarketFCNet, to approximate the market equilibrium. The method outputs the good allocation by taking in the context embedding. It is trained on unbiased estimator of the objective function of EG-convex program using ALMM and is evaluated using a metric called Nash Gap. The method is validated by experimental results.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is well-written and easy to understand. The motivation of the paper seems natural. The paper fills the gap of using deep learning for large scale market equilibrium computation, which can be promising for future study.

Weaknesses:
1. The proof of the unbiasedness of $\Delta \lambda_j$ and Lagrangian estimators in Sec 4.2 seems to be a bit hand-wavy. For example, should $b_i$’s be independent of each other? For a fixed $i$, is $b’_i$ an independent copy of $b_i$? It would be great if the authors could provide a formal (and more detailed) proof of the unbiasedness.

2. What is the effect of $k$ on the method performance? For example, if the dimension $k$ is very large, would the method fail to comprehend the context?

3. How to determine the architecture of allocation network? For example, can one use a Transformer or CNN as the allocation network?

Minor issues:

Line 164: It would be better to define $U(B)$ when introducing uniformly sampling to latter use.

Some equations are missing “.” or “,” at the end. Please fix those.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The submission is not in your area and extends beyond my current expertise (from theory and applications to specific tasks and methods).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The submission is not in your area and extends beyond my current expertise (from theory and applications to specific tasks and methods).

Weaknesses:
The submission is not in your area and extends beyond my current expertise (from theory and applications to specific tasks and methods).

Limitations:
The submission is not in your area and extends beyond my current expertise (from theory and applications to specific tasks and methods).

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a deep learning-based method called MarketFCNet to efficiently compute market equilibrium in large-scale contextual markets, where buyers and goods are represented by their contexts. The key idea is to parameterize the allocation of each good to each buyer using a neural network, and optimize the network parameters through an unbiased estimation of the objective function. This approach significantly reduces the computation complexity compared to traditional optimization methods, making it suitable for markets with millions of buyers. Experimental results demonstrate that MarketFCNet delivers competitive performance and much faster running times as the market scale expands, highlighting the potential of deep learning for approximating large-scale contextual market equilibrium.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The deep learning-based approach, MarketFCNet, can efficiently approximate the market equilibrium in large-scale contextual markets by parameterizing the allocation using a neural network. This significantly reduces the computation complexity compared to traditional methods.

The ability to handle large-scale markets with millions of buyers makes this approach highly relevant for real-world scenarios, such as job markets, online shopping platforms, and ad auctions with budget constraints.

The paper introduces a new metric called Nash Gap to quantify the deviation of the computed allocation and price pair from the true market equilibrium, providing a meaningful way to evaluate the approximated solutions.

Weaknesses:
The deep learning-based approach is inherently less interpretable compared to traditional optimization methods. Exploring ways to improve the interpretability of the learned allocation function, such as incorporating domain-specific constraints or incorporating interpretable components, could enhance the practical usability of the method.

The paper does not discuss potential overfitting issues that may arise when training the MarketFCNet model, especially in settings with a large number of parameters. Incorporating appropriate regularization techniques and cross-validation strategies could help mitigate overfitting and improve the generalization performance.

The paper assumes that the contexts of buyers and goods are homogeneous and can be directly used as inputs to the neural network. Extending the approach to handle heterogeneous context representations, potentially by incorporating feature engineering or meta-learning techniques, could increase the applicability to more diverse market scenarios.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
aYJ2T5TXoX;"REVIEW 
Summary:
The paper tries to formalize the concept of generalizability in experimental studies in machine learning research. It relies on three different types of kernels in order to quantify difference between the rankings in an experiment output. A core contribution is the development of an algorithm for estimating the number of experimental studies needed in order to generalize the results at a desired level.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The topic is interesting and worthwhile.
- The paper is clearly written.
- The formalization of generalizability is well-defined and nicely parameterized through the use of kernels.
- The practical usefulness of the algorithm is somewhat unclear to me.

Weaknesses:
- There is no discussion on the computational costs of the algorithm (except for a vague statement that it is very fast in the checklist).
- The empirical evidence for the algorithm's effectiveness appears somewhat weak to me (see respective item in _Questions_).
- The python package is not properly configured I think. I see this after having installed the package into a virtual environment with the correct python version and
  using `pip install . -r requirements.txt`:

  ```python
  In [1]: import genexpy
  ---------------------------------------------------------------------------
  ImportError                               Traceback (most recent call last)
  Cell In[1], line 1
  ----> 1 import genexpy

  File ~/.pyenv/versions/genexpy/lib/python3.11/site-packages/genexpy/__init__.py:4
        2 from .src import lower_bounds
        3 from .src import mmd
  ----> 4 from .src import probability_distributions
        5 from .src import rankings_utils
        6 from .src import relation_utils

  File ~/.pyenv/versions/genexpy/lib/python3.11/site-packages/genexpy/src/probability_distributions.py
  :11
        8 from typing import Literal
      10 from genexpy import kernels as ku
  ---> 11 from genexpy import rankings_utils as ru
      12 from genexpy import relation_utils as rlu
      15 def sample_from_sphere(na: int, n: int, rng: np.random.Generator) -> np.ndarray[float]:

  ImportError: cannot import name 'rankings_utils' from partially initialized module 'genexpy' (most l
  ikely due to a circular import) (/home/<anonymous_reviewer>/.pyenv/versions/genexpy/lib/python3.11/site-packages
  /genexpy/__init__.py)

Limitations:
Some of the limitations are discussed but I still think the paper could be more self-critical of for instance $n^*$. Possible computational costs are also not discussed.

There are no potential negative societal impacts of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper deals with experimental studies. After providing a mathematical formalization, it focuses on the generalizability of these studies. The main contribution is a quantitative estimate of the the size of the study to obtain generalizable results. Experiments on LLMs are conducted.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- [mathematical formulation] It is nice to have a solid formulation of experimental studies, this is quite relevant to the community.

Weaknesses:
- [train / test split] A concrete problem in machine learning practical experimentation is that of train / test split, and more particularly its absence (that is, training on the test). I do not see this issue discussed in the paper. Can it be incorporated in the setting? Is it possible to clarify whether the paper assumes that the training is done on a training set without calibration on a validation set or is this hidden somewhere? What would then be the influence on the number of experiments?
- [testing between rankings] If I understand correctly, the paper proposes (in Section 4.1) to check whether rankings are consistent by performing kernel two-sample test, with adapted kernels. This does not seem standard to me: there exists some ad-hoc statistical tests (e.g., Kendall's \tau, Spearman's \rho, etc.). Why not use them directly? Is there an advantage to using MMD?  

- [minor comments]:
  - missing ref line 111
  - repeated word ('of') line 300

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tries to formalise the notion of an experimental study by considering the sampling process of acquiring a dataset.  It then uses this notion to argue about generalisability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The problem of understanding the performance of machine learning when tested on new data is a very important problem.  The authors use some technically sophisticate methods to tackle this problem.

Weaknesses:
For me the authors model of an experiment is too simplistic and does not capture the problems faced by machine learning.  If we collect medical data then that data is likely to vary depending on the equipment used, the clinicians running the equipment and population where the data comes from.  These kinds of variations are the bugbear for machine learning, but not captured at all by the model.  Another issue is that a lot of data is non-stationary.  Even in the much used example of checkmate in one.  If a machine learns this very well, then players against the machine are likely to learn their mistake and alter their play.  Thus, I am not convinced that the model being proposed is particularly interesting.

Limitations:
This is fine.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors provide a formalism for the generalizability of experimental studies in ML.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Anything pushing to get better practices in evaluation of ML is very important.

Weaknesses:
I could quibble with some of the setup, which is a bit confusing to me: design factors being properties of the context rather than of the alternative, for example, is kind of odd, but I don't think this is very important.

The big problem is that there is a huge literature on a very similar problem and I have very little sense of how this work connects to it: starting with the Neyman-Pearson lemma and going down through the standard corpus of decision theory, we have a lot of statistical tools for thinking about this problem in very broad strokes. After reading this paper, I have a sense that you're trying to solve a very similar problem (given a sample from some population, what can I say about the reliability of my estimate? How many samples would I need to be sure that it's reliable?)

Section 4.3 seems to be rederiving some form of power analysis.

Looking at A.3.3, it appears that the procedure is essentially the following:
(1) [the inner loop from 1...n_rep] construct a null hypothesis at a given sample size, find the upper alpha quantile of that null hypothesis
(2) Repeat this at a variety of sample sizes
(3) Estimate a power-law relationship between sample size and the upper-alpha quantiles
(4) Predict the sample size which would have such an upper-alpha quantile

This procedure is an empirical version of power analysis where the null distribution is not known but simulated and extrapolated. If I know the type-I error rate, type-II error rate and a distribution under the null and under the alternative, deriving the required sample size is straightforward. Indeed, we have a CLT for MMD (at least under some kernels) [1], so these distributions are known asymptotically, which is likely plenty for the purposes of sample size determination. Do \alpha^* and \delta^* map onto concepts from Neyman-Pearson? It's entirely possible.

This is an important question because decision theory has very well established results on things like uniformly most powerful tests. When we just invent a new framework rather than relying on well-trod ones, we are likely to derive suboptimal procedures unless we compare very carefully to these existing procedures. There's no similarly sophisticated discussion of error properties in this paper, which would be reasonable if this were truly the first paper in its vein, but I don't think that's the case.

Further, it's not clear to me why these similarities between rankings should be the target of inference. Rather, shouldn't I care about whether, based on the sample of allowed-to-vary factors I've used, alternative A is preferred to alternative B? This is an extremely standard matter of decision theory as far as I can tell. By moving to these more complicated research questions about rankings it clouds this fact, but I'm not sure it needs to. If the target of inference were instead to be a rank of K alternatives, I believe a decision theorist would take a somewhat similar approach to what you've done here: define a similarity metric based on the research question. An example solution to a problem like this would be [2], [3]. I just don't see why we need this new framework to accomplish a task I think we already have the tools for.

It's entirely possible that there's a contribution here, but it can't just be ""this is a new task"". We have methods from decision theory that have been designed for a wide range of decision tasks, and its incumbent upon the authors to demonstrate why those existing tools do not fit the task in front of them.

[1] https://www.jmlr.org/papers/volume24/22-1136/22-1136.pdf
[2] https://onlinelibrary.wiley.com/doi/abs/10.1002/mcda.313
[3] https://www.sciencedirect.com/science/article/abs/pii/S0377221715008048

Limitations:
see above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a new mathematical framework and a corresponding new algorithm to evaluate the generalizability of published experimental studies, by adapting Montgomery's classification of experimental factors [44].
They demonstrate the efficacy of this framework in evaluating the generalizability of two popular published experimental studies.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper appears to be theoretical strong. 
It goes beyond the standard notion of reproducibility of an experimental study and comes up a definition of generalizability of an experimental study.
Morever, Proposition 4.2 provides a theoretical result on the sample size $n$ necessary to obtained a desired generalizability $\alpha^*$ for a desired similarity $\epsilon^*$, and the authors also provide an algorithm (A.3.3) to compute this sample size.
Since the similarity $\epsilon^*$ is hard to specify, they make it a function of the kernelized distance between rankings $\delta^*$.

2.
The empirical evaluation in Fig. 2 and Fig. 3, on the categorical encoder comparison from [41] or the BIG-bench framework for LLM comparison from [55], respectively, demonstrates the practical utility of the proposed approach in determining sample sizes to guarantee generalizability.

Weaknesses:
1. The clarity in the writing can be significantly improved:

1.A. Symbols are used before defining them, typos exist, and symbols are not used consistently:

1.A.a. On line 118, the symbol $\mathcal{R}_{n_a}$ is mentioned, but the relation of this symbol to the ranking on alternatives only becomes clear later in Definition 3.1.

1.A.b. The Section number is missing on line 111.

1.A.c. The symbol $m$, is defined as the number of shots, on line 115, whereas line 114 uses the symbol $n$ rather than $m$. Moreover, on line 88, $n$ is defined as the number of shots.

1.A.d. In contrast to 1.A.c, in eq. (1), after line 170, the symbol $n$ is now used without providing a definition. It now appears to be the size of any study, in a general definition, rather than the number of shots, as defined on line 88. 

1.A.e. On Sec. 5.3, line 315, $N$ is defined as the number of preliminary experiments, whereas on line 154, it is defined as the size of the sample of valid experimental conditions. Do these mean the same thing ?

1.B. Sec. 3.1 defines a ranking of alternatives as the primary result of an experimental study.
However, the effect size, i.e., the magnitude and sign of the difference between two alternatives, can be important in certain experiments.
The MMD kernel, used in Sec. 4.2, actually allows measuring this effect size, as discussed in [27], but the limitations imposed by the usage of this MMD kernel within the author's generalizability framework, are not clear despite the somewhat cryptic discussion in Sec. 6.

2. 
The experimental evaluation is limited to a comparison of ranking differences between alternatives, and does not include a measurement of the practical differences between alternatives, or the significance of these differences.

Limitations:
Please refer to the potential limitation underlying weakness #2. Is it possible to quantify the magnitude of differences between alternatives using the generalizability framework provided by the authors ? The authors mention this limitation in Sec. 6, but it is not clear why the MMD kernel cannot quantify magnitude of differences.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
p32gjG4yqw;"REVIEW 
Summary:
This work generalizes the ridgelet transform to equivariant neural networks, providing constructive proofs of universality in the general case as integrations over parameter distributions. Although such a direction had been taken up in prior work [33], they generalize it from scalar activations to vector activations, therefore encompassing more practical equivariant networks. The authors consider the form of the ridgelet transform for deep networks, and groups including the affine group and orthogonal group.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The authors provide a constructive universal approximation result, which is in contrast to many non-constructive universality results. They strictly improve on the past work of Sonoda et al [33] by extending from scalars to vectors, which is more realistic. They consider the implications of their framework on depth separations for equivariant networks.

Weaknesses:
Significance/novelty: The novelty relative to Sonoda et al [33] is limited, and the significance of this work to the universality and equivariance literatures is unclear. For example, many universality results already exist in equivariance (see e.g. work by Yarotsky [3], by Dym et al [2], etc.) — it is not clear how much value this extension of the ridgelet transform adds. 


Clarity: I found the writing of the paper extremely hard to follow. It did not provide sufficient background on the ridgelet transform, universality results for equivariant networks (whether constructive or non-constructive), or perhaps most importantly, motivation for why one should value constructive approximation theorems for equivariant networks. It felt that one had to have read the previous works by Sonoda et al, in order to grasp why this work was important or where its novelty was, such as how vector-valued equivariant feature maps are superior to scalar-valued feature maps, what exactly formal networks are, what the practical use or theoretical value of the ridgelet transform is, etc. The work would also benefit from an outline of the sections earlier in the paper, and a more concise and early statement of what the authors consider their main theorem/s. It was not clear what the central result about the ridgelet transform was, as the transform seemed to still involve an integral in all equivariant cases, without simplification. 

As a demonstration of the power of their theoretical formulation, the authors claim to show a depth separation, in which some class of networks is exponentially wide when shallow (constant number of layers), and only linearly wide when deep (linear number of layers). However, it is not clear whether they show that any shallow network is exponentially wide when representing a given function, or just the one constructed by the ridgelet transform — is this a strict depth separation?

Mathematical rigor: Although I did not check all of the math, some glaring errors stood out to me. First, the proof of Lemma 5 begins with, “Recall that a tensor product of irreducible representations is irreducible.” This is incorrect — for example, the tensor product of the irreps of the group of 3D rotations, SO(3), are reducible, and the irreps that appear in the decomposition of their tensor products are famously given by the Clebsch-Gordan coefficients (see e.g. [1]). Moreover, in the limitations section (6.1), the authors discuss the assumption that the group is locally compact, but say that this “excludes infinite-dimensional groups”. Yet, this is also false: for example, the infinite group SO(3) is compact (and therefore locally compact). In fact, several of the authors’ examples pertain to infinite groups, such as the affine group. These errors are surprising. 

Also, the mathematical techniques themselves do not appear to be novel (for instance, Schur’s lemma is quite standard, and the proofs included in the main body are rather simple — Lemmas 1 and 2 are in fact widely known), and there are no experiments or practical implications, so the merit of the paper must rest on the significance of the results themselves. Unfortunately, the the broader significance of the results are not clearly demonstrated. The authors claim to reveal “the close relationship between machine learning theory and modern algebra,” but the mathematical tools they use seem like the standard ones used already throughout the equivariance literature. I am not sure what the “major upgrade to machine learning theory from the perspective of modern algebra” will therefore be. 

[1] Clebsch-Gordan Nets: a Fully Fourier Space Spherical Convolutional Neural Network by Kondor, Lin, and Trivedi 2018

[2] On the Universality of Rotation Equivariant Point Cloud Networks by Nadav Dam and Haggai Maron 2020

[3] Universal approximations of invariant maps by neural networks by Dmitry Yarotsky 2018

Limitations:
Yes, the authors discussed limitations of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a unified approach to universal approximation theorems for neural networks using group representation theory. It extends to vector-valued joint-group-equivariant feature maps, providing a systematic method for both shallow and deep neural networks with nonlinear activation functions. By leveraging Schur's lemma, the paper shows that these networks can universally approximate any function within a certain class. It main contribution is the closed-form ridgelet transform, which offers a constructive proof and explicit parameter distribution for these networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.The paper introduces a unified constructive universal approximation theorem that applies to both shallow and deep neural networks using group representation theory. This is an innovative approach. It also extends previous work by incorporating vector-valued joint-group-equivariant feature maps.

2.  The paper is theoretically sounding, leveraging concepts from group representation theory and Schur's lemma. They perform the thorough and systematic development of the ridgelet transform, providing a closed-form solution for parameter distributions and ensuring the findings are theoretically well justified.

3. The paper is well-structured and clearly written. Definitions, theorems, and proofs are presented in a coherent manner, making it easier for readers to follow the details of the argument and understand the implications of the results.

4.  This work is significant since it provides a relationship between deep learning theory and modern algebra. By providing a unified framework that applies to a wide range of network architectures, the paper incentivize further research and development in the field of machine learning.

Weaknesses:
1.  While the paper is strong in its theoretical contributions, it lacks empirical validation through experiments or simulations. Demonstrating the practical applicability and effectiveness of the proposed ridgelet transform and the unified framework on real-world datasets or benchmark problems would strengthen the paper. Including even a small set of experiments could provide evidence of the practical relevance and performance of the theoretical results.

2. This work makes several assumptions, such as the local compactness of the group \( G \) and the boundedness of the composite operator \( \text{NN} \circ R \). While these assumptions are standard in group representation theory, the paper could benefit from a more detailed discussion on their implications and limitations. Exploring scenarios where these assumptions might not hold or providing guidance on how to relax these assumptions.

3. Some of the technical details, particularly those related to advanced concepts in group representation theory and the ridgelet transform, might be challenging for readers who are not experts in these areas. Providing additional intuitive explanations, diagrams, or examples to illustrate these concepts could enhance the clarity of the paper.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present a generalization of the work by Sonoda et al. by extending their formulation of universal approximation theorems applicable to a specific class of neural networks namely scalar-valued joint-group-invariant feature maps for ""formal deep network"" to a much larger class of learning machines. Their theory using tools from group representation theory allows them to uniformly treat both shallow and deep neural networks with a larger class of activation functions. They provide an explicit construction for parameter assignment (aka Ridgelet Transform) and apply it to vector valued joint group-equivariant feature maps.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The topic is well motivated and the writing is clear and understandable. The interspersed explanations in plain english are quite helpful in understanding a paper that leans quite heavily on sophisticated mathematical formalisms. (eg line 93-94). 
- The proofs and the notation are clear and succinct.
- The authors extend an earlier work to a much more practical and real world class of NNs by introducing *vector-valued joint group-equivariant* feature maps, which yields universal approximation theorems as corollaries. They also unify the treatment of both shallow and deep networks by leveraging Schur's Lemma.
- They provide explicit examples for depth 2 and depth $n$ fully connected network with an arbitrary activation in Section 4.2 which helps ground their method and significantly helps the reader understand how to leverage the tooling introduced by the authors.
- The paper provides formal support for the popular interpretation for the efficacy of DNNs compared to shallow networks, namely that they construct hierarchical representations which would take an exponential number of neurons to represent using a single layer.
- The limitations section is well written and is explicit about the assumptions made so that the reader is aware of the regime in which the proofs are applicable.

Weaknesses:
**Major**
- The biggest weakness of the work seems to be that it shares a vast amount of technical analysis, machinery and the fundamental proofs are shared with the earlier work by Sonoda et al. While the extension to a larger class of networks and the introduced vector values feature maps is certainly valuable, I am not fully convinced of the differential novelty of the work. Most of the (valuable) effort has been spent in a mostly natural extension of the previous work on the topic.


**Minor**
-  The authors mention that assumption (5) (that the network is given by the integral representation) in limitations is potentially an ""advantage"". If that is so, a discretized version would be the preferred model since it is also closer to real world NNs
- Typo on line 77  - mathmatical -> mathematical
- Typo on line 310 - cc-universaity -> cc-universality
- lines 135 - 137 would be significantly easier to read when broken into multiple lines

Limitations:
No limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
7g8WSOHJtP;"REVIEW 
Summary:
1. The paper unifies existing heterophilous graph neural networks (HTGNNs) into a Heterophilous Message-Passing (HTMP) mechanism.
2. The authors reveal that the effectiveness of HTMP is due to increasing differences among node representations belonging to different classes.
3. Guided by this revelation, the paper then introduces Compatibility Matrix-aware Graph Neural Network (CMGNN) to further enhance HTGNNs.
4. The authors conduct fair evaluations and comparative analysis on multiple benchmark datasets, highlighting the superior performance of the HTMP mechanism and the proposed CMGNN method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The claims are supported empirically by a detailed comparison across multiple benchmark datasets.
2. The paper is well-written and clearly structured, with each section logically building on the previous ones.

Weaknesses:
1. Several research publications [1, 2, 3] have used compatible matrices to boost the effectiveness of GNNs on heterophilic graphs. In-depth qualitative and quantitative comparisons are missing from this submission. Including such analyses would significantly increase the importance of the contributions.
2. Some claims made in the paper, such as Observation 1 and Observation 2 in Section 4, would benefit from additional analysis. For instance, including theoretical analysis with formal notations would provide more rigorous support for these claims.
3. Existing survey articles have unified and categorised message passing on heterophilic graphs [4,5,6]. This submission should compare and position the proposed HTMP unification against these categorisations.
4. The experiments lack a comparison of training times with baseline models. Including an analysis of the tradeoff between accuracy and training time would greatly enhance the results.




References:
1. Simplifying Node Classification on Heterophilous Graphs with Compatible Label Propagation, In TMLR'22,
2. Explicit pairwise factorized graph neural network for semi-supervised node classification, In UAI'21,
3. Graph Neural Networks with Heterophily, In AAAI'21,
4. Graph Neural Networks for Graphs with Heterophily: A Survey,
5. Learning from Graphs with Heterophily: Progress and Future,
6. Heterophily and Graph Neural Networks: Past, Present and Future.



**Edit post Rebuttal:**
The authors have promised to include detailed comparisons in a future revised version. Since these details cannot be verified within the review period, I will lower my confidence from 4 to 3. However, given that other major concerns have been addressed, I will raise my rating by one point from 4 to 5.

Limitations:
The authors have provided some discussion on the limitations of their work (for instance, see section 7 on Page 9).

Potential negative societal impacts are not relevant to this study.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work revisits the message-passing mechanisms in existing HTGNNs and reformulates them into a unified heterophilous message-passing (HTMP) mechanism. Based on HTMP, the authors propose a new framework named CMGNN. Experiments on 10 datasets with 13 different baseline models demonstrate the effectiveness of the proposed framework.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This work proposes a unified heterophilous message-passing (HTMP) mechanism, which could be a guideline for further research on heterophilous GNN. 
2. Based on the HTMP mechanism, this work proposes a new framework named CMGNN, which is novel and has basic value.
3. The effectiveness of the HTMP mechanism and CMGNN framework is well supported by experiment results.

Weaknesses:
1. Paper presentation could be further improved. For example, the conception of ""good"" heterophily and ""bad"" homophily deserves further explanation. There are some spelling mistakes, such as ""heterophilious"" in line 12.
2. If space permits, I feel like moving experiments in Appendix C to the main body would be better for the introduction of *Observation 1*. 
3. It might be hard to follow as this paper has so many equations, especially those about CMGNN. So I suggest providing a flow chart or a pseudo algorithm for better understanding.
4. Conclusions in this paper are mainly based on experiment results. It would be better if corresponding theoretical analysis or proofs are provided.

Limitations:
As the authors mentioned, this work mainly focuses on semi-supervised settings, which could be further generalized.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to address the question of ""why does message passing remain effective on heterophilous graphs"" and proposes a unified framework called heterophilous message-passing (HTMP) mechanism. It extensively reviews the architecture of existing heterophilous GNNs under this framework. It then moves on to discuss the empirical observation that the success of message passing in existing heterophilous GNNs is attributed to their implicitly enhancement of the compatibility matrix among classes, and proposed a new GNN approach called CMGNN to further enhance the separability of the compatibility matrix for different classes in the message passing process. The paper includes an extensive empirical analysis involving 10 benchmark datasets and 13 well-established baseline GNNs, and show that the proposed CMGNN approach has the best overall performance against the baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The writing is clear and well-organized for most parts of the paper; 
- The paper gives an extensive survey of existing message-passing GNNs under the HTMP mechanism in Table 1 and Appendix A. 
- The experiments are well-thought and extensive: it addresses the drawbacks of the previous homophilous and heterophilous node classification benchmarks identified in previous works by using more recent benchmark datasets, and include 13 baselines for a comprehensive evaluation of the proposed method.
- The proposed approach, CMGNN, has the best overall performance against 13 baselines on 10 benchmark datasets.

Weaknesses:
- This work builds upon the findings of several previous works regarding the effective designs for GNNs under heterophily and when is heterophily challenging (or in other words, ""bad"") for GNNs. While the authors cited these works in some parts of the paper ([6,9,12,18] in References), I feel that **some of the observations in the paper overlapped with the findings in previous works, and their connections and differences are not clearly stated in the paper**. 
  - For example, Observation 1 seems to overlap with the previous observations made in [6] (""to ensure that the neighborhood patterns for nodes with different labels are distinguishable, the inter- class similarity should be low"") and [9] (""two key factors, low-degree nodes and complex compatibility matrices, deteriorate the distinguishability of the neighborhood label distributions when coupled with heterophily, thus making heterophily a unique challenge for GNNs in most cases""). 
  - Given this, I also think that the claim in the related work section (line 732-734) that ""these reviews ... not exploring the reason behind the effectiveness of message passing in heterophilous graphs"" is inaccurate, as this paper is in fact built upon these analyses regarding the effectiveness of message passing in heterophilous graphs. 

- Section 5 (method) is too condensed to present a clear picture of how the proposed Compatibility Matrix-Aware GNN (CMGNN) works for the readers. For example, it is unclear what ""topology structure"" that the authors are considering as ""additional available node features"", and the term in Eq. 7 is not well explained. The authors also didn't explain clearly in the main paper how is the ""soft pseudo labels"" being generated for the model. It will help with the understanding if the authors can include a figure showing the architecture of the proposed CMGNN model. I feel the ""method"" section is the most novel part in the paper and deserves more length in the paper. 

- It would be good to analyze the computational complexity and/or compare the empirical runtime of the model with the baselines. 

- As a minor point, the ""Norm"" term in Eq. 3 should be explained as ""L1 normalization for matrix row vectors"" to avoid the confusion that the normalization is done with the L1 norm for *matrix* (instead of for vectors).

Limitations:
The authors acknowledged the limitation that the proposed HTMP framework is only applicable to GNNs following the message-passing mechanism. One additional limitation is that the paper is mostly empirical and does not give theoretical underpinnings.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
URkFX1jmOd;"REVIEW 
Summary:
This paper presents an approach, namely N2D3, for night-to-day image translation. Specifically, the proposed pipeline involves two stages: illumination degradation disentanglement and degradation-aware contrastive learning. The first stage decomposes an image into darkness, well-lit areas, light effects, and highlight regions. The second stage applies contrastive learning to these four types of nighttime degradations. Extensive experiments conducted on the BDD100K and Alderley datasets demonstrate that N2D3 outperforms existing methods.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
- The paper addresses the critical problem of night-to-day image translation in computer vision.
- The authors provide comprehensive experimental validation of the proposed method.

Weaknesses:
The reviewer has raised this paper for an ethics review due to a significant omission of a key citation. In Section 3.1, the authors introduce a color invariant term for light effect detection. However, this term was originally derived by Geusebroek et al. in their paper *Color Invariance* [1]. The authors devote an entire page to deriving the invariant term without appropriately citing the original work, which violates academic integrity. The authors should explain why this citation is missing, as it does not seem to be an unintentional oversight. This intended missing reference also made Eq. (1)-(5) lack logical coherence and hard to follow.

[1] Color Invariance. J. M. Geusebroek, R. van den Boomgaard, A. W. M. Smeulders, H. Geerts. IEEE TPAMI, 2001.

**Note that although the reviewer has raised the ethics review flag, the reviewer’s rating does not take this into account.** 

In addition to the missing citation, the reviewer has concerns about the technical soundness of the paper. Specifically, why are four types of degradation considered? Since the disentanglement of well-lit and light effect regions is the paper’s main contribution, ablation studies using only three types of degradation (darkness, well-lit, and highlight) should be provided.

Besides, the paper’s citation style is inconsistent. For instance, citations for the same conference sometimes include the abbreviation and publisher while others do not (e.g., [1], [19], and [26]). Additionally, some citations include the month of the conference while others do not (e.g., [24], [28]), and some contain volume information while others do not (e.g., [22], [23]). Ensuring consistent citation formatting would enhance the paper’s overall presentation quality.

Limitations:
The authors have adequately discussed the limitations of the work, and this paper does not have any negative social impacts.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new framework N2D3 for solving night to day image translation problem. Their framework consists of a physics-based disentanglement module and a contrastive learning module for preserving semantic consistency. Their method shows improved performance in terms of FID and downstream task performance on BDD100K and Alderley dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Using the Kubelka-Munk theory for different degradation types and applying a patch-based image translation is a novel method. 
- The figures are well-made. For instance, the visualization in Fig. 1 and Fig. 2 are intuitive and helpful for understanding the whole architecture.
- Quantitative evaluation results are convincing, showing the effectiveness of the proposed framework in terms of various metrics.

Weaknesses:
- Clarity of the method sections can be improved. For instance, including more rigorous definitions or visualizations of what well-lit and different light effects mean and provide a motivation why it is helpful to disentangle those illumination causes separately. 
- The authors can also add proper citations to previous work when they mention “by common practice”, for instance in line 107, line 142 and line 196.

Limitations:
As the authors already mentioned in the appendix, the current physics-aware degradation disentanglement module is designed mostly for illumination related effects and does not handle other types of degradation such as raindrops. I wonder how the authors think the framework could benefit or inspire other types of adverse weather image restoration tasks.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a comprehensive solution for Night2Day image translation by leveraging physical priors, photometric modeling, and contrastive learning, leading to state-of-the-art performance in visual quality and downstream vision tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors develop a photometric model based on Kubelka-Munk theory to extract physical priors from nighttime images. This model helps to disentangle different types of illumination degradations by analyzing the illumination distribution.
Overall, the paper presents a novel approach to handling nighttime image translation by considering the unique challenges posed by varying degradations and employing both physical modeling and advanced learning strategies to address these challenges effectively.

Weaknesses:
1. The writing is difficult to understand. The explanations and derivations for Eqs (1) to (5) lack logical coherence and necessary references, making them hard to follow.   The derivations for Eqs (7) to (9) also lack supporting references, casting doubt on their validity. 
2. The motivation for DAR is unclear. Please explain the motivation behind it.
3. There is no baseline network, making it difficult to determine the performance gain for the specific module. 
4. The ablation experiments lack in-depth analysis. For instance, there are no ablation experiments to verify the impact of introducing four regions for disentanglement versus three regions (e.g., excluding the light effects region).
5.There is a need to compare with more recent methods for unpaired image-to-image translation, such as COCO-FUNIT, StegoGAN, GP-UNIT, etc. Please check the reference [5], as it does not seem to be published in CVPR.

Limitations:
The motivation behind some methodological choices, is not clearly explained.
The ablation experiments are insufficient and lack depth. 
The paper lacks a discussion on the computational complexity and resource consumption of the proposed method.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes to address the night-to-day translation problem in which its learning basically can be briefly described by two steps: 1) illumination distribution as well as the physic priors built upon the Kubelka-Munk photometric model are firstly adopted to separate/disentangle the image regions into four degradation categories, i.e. darkness, well-lit, light effects, and high-light, in which such illumination degradation disentanglement is the main contribution of the proposed method; 2) the degradation-aware contrastive learning module is applied to maximize the mutual information between patches in the same spatial location from the generated image and the source image, where the anchor and its corresponding negative patches should be from the same degradation category (i.e. degradation-aware sampling) and the weights for each negative patch are determined by similar matrix obtained from the optimal transport computation (i.e. degradation-aware reweighting). Moreover, the GAN-based objective function is employed to bridge the domain gap between (generated) daytime and nighttime images. The translated images (from nighttime to daytime) are shown to have better quantitative and qualitative performance (in terms of FID) for aligning with the real nighttime image distribution.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ In addition to provide better translation performance (both quantitative and qualitative), the translated images produced by the proposed method are shown to have better structural similarity with respect to the corresponding daytime images (evaluated in Alderley dataset)  in comparison to the baselines. Moreover, the typical semantic segmentation model (pretrained on typical daytime dataset, i.e. Cityscapes) applied on the translated images (produced by the proposed method) leads to better segmentation results in comparison to being applied on the images generated by the baselines (i.e. indirect evidence showing that the images generated by the proposed better follows the daytime image distribution which the semantic segmentation model is trained on). 
+ The ablation study does demonstrate the contribution of illumination degradation disentanglement for separating the image patches into four different degradation categories.

Weaknesses:
- Although experimentally shown to be effective, the mechanism and the basic ideas behind leveraging the illuminance distribution as well as the physic priors for realizing disentanglement of four degradation categories (i.e. darkness, well-lit, light effects, and high-light) are not well explained, in which the physical meanings for Eq.1 to Eq.11 are hard to understand and follow. Basically, as such illumination degradation disentanglement is the main contribution of the proposed method, the description should be more self-contained and explanatory. 
- As the illumination degradation disentanglement plays a key role in the proposed method, it would be great to have the robustness analysis on such disentanglement if possible (i.e. how accurate is the disentanglement, is there any related dataset we could apply such analysis?) and how would the translation model learning be affected once there are erroneous disentanglement?

Limitations:
no potential negative societal impact is found.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
OiTr2v90t7;"REVIEW 
Summary:
This article adapts the recently-developed combinatoric concept of the “Pemutree” into a machine learning context, making links with existing methods in Bayesian nonparametrics, and providing a pathway for how to make the abstract mathematical concept relevant to data-driven approaches and inference. The theory is explained, and an example application in phylogenetic analysis is performed.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
This is an ambitious paper! Making links with recent combinatorial research and machine learning is a good thing to do! It has a large vision of building one grand unifying theory of discrete Bayesian nonparametrics, which (it is claimed) can be done with the framework presented. The review of discrete BNP at the start of the article is thorough. 

The potentially difficult subject matter of the abstract mathematical objects is explained fairly clearly through the use of figures and well-chosen notation. The theoretical aspects are explained thoroughly, and the application pursued emerges naturally from the framework developed earlier in the article: the coalescent analysis and the Mondrian process are good results to have. The scientific writing is of a fairly high standard, with only a couple of surprising vocabulary choices.

Weaknesses:
The subsequent developments of the theory and applications don’t quite live up to the grand vision set out earlier in the article. The authors struggle to represent the most widely-used discrete BNP object of the Dirichlet Process in this supposedly all-encompassing framework: perhaps this is doable in the future and represents work yet to be done, but the initial claims about the generality of permutree processes made in the article are not fully followed through on.

The article also (necessarily) spends a lot of time introducing the theoretical framework, quite heavily at the expense of presenting the data applications properly later in the manuscript. Squeezing all of the experimental results into “Demonstration” in half a page is really too brief to be very convincing, although there is a lot of interesting material in Appendix C that would ideally be in the main text. Many of the potentially thorny issues concerning inference and computation are therefore overlooked.

Some of the figures could be better designed: I found the visual interpretation of permutrees key to developing some understanding them, so making Figure 1 bigger and more prominent would be a help (I think figure 1 is more crucial than Figures 2/3/4 in this respect). The representation of the data in greenscale Figures 8 and 14 is very confusing: I don’t think I really learned anything from that representation.

Some of the language choices are a bit strange: line 69: “we dare to pay particular and explicit attention here”, line 869: “Roughly speaking, it is not possible in principle to naively implement a model with infinite parameters on current computers”, 

Line 250: “as an overall trend…” The analysis of the experimental results is not rigorous enough. You have real values and uncertainties for the perplexity. Do some tests or similar to establish more clearly the differences in performances rather than painting broad brushstrokes.

Line 80: some of the symbols used have already established meanings in a machine learning context, i.e. \otimes meaning kronecker product. Maybe make clear that this is a new notation that overrides any previous perceptions.

Limitations:
The practical models that are (presently) successfully captured in this framework are not the most widely used BNP models out there. The whole permutree framework seems best suited to the coalescent-type models pursued, and the other BNP models that have been successfully described, such as Mondrian processes, are interesting but not in widespread use.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
After giving an introduction to permutrees, a stochastic process on them is constructed by sampling the nodes according to an intensity function on the 2d unit interval and uniformly assigning the marks. It is shown how to add the edges to meet the requirements for the object being a labeled permutree. Paths from terminal nodes to other terminal nodes in the permutree can be used to represent sequential data. Finally, the model is used for an inference task involving DNA sequences.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* Sections 1-4 are very concise but nonetheless clear and easy to follow. This paper does a great job explaining the complex concepts of permutrees and permutree processes. 
* Figure 1-3 are very helpful
* the concept generalizes popular processes used in ML, e.g., the Mondrian processes

Weaknesses:
* As a reader unfamiliar with phylogenetic analysis, I did not immediately understand what the task in this setting is (and I am still unsure if I fully got it): Do you have a set of DNA sequences where some of the letters are masked, and you want to predict the masked letters? 
* Besides not really understanding what the goal of this task is, I think Section 5 does not provide enough explanation of how this goal then is achieved, i.e., the length of the paper/ the level of detail in Section 5 is a problem. It's ok to defer details to the appendix as long as one can still follow the main section without them, but I struggle with that. Example given: I find it crucial to know the likelihood function when it comes to a Bayesian inference task, which is not mentioned in the main section.

Limitations:
yes, there is a dedicated paragraph on limitations and I think it captures the limitations of the suggested concept appropriately.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce a prior for Bayesian nonparameterics called the permutree. They apply it to model complex phylogenetic data with both coalescence and recombination, a setting that previous processes such as the Kingman could not model; the model seems to perform state-of-the-art phylogenetic inference. In principle the process could also be used to model other combinatorial objects such as permutations or trees however this is not demonstrated.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
Modeling recombination is challenging and this model proposes a method to do so.

Weaknesses:
The writing is very challenging. There are multiple points where the writing is strange, for example the use of ""dare"" in ""For technical reasons (discussed immediately below), we dare to pay particular and explicit attention here to the set V of the “interior vertices” (i.e., vertices of degree at least 2) other than the terminal nodes."" "". The exposition is also very verbose and proposition is challenging to understand without reading the proof. Figures 3(c) and 1(b)-(d) are never mentioned in the text, the later is quite confusing since it seems to suggest that the permutree can model many combinatorial objects.

It seems that there is a disconnect between the description of the methods in sections 3, 4, and the experiment in section 5. See questions.

The ultimate goal of phylogenetics is to infer ancestry which is not identical to maximizing likelihood. To validate a bone fide phylogenetic inference method that handles recombination, one should show that inferred recombination events are realistic.

Limitations:
Discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors describe the concept of permutrees, how to sample permutrees in a stochastic process, and how to model data with permutrees. They apply it to tracking DNA changes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Interesting new model that unifies permutations, trees, partitions, and binary sequences 

Strong mathematical foundation

Practical applications

well-written

Weaknesses:
the figures are small and hard to read when printed in gray-scale

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
IdEaeCbhUW;"REVIEW 
Summary:
This paper addresses the problem that commonly in RL, small reaction times and high action frequencies are required, which is not the case for computations in the brain. As a more accurate model, the authors propose an RL method that learns an internal model to improve performance in high-latency applications.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper rightfully describes the issue that, usually, RL methods operate under different conditions than the human brain (higher latencies/lack of information). Developing a biologically plausible model that can deal with higher latencies and lower perceptual frequencies would be beneficial to get a better understanding of the human brain and for realizing certain applications in robotics. The approach of learning an internal model guiding the actions, if no observations are present, seems to follow intuition and could be promising. I would therefore rate the problem as relevant and interesting, and the general model (at least on a computational level - this could be stated more clearly in the paper) as ""plausible"".

The problem and approach (besides my points in the Weaknesses/Questions sections) are clearly described. The method is evaluated on a wide range of six control problems, showing that the model can produce reasonable control signals.

Weaknesses:
I found a few parts of the paper difficult to follow (mainly the policy and evaluation section, see also my questions for this), and the paper could benefit from improvements in these parts. In particular, I am uncertain whether the approach introduced in the paper fits the problem description. It seems that the approach targets the setting of high action frequency but not delays of perception.

Furthermore, I had problems understanding how the policy of the proposed approach works. The authors seem to define a probabilistic policy ($\pi_\omega$) defined by parameters $\omega$ with the past previous action as input (line 177). In Eq. 5, which describes the loss for learning the policy, the previous action does not seem to appear. For this loss, they need state-action pairs from a deterministic policy $\pi_\psi$. For a final evaluation, I would need clarification on this (see questions).

Also, the experiment section was not easy to follow. I believe it would be valuable to give an overview at the beginning of the section about how the evaluation is structured and state the goals of each conducted experiment (linked to the motivation of the paper). As an example, I think (please correct me if I am mistaken) that the comparison to SAC was conducted to show that the proposed method can learn a controller that, even with the limitation of a lower frequency, does not compromise much performance. I am a bit unclear as to why the proposed method outperforms SAC (what it does in 4/6 tasks), as to my understanding the strength of the method should be in the specific scenario where hardware is more similar to the human brain. That the proposed method achieves higher performance even in this standard scenario suggests to me rather a lack of appropriate parameter tuning. Also, the setting of the ASL subsection and Online planning section should be stated more clearly (see questions).

In lines 104-105, it is confusing to me to imply that the proposed method in comparison to model-based RL has the advantage that a model is not needed after training. The main purpose of applying model-based RL is to learn a model with the option to replan after training. Model-free RL which could be more similar for this application is not mentioned.

Section 3.2. is about macro actions but it does not even provide in a single sentence an overview of what it is. The authors claim that an advantage of their approach is that it uses the principles of RL. Based on my understanding the concepts of hierarchical reinforcement learning and movement primitives would be very relevant here but are not discussed.


## Minor:
- Line 17 ""not""
- Line 163 $\psi$ should be subscript
- Line 202 Typo in ""Experiemental""
- I find it confusing that parentheses are used for both equations and citations. In most papers, therefore, for citations square brackets are used.
- The plots do not use well the space in the paper (large whitespaces between subplots, the graphs could be made wider), and font sizes between plots differ significantly. Legends are usually integrated into the first subplot only but could be put, e.g., next to the plots to make this information more obvious.
- The Readme of the provided code seems to be incomplete. The code cannot be directly used to reproduce the figures of the paper.

Limitations:
I think so.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces the Hindsight-Sequence-Planner (HSP), a reinforcement learning (RL) model inspired by the brain's ability to achieve precise control using slow neurons. The model aims to mimic human-like sensory and reaction times by leveraging an environmental model for sequence learning. HSP demonstrates competitive performance with fewer observations and actor calls compared to faster RL models. The model is evaluated on various continuous control tasks, showing robust performance even with longer action sequences.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The concept of mimicking brain-like conditions in RL models is innovative and offers a fresh perspective on sequence learning. The integration of temporal recall and sequence learning inspired by neural mechanisms is novel.
2. The use of a temporal recall mechanism allows for fine-tuned action sequence learning despite operating on slower hardware.
3. HSP demonstrates competitive performance across various continuous control tasks, showcasing its robustness and adaptability.
4. The experiments cover a range of continuous control tasks and provide comparisons with state-of-the-art models like Soft Actor-Critic (SAC), highlighting HSP’s efficiency.

Weaknesses:
1. The paper lacks specific numerical performance comparisons to quantify improvements over baseline models.
2. Sections like ""Learning the Model"" and ""Learning Critic"" need further elaboration to highlight their specific contributions and novelty.
3. The broader implications and potential real-world applications of HSP are not fully discussed.
4. There is insufficient discussion on how HSP handles situations with highly inaccurate model predictions.
5. The related work section lacks details on ""Macro-Actions,"" the scalability issues of current methods, and the meaning of ""principles.""

Limitations:
1. The authors mention the reliance on an inaccurate model but could provide more in-depth analysis on how this affects different types of tasks?
2. The scalability of HSP to very large action spaces and high-dimensional state spaces is not thoroughly discussed.
3. Typo Issue: In Section Abstract: demonstrating that it not can achieve comparable performance at ‘human-like’ frequencies by relying on significantly fewer observations and actor calls. -->demonstrating that it can achieve comparable performance at ‘human-like’ frequencies by relying on significantly fewer observations and actor calls.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed HSP, a bio-mimic framework for learning-based control. Motivated by human brains, HSP can deal with observation and computation in different frequencies by making the ""actor"" produce action sequences, similar to the functioning pattern of ganglia and the prefrontal cortex in human brains. HSP employs a model-based training approach to achieve model-free control, resulting in precise behavior despite running on slow hardware. The authors demonstrate the performance of HSP on various continuous control tasks.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper is well structured with clear figures, and the motivation and introduction are interesting, bridging the gap between biological and artificial reinforcement learning systems. The empirical results across various continuous control tasks are impressive as well. Overall, the work presents a promising direction for developing more efficient and adaptable RL algorithms that could have broad implications for robotics and other real-world applications.

Weaknesses:
* One major concern is the performance of the proposed framework. The experiment results do not show a significantly better performance than traditional RL methods, and the performances are worse when $J=16$ in most cases. The latent space variant of HSP shows improvements only in one environment (Walker2d). Further investigation into why this approach doesn't generalize well to other environments would be valuable.
* The novelty of the proposed framework is also questionable. In control literature, especially trajectory optimization control, producing action sequences is commonly used, and a similar technique could be viewed as a variant of model predictive control (MPC). It would be better if the authors could provide a detailed comparison with existing control algorithms (like MPC) in the experiment section. 
* The comparison with model-based online planning is somewhat limited. A more comprehensive comparison with state-of-the-art model-based RL methods would provide better context for HSP's contributions.
* Just a small suggestion - The title of the paper mentions ""hardware,"" so I expect to see some real-world control experiments in the paper. It would be nice if the author could really demonstrate the control performance using ""slow hardware"" like Raspberry Pi or even slower platform.

Limitations:
Limitations are discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
MbbTN9rPzC;"REVIEW 
Summary:
The paper introduces a novel activation function called Quantile Activation (QACT), which aims to improve the robustness of neural networks against various data distortions. The authors propose an end-to-end framework that combines QACT with modified loss functions and quantile classifiers, evaluating their approach on several benchmark datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well organized and clearly written.
1. The paper delivers useful empirical and theoretical insights.
1. The experimental results showcase the superiority of the proposed method.

Weaknesses:
1. The proposed method seems a bit complex, which may lead to over-fitting in scenarios with limited training data.
1. Although the experiments are promising, it remains unclear how well the proposed method would scale to larger datasets or more complex tasks beyond those tested in the paper.
1. I feel that the evaluations are somewhat limited as only a few methods are compared against, lacking the most recent SOTAs. This limits the understanding of the real technical contribution of the proposed method.

Limitations:
The limitations should be discussed in the main paper, yet they are provided in the checklist.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces Quantile Activation (QACT) to enhance classification model robustness against distributional shifts. Unlike traditional classifiers, QACT outputs the relative quantile of a sample in its context distribution, allowing for context-dependent classification. Validated on datasets like CIFAR10C and MNISTC, QACT improves generalization and robustness, outperforming state-of-the-art models like DINOv2 under large distortions. The paper details QACT's implementation and suggests future research directions, including scaling and exploring theoretical links to biological neurons.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper is notable for its originality in proposing a context-aware activation function, demonstrates high quality through extensive validation, and has significant potential for enhancing generalization in classification models. The innovative use of quantile-based activations opens new opportunities for research and applications in machine learning.

Weaknesses:
- Lack of Clarity on Context Dependency

The concept of context dependency being batch-dependent is not clearly explained until the conclusion of the paper. This crucial detail should be introduced and elaborated on earlier to provide a better understanding of the method.

- Unclear Motivation in Introduction

The motivation and fundamental problem discussed in the introduction are not clearly articulated. The authors mention that, unlike NLP where context is considered, general classification systems do not incorporate context. However, in Vision Transformers (ViTs), image patches are treated similarly to words in NLP ""[...The meaning of a word is dependent on the context of the word. However, to our knowledge, this has not been considered for general classification systems.]"". In ViTs, an image patch is considered like a word, and the context comes from the other image patches.

- Insufficient Related Work on Robustness

The paper lacks a comprehensive review of related work concerning robustness to input distortions. Including a discussion of existing methods and how QACT compares or improves upon them would strengthen the paper.

- Limited Comparative Analysis

The comparison with other methods addressing robustness to input distortions is insufficient. The authors primarily compare QACT with DINOv2-Small, which is not a standard model for robustness. Including comparisons with other state-of-the-art methods specifically designed for robustness would provide a more complete evaluation of QACT's performance.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a new activation function called quantile activation (QACT) which outputs the relative quantile of the sample in the context distribution. Furthermore, the paper validates the proposed activation across several experimental settings, and compare it with conventional techniques. They test robustness against distortions, and find that the proposed activation can achieve a significantly higher generalization across distortions than the conventional classifiers, across different architectures.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
First, the authors develop existing approach in calibrating a pre-trained classifier to the level of a neuron. Thus, suitable forward and backward propagation equations required for learning are derived. Second, the authors also show that the extension can produce context dependent outputs at the level of each neuron of the neural network.

Weaknesses:
The writing of the paper meets the standard, but the notations are confusing. Nevertheless, it would be much better if the authors can polish and clarify them. For instance:
1. in line 107, the authors claim that ‘Assign $\mathbf y=1$ whenever $\mathbf y > (1-\tau)^{th}$ quantile of $\mathbf z$’. It seems that $\mathbf z$ is a vector and is impossible to have a vector be larger than a scalar. 
2. The authors write $z_i$ and $\mathbf z_i$ alternatively to mean the same quantity. Similar situations occur when the authors write $z$ and $\mathbf z$ (see line 119, Eqn. (4)), or QACT$(\textbf z)$ and QACT$(\mathbf z)$ (see lines 119 and 124).
3. The authors use bold lowercase letters to represent vectors (e.g., Eq. (1)) and variable distributions (e.g., lines 105, 106). Also, what is the difference between bold lowercase letters and normal lowercase letters?
Further clarifications can increase the readability of the paper.

Limitations:
Please see the questions and weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
nblJSwtdrJ;"REVIEW 
Summary:
The paper introduces Tina, a text-conditioned neural network diffusion model designed for train-once-for-all personalization. Tina utilizes a diffusion transformer model conditioned on task descriptions embedded using a CLIP model. This innovative approach aims to generate personalized models for various end-users and tasks based on text prompts, demonstrating significant generalization capabilities even when trained on relatively small datasets (~1000 samples). The model is evaluated under zero-shot/few-shot image prompts, varying numbers of personalized classes, natural language descriptions, and predicting unseen entities to assess its understanding of world knowledge.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The paper provides a comprehensive explanation of the design and framework of Tina.
- It conducts a detailed ablation study and experiments across different datasets.
- The topic is interesting, and the presentation is clear and easy to understand.
- very detailed and robust comparison with previous works.

Weaknesses:
- The model parameter size in the experiments is too small; larger models are needed to evaluate effectiveness.
- In Table 1, the results of direct fine-tuning should be included.
- We might need an ablation study on the impact of text prompts.
- We might need an ablation study to determine if the model merely memorizes and reproduces parameters.
- Figure 2 requires polishing for better clarity.

Limitations:
The model size is too small in exp.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To generate personalized models for a variety of end-users and tasks via text prompts, this paper introduces Tina, a text-conditioned neural network diffusion model. Tina employs a diffusion transformer model, complemented by a CLIP model to embed task descriptions. Remarkably, Tina demonstrates superior generalization capabilities even on small-scale datasets, performing well both within and outside the distribution of the training data. Furthermore, Tina exhibits robust performance under zero-shot/few-shot image prompts, natural language instructions, and unseen categories.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The method demonstrates excellent generalization, showcasing significant in-distribution and out-of-distribution performance even when trained on small datasets. It also exhibits robust behavior in predicting entities that have not been seen before.

2.Compared to existing text-to-image models (such as stable diffusion), text-to-video models (like Sora), and large language models (such as GPT-4), the concept of Tina, which generates personalized models suitable for specific tasks directly from text descriptions, is quite novel.

3.The experimental process is comprehensive and reliable. The paper conducts comparisons against baselines across multiple datasets, and it also undertakes experiments to validate generalization performance as well as performs ablation studies.

4.The experiments involves multiple datasets to verify the effectiveness of the proposed methods.

Weaknesses:
1.It is better to includes more comprehensive and competitive baselines to show the model’s effectiveness and advance. The two baselines come from one paper published in 2023. As for the experimental setting involves three widely-used datasets, I am wondering whether the experimental results excels or perform similarly to the SOTA performance on some of the three datasets. In other word, is it possible to apply the proposed strategy to some more advanced framework to make the performance similar to the SOTA, which ensure the proposed method have real applications in the real use.

2.The base model is CNN or ResNet in the experiments. Is the proposed method generalized to more advanced framework? Applying the proposed method on more advanced framework and obtain more advance performance indicates that the method has potential to be used in the real life.

3.We suggest providing necessary explanations in the captions of the model framework overview.

Limitations:
The limitations are fine with me.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work introduces Tina, a text-conditioned neural network diffusion model designed for generating personalized models from text prompts. Tina aims to enable efficient personalization by training a generic model once and then customizing it for various end-user tasks using task descriptions. Leveraging a diffusion transformer model and CLIP-based text embeddings, Tina demonstrates the ability to generate models for a wide range of personalized tasks. The approach shows promising results in generalizing to both seen and unseen tasks, achieving state-of-the-art performance in several benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Tina's train-once-for-all approach effectively addresses the need for personalized models without requiring extensive retraining, making it a practical solution for diverse end-user scenarios.
2. The model achieves competitive performance across multiple datasets, demonstrating its robustness and effectiveness in both in-distribution and out-of-distribution tasks.
3. Tina can handle various types of input prompts (text, images) and generalize to unseen classes and tasks, highlighting its versatility and potential for broader applications.

Weaknesses:
1. Some methodological details are sparse, such as the specific configurations and hyperparameters used for training Tina. Providing more granular details could help readers replicate the experiments.
2. The reason for adopting DiT as the weight generation model is not well justified. It would be good to see some results of adopting different kinds of diffusion models.

Limitations:
While limitations are discussed, the manuscript could benefit from a discussion of the scalability of Tina to larger datasets and more complex tasks.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
NMwPKjNTEP;"REVIEW 
Summary:
Unfortunately, the authors begin the manuscript by demonstrating a lack of knowledge about the topic. They claim that deep learning (DL) has been highly successful in the field of brain-computer interfaces (BCI) based on electroencephalogram (EEG) data. However, in reality, the application of deep learning in the BCI or EEG field is limited, and shallow learning with simple hand-engineered features is still the gold standard. Therefore, the paper's claims about the vulnerabilities of machine learning models seem to be more like science fiction and do not meet the standard of the NeurIPS.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Hard to spot any strength as this is an artificial toy example.

Weaknesses:
Lack of connection with real-world problems, especially the BCI and EEG fields, where shallow learning remains gold standards with non-existent vulnerabilities. ML in BCI has been trained for each subject at the bedside.

Limitations:
No application in the real world and a completely trivial problem below conference standards.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents an EEG backdoor for manipulating EEG BCI, called ManiBCI, where the adversary can arbitrarily control the output for any input samples. Experiments conducted on three EEG datasets demonstrate the effectiveness of ManiBCI; which easily bypass existing backdoor defenses.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- A backdoor attack for EEG BCI where the adversary can arbitrarily manipulate which target class the EEG BCI will misclassify without engaging the training stage.
- The use of EEG electrodes and frequencies in EEG backdoor attacks with reinforcement learning.
- Several experiments have been conducted to assess the proposed method.

Weaknesses:
- The proposed methodology is not well described. It mainly based on the application of Fourier transform and reinforcement learning.

Limitations:
Yes. The limitations were addressed in Appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents ManiBCI, a novel backdoor attack method targeting EEG-based brain-computer interface (BCI) systems. ManiBCI leverages a three-stage clean label poisoning approach without needing access to the training phase of the target deep learning models. This method optimally selects EEG electrodes and frequency masks for each class using reinforcement learning. The attack involves injecting these learned masks into the EEG data, leading to high misclassification rates while maintaining the original task's accuracy. Extensive experiments on three EEG datasets demonstrate ManiBCI's effectiveness and robustness. The key contributions of this work are: (1) Introducing a new type of stealthy and effective backdoor attack for EEG data. (2) Proposing a method that can manipulate multiple classes simultaneously without requiring control over the model's training process. (3) Providing experimental evidence of the attack's success across various datasets. This research highlights potential vulnerabilities in EEG-based BCI systems, emphasizing the need for robust defense mechanisms.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* Introduces a novel and stealthy backdoor attack method for EEG-based BCI systems using frequency transform.
* Demonstrates the ability to manipulate multiple target classes without needing access to the model's training phase.
* Provides strong experimental evidence of the method's effectiveness and robustness across multiple EEG datasets.

Weaknesses:
* Standard baselines (fast gradient sign method and universal adversarial perturbation) are not included for comparison [1][2]
* Limited to the datasets used in the experiments, raising questions about generalizability to other EEG datasets or real-world scenarios.
* The practical implementation of the proposed attack might be complex and computationally intensive due to the need for reinforcement learning optimization.

[1] Xiao Zhang and Dongrui Wu. On the vulnerability of CNN classifiers in EEG-based BCIs. IEEE
Transactions on Neural Systems and Rehabilitation Engineering, 27(5):814–825, 2019.
[2] Zihan Liu, Lubin Meng, Xiao Zhang, Weili Fang, and Dongrui Wu. Universal adversarial
perturbations for CNN classifiers in EEG-based BCIs, 2021.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a backdoor attack strategy for EEG, addressing three inherent issues: low quality, task variances, and morphology variances. The authors introduced a three-stage clean label poisoning attack. The proposed algorithm has been evaluated on three EEG datasets, demonstrating its effectiveness and robustness across datasets. This is an interesting work investigating backdoor attacks on EEG, and the customized strategy shows effectiveness in this particular domain. I believe this contribution will be beneficial to the community.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* This is a very interesting work, investigating backdoor attacks on EEG, and the customized strategy shows effectiveness in this particular domain.

* The experiments are relatively sufficient and validate the claimed contributions adequately.

Weaknesses:
* I am not the expertise in BA domain. In terms of general EEG analsyis, one of my main concern is the experiment settings. In normal EEG analysis domain, we usually set inter-subejct and intra-subject settings. I failed to see the calrifications of these experiment settings. Whether this strategy can work across subjects, and generalize on the EEG signals collected from new/unseen subject?

Limitations:
The limitations mentioned by the authors are appreciated.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
IfpNsorodK;"REVIEW 
Summary:
This work proposes a training-free time step-skipping method that can be used with existing ODE solvers for reduced NFE. The method was motivated by two observations: 1) a significant similarity in the model's outputs at time step size during the denoising process and 2) a high resemblance between the denoising process and SGD. The proposed method employed gradient replacement from past time steps and rapidly updated intermediate states inspired by Nesterov momentum. The proposed method yielded promising results.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
- Experimental results look promising with multiple diffusion models on diverse datasets.
- Accelerating diffusion models for sampling is an important issue and this work tried to address it.

Weaknesses:
- There have been a lot of prior works on accelerating diffusion models for sampling. While this manuscript cited many, it still missed important prior works - some of them look quite similar to the proposed method. Thus, the novelty of the proposed method is unclear in the current form of this manuscript. For example, using Nesterov acceleration for fast diffusion models is not really new (e.g., R Li et al., Hessian-Free High-Resolution Nesterov Acceleration For Sampling, ICML 2022). Eq (15) of this work can be seen as a special case of the following prior works such as [R1], DeepCache [28] (using past), [R3] (using three moments or future) or [R2] (using all). Some recent work like [R4] even used partial caching instead of using the whole results. A more theoretically grounded work on using Nesterov momentum for sampling can be found in [R5]. 
[R1] M Xia et al., Towards More Accurate Diffusion Model Acceleration with A Timestep Tuner, CVPR 2024.
[R2] A Pokle et al., Deep Equilibrium Approaches to Diffusion Models, NeurIPS 2022.
[R3] H Guo et al., Gaussian Mixture Solvers for Diffusion Models, NeurIPS 2023.
[R4] F Wimbauer et al., Cache Me if You Can: Accelerating Diffusion Models through Block Caching, CVPR 2023.
[R5] R Li et al., Hessian-Free High-Resolution Nesterov Acceleration For Sampling, ICML 2022.
- A number of acceleration works for diffusion models also investigated the feasibility of the parallel computation. Will the proposed method be parallelized for computation? 
- It is unclear if the proposed method was compared with other methods in terms of computation. Will 1 NFE of the proposed method take the same computation time as 1 NFE of other methods since the proposed method contains multiple evaluations of the neural network as in Eq. (15).
- The notation and explanation are quite confusing, so it is not easy to understand the whole idea as well as the algorithm itself.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new training-free acceleration method for the inference of diffusion probabilistic models. The key components of the presented time-skipping strategy are the use of past and future gradients to eliminate redundant neural function evaluations (NFE). The proposed method is shown effective compared to other training-free acceleration methods, leading to solid performance improvements especially for ODE solvers with less than 10 NFEs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
*    The method is training-free and can complement existing fast ODE solvers
*    The paper is well structured and puts the presented method in the proper context with respect to existing methods
*    The experimental results cover conditional and unconditional settings, showing performance improvements across the board.

Weaknesses:
*    The performance gap compared to training-based methods is still apparent, especially considering the latest distillation techniques resulting in one-step models.
*    The mathematical notations are a bit hard to follow up. I would advise the authors to add a schematic clarifying for a given setting of hyperparameters, which timepoints are being evaluated and which are being skipped.
*    The optimal setting of hyperparameters *k,l* is model/dataset dependent and it is not clear apriori how to set these. Therefore, this requires empirical experimentation which makes it time-consuming to get optimal performance when using the method out-of-the-box.

Limitations:
The authors were upfront about the limitations of their method.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
To accelerate the sampling speed in diffusion models, this paper proposes a training-free denoising method, dubbed PFDiff. 
Concretely, PFDiff employs the gradient from past time steps to update intermediate states, aiming to reduce unnecessary NFEs while correcting for discretization errors.
In this manner, PFDiff enables to improve classic samplers without any training computation.
Importantly, experimental results demonstrate the effectiveness of the proposed PFDiff.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. Reducing discretization errors in diffusion models in a training-free manner is attractive and practical.

2. The motivation of using previous gradients to guide the current sampling direction is intuitively plausible, and the proposed method is technically sound.

3. The presentation is excellent and the figures all are readable.

Weaknesses:
1. In my humble opinion, the theoretical analysis part is naive. Can you provide more explanation about why previous gradients is helpful to guide current sampling direction? Since different noise levels correspond to different gradients, is there any harm in denoising images with the proposed method?

2. Many works investigate using previous gradients to improve sampling speed, so the contribution is limited.

Limitations:
Please see in Weaknesses and Questions. If all of my concerns are addressed, I will improve my score.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes PFDiff, a fast training-free sampler for diffusion models. PFDiff updates the current state with both the past score network evaluation and the future score network evaluation. It can achieve good sample quality with less than 10 NFE. The authors showcase the effectiveness of PFDiff on various pre-trained diffusion models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. With proper tuning, the proposed PFDiff can outperform existing ODE solvers in the low-NFE regime on various datasets. 
2. The authors provide comprehensive technical details about the proposed algorithm.

Weaknesses:
1. Flawed justification for future gradient: The authors' claim that using future gradient information is better than using current gradient information is based on the mean value theorem (lines 164-167 and Appendix B.2). However, this theorem only guarantees the existence of an optimal point within an interval, not its specific location. Therefore, the mean value theorem itself doesn't justify the preference for future gradients. 
2. Missing justification for the approximation: While the authors claim that their approximation is better, there is no theoretical justification for it. The proof in Appendix B.2 assumes that the optimal point is already known, which is not informative. The manuscript will benefit from a further approximation error analysis. 
3. Expensive and case-specific tuning: the proposed method essentially defines a set of candidate points and searches for the optimal point by tuning parameters $k$ and $l$. This tuning process can be computationally expensive and needs to be done for each specific case, limiting its practicality.

Limitations:
The algorithm performance depends heavily on parameters $k$ and $l$ as shown in Table 7.  Optimal values for $k$ and $l$ vary based on the pre-trained model and the number of function evaluations. This necessitates extensive parameter tuning when applying the proposed method in practice.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes PFDiff, a training-free approach for accelerating diffusion models. Motivated by the high similarity of the diffusion network outputs at adjacent timesteps on the sampling trajectory, PFDiff utilizes past and future information for sampling with time-skipping, 
and decreases the number of function evaluations (NFEs) significantly. Experiments on various settings show significant acceleration, especially in the low NFE regime.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
- The method is training-free and can be plug into existing solvers.
- The motivation and overall method seem reasonable.
- The improvements is significant especially in the low NFE regime. State-of-the-art diffusion solvers like UniPC and DPM-Solver-v3 are compared.
- The finding that first-order solver (DDIM), along with PFDiff, can outperform high-order solvers, is intriguing.

Weaknesses:
- The highly concise writing and complex notations might be a bit confusing. Additional illustrations for certain local algorithm procedures can be helpful for understanding the overall idea.
- There are fundamental mistakes in the writing. Eqn. (8) (9) are represented as Euler discretizations of the original PF-ODE. However, both DDIM and the series of DPM-Solvers rely on exponential integrators to transform the PF-ODE into other forms, so that the linear term $x_t$ is cancelled. Though this does not mean the method is wrong, such simplified writing can be misleading. The authors are obligated to correct this, or I will be forced to reject this paper.
- It will be more convincing to include experiments on EDM, the SOTA diffusion model on CIFAR-10 and ImageNet 64x64.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
JkqrZqBO7d;"REVIEW 
Summary:
In pipelined model training, one important issue is to reduce the bubble sizes. 
One stream of work is to use the staleness, where the weight discrepancy is mitigated using stashed weights.
This work tries to reduce the overhead of storing weights with reversible architectures. 
Using the non-stashed updated weights, but with restored inputs to each stage, 
approximated gradients are obtained and parallel training is performed.
This leads to less memory usage on training at the cost of increased communication. 
Training results on resnet variants seem to maintain accuracy.

Soundness:
1: poor

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This is a nice adaptation of reversible architectures to pipelined training. If it works well, there is a potential for becoming a new popular pipelined training method.

- The idea of using reconstructed input instead of stored weights seem to be novel.

Weaknesses:
- Insufficient experiment size: Only compared on three different sizes of resnet. This is far from sufficient, especially with the largest model being resnet50. 

- No comparison on speedup: speedup on the training time is crucial, but the ""memory benefits and training time"" section does not disclose any data. Since the proposed scheme has larger communication, it is crucial to report the number.

- Classification accuracy drop: The final accuracy drops on all three datasets for resnet50. 0.6%p and 0.7%p are huge drops for those models. Given that this is the largest model among the tested ones, it draws a significant concern on whether this technique would work for larger models such as resnet152 or ViTs.

- There is no analysis or proof on why the proposed scheme would work. Why it is a good approximation, or why it is going to converge, etc.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method that combines reversible neural networks and parallel distributed training to enable learning with minimal memory usage, while incurring only slight communication and computation overhead. In this approach, the need for storing intermediate activations in traditional backpropagation is eliminated, thus reducing memory constraints and allowing for higher parallelism on the same device. This new method facilitates efficient learning by providing an innovative solution.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
The problem setup involving reversible architecture and distributed parallel training is intriguing. High memory consumption is a critical issue in learning, and reversible architecture has been proposed to address this problem. It is anticipated that these advantages can be similarly applied to distributed parallel training. Additionally, the paper is very well-written, making the ideas easy to understand. The figures and tables were also judged to be of high quality and well-prepared.

Weaknesses:
The main drawback of this paper is the insufficient experimentation. Although using reversible architecture in distributed training is a novel concept, it appears to be merely a combination of existing ideas. For this paper to have a significant impact, it must demonstrate the advantages and benefits of the proposed idea in an actual distributed learning environment. However, the experiments were conducted using only a single A100 GPU, and there is no demonstration of the performance improvements or limitations of the proposed idea in a real distributed environment. The values presented in the tables do not clearly differentiate from what can be achieved with existing reversible architectures. To improve the completeness of this paper, it is essential to analyze scenarios that necessitate the use of multiple GPUs, such as video applications, large-resolution diffusion, and large language models. The current data fails to effectively explain the benefits of the proposed idea.

Limitations:
Not relevant.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, the author proposes a new alternative algorithm (Parallel End-to-End Training with Reversible Architectures) for regular backpropagation, which significantly enhance the parallelization with a limited overhead compared to regular backpropagation and other alternatives to end-to-end training.
Specifically, the network is split into several stages (one layer or a set of layers) distributed across distinct devices, one batch data is split into several mini-batch data. The first device sequentially accesses the mini-batch data and pass them forward to the next stage until the final stage is reached. The backpropagation is initialized from the final stage to the first stage. It enables a significant parallelization of forward and backward computations across multiple devices.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* This paper is well-organized and easy to follow.

* The background information is very rich and makes it easy for someone who is not familiar with this field to understand the relevant techniques including the technique proposed by this paper.

* The figures about the core technique proposed by authors are very clear, which can help readers understand the technique at a glance.

* The paper evaluates the proposed techniques on multiple datasets and networks.

Weaknesses:
* From the comparison between the proposed method and other techniques from related work, it showcases that the proposed method does not have an overall crushing lead. There exists the method which can achieve higher speed and less time than proposed method with storage increased. 

* The low or even zero storage on proposed method is mainly due to reversible architectures. Maybe authors can extend proposed parallel training method to some non-reversible architectures (need memory storage for intermediate activations), then compare with other SOTA methods.

* It would be great if authors use more distributed devices to get more stages from a network, in this case, the performance of the proposed method is likely to be deeply explored. Because the proposed technique is aimed to deployed on the distributed devices.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose fusing delayed gradient pipeline parallelism with reversible models in order to capture the benefits of the former while mitigating the drawbacks with the latter.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper sets up a pretty compelling combination of ideas. This is a great example of a paper that clearly understands the strengths and weaknesses of two disparate techniques and fits them together like puzzle pieces.

- The paper is clear and methodical in laying out the motivation for the approach. By the time the method is introduced, its seems like the natural and obvious choice. This is good writing.

- The concept is solid. I really *want* to like this idea, since it seems to fit together so well.

Weaknesses:
- While the idea is presented fairly clearly, a lot of the analysis is estimates (S4.2) and generalizations (Tab 1). It's fine for motivating the idea, but not really good enough for proving it works as projected. I'm left wondering how much of this method will actually translate to a scaled-up implementation. (No question that it *was* implemented, but a pipeline-parallel model that doesn't actually pipeline across devices is...not particularly compelling.)

- The paper is a fusion of two ideas, designed to capture the computational performance benefits of pipeline parallelism while using reversible models to mitigate memory scaling. Some estimated results of memory footprint are presented in Table 3. No measured results are presented related to parallelism (timing, utilization, etc.). From this paper, it is not possible to determine whether it has succeeded. This is confused further by the section 4.2: ""Memory benefits and training time"" which does not discuss training time at all. The lack of computational results is fairly damning.

Limitations:
As described in weaknesses. Limitations, like computational performance details, are not well described.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
93qSRpucpN;"REVIEW 
Summary:
The paper proposed RGD, a novel method for integrating classifier guidance into classifier-free guidance diffusion models for solving offline MBO problems. Experiment results and ablation studies validate that the method outperforms state-of-the-art baselines and each proposed component is resonable.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Idea is intuitive and easy to follow

- Motivating example in the introduction makes the reader easy to understand the limitations of the prior method and the advantages of the proposed method

- Strong experiment results and detailed ablation studies make the proposed method more convincing

Weaknesses:
- For the diffusion-based proxy refinement part, it seems that there are several estimations to compute the distance between $p_{\phi}(y\vert \hat{x})$ and $p_{\theta}(y\vert \hat{x})$. Furthermore, it incurs additional hyperparameter $\alpha$, which should be carefully tuned.

Limitations:
There are a few minor comments on the manuscript.

- For figure 2, it seems that $\tilde{s}(x_T, y, \omega)$ should be written as $\tilde{s}(x_T, y, \hat{\omega})$. Furthermore, at first, it makes me confusion that RGD conducts classifer-guidance. However, that misleading part has been resolved after reading the manuscript.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper,  the authors proposed to combine both classifier guidance and classifier-free guidance for offline black-box optimization.  In addition, the authors propose a Proxy Refinement procedure by minimizing KL divergence between the Proxy distribution and diffusion distribution regarding $y$.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.  The paper is well-written and well-organized. 



2.  The paper introduces several refinement procedures to boost the offline optimization performance.  The proposed Diffusion-based Proxy Refinement procedure is interesting.

Weaknesses:
1.  **Technical contribution seems to be incremental**

Employing diffusion models for offline black-box optimization is not new.  The technical contribution of this paper seems to be incremental. The draft extends the paper ""Diffusion Models for Black-Box Optimization"" [1]. However,   detailed discussions about the relationship between the proposed method and the paper [1] are missing.

[1] Siddarth Krishnamoorthy, Satvik Mashkaria, and Aditya Grover. ""Diffusion Models for Black-Box Optimization."" ICML 2023. 


2.  **Part of the technical details are not clear.**

(a) In Equation (12),  the concrete computation procedure of $p_\theta (\hat{\boldsymbol{x}} | y)$ and $p_\theta (\hat{\boldsymbol{x}})$  via diffusion model is not clear. 

(b) The derivation of Equation (10) is not given.   It seems that Equation (10) is from the forward pass of the diffusion model.  However,  the forward pass  (Eq.32-32 in [10]) is regarding the distribution.  And the concrete  $\boldsymbol{x} _ t $ is constructed via the backward pass with  $s_\theta(\boldsymbol{x}_k,k)$ for $k \in T,\cdots, t+1$. 
 In addition,  how to choose $\mu(t)$ and $\sigma(t)$ in Equation (10)  is not clear.  

3.  **The additional proxy training, sample refinement procedure and proxy refinement procedure  increase the computation cost**

The additional proxy training, sample refinement procedure and proxy refinement procedure increase the computation cost. However, the time comparison with baselines is missing. 

4.  **The additional proxy training, sample refinement procedure and proxy refinement procedure bring many additional hyperparameters, which may overfit the offline BBO task**

In the offline BBO tasks,  the offline dataset is provided.  The evaluation is the black-box function value at the generated query at one time. 
The long-term convergence properties and exploration/exploitation balance are not considered.  As a result, there are risks that overfit the evaluation metric for the offline tasks.  The paper Introduces lots of additional hyperparameters, which increases the overfitting risks.

Limitations:
Additional computation cost and overfitting risk may be additional limitations besides the limitations discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a framework called Robust Guided Diffusion for the problem of Offline Black-box Optimization. The key idea is to formulate the solution as conditional generation of high-performance designs using a diffusion model which has explicit guidance from a proxy (surrogate) model. This proxy model is also refined/updated via a proxy-free diffusion procedure. Experimental analysis is shown on multiple tasks from design-bench benchmark.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Overall, I like the paper because it includes two simple changes to an existing approach (DDOM) that shows improved performance and the changes are validated by ablation choices.

Weaknesses:
- One major premise (repeated multiple times in the paper) in the paper is that proxy guidance conditional generation is more robust than updating the design with standard gradient ascent on the proxy. However, it is not immediately clear why this should be true and the justification for this key point is somewhat limited. If true, this will be much bigger insight going beyond black-box optimization. If it is only about the exploration/exploitation balance driven by w, we could also make standard gradient have this property by optimizing a upper/lower confidence bound on the objective. Please describe why this is the case either via some empirical experiment or theoretical insight. Also, in equation 11, we might evaluate the proxy far away from the training data depending on the values of s_\theta(x_t), \sigma(t), \mu(t).

- The related work coverage and corresponding experimental analysis of the paper can be improved. This problem has seen an extensive body of work recently. Please see the references below and discuss/compare them appropriately. Some of them are included in references but not compared in the experiments ([1], [2], [3]):

- [1] Yuan, Ye, et al. ""Importance-aware co-teaching for offline model-based optimization."" Advances in Neural Information Processing Systems 36 (2023).
- [2] Kim, Minsu, et al. ""Bootstrapped training of score-conditioned generator for offline design of biological sequences."" Advances in Neural Information Processing Systems 36 (2023).
- [3] Nguyen, Tung, Sudhanshu Agrawal, and Aditya Grover. ""ExPT: Synthetic pretraining for few-shot experimental design."" Advances in Neural Information Processing Systems 36 (2023).
- [4] Chemingui, Yassine, et al. ""Offline model-based optimization via policy-guided gradient search."" *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 38. No. 10. 2024.
- [5] Yao, Michael S., et al. ""Generative Adversarial Bayesian Optimization for Surrogate Objectives."" arXiv preprint arXiv:2402.06532 (2024).

Limitations:
Please see weaknesses section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a robust guided diffusion framework for offline black-box optimization, combining proxy and proxy-free diffusion for conditional generation. Key improvements include proxy-enhanced sampling and diffusion-based proxy refinement to address out-of-distribution issues. Experiments on the Design-Bench benchmark show the method outperforms existing techniques, validated by ablation studies.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The regularization of the proxy using the diffusion model is interesting. Additionally, optimizing the alpha parameter in an offline manner aligns well with the offline setup, enhancing the method's consistency and applicability.
- Experiments and ablations on four continuous and three discrete tasks validate the effectiveness of the proposed RGD method, showing improved performance and robustness.

Weaknesses:
- The paper lacks comparison with relevant approaches like ICT [1] and TRI-mentoring [2]. Despite referencing the latter in the related work section, it’s overlooked in the results.
- It is unclear why the results without proxy-enhanced sampling still achieve competitive outcomes, surpassing the dataset y_max. This contradicts the claims in lines 40-46. Where does the out-of-distribution (OOD) problem arise then? What is the distribution of the generated 128 candidates with and without the sampling? 
- The BDI reported results are significantly lower than in the original paper, especially for the ANT and TFBIND8 tasks. This also seems to be the case for BONET results. Did the authors change the evaluation setup?


[1]: Importance-aware Co-teaching for Offline Model-based Optimization, https://arxiv.org/abs/2309.11600

[2]: Parallel-mentoring for Offline Model-based Optimization, https://arxiv.org/abs/2309.11592

Limitations:
The authors address the limitations and potential negative impacts in their paper.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new method, named RGD, for Offline Black-box Optimization (BBO). RGD incorporates an improved proxy to guide the previous proxy-free method (i.e. DDOM[4]). Key technical innovations includes (1) improving the robustness of the proxy function against adversarial samples by consistency regularization with the diffusion process; (2) dynamic per-sample reweighting between proxy-guided and proxy-free sampling. Compared to previous approaches, RGD demonstrates superior performance on Design-Bench [3].

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Methodology: RGD integrates forward and reverse approaches for BBO, in a way that they can help with each other (e.g. using forward proxy to guide the reverse sampling and using the diffusion process to improve the forward proxy), which is technically sound and interesting.

Experiment: RGD demonstrates superior performance on Design-Bench, compared to the baselines.

Ablation: Ablations on different components of RGD are provided.

Weaknesses:
The reviewer would prefer some clarifications on the method and the experiments

i) Algorithm 1, Line 4, how to identify the adversarial examples? From Line 187-188, it looks like gradient ascent is utilized to find the x that maximize y, it is unclear to the reviewer that how to determine if the obtained x is an adversarial example

ii) Algorithm 1, Line 7, refine the proxy function via eq 15. It would be best if the author could provide further details on how to optimize eq (15), e.g. number of validation and adversarial samples, number of iterations for the bi-level optimization discussed in Appendix B.

iii) Algorithm 1 Line 13, optimizing \omega. Again, it would be best if the author could provide extra info on how to optimize \omega. From Algorithm 1, it looks like \omega is time dependent and optimized for each time step. How many training iterations are required for each time step. The reviewer also wonder if the obtained \omega are dramatically different between different time steps. 

iv) From Line 257-258, it looks like the baselines shown in Table 1 & 2 were re-implemented. If this is the case, the authors are encouraged to include more implementation details, e.g. the model architecture for the score function, etc. This could help follow-up works to reproduce the reported results. The reviewer also wonders if the source code will be made public.

Limitations:
Limitations have been discussed in the appendix

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
buSEDdP5YX;"REVIEW 
Summary:
The authors present two potential sources of error which can arise when composing sub-sampled DP mechanisms. On one hand, they discuss cases in which the composition of worst-case datasets does not yield the expected result, on the other hand, they disambiguate guarantees for mechanisms with Poisson sampling vs. sampling WOR.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
I appreciate that this paper points out some of the subtleties which arise regarding the distinctions between worst-case databases and dominating pairs and regarding the correct accounting of specific sampling schemes (Poisson/WOR). These subtleties can pass unnoticed, and lead to errors which compromise the privacy of individuals.

Weaknesses:
There is nothing particularly wrong with the paper. The facts stated are valid, and they are interesting, especially since they point out potential sources of confusion. However, none of this is surprising or even particularly novel. Most of this information is implied by earlier work (Zhu et al. in particular), and some of the facts stated here would be better suited as GitHub issues on the relevant accountants, followed by a technical report at a venue like TPDP or Journal of Privacy and Confidentiality. That is to say: I am not against this paper in general, but this is not a NEURIPS paper to me. It is a highly specialised piece of technical writing with a very narrow scope, and is likely to be of interest only to a very small community. I would recommend the authors to submit it to a venue which is better suited to its content.

Limitations:
The discussion on limitations is a bit lacking in my opinion. The authors state (in the checklist) that the ""main limitation is expressed in Conjecture 12"". Not being able to find a counterexample for a proposition is not really what one understands under the term ""limitation"" of a work. In particular, the supposed ""limitation"" is --by the authors' own admission-- easily resolved by just running the accountant on the two curves separately and taking the supremum. I would have much preferred an experimental section where the consequences of the pitfalls stated in the work are actually shown to affect the real-world use of DP-SGD or other mechanisms, and/or to see that specific privacy threats are practically enabled by overlooking these subtleties (e.g. through auditing).

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper examines the discrepancies between privacy accounting methods and their implementations, highlighting several cases where these mismatches lead to incorrect results. Specifically, it compares the noise requirements for achieving privacy guarantees under Poisson sampling versus sampling without replacement, and explores the limitations of worst-case dataset assumptions in subsampled mechanisms. Additionally, the authors address challenges in computing tight differential privacy (DP) bounds under the substitution relation of neighboring datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper addresses an important and timely topic in the field of differential privacy regarding privacy accounting.

- The findings have strong practical implications, potentially preventing unintended privacy breaches.

- The authors' message is well articulated, promoting better practices among DP practitioners.

- Despite critiquing existing methods, the authors maintain a respectful and constructive tone.

Weaknesses:
- The different messages of the paper may be convoluted sometimes, which makes the paper hard to follow.

- No viable technical solutions are provided for the identified issues, which might be a difficult research problem.

Limitations:
The authors discuss the limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The two main contributions of the paper is as follows:

the privacy guarantee of composition of subsampled mechanism may not be defined by worst-case dataset(s) for the underlying mechanism
Poisson subsampling and sampling without replacement may not have similar privacy guarantee.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper studies a very important problem of composition of subsampled privacy mechanism. There has been a lot of work in the recent past that performs a tight privacy accounting. This work is in the line of these works. These accounting results are used in deployment as well to show how much privacy loss has happened during training when using a prescribed noise scale. Based on these bounds, the training is stopped once we have expired the privacy budget. In this regard, their second result is very important because we definitely use subsampling without replacement in DP-SGD.

Weaknesses:
There are some typos, and the result for the gap is shown empirically. I have to state that I have not seen the Appendix so if the authors have a provable guarantee for this gap in the Appendix, please point me that. To me, the selling point of the paper is this result and it should be placed front and center. Most of the results that are given in the form of propositions and lemma are from previous works.

Limitations:
Mostly seem like an empirical study of the composition result.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the notion sampling with replacement for differential privacy. Most of the literature on machine learning with differential privacy benefits from privacy amplification by poisson sampling in the privacy analysis. However, when implementing the mechanisms, engineers ofter use the sub-sampling with replacement as a substitude for poisson sampling, mainly due to efficiency issues. This paper studies the gap between these two settings. Their main contributions are as follows: 

 - Identifying the Problem with DP-SGD Implementations: The authors highlight a critical issue with implementations of (DP-SGD). They argue that many implementations incorrectly assume that Poisson sampling and batch-sampling yield similar privacy guarantees, which is not necessarily true.

- Gap between Batch-Sampling and Poisson Sampling: The paper demonstrates a significant privacy gap between these two sampling methods. They provide an example showing that for certain hyperparameters, Poisson subsampling can result in an ϵ≈1, whereas batch-sampling without replacement can result in an ϵ>10. This discrepancy is critical for privacy accounting in DP-SGD.  Authors compare the privacy guarantees of Poisson subsampling and batch-sampling. They show that the privacy guarantees can differ significantly depending on the sampling technique used. Their analysis reveals that the method of sampling batches (Poisson vs. fixed-size) significantly impacts the resulting privacy guarantees, cautioning against the interchangeable use of different sampling techniques in privacy analysis.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Identifying an important problem with implementation of DP-SGD

Weaknesses:
- I have some concerns about the correctness of the results.

- There is not much technical novelty.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
hiwHaqFXGi;"REVIEW 
Summary:
The paper introduces DiGGR (Disentangled Generative Graph Representation Learning), a self-supervised learning framework that aims to guide graph mask modeling through disentangled latent factors to enhance the disentanglement of learned representations. Extensive experiments across 11 public datasets for node and graph classification tasks demonstrate the framework's effectiveness, significantly outperforming many existing self-supervised methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Innovative Approach: The DiGGR framework innovatively utilizes disentangled latent factors to guide graph mask modeling, a novel contribution in generative graph representation learning that significantly enhances the model's explainability and robustness.
Comprehensive Experiments: The paper conducts extensive experiments on multiple datasets and tasks, showing significant performance improvements over existing methods, thus providing strong empirical support for the proposed approach.

Weaknesses:
Complexity and Scalability: The framework appears computationally complex, which might limit its scalability to very large graphs or real-time applications. Unfortunately, this aspect is not extensively discussed in the paper.
Lack of Theoretical Analysis: While the empirical results are strong, the paper lacks a detailed theoretical analysis of why the disentanglement process improves performance, which could provide deeper insights into the method’s efficacy and limitations.

Limitations:
See Weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a framework called DiGGR, aimed at improving the robustness and explainability of generative graph models by addressing the issue of entangled graph representations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper tells the story in an easy-to-read way, and the whole paper is quite easy to follow.
2. The problem of disentangled learning is a very popular yet important task.
3. The paper conducts comprehensive experiments to evaluate their method.

Weaknesses:
1. Lack of novelty. Graph disentangled learning is not a new task. There are tons of existing methods for disentangled representation learning, such as those maximizing KL divergence or minimizing mutual information between two sets of representations. A lot of related works such as [1], [2], [3] and [4] are not discussed. Also node factorization is not a new idea, such as node clustering in [3].

[1] Disentangled graph collaborative filtering. SIGIR 2020.
[2] Disentangled Graph Convolutional Networks. ICML 2019.
[3] Deep Generative Model for Periodic Graphs. NeurIPS 2022.
[4] Disentangled contrastive learning on graphs. NeurIPS 2021.

2. The motivation of the proposed method is not clear to me. For example, why should we use mask? Also, why the proposed method sticks to GAE, not VGAE or other types of GNN, such as GCN, GIN or GAT?

Limitations:
Yes. Limitations have been discussed in the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work proposes a disentangled generative self-supervised learning method for graphs. The authors introduce a latent factor learning module to capture the heterogeneous factors in the nodes. The proposed method factorizes the graph into factor-specific subgraphs, and jointly trains a disentangled Graph MAE applying distinct masks for each subgraph. Experimental results demonstrate that DiGGR outperforms traditional methods that treat the graph holistically, without accounting for its latent structure.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method first explores a factorization method for generative graph SSL. 
2. The authors provide extensive experimental results and analysis on both node and graph-level tasks to show the improved effectiveness, interpretability, and generalization by using the proposed method.

Weaknesses:
- The computation complexity of the proposed method is quite high. Could the author pride training time comparison to the baseline methods to help us get a sense of the real complexity?
- Could the author provide more insights on how to find an optimal factor number K according to the statistics of diverse datasets? This might be useful for real-world applications.

Limitations:
Yes, the authors discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a self-supervised learning framework DiGGR, aimed at enhancing the disentanglement of learned graph representations. The authors argue that existing generative graph models tend to overlook the entanglement of learned representations, leading to non-robust and non-explainable models. DiGGR addresses this by introducing a latent factor learning module and a disentangled graph masked autoencoder, allowing for factor-wise graph representations. The framework is tested on various benchmarks, demonstrating its effectiveness in outperforming previous self-supervised methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper studies an interesting research problem that is disentangled graph representation learning. This research problem is very hot recently.

2. The model design is easy to understand. The paper provides a detailed explanation of the proposed model.

3. The experiments demonstrate the effectiveness of the model. The performance improvement on some comparisons seems to be significant.

Weaknesses:
1. One of my concerns is from the novelty. I think the model design is a little similar to the works [1-2]. The authors should make more comprehensive discussions to show the differences between them.

2. The experiments ignore some recent or related contrastive baselines [1-4] for comparisons.  The improvements on some datasets seem to be not significant. 

3.  More large-scale benchmarks should also be considered, e.g., OGB. The experimental settings are not very clear for reproducing the results.

[1] Disentangled contrastive learning on graphs. NeurIPS 2021.

[2] Disentangled Graph Contrastive Learning With Independence Promotion. TKDE 2022.

[3] Augmentation-Free Graph Contrastive Learning of Invariant-Discriminative Representations. TNNLS 2023.

[4] MA-GCL: Model Augmentation Tricks for Graph Contrastive Learning. AAAI 2023.

Limitations:
n/a

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
DSVGACQ3sO;"REVIEW 
Summary:
The paper studies the behaviour of amortised (supervised) causal discovery methods based on different training data distributions and its relation to more traditional causal discovery and the related identifiability theory. The authors empirically validate the intuitions about supervised causal discovery and generalisation of supervised learning methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper studies the behaviour of amortised causal discovery methods which have previously been unstudied.
- The empirical insights generally validate the intuition about identifiability and generalisation. Some examples give interesting insights into the identifiability and performance in the case of mixed assumptions.

Weaknesses:
- The paper is a purely empirical study of the generalisation behaviour of supervised causal discovery methods, validating general intuition without thorough novel insights.
- Given the empirical nature of this paper, I'd have expected to see a more thorough comparison, e.g. setting up a leave-one-out generalisation study or more in-depth analyses of the prediction on interesting individual SCMs such as the non-identifiable example or the performance of the prediction from new samples from a training set SCM.

Limitations:
n/a

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores why causal discovery from observational data, particularly with CSIvA, a transformer-based model, can achieve competitive performance despite seemingly avoiding the explicit assumptions that traditional methods make for identifiability. The authors demonstrate that constraints on the training data distribution implicitly define a prior on the test observations. When this prior is well-suited, the underlying model can be identifiable. In other words, prior knowledge of the test distribution is encoded in the training data through constraints on the structural causal model governing data generation.

Additionally, they provide a theoretical basis for training on observations sampled from multiple classes of identifiable SCMs, a strategy that enhances test generalization to a wide range of causal models. They show that training on mixtures of causal models offers an alternative approach that is less reliant on assumptions about the mechanisms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper bridges the gap between existing theoretical results on identifiability and practical observations. More importantly, it moves away from classical causality settings and quite restricted models, shifting towards more mainstream and modern models like transformers. This opens a pathway for causality research to integrate with large language models (LLMs), which represent the state-of-the-art in a wide range of applications.

Weaknesses:
The presentation can be significantly improved. Since the paper aims to offer novel insights, it is crucial to organize the arguments, theoretical results, and experimental findings effectively to support these insights.

Limitations:
n.a.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the level of generalization achievable when training a predictor to classify “X causes Y” vs. “Y causes X” from observational data. Motivated by recent works performing causal discovery using a pretrained transformer model, the works explores which cases result in predictors that generalize to graph-dataset pairs generated from unseen types of SCM models. This is mostly achieved through a set of empirical experiments on synthetic 2-node SCM data. The work derives a corollary of Hoyer et al [7] to argue why training on multiple identifiable classes of synthetic SCM instances may help generalization amortized causal discovery methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper takes a first step towards analysing why amortized causal discovery performs well in practice and often significantly better than classical approaches. This is generally an important direction and of interest to the field.

Weaknesses:
While the motivation of this work is generally well-grounded, the contribution and argument of the work itself have several weaknesses that, in my opinion, do not justify many of the claims made in the abstract, introduction, and throughout the paper.

First, a major aspect of amortized causal discovery with transformers (referenced in the title) is that of solving structure learning tasks in *high dimensions*. Lopez [10] already provide theoretical and empirical analyses of the bivariate case. Recent work showed that this idea can generalize to (very) large systems -- the literature of works on causal discovery with transformers cited in the paper all study significant large-dimensional problems (ranging from 20-100 variables). Despite this, the present paper limits its entire analysis to the bivariate case. Thus, it is misleading to claim the paper “demystifies amortized causal discovery with transformers”. No part of the analysis concerns multivariate causal discovery or transformers. The paper should be upfront and highlight much more clearly what its contributions are beyond Lopez et al [10], which already study the bivariate amortized causal discovery case.

The paper repeatedly states it analyses CSIvA. However, none of the algorithmic components of CSIvA, such as e.g. the auxiliary loss it is trained on or the architecture of the predictive model, are part of the analysis. The loss function studied here (p.3, l. 133) is the same as e.g. used by [13]. Hence, it would be more truthful to claim the analysis concerns general predictors trained on the classification task of X->Y vs X<-V, as in [10].

A major component of causal discovery performance is not only identifiability of the graph from the observational distribution, but also the intractably large search problem incurred by classical score- and constrained-based methods. The question is: do transformers outperform classical methods in large problem sizes because 1) (parts of) the graphs are identifiable to it, or 2) a prediction-based approach is better at finding the identifiable edges in a large system (as opposed to doing a search)? This question motivates amortizing causal discovery in the first place, but the two-variable special case studied here is ill-suited for answering it. Since the work only studies the bivariate case, the title and claims throughout the paper, as well as their ties to the (large-scale) transformer literature have to be recalibrated.

Section 3.2 seems unnecessary. The section only studies the generalization ability of CSIvA, which is no contribution. The takeaways (lines 195-) that “CSIvA generalizes well to test data generated by the same class of SCMs used for training” and that “it struggles when the test data are [from different SCMs]” are obvious and well-studied by CSIva or related works with the same approach. The same applies to the insight that “training […] exclusively on LiNGAM-generated data is equivalent to learning the distribution p(.|D, LiNGAM)”, implying identifiability.

Limitations:
-	The “theoretical result” (Proposition 1) is a simple corollary of Hoyer et al [7]. The paper makes otherwise no theoretical contribution to the problem underlying amortized causal discovery itself.

-	It is unclear whether “randomly initialized MLPs” are sensible nonlinear functions to use for constructing nonlinear mechanisms and non-Gaussian noise distributions. The fact that a few prior works used it is not a good reason. The shape and scale of randomly initialized neural network functions depends heavily on the activation function and weights distribution. The functions in these experiments could be anything from approximately constant or linear to very jumpy. Please provide additional motivation or evidence for why this is a good choice, and what hyperparameters are used, or consider as an alternative, e.g., samples from a GP, which are smooth and have an interpretable length-scale parameter, also in high dimensions.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper conducts an empirical study of the performance of supervised causal discovery methods, its generality, and the learnability vs. causal structure identifiability. The scope is the bivariate case, and with controlled mechanism and noise to establish the SCM for training and testing data. 

In my opinion, this paper gives two findings:

1) a previous claim (Lopez-Paz et al. [10]) said that, by using the supervised learning based approach, the performance of causal discovery can exceed the boundary of identifiability. which is not true

2) by using diverse training data (diverse = diverse mechanisms + diverse noise), supervised based causal discovery can achieve better OOD performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1 - the study of supervised causal learning, especially the DNN-based approach, is timely and important. 

2 - the experiment setup, is a good starting point. To my knowledge, this is the first paper study the performance, boundary of supervised-based causal discovery methods, setting the bivariate case, with the configuration in terms of mechanism + noise is valid.

3 - some findings are interesting, which can potentially benefit the community for further algorithm design.

Weaknesses:
1 - part of the study can be summarized as learnability vs. identifiability, or in my opinion, one question within this category is ""when and how can learnability exceeds the boundary of identifiability?"". in this regard, the current findings are still very limited, need to be further consolidated.
in this regard, a related work [1] is missing, I think it is helpful for this work.

2 - although not explicitly claimed, this paper suggests that ""CSIvA is capable of in-distribution generalization', is this true? or is this true just for bivariate case or generaly applicable?

3 - I suggest to use the term supervised-based approach, or supervised causal learning (SCL), rather than amortized causal discovery, which is more to the point.

4 -  one claim ""we conclude that the post-ANM is generally identifiable, which suggests that the setting of Example 2 is rather artificial""
I disagree. Although the space of all continuous distributions such that the bivariate post-ANM is non-identifiable is contained in a 2-dimensional space, thus it is a submanifold of the entire distribution space, thus its measure is 0. This is only a mathematical claim but lacks real-world relevance. I would argue that the setting of example 2 is quite valid in real-world setting, or the linear gaussian setting, is also commonly adoped in real-world, but had not been discussed in this work.

5 - potential conflict between section 3.3 and 3.4:
3.3 shows that when mixed two training dataset (different setting) together, would significantly compromise the SCL's performance; however, section 3.4 shows that the more diverse of the training data, the more gain on OOD setting.


[1] Dai, H., Ding, R., Jiang, Y., Han, S., & Zhang, D. (2023). Ml4c: Seeing causality through latent vicinity. In Proceedings of the 2023 SIAM International Conference on Data Mining (SDM) (pp. 226-234). Society for Industrial and Applied Mathematics.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
anxYEohntP;"REVIEW 
Summary:
The paper investigates the potential for Large Language Model (LLM) agents to exhibit prosocial behavior through irrational decision-making, paralleling human cognitive biases. It introduces the CogMir framework, which leverages the hallucination properties of LLMs to simulate and assess social intelligence through various cognitive biases. Experimental results demonstrate that LLM agents and humans show high consistency in irrational and prosocial decision-making under uncertain conditions.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Innovative Framework: The introduction of the CogMir framework is a novel approach to studying social intelligence in LLMs by mirroring human cognitive biases.
2. Comprehensive Evaluation: The paper provides a detailed evaluation of multiple cognitive biases, such as Herd Effect, Authority Effect, and Confirmation Bias, among others.
3. Interdisciplinary Approach: Combining insights from social sciences and evolutionary psychology enriches the study and provides a broader context for understanding LLM behavior.

Weaknesses:
1. Why use hallucinations to mirror human cognitive biases? I think more explanation is required.
2. How to manipulate hallucination?
3. Why use all new datasets in experiments? Do existing datasets all don't have the data you want?
4. I don't think the conclusion is interesting.

Limitations:
Please compare with existing multi-agent social system and point out your advantages.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper implements a framework for evaluating LLMs’ social cognitive biases. The social science experiments are automatically collected by LLMs and then verified by humans. The framework includes two communication mode for interaction between multiple humans and multiple LLMs. The experiments include seven LLMs, across seven social science experiments. Results show that most LLMs show cognitive biases in the designed scenarios.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	The paper investigates a relatively underexplored area, which is the social cognitive biases in LLMs. This evaluation plays an important role in assessing the human-like ability of LLMs.
2.	The experiments include seven different models, providing insights into the comparison of their difference in abilities.
3.	The paper develops a framework and collects corresponding data for future use by more LLMs.

Weaknesses:
1.	The paper treats the scenario where LLMs have wrong beliefs (for example, apple is blue) as where LLMs have cognitive bias because of some external influence (observing many others’ choices, authority, etc.) However, there are no experiments showing that the wrong beliefs are caused by the external influence. I mean (though with a pretty low probability), what if without the external influence, LLMs themselves hold the belief that apple is blue? I think a better way is to measure the change of LLMs’ belief towards a concept without and with the external influence.
2.	The paraphrasing may not be a good choice to evaluate rumor chain effect, since continuing paraphrasing a sentence will indeed lower the similarity with the original one, and this is nothing to do with how message spreads in LLM agents. I believe the authors should design a scenario closer to daily communication.
3.	Since the constructed dataset is an essential part in the framework, how do you construct the dataset becomes important. Further explanations are needed about: How do LLMs automatically do the literature search? What is your manual selection criteria about the social science experiments?
4.	Presentation of the paper needs further improvement. There are too many module names in section 3 and readers can easily get confused with these messy concepts. Also, how the modules are organized is not clearly illustrated. A big problem is that some names in Figure 2 cannot match those in texts. For example, is “Mirror Settings” the same as “Environmental Settings?”

Minor suggestions to presentation:

1.	Please be consistent in the terminology. Currently some terms are “presocial” while others are “pre-social.”
2.	It will be better to make the four titles in Fig. 2 the same as introduced in section 3.

Limitations:
I think the author can mention that the current method does not verify LLMs’ original beliefs towards the knowledge in the proposed datasets.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces **CogMir**, a novel framework designed to assess the social intelligence of LLM agents to mirror human cognitive biases. Through an evolutionary sociology perspective, the authors systematically evaluate the social intelligence of LLM agents, revealing their tendencies towards prosocial behaviors and irrational decision-making. The CogMir framework is applied to various cognitive bias scenarios, demonstrating high consistency between LLM agents and human behavior under uncertain conditions. The paper contributes to the understanding of LLM agents' social intelligence and provides a platform for further research.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Important research question: With the rapid development and application of LLM agents, the behavior studies of LLM agents especially under uncertain situations are getting more and more important.
2. Innovative framework: The introduction of CogMir is a significant contribution, offering a new way to evaluate and understand the social intelligence of LLM agents.
3. Open-ended design: CogMir's modular and dynamic design allows for continuous interpretative study and adaptation to future research needs.

Weaknesses:
1. The paper tries to demonstrate *LLM Agents can leverage hallucinations to mirror human cognitive biases*, while the experiments do not show how to measure hallucinations and what role hallucinations play here.
2. Human subjects are included in the experiments, while the recruitment and the details of them are missing. The paper claims LLM agents' behaviors are similar to humans, but there is no quantitative comparison between these two.

Limitations:
The paper includes the study of human-AI interaction but the ethical risks are not clearly stated. Such risks may also limit the research and application of this field.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores the potential of LLM agents to exhibit irrational social intelligence by mirroring human cognitive biases through their hallucination properties. The authors propose CogMir, a modular and dynamic multi-LLM agent framework that utilizes hallucination to assess and enhance social intelligence through cognitive biases.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1: The experiments explicitly compare LLM agent responses with known human cognitive biases, providing valuable insights into the similarities and differences between human and LLM decision-making processes.

S2: CogMir’s modular structure allows for flexibility in configuring experiments and exploring different social scenarios, making it adaptable for various research needs.

S3: CogMir’s open-ended nature encourages collaboration and further research, promoting the development and refinement of LLM agent social intelligence evaluation methodologies.

Weaknesses:
W1: The framework primarily focuses on language-based interactions, neglecting the simulation of non-verbal behaviors and their impact on social intelligence, limiting the scope of the analysis.

W2: The framework primarily focuses on language-based interactions, neglecting the simulation of non-verbal behaviors and their impact on social intelligence, limiting the scope of the analysis.

Limitations:
See limitation

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
HYwfZEhyK4;"REVIEW 
Summary:
Proposed a graph based learnable multi-agent framework. The framework consists of multiple stages : Forwarding: Election (K: Answer agents; R: Reviewer) -> Review -> K Discuss till a final conclusion is reached. Proposed a mechanism to learn the graph connections dynamically. 

The major Contributions Introduced in the paper: (A)  A new swarm intelligence geo-local framework smileGeo; (B) Dynamic learning strategy; (C) A new Geo-dataset (test mainly).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The major strengths of the proposed smileGeo frameworks are: 

 (a) the learnable Graph based communication strategy seems works well empirically. In table 2, authors demonstrated that it helps achieve better acc, but lower average token costs. 

(b) The proposed method is also scalable as shown in table 3. 

(c) Used attention-based GNN to predict optimal connections and optimal election. Also empirically justified the effectiveness of attention based GNN.

(d) Also constructed Simple rules of updating edges(connections) that works well in practice.

Weaknesses:
The major weaknesses are as follows:

(a) Comparisons with baselines seems unfair. 

(b) Missing details of the evaluation setup, metrics, etc.

Limitations:
yes, the authors adequately addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This works proposes a new visual geo-localization framework with multiple LVLM (Large Vision Language Model) agents. The agents communicate with each other to estimate the geo-location of the input image. A dynamic learning strategy is proposed to optimize the communication patterns among agents to improve efficiency. The method is evaluated on the proposed GeoGlobe dataset.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
+ The idea of tacking worldwide city-level geo-localization with multiple LVLM agents is very interesting. 
+ The result is surprisingly good with zero-shot setting, which is even better than powerful close-source models.
+ Detailed comparison with other agent-based methods is provided. The ablation study on the number of agents is also very detailed.
+ The writing is easy to follow.

Weaknesses:
- The authors could make the geo-localization setting more clear in the introduction, for example, the paper focuses on worldwide city-level geo-localization. There are lots of different settings for geo-localization problem and this could be confusing for some researchers.
- This paper provides a comparison with three traditional geo-localization methods, i.e., NetVLAD, GeM, and CosPlace. However, these three methods are either retrieval-based landmark matching methods or fine-grained classification-based place recognition methods. It would be better to provide a direct comparison with worldwide geo-localization method on city-level setting, e.g., [A]. Although I believe LVLM-based method is better at this setting, a comparison can make it more convincing.

[A] Pramanick, Shraman, et al. ""Where in the world is this image? transformer-based geo-localization in the wild."" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.
- There are only two qualitative results in the appendix. Given that the accuracy is over 60%, it should be easy to find successful and failed cases to demonstrate the actual output cases of the proposed methods. It can also better illustrate how multiple agents help the geo-localization process.
- There are also some existing worldwide geo-localization datasets that could be used for more comprehensive evaluation, e.g., IM2GPS3K, YFCC4K.

Limitations:
The authors mentioned the limitations in the checklist.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces smileGeo, a novel framework for visual geo-localization, which involves identifying the geographic location of an image. The authors argue that while Large Vision-Language Models (LVLMs) show promise in this area, their individual performance is limited. SmileGeo leverages the concept of ""swarm intelligence"" by enabling multiple LVLMs to collaborate and refine their location predictions through a multi-stage review process. To enhance efficiency, the framework incorporates a dynamic learning strategy that optimizes the selection of LVLMs for each image. Furthermore, the paper introduces ""GeoGlobe,"" a new dataset designed to evaluate visual geo-localization models in open-world scenarios where many images depict locations not seen during training. Experimental results demonstrate that smileGeo outperforms existing single LVLMs and image retrieval methods, highlighting the effectiveness of collaborative learning for visual geo-localization.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* The idea of using an ensemble of networks/agents for geolocalization is interesting and novel. The authors propose a graph-based social network to enable collaboration between the agents.
* The ability to search the internet and provide the agents with relevant information is interesting and improves the performance on the task of geolocalization.
* The paper proposes GeoGlobe, a new dataset for benchmarking models on the task of geo-localizing landmarks. The dataset could be utilized in future for other learning based geospatial tasks.

Weaknesses:
* The paper only seems to tackle the problem of geolocalizing **landmark images**. While this is a challenging problem, the current literature [1, 2, 3] has already tried to address the problem of geolocalizing arbitrary ground-level images. The latter problem requires learning sophisticated geographic and visual features. I think even searching the internet cannot effectively solve the geolocalization problem for non-landmark images.
* Limited applicability: The framework is built entirely upon the capabilities of different LVLMs (e.g. GPT4, LLaVA, etc). It seems the framework cannot generalize beyond the training data used for training LLMs.
* The work fails to address the practical applications and real-life use cases of the framework. Why do we require such a framework?
* The limitation and failure cases are not adequately mentioned in the paper.

[1] Vivanco Cepeda, Vicente, Gaurav Kumar Nayak, and Mubarak Shah. ""Geoclip: Clip-inspired alignment between locations and images for effective worldwide geo-localization."" Advances in Neural Information Processing Systems 36 (2023).

[2] Haas, Lukas, Michal Skreta, Silas Alberti, and Chelsea Finn. ""Pigeon: Predicting image geolocations."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12893-12902. 2024.

[3] Berton, Gabriele, Carlo Masone, and Barbara Caputo. ""Rethinking visual geo-localization for large-scale applications."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4878-4888. 2022.

Limitations:
Limitations are insufficiently addressed in the paper. The future works mentioned in the conclusion are vague and fail to specify specific future directions for the work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
cVbd7uTvD8;"REVIEW 
Summary:
This paper (SC3D) proposes a single image-to-3D reconstruction method. It combines multi-view diffusion model and a 3D reconstruction model, and uses the 3D reconstruction results as self condition to improve the multi-view generation process. The motivation of the proposed method is to improve the geometric consistency previous single image reconstruction pipeline, namely first generate multi-view images then perform sparse view reconstruction. The core idea proposed in the paper, 3D-aware feedback, is reasonable and also appear in concurrent works IM-3D and VideoMV. Several ablation studies need to be included to prove that the proposed 3D-feedback (including RGB and coordinate maps) are improving the reconstruction quality. Authors also need to justify more about the contribution w.r.t. related work VideoMV. Furthermore, there is still space to improve the readability in the submission.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Major:
- The idea of using 3D reconstruction rendering as condition to improve the geometry consistency of multi-view diffusion models is reasonable.
- The experiments are comprehensive. The results demonstrate that the proposed feedback mechanism is solid in the multi-view reconstruction approach.

Weaknesses:
- Claim about key contributions: the 3D-feedback idea appears already in VideoMV. Since the VideoMV is already available on Arxiv in March, authors need to justify more clearly about the difference and contribution w.r.t. VideoMV.
- lacks generalization results: the method is evaluated on google scan objects, which is standard. However, i am curious to see if the approach generalizes to real world images.
- lacks one ablation: SVD+RGBs feedback, which is missing in Tab. 2 and Tab. 3.
- the readability of the Alg.1 and Alg.2 can be improved. Currently it is too specific and looks like python program. A more abstract algorithm is expected in a scientific paper.
- A typo: in line 213, after comma, ""we"" instead of ""We"" (wrong capitalized ""W"")

Limitations:
- No obvious limitations are found in the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method for 3D asset generation conditioned on a single image. The approach follows the recent trend of a two-stage feed-forward model – first generating multi-view images and then using a sparse-view reconstructor to reconstruct the 3D object (specifically, LGM in this paper). This two-stage model has a significant drawback: the inconsistency of the multi-view generation model may result in an imperfect input for the reconstructor, thus causing quality degradation of the final generated 3D assets.

To address this issue, the authors propose adding a 3D-aware feedback mechanism to improve multi-view consistency and enhance the final reconstructed results. Specifically, a self-conditioned mechanism is introduced, where the output of the reconstruction model is fed into the diffusion model. This output is involved in the diffusion process, leading to better 3D consistency.

Overall, the method seems sound to me.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
(1) The problem definition and the motivation for the project are very clear.

(2) The paper is well-written and easy to understand.

(3) The method seems sound. By adding the rendering results of a reconstructor as input, which present strong multi-view consistency, the diffusion model is also capable of generating multi-view-consistent images.

(4) Table 3 appears reasonable and as expected.

(5) The appendix provides helpful details on training and network architectures, aiding in the reproduction of the results.

Weaknesses:
(1) Some related works lack citation and discussion:
(a) In ""Dmv3d: Denoising Multi-View Diffusion Using 3D Large Reconstruction Model"" [ICLR 2024], the paper uses a similar mechanism (though not entirely the same) by employing a 3D reconstructor as a multi-view image denoiser.
(b) “Carve3D: Improving Multi-View Reconstruction Consistency for Diffusion Models with RL Finetuning” [CVPR 2024] enhances multi-view consistency through RL fine-tuning.

(2) I encourage the authors to provide more visual results to help readers understand and appreciate the diffusion/reconstruction process. For example, could the authors provide some visual results of $\tilde{x}_0$ at different denoising steps?

(3) In the comparisons, although quantitative results are provided, could the authors include some qualitative (visual) comparisons to the baseline methods?

Other minor issues:

(1)	Line 118, The Plucker coordinate should be (d, o x d)

(2)	Line 206, We -> we

(3)	Line 225, meshe -> meshes

Limitations:
(1)	The method is limited to object-level reconstruction with a clean background. Though this is a common limitation in recent related works, I encourage the authors to explore this issue in future work.

(2)	As discussed in the paper, extracting high-quality surface geometry from the Gaussian model remains an open problem. This is an interesting topic for future research.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper observes that the current state-of-the-art image-to-3D generation models consist of two separate parts: generate multi-view images from a single image and run on top the 3D reconstruction. This process has no feedback loop, i.e. the reconstruction does not inform the image generation which in turn leads to a worse quality of reconstruction. They propose a method that builds in a feedback to loop back the feedback of the reconstruction into the diffusion process. They report superior 3D reconstruction quality over the usual two separate step method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The overall idea that drives the paper to provide a link between 3D reconstruction and diffusion at training and inference time is very powerful and novel, and has not been explored in existing text-to-3D papers. I think this is a significant asset of the paper in a crowded area.
2. The results presented seem to improve quite a bit over the existing state-of-the-art for the results shown.

Weaknesses:
1. The presentation of the paper is not clear. 
    - In Fig. 1 the paper is describing an iterative process. Fig. 1 has also no output, but suggests the 3D representation is the output. However, Fig. 2 suggests the (multi-view) images are the final output? Are the two decoders the same? In the paper you are referring to different models G and F. They are not mapped to the figure to get a better picture.
    - Lines 169-172: this is describing the training strategy. That should be moved to the part starting from line 180 where you are actually describing the training strategy.
    - Equation 1: c_skip is not explained
    - The paper has many typos. Especially in part 4, they appear in almost every paragraph.
    - Lines 227-228: This statement seems contradicting: “Directly employing a NeRF-based feed-forward model during the training process significantly reduces training speed due to the computational demands of volumetric rendering.”
    - Replacing the algorithm code with more concise pseudo code may make it much easier for more readers to understand.
2. Comparisons are not very comprehensive
    - None of the methods in Figure 1 are qualitatively compared. 
    - In Figure 3, it seems different views are compared in the first and second column
    - Minor: It would be also really helpful to introduce some visual cues into figure 9 to easier grasp the results.
3. Some claims are not justified
    - Is the section on augmenting the diffusion model with camera control in 3.1 a claimed contribution of the paper? The statement that: “This approach allows for more detailed and accurate 3D rendering, as pixel-specific embedding enhances the model’s ability to handle complex variations in depth and perspective across the video frames.” Is not justified at all, as other types of embeddings are not ablated.

Limitations:
Overall the paper does describe some limitations of the method, but it’s not clear if they are relevant. For example, is using the gaussian splatting method really a limitation in this case? I’d be interested to know how long this method takes (is it much slower than LGM baseline), how computationally intensive it is, or how sensitive it is to the initial generation by the multi-view diffusion model.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes SC3D for the single-image-to-3D generation, which integrates the diffusion-based multi-view generation and Gaussians-based 3D reconstruction through a self-conditioning mechanism. Specifically, during each denoising step, SC3D injects the rendered image and geometric map from the reconstruction model into the denoising process to enhance the multi-view consistency of the multi-view generated images. Experiments on GSO dataset demonstrates its superiority over existing methods mentioned in this paper.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. SC3D integrates multi-view image generation and 3D reconstruction into a single framework, ensuring similar data distribution between the two modules and thereby improving reconstruction quality during the reference process.

2. SC3D proposes a self-conditiond 3D-award feedback mechanism to bridge the multi-view image generation and 3D reconstruction, in which the rendered images and geometric map are injected in the multi-view generation network. Such design makes sense and could improve the consistency of the generated results from the multi-view generation network.

Weaknesses:
1. Lack detailed visual comparisons with baseline methods. The authors only compare SC3D with LGM but do not show results generated from other baselines, making the visual comparison results less convincing.

2. The paper suffers from poor organization. For example, Figure 4 and Figure 5 are not referenced anywhere in the text. The purpose of Figure 6 is confusing, as its caption suggests it shows results from another work, and it is difficult to discern differences among the three rows. Additionally, the paper's typesetting is of poor quality. There are many blank spaces in the text.

Limitations:
Please refer the weaknesses and questions above.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
TKbGTj0YCZ;"REVIEW 
Summary:
The authors propose a method to train a predictive model (a regressor in their experiments) that minimizes an objective comprising the average performance loss across multiple domains (environments) along with a penalty term for the normalized truncated Wasserstein (NTW) distance between the non-conformity score CDFs of each environment and the importance-weighted one used to address covariate shift. Their experimental results demonstrate that the proposed NTW distance objective is correlated with coverage differences due to concept shift and can achieve different tradeoffs with the average prediction residuals, thereby reducing this gap.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Overall, the paper is easy to follow and the reasoning behind the proposed formulation is compelling. The problem that the authors address is relevant. The experimental results show that the proposed NTW distance is capturing the coverage difference due to concept shift.

Weaknesses:
While the motivation behind the regularization is compelling, it is not entirely clear, both empirically and theoretically, what specific benefits it offers over other state-of-the-art approaches that address differences in the non-conformity score distributions. To highlight the advantages of the proposed NTW regularization over simply minimizing the prediction residuals and then applying various post-hoc conformal prediction techniques, I suggest that the authors demonstrate the validity and efficiency of the prediction intervals obtained for different alphas (error levels) on test data. Additionally, they should provide empirical comparisons or some fundamental discussion/theoretical results in relation to the following approaches:

* Split conformal prediction and group conditional split conformal prediction (where each environment is treated as a separate group). The latter can include standard SCP conditioned on each group or an approach such as the one in section 4.1 of [Barber et al. 2020, ""The Limits of Distribution-Free Conditional Predictive Inference""] or BatchGCP/BatchMVP in [Jung et al. 2022, ""Batch Multivalid Conformal Prediction""].
	
* Performance of the covariate shift split conformal prediction, as already discussed in the paper. For example, if this is built on top of a model that minimizes ERM, DRO or V-REx does the proposed approach provide better prediction sets in terms of conditional coverage/validity on the domains.

* An adaptive approach such as the one by [Amoukou and Brunel 2023, Adaptive Conformal Prediction by Reweighing Nonconformity Score].

Providing such comparative results or analysis would significantly strengthen the paper's argument.

I also think the authors should discuss how their work relates to [Barber, Rina Foygel, et al. ""Conformal prediction beyond exchangeability""], where it is suggested that the non-conformity scores should be weighted based on the total variation distance between the source and target distributions. This approach could potentially serve as another baseline to consider, given the distance between the distributions of the non-conformity scores under P and Q(e).

Limitations:
yes, they mention some of the limitations of the proposed work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper ""Robust Conformal Prediction under Joint Distribution Shift"" investigates the problem of predictive inference under the setting where we have both covariate shift and concept shift. The authors propose a conformal prediction-based procedure that accounts for such distribution shifts and illustrate the performance through experiments.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This work provides extensive and thorough experimental results.

Weaknesses:
The paper doesn't read very well as some notations appear without definitions and relevant assumptions are not written clearly. For example:

- It is assumed that the likelihood ratio $dQ/dP$ is known, but this was not clearly stated in the problem setting.
- Some assumptions are not written in advance but are rather introduced when they are needed.

It would be better if the authors could provide sufficient intuition and motivation for their methodology.

Limitations:
I don't think this work has negative societal impacts.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper adresses the issue of conformal prediction under distribution shift with multiple test domains. The goal is to reduce the deviation in coverage caused by different potential distribution shifts across these domains. The paper firstly proposes a way of disentangling a joint distribution shift (shift effects both in covariate and label distributions, which they term ""concept"") by employing weighted conformal prediction (Tibshirani et al, 2019) to address covariate shift, and then quantify the remaining shift in terms of a truncated normalized Wasserstein distance (D-NTW) between the original and weighted conformal score distributions (empirical CDFs). This D-NTW is then used as a regularizer term in a training algorithm to explicitly ensure that coverage deviations are minimized across test domains. Experiments include assessing the correlation between D-NTW and the actual expected coverage difference, which is shown to be high (vs. other distributional distance metrics), and comparing to two multi-test-domain optimization methods on a variety of datasets to show that the coverage difference is lower while not compromising on prediction accuracy.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Addressing the issue of distribution shift in conformal prediction is a relevant problem, particularly for the less explored label shift setting
- Attempts are made to disentangle covariate and label (concept) shifts, which can provide insights into the model's adaptation abilities
- The suggested D-NTW distance metric is well motivated and benchmarked against multiple sensible alternative distance metrics
- An interesting range of datasets from different domains is considered
- The paper is clearly written and good to follow, including Fig I visualizing the procedure (albeit it is somewhat hard to read, especially part (c))

Weaknesses:
My main concerns are w.r.t. practicality and evaluation. A fundamental requirement of the proposed algorithm and D-NTW is the availability of labelled samples from every test domain $Q_{XY}^{(e)}$ in order to obtain the conformal test score distributions $F_{Q^{(e)}}$, from which then both the likelihood ratios for covariate shifts and D-NTW can be explicitly computed. Beyond existing concerns about the practicality of estimating a likelihood $\textit{ratio}$, we now require actually explicitly estimating every test domain distribution, which is prone to much error. Regardless, if we can now explicitly obtain $F_{Q^{(e)}}$ for every test domain, I am wondering why the direct solution is not to just compute a conformal quantile $q^{(e)}$ on the basis of this for every test domain and thus optimally adapt to the existing joint shifts per domain. Perhaps the loss of the disentanglement of shift contributions is the motivation? In general, I find this requirement to require knowing or estimating the test domain distributions on the basis of labelled test domain data and thus the use of extensive density estimation quite limiting, especially if we consider high-dimensional settings. Perhaps this is why the considered experiments are for 1-D regression data only. I was also wondering if the authors were able to make any connections between their proposals and obtainable coverage guarantees on the test domains. They propose a bound on the distance of D-NTW from the expected coverage difference, but perhaps more explicit connections to conformal coverage guarantees of the form in Eq. 3, e.g. by leveraging guarantees from (Tibshirani et al, 2019) and their linear decomposition of shift effects are worth investigating.

In regards to evaluation, I was missing a closer connection to existing conformal methods under shift, and the actual goals of these methods in terms of coverage guarantees on test data. In their comparison to test-domain optimization algorithms I was not surprised that their algorithm performs better on expected coverage difference, since it explicitly $\textit{optimizes}$ for this goal, while the baselines target e.g. variance minimization. It would be more interesting to compare to conformal algorithms for shift such as the mentioned [2,3], showing e.g. that those are not able to fully capture the joint shift or are overly conservative, thus compromising on the metric. For example, I was surprised that [1] was not mentioned, since it explicitly targets label shifts. Similarly, while it is nice that the relative coverage difference is minimized or correlates well with D-NTW, this does not tell explicitly how my conformal methods will now perform on the test domains. It would be nice to also obtain an explicit assessment of the coverage $(1-\alpha)$ on test domains, and the obtained prediction set sizes. Even if target coverage is not satisfied, it would already be a contribution to show that the proposed algorithm achieves better robustness by being closer to target coverage, or smaller set sizes at the same level. 

Minor: multiple typos e.g. L75, Fig I caption, L89, Eq. 2

References
- [1] Podkopaev, Aleksandr, and Aaditya Ramdas. ""Distribution-free uncertainty quantification for classification under label shift."" Uncertainty in artificial intelligence. PMLR, 2021.
- [2] Cauchois, Maxime, et al. ""Robust validation: Confident predictions even when distributions shift."" Journal of the American Statistical Association (2024): 1-66.
- [3] Zou, Xin, and Weiwei Liu. ""Coverage-Guaranteed Prediction Sets for Out-of-Distribution Data."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 15. 2024.

Limitations:
The requirements of the algorithm and density estimations are mentioned, but the limitations of the approach are not explicitly discussed in terms of imposed assumptions on the problem setting. A more through discussion of the practical limitations would be helpful (some of which are inherited e.g. from (Tibshirani et al, 2019) simply by their use of likelihood ratio weights).  A small subsection in sec 6 mentions difficulties of optimization algorithms.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the coverage difference caused by covariate and concept shifts. Authors introduce the Normalized Truncated Wasserstein distance (NTW) as a metric for capturing coverage difference expectation under concept shift by comparing the test and weighted calibration conformal score CDFs. They also develop an end-to-end algorithm called Multi-domain Robust Conformal Prediction (mRCP) to incorporate NTW during training, allowing coverage to approach confidence in all test domains.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. introduced NTW as a metric to capture the coverage gap

2. high correlation between NTW and coverage difference expectation; mRCP can balance residual size and coverage gap

Weaknesses:
1. section 3.1 and 3.2 introduce some important definitions:  while authors provide some explanation, the theoretical understanding of them are very limited

2. simulation:  authors mention the mRCP can achieve a balance between coverage gap and size of residual, further simulations need to be carried out ( I would be interested to see a plot including the avg. coverage vs avg. residual size

Limitations:
Authors mentioned the limitation in the discussion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper tackles the challenge of obtaining conformal predictions that remain robust under distribution shifts. This is an important issue in many machine learning applications where the underlying data distribution may change across the training (or calibration) and test data sets. The authors propose an algorithm that appears promising based on empirical results. However, significant improvements are needed in terms of writing quality, clarity, citation of relevant literature, mathematical rigor, and explanation of the main ideas. Addressing these substantial weaknesses would make the paper more accessible and impactful for the research community.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The problem of robust conformal predictions under distribution shifts is timely and relevant.

- The paper introduces a concrete algorithm that demonstrates promising performance in some practical scenarios.

Weaknesses:
- Writing Quality: The paper is difficult to read and understand, even for experts. Key concepts are not clearly explained, and the text contains numerous awkward or unclear sentences, as well as pervasive grammatical errors and typos. Some sections seem poorly written, possibly by AI, while others could have been significantly improved with better editing.

 - Missing References: Important related works, such as ""Conformal prediction beyond exchangeability"" by Barber et al. (2023), are not discussed, which limits the paper's contextual grounding in existing literature.

- Lack of Statistical/Mathematical Rigor: The mathematical details in the paper are imprecise. Assumptions and approximations are not clearly stated, and there is a frequent confusion between population and sample quantities in key sections.

 - Unclear Core Idea: The main idea of the proposed algorithm, particularly how it handles concept shift through covariate shift adjustments (as in Equation 10), is not clearly articulated and remains confusing.

Limitations:
The paper's main limitations are its poor writing quality and lack of mathematical precision. These issues make it difficult to understand the main ideas and verify the soundness of the proposed method.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
3gZBGBglBf;"REVIEW 
Summary:
The paper highlights how temporal autocorrelations in EEG data can lead to misleadingly high decoding accuracy in brain-computer interface (BCI) tasks. Using a novel approach with a ""watermelon EEG dataset,"" the authors demonstrate that many reported high performances may exploit these autocorrelations rather than genuine neural activity. They propose a unified framework to address this issue across various EEG tasks and recommend improved experimental designs and data splitting strategies to ensure more accurate and reliable results in BCI research.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Novel Problem Formulation: The paper introduces a novel problem formulation by addressing the potential overestimation of decoding performance in EEG-based brain-computer interfaces (BCIs) due to temporal autocorrelations. This is an innovative perspective that has not been extensively explored in prior research.

2. Creative Use of Non-Human Subjects: The use of watermelons as a model to eliminate stimulus-driven neural responses is highly original. This approach allows for the isolation of temporal autocorrelation effects in EEG signals, providing a unique method to investigate the problem.

3. Impact on BCI Research: The findings have significant implications for BCI research, highlighting a critical issue that could affect the validity of many existing studies. By identifying and addressing this pitfall, the paper provides good insight for more accurate and reliable BCI systems.

Weaknesses:
Plz go and check questions.

Limitations:
While the authors recommend avoiding certain data splitting strategies, the practical implications and feasibility of implementing alternative strategies in real-world BCI applications are not fully explored.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper investigates the potential overestimation of decoding accuracy in brain-computer interface (BCI) tasks that utilize EEG signals. The authors address concerns that high reported decoding accuracies may be attributed to the inherent temporal autocorrelation present in EEG signals rather than the actual decoding of neural responses to stimuli. It contributes to the field of BCI by identifying a potential source of bias in decoding performance, providing a novel dataset to study this issue, and emphasizing the need for careful experimental design to ensure the robustness and reliability of BCI systems.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. This article explores the issue of Overestimated Decoding Performance Arising from Temporal Autocorrelations and verifies it through experiments, with both the expressed viewpoint and experimental process offering high enlightenment value to the BCI community.

2. The self-collected Watermelon EEG is interesting. The use of Watermelon EEG dataset to simulate EEG data without neural activity is a good method to isolate the effects of temporal autocorrelations. 

3. The paper provides empirical evidence through experiments that show high decoding accuracies can be achieved even with non-neural datasets, suggesting that reported accuracies in BCI might be influenced by factors other than the models' ability to interpret neural information. The experiment is solid.

Weaknesses:
1. This article only covers image decoding, emotion recognition, and ASAD tasks, and to further substantiate the viewpoint presented in this paper, the use of more other tasks or datasets is recommended.

2. The presentation still needs improvement, such as Figures 1 and 2. Some technical terms may be ambiguous, such as “domain”, and should be given more rigorous and clear definitions.

3. The paper only uses a simple CNN (or some parts of this CNN) for EEG classification. A broader range of model testing (e.g. EEGNet and EEG Conformer) would contribute to enhancing the reliability of the research presented in this paper.

Limitations:
The authors have addressed some limitations.But there are still some questions. Please see the weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors have correctly identified a significant issue of numerous hyperbolic or irreproducible results in EEG decoding or classification tasks. However, their evaluation approach of recording signals from electrodes placed on a watermelon needs correction. The authors are advised to consult the definition of EEG, as a watermelon is not a brain and does not generate any electrical signals. Therefore, the recorded electrical noises, even when amplified using equipment typically used for EEG, do not constitute EEG data. In summary, while the authors' intentions were good, the numerous errors in their approach make it unacceptable for publication at a top conference such as NeurIPS.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
An excellent intention to discuss problems with many overblown EEG decoding publications. Yet the conclusions are obvious and many reputable researchers defend their approaches with leave-one-out-subject evaluations to avoid the obvious issues in training and testing data splitting identified by the authors.

Weaknesses:
There were unacceptable errors in using EEG terms since instead some environmental or amplifier Brownian noises were recorded after placing electrodes on a watermelon, which probably acted as an electromagnetic antenna capturing all possible low-frequency noises in a room. The CNN application with data splitting issues is too basic for NeurIPS.

Limitations:
Watermelon cannot produce EEG, even if an EEG amplifier records some electrical noise.
The presented study thus hardly relates to EEG decoding problems but seems to report on obvious issues in machine learning due to erroneous data splitting into training and testing sets, thus making it too trivial for NeurIPS.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
Authors hypothesise that the high temporal correlation of EEG data contributes to the high BCI decoding accuracies reported in some prior BCI studies. Specifically, the highly questionable data partitioning practice of splitting continuous EEG data with the same label (or subject) across train/test sets. They present a framework to assess the impact of temporal correlation of EEG features on three different BCI decoding tasks applied to independent datasets, human and watermelon (phantom) EEG data. The inclusion of watermelon dataset is to separate the influence of stimulus-driven responses from highly correlated temporal EEG features that is not fully eliminated when using human EEG data. Results based on the standard data partitioning show high BCI decoding performance for the various tasks even when using watermelon EEG data, and performance is significantly reduced to around chance level when the impact of temporal autocorrelation is mitigated with alternative data partitioning schemes.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality**

-	The inclusion of “phantom EEG” recorded from watermelon to disambiguate stimulus-driven neural responses and temporal autocorrelation during the data analysis, which is not fully eliminated with human EEG recordings. 

**Quality**

-	Authors provide a theoretical basis to justify their hypotheses and experiment design plan. 
-	Analysis plan includes data partitioning used in different BCI decoding tasks (image classification, emotion recognition, auditory spatial attention) applied to a different BCI task (speech evoked response).  

**Clarity**

-	The paper is generally well-written. Problem well illustrated in Figure 1.
-	Some areas require clarity (maybe figures?) to better illustrate the different analyses in the framework (for other applications) and results.

**Significance**

-	Highlights the need for more robust experimental design and data partitioning practices in BCI decoding tasks to minimise the impact of inherent temporal correlations of EEG data on performance.
-	The paper demonstrates a limitation of deep learning models (“black box”) in relation to correlation vs. causation.

Weaknesses:
•	Adding the performance of the current framework on the actual datasets (CVPR, DEAP, KUL, if publicly available), as well as additional independent BCI datasets would provide other benchmarks for comparison. 

•	Not sure why there is a need to match number of the subjects in the SparrKULee dataset to that of the WM “subjects”. The objective of the study is to provide a framework for exploring the impact of temporal correlation of EEG features on BCI decoding performance, not directly comparing both datasets. So, it is fine to include data from all subjects in SparrKULee database. 
> To match the number of subjects in the Watermelon EEG Dataset, EEG data from 10 subjects… from the SparrKULee Dataset were used.

Limitations:
Authors acknowledge limitations of their work.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
7Su7gAei1l;"REVIEW 
Summary:
This paper introduces SymmetricDiffusers, a new approach to learning complex distributions. It works by breaking down the problem into simpler steps: learning how to reverse a transformation using deep neural networks. The authors identify a particularly effective method for this reversal step (the riffle shuffle) and provide guidance on choosing the right length for the process based on mathematical properties. Additionally, they propose a more powerful alternative to a common distribution (the generalized Plackett-Luce distribution) and a theoretically sound strategy for improving efficiency (the denoising schedule). Experiments show that SymmetricDiffusers performs extremely well on various tasks, including sorting images, solving puzzles, and optimizing routes.

Soundness:
1: poor

Presentation:
3: good

Contribution:
1: poor

Strengths:
The proposed method in general is interesting and seems effective in the examples studied in this paper.

Also, the problem studied is interesting.

Weaknesses:
My main concern is the proposed method is not the state-of-the-art. A clear approach is to learn invariant features and equivariant group actions using some existing methods like that proposed in Robin Winter, et al. Unsupervised Learning of Group Invariant and
Equivariant Representations, NeurIPS 2022. Such an approach. 

In Robin Winter et al.'s paper, the authors have studied the symmetric group and my understanding is disentangling invariant features and equivariant groups enables the design of flexible diffusion models since you only need to perform diffusion modeling in the invariant latent space. 

I want to see the comparison of the approach proposed in Robin Winter et al's paper. My understanding is that Robin Winter's approach enables building state-of-the-art diffusion models for molecular generation.

Limitations:
My main concern is the proposed method is not the state-of-the-art. A clear approach is to learn invariant features and equivariant group actions using some existing methods like that proposed in Robin Winter, et al. Unsupervised Learning of Group Invariant and
Equivariant Representations, NeurIPS 2022. Such an approach. 

In Robin Winter et al.'s paper, the authors have studied the symmetric group and my understanding is disentangling invariant features and equivariant groups enables the design of flexible diffusion models since you only need to perform diffusion modeling in the invariant latent space. 

I want to see the comparison of the approach proposed in Robin Winter et al's paper. My understanding is that Robin Winter's approach enables building state-of-the-art diffusion models for molecular generation.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
I am unable to review this paper as it lies outside my area of expertise.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I am unable to review this paper as it lies outside my area of expertise.

Weaknesses:
I am unable to review this paper as it lies outside my area of expertise.

Limitations:
I am unable to review this paper as it lies outside my area of expertise.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors aim to create a discrete diffusion model that generates permutations. This model can then be used to solve combinatorial problems including jigsaws and travelling salesman problems. To formulate their model they cover a range of forward shuffling strategies and discuss how to parametrize the reverse transition. During sampling, they also use beam search to find high probability samples. They find their method performs competitively on computational experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The idea of using riffle shuffles to create a corruption process over permutations and then parametrizing the time reversal of this process is novel and interesting. I enjoyed reading the paper. I believe further work will build on this as there are many instances in machine learning where needing to learn permutations crops up.

The paper is quite well written and easy to understand. It is not overloaded with mathematical equations and intuition is given for some concepts.

The experimental results seem promising as it performs on par or better (especially in high dimensions) than previous methods for learning permutations. I appreciate the ablation studies into the greedy search vs beam search and types of shuffle (in the appendix).

Weaknesses:
I think some further clarification is required for the weaknesses of the random transposition and random insertion style of card shuffling. You later say that you can merge steps of the forward process if each individual step does not induce enough mixing and so the stated weakness that these styles of shuffle have slow mixing seems moot.

You mention that you do not have access to $q(X_t | X_0)$, and I think it should also be discussed that $q(X_{t-1} | X_t, X_0)$ is also unavailable since this distribution is used in standard diffusion models to re-write the variational bound in a lower variance form, see Appendix A in https://arxiv.org/pdf/2006.11239 .

You dedicate a lot of space to discussing the various forward noising processes with different shuffling methods, which is quite interesting. However, the ablations with these different styles of shuffle are in the appendix and I think it should be in the main since they have been given such prominence earlier on in the discussion of the method, it is strange they are not included in the main experiments.

I find it difficult to follow the description of the inverse transposition parametrization, there is no intuition given for the functions $\phi$ and $\psi$ nor the functional form of $p_{IT}(\sigma)$. Perhaps this is due to space limitations but since inverse transposition is not in the main experiments (see above point), I think you should either relegate a lot of this to the appendix if you only use the riffle shuffle in practice, or try and shift the wording to properly explain these types of forward and inverse process and have experiments for them in the main.

Limitations:
The authors do a good job of discussing the limitations of various parametrizations of their method.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a discrete diffusion model to learn distribution over the finite symmetric group $S_n$. The forward process is built off of random walks on finite groups (in this case, card shuffles), and the paper learns to reverse this diffusion process with standard discrete diffusion arguments.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Overall, I really like the paper's contributions and presentation.

* The idea is a neat application of the discrete diffusion ideas to an important area. In particular, the structure of $S_n$ is sufficiently different from standard image/text datasets as to necessitate this paper.

* The presentation is very good and the contributions are numerous.

* For the experiments listed, the method seems to provide a very strong improvement over baseline methods. In particular, these other methods are based on fundamentally different technology, so this highlights that discrete diffusion can become a very promising direction here.

Weaknesses:
There are three primary weaknesses. These should all be addressable to some degree, and I'll take any response into consideration when recalibrating my final score.

1. The model proposes to directly learn the reverse transition densities $p_\theta(X_{t - 1}, X_t)$. The issue with doing this for standard diffusion models is that this seems to hurt model training since it increases the variance of training (as, in particular, one must sample the two $X_{t - 1}, X_t$ for training instead of just one $X_t$). As such, most works use the (ultimately equivalent) mean/score-parameterizations [1, 2]. I would want to hear a bit more about if this would be applicable in the $S_n$ case (and training with this parameterization might improve the model) or if this is not possible.

2. (Related to the above). Since most modern discrete diffusion methods are formulated in continuous time, I think the paper would benefit greatly with a discussion about potentially extending the current methods to this realm. In particular, works like [3, 4, 5] have established a working theory for discrete diffusion in continuous time, so it would be beneficial to discuss how the proposed framework might fit into the established theory.

3. The experiments, while showing good results, do not show that the method is particularly scalable, which seems to be a fundamental problem in prior work that was explicitly mentioned in this paper. In particular, it seems that the maximum value of $n$ in $S_n$ is 100. While some discussion is made here that talks about transformer layers, I think large values of $n$ aren't that big of an issue in transformers due to systems like Flash Attention. So, it should be made more clear if this is a fundamental problem with the existing method, or a larger scale example (even toy) should be presented.

[1] https://arxiv.org/abs/2006.11239

[2] https://arxiv.org/abs/2011.13456

[3] https://arxiv.org/abs/2205.14987

[4] https://arxiv.org/abs/2211.16750

[5] https://arxiv.org/abs/2310.16834

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
5KEb1mqZRl;"REVIEW 
Summary:
The authors proposed a novel compression strategy of transformer based trackers. Unlike previous works, it divides the teacher network into multiple segments, each segment corresponds a single transformer layer of student network, then train each student layer separately. It also introduced some training strategies to enhance performance including (progressive) replacement training, prediction guidance and feature mimicking. Such compression framework is insensitive to the change of architecture of teacher network.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. Effectiveness. The experiment results clearly demonstrated significant improvement of inference speed while reserving the majority of tracking accuracy.
2. Flexibility. The proposed compression strategy is insensitive to the change of architecture of tracking models, making it easy to apply on almost any transformer based trackers. The segmentation strategy and the size of student network also supports user customization, which enables the user to design student network according to their unique demands. Such flexibility shows excellent application prospects in end-side scenarios.

Weaknesses:
The detailed strategy of dividing the teacher network is not stated clearly in the paper. Base on the pseudo code provided in page 13, it seems that the segmentation strategy is simply mapping the list of transformer blocks of student network to that of the teacher network base on the lengths of the two lists. This could be too simple.

For example, assume teacher network has 8 transformer blocks in module 1 and 2 blocks in module 2, while student network consists of 2 blocks, then the second student block would have to emulate the last 3 blocks of module 1 and the 2 blocks of module 2, while module 1 and  module 2 might have been trained separately and possess different knowledge. Empirically, this would result in sub-optical performance.

A brief discuss on the divide strategy could help this paper become more informative.

Limitations:
The paper clearly addressed its limitations including inefficient training process and the performance gap between teacher and student network.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to  distill knowledge from larger teacher models into more compact student trackers. Three techniques are proposed: A stage division strategy that segments the transformer layers of the teacher model. Replacement training technique. Prediction guidance and stage-wise feature mimicking. Experiment verifys the effectiveness of the method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The proposed techniques are comprehensive and include a bunch of methods to improve the performance and efficiency of the trackers, 
2.	The experiments are extensive which includes 5 VOT benchmarks. 
3.	The speed is fast when applying the 2 layer tracker variants.

Weaknesses:
1.	The most obvious weakness is that the whole method consists of many distilling techniques, including training strategies, feature mimicking, and loss guidance. It is hard to see the inherent consistency between those techniques. This may harm the generalization ability and transferability of the proposed framework, as the author claims the framework is general. 
2.	The overall method is complex. I am worried about its application to other researchers.
3.	When applied to the Mixformer v2, which has only 2 layers, performance can be improved marginally while speed is unchanged. This may indicate the method's shortcomings. Complex techniques only bring a little improvement.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces CompressTracker, a novel general model compression framework that enhances the efficiency of transformer-based object tracking models. It innovatively segments transformer layers into stages, enabling a more effective emulation of complex teacher models by lightweight student models. The framework incorporates a unique replacement training technique, prediction guidance, and feature mimicking to refine the student model's performance. Extensive experiments demonstrate CompressTracker's effectiveness in significantly speeding up tracking models with minimal loss of accuracy, showcasing its potential for real-time applications on resource-constrained devices.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1）	Innovative Approach: The paper presents a novel compression framework, CompressTracker, which innovatively addresses the challenge of deploying transformer-based trackers on resource-limited devices by significantly reducing model size and computation cost without substantial loss of accuracy.

2）Structural Flexibility: A key advantage of the proposed framework is its structural agnosticism, allowing it to be compatible with any transformer architecture. This flexibility enables the adaptation of CompressTracker to various student model configurations, catering to diverse deployment environments and computational constraints.

3）Efficiency and Performance: The paper demonstrates through extensive experiments that CompressTracker achieves a remarkable balance between inference speed and tracking accuracy. It notably accelerates the tracking process while maintaining high performance levels, as evidenced by the nearly 96% retention of original accuracy with a 2.17× speedup.

Weaknesses:
1）The concept of ""prediction guidance and stage-wise feature mimicking"" and the idea of BEVDistill [1] seem somewhat similar.

2）Despite the model's efficiency in inference, the training process for CompressTracker is relatively inefficient.

3）While the paper shows promising results on certain benchmarks, there may be concerns about how well these findings generalize across different types of tracking tasks and real-world scenarios.

4）The paper does not compare with other model compression techniques, such as knowledge distillation, model quantization, and pruning.

5）According to the results in Table 3, I observed that the outcomes of CompressTracker-2 are inferior to those of MixFormerV2-S. What could be the reason for this?

6）It is necessary to apply compression to other tracking models in order to further validate the efficacy of the CompressTracker presented in this paper.

7）The authors lack a sufficiently comprehensive review of the related work. The authors should give more reasonable related work by carefully introducing the recent approaches to tracking with compression, such as [2].

[1] BEVDistill: Cross-Modal BEV Distillation for Multi-View 3D Object Detection, ICLR 2023.

[2] Distilled Siamese Networks for Visual Tracking, TPAMI 2021.

Limitations:
Please refer to weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors proposed a general model compression framework for efficient Transformer object tracking, named CompressTracker. The method adopts a novel stage partitioning strategy to divide the Transformer layers of the teacher model into different stages, enabling the student model to more effectively simulate each corresponding teacher stage. The authors also designed a unique replacement training technique, which involves randomly replacing specific stages in the student model with specific stages in the teacher model. Replacement training enhances the student model's ability to replicate the behavior of the teacher model. To further force the student model to simulate the teacher model, we combine predictive guidance and staged feature imitation to provide additional supervision during the compression process of the teacher model. The authors conducted a series of experiments to verify the effectiveness and generality of CompressTracker.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The author has clear ideas and the article is easy to understand. He proposes a general compression framework for single object tracking. This method can efficiently compress large object tracking models into small models. The author has conducted a large number of experiments to prove the effectiveness of this method.

Weaknesses:
The font size of the pictures in the article is too small. The author can adjust the font size appropriately to facilitate reading. The training time line in Figure 1a is blocked, resulting in incomplete display. The font size of the tables is inconsistent, for example, the font size of Tables 5, 6, 7, and 8 is too large. The abstract is redundant and can be appropriately deleted.

Limitations:
For lightweight tracking models, the training time is too long. The author can try to find new ways to reduce the time spent on training.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces CompressTracker, a general model compression framework for efficient transformer-based object tracking. CompressTracker divides the teacher model into stages corresponding to student model layers and randomly replaces student stages with teacher stages during training. It also aligns the teacher and student models using prediction guidance and feature mimicking. The framework gradually increases the probability of using student stages throughout training. CompressTracker achieves significant speed improvements while maintaining high accuracy. For example, CompressTracker-4 accelerates OSTrack by 2.17x while preserving 96% of its accuracy on LaSOT.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Versatility: Compatible with various transformer architectures for student models.
- Efficiency: Achieves a good balance between inference speed and tracking accuracy.
- Streamlined training: Offers a single-step, end-to-end training process, simplifying the compression pipeline.

Weaknesses:
- Limited theoretical analysis: The paper focuses on empirical results without providing much theoretical justification for the proposed methods.
- Lack of ablation on some components: Some components of the framework are not thoroughly explored. For instance, the impact of different feature mimicking strategies is not extensively analyzed.
- Performance and Efficiency Trade-off: While CompressTracker maintains high accuracy, there's a slight performance drop compared to the original model. Training time for CompressTracker-4 (with only 4 blocks) exceeds that of the original OSTrack. This trade-off between training efficiency, inference speed, and model performance requires further optimization.
- The core idea of reducing the number of Transformer blocks is not new. Similar approaches have been used in other models like TinyViT[1] and MiniViT[2].


[1] Wu K, Zhang J, Peng H, et al. Tinyvit: Fast pretraining distillation for small vision transformers[C]//European conference on computer vision. Cham: Springer Nature Switzerland, 2022: 68-85.
[2] Zhang J, Peng H, Wu K, et al. Minivit: Compressing vision transformers with weight multiplexing[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 12145-12154.

Limitations:
No

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
nfC1OA6NeE;"REVIEW 
Summary:
This work derives SDEs for adaptive gradient methods and study the role of gradient noise. The analysis starts from theoretically driving the SDE for SignSGD and highlight its significant difference from SGD. The work further generalize the SDE analysis for AdamW and RMSpropW, two popular adaptive optimizers with decoupled weight decay and reveal key properties of weight decay. Finally, the work integrates the derived SDEs with Euler-Maruyama to confirm that the SDEs faithfully track their respective optimizers with various modern neural networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
-The theoretical results are novel. To my knowledge, this is a first SDE analysis for SignSGD with quantitatively accurate descriptions.

-The theoretical analysis reports some novel properties in terms of gradient noise and convergence. These properties are interesting.

-The proofs seem complete and reasonable.

-A useful theory should be quantitatively verifiable. This work definitely make it. The experiments that SDEs fit the empirical results with various optimizers and models are informative and impressive.

Weaknesses:
-It seems that the reported theoretical results and insights cannot directly lead to some theory-inspired and improved methods. This raise a question on the significance of this work.

-While this work did literature review, some important references are still missing, such as [1] on analyzing Adam using SDEs. As weight decay plays a key role in the results, it may be helpful to review recent papers analyzing novel or overlooked properties of weight decay.


Reference:

[1] Xie, Z., Wang, X., Zhang, H., Sato, I., & Sugiyama, M. (2022, June). Adaptive inertia: Disentangling the effects of adaptive learning rate and momentum. In International conference on machine learning (pp. 24430-24459). PMLR.

Limitations:
This work discussed the limitations in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors derive SDE for signSGD and Adam(W). The experiments show that the algorithm will converge toward the limit of the theorem indicates.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The authors propose ""accurate"" SDEs for algorithms Sign-SGD and Adam(W).

Weaknesses:
1. In Remark after Lemma 3.6, the authors claim that Sign-SGD is (almost) linear in $\sigma_{max}$. However, with $\Delta$ either in Phase 2 or Phase 3, there should be $\sigma_{max}^2$ in the final bound.

2. All the stationarity holds when Hessian is the same from $X_0$ to $X_t$ and convergence holds for strongly convex. However, the hessian changes a lot during network training.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper derives SDEs for SignSGD, RMSprop, and Adam.
The analysis offers insights into the convergence speed, stationary distribution, and robustness to heavy-tail noise of adaptive methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The derived SDE for SignSGD exhibits three different phases of the dynamics.

- The analysis reveals the difference between SignSGD and SGD in terms of the asymptotic expected loss, the robustness of noise variance, etc.

- The analysis of AdamW provides insights into the different roles of noise, curvature, and weight decay.

Weaknesses:
Refer to Questions and Limitations.

Limitations:
The SDE for AdamW is limited to quadratic functions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
t8ch1OCvHh;"REVIEW 
Summary:
This paper pays attention to extremely large outliers in LLMs and further investigates the reasons behind these ""attention spikes."" Consequently, the authors propose two methods to enhance the performance of quantized models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The analysis of attention spikes is thorough and comprehensive.

2. The exploration of the relationship between attention spikes and Gated Linear Units (GLU) variants is both interesting and insightful.

Weaknesses:
1. The proposed QFeM method is not hardware-friendly, as it maintains some modules at high precision and cannot directly utilize low-bit INT General Matrix Multiply (GEMM) for activations and weights.

2. The proposed QFeP method bears a strong resemblance to a previously researched method, IntactKV[1], yet lacks a detailed comparative discussion.

3. The experimental settings are limited to W8A8 configurations, which previous research, such as SmoothQuant[2], has shown can nearly achieve lossless quantization for W8A8 models.

4. The authors have not included comparisons with state-of-the-art baselines, such as OmniQuant[3], AffineQuant[4], QLLM[5], and QuaRot[6].


[1]. Liu, Ruikang, et al. ""IntactKV: Improving Large Language Model Quantization by Keeping Pivot Tokens Intact."" arXiv preprint arXiv:2403.01241 (2024).

[2]. Xiao, Guangxuan, et al. ""Smoothquant: Accurate and efficient post-training quantization for large language models."" International Conference on Machine Learning. PMLR, 2023.

[3]. Shao, Wenqi, et al. ""Omniquant: Omnidirectionally calibrated quantization for large language models."" arXiv preprint arXiv:2308.13137 (2023).

[4]. Ma, Yuexiao, et al. ""Affinequant: Affine transformation quantization for large language models."" arXiv preprint arXiv:2403.12544 (2024).

[5]. Liu, Jing, et al. ""Qllm: Accurate and efficient low-bitwidth quantization for large language models."" arXiv preprint arXiv:2310.08041 (2023).

[6]. Ashkboos, Saleh, et al. ""Quarot: Outlier-free 4-bit inference in rotated llms."" arXiv preprint arXiv:2404.00456 (2024).

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper identifies some of the underlying causes for why activation quantization (PTQ) could lead to low performance and suggests some methods to address these issues.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
Please see the “Questions” section.

Weaknesses:
Please see the “Questions” section.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the precision challenges posed by the large language models (LLMs) quantization during inference, specifically focusing on the quantization errors in GLU-based feedforward networks. The authors identify that GLU variants in LLMs cause significant local quantization errors due to excessive activation magnitudes, referred to as activation spikes. They observe that GLU-implemented models have larger spikes than non-GLU-implemented models.  They propose two methods, Quantization-free Module (QFeM) and Quantization-free Prefix (QFeP), to isolate and mitigate these spikes during quantization. QFeM leave some linear layers unquantized (usually those layers that cause large activation spikes in the first several layers), and QFeP introduce an additional prefix before the inference process. Their extensive experiments show that these methods improve quantization performance and are compatible with existing techniques.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The identification of activation spikes in GLU-based LLMs is novel.
2. The paper is well-structured and clear.
3. The QFeP method is novel,  and is somehow similar to the finding of ""sink token"" in StreamLLM [1].

[1] Xiao, Guangxuan, et al. ""Efficient streaming language models with attention sinks."" arXiv preprint arXiv:2309.17453 (2023).

Weaknesses:
1. My major concern is about the baseline of SmoothQuant reported in Table 4. For example, In Table 7 of SmoothQuant's original paper, they report that W8A8 SQ's PPL of Llama-7B on WikiText-2 dataset is 5.515, while the authors report a PPL of 9.907 on the same dataset. Is there a specific reason about this large gap?

2. In Table 3, the improvement brought by the QFeP method does not seem significant, especially when combining with the QFeM method.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces activation quantization methods for GLU-based LLMs, which often face challenges due to activation spikes. To effectively manage these spikes and enable activation quantization using a PTQ-based approach, the paper proposes a Quantization-free Module (QFeM) and a Quantization-free Prefix (QFeP). Specifically, QFeM aims to partially bypass quantization for linear layers where large quantization errors occur. QFeP identifies the prefix that triggers activation spikes and preserves its context as a key-value (KV) cache, preventing the recurrence of activation spikes in subsequent tokens. The paper presents extensive experimental results to compare the accuracy of the quantized models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1) This paper is well organized and easy to understand.
2) The proposed QFeM and QFeP effectively mitigate the impact of activation spikes on activation quantization, preserving the accuracy of LLMs even when activation quantization is applied.
3) The ablation study thoroughly examines the effects of QFeM and QFeP, providing valuable insights.

Weaknesses:
1) The perplexity/accuracy results of the baseline methods deviate from the results reported in previous papers.

2) The paper does not compare its method with the state-of-the-art LLM quantization method [1], which enables W4A4 quantization (partially using 8-bit operations) with a PTQ approach.

[1] Zhao, Yilong, et al. ""Atom: Low-bit quantization for efficient and accurate llm serving."" Proceedings of Machine Learning and Systems 6 (2024): 196-209.

Limitations:
The proposed method is limited to GLU-based LLMs.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
sxZlp9ZoHD;"REVIEW 
Summary:
The paper proposes the Retentive Network (RetNet) as a foundation architecture for large language models. RetNet has a multi-scale retention mechanism with three computation paradigms: parallel, recurrent, and chunkwise recurrent. 
The retention mechanism starts with a recurrent modeling formulation and derives a parallel formulation. It maps input vectors to state vectors recurrently and implements a linear transform to encode sequence information. Then, it makes the projection content-aware by using learnable matrices. The retention layer is defined using these matrices and a complex position embedding, combining causal masking and exponential decay along relative distance. 
It achieves low-cost inference, efficient long-sequence modeling, comparable performance to Transformers, and parallel training. Experimental results show its superiority in language modeling, inference cost, and training throughput.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The RetNet also shows competitive performance in language modeling and knowledge-intensive tasks compared to other Transformer variants and has the potential to replace Transformers for large language models.
2. Achieves significantly better inference efficiency in terms of memory, speed, and latency.

Weaknesses:
1. The paper presents the scaling curves of RetNet and Transformer with model sizes ranging from 1.3B to 6.7B, concluding that RetNet is favorable in terms of size scaling and starts to outperform Transformer when the model size is larger than 2B. However, it does not provide a detailed explanation for this trend. Understanding the underlying reasons for this performance difference with increasing model size could provide more insights into the effectiveness of RetNet and its potential advantages over Transformer.
2. The use of $\gamma$ in the RetNet may appear somewhat heuristic. The paper assigns different $\gamma$ for each head in the multi-scale retention (MSR) module and keeps them fixed among different layers. While this approach is used to achieve certain effects, such as enhancing the non-linearity of the retention layers and improving the model's performance, the specific rationale for choosing these values and the potential impact on the model's behavior could be further explained.

Limitations:
none

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a linear attention model called RetNet for language modeling, which has a linear training complexity and constant inference complexity.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. RetNet has both linear time complexity and constant inference memory complexity. 
2. RetNet has a chunk recurrent form which can be beneficial for speculative decoding.

Weaknesses:
1. The authors introduce a new term called ""Retention,"" but this is essentially the same as Linear Attention without the denominator, which has already been proposed in [1].
2. Lack of comparison with the baselines on open source pretraining data. All the training experiments are conducted on in-house data mixtures, which harms the reproducibility.
3. The paper doesn't compare RetNet with other linear attention model (such as GLA, RWKV, Mamba) on downstream tasks with standard metrics instead of perplexity. Table 2 only include RetNet and Transformer. The efficiency measurment of RetNet+ is absent.
4. The evaluation on MMLU/Qasper is using perplexity but not the widely-used accuracy/F1 metric. The perplexity results don't necessarily mean that the model can make correct choices for the samples in MMLU, and has less guidance for the model's downstream performance.
5. Missing citations: The authors should also cite [1] for the normalization after retention, and discuss the details of the triton implementation of RetNet and its difference from the implementation in the Flash Linear Attention [2] library.

[1] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7025–7041, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics.

[2] Yang, Songlin and Zhang, Yu. FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism. https://github.com/sustcsonglin/flash-linear-attention

Limitations:
No, the authors should have a limitation section to point out the strong assumptions of their approximation of self-attention and relative position embedding.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents Retentive Network (RetNet), a family of efficient models that incorporate exponential decay within a linear attention-like structure. RetNet shares similarities with state-space models and linearized attention, enabling both training parallelism and O(1) inference cost. Additionally, RetNet supports chunk-wise parallel computation for efficient long-sequence training. Experimental results demonstrate RetNet achieves performance comparable to Transformers and outperforms other efficient variants on language modeling and vision tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The structure of RetNet is easy to understand and follow
- RetNet exhibits promising training and inference efficiency, and is able to scale up to 6B.
- Comprehensive evaluation on both language and vision tasks, highlighting its generalizability.

Weaknesses:
- Some experiments could be improved
- Some claims may be misleading
- RetNet's performance lags behind Transformers at smaller model scales, suggesting it might be more demanding in terms of capacity and compute resources for optimal performance. This trade-off should be carefully considered and analyzed.

Limitations:
I didn't see serious problems.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
PlBUSoSUJG;"REVIEW 
Summary:
Disclaimer: I do not have the mathematical background required to check all proofs. It is also my first time reviewing for NeurIPS. However, I have contributions in classical (deep) RL and finishing my PhD.

In this paper, authors use a novel policy parametrization for RL. Namely, the SoftTreeMax policy that replaces the traditional logit values by small horizon trajectories rewards values. The authors claim and prove that SoftTreeMax policy gradient has less variance than traditional policy gradient. They do experiments with PPO on Atari. 

I summarize my review as follows. The idea and work in this paper are novel, strong and well-motivated. However, it is in my opinion poorly presented, and poorly situated compared to existing related work. 

I very slightly lean towards accepting this paper as is but give feedbacks for the authors to increase their score in the following sections.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
Originality: it is the first time that I see someone use look-ahead information in the policy parametrization. I have already seen it done in the critic part of Policy Gradient (PG) methods, e.g. for advantages estimation in PPO using n-step returns with n>1 and generalized advantage estimations.

Contribution: if the proofs of variance reduction as a function of the depth of the tree expansion in the logit values and as a function of the approximation error of the forward models are correct, I believe this work opens a whole new avenue for future work (which, by the way, is missing from the paper).

Weaknesses:
In the following comments, I assume that the techincal results are correct.
 
First weakness: clarity.

The proposed algorithm (PPO with SoftTreeMaxPolicy) makes heavy use of three major branches of reinforcement learning namely policy gradient methods (core of the paper), model-based RL (use of a forward model to compute small trajectories), efficient implementations of tree search (control the exponential cost of look-ahead search). Those components are all central to your work but the paper lacks a summary of how those components work together.  I recommend to summarize those in a schematic (see Questions section).  

Second weakness: baselines and related work.

The related work mentionned in this work does not clearly help to understand the significance of your work. There are well-studied tools reducing the variance in the gradient estimates (n-steps returns and gae-lambda), and some work combining TS and PG. It would have been nice to see comparisons with those work. I recommend the authors to rewrite and extend their related work section (see Questions section). 

REFERENCES.

TS + PG references:
[An Actor-Critic Algorithm Using a Binary Tree Action Selector, Action Guidance with MCTS for Deep Reinforcement Learning, Policy Gradient Algorithms with Monte-Carlo Tree Search for Non-Markov Decision Processes, Policy Gradient Search: Online Planning
and Expert Iteration without Search Trees]

Better sample efficiency and variance reduction:
[Schulman et al., 2015b; Mnih et al., 2016; Munos
et al., 2016; Schulman et al., 2017; Hessel et al., 2018;
Schrittwieser et al., 2020; Wurman et al., 2022; Chebotar
et al., 2023; Schwarzer et al., 2023, Brett Daley, Martha White, 2024]

Limitations:
No limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a model-based online planning method called SoftTreeMax. The method acts as an extension of the softmax, by replacing the logit in softmax with a n-step return. Based on SoftTreeMax, the paper proposes two learning algorithms, C-SoftTreeMax and E-SoftTreeMax. The work includes a mathematical analysis of both algorithms to prove the policy gradient variance is bounded and exponentially decays when the planning horizon becomes longer. Then, the paper provides an empirical test on C-SoftTreeMax to compare the learned policy and the variance with PPO.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
* The method is theoretically sound. The paper provides a detailed theoretical analysis to show the advantage of the decayed policy gradient variance of the proposed algorithm. 

* The paper reports the implementation and computation in detail and ensures reproducibility.

* The paper empirically checks C-SoftTreeMax. The empirical results compare between the performance and the change of variance as the agent learns. Curves clearly suggest the inverse relationship between the performance and the variance, thus supporting the importance of having a method with guaranteed decaying variance.

Weaknesses:
* The main concern I have is the difference between the proposed method and the n-step return. 

    In Section 3, the paper defines the SoftTreeMax logit as 

    $$l_{s,a}(d; \theta) = \gamma^{-d} \left[ \sum^{d-1}_{t=0} \gamma^t r_t + \gamma^d \theta(s_d) \right] .$$

    In the Actor-Critic method, it is normal to use $Q(s,a)$ for the scoring function $\theta$, and learn a policy $\pi(a|s) \propto exp(Q(s,a))$, for example, SAC, a commonly known online learning algorithm, with state-of-the-art performance. In this case, $l_{s,a}(d; \theta)$ can be rewritten to 

    $$ \gamma^{-d} \left[ \sum^{d-1}_{t=0} \gamma^{t-1} r_t + \gamma^d E_a Q(s_d, a_d) \right] .$$
 
    This equation is exactly the same as the equation in n-step TD, except the normalizer $\gamma^{-d}. However, the normalizer can be written as part of the temperature in softmax. 

    In C-SoftTreeMax, the learned policy is written as $\pi^C_{d,\theta} (a|s) \propto \exp [\beta E^{\pi_b} l_{s,a}(d; \theta)] $ in formula (3). 

    Replacing $l_{s,a}$ with the equation in (2), we have $\pi^C_{d,\theta} (a|s) \propto \exp [\beta E^{\pi_b} [\gamma^{-d}  \sum^{d-1}_{t=0} \gamma^t r_t + \gamma^d \theta(s_d) ]] $. 

     As the expectation is taken on $\pi_b$, the normalizer $\gamma^{-d}$ does not depend on $\pi_b$, thus could be moved out of the expectation. Then (3) is changed to $$ \pi^C_{d,\theta} (a|s) \propto \exp [(\beta\gamma^{-d}) \mathbb{E}^{\pi_b} [ \sum^{d-1}_{t=0} \gamma^t r_t + \gamma^d \theta(s_d)) ]]  .$$ 

     Now the temperature becomes $\beta\gamma^{-d}$, which is a tunable parameter, and the expectation term becomes n-step TD when using $Q(s,a)$ for $\theta$.
     Similarly, when using the state value $V(s) = \theta$, the normalizer can be written as part of the temperature, and the expectation term becomes n-step TD.
     In this case, using the proposed method seems to be no different from simply changing the value update from TD(0) to n-step TD. 

* The empirical test seems to be incomplete. The paper proposes two algorithms, C-SoftTreeMax and E-SoftTreeMax. But only C-SoftTreeMax was empirically tested. The paper explains the empirical test for E-SoftTreeMax was left for future work on risk-averse RL. But it does not make sense to me that the empirical test is completely omitted while the method is listed as a new algorithm in the paper. 

* There could be a stronger baseline. Given the similarity between SAC policy ($\pi(a|s) \propto exp(Q(s,a))$) and the policy learned by C-SoftTreeMax ($\pi(a|s) \propto exp(W_\theta(s,a))$), it may worth testing SAC, both 1-step TD and n-step TD in critic update.

* The empirical test could include more seeds. Currently there are only five seeds. The variance calculated based on 5 seeds could be highly inaccurate.

* The empirical test is limited to discrete action space. It is acceptable and understandable to leave continuous control experiments to future work, but still, adding continuous control results could make the paper more convincing.

* The new method introduces a new parameter, the planning length $d$. According to the empirical results (Figures 3, 4, and 5), different $d$ gave very different performances. The best setting is different across tasks. In 3 out of 8 tasks, (NameThisGame, Phoenix, and VideoPinball), longer planning hurts the performance. It is acceptable to say the performance is sensitive to the value of $d$ and leave this parameter as a tunable one, but it could be better to give an indication of how to pick this value when the reader would like to apply this method to a new task.


**Typo:**
* Line 307, “one week The” -> “one week. The”

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new policy parameterization called SoftTreeMax, which can reduce the variance of the stochastic policy gradient. The authors consider finite state and action spaces throughout the paper. They start by taking a softmax tabular policy and replacing the logit $\theta(s,a)$ with a score from a trajectory of horizon $d$ sampled from a behavior policy. They then address the normalization accordingly. Because of the randomness of the trajectory of size $d$, the authors take an expectation with respect to the randomness of the trajectory of size $d$ to well-define the policy parameterization. By taking the expectation before or after the exponent operator, the authors propose two policy parameterizations: C-SoftTreeMax and E-SoftTreeMax. It turns out that under either C-SoftTreeMax or E-SoftTreeMax parameterization, the variance of the stochastic gradient of the value function decays exponentially with the planning horizon $d$. The theoretical findings are well supported by simulations. Finally, the authors propose a parallel GPU-based simulator for the practical implementation of SoftTreeMax. By applying the C-SoftTreeMax parameterization to PPO and adapting $\theta(s)$ with a neural network, as well as using the parallel GPU-based implementation, leads to better performance compared to distributed PPO.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The concept of integrating tree search (planning) directly into policy parameterization is both intriguing and innovative. It demonstrates two key advantages: 1) With the SoftTreeMax parameterization, the variance of the stochastic gradient of the value function decreases exponentially with the planning horizon. 2) It can be easily combined with various policy gradient algorithms, such as REINFORCE and PPO, to enhance their performance.
- Although computing the parameterization is computationally intensive, the authors offer a practical solution through parallel GPU-based simulation.
- SoftTreeMax shows significant potential for extension to infinite or large spaces, making it a promising method for addressing complex problems.

Weaknesses:
- Not really a weakness. Since the SoftTreeMax can be beneficial to different policy gradient methods, I expect to see its improvement applied on some (original) policy gradient methods for the experiments, such as REINFORCE and actor-critic, to demonstrate its generality, which is another strength of the paper.
- Since SoftTreeMax is inspired from tree search, the authors can highlight the difference between the ""tree search"" literature and the ""tree expansion"" used in this paper in order to avoid confusion. The tree search method uses a max to find the best policy, while here the authors use a fixed behavior policy.
- For the tree search literature, the authors can cite Efroni et al. [2018] and Protopapas et Barakat [2024]. In particular, Protopapas et Barakat [2024] combine a variant of PG, the policy mirror descent, with tree search.
- For the theoretical PG literature in Line 312, the authors can cite Yuan et al. [2022], where they provide a general analysis of PG, including softmax tabular parameterization as a special case.

Efroni, Y., Dalal, G., Scherrer, B., and Mannor, S. Beyond the one-step greedy approach in reinforcement learning. In Proceedings of the 35th International Conference on Machine Learning, volume 80 of Proceedings of Machine Learning Research, pp. 1387–1396. PMLR, 10–15 Jul 2018.

Kimon Protopapas, Anas Barakat (2024). Policy Mirror Descent with Lookahead.

Rui Yuan, Robert M. Gower, and Alessandro Lazaric. A general sample complexity analysis of vanilla policy gradient. In Proceedings of The 25th International Conference on Artificial Intelligence and Statistics, volume 151 of Proceedings of Machine Learning Research, pages 3332–3380. PMLR, 28–30 Mar 2022.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a model based policy gradient (PG) algorithm that tries to decrease the variance of the gradient updates in PG methods and thus (hopefully) improving their sample complexity. The proposed approach works by modifying the softmax policy to work not just not with simple score functions (mapping states to real numbers), but rather with the truncated expected discounted return from a state. The resulting policy is called ""SoftTreeMax"". As such, this paper seems a little bit similar to decision time planning (or MCTS type) algorithms.

The paper proposes two variants of SoftTreeMax and analyzes their variance properties; in particular, the results show that as the size of truncation increases, the variance decreases exponentially. The paper also has a theorem which shows that SoftTreeMax can estimate the gradients well even with an approximate model. There are also some experiments on certain Atari games which show the efficacy of SoftTreeMax against the softmax policy using the popular PPO algorithm.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The proposed algorithm seems novel to me in that I have not seen tree search used with policy gradient. As far as I remember, most tree search methods employ value function based RL algorithms (most popular being AlphaZero). In that way, this paper is quite novel. Further, the modification of logits to instead include the expected return seems novel too. (Although, it might be related to the ""soft-Q learning"" papers a little bit; something which could be mentioned in the paper. For instance, see https://arxiv.org/pdf/1704.06440, Eq. 2.)

Quality and Clarity: The paper is well written and has clear theorem statements that are easy to interpret. The setting considered is the popular discounted finite MDP, making the results very easy to follow. The experiments are also conducted using popular benchmarks and thus help the reader understand and compare the proposed method. Another nice thing is that the experimental results clearly align with the theoretical statements (such as the variance reduction with depth) and the benefits in terms of achieving better sample complexity are quite clear. Overall, the paper seems like a logical next step: using policy gradient methods with model based and tree search based methods. (In particular, for continuous action spaces, this would be highly useful and relevant.)

Significance: The paper combines, a very popular class of RL methods - policy gradients, with tree search like planning algorithms. For many difficult problems, particularly those with very large search spaces, Tree search has been shown to be very effective. On the other hand, policy gradient methods are broadly the most popular way of dealing with continuous action RL problems. Therefore, an algorithm combining these two solution methods can have immense applications, especially as AI methods continue to be adopted for solving more and more real-world decision making tasks (robotics being a good representative application area).

Weaknesses:
- The major weaknesses of this paper are not enough discussion on the exact mechanisms for implementing SoftTreeMax. So SoftTreeMax is akin to a policy parameterization (such as softmax). As such, when implementing an RL agent, we need a large number of other details (some details being how is the exploration policy chosen, what optimizer is used to update the gradients, how are the function approximators initialized, is there any processing of the observation features, etc.). These implementation details are important since they can affect the performance of the RL algorithms more than the core idea itself (for instance, recall RainbowDQN https://arxiv.org/abs/1710.02298 and PPO https://arxiv.org/abs/2005.12729).

- Can you guess how SoftTreeMax run on a continuous action space MDP, such as those from Mujoco?

- Does SoftTreeMax solve the problems of Softmax policies? I know it helps with variance, but what about softmax's dependence on the initialization? Or the possibly slow convergence of sotmax PG in MDPs due to an exponential dependence on the state space size (https://arxiv.org/abs/2102.11270)? Do you have any intuition? From the nice discussion in Appendix C.1, it seems that the answer is no. In particular, choosing the behavior policy while generating the tree could be quite important. Maybe some comments on how this policy is chosen in the implementation could be helpful (I saw the high-level comments about controlling for the eigenvalues of transition matrix; but those seem high-level ideas).

- What does SoftTreeMax really do? Is it just an MCTS like decision time planning algorithm that is helped a tiny bit by the theta values in Eq. 2? For instance, what if I do not update the theta values at all (i.e. set learning rate to zero). In that case, would the performance decrease drastically? Related to that, do you have any comments on how it relates with MCTS? I remember Bertsekas (http://web.mit.edu/dimitrib/www/LessonsfromAlphazero.pdf) said that as the depth increases, the utility / importance of the learned values (in this case theta values) in decision making decreases. Would that apply here as well?

- Once the policy is learned, how do you sample from it? I guess, you still need a model to sample from this policy. This seems like a clear disadvantage (although, this is true for all tree search based / decision time planning methods). Maybe mention this as a limitation.

line 53: ""In all tested Atari games, our results outperform the baseline and obtain up to 5x more reward."" --> this is not true? For instance, results on VideoPinball or Breakout show that PPO is pretty much as good. Also, the paper doesn't specify how the hyperparameters were chosen. So maybe PPO with a better choice of hyper-params might outperform SoftTreeMax?

Limitations:
The major limitation seems to be that the paper doesn't provide

1.  a written algorithm - while I understand that this is almost straightforward, just putting a clear algorithm (specifying order of planning/simulation steps, how to take real world actions with SoftTreeMax policy, computing gradients, and updating the policy) would be highly useful.

2. The paper doesn't run experiments on continuous action space environments, which is arguably the more relevant domain for PG methods. (For discrete environments like Atari, we already have a plethora of value function + tree search based methods - like AlphaZero or MuZero.) Further, there are no hyper-parameter settings specified; it would be difficult to reproduce these results without looking at the codebase.


Further, a limitations section is missing
- if the PG is biased, would that affect convergence? In particular, note that Theorem 4.8 shows that the bias would scale with the state space size S (which is generally huge). Maybe setting d = \log(S) / \log(1 / gamma) helps with this and reduces this error to \log(S). But the computational complexity is probably exponential in d, so each gradient update becomes O(S), which is also bad..

Maybe a discussion between these factors would be helpful
---- how does the increase in bias affect the convergence speed (and the quality of the final policy obtained). What bias does the pruning approach introduce?
---- how does the decrease in the variance of gradient update affect the convergence speed (and the quality of the final policy obtained). Does the additional variance (in computing Lemma 4.6) introduced by pruning remove counteract the variance reduction (as given in Theorem 4.4)?
---- how does changing the bias / variance affect d and thus the computational complexity (edited)

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
QsxldAFMFx;"REVIEW 
Summary:
This paper adapts Maia, a group-level model and variant of the AlphaZero model proposed by McIlroy-Young et al. (2020), to function at an individual level. The authors focused on scaling the fit to individual behavior using techniques like LoRA for fine-tuning LLMs. Moreover, the learned embeddings were shown to encode decision-making styles that correspond to individual behaviors. Linear combinations of these embeddings can result in desirable decision-making styles, creating synthetic styles.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This is an interesting paper that treats fitting individual data as a task in a multi-task learning framework. Recent fine-tuning techniques for LLMs have shown promise in adapting a base model, which captures group-level behavior, to individual-level data on a large scale, accommodating hundreds of thousands of individual datasets.

Weaknesses:
The paper lacks a proper benchmark for comparing the accuracy gains from their proposed method. Hypothetically, if the datasets they considered (Chess and Rocket League) exhibit no individual variability in behaviors, meaning all individuals behave identically to the group-level agent, we should expect no accuracy gain whatsoever from adapting a group-level base model to individual-level data. Conversely, if significant variability stems from individual differences, we should expect a substantial gain. Currently, the 0.4% and 4.8% accuracy gains reported in Table 1 are not compared to any benchmark that considers the maximal possible variability that could be extracted from individual-level data. This leaves readers uncertain about the actual gains achieved by the method. It would be beneficial to establish a theoretically optimal benchmark for maximal possible accuracy gains across all datasets.

Limitations:
This paper did not explicitly include a section for Limitation and Future Research.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper explores modelling user behaviour for chess and rocket league using a PEFT-based method, wherein users are modelled using a composition of MHR adapters. The authors evaluate their approach both for predicting which player played a particular game and for predicting the next move of a given player, and find that their MHR-based approach outperforms prior work in both cases. Further analysis shows that the MHR-based approach further allows steering and combining user vectors to produce new styles, and that the vectors are able to capture a wide diversity of strategies.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The approach is simple but intuitive, applying MHR-style adapters to stylometry and exploring the benefits of this approach. The approach does seem to result in improved performance over prior work, and the analysis of the adapters showing that they can be combined to create new styles, or ‘steer’ a user adapter towards a particular style, is interesting, and highlights that using MHR for learning style vectors allows for interesting analyses/applications. The experimental setup seems reasonable, and the authors show their approach works across more than just chess (although chess is the main focus of the work).

Weaknesses:
- The MHR approach does seem to underperform full-finetuning when there is a large number of games available, as shown in Figure 2, and as expected for a PEFT-based method.
- While older stylometry work [1] was able to produce user vectors by just performing inference (albeit not being generative), this approach requires training, and so might be more computationally expensive as a result (PEFT training is cheap, but still requires computing the full backward pass to compute gradients for the adapters). Grounding some of the discussion in section 5.1 with compute estimates (e.g. estimated FLOPs) would be useful.
- The novelty is somewhat limited, as the primary method (MHR adapters, linearly interpolating between adapters) has been explored in prior work. However, I think that the application of this idea to a new domain (user identification/move generation) is still novel and interesting.

[1] R. McIlroy-Young, Y. Wang, S. Sen, J. Kleinberg, and A. Anderson. Detecting individual decision making style: Exploring behavioral stylometry in chess. Advances in Neural Information Process439 ing Systems, 34:24482–24497, 2021.

Limitations:
The authors address the limitations of their approach reasonably.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Authors found a way to obtain player stylometry using BC on a massive amounts of data. Their idea uses multiple LoRAs and routing matrix to obtain a style vector for each player.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Very interesting technical solution and innovative way to use LoRA and routing matrix to obtain player style vectrors using behavioural cloning. 
- I think there is a large potential in using this technique in analyzing large game play databases. Possibly to glean new insights. 
- Potential also exists to combine these ideas to IRL / adversarial imitation learning schemes. It is not clear that which expert demos we should use to show to the IRL algorithm. Should it be just one pleayer, or maybe k-most similar players to some seed player?

Weaknesses:
- Applications as of now are not very compelling. One option would be to take lichess.org data and ask some interesting research question about the player data that has not seen a sufficient answer. Then use this technique to answer it. 
- Another option would be to integrate this technique to some IRL scheme. 
- Authors cite speaker recognition in the related work, but did not study it more closely. In biometric recognition we have essentially similar task as in the present paper. Idea is to turn observed data (audio in speaker recognition) into fixed length vector and then by comparing these vectors obtain downstream recognition. Where the present paper goes wrong is that evaluation metric is identification accuracy, whereas in biometrics it is known that task is verification task and metric is ROC or DET curve and equal error rate (EER) summary statistic. Authors should use it, in addition to identification. 
- Speaker verification also gives nice possibility for extensions to the current work. One idea back in the day was to do Baysian adaptation from the general speaker model to a specific speaker model. These ideas were also presented in the following paper https://arxiv.org/abs/2012.01244

Limitations:
-

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, the authors propose to solve the problem of behavior stylometry, which is to identify the style of a player’s policy in the game, by regarding it as a multi-task learning problem. Each player’s style is a distinct task. Previous methods are either not scalable or not generative, in that they cannot predict the moves of a player given a query set of games played by that player. The authors aim to design a model capable of generating the moves of different players in a scalable manner. To do that, the authors trained a set of Low Rank Adapters (LoRAs) over a base model. The base model is trained with behavior cloning on the whole dataset, and the LoRAs are trained on a specific set of training player dataset separately. Additionally, a routing matrix is trained with LoRAs to specify the distribution over the LoRAs for each player. In this case, the authors claim that the routing matrix is a compact representation of the players’ skills, and encourage the adapters to learn different latent skills while preserved the shared knowledge within the base model. The authors also claim that the routing matrix supports few-shot learning, and induces a series of applicable benefits on stylometry, such as interpreting and manipulating the style of a player.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well written and clearly conveyed.
2. The proposed method achieves convincing results. 
3. Their dataset is partitioned and rotated properly so that they can analyze the inter-player and intra-player consistency of the method, which provides strong convincing for the paper.

Weaknesses:
1. The main concern I have for this paper is the novelty of the proposed method. The base model for Chess is from a previous work called Maia, and trained with existing techniques such as LoRA and Polytropon, either of which is novel to me.
2. The significance of the problem setting, behavior stylometry, is not clear. The authors only have a very short introduction of what it is and its usage in the first section. However, the benefit of modeling accurate behavior stylometry is still vague. For example,  we have very high performance models in chess, even outperforming human players, then why do we need to model the style of human players via behavior stylometry? To me, accurately predict the moves of a player is not the ultimate goal of a game, the ultimate goal is to win the game. How can behavior stylometry help in winning the game? This should be better explained with more introduction and related experiments.

Limitations:
As stated in the Weakness part, the analyze on the significance of behavior stylometry is limited. The authors work on a sub-problem of the game, and didn’t connect it back to the game itself.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
vYmvgxpgwH;"REVIEW 
Summary:
This paper explores compute-optimal inference for large language
models (LLMs), focusing on designing models and strategies that
balance additional inference-time computation with improved
performance. The study evaluates the effectiveness and efficiency of
various inference strategies, including Greedy Search, Majority
Voting, Best-of-N, and Weighted Voting, across different model sizes
(e.g., 7B and 34B) and computational budgets. Experimental results
indicate that smaller models with advanced tree search algorithms can
achieve a Pareto-optimal trade-off, offering significant benefits for
end-device deployment. For example, the Llemma-7B model matches the
accuracy of the Llemma-34B model on the MATH500 dataset while using
half the FLOPs. These findings suggest that smaller models with
sophisticated decoding algorithms can enhance problem-solving accuracy
across various generation tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper focuses on an interesting topic and should be of interest
  to the audience of NeurIPS.
- It considers a comprehensive experimental investigation to confirm
  the claims.
- The proposed tree search algorithm is interesting and seems to
  outperform the competition.

Weaknesses:
- Although the paper offers quite thorough experimental analysis, it
  does not look deep in terms of theoretical ideas (although there are
  2 theorems), which may be a problem for a flagship venue like
  NeurIPS.
- Overall findings on the possibility to train an equally accurate
  model with fewer computational resources do not look surprising.
- The paper would benefit from additional proof-reading as there are a
  large number of typos present.

Limitations:
The paper concentrates on mathematical problem-solving tasks using 7B
and 34B models, with findings potentially not applicable to other
domains. Future research should explore a broader range of model sizes
and different training datasets to better understand compute-optimal
inference in mathematical problem-solving.

I should also say that these limitations have been explicitly
discussed by the authors themselves (so not a criticism).

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents an approach to select an optimal inference strategy for LLMs and empirical analysis on Math problem solving tasks. The main idea is to select an inference strategy based on a computational budget (FLOPs). The underlying policy model samples solutions by generating tokens based on the budget and a ranking model consumes these tokens. A new reward model is developed  to explore the solution space more effectively. The reward acts as a weighted majority function over the solutions.
Experiments are performed on Math problem solving benchmarks. Some of the key insights from the experiments is that a smaller LLM can outperform the larger LLM in terms of using a smaller computational budget while maintaining similar accuracy. They also show that the proposed approach with a smaller budget has comparable accuracy than sampling with a larger budget.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The insights that inference time strategy can compensate for using smaller LLMs in generation seems to be interesting
- The experiments also provide a basis for analyzing scaling properties of inference which can be significant

Weaknesses:
- In terms of the method itself, I was not sure if it is very novel. It seems to be a smaller variation on the tree search methods that search for solutions in the generated space
- In terms of comparisons, I was not sure about the significance of the benchmark, i.e., are there some properties that make the proposed reward reranking more optimal in Llema model specifically (due to the structure of math problems, etc.). In general, since the main contribution of the paper is empirical, I think there should be experiments or discussions different LLMs to make the contribution more significant. 
-Overall, the empirical conclusions seem very tied to the specific benchmarks, so I was a little unsure regarding the significance of the conclusions.

Limitations:
Limitations regarding the datasets are mentioned.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the optimal training configurations of large language models (LLMs) during inference. The proposed inference strategy, REward BAlanced SEarch (REBASE), combines the strengths of Monte Carlo Tree Search (MCTS) with reduced inference costs, resulting in improved performance on math-domain tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper provides a comprehensive overview, i,e, the inference scaling law, of the performance of different sampling strategies under various inference configurations.
2. The novel REBASE inference strategy achieves better downstream task performance under the same computational budget or even less.

Weaknesses:
### Major 

1. Did you take into account the inference cost of the reward model (RM) in your analysis? As the REBASE frequently uses RM to judge the quality of immediate solutions than other sampling strategies, such as, weighted major voting, It's crucial to consider this aspect to provide a holistic view of the efficiency and practicality of your proposed strategy.

2. The base model with post-training techniques such as SFT and RLHF inherently limits the upper bound of performance. It seems that adding more tricks during inference could improve performance, but the marginal effect may result in diminished returns when using models already tuned by the RLHF process. Could you compare the performance gains of REBASE between the base model, the SFT model, and the Chat model? Is the performance gain only significant in models that have not been tuned?

3. In Section 4.2, the observation in ""Scaling law of compute-optimal inference"" indicates that the optimal inference strategy is invariant to the amount of compute but depends on the model size, i.e., the model's inherent capacity. This raises a concern: does the inference strategy significantly improve the model's performance, or does it only take effect in certain scenarios, such as with base models that have not been aligned?

4. The paper focuses solely on the math domain. To strengthen your claims, a more comprehensive evaluation across general domains using widely adopted benchmarks, such as MMLU, SuperGLUE, HumanEval, etc,  is necessary. 

5. There appears to be no significant improvement in the GSM8K datasets than MATH500 dataset. 

### Minor

1. Figures. 2 and 3 are not referenced in the main manuscript. 

2. Figures. 2 and 3 appear to be in draft form and are somewhat vague.

Limitations:
See Weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
Wmodg5UiEd;"REVIEW 
Summary:
The work studies linear contextual dueling bandits with adversarial feedback. In each round $t$ the agent observes a context $x_t$ and chooses two actions $(a_t,b_t)$. The environment generates a binary preference label $\ell_t = \mathbb{I}(a_t > b_t)$. The underlying assumption is that there exists a linear reward function $r(x,a) = \theta_{\star}^{\top}\phi(x,a)$, where $\theta_*$ is a latent $d$-dimensional vector and $\phi$ is a known feature map such that $\|\|\theta_*\|\|_2 \le B$ and $\|\|\phi\|\|_2 \le 1$.

Based on this, the preference $\ell_t$ is a random variable such that
$$
\mathbb{P}\big(a > b \mid x\big) = \sigma\big(r(x,a)-r(x,b)\big)
$$
The link function $\sigma$ is antisymmetric, $\sigma(-z) = 1-\sigma(z)$ and such that $\sigma' \ge \kappa > 0$.

It is further assumed that a nonoblivious adversary may occasionally flip the preference label with the knowlege of $(a_t,b_t)$, where $C_T$ indicates the number of flips in the first $T$ rounds.

The regret is measured according to the following formula
$$
\max_a \sum_t 2r(x_t,a) - \sum_t \Big( r(x_t,a_t) + r(x_t,a_t) \Big)
$$
The main result is a regret bound of the order $d\sqrt{T} + dC$, ignoring logarithmic factors. This bound is tight because $\Omega(d\sqrt{T})$ is the known lower bound without adversarial corruption and $\Omega(dC)$ is shown to be the regret due to the adversarial corruption.

Experiments complete the contribution.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The topic is of adversarial feedback in dueling bandits is interesting and was only studied in a $K$-armed setting.

The regret bound is tight up to log factors

The related work section is complete and accurate.

The experimental section is significant.

Weaknesses:
The main results (upper and lower bounds) appear to be mostly based on combinations of known techniques from previous papers. The originality of the technical contributions is unclear.

The analysis of the $C$ unknown case (Section 5.2) is trivial.

Assumption 3.2 could create bad dependencies in the bounds on $B$ (and $\|\|\theta_{*}\|\|_2$).

The conditions $\sigma' \le 1$ and $\phi_t^{\top}\theta_{*} \le 1$ in the paragraph before (4.3) are not explicitely stated in the assumptions. Moreover, the second condition seem to imply that $B \le 1$.

There is no discussion on the hardness of computing $(a_t,b_t)$.

Limitations:
There is an explicit limiation paragraph in the conclusions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigated the contextual dueling bandits with adversarial feedback, where the adversary can corrupt the binary feedback of the agent to a certain level. A new algorithm named RCDB has been proposed. The key idea lies in the utilization of uncertainty-weight MLE. Regret analysis of RCDB was provided along with some experimental evaluations.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- This paper studies a known problem but with new angle, i.e., the adversary can corrupt the binary feedback of the agent to a certain level. The problem is well motivated. 
- A novel algorithm named RCDB was designed and incorporated uncertainty-dependent weighting into the MLE.
- The theoretical performance of RCDB in terms of regret is provided. 
- Experimental results to validate the performance of RCDB is presented and compared to existing methods.

Weaknesses:
- Assumption 3.1 assumes a linear reward. The reviewer agrees that this is a ""widely-used"" assumption in the recent RLHF literature, but was curious if your framework and regret analysis can be extended without such an assumption? If not, what are the new challenges? Can you comment on this? 
- The construction of the parameter estimator of $\theta$ requires the Taylor expansion. How the $\approx$s in Section 4 impact the regret analysis? 
- The experiments were run for multiple times, however, the variance is not shown in Figure 1. 
- The paper is very dense, and the authors have changed the template a bit, e.g., the space has been largely squeezed throughout the paper.

Limitations:
N/A.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The author proposed an algorithm, coined robust contextual dueling bandits (RCDB) for advarial feedback, using uncertainty-weighted maximum likelihood estimation. The algorithm guarantees $\widetilde{O}(d\sqrt{T}+CT)$.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Their algorithm is not limited to a finite number of arms.

2. Their algorithm considers adversarial attacks based on selected actions (although the maximum number of adversarial attacks is restricted).

Weaknesses:
1. Hard to follow the paper

Limitations:
1. The authors precisely pointed out the limitations of their results, such as the reward function being linear with a known feature map.

2. Additionally, choosing $a_t, b_t$ (by computing argmax) might be infeasible when interaction with the environment needs to be fast.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the Contextual Dueling Bandits from Adversarial Feedback problem, in a linear reward setting. The authors propose an algorithm named robust contextual dueling bandits (RCDB), which is designed based on uncertainty-weighted regression and MLE. The authors prove that the proposed algorithm achieves a nearly optimal regret upper bound that matches the lower bound both in scenarios with and without (C=0) adversarial feedback. Experimental evaluations are provided to validate the theoretical results.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem is well-motivated and important.
2. The paper is well-written.
3. The authors prove a nearly optimal regret upper bound for the proposed algorithm that matches the lower bound with and without (C=0) adversarial feedback. 
4. The authors also conduct some experiments to validate the theoretical results.

Weaknesses:
1. The title may be a little bit misleading, I think the setting of this paper is the adversarial corruption setting, not the setting with completely adversarial feedback. And the setting is the linear reward model, which is not specified in the title.

2. I have not checked the details, but I feel the uncertainty-weighted technique (which is the key to dealing with the corruption in the linear bandits) is mostly based on the previous works, could the authors highlight the technical difficulties in the dueling bandit setting?

Limitations:
No negative societal impact of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
5p57uCUH8k;"REVIEW 
Summary:
This paper formulates the higher-order curve estimation problem as a NODE problem, enabling effective and accurate solutions with standard ODE solvers.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is well-written and structurally organised.

Weaknesses:
Reference formats are not consistent.

Limitations:
As described above

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper mainly addressed the problem of insufficient data for low-light enhancement. Specifically, it proposed CLODE , which employs Neural Ordinary Differential Equations to learn the continuous dynamics of the latent image for the first time. The experiments demonstrate the CLODE performs better than other unsupervised learning methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
+ This is the first attempt to formulate the higher-order curve estimation problem as a NODE problem.
+ CLODE can offer user controllability, enabling users to manually adjust exposure.

Weaknesses:
- Details of User Controllable Design. Despite the better result with use control, detail of the users is missing. For example, the number of volunteers, and whether they are banned from the ground truth image before they adjust the output image. Also, involving human feedback bring much more time in the inference stage.
- In Sec. 3.3 Inference Process, the relationship between the output image IT and noise-free image is questioned. Each iteration includes a noise removal module, yet the output image still contains some noise, contradicting the expectation of a noise-free result in the model.
- Experimental Setup: The experimental setting described in [1] seems more suitable for unsupervised methods. Using only a single dataset for training in this study does not adequately reflect the advantages of the proposed method. A specific analysis comparing and justifying the differences in experimental setups is necessary.
- Model Iteration Selection in ""CLODE+"" (Table 2): The manual operation required to select the iteration step raises concerns. How is this value determined to ensure suitable results? This approach appears more suited to image retouching tasks than enhancement.
- Concern about the fair comparison with previous methods. This paper uses 5 different losses. I wonder whether only part of them is used in previous methods, are the proposed method align with previous methods? For example, some Retinex-based method does not explicitly consider the impact of noise, and they do not have Noise Removal process. Does CLODE still outperform other methods without noise removal? More ablation experiments are needed for thorough explanation.
- Effectiveness of Noise Removal Module: In the first toy scene in Figure 4, as well as Figures 7 and 8, there is noticeable noise residue and some degree of color distortion, which casts doubt on the effectiveness of CLODE and its noise removal module for low-light enhancement.
- More explanation of the superiority of CLODE. Can author provide clearer explanation of the mechanism? For example, in Figure 9 of Supp material, is the better results comes from the more iterations, or more iterations at the early stage, where the estimation is harder?

[1] Learning a Simple Low-light Image Enhancer from Paired Low-light Instances

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This manuscript introduces CLODE, which learns low-light image enhancement using neural ordinary differential equations (NODE). The key innovation lies in formulating the higher-order curve estimation problem as a NODE problem. Experimental results show that the proposed approach outperforms state-of-the-art unsupervised counterparts across several benchmarks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.	The paper is easy to follow.
2.	Using neural ordinary differential equations to address the iterative curve-adjustment update process shows better performance.

Weaknesses:
1.	The novelty is limited, and the technical contribution is incremental. Apart from formulating the curve estimation as a NODE problem, the paper lacks innovation，which is the main reason why I gave this paper a lower score.
2.	More strong supervised baselines should be included for reference. Comparing only a few relatively weak baselines can lead to a misunderstanding of the current gap between supervised and unsupervised methods. 
3.	Additionally, the authors should report some perceptual metrics for better comparison.
4.	The writing and the presentation need improvement.

Limitations:
The authors have discussed the limitations of the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper This paper proposes an ODE-based  method to tackle low-light image enhancement problems. The motivation of the paper is inspired by the observation that the conventional discrete iterative approaches set fixed update steps. It does not only miss the optimal solution and also does not guarantee the convergence.  Hence, the proposed method takes the iterative curve-adjustment approach and formulates them into solving neural ordinary differential equations. This method is used to work with unsupervised learning to estimate the higher order curve parameters to reconstruct image structure details. Comprehensive experiments demonstrate that the proposed method outperforms the baseline methods on LOL and SICE  benchmarking datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Strengths

1. This paper proposes a novel method that integrates the neural networks into an ODE optimization framework. The neural network is playing an adaptive set of updatable parameters. 
2. Comprehensive experiments show that the proposed method outperforms the baseline methods in the task of low-light image enhancement. 
3. The motivation of this paper is strong and solid. It is inspired by the drawbacks of the existing methods and tackle the problems directly in the proposed method.
4. This paper addresses the limitation of the proposed method.

Weaknesses:
Weakness
1. Based on the visual comparison in Figure 4, the proposed method tends to produce over-exposed areas for highlight regions.  
2. The processing speed of the proposed method is one of the limitations.

Limitations:
The limitation is included in the main manuscript. The processing speed (inference speed) of the proposed method is slow compared to dedicated supervised DL methods.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a Neural ODE method for curve-adjustment-based low light image enhancement methods to achieve better results which are often sub-optimal for fixed discrete step methods. Specifically, the proposed method reformulates the curve-adjustment-based from the discrete version into the ODE problem by introducing a continuous state. An ODE solver is adopted for the optimization to find the optimal step for the enhancement. Additionally, a simple denosier and a curve parameter estimation module are proposed for noise removal and parameter estimation, respectively. Extensive experiments are conducted to show the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Turn the discrete curve-adjustment method into a NODE problem, benefited from the optimization to search for the optimal step.
2. User control support during inference is good for the application of the proposed method.
3. The proposed method seems to have good performance over other competitors.

Weaknesses:
1. The proposed method faces color casts, which is obvious in almost all qualitative results, even with a color constraint in loss functions.
2. The proposed method proposes to denoiser and curve parameter estimator in the NODE framework, however, generalize the method to existing curve-ajustment-based method seems to be a more attractive solution.
3. The denoiser seems to be weak since there is so much noise left for the qualitative results.

Limitations:
Yes, it is discussed in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
7yqjVgWWxx;"REVIEW 
Summary:
This paper proves that the probability ratio that appears when computing the time reverse rate matrix for an absorbing state diffusion model has a simple form composed of the conditional distributions of clean data given partial masking scaled by an analytic time dependent weighting. They exploit this form to simplify the parametrization of absorbing state diffusion models and show this improves performance and sampling speed on text datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This work is clearly written and I think theorem 1 will be genuinely useful for future work in absorbing state diffusion models. The fact that the time reversal of the rate matrix has a simple relation to the conditional distributions of clean data that is independent of time makes the target of optimization much clearer. This removes needless complexity when trying to condition models on time, even though the relationship with time is known analytically. This also helps model convergence since the scale factor of the target is known allowing the network to be targeting normalized quantities which is highly desirable for neural net training.

The removal of the time conditioning also has a significant benefit with respect to model speed ups. It is quite surprising that the absorbing state literature does not use this trick where at most L neural network evaluations are required for L length data. This paper should significantly help existing implementations in this regard by removing needless calls to the network.

Weaknesses:
Theorem 2 is wrong. Line C.14 in the proof is incorrect, it's not an equality but a lower bound. The correct version should read

$q\_\theta(x\_0) = \sum\_{\pi} U(\pi) q\_\theta(x\_0 | \pi)$

$q\_\theta(x_0) = \sum\_{\pi} U(\pi) \prod\_{l=1}^d q\_\theta(x\_0^{\pi(l)} | x\_0^{\pi( <l )} )$

$q\_\theta(x_0) = \mathbb{E}\_{\pi \sim U(S\_d) } [ \prod\_{l=1}^d q\_\theta (x\_0^{\pi(l)} | x\_0^{\pi (<l)}) ] $

$ \log q\_\theta(x\_0) = \log ( \mathbb{E}\_{\pi \sim U(S\_d)} [ \prod\_{l=1}^d q\_\theta(x\_0^{\pi(l)} | x\_0^{\pi(<l)}) ] )$

$ \log q\_\theta(x\_0) \geq \mathbb{E}\_{\pi \sim U(S\_d)} [ \sum\_{l=1}^d \log q\_\theta (x\_0^{\pi(l)} | x\_0^{\pi(<l)} ) ]$

When your generative model is a mixture of different generation paths (which an absorbing state diffusion model is), then you need to apply Jensen's inequality. See https://arxiv.org/pdf/2110.02037 equation (2).

Therefore, the authors should remove Section 3.3. I think this section should be replaced with discussion of Autoregressive Diffusion Models https://arxiv.org/pdf/2110.02037 which the author's model has basically reduced to. Autoregressive diffusion models randomly sample a generation order and gradually infill tokens with no dependence on time. The link between Autoregressive Diffusion Models and absorbing state diffusion should be clearly discussed in this paper and this reference is glaringly missing.

The authors should also remove line SEDD-S* in Table 2 since it is based on Theorem 2. Your models are then really not doing favourably compared to standard SEDD. What is your explanation for this and new narrative for Table 2?

In the paper's current state I cannot recommend acceptance since a large part of the narrative is based around Theorem 2. However, I believe the contributions surrounding Theorem 1 with regards to making models simpler and achieve good speed up stand alone as a worthy contribution. Therefore, if the authors clearly describe how they will adjust the narrative under this new information I will be happy to raise my score.

I think it would also be good to include a baseline against autoregressive diffusion models since they propose additional tricks relating to picking how many tokens to reveal. However, I appreciate this would be difficult in the limited time of the rebuttal period and is not required for an increase in score.

Limitations:
The authors adequately discuss the limitations in Section 6.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a simplified discrete diffusion model to improve upon prior language diffusion models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The method is simple and scalable. It is overall a nice insight, and the authors do a good job in extracting the relevant and impactful applications of this.

* The method seems to improve upon previous results, in particular resulting in better/faster sample quality.

* The presentation is pretty clear and direct.

Weaknesses:
* Although the method speeds up sampling, especially in the large sample step regime, this is a bit misleading/irrelevant. In particular, under more standard sampling practices, the gain is naturally not as big, so the claim of 3.5x improvement is a bit misleading. Furthermore, this does not really improve the sample quality at a smaller number of steps, which is the critical question. As a comparison, this would be like sampling from a standard diffusion model with 4096 timesteps, showing that you can speed it up in that regime, and then claiming a general improvement.

* The results are ultimately a bit marginal. The improvements on sample quality are nice, but I think there is a mistranslation between figure 2 and table 1 (there is no 15 generative perplexity for RADD in that table). Until this is clarified, I'm trusting the results of table 1 more. Table 2 also shows a slight improvement.

* The exact likelihood computation is never applied. I want table 2 to showcase this exact likelihood instead of just a bound.

* The model size is only small. I want to see a similar improvement for the medium quality.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work derives a new interesting connection between the concrete score and conditional target densities in absorbing diffusion models, which decomposes the time-dependent ratio between marginal probabilities (of two transitive states) as a conditional distribution on clean data scaled by an analytic time-dependent scalar, and hence inspires the commonly-used scaling trick and new re-parameterizations. In addition, it also simplifies the original complicated loss objective (denoising score entropy; DSE) as a more straightforward denoising cross-entropy loss (DCE) that enables the exact log-likelihood computation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is generally well-written and easy to follow.
2. This work proposes valuable insights of decoupled model parameterizations and simplified learning objectives, whose effectivenesses are theoretically grounded.
3. The proposed methods are also numerically verified, which advances the development of (absorbing) discrete diffusion models.

Weaknesses:
1. Despite that the overall presentation is good, it can be further improved by interpreting or illustrating more about the problem formulation. For example, what is the intuition behind the absorbing matrix $Q^{\text{absorb}}$ (eq. (2.4))? Why do we require a more complicated DSE loss instead of usual score-matching objectives (e.g. MSE)? Note that in eq. (2.6), the score network must be *additionally* positive. 
2. Although the proposed method (RADD) is reported to be superior for efficient sampling, the performance of RADD need further verifications on language modeling tasks (Table 2). The hyper-parameters should be fine-tuned to better demonstrate the capability of RADD.

Limitations:
As is stated by authors, future explorations include flexible variable-length texts generation and applications to models with larger scales.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
8Hy3KMZTL5;"REVIEW 
Summary:
The paper presents H-CLIP, a novel framework for open-vocabulary semantic segmentation using the CLIP model. The framework addresses three key challenges: high computational cost, misalignment between CLIP's image and text modalities, and degraded generalization ability on unseen categories when fine-tuning for pixel-level predictions. H-CLIP employs a symmetrical parameter-efficient fine-tuning (PEFT) strategy conducted in hyperspherical space for both CLIP modalities. This strategy uses efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module to mitigate misalignment issues. Additionally, an orthogonality constraint based on the hyperspherical energy principle is applied to the text encoder to preserve the generalization ability of the pre-trained model.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The introduction of the H-CLIP framework for open-vocabulary semantic segmentation represents a significant innovation. The use of a symmetrical parameter-efficient fine-tuning (PEFT) strategy in hyperspherical space is a unique approach to addressing the challenges associated with fine-tuning vision-language models.

The paper provides extensive experimental results across multiple benchmarks, including ADE20K, PASCAL VOC, and PASCAL-Context. These experiments validate the effectiveness of H-CLIP, showing its superior performance compared to state-of-the-art methods.

Weaknesses:
1. Consider that the expression in formula 5 does not specify how to interact with the \boldsymbol{R} matrix.

2. The paper states that current fine-tuning strategies are usually asymmetrical, but it does not provide enough evidence or references to support this claim. The authors should provide empirical evidence or references to support the claim of asymmetry.

3. While the paper extensively discusses the orthogonality constraint in the CLIP image encoder, it lacks an in-depth analysis of how the misalignment problem impacts segmentation performance. The authors should discuss the specific effects of misalignment on segmentation.

4. The paper should mention SAM (Segment Anything) and how the current work is still significant

Limitations:
The work is on semantic segmentation and there is no qualitative comparison shown in the main paper. There are some visuals in the supplementary, but most of those are from test set and there is no comparison shown with the baselines and existing methods, so it is not clear where the improvement is coming from. It will be good to see how the results improve with and without alignment.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents H-CLIP, a novel approach for parameter-efficient fine-tuning of the CLIP model in hyperspherical space, specifically for open-vocabulary semantic segmentation. H-CLIP includes the introduction of a symmetrical parameter-efficient fine-tuning strategy, leveraging hyperspherical energy principles. And a dual cross-relation communication module is utilized to enhance cross-modal and cross-layer alignment.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- This paper is well-motivated. The proposed H-CLIP effectively addresses common issues in fine-tuning CLIP.
- The paper effectively argues that maintaining the hyperspherical energy helps preserve the model's generalization ability, a critical factor in multi-modal tasks.
- The ablation experiments are thorough and effectively support the arguments.

Weaknesses:
- The writing needs improvement. The introduction lacks transitions from existing problems to the approach of this paper, such as introducing the advantages of Hyperspherical Space. 
- Some formula descriptions can be optimized, for example, explaining the meaning of * in Formula 9. 
- Details about comparison methods are needed. In Table 1, the compared method SAN includes an additional backbone.

Limitations:
The authors provide no analysis of the limitations and broader impact. The author can analyze the limitations of this fine-tuning strategy in the field of OVS.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes H-CLIP, a symmetrical parameter-efficient fine-tuning (PEFT) strategy conducted in hyperspherical space for both of the two CLIP modalities. The PEFT strategy is achieved by a series of efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module among all learnable matrices. Extensive evaluations across various benchmarks show that H-CLIP achieves new SOTA open-vocabulary semantic segmentation results while only requiring updating approximately 4% of the total parameters of CLIP.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper achieves SOTA performance.
The Parameter-efficient Fine-tuning is explained by tensor computation.

Weaknesses:
1.The novelty is limited.  Partial orthogonal fine-tuning (POF) doesn't directly address the challenges of OVSS but rather offers a generic PEFT approach, so what is the difference between POF and OFT[1]? In Equ.5, which module's weights are used by the pre-trained weight matrix, Q(K or V)’s projection layer or FFN? Method details need detailed explanation.
2.Some concerns about DCRC. In this section, the author discusses the use of two k layers deep neural network to update the fourth-order tensor in Equ. 7, and provides some mathematical proof. However, these proofs only show that reversible transformations S(·) can be replaced by reversible matrices S (as shown in Equ. 11,12,14,15), and the authors use k layers deep neural network to replace such reversible matrices S, which cannot explain the meaning of reversible transformations. In other words, why adopting reversible transformations to update the fourth-order tensor in Equ. 7, and what is the role of reversible transformations? Is this approach also work in other fields other than semantic segmentation tasks? In addition, If the block diagonal structure is not adopted, Equ. 16 seems to require only one reversible matrices S4 for mapping. Does this reduce the number of parameters?
3.Insufficient experimental analysis. 
1)The decoder of HCLIP seems to be learnable as well. Does the param in Table 2 calculate the decoder part? And, Is the proposed PEFT method applicable to various decoders? If it is replaced with linear probe, is the proposed method still effective? Need further exploration.
2)If a different VFM is adopted (not CLIP), is the proposed method still valid?
3)The proposed method should be compared with more PEFT methods such as VPT, Adapter, LST, SSF [1-5] .

[1] Zeju Qiu, Weiyang Liu, Haiwen Feng, Yuxuan Xue, Yao Feng, Zhen Liu, Dan Zhang, Adrian Weller, and Bernhard Schölkopf. Controlling text-to-image diffusion by orthogonal finetuning. Advances in Neural Information Processing Systems, 36:79320–79362, 2023.
[2] Menglin Jia, Luming Tang, Bor-Chun Chen, Claire Cardie, Serge Belongie, Bharath Hariharan, and Ser-Nam Lim. Visual prompt tuning. In European Conference on Computer Vision, pages 709–727. Springer, 2022.
[3] Neil Houlsby, Andrei Giurgiu, Stanislaw Jastrzebski, Bruna Morrone, Quentin De Laroussilhe, Andrea Gesmundo, Mona Attariyan, and Sylvain Gelly. Parameter-efficient transfer learning for nlp. In International conference on machine learning, pages 2790–2799. PMLR, 2019.
[4] Yi-Lin Sung, Jaemin Cho, and Mohit Bansal. Lst: Ladder side-tuning for parameter and memory efficient transfer learning. Advances in Neural Information Processing Systems, 35:12991–13005, 2022.
[5] Dongze Lian, Daquan Zhou, Jiashi Feng, and Xinchao Wang. Scaling & shifting your features: A new baseline for efficient model tuning. Advances in Neural Information Processing Systems, 35:109–123, 2022.

Limitations:
The limitation of the proposed method should be discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a novel method called Parameter-Efficient Fine-Tuning in Hyperspherical Space for efficiently solving the open-vocabulary semantic segmentation problem. The method introduces a series of efficient block-diagonal learnable transformation matrices and a dual cross-relation communication module among all learnable matrices. To maintain the generalization ability offered by the CLIP text encoder, the authors designed a constraint to PEFT based on Hyperspherical Energy. Comprehensive results on open-vocabulary semantic segmentation benchmarks demonstrate the strong performance of this PEFT method by training only 4% of the total parameters of CLIP.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The idea of introducing hyperspherical space to achieve parameter-efficient training is interesting. This approach attains state-of-the-art performance on current open-vocabulary semantic segmentation benchmarks with fewer learnable parameters. Additionally, it demonstrates better parameter efficiency than LORA on open-vocabulary semantic segmentation tasks, as shown in Table 3.

Weaknesses:
I do not see any clear weaknesses. However, I acknowledge that I am not familiar with hyperspherical theorems.

Limitations:
See in questions. No other clear limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
3zYmlmkIuK;"REVIEW 
Summary:
In this paper, the authors study multi-agent reinforcement learning where agents cooperate through asynchronous communications with a central server to learn a shared environment. They consider the following two settings: multi-agent contextual bandits with general function approximation, and multi-agent RL with general function approximation. For both settings, they propose provably efficient algorithms with low regret and low communication complexity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem of asynchronous MARL with general function approximation is interesting and important.

2. This paper is the first to consider the setting with general function approximation. The results are solid and the proof looks good to me.

3. For both settings, the authors propose provably efficient algorithms. The results generalize previous results under the linear setting.

Weaknesses:
1. It seems that part of the techniques is from previous results, such as the bonus function oracle. It will be helpful if there is a section discussing technical novelty.

2. It seems that the setting is closely related to low switching RL and RL with delayed feedback. It will be interesting if the authors could briefly discuss about the connections.

3. For the communication complexity bound in theorem 5.1, should it be $/\alpha$ instead of $\alpha$? In addition, why not choose $\alpha=1/M$ in both theorems? In this way, the communication cost can be improved. (Please correct me if I misunderstood anything) 

4. Line ?? in line 214 of page 6. Please correct it.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose two algorithms for asynchronous communication in multi-agent reinforcement learning with generalized value function approximation: Asynchronous-NLin-UCB for context bandit scenarios and Asynchronous-NLSVI-UCB for episodic MDP scenarios. These algorithms achieve near-optimal regret with low communication complexity. The authors theoretically show the trade-off between regret and communication complexity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The authors provide a detailed background on the related literature concerning regret, communication complexity, and the presence of asynchronous update, which is greatly helpful in understanding the contributions of the proposed algorithms. 
- The theoretical foundations and proofs regarding the communication criterion are important and interesting. Also, the trade-off between regret and communication complexity via parameter $\alpha$ offers valuable insights. 
- The approach of receiving decisions and bonus functions from a central server instead of historical data is intuitive and appears crucial from a privacy perspective.

Weaknesses:
While I have studied general value function approximation, I do not have research background in this field for multi-agent scenarios. Therefore, my critique may not have captured the weaknesses of this paper. 
I’m open to revising my score based on the authors' responses.

As far as I know, MARL often adopts the Centralized Training Decentralized Execution(CTDE) framework to avoid the action space growing exponentially with the number of users. However, it is unclear whether the proposed scenario follows ""decentralized execution"". Agents are supposed to execute based on partial observations in a decentralized manner, but the proposed approach appears to involve a central server consistently during execution. If the proposed scenario is inconsistent with CTDE, I would be interested to hear from the authors what the distinct advantages or necessity of this scenario is.

Typos:

- Line 214: Reference to the label is not correctly written.
- Theorems 4.3 and 5.1: $\tilde{\beta}$ is not properly defined in the statement, and $\beta_t$ should be fixed to $\tilde{\beta}$.
- Theorem 5.1: Total communication complexity should be fixed to $O((1+M\alpha)^2 / \alpha)$.

Limitations:
The requirement for global state instead of partial observation may limit the practical applicability of the proposed methods.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the asynchronous multi-agent bandit and RL problem with general function approximation (measured by Elude dimension). The main contribution is to establish $\tilde{O}(\sqrt{\text{dim} T})$ regret bound with $\tilde{O}(M^2 \text{dim})$ communication complexity.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper is well written and the contribution is solid.

Weaknesses:
I think the major concern is non-optimal complexity bounds. Although it seems unreasonable to ask for a matching upper\&lower regret bound for the contextual bandit problem, the part about RL could be possibly improved (at least, the dependence on $H$ is not tight). Also I am curious that what is the current best lower bound for the communication cost to reach an $\sqrt{T}$ regret bound. It would be an interesting problem to study the exact trade-off between the communication cost and regret.

A minor concern might be about the technical novelty given previous methods on measuring the uncertainty.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studied the distributed federated contextual bandit and federated reinforcement learning (FRL) in the presence of a trusted server. In both problems, nonlinearity and asynchronous communications are explored. Similar algorithms for contextual bandit and FRL that encourage exploration via bonus functions are proposed. Finite-time convergence results in terms of regrets are established for both algorithms and communication complexities are also characterized.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* This paper studied the asynchronous federated learning problem where only one agent is activated to sample data and infrequently communicate with server.
* The trigger-based communication is an interesting approach in multi-agent or multi-learner problems.

Weaknesses:
* The clarity of some of the important quantities are not well defined or explained. For example, 
1)	In the sample complexity result of Theorem 4.3, $\tilde{\beta}_1$ is used. However, it was not defined. It’s unclear what this notation is referring to. Similarly, in Theorem 5.1, $\tilde{\beta}_2$ is used.
2)	The oracle for to compute bonus term bk+1,h is crucial in understanding the algorithms. However, it was not very well-explained or shown anywhere in the main paper.	
3)	The two sentences from Line 300 to Line 302 are confusing. Please clarify them.
* Typos:
1)	An extra closing parenthesis appeared in Line 141.
2)	Line 214, ?? -> 12
3)	In Line 1 of algorithm 3, $k=[K]$ -> $k\in [K]$.
4)	In Line 154, the trajectory should be $(s_h, a_h, \cdots, s_H, a_H)$.

Limitations:
Please see weaknesses and questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
1MQXBnEbE8;"REVIEW 
Summary:
This paper presents a novel imputation method, based on a bipartite graph constructed from the data and the missing-data patterns, and two components which allow to measure similarities between the features and samples.

The method shows very good results in terms of MAE on several datasets for MCAR, MAR and MNAR data.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well written. 

- Experiments are well conducted, on several datasets, with different missing-data ratios and considering MCAR, MAR or MNAR data. The authors have made an effort to compare themselves with many other imputation methods. 

- There is a true discussion on the parameters to choose in the experiments. The authors are honest about the performance of their method, and give explanations when another method is better.

Weaknesses:
- Although well presented, the method is complicated to understand. 

- The methods uses 8 MLPs and one GNN. The authors discuss in Appendix the computational resources, but do not compare other methods on this point.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces M3-Impute, a mask-guided representation learning method for missing value imputation. The core idea of M3-Impute is to leverage missingness information as an explicit input to the model through masking schemes. This approach allows M3-Impute to effectively learn both feature-wise and sample-wise correlations, accommodating various types of data missingness. The model employs a variant of GraphSAGE for graph representation learning, incorporating edge embeddings via neighborhood aggregation. It outperforms traditional tabular data models in various benchmark datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper presents a novel approach to missing value imputation through the introduction of a mask-guided representation learning method (M3-Impute). The originality of the work lies in its utilization of missingness information as a model input, employing innovative masking schemes. This allows M3-Impute to accurately capture feature-wise and sample-wise correlations despite varying types of missing data (MCAR, MAR, MNAR). The use of GraphSAGE for graph representation learning, combined with edge embeddings via neighborhood aggregation, further distinguishes this work from traditional tabular data models.

2. The quality of the research is demonstrated through comprehensive experiments across multiple datasets and missing data mechanisms. The empirical results show that M3-Impute consistently outperforms baseline methods. The authors include a code package and datasets with the submission.

Weaknesses:
1. The paper evaluates the sensitivity of the M3-Impute model to the initialization parameter ϵ (Table 3), demonstrating that a non-zero value of ϵ improves imputation accuracy. However, the lack of detailed sensitivity analysis for other critical hyperparameters, such as the learning rate, batch size, number of GNN layers, and the dropout rate, represents a weakness.

2. The paper also has notable limitations in its contextualization relative to prior work. While it effectively presents M3-Impute and compares it against several baseline models, it lacks a deeper analysis of how these baseline models have evolved and the specific innovations they have introduced over time. For instance, the paper mentions GRAPE and IGRM as key prior graph-based imputation methods but does not adequately explore their strengths and weaknesses or how M3-Impute directly addresses the limitations of these methods. This omission makes it challenging to understand the novelty and improvements offered by M3-Impute.

3. Relying solely on MAE to evaluate the performance of imputation models has several limitations. MAE measures the average magnitude of errors but does not account for the variance or distribution of those errors, making it insensitive to outliers and providing no insight into model bias. This can result in an incomplete understanding of a model's performance, particularly in contexts where large errors or systematic biases are important considerations. To address these limitations, incorporating RMSE alongside MAE would be beneficial. RMSE penalizes larger errors more heavily, offering additional insight into the presence and impact of significant errors in the model's predictions.

Limitations:
The authors should consider summarizing the limitations of their method in the conclusion to provide a comprehensive overview of their work. Specifically, in Section 4.2, the authors discuss the cases of MAE degradation for the Kin8nm and Naval datasets. They attribute this to the independence of features in Kin8nm, which prevents observed features from aiding in the imputation of missing values, and the strong linear correlations between nearly all features in the Naval dataset. Summarizing these points in the conclusion would give readers a clear understanding of the method's limitations and the contexts in which it performs best.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study proposes a missing value imputation method. The proposed method tackles the missing value imputation problem as a link prediction task on the bipartite graph. It represents a data matrix with missing values as a bipartite graph, then uses a graph neural network on the bipartite graph to learn the embeddings of samples and features. Next, a feature correlation unit and a sample correlation unit are employed to obtain feature-wise and sample-wise correlations, which are then fed into an MLP to obtain imputed values.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
- An advanced and novel approach to missing value imputation. The idea is intuitive.
- Experimental comparison is done with various existing methods, including recent ones.
- Significant performance improvements achieved.

Weaknesses:
- The size of the bipartite graph may drastically increase with data size and dimensionality.
- No investigation on various missingness scenarios and missing rates.

Limitations:
-

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of missing values in data analysis and machine learning by proposing M3-Impute, a novel imputation method. Traditional imputation techniques often neglect 'missingness' information and fail to explicitly model feature and sample correlations, leading to suboptimal results. M3-Impute innovatively incorporates missingness information and correlations through advanced masking schemes. It represents data as a bipartite graph and utilizes a graph neural network with a refined initialization process to learn node embeddings. These embeddings are further optimized using feature correlation and sample correlation units, which explicitly consider the correlations during imputation. The method's effectiveness is demonstrated through experiments on 15 benchmark datasets with three different missing patterns.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The feature correlation unit (FCU) and sample correlation unit (SCU) are particularly compelling. The FCU learns correlations between the target missing feature and observed features within each sample, refined by a soft mask on missingness information. Similarly, the SCU computes sample-wise correlations, enhanced by another soft mask on missingness information for pairs of samples.

Integrating FCU and SCU outputs to estimate missing values is methodologically sound. Extensive experiments on 15 open datasets show M3-Impute's superior performance in 13 out of 15 cases under various missing value patterns. The reported improvements in mean absolute error (MAE), up to 11.47% over the second-best method, underscore M3-Impute's practical relevance and robustness.

Weaknesses:
M3-Impute demonstrates strong performance across many datasets but shows limitations in handling datasets with highly independent features or strong linear correlations, such as the KIN8NM and NAVAL datasets.

The model's robustness in general scenarios may degrade when confronted with datasets containing extreme correlation structures.

Future improvements could concentrate on enhancing M3-Impute's adaptability to these challenging cases to broaden its applicability and robustness across diverse datasets.

Limitations:
Although M3-Impute outperforms other baselines on most datasets, two cases (KIN8NM, NAVAL dataset) highlight its limitations in handling datasets with either highly independent features or strongly linear correlations. This suggests that while M3-Impute is robust in general scenarios, its performance may degrade in datasets with extreme correlation structures.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposed a new imputations method called M3-impute. M3-impute follows the basic structure of some recent imputation methods: a undirected bipartite graph is constructed with nodes for features and samples, where edge weights correspond to observed data at the given feature-sample pair. Previous approaches use Graph Neural Networks (GNNs) to impute missing values via edge weight prediction. M3-impute improves these approaches by adding two new components on top of an initial GNN to model feature-wise and sample-wise correlations respectively. Empirical results show that M3-impute achieves competitive performance in terms of MAE for imputation across several tabular datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is generally well written.
- Empirical results are extensive. Many other imputations methods are included for comparison, providing a good representation for the state-of-the-art for tabular data imputation. Ablation studies and robustness studies also further strengthen the credibility of the methodology.

Weaknesses:
- The paper does not support categorical features. This is a big weakness compared to other imputation methods that can handle categorical features such as iterative approaches like hyperimpute. 
- The paper does not discuss the impact of missing value imputation on downstream tasks. Imputation is usually a preprocessing step, and thus assessing the impact on possible downstream tasks is paramount. For example, in supervised learning, some recent evidence suggests that mean/zero imputation is as good as more complex imputations [1, 2]. 

[1] Le Morvan, Marine, et al. ""What’s a good imputation to predict with missing values?."" Advances in Neural Information Processing Systems 34 (2021): 11530-11540.
[2] Van Ness, Mike, and Madeleine Udell. ""In defense of zero imputation for tabular deep learning."" NeurIPS 2023 Second Table Representation Learning Workshop. 2023.

Limitations:
See weaknesses. In particular, not handling categorical features is not mentioned in the paper anywhere as a limitation of the method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes M^3-Impute, a missing value imputation method that utilizes GNNs to learn embeddings of samples and features. By incorporating feature correlation unit and sample correlation unit, M^3-Impute effectively captures correlations between features and samples for accurate imputation.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
S1. This paper introduces a novel masking scheme that effectively utilizes missing information for modeling.

S2. This paper proposes the feature correlation unit (FCU) and sample correlation unit (SCU), which help to consider feature and sample correlations during imputation.

S3. Experimental evaluations on various datasets compare the proposed method with state-of-the-art approaches, demonstrating good imputation performance.

Weaknesses:
W1. SCU takes into account the pairwise similarity of \mathcal{P} during its construction, which subsequently determines the scalar parameter \alpha during imputation. The initialization of \mathcal{P} seems to directly impact the model's performance and remains unchanged once set. It would be beneficial for the authors to discuss this aspect and, if possible, provide some experimental evidence to support their approach.

W2. In Section 4.4, the paper mentions different sampling strategies for SCU and uses a new strategy in the ablation study (Table 2), which is different from the strategy mentioned in Section 3.4. The authors claim that this strategy leads to inferior performance compared to previous strategies, thereby highlighting the superiority of the ablation study results. This lacks experimental evidence and results in inconsistency in the experimental setup.

W3. As a crucial parameter, the size of \mathcal{P} directly affects the construction of SCU. Table 3 presents experimental results with different sizes, but the differences are not significant, which is somewhat counterintuitive. Although the authors discuss the experimental results in Section 4.4, the performance fluctuation of 0.01 to 0.02 does not clearly reflect the ""decrease then increase"" trend mentioned by the authors. Exploring larger peer values and providing more detailed analysis and guidance on parameter selection might be beneficial.

W4. The authors emphasize the importance of specific missingness information throughout the paper. Intuitively, different types of missing data (MAR, MNAR, MCAR) might offer varying types of missingness information, potentially impacting the model's performance. While the paper experiments with data of different missingness types, a more thorough discussion of the results could enhance the motivation and clarity of the paper.

W5. In Section 4.3, the caption of Table 2 references a concept, the uniform sampling strategy, which is introduced for the first time in a later subsection. The authors might consider adjusting the structure of the paper for better clarity.

W6. In Figure 3, in the subfigure with the missing ratio of 0.7, two bars exceed the upper boundary and need adjustment.

W7. It would be helpful to add independent labels to all subfigures, such as (a), (b), etc., to facilitate referencing.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This is a novel approach for imputing missing data using mask-guided representation learning. The main contributions include the development of an imputation model that leverages both feature and sample correlations. This model improves imputation accuracy and robustness compared to existing methods. The paper also provides comprehensive experiments and ablation studies to validate the effectiveness of the proposed approach across various datasets and missing data scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Novelty: A unique mask-guided representation learning method that effectively combines feature-wise and sample-wise correlations.
- Comprehensive experiments
- Strong empirical performance

Weaknesses:
- Computational complexity 

## Minor Points

l4: ""Existing imputation methods, however, fall short of considering the ‘missingness’ information in the data  during initialization and modeling the entangled feature and sample correlations  explicitly during the learning process,""
-> This is not true. Many existing methods consider missingness patterns.

The distinction between ""statistical"" and ""learning based"" methods seems off. Certainly most learning based methods are statistical, and vice versa.

l. 38 ""struggles"" -> struggle

Limitations:
- Gives little insight into why it works better on some datasets than others.
- Would be interesting to understand better how robust results are under systematic changes in datasets, e.g., different types of missingness.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
LlcygqLdwO;"REVIEW 
Summary:
Saliency methods are among the most popular approaches to explaining an existing black-box image classifier. However, they are limited to localizing class objects in an image. In addition, since they rely on per-pixel importance, they are unable to generalize accross multiple instance to provide a global explanation of the image classifier. To address these limitation, an existing work, Testing with Concept Activation Vectors (TCAV), provides global explanations via learning concept vectors by learning from a set of example images with a known concept. However, TCAV can only provide global explanations, limiting them from providing location information of where the concept is located in the image. Inspired to solve this problem, the authors present Visual-TCAV, an approach that provides both local and global explanations. The authors realize this by learning a Pooled-CAV per concept based on the feature maps of a chosen layer in the network and combining this with the integrated gradients (IG) of the same features maps for a given instance image. The resulting saliency provides a localization of the concept in the instance. To achieve global explanations, they analyze the aggregation of the concept activations across images for a particular class. The authors provide analysis on layer selection, local explanations, and global explanations across several popular CNN-based model pretrained on ImageNet. In addition, they conduct a validation experiment to verify the effectiveness of their method where the ground-truth concept is known.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The methods is able to add localization to the existing TCAV approach increasing is ability to explain black-box classifier CNNs.

The authors address that their approach only considers positive activations in the features and discusses the usefulness of accounting for negatively activated features in the future.

The presentation of the paper is clean and easy to follow. The methods are made simple to understand and are effective. Figure 1 was particularly effective at communicating their method.

The experimentation and analysis was decently extensive. They analyze the effect of choosing shallow, middle, and deep layers for their approach providing interesting findings on where certain concepts are activated. 

The validation experiment shows the faithfulness of their approach's ability to find the targeted concept in a set of example images.

Qualitative results show strong localization ability of their method to identify queried concepts.

Their approach is relatively fast to run for local and global explanations.

Weaknesses:
The authors show the activation of different types of concepts at different layers. It has been shown that certain levels of layers have been associated with different types of concepts such textures, shapes, objects, etc in [1]. This work should be referenced and discussed compared to their findings on activations at different layers.

While the paper analyzes across common CNN models, they do not analyze on ViTs or models trained on other datasets other than ImageNet.

The authors utilize generative models to create certain images containing a concept, but do discuss why this was necessary. It is an interesting avenue, but I'm unsure of it's necessity in this work if no further analysis was done on generative images in particular.


[1] Chris Olah, Alexander Mordvintsev, and Ludwig Schubert. “Feature Visualization”.
In: Distill. 2017.

Limitations:
The paper doesn't provide generalization of their method to visual transformer methods which are also becoming popular in explainability and interpretability work.

While this approach is effective at identifying concepts, it requires a manually selected set of example images with a predefined concept. While they mention that generative approaches could help with reducing the number of required examples, the choosing of the correct concepts is still a limitation. This is particularly a problem in more specialized domains such as medical diagnosis where concepts may not be known or are more difficult to explain / generate.

In addition, one has to ablate through several layers for each concept to find which one effectively captures the concept.

The paper only analyzes models pretrained on ImageNet. To convince the generalizability of this approach, analysis on a model trained on other datasets would be necessary.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents a method for combining TCAV with saliency maps to illustrate where feature-related concepts (e.g., “stripes” or “grass”) are activated in an image. The evaluation is largely qualitative, but the method seems to work well on ImageNet classifcation tasks. The method is also validated on a controlled dataset with known ground-truth features, where it performs as expected.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ The method is straightforward and seems to work well.
+ The method is evaluated with a modified dataset where the ground truth importance of concepts is known, or at least well controlled. I haven’t seen this done in many papers in this field and it’s a very nice addition to the work.
+ The paper is well-written and enjoyable to read. The investigation of the proposed method is quite thorough.

Weaknesses:
- The method is not particularly novel – there are other methods which localize CAVs in an image to provide local explanations. I think both of the automatic concept-extraction methods (ACE and ICE) described in Section 2 do this; the recent method CAVLI (https://ieeexplore.ieee.org/document/10208704) also does this.
- This paper does not provide any comparisons to other methods.
- Like other approaches based on TCAV, the method requires the user to identify the concepts of interest and curate training datasets to visually represent each concept. This limits the usefulness of non-automatic TCAV methods for general-purpose model explanations.
- Like most other saliency map approaches, the explanations only focus on how concepts that are present in an image contribute to the decision. However, model decisions may also depend on the absence of features. This approach does not have a way to represent feature-absence in the explanation, which can lead to confusing explanations in some cases, as shown in the validation experiment.

Limitations:
Not applicable

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel technique, Visual TCAV, which unifies concept-based explainability with saliency maps. Visual TCAV produces local explanations in the form of saliency maps, which highlight the pixels in the image that represent a given user-defined concept. The visualization is enriched with an attribution score which represents the importance of a concept in the prediction of a given class. Visual TCAV can also produce global explanations by aggregating attribution scores of multiple images belonging to the same predicted class.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is overall well written and experiments are discussed in detail.
The technique addresses the limitations of TCAV and overcomes them by providing both a visual and quantitative analysis of a concept’s influence in a prediction. It also enriches the ability of TCAV of providing global explanations, allowing it to measure the influence of a concept and not only the sensitivity of a model to it. 
I find the visualization step to be particularly crucial for a fair analysis of convolutional neural networks and for bias detection.

Weaknesses:
Minor remarks: The use of the notation was inconsistent throughout the paper and some of the figures lacked clarity. In particular : 

•	The index referring to the feature maps in the considered layer, k, is not explicitly mentioned in the text. It is understandable from Figure 1 that k refers to the index of such feature maps, but it should be written explicitly for better clarity, as it is frequently used in the formulas.

•	In line 145 the raw concept map is indicated with M_{raw}^c but in equation 1 the notation changes and becomes M^{c, raw}

•	In equation 3, the indices i,j are further introduced when denoting M_{ij}^c. I believe this refers to a pixel-wise notation, with i,j indexing the pixels in the rows and columns of the image, but it is not mentioned explicitly. The non uniformity in the notation is somewhat disturbing.

•	The sentence in line 168-169 seems to imply that additivity holds, while later on in the paragraph it is specified that the measure is concept-wise. I would suggest a rephrasing.

•	The normalized logits presented in line 184-186 could be better expressed with a formula, to avoid misunderstandings on their derivation. Moreover, in line 188, are the normalized attributions the same as the normalised logits? If yes, use a consistent terminology.

•	The normalised attributions are indexed by an index t. It is defined only in line 193 that t represents the target class. It should be clarified before its first appearance. 

•	In equation 4 it appears for the first time the notation p_k^{c, norm}. Is it used to represented the pooled-CAV (p^c) rescaled to [0,1]?

•	There is no legend in Figure 2 to investigate the portrayed degrees of activations. In figure 2d the focus seems to be on the parashoot other than the sky. A legend would help interpret the figures more clearly.

•	I suggest the use of a colour-blind friendly palette for Figure 4 (for instance, figure 5 and 6 are better suited). 

•	Figure 6 caption mentions a statistical significance test: how was the test conducted? Is it the same test described in the TCAV paper?

Limitations:
I find that the main limitation of the method is mainly related to the use of concept-based explainability. It may be possible to craft concepts that incorporate social biases or that may be misleading. Clearly, this is a broader issue, not related directly to this paper, but for which there may be an interesting ground for discussion.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
DsN7tHNo78;"REVIEW 
Summary:
This paper focuses on Zero-Shot Composed Image Retrieval (ZS-CIR), which requires retrieving an image matching a reference image while incorporating specified textual modifications. The authors argue that a key challenge in ZS-CIR is training models on limited intention-relevant datasets to understand human intention implicitly expressed in textual modifications for accurately retrieving target images. Therefore, they introduce an image-text dataset incorporated with pseudo-manipulation intentions to enhance the training of ZS-CIR models in understanding human manipulation intents, based on LLaVA. They also propose to use Q-former to compress the features generated by CLIP for retrieval. The experimental results show the improvements of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors propose a large-scale pretraining dataset for ZS-CIR.
2. The experimental results show the improvements of the proposed method.

Weaknesses:
1. The concept of ""intention"" discussed throughout the whole paper is unclear. Based on Figure 1, the authors haven't explained what's ""intention"" to the MLLM. Moreover, the pseudo-manipulation description is the same as the rewritten caption in semantics. I can't find any ""intention"" added into this pseudo-manipulation description. The key novelty of considering human ""intention"" is farfetched.
2. The proposed model lacks novelty. In the model architecture, the authors just add a Q-Former [1] after the CLIP encoder, which is prevalent in existing research based on CLIP-like models. And the authors even do not cite any relevant work.
3. Existing work on CLIP-based ZS-CIR generally compares the experimental results with different CLIP variants, and different methods may be superior with different CLIP variants. The authors only experimented with one CLIP variant, which is insufficient.
4. In Figure 4, the compared method also accurately captures the ""intention"" in the modification text, which cannot show the superiority of the proposed method in capturing ""intention"".

References:

[1] Li, J., Li, D., Savarese, S., & Hoi, S. (2023, July). Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning (pp. 19730-19742). PMLR.

Limitations:
The authors addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces De-MINDS, a novel framework for Zero-Shot Composed Image Retrieval (ZS-CIR) that aims to bridge the gap between pre-training and retrieval by incorporating intention-based pseudo-manipulation descriptions. The authors propose intent-CC3M, a dataset featuring these descriptions generated through chain-of-thought prompting by a Multi-modal Large Language Model (MLLM). They also introduce a manipulation intention understanding network that uses learnable queries to enhance the model's ability to understand user intentions from manipulation descriptions. The paper demonstrates significant performance improvements across four ZS-CIR tasks compared to state-of-the-art models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The introduction of intent-CC3M as a dataset for training mapping networks to align intention-relevant visual information is innovative and potentially impactful.
- The proposed De-MINDS framework shows significant performance improvements over state-of-the-art models across multiple ZS-CIR tasks.
- The approach addresses the challenge of understanding manipulation intentions in user descriptions, which is crucial for accurate image retrieval.
- The ablation studies provide insights into the contributions of different components of the proposed method.

Weaknesses:
Major Weaknesses:

1. Experimental Gaps:
   - The paper lacks experimental evidence to support the claim that caption redundancy leads to inaccurate retrieval, as mentioned in the introduction.
   - There's no evaluation of the method's performance with longer text encoders like LongCLIP, which could potentially address some of the stated limitations of CLIP.
   - The comparison with a baseline (other than CIRR and Fashion-IQ) using only f_theta (trained on Intent-CC3M) without De-MINDS (ablation model '4') is missing, which would provide a fairer comparison.

2. Methodological Concerns:
   - The justification for using CC3M as the base dataset for creating intent-CC3M is not clearly explained.
   - There's no exploration of De-MINDS' performance when prompt options are mismatched with their intended tasks or in scenarios where the task is not known in advance.

3. Incomplete Ablation Studies:
   - The ablation study for the T sampling ratios (50%, 30%, 20%) is missing, and there's no explanation why concatenation of them wasn't considered as an alternative.
   - The ablation study lacks an exploration of the impact of the number of learnable queries, despite its apparent significance.

Minor Weaknesses:

1. Presentation Issues:
   - The prompt types (a), (b), and (c) are not clearly explained in the context they are introduced, requiring readers to refer back to previous sections.
   - There are inconsistencies between the notation in the text and figures (e.g., X vs q in Figure 2).

2. Comparative Analysis:
   - The paper doesn't include evaluations on CIRCO and GeneCIS datasets, which were used in baseline studies.

3. Clarity:
   - More details are needed on certain aspects, such as the ""cos distill"" mentioned in ablation model '9'.

Limitations:
The authors acknowledge the computational intensity of generating pseudo-manipulation descriptions using MLLMs and the potential introduction of irrelevant details in these descriptions. However, they could further discuss the implications of these limitations on the practical applicability of their method in real-world scenarios.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an image-text dataset (intent-CC3M) for Zero-Shot Composed Image Retrieval (ZS-CIR) models to make better understanding of human manipulation intentions. Specifically, captions are re-written with LLaVA model to provide more details, and additional manipulation reasoning prompt is applied to make pseudo-manipulation description. With this dataset, the paper proposes De-MINDS framework (unDErstanding of Manipulation INtention from target Description before Searching), which utilizes pseudo-manipulation descriptions. The model training involves reasoning distillation and cross-modal alignment. The method shows state-of-the-art performance with ViT-L backbone comparisons.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper proposes to leverage LLaVA model to elaborate the image caption and further utilize LLaVA's reasoning capability to build a pseudo manipulation. The proposal is intuitive and clear, and the presentation of this paper is also clear. Extensive results including various ablations and qualitative results demonstrate the proposed method.

Weaknesses:
The proposed method of utilizing LLM, referred to as MLLM, is not entirely novel, as it has been previously addressed in works [1, 2] (please also refer to [1]). Furthermore, the evaluation of the proposed method is limited to the ViT-L backbone, which raises concerns about its effectiveness with other, more robust backbones (such as ViT-G).

[1] Jang, et al. Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval, CVPR2024
[2] Karthik, et al, Vision-by-Language for Training-Free Compositional Image Retrieval, ICLR2024

Limitations:
The paper handles possible limitations properly.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
cRs4jvF4mO;"REVIEW 
Summary:
This paper proposes new methods, Kernel Density Forest (KDF) and Kernel Density Network (KDN), to address issues in confidence calibration for traditional deep learning models and random forests. The motivation stems from the existing literature that deep neural networks using ReLU tend to exhibit high confidence on out-of-distribution (OOD) data due to affine transformations. The proposed methods improve confidence calibration for both in-distribution (ID) and OOD data by partitioning the feature space into polytopes and replacing affine functions within each polytope with Gaussian kernels. Experimental results demonstrate that the proposed methods outperform existing techniques in terms of calibration performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Originality:
The approach of replacing affine functions within polytopes with Gaussian kernels is novel. The proposed methods address the confidence calibration problem for both ID and OOD data simultaneously, providing an integrated solution to these calibration issues.

Quality:
The theoretical proofs are robust, and the effectiveness of the proposed methods is validated through both simulations and real-world datasets.

Clarity:
The paper is written clearly and concisely.

Weaknesses:
Validity of Metrics:
The paper evaluates calibration using Maximum Calibration Error (MCE) for ID data, but does not justify the use of MCE over Expected Calibration Error (ECE) or Adaptive Calibration Error (ACE)[1]. A more detailed explanation and comparison of these metrics would enhance the paper's credibility. Additionally, the definition and justification for OCE (Out-of-distribution Calibration Error) would benefit from a similar comparison with ACE.

[1] https://arxiv.org/abs/1904.01685

Experiments:
To emphasize the effectiveness of the proposed methods, a comparison of execution times would be beneficial, especially since practical applications like web Click-Through Rate (CTR) estimation place significant importance on runtime. The paper should clarify what the noise in Table 1 represents. It would also be advantageous to include experiments on larger and more varied datasets, as well as an evaluation of the methods' performance when combined with in-training calibration methods, which are commonly used alongside post-hoc calibration methods.

Limitations:
This paper mentions computational complexity and limitations in practical applications, but lacks detailed experimental results to support these claims. Including such data would provide valuable insights for future research and implementation.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a novel approach for OOD detection by learning a posterior distribution that is calibrated for both ID and OOD individuals. It models the class-wise conditional distribution of features by a gaussian kernel respectively for a set of polytopes that cover the feature space. The tail property of gaussian kernels contribute to both ID and OOD calibration. Empirical evidence shows the power of the proposed algorithm across tabular and vision dataset under both ID and OOD settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well motivated from the tradeoff of ID calibration and OOD calibration for current approaches for OOD detection methods. The technique of gaussian kernel has a clear geometric intuitive. Compared to affine functions, the tail property ensures that the posterior distribution converges to the prior of labels when a OOD sample deviates far enough from the training support, as proved in Proposition 2. On the other hand, the interpolation by gaussian kernels between neighboring polytopes contributes to ID calibration.

Weaknesses:
The major concern is insufficient discussion over the research context of the paper, which renders it hard to precisely evaluate the contribution. The related work section is short. Section 2 shows that ""OOD detection"" is the closest area to this paper, but this keyword is totally absent from the introduction, where the research area is named ""OOD confidence calibration"". What is the relation between OOD detection and OOD confidence calibration? 
The introduction also reveals two potential approaches for this area: discriminative and generative methods. There are also two settings: ID and OOD confidence calibration. The readers might expect to review current progress for all those categories in the related work section.

Limitations:
The author has addressed limitations of their work in terms of sample complexity.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a way to calibrate ReLU networks or random forests by breaking them down into piecewise linear functions on polytopes and replacing the linear parts with Gaussian kernels. This approximation allows to naturally calibrate the models for the ID domain, where confidence will be high due to the density of ID samples that translates into high kernel values, and for the OOD domain, where confidence will be low due to the large distance to ID samples.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The method is novel and mathematically grounded
- The presentation is clear
- The benchmarks are OK

Weaknesses:
The main weakness I find is about the computational time of the method. The number of polytopes scales exponentially with the number of neurons, so I am concerned with the applicability of the method to large (or even medium-scale) neural networks. What is the computational cost of the method for the considered benchmarks, in terms of runtime?

The toy simulations are unnecessarily tedious to grasp and take up a lot of space. I do not say that they are complex, but they hinder the reading flow and do not bring much to the presentation. I would advise putting some of them in the appendix to leave more space for other explanations. Indeed, Section 5 is difficult to read (many ""chunk"" paragraphs with mathematical notations) and would benefit from more structured writing and more flow.

Limitations:
The authors have adequately addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
bNVvcgfEwD;"REVIEW 
Summary:
The paper examines the convergence properties of Adafactor, an adaptive learning rate optimizer designed for deep learning tasks, particularly in memory-constrained environments. The study focuses on Adafactor’s performance in non-convex optimization scenarios and provides theoretical convergence proofs under smooth conditions. Despite its widespread practical use, especially for training large language models, Adafactor’s theoretical understanding has been limited. This research fills that gap by proving that Adafactor can reach a stationary point with a specific convergence rate, highlighting both its efficiency and the impact of different parameter settings on its performance. The paper also introduces modifications to the default hyper-parameter settings based on theoretical insights, which are validated through empirical tests, showing potential improvements over traditional setups.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The analysis of Adafactor’s convergence is crucial for its application in training extremely large models, such as large language models (LLMs). This study significantly contributes to the theoretical foundations of Adafactor, supporting its practical use with a solid mathematical framework.

Weaknesses:
The manuscript could benefit from a deeper discussion on the proving techniques and the tightness of the provided convergence bounds.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the convergence of Adafactor for non-convex smooth objectives. The paper looks at both full batch and stochastic cases and analyze the convergence rate. Experiments are provide to validate some of the findings about the hyperparameters. 

The main contributions of this paper are: (1) convergence rates for both full-batch and stochastic cases for Adafactor (2) Provide empirical evidence that the hyperparameters leading to optimal convergence rates yields better empirical performance. My main concern regarding this paper is about novelty and lack of comprehensive experimental evidence. First regarding novelty, convergence of Adaptive methods has been studied in several earlier papers for full-batch settings (e.g. De et.al., 2018). Similar, the issue of second moment decay parameter increasing at the rate of 1 - 1/k is well known (e.g. Reddi et al., 2018) and under this particular schedule, the algorithms roughly boils down to Adagrad like schedule (instead of exponential moving average). Both these contributions are quite well-known for adaptive methods. Thus, it is not entirely clear to me if these contributions for Adafactor are novel enough to warrant acceptance.

Furthermore, Shazeer et.al., in Section 7.2 of their paper already discuss about the aspect of second moments decay. The experiments in the current paper are neither comprehensive or convincing that this leads to he optimal convergence in practice. It is important to do a very thorough investigation if the authors have to demonstrate this phenomenon (e.g. try it in different NLP settings where adaptive methods are very effective). Overall, in my opinion, the main weaknesses of this paper are novelty and poor empirical study.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
See summary

Weaknesses:
See summary

Limitations:
See summary

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the convergence of a memory-efficient, adaptive algorithm, Adafactor, under non-convex smooth settings. First, the authors show that in the full-batch setting (with appropriate hyperparameters), Adafactor converges to a stationary point at an $\tilde{O}(1/\sqrt{T})$ rate. For the stochastic setting, they study two regimes: with and without clipping $\eta_k$, and show that under appropriate selection of hyperparameters, Adafactor attains an $\tilde{O}(1/\sqrt{T})$ rate of convergence to the stationary point, matching SGD up to logarithmic factors. The observations are complemented with empirical findings.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Considering large-scale language model training, the work focuses on an important memory-constrained practical optimizer for which we have limited theoretical understanding. The work is clearly motivated, the introduction to Adafactor and its connection to Adam is concisely discussed, and the paper is easy to follow in general. Even though it’s not exactly vanilla Adafactor, it’s exciting to see the authors have established bounds matching SGD in the stochastic case. I also appreciate the authors not shying away from discussing the impact of various hyperparameters (and potential negative points).

Weaknesses:
Even though there are space constraints, I would like to see some proof sketch (at least for the full-batch case) in the main body to provide a general sense for the reader. For example, it could be as simple as starting from smoothness in Taylor series (Eq. 14), telescoping over $k$, lower bounding (a), upper bounding (b) in Eq 20. The more critical step appears to be the lower bound, and a general flavour of how Lemmas $A.2, A.3$ are used to achieve that would be nice to see.

This is minor, but it would be helpful for the reader if figures are referenced whenever a discussion about some experiment is invoked. Two instances I noticed are line 222, the effect of $\epsilon_1$, and line 265, the effect of time-increasing $d_k$. Please look for other instances, if any.

Figure 1, experiment on the effect of different decay rate parameter $c$: Showing the test performance is nice, but as the discussion is about train-loss convergence, their time-evolution should be included. It’s fine if the convergence doesn’t speed up with increasing ($c$) and doesn’t match theory, but its important to have them as the entire work deals only with train-loss convergence.

Limitations:
Yes, the authors have addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
TEwQSWWn7S;"REVIEW 
Summary:
Training RBMs is challenging and slow due to the multiple second-order phase transitions and associated slow mixing of MCMC sampling. The paper introduces a pre-training method, consisting in integrating the principal directions of the dataset into a low-rank RBM through a convex optimization procedure. The Gibbs-Boltzmann equilibrium distribution of the pre-trained model can be efficiently sampled via a static Monte Carlo process. Starting from the pre-trained model, the standard Persistent Contrastive Divergence (PCD) training procedure for RBMs partially overcomes the problem of second-order phase transitions. The pre-training method is tested on the MNIST 01 dataset, a synthesized “Mickey” dataset, and the Human Genome dataset (HGD). The method is shown to outperform the PCD algorithm and the Jarzynski reweighting method (JarRBM).
The paper also introduces a new method to sample from the trained model, called Parallel Trajectory Tempering (PTT), and compares it with the Annealed Importance Sampling (AIS) method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Technically sound.

The proposed pre-training method is shown to improve on other training schemes for RBMs (namely the standard PCD method, and the Jarzynski reweighting method).

The novel PTT method is shown to improve on standard Gibbs sampling.

Weaknesses:
My main concern is about the usefulness of the RBM approach to generative modeling. (cf question Q1 below).

The PTT sampling method seems to require more memory than standard methods for RBMs.

It is unclear how novel the proposed pre-training method is. (cf question Q2 below)

The paper contains a link to a github repository that reveals the author’s identity (section 8 page 9):
https://github.com/nbereux/fast-RBM

Minor. Line 465: Appendix A.2 references itself.

Limitations:
See Weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work proposes to pretrain RBMS with a recently developed convex approximation, the restricted coulomb machine and then fine-tune the model using standard techniques like PCD. Further, a novel sampling technique, PTT is proposed that can sample from the final trained model by employing a sequence of model snaphsots during training, that are then connected via replica exchange in the style of parallel tempering.

Experiments are conducted and results show that the sampled distributions, when projected to the first few principal components, match the true distribution better. Moreover, log-likelihood comparisons based on single training runs show that the proposed method starts at much higher likelihood values due to the initialisation and there is some evidence that the resulting model als reaches higher likelihood values. Further experiments for PTT show that it is more likely to jump between clusters of the distribution than PCD based on gibbs sampling

Disclaimer: This is an emergency review. I have not had the time to do a detailled analysis, or implement/reproduce any of the results. While I am expert in the field, I will adapt my confidence accordingly. I will not fullfy abide to the review format.

Edit: An edit has been performed that only included changes of the format, but not the content.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The use of the approximation using the restricted coulomb machine as an initialisation is an interesting idea that is worth investigating and the results in Fig 3C suggest that this pretraining approach is very effective.

- The idea of using the training models as sampling steps is an interesting approach as well.

In general, i can see that both techniques can become tools in a more general RBM toolbox, however both of them seem to be incremental changes, even though they could impact RBM training a lot.

Weaknesses:
The main weaknesses of the present work are in two areas: experiments/comparisons and language, of which I only deem the former critical, while the latter will limit the potential impact of the article in the machine-learning community. As a result of the weaknesses, this paper has a number of misleading claims or claims that are not supported by the data presented in the work.

Experiments/Comparisons:

- The authors dismiss using PT for being ""expensive"" and do not compare against it. This is against evidence that even single steps of PT chains with a moderate number of parallel chains can be more efficient than PCD using similar resources. This is especially interesting, since the authors use PCD-100 for training, which allows a lot of resources for PT.  Note that the authors themselves reference [40] which shows an order of magnitudes improvement over Gibbs sampling already with 10 chains.
Another example is given by 
[*] https://www.sciencedirect.com/science/article/pii/S0004370219301948
where the authors used PT for training with k=50 chains and only a single sampling step per iteration. Note that this paper uses a very simple heuristic to improve PT sampling: choose the reference (temperature ->infty) distribution as not the uniform, but the marginal data distribution. This is in contrast to [40] that used the uniform distirbution, leading to a significantly worse baseline.

- There are almost no comparisons of log-likelihood values or values of normalisation constants for the proposed technique. While some are given in Figure 3C, they are only single run and only using approximated likelihoods. Due to this, the phrase "" significantly higher log-likelihoods"" in the conclusion is NOT supported by the data, given there is not enough data to test for significance or even measure the variability.

- The training is also cut short, or the RBMs trained are not powerful enough, since 3B shows clear artefacts in all samples, indicating that none of the machines approximate the dataset well. Since RBMS with enough latent variables are universal function approximators, we can not get a definitive statement of whether the proposed pretraining does allow for better likelihoods. Since the experiments are not very expensive, this reviewer would propose to at least repeat experiments in order to obtain error bars on Fig 3C. 

- The learning rate of 0.01 used in the experiments seems to be on the high end. This is not only bad for PCD training, but also for obtaining high likelihood values, and could explain big parts of the leveling out of the graph in Fig 3C. While it is okay to use a high learning rate in the beginning, keeping it constant over the course of training seems like an oversight.

- For PTT especially, there are no good comparisons that compare the quality of the samples in terms of representing the true distributions. While visual examples are shown that show visually good mixing, the baseline to compare to is again PCD, and not PT, nor stacked tempering. Since, again [40] showed that both alternatives clearly beat gibbs sampling given the same amount of resources, we do not know how good this sampling scheme really is, compared to strong baselines. This reviewer suggests to compare in at least one experiment the proposed approach to an approach with known normalisation constant and then measure how well estimators based on these samples work, similar to [*]. This would also partially verify 3F. 
- As an addendum to the previous point: [*] showed that the performance of AIS improves significantly with the choice of reference distribution. Using the same distribution as [*] or the pretrained distribution proposed in this work, might diminish any performance gains by PTT. This would still be a major improvement to the state of the art.



Language:
- While in general well written, this work reads like targeting a physics community, not the ML community. As a result, this paper includes  slang terms mostly encountered in statistical physics/thermodynamics, which do not have a clear meaning in the statistics community. This includes terms like ""phase transitions"" (first and second order), ""equilibrium models"" (not consequential for the article, nor the review, but this reviewer genuinely does not know what this term is supposed to mean), ""critical transitions"", ""relaxation times"", ""free energy"". Since most of these terms appear in sections where the authors try to explain the method and/or its consequences, a significant number of readers will not be able to understand those reasonings as they do not have a grasp of the physical analogies. This reviewer would propose to replace some of the terms by the statistical equivalent, or to introduce them. 
- Some of the explainations and reasonings are misleading. The initial paragraph highlights that RBMs are supposedly interpretable. While they are simple models, Binary RBMs are still universal function approximators and thus it is highly unlikely that the latent space has any meaning that aligns with any human interpretable semantics. If the authors disagree with this, This reviewer encourages them to add a citation to line 25.
- Missing citations: the datasets used should be cited.
- Figure references missing: the article does not always refer to the figure they are talking about, e..g, line 205. In general references should include the subfigure letter as the article does later, e.g., lines 263+

Limitations:
The authors mark ""Yes"" for point 2 ""Limitations"" on the checklist. This reviewer has not found that the authors discussed the limitations of their work. This especially includes the guidelines that say that authors should ""reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs."" However, in fairness, the authors answer ""[NO]"" in question 7, discussing the presence of error bars or significance tests to measure significance. However, since this is not part of the final publication, and the authors include misleading claims about significance of results, this must be discussed in the main text, or the phrasing weakened.

I have not found any information on runtime or CPU/GPU hours, but the CPU/GPU were reported, so point 8 is partially fulfilled.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The manuscript suggests to apply 

Aurélien Decelle and Cyril Furtlehner. Exact training of restricted Boltzmann machines on intrinsically low dimensional data. Physical Review Letters, 127(15):158303, 2021.

to initialise persistent contrastive divergence (PCD) learning for RBM training and estimating the log likelihood / partition function of RBMs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
While one may argue that the novelty is limited because the interesting theoretical work was done in the abovementioned paper by Aurélien Decelle and Cyril Furtlehner, the idea is sound and the approach may be useful in practice.

Weaknesses:
The novelty is limited because the interesting theoretical work was done in the abovementioned paper by Aurélien Decelle and Cyril Furtlehner.

My **main criticism** refers to the empirical evaluation, and I think these questions should be addressed:
- Was enough effort out into the baseline methods (including hyperparameter choice)?
- What would be an example where the PCA decomposition is misleading (i.e., not helping or even slowing down the process)?
- What about MNIST with all 10 digits?

**Details** (not ordered by importance):

* „On the diametrically opposite side (on interpretability)  are generative ConvNets [9, 10], where the energy function is formulated as a deep neural network, which are capable of synthesizing photorealistic images but are almost impossible to interpret as a physical model.“: Not clear, perhaps add half a sentence to elaborate.

* „second-order phase transition“: define what this is already in the beginning

* Beginning of section 2: I suggest to add the analysis in 

  Fischer, Igel. Bounding the Bias of Contrastive Divergence Learning. Neural Computation, 2011
  https://direct.mit.edu/neco/article-abstract/23/3/664/7646/Bounding-the-Bias-of-Contrastive-Divergence?redirectedFrom=fulltext

  to the discussion of the limitations of CD.

* „much better than those obtained with the standard Annealing Important Sampling (AIS) techniques“:
In [42], several methods are discussed, in particular one based on Bennett’s Acceptance Ratio method (BAR), which performed in general better than standard AIS. How does the proposed method perform in comparison to BAR?

* What if linear PCA is not well suited to find a good representation of the data because of a highly non-linear latent structure?

* I am not fully happy with the selected benchmark tasks. 
In particular: How does the method perform on MNIST with all 10 digits?

**Minor** comments: 

The reference list should be revised. Inconsistent capitalisation, author first name abbreviations, etc.

Limitations:
I think the limitations should have been explored in more depth.
What would be an example where the PCA decomposition is misleading (i.e., not helping or even slowing down the process)?

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
m772lelDe3;"REVIEW 
Summary:
The paper studies the conditions that lead to consensus in matrix-weighted consensus networks when constant time delays are present. The analysis considers both leaderless and leader-follower settings. The paper considers single integrators with uniform time delays, heterogeneous time delays, and double integrators with two constant time delays. The paper derives the conditions for asymptotic convergence to a consensus or clustering configuration. The mathematical techniques include direct eigenvalue evaluation and application of the Lyapunov-Krasovkii theorem. The paper explains how the derived results can be applied to the problem of bearing-based network localization.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
S1. 1The paper provides a novel theoretical characterization of the asymptotic consensus for matrix-weighted consensus networks in several settings. In multi-agent networks  

S2.	The analysis is clearly presented and the arguments are logical and well-structured. The employed techniques are innovative.

Weaknesses:
W1.	Although the scope of NeurIPS is broad, and papers are encouraged from diverse fields, this paper makes little attempt to connect to any learning-based problem or application. The paper seems to be a poor match for a machine learning conference. There is not a single citation of a paper from one of the leading machine learning conferences or journals. 24 of the 37 references are associated with papers in control journals and conferences. The paper would be of much more interest to the control community. If the authors consider this to be a significant research contribution that furthers the understanding of matrix-weighted consensus, then why not submit it to Automatica (this is the forum for several other cited matrix-weighted consensus papers), or IEEE Trans. Automatic Control, or IEEE Trans. Control of Networked Systems. If there is a belief that the paper exposes a problem or a technique to the machine learning community, then there must be a much more convincing effort to highlight the connections – where/how would the machine learning community find the presented results useful? 

W2.	The main paper shows how the theoretical results are applicable to the bearing-based network localization problem. There is very little explanation of whether the proposed approach to localization is advantageous, and how the theoretical results are useful – whether it is for analysis of a network, or for design of a network. The appendix provides the results of simulations. But even there, the discussion is limited to simple observations regarding the behaviour of the simulated network (consensus/instability). The simulation analysis needs to be more convincing and explain in detail how the theoretical results are useful for this problem. Alternatively, additional theory could be provided that pertains to the bearings-based network localization task. 

W3.	The presentation of the paper should be improved. In particular, the figures on pages 16-18 are far too small. The text in these figures is illegible. It becomes almost impossible to understand what information the figures are supposed to convey.

Limitations:
The paper includes one or two sentences in the conclusion to discuss the limitations. The only acknowledged limitation is the restriction to the constant time delay setting. The paper would be strengthened by a much more thorough discussion of the limitations. For example, it would be helpful to understand whether the authors consider the constant time restriction to simply lead to conservative bounds, or whether the results would be completely inapplicable in a variable time delay setting.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Paper under review analyzes the consensus of agents over a network. The agents have arbitrary but identical state space dimension, so not just scalar dynamics. The communication between agents is delayed and can be heterogeneous. Lyapunov–Razumikhin functionals with an LMI (that grows with the size of the network) are the main analysis tools. The literature in this area is vast. The results may already be contained as special cases of more general results from the control literature that was not referenced in the original submission

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- originality: very crowded area of research
- quality: high quality except for not being very precise about what is different from other work, and lacks a detailed discussion about what impacts the LMI bound
- clarity: paper is well written
- significance: more general results on the same topic appear to already exist

Weaknesses:
- Authors discuss some of the relevant literature but are never explicit about what actually is different. This makes it very challenging to understand what is different and new about this contribution.
- Along the same lines as the above statement. Results and analysis techniques may be already present in the papers referenced below.   
	- Jiang, W., Liu, K., & Charalambous, T. (2022) in particular solves the more general problem of consensus with heterogenous delays where each agent is an arbitrary linear dynamical system (A,B,C). The results in this paper appear to be a subset of that class of systems.
- LMI grows with the size of the network making it not scale well
- No interpretation of the LMI once it is derived (but they are huge matrices which kind of make them hard to interpret)

Limitations:
limitations discussion is not really sufficient, they only discuss future directions

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates consensus conditions for matrix-weighted consensus networks, both leaderless and leader-follower, in the presence of constant time-delays. It explores delayed consensus algorithms for networks of single- and double-integrators using relative positions. The study derives conditions for networks to achieve consensus or clustering using eigenvalue evaluation and the Lyapunov-Krasovkii theorem. It also discusses an application in bearing-based network localization. Some numerical simulations are also provided to demonstrate effectiveness of the results.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper studied an important problem in the control theory, proposing novel algorithms and extending existing work to matrix-weighted networks with time-delays. They provided structured and rigorous mathematical analysis/proof of their results and provide helpful numerical simulations to validate their theoretical results.

Weaknesses:
In my opinion, the major weakness (in terms of publication at NeurIPS) of this paper is below. This paper has strong focus on control theory and consensus algorithms, which makes it less relevant to the core interests of the NeurIPS audience, since NeurIPS emphasizes more on machine learning methodologies and applications. The paper does not clearly establish connections to machine learning problems or provide good experimental results involving machine learning tasks.

This paper contains quite some valuable novel research results, but it seems to me it is more appropriate for publication on a traditional control journal or conference, rather than such a top tier machine learning conference like NeurIPS. It is just not a good fit, and it might be better to reserve the space (which is quite limited) for some other good candidate paper submissions more relevant to Machine Learning, which are more aligned with the interests of core audiences of NeurIPS.

Limitations:
N/A.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
AZuONuYzKl;"REVIEW 
Summary:
The paper addresses a major challenge in biology: identifying evolutionary traits, which are features common to a group of species with a shared ancestor in the phylogenetic tree. Compared to the existing works, this submission proposes new architectures and loss to avoid the over-specification problems. In the experiments, the authors demonstrate that the proposed method improves existing works and set up ablation studies to show the impact of different components of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
[+] The paper introduces HComP-Net, a new architecture designed to discover evolutionary traits from images in a hierarchical manner. This addresses the limitations of current prototype-based methods that operate over a flat structure of classes.
[+] Together with the architecture, the paper proposes contrastive loss and several additional losses to improve the performance.
[+] The inclusion of a novel masking module allows for the exclusion of over-specific prototypes at higher levels of the tree without compromising classification performance. This helps maintain the accuracy and effectiveness of the model.
[+] The proposed method not only improves the accuracy and other metrics, but also shows the generalizability to unseen species.

Weaknesses:
[-] More background: For most of the machine learning conference readers, I guess the proposed problem background is required. Therefore, more related work and background sections should be useful. 
[-] I wonder whether the proposed framework can address ""Convergent evolution"" and other similarity cases. Since these species can have similar features but should not be very close in the evolutionary trees. I suggest the authors to include more details and discussions about the background knowledge.
[-] While the framework has shown promising results on datasets of birds and other animals, I wonder whether the method can show its scalability to larger and more diverse datasets.

Limitations:
I do not think this work has potential negative social impact. The problems sounds very interesting.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a novel deep learning based algorithm named HComP-Net that can detect evolutionary traits common to groups of species with shared ancestors. Based on earlier studies, they aim to build a model that can accurately isolate common traits of specific species and reject over-specific features.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors presented their aims and methods quite clearly. While inspired by the earlier studies, they point out how their study is different from the earlier studies. To identify common visual features (i.e., evolutionary traits), they 1) combined two novel loss functions with a previously proposed loss and 2) used a novel masking module. Their results are compelling, which suggest the learning power of HComp-Net and its utility in detecting evolutionary traits. As HComp-Net may be used in other domains, this study can be of interest to other researchers.

Weaknesses:
HComp-Net was tested with only 3 datasets, which is understandable, as proper datasets may not be readily available. Still, a more thorough evaluation is desirable in the future.

Limitations:
The authors provided the limitations in the appendix.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a method that automatically learns multiple orthogonal embeddings to act as prototypes. This approach helps the discovery of hierarchical similarities by representing data in a structured space.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The writing is clear and easy to follow.   
The authors conduct experiments that with other methods, and the visualization of prototypes in feature maps, They also perform an ablation study on different parts of the loss functions.

Weaknesses:
In the comparison with HPNet, the authors modify HComP-Net by removing the final two max pooling layers, resulting in a more detailed 26x26 feature map. In contrast, HPNet produces only a 7x7 feature map as shown in figure 4(a). Since the architecture and effectiveness of these networks heavily depend on the resolution of feature maps, this discrepancy raises concerns about the fairness of the comparison. To ensure a fair comparison:
  * HPNet should also be adjusted to generate a larger feature map. 
  * This adjustment and its impact on performance should also be included in the ablation study section.

In the generalizing to unseen species section, the evaluation method used by the authors could be extended to include comparisons with non-hierarchical methods. This would provide a more comprehensive evaluation of the method's effectiveness across different types of classification challenges.

Limitations:
There are several limitations concerning the comparison with other methods.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors investigate the use of prototype-based explainability (as in ProtoPNet) for the visual discovery of evolutionary traits in biology image repositories.
In particular, the authors aim to find traits that apply to group of species in a hierarchical fashion, according ot the tree-of-life hierarchy. 
The authors identify three challenges with state-of-the-art prototype methods such as learning over-specific prototypes that do not apply to all species in a given group, and prototypes that do not descriminate between the group and other groups of species in the hierarchy. Ther main contribution is the design of a loss and a masking mechanism to mitigate those issues.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ Interesting application and qualitative results in evolutionary biology.
+ Dedicated focus on learning discriminative hierarchy-level features in an interpretable way.
+ The authors provide their source code.

Weaknesses:
- The results are limited to three relatively small datasets. Why are there no result on the iNaturalist dataset which is at least 57x larger than CUB-200- 2011?
- It was hard to judge the effectiveness of the approach from the provided figures. The images are quite small.
- It was also hard to assess the effectiveness of masking. The figures did not illustrate how it helps.

Minor: I encountered several language issues. Below are ones I noted:
- hiearchy
- seperation
- Futhermore
- scenarious => scenarios 
- overlayed => overlaid
- indicating to difference => to a difference
- hasn’t => has not [avoid abbreviations in a scientific text]

Limitations:
A fundamental limitation in the application domain of interpretable biological traits is discussed in section I. Beyond a few ablation studies focusing on the introduced losses, I missed a discussion on the limitations of the approach, in particular the effectiveness of masking.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
wH36UKML4x;"REVIEW 
Summary:
This paper addresses the problem of subpopulation generalization, also known as spurious correlations. Building on the Last Layer Retraining (DFR) method, it removes the constraints on a small subset of annotations. The paper introduces the Environment-based Validation and Loss-based Sampling (EVaLS) method. Unlike DFR, EVaLS divides the validation set $D^{val}$ into two parts: (1) $D^{LL}$,  where losses from an ERM-trained model are used as a proxy for identifying minority groups for retraining, and (2) $D^{MS}$, where environment inference methods are used for partitioning environments. The paper presents theoretical insights and empirical results demonstrating the effectiveness of EVaLS.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper is well-structured and presented in a clear and organized manner, making it easy to comprehend and follow along.
* The proposed method is simple but effective and explores a relatively challenging area in existing literature (*i.e.* subgroup generalization without group annotations). 
* The authors provide some theoretical analysis to support their claims.

Weaknesses:
* The novelty and contribution of the proposed method may be limited for the following reasons: 1) The paper combines multiple previously proposed methods (*i.e.* DFR [1], EIIL [2]) all at once, which inherently guarantees a nontrivial performance; (2) The primary technical contribution, at least from my perspective, is the loss-based sampling, which has been already explored extensively in the noisy label literature and has been used as tools for pseudo-labeling. 
* The paper fails to discuss recently proposed methods that also require no group annotations, such as SELF [3], BAM [4], and BPA [5]. In particular, SELF is also a direct follow-up of DFR. The authors are encouraged to discuss the limitations and strengths of loss-based schemes against the class-based schemes advocated by SELF and BAM.
* More analyses can be included to provide further understanding of the selected loss-based samples. For example, given a threshold, how much percent of the high-loss and low-loss samples are indeed the minority and majority samples and how does this percentage change with the threshold?

[1] Last Layer Re-Training is Sufficient for Robustness to Spurious Correlations, ICLR 2023

[2] Environment inference for invariant learning. ICML 2021

[3] Towards Last-layer Retraining for Group Robustness with Fewer Annotations. NeurIPS 2023 

[4] Bias Amplification Enhances Minority Performance. TMLR 2024

[5] Unsupervised learning of debiased representations with pseudo-attributes. CVPR 2022

Limitations:
Aforementioned in Weaknesses and Questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To address the issue of spurious correlations when group labels are unavailable, this paper proposes a new method called EVaLS. It first creates a balanced training dataset using loss-based sampling. Then, it evaluates the accuracy of the balanced training set based on the inferred environments from the validation set, and selects models accordingly.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written, and includes a rich set of experiments with necessary theoretical explanations.

2. It is essential to discuss the multiple  (unknown) spurious features case which has been overlooked in previous studies.

Weaknesses:
1. Why is the approach of using high-loss points (considered as the minority group) more effective than directly using misclassified points (considered as the minority group) in methods like JTT? Intuitively, compared to misclassified points, high-loss points are more ""implicit"" and no obvious thresholds, which could potentially result in high-loss points actually belonging to the majority group, thus exacerbating the imbalance in resampling.

2. If the author can show the balance level of samples obtained through loss-based sampling compared to directly using labels (misclassified points), it could further illustrate the advantages of loss-based sampling.

3. In Section ""Mitigating Multiple Shortcut Attributes"", if color is treated as a known spurious attribute and shape as an unknown spurious attribute, how would the performance of EVaLS be affected? Based on my understanding, there is a possibility that simplicity bias could cause the model to prioritize learning the simpler feature, color, and struggle to learn the more complex shape attribute. Therefore, considering color as known and shape as unknown can better show the performance of EVaLS in handling complex spurious features.

Limitations:
See weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies how to improve the model’s robustness to multiple spurious correlations when the group labels (indicator for spurious correlation) are unknown in general. The proposed approach, EVaLS, leverages the loss from a base ERM model to sample a balanced subset to prevent learning from spurious correlations. In addition, a new synthetic dataset (Dominoes-CMF) for multiple spurious attributes is crafted. Empirically, the proposed approach sometimes has advantages over the rest of the baselines when using the same amount of additional information (group label).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The main paper is generally well-written. 
2. The theoretical analysis in Section 3.3 (with derivations and proofs in Appendix) shows that for one-dimensional Gaussian distributions, choosing the tails on the two sides of the distributions creates balanced groups, even though the original data distribution is skewed. 
3. Environment inference technique is demonstrated to be useful for separating the dataset into groups with different distributions of the subpopulations and then for model selection. 
4. The proposed technique only requires last-layer retraining on part of the validation set, which is generally more efficient.

Weaknesses:
1. Figure 2 attempts to illustrate more minority samples have high loss while the majority samples have low loss. However, in each of the plots, only the % of one of the minority or majority groups is shown. The illustration can be improved by showing the % of both majority and minority groups in the same plot, and showing the actual distribution of the loss for the groups. 
2. Though the idea is straightforward, it is unclear how the loss-based instance sampling is actually implemented. It is helpful to provide an algorithm or pseudocode to improve the presentation. 
3. The theoretical analysis is generally sound but limited to a case without discussing the use of the loss (which may not be Gaussian) and the spurious correlations (which involve at least two dimensions of core and spurious features [1]). 
4. The experimental results are less polished and sometimes the advantages are not so clear over other baselines. Some results are missing for datasets such as UrbanCars and MultiNLI. Only a few baselines are compared for the new dataset in Table 2. There is also no convincing and fine-grained analysis (e.g., ablation study) to understand how the proposed approach ensures data balancing and improves group robustness. 
5. The paper initially focuses on improving group robustness when multiple spurious correlations are present, but the experimental results are lacking for these more challenging datasets. 

[1] Sagawa, Shiori, Aditi Raghunathan, Pang Wei Koh, and Percy Liang. ""An investigation of why overparameterization exacerbates spurious correlations."" In *International Conference on Machine Learning*, pp. 8346-8356. PMLR, 2020.

Limitations:
The authors have discussed the limitations in Section 5.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
lizRmKCnBp;"REVIEW 
Summary:
The manuscript introduces a neural compression paradigm for effectively compressing diverse sets of 3D geometry models. The authors propose a two-stage framework that first converts irregular mesh models into a regular 4D TSDF-Def volume representation and then employs a quantization-aware auto-decoder network to achieve redundancy elimination and compact representation. The method claims to compress a large number of 3D mesh models with high accuracy and preservation of geometric details, outperforming state-of-the-art methods both quantitatively and qualitatively.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper presents a unique method for compressing 3D geometry sets by leveraging neural networks, which is a significant advancement in the field. NeCGS achieves an impressive compression ratio, which is a critical metric for 3D geometry data compression.
- The method maintains high accuracy and preserves detailed geometric structures even at high compression ratios. The authors have conducted comprehensive experiments and ablation studies across various datasets, demonstrating the effectiveness of their approach.
- The inclusion of source code in the supplemental material enhances the reproducibility and transparency of the research.
- The paper is well-organized, with clear explanations of the methodology and results.

Weaknesses:
- The manuscript mentions that the optimization process for TSDF-Def volumes is time-consuming (over 15 hours), which could be a limitation for practical applications. The manuscript should address the long optimization time required for the TSDF-Def volumes. Future work could focus on accelerating this process to make the method more practical.
- While the method performs well on tested datasets, it is unclear how well it generalizes to other, more complex, or varied 3D geometry sets, such as some geometry with thin structures or open boundaries (cloth). 
- The choice of an auto-decoder network is effective, but the paper could benefit from a more detailed explanation of why this architecture was chosen over others.
- While the method outperforms existing techniques, a more thorough comparison in terms of trade-offs, especially related to computational resources, would be insightful.
- The paper could provide more insights into how the method scales with the size and complexity of the 3D geometry sets. The paper should include scalability tests to understand how the method performs with larger and more complex datasets.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a neural compression algorithm, NeCGS to significantly compress geometry datasets. The algorithm mainly consists of 2 components, 1) regular geometry representation: This is an optimization algorithm to optimize the TSDF field such that the error between the original geometries and the geometries reconstructed by the deformable marching cube algorithm is minimized and 2) compact neural representation: regresses the optimized TSDF-def fields from compressed latent states, quantizes the latent states and compresses them further into bitstreams. The trained decoder can then be used to reconstruct the TSDF-def fields and the geometries can be reconstructed using the DMC algorithm.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The NeCGS algorithm can provide high compression ratios with impressive reconstruction capability of the geometries. Better geometry representations can be achieved using the proposed optimization algorithm. This is evident from the ability of the DMC method to accurately reconstruct surfaces. The DMC algorithm is also significant and seems to provide better reconstruction of detailed structure in the geometries. Overall, the developed compression method has high potential and the results presented in the paper are very impressive.

Weaknesses:
The biggest weakness of the proposed approach is the computational cost of the method. The exorbitantly large times required to compress the datasets reduce the value proposition. Additionally, it is not clear how much the computational cost scales with the size of the geometry dataset.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method to compress 3D geometry of diverse categories of objects. In the first step, the paper proposes a method to first convert an irregular mesh to a regular representation like a 4D TSDF-Def volume that implicitly describes the geometry. After this, an auto-decoder is trained that learns to reconstruct the 4D TSDF-Def volume from a compressed feature vector which is unique for each shape. Hence, with this design the model can summarize the similarity of local geometric structures within and across different 3D meshes resulting in a compact representation. Results on AMA, DT4D and Thingi10K datasets shows that the model can achieve compression of 3D models to a reasonable extent.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) **Clarity:** the paper is well written with each component of the method explained clearly which is easy to understand.
2) **Reproducibility:** All the details to replicate the results are provided along with the code and architecture details in the supplementary material.

Weaknesses:
1. The intuition behind preferring TSDF-Def 4D volume over TSDF 3D volume is unclear, even though an ablation study shows better reconstruction for thin structures. The quantitative results in Table 2 only show marginal improvements. An brief intuitive explanation of the design choice is helpful.
2. There are lot of methods which try to compress a neural field. For e.g. Triplanes[1], HashGrid [2], Vector Quantization [3], TensoRF [4], Dictionary Fields [5]. It is not very clear why this method does not compare with all these techniques which can be used for compression? 
3. Can this method generalize? Can I use the trained auto-decoder setting to compress a new 3D mesh on which the model is not trained on? How about other methods with which the method compares.
4. The paper does not do a relative comparison of the compression time with the baseline methods. Given the optimization time shown in Table 3, I have concerns about the practical usage of this method.

[1] Peng, Songyou, et al. ""Convolutional occupancy networks."" ECCV, 2020. \
[2] Müller, T., Evans, A., Schied, C., & Keller, A. (2022). Instant neural graphics primitives with a multiresolution hash encoding. ACM TOG, 2022. \
[3] Takikawa, Towaki, et al. ""Variable bitrate neural fields."" ACM SIGGRAPH, 2022. \
[4] Chen, Anpei, et al. ""Tensorf: Tensorial radiance fields."" ECCV, 2022. \
[5] Chen, Anpei, et al. ""Dictionary fields: Learning a neural basis decomposition."" ACM TOG, 2023.

Limitations:
Limitations are adequately discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
this paper looks at the problem of compressing 3d shapes (esp geometry). this paper proposes a two stage approach. the first stage is regular geometry representation. the second stage is compact neural compression. results show some improvements.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. compressing 3d shapes is important to many applications

Weaknesses:
1. this paper over claims what it does. in L1-3, it says that they made the first attempt to tackle the problem of compressing 3D geometry sets containing diverse categories. this isn't true. there are at least two papers doing geometry compression of 3D geometry [a], [b].  

[a] On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes https://arxiv.org/abs/2009.09808
[b] Neural Progressive Meshes https://arxiv.org/abs/2308.05741

2. [a] and [b] are very important references but they are not cited nor discussed. it's not necessary to compare the proposed method with [a] and [b], but at least the authors should acknowledge the existence of these two papers.

3. optimization time is too long

4. it is unclear whether the proposed method is reproducible

5. typo L43: Matching cubes -> Marching cubes

Limitations:
yes

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";0
dQmEIwRw16;"REVIEW 
Summary:
This paper studies the loss function for soft class labels and entropy-based clustering. In particular, it introduces a new loss function called 'collision cross-entropy' as an alternative to Shannon's cross-entropy when class labels are represented by soft categorical distributions. The motivation for this new loss function is to handle ambiguous targets/labels in classification. The authors provide an EM algorithm for pseudo-label estimation and conduct experiments to demonstrate that this approach leads to improvements in classification accuracy when models are trained with soft, uncertain targets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The proposed collision cross-entropy may have advantages over Shannon's cross-entropy when handling soft labels in certain scenarios.
- Through experiments, the authors demonstrate that the proposed method achieves better robustness to label uncertainty, which is important for self-labeled clustering methods.

Weaknesses:
- [**Theory-1**] The main contribution of this paper is proposing the new loss function 'collision cross-entropy'. However, there is not much theoretical analysis about this loss function. From the current paper presentation, the Eqn. (9) can be interpreted as a modified version (or inspired by) Eqn. (6). For example, by minimizing the new objective for learning linear models, could this new loss lead to the right linear classification model?

- [**Theory-2**] For the EM algorithm, is there any convergence analysis for the EM algorithm proposed in this paper? 

- [**Experiments**] State-of-the-art for comparison. The methods for comparison in Table 1/2/3 are not very recent. It is possible that the previous methods still work well and be the state-of-the-art. However, I found some recent papers could achieve much better results, for example, the ACC on CIFAR10 of [DTC+2023] is 92%+, however the result in this paper is <84%.


[DTC+2023] Unsupervised Manifold Linearizing and Clustering. Tianjiao Ding, Shengbang Tong, Kwan Ho Ryan Chan, Xili Dai, Yi Ma, Benjamin D. Haeffele. ICCV 2023.

Limitations:
See Weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Soft labels are often used to represent ambiguous/noisy/uncertain targets in classification, particularly in self-labelled clustering, where pseudo-labels are estimated together with model parameters.
The authors propose an alternative to Shannon cross-entropy for a loss term, called the collision probability. 
This term arises as a limiting case of a Renyi entropy, or as a probability that two random variables are equal.
The collision cross entropy admits several advantageous properties: it is robust to large deviations in the target data, it agrees with Shannon cross-entropy for one-hot labels, it is symmetric, and points that are labelled as uniform distribution have no contribution to training.
The authors provide an EM algorithm for pseudo-label estimation and show state of the art results.

Soundness:
3: good

Presentation:
3: good

Contribution:
1: poor

Strengths:
- The paper is **very well written**, providing strong intuition and flowing prose. The intuition in Figure 1 is helpful, especially in showing that the proposed measure is robust to large target errors. 
- The main technical element appears to be an EM algorithm for solving the clustering problem obtained by using a collision cross-entropy in place of the Shannon entropy (which a swap in the arguments between equations (10) and (11)). This algorithm appears to be **technically sound**, and guarantees convergence of the subproblem in the M step. 
- The proposed term has **several nice properties** (as I mentioned earlier). Tt is robust to large deviations in the target data, it agrees with Shannon cross-entropy for one-hot labels, it is symmetric, and points that are labelled as uniform distribution have no contribution to training.

Weaknesses:
- My main concern is that **conceptually, the contributions are rather limited**. Generalisations of entropy are well-known, and as far as I understand (correct me if I am wrong), the main contribution is that authors use a different measure of entropy to Shannon entropy inside existing formulations. This leads the authors to investigate EM-algorithm and empirical performances, but as the paper is currently written (see further comments below), I cannot see whether these EM-algorithm and empirical performance benefits are actually real and beneficial. I also do not understand why this particular notion of entropy was used, compared with the other spectra of entropies. 
- An incomplete review of relevant generalized formulations of entropy is provided. This is not a weakness per se, however **perhaps the title in section 2.2 could be changed to something like Renyi Entropy**. Similarly tone down the discussion of generalised entropy measures throughout the paper. Alternatively, the authors might consider expanding their discussion and including more well-known entropy measures. For example, see section 8 and 11 of [1].
- The bold numbers in table 2 require clarification. The caption doesn't mention the number of trials (however the text mentions 6 trials). Compared with MIGD, excluding MNIST, due to the high variance in the trials, the results do not appear to be **statistically significant**. Perhaps the authors could consider running more trials and performing a significance test, and/or also bold relevant entries in MIGD. 
- As above for Table 3, 4 and 5. 
- It is **not clear how long the method takes to run** compared with competitors. Does the EM algorithm outperform a naive marginalisation of the log likelihood (using e.g. MC), both in terms of time and in terms of predictive performance? 
- Related to the above, is the reason for specialising on $\alpha \to 1$ because it allows for the EM algorithm? If you consider other values of $\alpha$, how do the results compare in terms of time and performance. Or is this setting intractable?


[1] Generalized Thermostatistics, Jan Naudts, 2011.


Minor:
- The text in the tables is too small to read without zooming in a lot.
- Recommend less active tense in the abstract: ""In case of soft labels y, Shannon’s CE teaches the model predictions σ to reproduce the uncertainty in each training example"" could be ""In case of soft labels y, Shannon’s CE results in model predictions σ which reproduce the uncertainty in each training example"".

Limitations:
The author checklist appears to be incomplete. The authors answer NA to ""Does the paper discuss the limitations of the work performed by the authors?"", without a justification. I do see a small discussion around local minima and numerical instability towards the end of section 4, but I think these could be further elaborated on.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces the concept of collision cross-entropy (CCE) as an alternative to Shannon's cross-entropy (SCE) for self-labeling in the context of unsupervised and semi-supervised learning. The primary motivation is to address the limitations of SCE, especially its sensitivity to label noise and uncertainty. CCE aims to enhance robustness to such uncertainties by defining a probabilistic interpretation that encourages collision events between predicted and true distributions. The paper provides theoretical foundations, describes an EM algorithm for efficient optimization, and presents experimental results demonstrating the superior performance of CCE over SCE on the task of deep clustering.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Originality
-The paper introduces a novel loss function, the collision cross-entropy, which is well-motivated by the need to handle soft and uncertain labels in classification tasks, particularly in self-labeled clustering. The idea of maximizing the collision probability is distinct from the traditional approach of minimizing the (implicit) KL divergence between distributions.

Quality
-The paper provides a solid theoretical foundation for the collision cross-entropy, including its properties and relationship to other entropy measures. The derivation of an efficient EM algorithm for pseudo-label estimation further strengthens the paper's technical contribution.

Clarity 
- The paper is generally well-written and organized. The motivation, theoretical analysis, and experimental results are presented clearly. The authors provide sufficient details for an expert reader to understand and potentially reproduce the work.

Significance
- The proposed collision cross-entropy has the potential to be a valuable tool for handling soft and uncertain labels in various machine learning tasks.

Weaknesses:
Quality
- The superiority of CCE seems to hinge on making the model capture the same ""decisions"" as the target distribution, without forcing the model to capture the entirety of the distribution, as well as de-weighting target distributions which are not spiky. While the properties of the loss are clear, it is not self-evident to me that the properties *of the loss function* translate into necessarily *better properties for models*, both as a function for training a classification model directly or for clustering. 
- In addition, the experiments were conducted on fairly old architectures (VGG, ResNet) and small datasets. Often improvements on small datasets do not translate into improvements on larger-scale models. I would encourage the authors to examine for full imagenet dataset at the very least. This also open up the capability to look at various robustness / calibration properties of the models on the various corrupted forms of ImageNet. 

Clarity
- Certain sections, the task to which this method is applied and the desired model properties for the task could be more clearly explained. It took me a while to get my head around the deep clustering task which the authors are solving. 

Significance
- The impact of CCE on real-world applications beyond the presented datasets and tasks could be further elaborated. This notion that CCE is better for noisy pseudo labels immediately suggests to me examining it as a loss function for doing distillation / noisy teacher-student training of a model on a pseudo-labelled corpus of data, however, I didn't see any links to the area of distillation / teacher-student training within this paper.
- The significance would be bolstered by demonstrating CCE's performance on larger scale, more diverse and challenging datasets.

Limitations:
**Strengths:**
- The paper acknowledges the need for robustness to label noise and addresses this effectively through CCE.
- The paper briefly mentions the potential increase in privacy disclosure risk with larger synthetic datasets but does not elaborate on this limitation or discuss potential mitigation strategies. It would be beneficial to include a more detailed discussion of the privacy implications of the proposed method and any potential negative societal impacts.

**Weaknesses:**
- The discussion on limitations could be more explicit, particularly regarding any assumptions made and potential edge cases where CCE may not perform optimally.
- The experimental evaluation uses very old model architectures (VGG, ResNet) and small datasets (CIFAR-10, CIFAR-100, MNIST, STL-10) which feature images only as large as 96x96 pixels. I would be curious whether the advantages of this method translate to high-dimensional image data with more classes, and on more modern, transformer-based  architectures (eg: ViT) .
- Similarly, could the author's consider extending this approach to deal with pseudo-labelled sequence data, for language models or for translation models, for example?

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on the choice of the loss function in problems with soft distributions of the labels, in particular in the context of pseudo-labeling for unsupervised or self-supervised problems such as clustering. In sections 1-2 the paper gives a thorough review of existing practices and relevant theoretical research. In section 3 the paper proposes a new collision cross-entropy loss as a replacement of the standard Shannon loss, and discusses various aspects of this new loss. In section 4 the paper proposes a new EM algorithm for pseudo-label estimation in connection with the new loss. Finally, in Section 5 the new algorithm is experimentally compared with existing ones and is shown to outperform them.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is generally well-written. Sections 1-3 contain a thorough discussion of entropies and losses suitable for self-labeled clustering, with abundant references. The main point of the paper, the new collision cross-entropy loss, is well explained and motivated. The paper provides an experimental comparison of the proposed algorithm with alternatives and shows its significant advantage.

Weaknesses:
I'm confused by mixing the discussion of losses and EM algorithms in section 4. The bulk of the paper is focused exclusively on the advantages of the proposed new collision cross-entropy loss. The main claim in the abstract and introduction is that the proposed loss is better than the standard Shannon loss. The EM algorithm is mentioned only in the last line of abstract, as if in passing. However, the experimental comparison in section 5 obviously crucially depends on the EM algorithm proposed in section 4. How can we tell if the experimentally demonstrated advantage is due to the new loss or the EM algorithm? Since the main claim is about the superiority of the loss, why not just take any existing soft-labeled clustering algorithms and replace the standard Shannon loss by the proposed new loss? In my opinion, the lack of such a direct comparison substantially weakens the main claim of the paper. The advantage shown experimentally is good, but the conceptual takeaway may be misleading.

I found section 4 on the EM algorithm harder to read relative to the other sections (in fact, I'm not familiar with such algorithms and not even sure what EM stands for - apparently Expectation-Maximization, but this acronym is not explained in the paper). In constrast to the other sections, this one seems to assume familiarity of the reader with related algorithms. I didn't understand, for example, how equation (14) (E-step) was derived.

Another weakness I see is that the strongest results of the paper are largely experimental (not counting general arguments and auxiliary theoretical constructions in Section 4), but, as far as I understand, they are not easily verifiable since the code is not open-sourced.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";0
fTOw3BzcWs;"REVIEW 
Summary:
The paper introduces ExID, an offline reinforcement learning algorithm that enhances learning performance in limited data scenarios by combining domain knowledge in the form of simple decision trees with agent experience replay data.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Domain Knowledge Utilization: ExID incorporates domain knowledge to guide decision-making in data-limited scenarios
* Teacher-Student Architecture: A teacher network, informed by domain knowledge, regularizes a student critic network to improve generalization.
* Regularization with Domain Knowledge: The algorithm uses a regularization term to align the critic's decisions with the teacher's advice for states covered by domain knowledge.

Weaknesses:
* Discrete Action Space Limitation: The algorithm is currently limited to discrete action spaces, necessitating future extensions for continuous action domains.
* Hyperparameter Tuning Challenge: The need for precise hyperparameter tuning complicates the deployment of ExID in scenarios where extensive optimization is impractical.
* The paper does not have enough strong experiment comparisions. The methods of the paper is related with offline RL methods, such as SCQ[1], ReDS[2], A2PR[3], CPED[4]. But it lacks the experiments comparisions with offlien RL methods. I think adding some SOTA baseline methods will improve your paper. It is not required that experimental comparisons must be given, but at least add some discussion with these methods to the paper.  

References：

[1] Shimizu, Yutaka, et al. ""Strategically Conservative Q-Learning."" arXiv preprint arXiv:2406.04534 (2024).

[2] Singh, Anikait, et al. ""ReDS: offline reinforcement learning with heteroskedastic datasets via support constraints."" Proceedings of the 37th International Conference on Neural Information Processing Systems. 2023.

[3] Liu, Tenglong, et al. ""Adaptive Advantage-Guided Policy Regularization for Offline Reinforcement Learning."" In International Conference on Machine Learning (ICML). PMLR, 2024.

[4] Zhang, Jing, et al. ""Constrained policy optimization with explicit behavior density for offline reinforcement learning."" Advances in Neural Information Processing Systems. 2023

Limitations:
* The paper only conducts experiments in several simulated environments and a real-world sales promotion dataset, which may not fully verify the effectiveness and applicability of the algorithm in more diverse and complex real-world scenarios.
* The performance of the ExID algorithm heavily relies on the quality of the domain knowledge. If the domain knowledge is incomplete, inaccurate, or biased, it may mislead the learning process and result in suboptimal policies. Moreover, obtaining high-quality domain knowledge can be challenging and time-consuming in practice.
* The proposed method mainly concentrates on discrete action spaces, and its performance and applicability in continuous action spaces are not clear. This limits the algorithm's utility in many real-world control tasks that involve continuous action spaces.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies offline RL when data is limited. The authors propose a domain knowledge-based regularization technique to learn from an initial tracker network and limited data buffer. The experiments verified the effectiveness of the proposal, which outperforms the classic RL baseline methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed method is simple and technically reasonable.
2. The experimental results on the real sales promotion dataset show the proposal is a promising solution in real-world applications.

Weaknesses:
1. The technical novelty is limited. Despite the claimed use of expert knowledge, the method adopted by the paper is to directly train a policy from the knowledge, which assumes that the information provided by the domain knowledge is at the state-action level (a decision tree in this paper), which limits the feasibility of this method. Compared to the use of knowledge between latent concepts discussed in neuro-symbolic learning, I think it's more like traditional model distillation. 
2. In practice, limited offline data may come from domain knowledge-based strategies, such as human-designed rules, thus I have great concerns about whether these two can promote each other. Empirical studies on more real-world datasets or rigorous theoretical analysis will provide support to this issue and further improve this work.  
3. The introduction uses the sales task as an example, but the visualization is based on the Mountain Car dataset.
4. Definition 4.1 seems strange, why not directly define the offline dataset as a subset of the complete state spaces?
5. The $\eta$ in Proposition 4.2 is not well defined.

Limitations:
The authors have provided a discussion about the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel technique ExID, a domain knowledge-based regularization method, that adaptively refines initial domain knowledge to boost performance of offline reinforcement learning (RL) in limited-data scenarios. The key insight is leveraging a teacher policy, trained with domain knowledge, to guide the learning process of the offline-optimized RL agent (student policy). This mitigates the issue of erroneous actions in sparse samples and unobserved states by having the domain knowledge-induced teacher network to cover them. And the initial domain knowledge would be improved when the student policy reaches a better perform than the teacher policy.  Empirical evaluations on standard discrete environment datasets demonstrate a substantial average performance increase compared to traditional offline RL algorithms operating on limited data

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. Originality: The paper's originality lies in its integration of domain knowledge into offline RL through a teacher policy network. This approach addresses performance degradation in limited-data settings, which is a novel and underexplored area. The introduction of the domain knowledge-based regularization technique and adaptive refinement of initial domain knowledge are particularly innovative.

2. Quality: The quality of the work is evidenced by the solid theoretical analysis and the thorough empirical evaluations conducted on multiple standard datasets, including OpenAI Gym environments (Mountain Car, Cart-Pole, Lunar Lander) and MiniGrid environments, as well as a real-world sales promotion dataset. The results consistently show that ExID outperforms existing offline RL algorithms in these settings.

3. Clarity: The paper is well-structured, with clear explanations of the problem, methodology, and results. The use of diagrams and tables helps understand the motivation of the problem (figure 1), the proposed method (figure 2), illustrate the effectiveness of ExID (Table 1-2). Each section logically follows from the previous one, making the overall argument easy to follow.

4. Significance: By tackling the challenge of limited data in offline RL, the paper makes a significant contribution to the field. The proposed approach has practical implications for various real-world applications where data is scarce and expert knowledge is available, such as in business, healthcare, and robotics.

Weaknesses:
1. Generalization to Continuous Domains: The paper is limited to discrete action spaces, which restricts its applicability to a broader range of RL problems involving continuous action spaces. This limitation is acknowledged by the authors.


2. Scalability: The scalability of ExID to more complex environments that requires a complex representation (e.g., a significant large tree) of domain knowledge is not thoroughly explored.  It would be beneficial to understand how the method performs in such settings and what challenges might arise because the challenging of updating the domain knowledge represented in a complex representation could hinder the learning process of the student policy in the proposed method ExID.

Limitations:
The authors acknowledge several limitations of their work, including the reliance on the quality of domain knowledge and the focus on discrete action spaces. While these limitations are well-addressed in the paper, it may be worth to consider a broad evaluation:
   * Conducting experiments on a wider variety of environments that have larger state and action spaces, would provide a more comprehensive evaluation of the method's applicability.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";0
ohi00YhT3T;"REVIEW 
Summary:
The paper introduces a new, subject-agnostic visual reconstruction pipeline. They introduce a way to integrate across fMRI readings from different subjects and enhance their integration using LLMs. Through this integration they see a consistent improvement of high level semantic feature baselines in their reconstruction.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* An interesting new approach to align fMRI feature extraction with ViTs and LLMs
* Brain captioning with LLMs is highly novel. The results are promising in Table 1.
* Promising results on NSD reconstruction.
* Interesting set of analyses using GradCAM.

Weaknesses:
* Concerns about preprocessing and claims about cross-subject analysis
    * The authors propose a trilinear interpolation to combine BOLD signals across patches across subjects. To me, this makes many assumptions that may not be true or might miss several problems. The authors need to address these questions to substantiate the claim of removing subject-specific modeling or using some kind of mixed subject pretraining to combine subjects. 
    * For example, this method ignores inter-subject variability such as anatomical differences or functional differences, which can vary among individual subjects. 
    * The interpolation makes a linear assumption on BOLD responses which may not be true. I realize this will follow with alignment to a ViT later but this imposes strong assumptions about BOLD signal space. 
    * My main concern here is whether this kind of preprocessing will generalize to modeling over new subjects. Is this approach applicable to general subject populations? Could the authors add some clarification and expand on this point?
* LLM Interaction details
    * My understanding is that there is a finetuning objective when using the LLM interaction. I think section 3.4 was cut off somehow and this wasn’t very clear to me.
    * Some details were missing making Table 1 a bit difficult to understand. See questions

Limitations:
* Limitations are addressed but may want to consider toning down some of the claims about subject variability (see weaknesses).

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a framework that integrates 3D brain structures with visual semantics using a Vision Transformer 3D. By aligning fMRI features with multiple levels of visual embeddings, it eliminates the need for subject-specific models and allows extraction from single-trial data. The extractor consolidates multi-level visual features into one network, simplifying integration with LLMs. The topic is intriguing, and the proposed method offers practical applications.

Major concerns and minor comments include:

1. Benchmarking with Real-World Datasets: As a novel machine learning approach, the performance of the proposed method should be benchmarked with more real-world fMRI datasets to evaluate the generalizability of the results. The current study may be limited in scope or sample size, and using a wider variety of datasets will help demonstrate the robustness and scalability of the approach across different subjects and conditions. It would be beneficial to include datasets with diverse characteristics, such as different brain regions, tasks, and populations, to ensure comprehensive evaluation.

2. Discussion on Advantages and Disadvantages: The author(s) should discuss the advantages and disadvantages of the proposed method in the field of neuroscience and brain decoding. This discussion should include a comparison with existing approaches, highlighting the unique contributions and potential limitations of the new method. Additionally, insights into the interpretability and explainability of the proposed method would be valuable. For instance, how does this method enhance our understanding of brain activity patterns? Are there any trade-offs between model complexity and interpretability? Addressing these questions will provide a clearer picture of the method’s potential impact and areas for improvement.

3. Details on Cross-Validation: Please provide more details about the cross-validation used in the empirical studies, perhaps in Section 4. It is essential to specify the type of cross-validation technique employed (e.g., k-fold, leave-one-out) and the rationale behind its selection. Detailed information on the partitioning of the data, the number of folds, and any stratification strategies used will help in understanding the robustness of the validation process. Additionally, discussing the metrics used for evaluation and how they were computed across different folds will add clarity to the reported results.

4. Pseudocode for the Proposed Method: The proposed method can be summarized in the form of pseudocode (algorithm). Providing a step-by-step algorithmic representation will make the methodology more transparent and easier to reproduce. The pseudocode should outline the key steps involved in data preprocessing, feature extraction, model training, and inference. Including comments within the pseudocode to explain the purpose of each step and any critical hyperparameters or configurations will further enhance understanding.

5. Minor Typos and Grammar Mistakes: There are some minor typos and grammatical mistakes throughout the paper. A thorough proofreading and editing process is recommended to improve the overall readability and professionalism of the manuscript.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Please see the Summary section

Weaknesses:
Please see the Summary section

Limitations:
Please see the Summary section

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an innovative framework that leverages Vision Transformer 3D (ViT3D) to integrate 3D brain structures with visual semantics, enhancing visual reconstruction and language interaction from fMRI data. By aligning fMRI features with visual embeddings through a unified feature extractor and integrating with LLMs, the framework enhances decoding capabilities, enabling tasks such as brain captioning, complex reasoning, concept localization, and visual reconstruction. The framework demonstrates exceptional performance and holds significant potential for applications in neuroscience and human-computer interaction.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Provides a multi-task visual neural decoding framework that is simpler and more elegant compared to previous methods, offering a robust example for integrating visual and neural signals with unified modeling. Introduces a method for precise localization of language-based concepts within brain signals simply using the GradCAM method.

Weaknesses:
The Dual-Stream fMRI Feature Extractor is already a common method in the field, but the combination of the two features appears suboptimal. The authors thoroughly discuss the trade-off between low-level and high-level features. However, based on the results in fig 4, the reconstructed images’ low-level features are inferior to previous methods.

Limitations:
The authors discuss the limitations well in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
- This model focuses on the task of reconstructing image stimuli from fMRI readings
- Instead of training a subject specific model, a subject-generic model is trained
- The model is built around a pre-trained LLM core, which is finetuned to take the fMRI as input and then:
  - engage in dialogue about the image stimulus
  - reconstruct the image stimuli by generating a language prompt which is then passed to UnCLIP
  - create a heatmap of activations in the brain associated with a given concept

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The multi-subject aspect of the training appears novel
- The use of an LLM for image reconstruction and dialogue about the stimulus also appears novel
- The image reconstructions are of comparable or better quality than existing approaches
- Ultimately, I think the evaluation could be improved (see Weaknesses), but given the novelty of the approach (using an LLM to do decoding and reconstruction), I argue for acceptance.

Weaknesses:
- It's unclear how much the image reconstructions reflect visual processing in the brain. It would be good to do an ablation to see how much the reconstruction depends on $\hat{z}_c$. What if $\hat{z}_c$ were replaced with random noise in the same way that an ablation was done on $\hat{z}_v$? Basically, I am curious about how much decoding can be done from the prompt $a_r$ only.
- In the same vein, it would be interesting to do an experiment where the generated prompt $a_r$ is given to a generative model that does not require $z_c$ and $z_v$, i.e., a model that can take a text prompt and generate an image.
- It's claimed that the model can engage in complex dialogue about the semantic content of the stimuli, but from the example prompts, it seems that many of the answers to the dialogue questions may be available in the prompt. For example, figure 11 seems to suggest that many of the complex reasoning prompts contain enough information to answer the question in the prompt. E.g. ``What is the distribution of the cows in the field based on the description and their positions in the image.'' A good ablation test would be to perform the complex reasoning task without the fMRI inputs.
- Some technical details are hard to follow, especially with regards to concept localization (see questions).

Limitations:
Limitations and potential negative societal impact are discussed in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
TuspoNzIdB;"REVIEW 
Summary:
This paper introduces a novel method, Hydra, for ab initio heterogeneous cryo-EM reconstruction. Different from existing approaches, Hydra separately models conformational and compositional heterogeneity by integrating K-parameterized neural fields to represent cryo-EM density maps. Furthermore, Hydra employs a hybrid optimization strategy to optimize particle poses, heterogeneity, and density map representations concurrently. The authors assess the efficacy of Hydra on three datasets comprising various protein complexes (two synthetic, one experimental). Extensive experimental results indicate that Hydra outperforms three baseline methods regarding reconstruction quality, particle classification, and pose estimation accuracy.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
This paper aims to tackle ab-initio reconstruction – simultaneously estimating poses and reconstructing 3D structure, which is one of the most challenging problems in cryo-EM. The scope of the problem has been further extended to a more challenging setting by assuming the captured particle images exhibit the motions of structure and conformational heterogeneity. Thus, the problem setting is novel and challenging.

This paper proposes Hydra, the first ab-initio heterogeneous cryo-EM reconstruction method based on a mixture of neural networks to estimate conformational and compositional heterogeneity in the training process simultaneously.

Weaknesses:
**Synthetic Dataset**

The number of images in the tomotwin3 synthetic dataset is too small to match the real cryo-EM setting; there are only 3000 particle images with a very low SNR of 0.01 (tomotwin3). In a real scenario, there would be 50,000 to more than 100,000 particles with conformational variability. Without reporting the 3D resolutions of the results, it is very hard to quantitatively evaluate the reconstruction results. Additionally, this synthetic dataset lacks conformational variability, which hinders the evaluation of conformational variability recovery. The combination ratio of the three types of particles is not explored either; it remains unclear if Hydra would be sensitive to a class with only a small ratio of particle numbers. In Section 4.3, the dataset settings that include pre-catalytic spliceosome, 80S ribosome, and SARS-CoV-2 spike protein are unrealistic. To sum up, I recognize a significant gap between this synthetic dataset and real datasets, and I feel the experiment is not sufficient to evaluate Hydra adequately.

**Reconstruction Quality Evaluation**

For qualitative evaluation, the differences between the various states in Figure 4 are very subtle, making it difficult to judge whether the surface changes are due to different conformations or the result of applying different thresholds to the density map. Also, I argue that cryoDRGN-AI and cryoDRGN2 also account for conformational heterogeneity without explicitly classifying particles; their qualitative results should also be compared in Figure 4.

For quantitative evaluation in Table 1 and Table 2, the authors only use Img-FSC to compare reconstruction quality. To the best of my knowledge, prior work such as cryoSPARC and cryoDRGN reports widely used 3D resolution calculated by thresholding the FSC curve between two half maps of the reconstruction for experimental datasets or between the reconstructed 3D density map and the synthetic ground truth density map (available for synthetic datasets).

**Choice of Metrics for Pose Evaluation**

The authors use the median Frobenius norm as the metric to compare pose error, which ignores the translation part. Referring to DRGN-AI, the in-plane and out-of-plane angle error, along with the translation error, should be reported. Additionally, showing the angle distribution of poses for comparison could be beneficial.

**Ablation Study**

This paper misses the ablation study of the number of K. I would like to know how to determine K in real cases. In this paper, the authors run CryoSPARC multiple times to determine the best K for H.

**Miscellaneous**

1.	The chirality of the reconstruction results for the pre-catalytic spliceosome in Figure 4 appears to be incorrect.
2.	The construction of the ribosplike dataset involves a mixture of three different proteins. In real single-particle cryo-EM experiments, this scenario seems rare as the purified samples are carefully prepared and should not contain or only contain a very small ratio of undesired particles that can be easily filtered out in 2D classification. Maybe in cryo-ET, Hydra can perform one-for-all sub-tomo averaging?
3.	What’s the protocol to run CryoSPARC on a synthetic dataset? If it performs so well in Table 1, what is the advantage of Hydra?
4.	This paper seems rushed. In Lines 57 – 62, the first three contributions have basically the same meaning; please consider rephrasing them.

Limitations:
The authors have addressed the limitations of Hydra in the main paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, author(s) porpose Hydra, an *ab initio* approach to model conformational and compositional heterogeneity. They ahieve this by parameterizing structures of proteins as a mixture of *K* neural fields.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Originality:

- Authors propose to incorporate neural network ensemble with recent *ab initio* reconstruction method to enhance the ability to model complex heterogeneity.

Quality:

- Through qualitative experiments, authors demonstrate the method is capable of clearly distinguish different compositional states in the synthetic dataset.
- Hydra is able to identify both compositional and conformational heterogeneity in real datasets.
- The method exhibits better quantitative results compared to existing *ab initio* reconstruction methods.

Weaknesses:
Novelty:

- The hierarchical pose search (HPS) method for pose estimation is proposed in DRGN-AI [1].
- Using an ensemble of representations to model heterogeneity in protein cryo-EM reconstruction has been adopted in many previous works [2][3].

[1] Levy, Axel, et al. ""Revealing biomolecular structure and motion with neural ab initio cryo-EM reconstruction."" *bioRxiv* (2024): 2024-05.  
[2] Punjani, Ali, and David J. Fleet. ""3D variability analysis: Resolving continuous flexibility and discrete heterogeneity from single particle cryo-EM."" *Journal of structural biology* 213.2 (2021): 107702.  
[3] Kimanius, Dari, Kiarash Jamali, and Sjors Scheres. ""Sparse Fourier backpropagation in cryo-EM reconstruction."" *Advances in Neural Information Processing Systems* 35 (2022): 12395-12408.

Limitations:
Authors discussed the limitation in their paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work describes a new method for ab initio heterogeneous reconstruction in cryo-EM using mixtures of neural fields. This generalizes previous approaches, such as CryoDRGN and DRGN-AI, which attempts to reconstruct 3D molecular densities using a single neural field representation. The resulting method is able to handle both compositional (discrete) and conformational (continuous) heterogeneity, with each mixture component handling the continuous variability of each distinct compositional state. The performance of the method is evaluated on two synthetic and one experimental dataset.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This represents a natural and well-structured extension of previous neural-field approaches to cryo-EM reconstruction. The method is well-motivated and described with an appropriate level of detail. Finally, the numerical results verify many important aspects of the proposed method. Overall, the writing is clear and easy to understand.

Weaknesses:
The most important issue is the lack of experimental validation for the combined estimation of compositional and conformational heterogeneity. While this is tested in the third experiment (Section 4.3), this is only on a synthetic dataset. As the authors are no doubt aware, however, the behavior of a reconstruction algorithm can be quite different when applied to real data. It is therefore encouraging that the authors present results on an experimental dataset (Section 4.2), but this only covers compositional heterogeneity (and not conformational). That being said, validating the full method on an experimental dataset would make a stronger case for the proposed work.

Limitations:
As stated above, the main limitation of the work is its lack of validation on experimental data (for both compositional and conformational variability). It is also not clear how computationally intensive the implementation is and how this can be mitigated.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents a neural network-based methodology for modeling both compositional and conformational protein states in cryo-electron microscopy (Cryo-EM) 3D reconstruction. 

In particular, the authors propose a fully *ab initio* approach, named Hydra, which enables the joint inference of poses, conformations, and class identities.

The novelty of this approach lies in its ability to capture both discrete (compositional) and continuous (conformational) heterogeneity within Cryo-EM datasets, without relying on pre-computed pose estimations. Previous methods have either struggled with accurate pose estimation, relied on coarse initializations and upstream algorithms, or had limited capacity to represent complex biomolecular mixtures.

Moreover, the authors validate their proposed approach by comparing it against other popular methodologies, using both synthetic and real datasets, showing the potential of Hydra to capture both compositional and conformational heterogeneity.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper proposes a novel approach that improves over previous methods. In particular, the proposed method extends previous work, DRGN-AI, to use mixture models of multiple neural fields instead of a single neural field to model conformal heterogeneity. In addition, the proposed approach can directly classify the reconstructed sample between 1 of K different classes, providing advantages over methods that rely on downstream classification tasks.  Bibliographic references are exhaustive and well-discussed. The provided results highlight the significance of the method, showing substantial improvements over three pre-existing methods.

Weaknesses:
Although the manuscript is generally well-written, it may be hard to follow for someone who is not an expert in the field. In particular, there are some areas where accessibility to a broader readership might be possible:
* For me, reading some previous work was necessary to understand the context of this work sufficiently to appreciate and understand the contributions of this manuscript. A clearer introduction tailored to a broader readership could make this submission more self-contained, which I would find desirable.
* The introduction lacks a straightforward definition of the taxonomy used throughout the paper. While the authors introduce the context of their research, in particular regarding cryo-EM, I think that the reader should be introduced to the concepts of “poses”, “conformational states”, and “compositional states”, and then to why they are relevant to the presented research and future users. Casting this into a concise but clear way will be much appreciated by future readers, I believe. After that, the authors address how their technical contributions addressed the main challenges presented to them. In my opinion, such changes will allow non EM experts to appreciate the presented work much better and potentially allow other fields to benefit from the same/similar ideas and methods.
* The proposed method seems to rely heavily on the DRGN-AI approach from Levy et. al. Although the authors explicitly state in the introduction that this paper represents an extension of that method, throughout the paper it’s not always clear whether the methodological choices described are novel contributions or are unaltered from the previous method. For example, in Section 3.4, the sentence “We use the pose estimation strategy introduced in [25]” may lead the reader to think that the authors are experimenting with a new pose estimation approach from the literature, while it was already used in the DRGN-AI (or at least I believe so). 
* While results are clear and easy to follow, they only present standard deviations in Table 1 and not in the other tables. Moreover, it is unclear why the CryoSPARC result in Table 1 does not report any standard deviation.
* Will the authors provide a public code repository enabling others to replicate the presented results and use the method in the context of their data and experimentation?

Limitations:
- The main limitation of this work is the need to know the number of classes K in advance. This can be addressed by an exhaustive search of the optimal value of K, however, this further worsens the second main limitation that is related to the computational cost of the method. The authors address both limitations and propose a possible future direction to reduce computational cost.

- Another point of view of the previous limitation is related to scalability. Given a fixed computational budget, since a new neural field is required for each additional protein class, the possible choice of the number of desired classes K is limited, potentially reducing the applicability of the method in datasets that contain a higher number of different macromolecules.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This submission presents Hydra, a method for handling heterogeneous cryo-EM reconstruction. Hydra can model both conformational and compositional heterogeneity and can perform ab initio reconstruction. To achieve this, it parameterizes structures as arising from one of K neural fields. In the optimization pipeline, the conformations, poses, class probabilities, and neural fields are optimized to maximize the likelihood of the observed images. The authors demonstrate the reconstruction of multiple protein complexes from an experimental dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The proposed approach can handle ab initio reconstruction, meaning it does not require pre-computed image poses.
2. The capability of handling compositional heterogeneity is not well-explored.
3. Results on experimental datasets are provided, with comparisons to baseline methods.

Weaknesses:
1. Compared to DRGN-AI, the proposed approach primarily changes the single neural representation to multiple ones.
2. The determination of K seems tricky.
3. The paper lacks comparisons with conventional approaches such as cryoSPARC and RELION, especially qualitative comparisons. More results, preferably video results, are needed.
4. It is unclear whether the pose predictions are accurate. More visualization analysis would be helpful.

Limitations:
n/a

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
clBiQUgj4w;"REVIEW 
Summary:
This paper proposes a novel and effective technique to enhance long-term time series forecasting. The proposed technique, Residual Cycle Forecasting (RCF), directly models periodic cycles with learnable parameters, decomposing the learning of time series into periodic cycles and residual components. The residual can be learned with a simple architecture like Linear or MLP, and this technique can be integrated into existing forecasting models. Results show significant improvement achieved by using this technique.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
* The proposed method is simple and effective. It not only improves accuracy but is also more parameter efficient and can be easily integrated into different backbones.
* The paper is well-written with good clarity. Both the problem and analysis are presented clearly.
* The experiments are conducted with high quality, providing extensive and comprehensive analysis of the proposed technique. The results are consistent with prior related work. Limitation of the proposed method is also clearly stated.

Weaknesses:
The notations are not strictly consistent. The notations for instance normalization (in Section 3.2 and Algorithm 2) are independent from the whole framework and not consistent with the previous problem definition.

Limitations:
See Weaknesses and Questions

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces CycleNet, a novel time series forecasting method that enhances long-term prediction accuracy by explicitly modeling the inherent periodic patterns present in time series data. The core contribution of paper is introducing the Residual Cycle Forecasting (RCF) technique, which leverages learnable recurrent cycles to represent these periodic patterns and predicts the residuals, significantly improving upon the performance of existing models with reduced computational complexity. CycleNet demonstrates state-of-the-art results across various domains, such as electricity, weather, and energy forecasting, while offering over 90% reduction in parameter quantity, highlighting its efficiency and effectiveness in capturing long-term dependencies for accurate forecasting.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The author tries to model the period information explicitly in the time series prediction task, and the motivation is intuitive and reasonable.

2. The method designed by the author is reasonable and closely related to motivation. Combined with the experimental results, the author gives a simple but effective method.

3. The author exposes the code and gives a detailed description, which increases the reproducibility of the model.

4. The limitations of this method are clearly discussed and the possible problems are pointed out.

Weaknesses:
1. In the introduction, the author's statement establishes a close relationship between long-term prediction and periodic information. In the absence of some experimental support, this is not rigorous. Periodic information may be useful for long-term forecasting in certain situations, but it is not appropriate for all tasks, nor is it the only important information that these tasks require. The author slightly obfuscates these to highlight the motivation of this article.

2. The authors select data sets with different periodicity to illustrate the validity of the model. However, the authors only demonstrate the validity, and experiments can be added to show how the proposed method performs differently on periodically different data sets and discuss the underlying rules. At the same time, it is also worth showing how CycleNet performs on data sets where there is no obvious periodicity.

3. Combined with the results in Table 2 and Table 4, the results predicted with Linear alone are even better than some well-designed methods, whether this is due to the particularity of the data set or some other reason.

Limitations:
Please refer to the weakness

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a novel technique for improving the accuracy of multivariate long-term time series forecasting. The technique, called Residual Cycle Forecasting (RCF), involves learning the cyclical patterns of time series through recurrent cycles, which can be used as a pre-processing step for any forecasting model. The authors also propose CycleNet, a linear-based model that uses RCF to enhance its predictions. CycleNet first uses RevIn to account for distribution shift, then subtracts the learned RCF from the input data. The backbone of the model predicts the future residual, adds the learned RCF, and reverses RevIN from the outputs to obtain the final prediction. 

The proposed method is evaluated on eight multivariate time series datasets and compared against several baselines. The results show competitive performance and resource consumption.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* Clear and concise writing style
 * Novel approach to time series decomposition
 * Comparison of performance and resource consumption with baseline methods
 * Ablation study and parameter impact analysis (e.g., $W$)
 * Easy-to-understand model design and components
 * Informative figures to illustrate model and results
 * Thorough discussion of results, including strengths and limitations
 * Code and data provided for reproducibility and transparency.

Weaknesses:
* Lack of clarity to specify which results are from the authors (reproduced or produced) and which ones are collected from previous papers (if so, which ones)
 * Incomplete comparison with existing time series decomposition baselines, such as LD, TDFNet, or SparseTSF (even though this one was in the related works)
 * Model was not compared to RLinear (also relying on RevIN)

Limitations:
Authors have discussed some limitations of their proposal and especially when each channel have different period cycle.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a learnable Seasonal-Trend Decomposition method (CycleNet) to improve the prediction performance of current long-term multivariate time series forecasting models. Specifically, it firstly model the periodic patterns of sequences through globally shared recurrent cycles and then predicts the residual components of the modeled cycles. Extensive experiments are conducted to evaluate the proposed method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The proposed method is a model-agnostic solution applicable to different kinds of models.
2. Although it is simple, it is able to achieve good performance improvement in many cases.
3. Extensive experiments are conduct to evaluate the proposal.

Weaknesses:
1. It seems that the proposal (CycleNet) does not work well for complex datasets, e.g., Traffic. It is better to show more results on the same kind of datasets like the PEMS datasets used in the iTransformer paper.
2. The Section 3 is not well written and lacks a lot of details, especially about the Learnable Recurrent Cycles. The authors may consider to reorganize Section 3 and Appendix A.1 to make Section 3 more clear.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
g7lYP11Erv;"REVIEW 
Summary:
This paper investigates the 3D domain generalization (3DDG) ability of large 3D models using prompt learning. They utilize parameter-efficient prompt tuning to boost the performance of 3D point cloud recognition models. The paper observes that while prompt tuning improves downstream tasks, it often reduces the generalization ability of the models. Thus, they introduce a comprehensive framework to maintain good generalization by allowing learnable prompts to interact actively with the pre-trained general knowledge in large 3D models. This framework imposes explicit three regulation constraints on the prompt learning trajectory, maximizing mutual agreement between task-specific predictions and task-agnostic knowledge. They also develop three new benchmarks to evaluate 3D domain generalization: base-to-new class generalization, cross-dataset generalization, and few-shot generalization.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The newly created benchmarks provide a more holistic evaluation of 3D domain generalization, addressing real-world challenges such as transferring to unseen classes and handling corrupted data.
2. This paper achieves consistent improvements in generalization ability across various large 3D models and benchmarks, demonstrating its effectiveness.
3. The use of lightweight prompt tuning makes the framework computationally efficient, reducing the need for extensive retraining of large models.

Weaknesses:
1. According to the paper's introduction, there are already substantial works on domain adaptation and domain generalization for 3D point clouds, including both object-level data and real scanned radar data. Many advanced methods also utilize beyond PointNet and ModelNet dataset. Consequently, the authors need to provide a more rigorous and detailed motivation for their study.
2. The right part of eq.(1) needs a more detailed explanation to describe its components clearly.
3. The method proposed in this paper seems overly simplistic and lacks novelty. Beyond the three general constraints mentioned, are there any specific designs for integrating LLMs with 3D point cloud multimodal learning?
4. This paper does not introduce any additional designs for domain adaptation. Although it proposes a new benchmark, it essentially applies transfer learning.

Limitations:
This paper lacks an analysis of its limitations. The effectiveness of the text diversity constraint relies on the quality and relevance of the text descriptions, which may vary depending on the source (LLMs or manual templates).

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this work, the authors propose a regularization method for the prompt learning of generalizable point cloud analysis, which can strengthen the performances of learned representations on downstream 3D task while keeping its generalizability. The regularization consists of three components: mutual agreement constraint, text diversity constraint, and model ensemble constraint, which is a plug-and-play method for existing 3D large multi-modal models. Moreover, this work also includes new benchmarks for the evaluation of 3D point cloud domain generalization. Results on the proposed benchmark confirm the effectiveness of the proposed regularization method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The whole framework is simple but effective;

2. The writting is good and easy to follow;

3. The construction of new benchmarks may be beneficial to the community.

Weaknesses:
My major concern about this work is its novelty. As the prompt tuning method has been well studied in other areas, e.g., text-to-image generation. The proposed regularization constraint in Eq.2 is somewhat similar as the preservation loss term proposed in [1], while the other terms improve the robustness by straightforward average operation. I am not sure if the novelty is enough.

[1] DreamBooth: Fine Tuning Text-to-Image Diffusion Models for Subject-Driven Generation

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the 3D domain generalization (3DDG) capability of large 3D models based on prompt learning. The authors propose a comprehensive regulation framework that employs lightweight prompt learning to improve both task-specific performance and domain generalization ability. The framework consists of three main components: mutual agreement constraint, text diversity constraint, and model ensemble constraint. Additionally, the authors introduce three new 3DDG evaluation benchmarks: base-to-new, cross-dataset, and few-shot generalization benchmarks. Experimental results demonstrate that the proposed method significantly enhances model generalization while improving specific task performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1 The paper demonstrates significant originality by being the first to address the 3DDG problem for large multi-modal 3D models and proposing a novel regulation framework with innovative constraint mechanisms. 

2 The quality of the research is evident in its comprehensive experimental design and the significant improvements shown across multiple benchmarks and models. 

3 The work's significance lies in addressing the critical issue of domain generalization in 3D point cloud analysis, potentially impacting related fields broadly. Furthermore, the introduction of new benchmarks provides valuable tools for future research in 3DDG。

Weaknesses:
1 There's no detailed comparison of training time between the proposed method and baseline approaches or full fine-tuning of large 3D models.

2 Limited validation across diverse point cloud tasks and real-world scenarios.

Limitations:
By a more detailed discussion, the authors could provide a more balanced view of their work, demonstrating scientific rigor and offering valuable insights for researchers looking to build upon or apply their method. This would significantly strengthen the paper and its contribution to the field.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
Wyp8vsL9de;"REVIEW 
Summary:
The paper analyses complexity of generalized symmetric eigenvalue problem computation.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Solid analysis of the computations involved in generalized eigenvalue problems.
- Excellent survey of related works.
- Relevant applications in ML (PCA).

Weaknesses:
- The assumption of H being symmetric needs to be in the Abstract as well, as it is quite significant.
- Missing citation to kernel PCA [1]
- While the analysis is strong, the computational aspect is not as emphasized. For example, Algorithm 1 is not shown in main body and only in Appendix, and the paper has no experimental evaluation.

[1] Schölkopf, Bernhard, Alexander Smola, and Klaus-Robert Müller. ""Nonlinear component analysis as a kernel eigenvalue problem."" Neural computation 10.5 (1998): 1299-1319.

Limitations:
Addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers the following optimization problem: given Hermitian $H$ and Hermitian, positive definite $S$, find matrices $C$ and $\Lambda$ such that $HC=SC\Lambda$ where $C$ is the eigenvectors and $\Lambda$ is the eigenvalues. The important application is when the eigenvectors of the interest form an invariant subspace, which covers applications such as DFT and PCA. So the goal is to compute a rank-$k$ projection onto the top or bottom-$k$ eigenvectors. The main contribution of this paper is a stable algorithm that computes an approximate projector $\tilde \Pi_k$ such that $\|\tilde \Pi_k - \Pi_k\|\leq \epsilon$ where $\Pi_k$ is the projector onto top/bottom-$k$ principal components. The algorithm runs in the current matrix multiplication time up to log factors in $n$, $\kappa(S)$, $1/\epsilon$  and the gap between $\lambda_k, \lambda_{k+1}$. It uses polylog bits of precision in the above parameters. It also provides error bounds for Cholesky beyond $O(n^3)$ classical algorithm. As corollaries, the algorithm leads to improvement for DFT, PCA and Block-PCA.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper designs a novel algorithm for invariant subspace projection and PCA in the current matrix multiplication time. The algorithm is a good combination of existing techniques (such as approximating the sign function via inverse Newton) and new insights, such as reducing gap and midpoint of eigenvalues computation to eigenvalue threshold counting, and solve the counting via smoothed analysis. 

The paper is well-written and results are presented clearly in the main body, some intuitions and proof sketches are provided for better understanding of the algorithms and analysis. Overall, I think this is a good paper with solid results.

Weaknesses:
The only complaint I have with this paper (might be a bit unfair) is due to the sheer amount of contents and its very numerical linear algebra nature, this paper might be better suited for conferences such as ISSAC or journals such as SIAM journal on matrix analysis. Otherwise, I think this paper is definitely strong enough to be published in NeurIPS.

Limitations:
Authors have discussed limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper tackles the fundamental problem of GEP subspace approximation (with forward error approximation). The problem is very relevant to different areas of Machine learning. This paper improves the time complexity of this approximation from cubic in n to matrix multiplication time, which is a significant improvement. 

I have read the first nine pages and Appendix B, and the idea seems reasonable, and the paper is well explained. However, given the length of the paper (which exceeds 50 pages), I have not been able to verify the correctness of the solution. Therefore, I will not comment on the strengths and weaknesses of the paper and give a low confidence score. 

In my opinion, a more theoretical venue such as STOC/FOCS would be more appropriate for this paper (both because of the emphasis on theory and because the reviewers have significantly lower paper load). I wish the authors the best.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
-

Weaknesses:
-

Limitations:
The authors have adequately addressed the limitations of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a novel approach to approximating invariant subspaces of generalized eigenvalue problems (GEPs), which are fundamental in many applications such as Principal Component Analysis (PCA) and Density Functional Theory (DFT). The authors introduce an algorithm that computes a spectral projector $ \Pi_k $ with a forward-error approximation guarantee in near matrix multiplication time $ O(n^{\omega + \eta}) $, where $ \omega $ is the matrix multiplication exponent. The approach advances a new analysis for Cholesky factorization and a smoothed analysis for computing spectral gaps, which are key innovations applied to obtain the desired bound with high probability.

The paper's technical claims are well-supported by rigorous mathematical proofs and thorough analysis. The use of Cholesky factorization and smoothed analysis for spectral gaps is innovative and effectively addresses the computational challenges. The proposed algorithm’s performance is theoretically grounded. However, some sections could benefit from additional explanations, empirical computations and examples to enhance understanding, especially for readers less familiar with the theoretical work developed in the paper, and potentially reaching a larger audience.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- **Originality:** The paper introduces a novel approach to solving GEPs and PCA with nearly matrix multiplication time complexity, which is a significant advancement over classical methods. The novel analysis for Cholesky factorization and a smoothed analysis for computing spectral gaps could have a wider application too.
- **Quality:** The mathematical rigor and comprehensive analysis in terms of bit complexity ensure that the proposed methods are both theoretically sound and practically applicable.
- **Clarity:** The paper is generally well-written, with detailed explanations of the methods and thorough proofs.
- **Significance:** The results have broad implications for many applications in machine learning and scientific computing, providing a more efficient computational framework.

Weaknesses:
- **Complexity of Implementation:** The proposed methods, while theoretically sound, may be complex to implement in practice. Detailed implementation guidelines for the algorithms 1, 2, 3, and 4. A standard implementation also would be of value, showing how some of the factors hidden in the $O(.)$ complexity could play a significant role in practice. For example, as it is, it remains open how, for example, the significance of the factor defining the bound on the approximation error $\epsilon$ (which appears inside a $\text{log} \text{log}$ expression) when working with varied sizes of matrices $n$ and approximation error $\epsilon$. Theorem 1 also should include some comments about this issue.
- **Hyperparameter Sensitivity:** The algorithm's performance depends on several parameters (e.g., spectral gap, condition number). A more detailed discussion on selecting these parameters and their impact on performance would be helpful, along with empirical implementations and discussion.

Limitations:
The presented paper discusses in great depth the theoretical underpinning of the proposed algorithm, and we understand this already has a significant impact. Nevertheless, the lack of practical implementation, empirical validation, and analysis of some aspects of the algorithms hinders the potentially higher impact of the proposed method. Even a limited empirical section, including investigations into the role of the hyperparameters and covering varied structures of matrices, would highly improve the paper's potential impact and readership.

Rating:
8: accept, good paper

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the computational cost of invariant subspaces for generalized eigenvalue problems (GEPs) which is a fundamental computational problem with applications in machine learning and scientific computing. The authors propose a novel method that approximates the spectral projectors of GEPs, and give the first $\tilde{O}(n^{\omega})$ bit complexity result for forward-error approximation in the floating point model, improve upon the previous $\tilde{O}(n^3)$ result. Based on this result, the authors also give new matrix multiplication-type upper bounds for PCA problems in terms of bit complexity.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written with very solid theoretical results and proofs. The problem, the model, the results are stated clearly. The existing works and previous results are discussed thoroughly.
2. The idea of reducing the problem to approximating the average and the gap among two adjacent eigenvalues is interesting. The result of approximating these two quantities (Theorem 3.1) can have potential other applications.
3. This paper gives a new stability analysis of the Cholesky factorization under floating point model and improves upon the previous $\tilde{O}(n^3)$ result, which is of independent interest (especially in the TCS community).

Weaknesses:
The main concern is that this paper may be too dense with technical details and rigorous proofs as a NeurIPS submission (instead of a TCS conference). It might be better if the authors consider presenting this paper in a way that is easier to follow (e.g., delay more proof details to appendix, while adding some examples of what the results look like for specific schemes of spectral decay, and more detailed discussion about the potential applications / examples of GEPs), so that this problem can be brought to more people in this community.

Limitations:
The authors have adequately discussed the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
TuCQdBo4NC;"REVIEW 
Summary:
This article proposes a robust algorithm for spiking neural networks. The algorithm includes a frequency-domain filter with a hard threshold and trainable neuron leakage parameters. The author's motivation in organizing the paper is based on biological interpretability, adopting an engineering approach in methodology, and attempting to propose a unified, robust framework for spiking neural networks. The author combines multiple previous methods in the experiment and provides better robustness results under adversarial disturbances.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The author provides better robustness against disturbances. The method was verified by the author in GN FGSM, PGD, BIM, and CW.

I think the author's motivation is very important and urgent, consistent with the interests of NeurIPS.

Weaknesses:
The author proposes that the robustness of SNN lacks theoretical analysis, but in reality, the theoretical analysis provided by the author is not significantly different from the theoretical analysis in StoG. The conclusions presented in the paper corresponding to the StoG method are similar to the theory proposed by the author. The innovation point here is not clear.
 
The author proposes that frequency encoding is based on cognitive motivation, i.e., selective visual attention mechanism, rather than coding level, which is inconsistent with the motivation behind the dynamic innovation points of variable dynamic parameters proposed later.
 
The two methods proposed by the author, FE and EL, did not conduct a detailed ablation study to determine the effectiveness of the module. Especially lacking in the performance of using the EL method alone.
 
What is the difference between the El method proposed by the author and the method proposed by Ding et al. in ICML 2024?
[Ding et al., 2024] https://arxiv.org/abs/2405.20694.

Limitations:
See weakness and questions

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a unified framework for SNN robustness, based on this framework, this paper further proposes a frequency encoding (FE) method for SNNs to decrease the input perturbations and proposes an evolutionary membrane potential leak factor (EL) to ensure that different neurons in the network learn the optimal robustness leak factor at different time steps, thus improving the robustness of SNNs. Extensive experiments are conducted to verify the effectiveness of this method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors present a unified framework for SNN robustness constraints, which provides a potential explanation for the robustness improvements achieved by previous work and inspires enhancements in the encoding method and the leak factor for SNN robustness.

2. The proposed FEEL method crops information from high-frequency to low-frequency to remove the input noise and learn the optimal robustness leak factor at different time steps. The extensive results demonstrate that the FEEL method is state-of-the-art. Both the FE and EL methods further enhance the robustness of current defense strategies.

Weaknesses:
1. The Frequency Encoding (FE) is proposed to suppress the perturbation $\varepsilon (t) $ in Eq. (6). The implementation of FE is based on the cropping operation in Eq. (9). Although such an implementation gives the benefit of $\varepsilon (t) $ suppression for $T>1$, it also brings the drawback of valid information loss. It is not clear whether the benefits outweigh the drawbacks or vice versa. Please provide more evidence or analysis to support the performance improvement by FE (as compared with direct coding) in Table 2.

2. Section 4.3 introduces the implementation of considering leak factor  $\lambda$ in the first term of Eq. (6). According to the objective of Eq. (6), an intuitive approach is to minimize  $\lambda$. However, the authors proposed a learnable leak factor, which seems to contradict this intuitive approach. Please clarify it.

3. There is a new attack method [1] specifically designed for SNNs which outperforms attacks designed for ANNs. I wonder how the proposed method in this paper performs under such a kind of attack. 

[1] Bu T, Ding J, Hao Z, et al. Rate gradient approximation attack threats deep spiking neural networks. Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023: 7896-7906.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper aims to enhance the robustness of SNN. The authors first present a unified framework for SNN robustness. They propose a frequency encoding method that filter the noise in frequency domain. Based on that, they also propose the trainable leaky parameter to better constrain robustness. Experimental results on various datasets validate that both our FE and EL methods can effectively improve the robustness of SNN to different noises.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The frequency encoding method is novel. The FE-SNN is able to filter out noise by processing information in the frequency domain.

The authors conducted very comprehensive experiments to demonstrate the effectiveness of the proposed method. The experiments results demonstrate that FEEL can be combined with adversarial training or other robustness enhancement algorithms to obtain more robust SNNs.

Weaknesses:
The theoretical framework is not rigorous enough. It is not obvious from Eq. 6 that a smaller TERM 1 (term 1) will result in less perturbation in the output, since the change of term 1 may affect $$. The authors need more solid theory to support the FE and EL methods.

The robustness improvement is not significant. Sometimes robustness performance of FEEL is even worse than FE.

Limitations:
Authors are encouraged to introduce the additional training/inference cost of the proposed method.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
evP9mxNNxJ;"REVIEW 
Summary:
This paper investigates the evaluation of large vision-language models (LVLMs) and the currently used benchmarks. Within the paper two primary issues are identified: the lack of need for visual information, and data leakage. Based on these issues a new compiled benchmark is proposed MMStar that includes a set of multi-modal samples validated by humans, and a variety of models are evaluated on this new benchmark.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
With the rapid progress of the field of LLM and LVLMs, it is crucial that we have reliable benchmarks for evaluation - this work explores existing benchmarks, identifies potential issues therein, and proposes a new benchmark which contributes to the quality of evaluation in the field. Additionally, the work proposes new metrics for evaluation and performs a benchmark evaluation of various models.

Weaknesses:
1) Random Chance

An issue in this work is that it insufficiently accounts for random guessing and data bias. As shown in [27], the ground-truth chance for it being answer A or B on MMbench are 26.4% (due to having questions with less than 4 options), and moreover, certain models may prefer certain answers. LLMs may get correct answers even when they require visual input. Given the 26.4% baseline for MMbench from [27], it is also surprising that the random choice value reported in this paper, for MMbench, is 0.0 in both Table 1 and Table 2. Moreover, an additional baseline based on majority class may also be necessary here.

For the results in Figure 2, which require 6 out of 8 models to get a hit it is unlikely that random guessing has a big influence. However, for the results in Table 1 this is not the case. For instance, it appears that all MMBench results in Table 1 are below the 26.4% from [27] - which means all LLM do worse than always answering A (or B). For other benchmarks there may similarly be data biases in which answer is more frequent.

Relatedly, it may be the case that such data biases about which options are more frequently chosen are more pronounced for multi-modal questions, i.e., when asked about colour the answer is more often grey across all datasets and settings - which means LLM do not learn this data bias as their training data does not include such questions, but LVLM may learn this because the same bias is present in their training data. While this is unlikely to fully explain the phenomenon observed in Table 2, it may explain part of it and not be directly related to data leakage. 

2) Not all benchmarks

The issues identified with existing benchmarks do not hold evenly across the benchmarks tested. In particular, MMBench and slightly lesser, MathVista seem to do pretty well with respect to these issues. This also appears to be reflected in the construction of MMStar, where after Manual Review the proportion of questions from these two existing benchmarks jumps considerable - in the end making up almost half of the MMStar benchmark. Which raises questions whether there is any benefit of using MMStar versus simply using MathVista and MMBench. 

3) Manual review

The manual review description is insufficiently clear. From the three criteria applied, only the first one is somewhat clear - for the other two it is unclear how the 'experts' judged this. I would expect description (can be in appendix) of agreement rates between these experts for these criteria, as well as further description of what these criteria entail. 

4) New metrics

The newly proposed metrics MG and ML are somewhat unclear in what they measure. If the 1500 questions in MMStar all require visual input as determined by the manual review, then what additional information does the MG metric give? Given the discussion above about chance, it appears the MG metric is more of a correction for random guessing. The ML metric similarly doesn't account for random guessing, or the aforementioned potential for data biases.

Limitations:
I believe question 9 in the paper checklist has been answered incorrectly - or at least the justification given is not valid. Given that the paper discusses a benchmark that may include images of people or copyrighted material it is crucial that the authors affirm whether the work has been done in accordance to the ethical guidelines. Even if the data is a compilation of existing datasets - by selecting and combining information from these the new resulting dataset may be biased in ways the original datasets were not (e.g., by selecting only those images containing users from a certain demographic).

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The current benchmarks used to evaluate Vision Language Models (VLMs) contain several flaws. In particular, a lot of questions can be answered without looking at the image at all. These benchmarks still being hard, the best proprietary models without looking at the images can obtain better scores than strong VLM baselines (looking at the images). As a result, the authors create MMStar, a difficult benchmark aiming at evaluating the capability of the vision-language tasks. They manually review their benchmark, and do several ablations to confirm its importance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The authors provide a benchmark that is hard to be good at without looking at the images. This is a problem for some questions in MMMU and MathVista currently.
- The authors manually reviewed the examples of the benchmark.
- The dataset is nicely divided into 6 subtasks, evaluating different aspects. The fact that there is exactly the same number of examples in each of these subtasks is appreciated.
- The fact that each question is a MCQ, instead of an open-ended question that would be difficult to evaluate due to the different output formats of the models, is also appreciated.

Weaknesses:
- In the released dataset, the choices are directly integrated into the prompt. It would be good to also add a column with only the original question, and another column containing the list with the possible options, so that researchers could evaluate their models with the prompts they used during their fine-tuning.
- As the authors mentioned, it would be useful to also create a test set for this benchmark.
- It would have probably made more sense to publish this in the Datasets and Benchmarks track.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors examine current benchmarks for large vision-language models (LVLMs) and identify two main problems: 1) many samples do not require visual content, and 2) there is unintentional data leakage in LLM and LVLM training. To address these issues, they developed a multimodal benchmark called MMStar, consisting of 1,500 samples, and proposed two metrics to measure data leakage and performance gain in LVLMs’ multimodal training. They conducted empirical evaluations on 16 LVLMs to report their performance on MMStar.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-organized and easy to follow.
2. The motivation behind the study is clear, and the empirical analysis is thorough.
3. Data curation for MMStar is comprehensively explained.
4. The proposed performance metrics are intuitive and effectively presented.

Weaknesses:
1. The authors only consider multiple-choice questions for the MMStar benchmark. Including a wider variety of well-curated questions without choices would be great.
2. Similar to Figure 2, the authors should provide the LLM Hit Rate for the MMStar benchmark.
3. What is the percentage distribution of the 1,500 samples across the four difficulty categories?

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors have identified two primary concerns with the benchmarks commonly used for large vision-language models (LVLMs). Firstly, many samples do not require visual content to answer the questions. Secondly, they noted unintentional data leakage during LVLM training. They assessed eight large language models (LLMs) across six widely-used multi-modal LLM benchmarks, demonstrating that LLMs can correctly answer a significant portion of questions without visual input. To more reliably evaluate LVLM performance, they developed a new benchmark by meticulously filtering data from six existing benchmarks with three requirements: 1) visual dependency, 2) minimal data leakage and 3) multi-modal capability for resolutions. Additionally, they designed two metrics: Multi-modal Gain, to quantify the improvement from multi-modal training, and Multi-modal Leakage, to assess the extent of potential data leakage. Using this new benchmark and the two metrics, they provide a comprehensive comparison of state-of-the-art LVLMs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-structured and clearly articulated, facilitating ease of comprehension.

- The evaluation process is meticulously designed, and the conclusions drawn from it are convincing.

- The findings presented in this paper meaningfully impact multi-modal large language model (LLM) research. Researchers have depended heavily on benchmarks without thoroughly examining their quality. The authors question the reliability of evaluations based on these benchmarks. Without a reliable benchmark, it is impossible to faithfully measure actual multi-modal gain. They developed a new benchmark, MMStar, which facilitates more reliable evaluations.

- Using the MMStar benchmark, the authors evaluated two closed-source and fourteen open-source large vision-language models (LVLMs), with the results presented in Table 3. As expected, GPT4 emerged as the top performer in five out of six tasks. Additionally, they underscored the efficacy of smaller-scale models by highlighting that TinyLLaVa, a model with 3 billion parameters, outperformed some larger competitors with 7 billion and 13 billion parameters, thereby emphasizing the potential of smaller-scale LVLMs.

Weaknesses:
- The proposed metrics, Multi-modal Gain and Multi-modal Leakage, are dependent on the base LLM utilized in the large vision-language models. This dependency complicates the use of these metrics for directly comparing the multi-modal gain across different LVLMs.

- The manual review step aggressively reduces the MMStar benchmark from 11,607 samples to 1,500 samples. The explanation provided in Section 3.1 for this reduction is somewhat vague and lacks clear, objective criteria for filtering. I am curious about the rationale behind such an aggressive reduction by nearly tenfold. Is this reduction due to a scarcity of data meeting the three specified criteria mentioned between line 187 to 189, or are there other reasons for this decision?

Limitations:
The authors properly discuss the limitation in the paper

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
7Fzx3Akdt5;"REVIEW 
Summary:
The paper studies the problem of exact community detection in correlated stochastic block models (with two symmetric communities). More precisely, a graph is sampled from an SBM with parameters p and q and each edge in the graph is then kept with probability q. This downsampling is performed K times independently at random to obtain K different graphs. Each graph is then randomly permuted. From these K correlated sample we want to recover the communities of the model.

The authors provide a necessary and sufficient condition on the parameters p,q,k, and s for exact recovery. 
In particular, they show there exists a regime of p,q,s where K-1 graphs are not enough to recover exactly the communities, but K samples are enough. In particular, there exists a regime in which K samples might not even be enough to solve graph matching perfectly (i.e., recover the random permutation). Therefore, it is necessary to aggregate imperfect matching information with techniques for community detection.

The case for K=2 has been solved by Gaudio, Rácz, and Sridhar. This paper extends their result for any fixed K>=2.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
I think the problem is a natural and interesting one. The extension from the case of two to more correlated graphs appears highly nontrivial, since there are many ways in which one could try and resolve potentially conflicting information arising from matching different pairs of graphs.

The paper is also well written, both in explaining the background and motivation to the problem, and also when giving a flavour of the techniques used in the analysis.

Weaknesses:
Perhaps the main weakness of the paper is that the analysis is not algorithmic, in the sense that there is not an efficient algorithm able to recover the communities. This is, however, more an invitation to future work rather than a flaw of the paper itself.

The other point I'd like to raise is that the proofs in this paper are fairly lengthy and involved. Is NeurIPS the best venue for these kind of papers? I believe the paper will be interesting to the NeurIPS community and that's why I recommend acceptance, but in a perfect world I'd like results of this kind to be also checked for correctness (disclaimer: I did not read the appendix).

Limitations:
Yes.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Given K correlated SBMs, the authors derive information-theoretic conditions for (i) the exact recovery of the community structure and (ii) the perfect recovery of the planted alignment $\pi^*$.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is well-written, and I enjoyed reading it. It generalizes [37] and [18] to multiple (K \ge 3) SBMs. The interplay between community recovery and graph alignment by combining the information from the K graphs is well-explained and well-executed.

Weaknesses:
No major weaknesses, the paper opens and closes the problem it intends to solve. The discussion section lays directions for interesting future works. I hesitate between 7 and 8. But for a higher grade, I would have liked to see a bit more (such as graphs with 2 communities of different sizes, or more than 2 communities, which I believe is not that much harder). In any case, the paper is a clear accept.

Minor comments:

* Since the authors consider SBM with two balanced communities and edge probabilities p&q, the term planted partition model may be more adapted.

* In the planted partition model, the key information-theoretic quantity for exact recovery is the Rényi divergence of order 1/2 between two Bernoulli distributions. The CH divergence is only needed for SBM with general connection probabilities and/or block sizes. 

* Typo line 143: ""for K \ge graphs"", 3 is missing.

* It may be useful to define earlier ""almost exact community labeling"" and ""partial almost exact graph matching"" (which I believe are accounted first in lines 179 and 180, and not before, and define only lines 241).

Limitations:
The theoretical results and their assumptions are clearly stated. The work is purely theoretical and does not require more discussion on societal impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies the problem of exact community recovery from multiple ($K$) correlated graphs in 2-community balanced symmetric SBM. Prior work of [Gaudio, Ra\'cz, and Sridhar 2022] for $K=2$ case. The paper generalizes their result for any $K$ (constant) number of graphs. In particular, the main result of the paper determines a sharp information-theoretic threshold in terms of $a,b,s$ (correlated SBM parameters) and $K$ such that 
1. (Theorem 1) Above the threshold, the optimal MAP estimator (not efficient though) achieves exact recovery with high probability. To show this, the main challenge is to combine the information from more than two networks when none of the pairs can be matched exactly.
2. (Theorem 2) Below the threshold, any estimator fails to exactly recover the communities with high probability.

In particular, some interesting highlights from their results are that there is a region of parameter $(a,b,s)$ such that exact recovery is (i) impossible using $K-1$ graphs but possible using $K$ graphs AND (ii) matching the vertex labels of any of the graph pair is impossible.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper provides a clean characterization of the precise information theoretic limit for an important problem in exact community recovery literature.
2. The paper is well-written with clear intuitions of interplay between exact recovery and graph matching.

Weaknesses:
I do not see any major weaknesses in the paper.

Limitations:
Yes, the authors list out both negative and positive societal impacts of their work and graph matching in general.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Theoretical work showing conditions for exact community recovery in $K$ correlated $2$-community SBMs where node labels are not maintained between networks. The work extends previous work for $K=2$ which introduces new challenges and proof mechanisms to allow for $K \ge 3$ networks. Theorems 1 and 2 provide necessary and sufficient conditions for exact community recovery relating to the difficulty of the pairwise matching problem $T_c(a, b)$ and the individual exact community recovery problem $D_+(a,b)$.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This research fully answers the open question of [18] introducing novel ideas top extend existing techniques to work in the much more complicated scenario with $K \ge 3$ networks. I feel the paper was well-structured to introduce and explain the problem at hand to someone outside of the research area. Section 2 did an excellent job introducing the problem highlighting the interplay between the community recovery and graph matching. Section 3 giving the main results along with a helpful high level description of the algorithm used within the proof and Figure 2 demonstrates these results pictorially highlighting the regions where their research come into play. Section 4 gave more details of the proof, while still a bit technically in places was still useful for myself who wanted to get the vibe of the proof without delving into the details in the Appendix.

The paper is very clear in its goal, what is has achieved and possible future directions in this area.

Weaknesses:
While reading this paper, I was unsure whether this paper fitted the remit of NeurIPS and would be better suited in another journal rather than conference paper. I felt more comfortable about this upon noticing that [18], which supplied the problem for this work, was itself inspired by work of community recovery in correlated SBMs [37] published in NeurIPS. I recognise this is a personal bias of what I consider a NeurIPS paper.

There is some discussion of how this work relates to real-world networks, but it is unclear how much further research needs to be done before these ideas can be used for practical analysis of real graphs. A number of my questions below relate to applying these ideas to more realistic networks.

Limitations:
The authors highlight the positive and negative impact of their results, particular for de-anomalising multiple networks.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
jYypS5VIPj;"REVIEW 
Summary:
This paper proposes a graph-based approach for Few-shot Semantic Segmentation (FSS) based on the Segment Anything Model (SAM).
The authors propose a Positive-Negative Alignment module to select point prompts and a Point-Mask Clustering module to align the granularity of masks and selected points.
The proposed method surpasses state-of-the-art generalist models on COCO-20i and LVIS-92i datasets, and also performs well on One-shot Part Segmentation and Cross Domain FSS datasets.
Moreover, the proposed method is hyperparameter-free and efficient.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. How to conduct prompt engineering for vision foundation models(SAM) is still a challenging problem in the research community. This work fills this gap.
2. The proposed method is systematic, consisting of three modules: Positive-Negative Alignment, Point-Mask Clustering, and Positive and Overshooting Gating. Each module is well-designed and novel.
3. The paper is well-written and easy to follow. The authors can accurately describe the well-designed modules.
4. The experiments are comprehensive and the results are convincing. The proposed method significantly outperforms the SOTA on COCO-20i. The cross-domain validation further demonstrates the generalization and robustness of the method.
5. The proposed method is hyperparameter-free, which is a great advantage and increases the practical value of the method.
6. I briefly checked the code, and the submitted code is complete, which increases the reproducibility of the paper.

Weaknesses:
1. In L173, ""This is the precondition for the 
efficacy of our PMC module, as even slight errors could significantly impact the clustering accuracy""
The authors should provide some visualization examples or experiments to verify this statement.

2. In Table 4, I notice weak or strong connected components are not clearly defined and the difference are also very small. The authors should provide more analysis to explain this.
And what is the hyperparameter for K-means++? Have you tried different hyperparameters for K-means++?

3. I notice the authors conduct ablation studies on different datasets for different components. I hope the authors can provide more experiments on part segmentation. Since previous SOTA Matcher require different hyperparameters for part segmentation. Conducting ablation on part segmentation can further prove the hyperparameter-free advantage of the proposed method.

Limitations:
The authors have discussed the limitations of the proposed method in appendix.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper first proposes graph-based approach for SAM-based few-shot semantic segmentation, modeling the relationship of SAM-generated masks in an automatic clustering manner. A positive-negative alignment module and a post-gating strategy based on the weakly connected graph components, enabling a hyper parameter-free pipeline. Extensive experimental comparisons and analysis across several datasets over various settings show the effectiveness and efficiency of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1、The graph-based approach for SAM-based methods proposed in the article is more insightful. Because SAM-based methods can generate many patches with point prompts, and how to group them is a good question.
2.The experiments are comprehensive. The ablation experiment demonstrates the effectiveness of the modules. The visualisation also illustrates the idea expressed.

Weaknesses:
1. The captions are not detailed, especially in Figure I and 2.
2. The overall organisation and presentation of the article are poor and need improvement.
Some details:
① On the left side of the first line of Figure 2, the images are ‘reference on top and target on bottom, but the features become ‘reference on bottom target on top’; it's not consistent.
② In my understanding, in Figure 2，positive-negative alignment should be conducted on the target image using the foreground and background features of the reference's image, but the part is very unclear and not informative and does not show the above content.
③Point-Mask Clustering part in Figure 2 can be understood that it wants to express the cluster of the points to different targets in the images, but the drawing is very casual, and there is no mark or expression.
3. How to conduct mask merging? Is each point used to generate a mask and then merge, or is the cluster of points used together to SAM to generate mask prediction, or are there other ways? Give the details.

Limitations:
The author provides the limitations in the appendix. They say the coordinates of points will miss the small objects. This is a real problem and is suitable for further study.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper extends SAM to few-shot semantic segmentation tasks by proposing an approach based on graph analysis and representation learning. The contributions include a Positive-Negative Alignment module to generate the initial points prompt using DINOv2 features, as well as Point-Mask Clustering and Post Gating modules to filter the proper points based on the SAM generated masks. The proposed method exhibits efficiency and performance advantages over previous work such as PerSAM and Matcher.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
(1)	The paper is well-written and easy to follow.

(2)	The proposed method is hyperparameter-free and shows significant improvements in both efficiency and performance.

(3)	Experiments on various datasets and tasks validated the generality of the proposed method.

Weaknesses:
(1)	The proposed method is specifically tailored for DINOv2 and SAM, is it possible to apply it to other foundation models? This could broaden the impact of the method. 

(2)	In Figure 1(b), the scales of the axes are unclear. Are the starting points of the nine axes all 0?

(3)	On the FSS-1000 dataset, the performance of the proposed method is comparable to Matcher, which deserves a discussion. For example, what factors may have contributed to this result.

Limitations:
The limitations have been discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
4DcpFagQ9e;"REVIEW 
Summary:
This paper introduces a novel algorithm called Score Distillation via Inversion (SDI) that enhances the quality of 3D shape generation. By inverting Denoising Diffusion Implicit Models (DDIM) at each step and incorporating the initial noise into the estimated score, SDI addresses the over-smoothing and detail loss issues associated with traditional Score Distillation Sampling (SDS) methods in 3D generation. Experimental results demonstrate that SDI significantly improves the quality of 3D shapes, closely matching the quality of 2D image generation, without the need for additional neural network training or multi-view supervision. The work provides valuable insights into understanding and improving 3D asset generation with diffusion models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper provides a theoretical analysis that links Score Distillation Sampling(SDS) with Denoising Diffusion Implicit Models(DDIM), offering a deeper understanding of the discrepancies between 2D and 3D generation process, which is different from the experimental enhancement made by most of the previous methods.
2. The modification to the existing SDS method is straightforward and does not require training additional networks or multi-view supervision. Furthermore, SDI significantly improves the quality of 3D shape generation, which makes it a practical solution for enhancing 3D generation.
3. This paper provides thorough analysis and present a set of convincing ablation experiments.

Weaknesses:
1.	According to the authors’ theory and hypothesis, the better the noise term is Eq. (8) is estimated, the better the 3D generation quality is improved. However, in the ablations about the choice of k(x), SGD optimization doesn’t show significant improvement. Also, this phenomenon can be observed from Figure.10, in which green curve and purple curve present lowest and comparable MSE with orange curve, while the orange curve get the best result. I would appreciate it if the authors can provide some explanations.
2.    One of the paper's key perspective is that SDS introduces high variance. I have read another paper[1] dedicated to  minimizing the variance introduced in SDS, and maybe it's also worthwhile to compare with that.



[1] Tang, B., Wang, J., Wu, Z., & Zhang, L. (2023). Stable score distillation for high-quality 3d generation. ArXiv Preprint ArXiv:2312.09305.

Limitations:
Please refer to the weaknesses part.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces an effective SDS modification that replaces random noise samples in the SDS objective with those obtained with DDIM inversion. The proposed technique enhances text-to-3D generation, outperforming SDS and being comparable to more sophisticated methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper is well organized and provides valuable intuitions, illustrations, and discussions over reading. The additional discussion in Appendix E is also beneficial.

* The proposed method is simple, reasonable and well motivated. The derivation is clear.

* The experimental results are promising, and the ablation study in Sec 6.2 is highly valuable and interesting.

Weaknesses:
* Inversion with negative guidance is surprising, and its rationale remains somewhat unclear. A more detailed investigation and justification would be useful.

* The idea of using DDIM inversion is very similar to ISM[1]. The primary distinction is guided vs unguided inversion. Although the authors justify the guided inversion with negative guidance in Fig. 10, a thorough discussion and detailed comparisons are essential.

* The quantitative evaluation relies on CLIP-IQA, which does not seem quite suitable for 3D generation evaluation. Conducting a user study would be highly valuable. If it is unavailable, some recent works [2,3] aim to propose automated evaluation metrics. For example, would it be reasonable to evaluate ImageReward[4] following [2]?

* The quantitative results miss NFSD and the qualitative comparisons are presented only for two prompts. Could the authors add NFSD to Tab.1 and provide more visual comparisons with all methods?

* There is no evaluation of diversity under a given prompt. It would be useful to include a few generated results for different seeds.

To sum up, I am generally optimistic about the submission but have some concerns regarding the evaluation and close connection with [1], as discussed above.

[1] Liang et al. LucidDreamer: Towards High-Fidelity Text-to-3D Generation via Interval Score Matching.

[2] He et al. T3Bench: Benchmarking Current Progress in Text-to-3D Generation.

[3] Wu et al. GPT-4V(ision) is a Human-Aligned Evaluator for Text-to-3D Generation.

[4] Xu et al. ImageReward: Learning and Evaluating Human Preferences for Text-to-Image Generation.

Limitations:
The authors addressed the limitations and potential negative social impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper connects score distillation sampling (SDS) to a DDIM sampling process. The proposed method Score Distillation via Inversion (SDI) replaces the original random noise in SDS with DDIM inversion, and show significantly improved quality compared to SDS and other state-of-the-art prior methods.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The connection from SDS to DDIM sampling is interesting. The motivation is also clear that it considers the change of the ""sample prediction"" variable to guide the 3D generation process. The method is supported by well derived theory (for 2D) and the DDIM inversion is a clever solution.
2. The results generally look good. It shows significant improvement over its baseline method SDS, and is competitive to recent state-of-the-art text-to-3D methods. The experiments are thorough.
3. The proposed method and results could provide insights for future research in the important direction of text-to-3D distillation.

Weaknesses:
1. Overall, the visual results seem to be having a little bit ""gray"" color/style shift. Is this due to the approximation or some bias in the theory? Is there any hypothesis for theoretical / practical reasons?
2. The ddim inversion and approximation could introduce additional computation cost and training noise.
3. In ""ISM as a special case"", it seems the main difference of the proposed method and ISM is having the additional text condition. Is there any other key differences in method implementation or theory? More details about the contribution of this work based on ISM could be helpful.

Limitations:
The authors discussed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
UoxuaOGV6B;"REVIEW 
Summary:
In summary, this paper investigates advancements in Parameter-Efficient Fine-Tuning (PEFT) for pre-trained neural networks by integrating spectral information from pretrained weights, aiming to enhance the classic LoRA approach. By employing Singular Value Decomposition (SVD), the authors introduce two spectral adaptation techniques: additive tuning and orthogonal rotation of the top singular vectors.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The topic of LoRA adaptation addressed in this paper is valuable due to its applicability to a wide range of high-level and low-level tasks. The authors introduce two variants of the proposed enhanced LoRA method. In specific datasets and tasks, these variants demonstrate improvements over state-of-the-art (SOTA) methods.

Weaknesses:
There are some typos that need to be corrected. For example, a spelling mistake in the word ""Apppendix"" in line 32, it should be corrected to ""Appendix."" Additionally, the word ""digged"" in line 20 should be corrected to ""dug"" to use the proper past tense of the verb ""dig."".

It is not common to put the whole literature review in Appendix.

In line 53, the phrase ""orthogonal rotating the top singular vector space."" This phrase should be corrected to ""orthogonally rotating the top singular vector space"" to properly use the adverbial form ""orthogonally,"" which modifies the verb ""rotating.""

Additionally, if the spectral space is modified, the rank will also change. This implies that the optimal ranks of the two spectral spaces differ, making it unfair to compare different LoRA absed methods using the same rank.

There is a logic error of the claim stated in line 82-93: Specifically, the statement that ""these methods require storing all U, S and V during training while only the diagonal vector of S is tuned, which nearly doubles the storage requirement compared to pretraining when fine-tuning on downstream tasks"" is misleading. Storing all components (U, S, and V) does indeed increase storage, but it's not clear why this would ""nearly double"" the storage requirement. The increase in storage would depend on the specifics of the matrix dimensions and the storage format. The phrase needs to clarify how the storage requirement nearly doubles to avoid logical inconsistency.

Limitations:
Instead of fine-tuning the singular values of the weights, this paper proposes fine-tuning the singular vectors of the weights. However, the motivation behind this approach is not convincingly presented. For example, The motivation for fine-tuning the singular value vectors is not clearly articulated. As we know, U is an orthogonal matrix representing the left singular vectors, S is a diagonal matrix of singular values, and V is an orthogonal matrix representing the right singular vectors. A critical question that arises is whether U and V maintain their orthogonality after fine-tuning. Orthogonal U and V matrices provide an optimal basis for representing the weight matrices. Losing orthogonality results in a suboptimal basis, which can lead to less efficient representations of the neural network weights. Also, without orthogonality, the interpretability and distribution of the singular values will also be affected. Furthermore, orthogonal matrices are numerically stable and well-conditioned, meaning small changes in the data lead to small changes in the results. Without orthogonality, the resulting matrices may become ill-conditioned, causing numerical instability and issues in the training and optimization algorithms.

Additionally, the key contribution of the paper lacks clarity and contains logical errors. It would be helpful if the authors could address these concerns and provide further clarification.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes fine-tuning pretrained model weights in the spectral space for parameter efficiency. It explores two spectral adaptation mechanisms: additive tuning and orthogonal rotation of top singular vectors. The authors introduce these methods, providing theoretical analyses on rank capacity and robustness to support their approach. Experiments on language and diffusion model fine-tuning demonstrate the proposed method's superiority over previous parameter-efficient fine-tuning techniques.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The proposed spectral adapters, which introduce spectral adaptation, are interesting.

2. The theoretical analysis showing that spectral adapters have a larger rank capacity than LoRA is reasonable.

3. Experiments on language and diffusion model fine-tuning demonstrate that the proposed method outperforms other parameter-efficient fine-tuning techniques while maintaining efficiency.

Weaknesses:
1. The analysis of spectral adaptation robustness in Section 3.2 could be clearer. It would be helpful to provide what $\mathcal{R}(X)$ denotes and more clearly explain why fine-tuning $u^*$ is considered noiseless.

2. The paper's organization might benefit from some restructuring. Consider moving certain content, such as Lemma 4.1 and Table 3, from the experiments section to the methods section. Additionally, restructuring the experiments section could improve clarity.

3. While spectral adaptation is proposed, it would be valuable to more clearly demonstrate in the methods section how this approach is superior to prior works like SVDiff and OFT.

4. In Figures 1 and 8, using validation loss instead of training loss only for comparing PEFT methods could provide more meaningful insights.

5. It might be beneficial to discuss limitations in a separate section rather than within the Experiments section.

6. Including quantitative measures alongside the qualitative comparisons in the image generation results could strengthen the analysis.

Limitations:
The authors mention some limitations in the checklist part.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, the authors proposed to modulate top-r singular vectors after performing SVD on the pretrained weights. Both theoretical analysis and experiments have shown that the proposed two types of spectral fine-tuning methods can improve the representation capacity of low-rank adapters.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The introduction of the proposed method is pretty clear, and the theoretical analysis and review of other PEFT methods help the readers quickly and comprehensively understand the core of the proposed method.
2. The advantages of the proposed method under low-rank conditions are very obvious (Figure 1 and Figure 8).

Weaknesses:
1. The proposed method have two versions, including spectral adapter^A and spectral adapter^R. However, the application boundaries of these two methods are not clear. Some experiments are applied by adapter^A, but others are applied by adapter^R. It's better to further discuss the difference or relationship between these two types of versions.
2. It's better to study the selection of columns of U and V. For example, bottom-r selection and random-r selection.
3. If we can combine addition and rotation?

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a new low rank adapter for large models. The idea is to apply the adapter in the SVD decomposition of a weight matrix. Two methods are proposed. First, train parameters that get added to top r columns of U and V matrices. Second, train parameters that rotate top r columns of U and V matrices. The resulting methods are shown to perform well on LLMs and diffusion models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I like the idea presented in the paper. It is intuitive and simple. A similar idea was presented recently in the PiSSA paper [1]. I think the experiments are sufficient and interesting. Moreover, compared to PiSSA that directly tunes the top r eigenvectors, additively tuning them has the benefit of providing a clear intuition when merging different adapters. I specially enjoyed reading the discussion on adapter merging as this is a topic that I have been thinking for a while now.


[1]: https://arxiv.org/abs/2404.02948

Weaknesses:
Evaluation is a big problem in this paper.

- LLM evaluations are few while diffusion evaluations seem to be mostly quantitative. I want to caveat this by saying that while few, the GSM-8k experiments with Mistral-7B are good enough to convince me that the method works. However, a comprehensive evaluation would have been more convincing and could shed light on cases where the method fails. 
- I wish the authors had focused more on LLM evaluations. I may be biased as I work on LLMs.
- Figure 8 of supplementary (above like 586) is interesting and the authors should consider moving it to the main body of the paper.

Limitations:
The limitations section is missing. One limitation that I can think of is: is it possible for a model to overfit since we are always tuning the most important eigenvectors. What if the fine-tuning dataset is structured so that only a select non-top eigenvectors need to be updated. Datasets like GSM-8k restrict the model output to a narrow pattern that is significantly different from what the model outputs normally. Hence, updating the top eigenvectors makes sense. However, what if the fine-tuning dataset is supposed to modify only a few things that the model has learnt. For instance, there's a lora that switches the model output from Joe Biden to whoever is the next US president. I believe that updating the top eigenvectors is actually a problem here, and, as the authors note in lora merging section (lines 210-214), there  may need to be some eigenvector scheduling.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
Kc37srXvan;"REVIEW 
Summary:
In summary, PointMamba, an innovative state space model tailored for point cloud analysis, successfully harnesses the global modeling prowess of Mamba, a representative SSM from the NLP domain. By adopting a linear complexity algorithm, PointMamba addresses the computational challenges posed by traditional Transformer-based methods while maintaining their global modeling capabilities. Its key techniques include utilizing space-filling curves for effective point tokenization and employing a non-hierarchical Mamba encoder as the foundation. Extensive experiments across multiple datasets validate the superior performance of PointMamba while significantly reducing GPU memory usage and FLOPs. This work not only demonstrates the vast potential of SSMs in 3D vision tasks but also provides a simple yet effective baseline for future research in this domain.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. **Linear Complexity with Global Modeling**: PointMamba leverages state space models to achieve linear complexity while maintaining global modeling capabilities, overcoming the computational challenges of traditional Transformers.

2. **Efficient Point Cloud Representation**: The use of space-filling curves for point tokenization enables efficient representation of point clouds, capturing spatial structure while facilitating global feature extraction.

3. **Simple and Effective Mamba Encoder**: The non-hierarchical Mamba encoder provides a simple yet powerful backbone for PointMamba, enabling fast and accurate global feature modeling.

4. **Superior Performance**: Comprehensive evaluations show that PointMamba achieves state-of-the-art performance across multiple datasets, demonstrating its effectiveness for point cloud analysis tasks.

Weaknesses:
1. Although PointMamba borrows structurally from Mamba, it may not take full advantage of the unique characteristics of point cloud data, such as spatial distribution, density variations, and local geometry.

2. PointMamba, while inheriting the strengths of Mamba, may have inherited some of the limitations of its design for natural language processing tasks. These limitations may not be applicable to point cloud analysis tasks, such as the lack of specific preprocessing steps, feature extraction methods, or post-processing techniques for point cloud data. The feature extraction and processing steps in the paper are largely similar to those of the previous PointMAE.

3. Although PointMamba achieves global modeling of linear complexity by introducing state-space models, there may be trade-offs between model complexity and performance in real-world applications. Because according to the experimental results of the thesis, the effect of partially using PointMamba becomes worse instead.

4. According to Table 5 of the ablation experiment, the serialization operations Hiber and Trans-Hiber have the most obvious impact on the experimental results. Meanwhile, the framework diagram at the core of the paper, i.e., Figure 4, shows that the only change is Hiber, from the directly cited literature [27], and there is almost no additional description or setting in the paper.

Limitations:
Content-wise, the paper has no obvious limitations, and its core technology builds on Mamba, which is popular in other fields. Formatting-wise, the paper shows the mamba icon several times; please confirm whether this representation is appropriate in an academic paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper propose a simple but effective Mamba-based method named PointMamba for point cloud analysis. This paper is the first paper that studies the Mamba-based method for point clouds. The experiments are comprehensive and the paper is in very good shape.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1.	This paper demonstrates excellent writing quality, with a clear and evident motivation.
2.	Figure 1 is highly comprehensible, particularly in its comparisons with Point-MAE.

Weaknesses:
1.	(No need for experiments) The use of validation datasets is prevalent in the field, with consistently high metrics reported. I encourage the inclusion of indoor segmentation and detection tasks, as exemplified in Point-M2AE and MaskPoint, in future iterations of your work. Additionally, tackling the classification task on Objaverse-LVIS appears to be a more demanding and stimulating challenge.
2.	To be honest, there are numerous papers that adhere to the evaluation paradigm established by Point-BERT. However, I am genuinely interested in exploring novel discoveries in self-supervised learning specifically applied to point clouds.

Limitations:
1.	The authors have addressed the limitations and potential negative societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces PointMamba, an interesting method for point cloud analysis that utilizes a linear complexity state space model (SSM) instead of traditional Transformer architectures. PointMamba employs space-filling curves for point tokenization and features a simple, non-hierarchical Mamba encoder. Additionally, the authors propose an effective order indicator and serialization-based mask modeling strategy. Comprehensive evaluations across multiple datasets demonstrate PointMamba's superior performance and significantly reduced computational costs compared to Transformer-based methods.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The paper reads very well. I appreciate the motivation of the paper and agree that we should make an effective method as simple as possible. As the first Mamba-based work for point cloud tasks, the proposed method is simple, elegant, and effective, establishing a solid Mamba-based baseline for point cloud analysis tasks.

2. The paper considers the limitations of Mamba's unidirectional modeling and proposes practical solutions like serialization-based mask modeling strategies and order indicators.

3. The theoretical analysis of Mamba used in point cloud is reasonable, and the paper is easy to reproduce.

4. The experiments are convincing and support the main idea of the paper. The authors provide thorough evaluations, including comparisons with SOTA methods, ablation studies, and analyses of each component.

Weaknesses:
1. Some experiments are missing. For example, from Table 11, it would be beneficial to include more masking ratios, especially the performance when masking 90% point patches. Besides, from Table 12, could the authors provide a more detailed analysis of why average pooling performs better? Additionally, what about the performance of max pooling?

2. Have the authors considered the potential benefits of integrating PointMamba with Transformer architectures? For example, a transformer layer can be used as the final prediction head. Such a combination can take advantage of both architectures and will not introduce many computational costs.

3. The point clouds have complex structures, and this paper only considers two orders. In my view, introducing more orders (e.g., introducing three serialization methods and tripling the input length) can better capture the geometry information of the point clouds, which might be beneficial for learning. 

4. During the pre-training strategy, do the authors use different order indicators or the same indicator? Figure 4 is somewhat ambiguous.

Minor: Using space-filling curves to scan point clouds is an interesting attempt. Although it is common knowledge for some readers, it still would be helpful if the authors provided a more detailed introduction to space-filling curves in the preliminaries part.

typos: PointMAE -> Point-MAE

Limitations:
The authors have adequately discussed the limitations of their work.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work utilizes the Mamba architecture for point clouds. It employs Hilbert and Trans-Hilbert curves to order the point clouds, thus addressing the unidirectional modeling nature of Mamba. Additionally, it replaces transformer blocks with Mamba blocks. The proposed PointMamba demonstrates reasonable performance following pre-training.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work tries introducing a new network architecture to the point cloud domain, which is appreciated.
2. The presentation is clear, accompanied by well-crafted figures.

Weaknesses:
1. While it is worthwhile to explore how the Mamba architecture performs when applied to point clouds, this work exhibits limited novelty or insights in architectural innovation for point clouds. The authors themselves acknowledge this by stating, “It should be noted that this paper does not claim algorithmic novelty but rather presents a simple and solid Mamba-based baseline.” In my opinion, works that directly adopt an existing network from other domains to point clouds without sufficient insights and innovation should not be accepted by top-tier conferences like NeurIPS.

2. The authors only demonstrate the performance of **pre-trained** PointMamba by comparing it with other widely used baselines like PointNext. This implies that PointMamba cannot surpass previous methods when trained from scratch. To effectively showcase the solid merits of PointMamba, it would be more reasonable to provide comparisons without pre-training.

3. In Table 4, an important baseline, PointNext, is omitted. According to the PointNext paper, PointNext exhibits significantly better performance than the proposed PointMamba (87% without pre-training vs. 84.4% with pre-training). This omission raises doubts about the effectiveness of PointMamba. To give a ""solid Mamba-based baseline"", comprehensive comparisons with previous methods should not be omitted.

4. In Figure 1, it would be beneficial to include comparisons with widely used methods like PointNext to convincingly demonstrate the necessity of introducing Mamba for point clouds. While PointMamba likely offers inference advantages, this contribution stems from the original Mamba paper, not this work. Additionally, does PointMamba have lower training efficiency?

5. The paper claims that the proposed Hilbert and Trans-Hilbert ordering is advantageous based on results from ScanObjectNN. However, I am not entirely convinced. I suggest the authors also present results on ShapeNet-Part and ModelNet40, for which only comparing against a random baseline is enough.

Limitations:
See the weakness section.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
SKhR5CuiqQ;"REVIEW 
Summary:
The present paper introduces a training-free method to sample differentiable representations using pre-trained diffusion models. This is achieved by pulling back the dynamics of the reverse-time process from the image to the parameter space. Moreover, training-free methods for conditional sampling are employed to (approximately) satisfy implicit constraints. Numerical experiments show the performance of the method for images, panoramas, and 3D NeRFs. In particular, it is shown that the method can improve upon the baselines in terms of quality and diversity of generated representations.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed framework is versatile and can be used for various differentiable representations. The presented numerical experiments show that the resulting methods can improve quality and diversity compared to other training-free baselines.

Weaknesses:
1. Baselines and related works: Further discussion of related works and additional numerical comparisons are needed.
    - It would be helpful to provide further discussion on related works, such as Zero-1-to-3, HiFA, Magic123, LatentNeRF, Fantasia3D, ...
    - While the presented results are promising, additional experiments and visualizations would strengthen the validation of the method.
    - A more detailed comparison to methods requiring fine-tuning/training could provide a clearer picture of the method's performance. In particular, it would be good to add comparisons in terms of (overall) runtime/NFEs.
    - Why are there no PSNR/SSIM/LPIPS numbers in Table 1 for the baselines, e.g., SJC?
    - While DreamFusion is similar to SJC, it would still be interesting to add empirical comparisons.

2. Presentation and theory: Further details and explanations could be provided (e.g., also in the appendix).
    - The geometric perspective and pullback operation might be challenging for readers not well-versed in these areas. One could improve accessibility by providing more intuitive explanations.
    - Precise details on how RePaint is adapted to this setting seem to be missing. In this context it would also be good to clarify the connections to other methods for posterior sampling (DPS/ReSample/...).
    - It would be helpful to add some explanation on the ""separately managed noise object"" $\epsilon(t)$ or $\epsilon(\pi)$.

Limitations:
Only a few limitations are mentioned and it would be good to enumerate potential failure cases of the proposed method.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a zero-shot method to sample neural representations with pre-trained diffusion models. By pulling back the measure over the data space through the representation, the authors express the PF ODE in the parameter space. Solving this ODE can directly provide parameter samples of the representation. The authors also discuss cases where the representation needs to model coupled images. In this scenario, the PF ODE is expressed by expectation and can be solved following the same principle. This method offers random samples rather than returning the mode, which can yield results with higher diversity compared to baselines without sacrificing performance. The method is also compatible with RePaint, which can ensure the implicit constraint (e.g., the inductive bias of the representation or practical consistency constraint).

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. This work introduces a zero-shot method to sample neural representations by pre-trained diffusion models. The method is fully zero-shot and does not rely on the specific form of the representation. Therefore, it may gain great potential in downstream tasks where training is difficult, or the representation is highly restrictive.
2. This method can generate samples with significantly higher diversity compared to the baselines, and can ensure consistency constraint using RePaint, improving global coherence and quality of generation.
3. The idea to handle the Jacobian matrix by approximating each ODE step using an optimization task is cute and neat.

Weaknesses:
My primary concern is its runtime. If I understand correctly, you need to run the optimization for several iterations for each data point and each Euler step. And for tasks like NeRF, where the PF ODE involves expectations, this optimization also involves an MC estimator. However, this paper only reported the runtime for 3D NeRF experiments. I am curious on 

a. what is the runtime for other experiments? What is the runtime for the baselines? Is this method significantly more expensive than others?

b. How many samples do you use to estimate the expectation? Does it contribute to the runtime? Will reducing the number of samples increase the variance and lead to suboptimal performance?

I would still appreciate this approach even though it may be more expensive. However, reporting the runtime and comparing it with the previous baselines can help us understand this method better.

Limitations:
I did not find discussions on limitations even though Sec 6 is called Limitations and Conclusion. I am unsure if this work has no limitations, such as runtime. Could the author please clarify this?

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a method for sampling a differential representation using a pre-trained diffusion model. Instead of sampling in the image space, the authors propose sampling in the parameter space of differential representations by 'pulling back' the probability distribution from the image space to the parameter space of the differential representation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper introduces a novel method for sampling a differentiable representation for a pre-trained diffusion model, which is highly relevant.
- Their method appears to be well-principled.
- Although their results are limited, they seem promising.

Weaknesses:
- The technical part of the paper is difficult to follow.
- Comparison to the state-of-the-art is provided only in Section 5.1, where a differential representation is not required.
- The code is not shared at this stage.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel, training-free method to sample through differentiable functions using pretrained diffusion models.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
It sounds a general method that could apply to many different scenarios.

Weaknesses:
More systematic evaluation of the method in addition to image examples would be great.

Limitations:
The authors have discusses some limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

";1
Y4L8GQXZZO;"REVIEW 
Summary:
In this papaer, the authors proposed a method to finetune vision-language model (CLIP) via prompt learning. Specifically, the author optimize a global textual prompt which will be optimized via FedAvg, while each client additionally optimize a local textual prompt. The proposed method is a combination of the existing methods CoOp and PromptFL and outperforms both methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written.
2. The theoretical analysis of the proposed method is thorough and clear.

Weaknesses:
1. The proposed method is basically a combination of both CoOp and PromptFL, or to be more specific, optimizing both client-specific and client-agnostic textual prompt. The novelty of the proposed needs to be further highlighted in the main paper.
2. The combination strategy seems to be a bit straight forward, i.e., using a single scalar value $\theta$, the author should discuss other possibilities.
3. The authors should compare with other prompt-based tuning methods for FL, e.g., [1][2]. Also, adding more ablation studies could be beneficial.

[1] Guo T, Guo S, Wang J. Pfedprompt: Learning personalized prompt for vision-language models in federated learning[C]//Proceedings of the ACM Web Conference 2023. 2023: 1364-1374.

[2] Qiu C, Li X, Mummadi C K, et al. Text-driven Prompt Generation for Vision-Language Models in Federated Learning[J]. arXiv preprint arXiv:2310.06123, 2023.

Limitations:
The authors do not discuss the limitation of the method.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper discusses the integration of pretrained vision-language foundation models, such as CLIP, into federated learning. The idea is to use prompt-based federated learning to minimize the communication and computational costs. The authors go into the theoretical analysis to understand the performance of this approach

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The authors provide a theoretical understanding on prompt Federated Learning for models like CLIP.
- The results demonstrate that the combination of global and local prompts can support increased accuracies. 
- The overall investigation of this space is interesting and useful for the community

Weaknesses:
- While the overall approach is quite interesting, the authors restrict their investigation in a much simpler classification task. It is unclear why a larger foundation model is required for simpler classification tasks, where traditional FL techniques on vanilla models (e.g., reset, mobileNet etc) might work better towards learning new unseen distributions that a foundation model might not have seen (e.g,. x-ray hospital images etc). 
- Following the previous comment, it is unclear how this method would work on unseen distributions that most FL applications are mostly useful for. Most FL models are quite good at generalising to data that can be found on the public internet, but it is unknown how well they would work on out-of-distribution private data. 
- Given the fact that there are a few related prompt-based FL methods (e.g., promtFL, FedPrompt, etc) limit a bit the novelty of this work to mostly providing a solid theoretical analysis. 
- There might be some privacy implications from this work, as sensitive data are shared. 
- The evaluation was done with mostly 10 (up to 50) users, whereas in typical FL scenarios we want to learn from million of users. It is unclear how these results would generalise. Overall, the evaluation is a bit simplistic, showing only accuracy results on a few tasks.

Limitations:
See above

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies theoretical properties of prompt based federated learning methods for visual language models. Following the proposed theoretical results, a new algorithm named Global-local Prompt Portfolio for Federated Learning (PromptFolio) was proposed. The proposed algorithm was examined in image classification tasks using a CLIP model.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Analyzing prompt-based federated learning methods with feature representations and exploiting their relation to portfolio optimization is a nice idea. The proposed analyses also led to a new algorithm for prompt based federated learning.

Weaknesses:
Experimental analyses are limited and should be improved as well.

Limitations:
Limitations were partially addressed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The proposed methodology offers a novel take on analyzing federated learning using prompt learning (for foundation models) via feature learning theory. At its core, the idea is to identify and monitor task-relevant and task-irrelevant features, and leveraging inspiration from portfolio optimization which says to combine independent assets to maintain income while decreasing risk of investment, construct a 2 part prompt. One part which is global and another which is local, to help with personalization. By mixing these prompts optimally, the proposed PromptFolio method highlights an improvement in overall performance of the FL system.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
- A key strength of the proposed method is its novel take on explaining prompt-based federated learning, using feature learning theory.
- Clarity in idea and its link to existing work.

Weaknesses:
Abstract/Conceptual Questions:
- Could the authors comment about the few-shot learning aspect of foundation models (+prompt learning) and how the optimal mixing coefficient would be affected in that scenario?
- An adjacent sub-field that contains foundation models and federated learning is the study of their impact in learning new classes using prior knowledge (base vs. new classes performance). Could you comment on how this sub-field could relate to the proposed method?

Clarifications:
- Could the authors clarify the exact dimensionality of the prompt used in each of the baselines as well as PromptFolio?
- Could the authors mention if there are previous works that have proposed a 2-level prompt learning idea? (since Section 2.2 L 107 mentions that an exploration of such a idea and the cooperation between prompts is sparse)
- In Section 4. L. 224, the authors mention that ""task-relevant coefficients can be directly added"". Could you clarify this statement?
- Could the authors highlight the data setting used to generate Table 1?
- Could the authors explicitly state the train and test conditions (settings) used within the experiments?

Suggestions:
- Since the proposal of PromptFL, there are have been further advances to the idea of prompt learning within the federated setting. Could the authors comment and draw comparisons against the more recent methods?

Limitations:
yes, the authors have addressed limitations within their choices of the experiment setting. However, I would recommend drawing more broad comparisons as to potential missing pieces in comparison to more recent work as well as the finer points from the feature learning theory that don't fully match up to the settings in the FL system.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper tackles the interesting problem of analyzing federated learning (FL) from vision-language foundation models like CLIP. The authors develop a theoretical framework based on feature learning theory to understand how prompt-based FL works. They introduce a new algorithm called PromptFolio that mixes global and local prompts, drawing an analogy to portfolio optimization in finance. The theoretical analysis shows how this approach can balance generalization and personalization in FL. They back up their claims with experiments on several datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The feature learning approach they develop seems pretty solid. 
- The connection between prompt mixing and financial portfolio theory is novel.
- The ablation studies on data heterogeneity and client numbers are particularly informative.

Weaknesses:
- This reviewer is confused by the feature learning theory (I am not familiar with it), which is introduced in a relatively short section. Is it (looks like) a common sense in deep learning or a kind of new theory proposed recently? It would be helpful to have a more thorough introduction to the theory.
- Using a single activation function for the text encoder is a pretty big simplification from how CLIP and other large language models actually work.
- All the experiments focus on classification tasks. Any thoughts on how PromptFolio might perform on other vision-language tasks like image captioning or visual QA in a federated setting?
- Why is the proposed method on OxfordPets dataset not as good as coop? While it seems to be dominating on other datasets.

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
RxXdokK2qz;"REVIEW 
Summary:
This paper studies the asymptotic bias in non-linear stochastic approximation algorithms with Markovian noise and fixed step-size. Upon applying the averaging technique of Polyak and Ruppert, the authors identify that, in general, the bias is of the same order of the step-size. The main source of bias is characterized, and an extrapolation technique is employed so that bias is attenuated. Finally, a few numerical studies are presented for illustration of the theoretical contributions.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
I find the overall contribution of the paper to be very interesting. As far as the reviewer is aware, the characterization of bias for nonlinear SA with parameter dependent Markovian noise is novel and original. 

The analysis seems sound. The reviewer did not have time to look over every single proof carefully, but haven't found egregious errors in the proofs revised.

The problem setup, assumptions and approach to analysis are clearly stated. The paper is well-written, but some polishing is needed.

Weaknesses:
In particular, there have been recent papers, achieving similar bias characterizations and higher order error bounds for stochastic approximation with Markovian noise (see for example [R1] and [R2] which both deal with linear recursions). The approach to analysis in [R2] is similar to the present paper in the sense that the bias characterization is also given in terms of solutions to Poisson's equations. Moreover, this is not the first time that Richardson-Romberg extrapolation is used for bias attenuation in stochastic approximation: it was previously proposed in [R1] to kill the dominant term of order O(\alpha) just like the present paper.

The authors do cite [R1] in the present work, but the reviewer feels that a deeper discussion is needed on how the present paper improves upon [R1] and [R2]. I encourage the authors to include a citation on previous uses of this technique as well.


[R1] Huo, Dongyan, Yudong Chen, and Qiaomin Xie. ""Bias and extrapolation in Markovian linear stochastic approximation with constant stepsizes."" Abstract Proceedings of the 2023 ACM SIGMETRICS International Conference on Measurement and Modeling of Computer Systems. 2023.

[R2] Lauand, Caio Kalil, and Sean Meyn. ""The curse of memory in stochastic approximation."" 2023 62nd IEEE Conference on Decision and Control (CDC). IEEE, 2023.

Limitations:
The authors discuss the limitations of their work and assumptions in section 3.3.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies non-linear stochastic approximation scheme ($\theta_n$) driven by a uniformly geometrically ergodic MC $(X_n)$. Moreover, it is allowed the evolution of the Markovian noise $X_n$ to depend on $\theta_n$.  The authors study the asymptotic behaviour of the last iterate, Polyak-Ruppert and Richardson-Romberg procedures for differentiable test functions. The result generalizes recent result of Aymeric Dieuleveut, Alain Durmus, and Francis Bach. Bridging the gap between constant step size stochastic gradient descent and markov chains. The Annals of Statistics, 48(3):pp. 1348–1382, 2020.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Novel technique based on infinitesimal generator comparison

Weaknesses:
- It would be good to provide non-asymptotic results rather than asymptotic
- I think that the result of theorem 3 is very weak since it is obtained using Chebyshev’s inequality. Could you please comment on $\alpha^{5/4}$ term?

Limitations:
-

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies stochastic approximation with constant stepsize and Markovian noise. The authors provide a characterization of the bias (i.e., the difference between the expectation of the iterate and the desired limit), show that Polyak-Ruppert averaging can help reduce the variance but not the bias, and numerically demonstrate that Richardson-Romberg extrapolation helps reduce the bias.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The writing is mostly clear and the proof is written with high quality. The result is an extension of [17,36] to the case where the Markov chain can have its transition probability matrix as a function of the stochastic iterate, which enables the authors to apply the results to RL algorithms with time-varying behavior policies (though the results are not formally presented in this paper). Overall, this is a strong theoretical work and the techniques developed in this work are quite novel.

Weaknesses:
(1) Assumption (A2) is relatively strong and difficult to verify in practice.

(2) The authors claim that Assumption (A4) implies the global exponential stability of the ODE, but did not provide a reference for the claim. Also, the claim in lines 466-471 needs formal justification.

Limitations:
The results are mostly asymptotic. A characterization of the stochastic approximation algorithm behavior for a finite $n$ would be preferred for practical purposes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
N5H4z0Pzvn;"REVIEW 
Summary:
This paper investigates alternative training methods for Generative Flow Networks (GFlowNets) by evaluating various divergence measures, including Renyi-α, Tsallis-α, reverse, and forward Kullback-Leibler (KL) divergences. Traditional methods focusing on minimizing log-squared differences are shown to lead to biased and high-variance estimators. The authors propose efficient estimators for the stochastic gradients of these divergences and introduce control variates to reduce gradient variance. Empirical results across diverse tasks demonstrate that minimizing these divergence measures significantly accelerates training convergence and enhances stability compared to traditional methods. The paper establishes theoretical connections between GFlowNets and variational inference, extending these insights to arbitrary topological spaces, and highlights the practical effectiveness of control variates in improving training efficiency.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- **Comprehensive Evaluation**: The paper thoroughly evaluates multiple divergence measures and their impact on GFlowNet training.
- **Theoretical Insights**: Establishing theoretical connections between GFlowNets and VI broadens the understanding of these models. 
- **Practical Contributions**: The design of control variates to reduce gradient variance is a significant practical contribution that can be applied in various optimization scenarios.
- **Variance Reduction Techniques**: The introduction of variance reduction techniques, including control variates and leave-one-out estimators, effectively addresses the high variance issue in gradient estimates, enhancing the learning stability and efficiency.

Weaknesses:
- **Theory** The theoretical contribution builds upon previous works [4,5]. However, I do not think the theory itself has enough contribution. Because the Measurable pointed DAG is from previous work [5] and the main theoretical claim, Proposition 1, has limited novelty compared with Proposition 1 in [4]. Therefore, unlike previous works [4,5], they have their novel theoretical contributions and synthetic experiments would suffice. Thus, for this paper, stronger experiments are required, as will be discussed in the next.  
- **Experiments - Part 1** This paper does not contain enough real-world tasks, such as fragment-based molecule generation [1], graph combinatorial optimization [2], and RNA sequence generation [3] to illustrate its main contribution. These are standard tasks in evaluating GFN performances and are necessary to include. For the only real-world task in the paper, i.e., the BPI task, this paper admits that ""not statistically significant"" in line 334. Therefore, it is unclear whether the proposed methods to use other divergence measures will be meaningful in real-world scenarios. 
- **Experiments - Part 2** For the synthetic tasks, the plots in Figure 3 are also missing some lines. For example, why are there only two lines in the **Sets** plots? Also, there is no single best loss function that is uniformly better than the KL baseline. Therefore, additional divergence might seem unnecessary. There should be stronger evidence to support the use of other measures. Or, the authors can provide a guideline on how to choose the best measures given prior information about the tasks. 

- I would raise my scores if additional experiments are included and the proposed methods are indeed beneficial in more complex tasks. 


[1] Jain, Moksh, et al. ""Biological sequence design with gflownets."" International Conference on Machine Learning. PMLR, 2022.

[2] Zhang, Dinghuai, et al. ""Let the flows tell: Solving graph combinatorial problems with GFlowNets."" Advances in Neural Information Processing Systems 36 (2024).

[3] Kim, Minsu, et al. ""Local search gflownets."" arXiv preprint arXiv:2310.02710 (2023).

[4] Malkin, Nikolay, et al. ""GFlowNets and variational inference."" arXiv preprint arXiv:2210.00580 (2022).

[5] Lahlou, Salem, et al. ""A theory of continuous generative flow networks."" International Conference on Machine Learning. PMLR, 2023.

Limitations:
See **Weaknesses**.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
GFlowNets are a probabilistic framework for training amortized samplers for high-dimensional compositional spaces. The samplers are typically trained using local consistency objectives which are squared log losses. This paper examines alternatives to these obejctives in the form of general statistical f-divergence measures. The authors consider forward and reverse KL, Renyi-$\alpha$ and Tsallis-$\alpha$ divergences. The authors derive gradients for the divergences in the case of a fixed $p_B$ and on-policy samples. These gradient computations rely on REINFORCE-style estimators and consequently can suffer from high-variance gradient estimates which affect the optimization procedure. For variance reduction, the authors derive control variates for their gradient estimators. Through a series of experiments on a variety of tasks, the authors demonstrate the effectiveness of these divergences for training GFlowNets.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
* The paper is quite well written and clear. The authors are thorough and clear in introducing the central ideas and provide sufficient details making it easy to follow.
* The paper studies the important problem of finding the ""right"" learning objective in the context of training GFlowNets. Following a long line of work in VI, the authors leverage statistical divergences and propose optimizing them directly to learn the GFN sampler.
* Additionally, the authors improve upon prior work drawing a connection between GFNs and VI by proposing principled control variates to reduce the variance in the gradient estimates for the divergences from the REINFORCE estimators. The CVs seem to have a significant effect on the performance.
* The empirical analysis covers a variety of problems including continuous and discrete spaces as wells as general DAGs and tree spaces in the case of discrete spaces. 
* The authors also include code with the submission aiding reproducibility.

Weaknesses:
* The paper considers the on-policy setting for training GFlowNets and the proposed learning objectives based on the divergences assume on-policy smaples. However, existing flow-based objectives are all off-policy, and the advantage of GFlowNets on a lot of tasks (specifically challenging tasks with multi-modal target distributions) comes from the ability to train on off-policy trajectories (e.g. replay buffer[1] to local search [2]). So while the proposed learning objectives empirically perform better than TB on tasks where on-policy sampling is enough, it lacks the flexibility of accomodating off-policy training.
* The authors also assume that the backward policy $P_B$ is fixed (L153). While this is true in some cases, learning $P_B$ results in significant improvements to the learned sampler[3,4]. As far as I can tell, modifying the proposed objectives to accomodate learning the P_B is non-trivial.
* I appreciate the diversity of problems studied by the authors in their experiments, but there are some gaps in the empirical analysis. Specifically, the authors only include a TB baseline and not other objectives such as SubTB, DB which can perform better than TB (and have better training stability) in some cases. Additionally, the paper also does not include the VarGrad-style objective [5] which does away the need for estimating $Z$ in TB.
* Moreover, the tasks consider relatively small tasks so it is not clear how scalable the proposed objectives are. 


[1] Towards Understanding and Improving GFlowNet Training. Max W. Shen, Emmanuel Bengio, Ehsan Hajiramezanali, Andreas Loukas, Kyunghyun Cho, Tommaso Biancalani. ICML 2023. 

[2] Local Search GFlowNets. Minsu Kim, Taeyoung Yun, Emmanuel Bengio, Dinghuai Zhang, Yoshua Bengio, Sungsoo Ahn, Jinkyoo Park. ICLR 2024. 

[3] Trajectory balance: Improved credit assignment in GFlowNets. Nikolay Malkin, Moksh Jain, Emmanuel Bengio, Chen Sun, Yoshua Bengio. NeurIPS 2022. 

[4] A theory of continuous generative flow networks. Salem Lahlou, Tristan Deleu, Pablo Lemos, Dinghuai Zhang, Alexandra Volokhova, Alex Hernández-García, Léna Néhale Ezzine, Yoshua Bengio, Nikolay Malkin. ICML 2023.

[5] Robust Scheduling with GFlowNets. David W. Zhang, Corrado Rainone, Markus Peschl, Roberto Bondesan. ICLR 2023.

Limitations:
The authors do not explicitly address the limitations of their approach (discussed in the weaknesses) section, though the assumptions are mentioned briefly in Section 2 and 3. The authors only mention the choice of $\alpha$ as a limitation.

There is also no discussion of broader impacts (the reference in the checklist is broken too).

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the potential of using a variety of divergence measures directly as training losses for GFlowNets, relying on many of the connections made between GFlowNets and variational inference. Training GFlowNets essentially consists in enforcing balance/flow-matching conditions between a proposal and a target distribution given some data. Given the links between GFlowNets and variational inference, training the GFlowNet to directly minimize the KL divergence was observed to result in biased and high-variance estimators in general. This paper aims to verify the latter claim, for different divergence measures (KL, reverse KL, Tsallis-\$\alpha\$, Renyi-\$\alpha\$). The paper alleviates the afore-mentioned limits by proposing low-variance gradient estimates to that of the latter quantities through control variates. Finally, the paper verifies the proposed methods experimentally through an extensive set of experiments.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- The paper widens the scope of the theoretical links between GFlowNets and variational inference beyond the assumption of finitely supported measures. 
- The authors alleviate the high variance of divergence measures' gradients in practice, relying on control variates. 
- The paper is very well written and presented and is easy to follow. 
- The experimental setup is exhaustive, and shows the effect of each of the proposed design choices.

Weaknesses:
See questions.

Limitations:
Limitations adequately addressed throughout the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates divergence measures as learning objectives for Generative Flow Networks, which are amortized inference models designed for sampling from unnormalized distributions over composable objects. The authors review four divergence measures - Renyi-\alpha, Tsallis-\alpha, reverse and forward Kullback-Leibler divergences, and design estimators for their stochastic gradients in the context of training enerative Flow Networks. The authors verify that minimizing these divergences yields correct and empirically effective training schemes on several toy environment, and show that it often lead to faster convergence than previously proposed optimization methods. The authors also design control variates based on REINFORCE and score-matching estimators to reduce gradient variance.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The paper provides evaluation of different divergence measures as learning objectives for GFlowNets, showcasing their effectiveness in improving training convergence in several toy environments.

- The authors develop control variates for reducing the variance of gradient estimates.

Weaknesses:
1. While the paper provides empirical evidence for the effectiveness of divergence-based objectives, a more extensive comparison with traditional GFlowNet training methods across a wider range of datasets and applications would strengthen the claims.

2. The choice of the \alpha parameter in Renyi-\alpha and Tsallis-\alpha divergences is not extensively explored. More insights into the impact of \alpha on the learning dynamics and guidance on selecting an appropriate value would be beneficial.

3. The computational overhead introduced by the control variates and their impact on training time is not explicitly discussed.

Limitations:
Yes.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
FmNoFIImZG;"REVIEW 
Summary:
The authors address the problem of data augmentation for classification tasks by proposing a novel approach that utilizes separate energy-based models (EBMs) to generate synthetic data for each class. These EBMs are derived directly from the logits of a binary classifier whose goal is to distinguish real data from negative data. These logits (their logsumexp over positive and negative classes) are reinterpreted them as an energy function for each class.
In order to avoid training these classifiers on the data itself, the authors take advantage of a pre-trained Prior-Data Fitted Network (PFN).

The authors perform a comprehensive evaluation of their method comparing it to competing methods for data augmentation on a number of small real world datasets. They show that their method is the only one that offers a consistent improvement in classification from the addition of synthetic data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-structured and clearly written, effectively communicating the key ideas and concepts.
- The authors introduce a novel approach by leveraging a Prior-Data Fitted Network (PFN) to obtain energy-based models (EBMs), which, to the best of my knowledge, has not been previously explored.
- The experimental results section is thorough and comprehensive. They compare their approach to a wide range of relevant and competitive methods for synthetic data generation.
- The appendices are well-organized and provide valuable supplementary information. The paper strikes a good balance with the most relevant information presented in main text and additional details provided in the appendix.

Weaknesses:
- Interpreting classifier logits as energy functions may not necessarily yield good density models. Given a classifier with logits $f(x)[y]$, one can add an arbitrary function $g(x)$ that is constant over $y$, $f(x)[y] \rightarrow f(x)[y] + g(x)$, without affecting the conditional probabilities $p(y\vert x)$ due to the invariance of the softmax function to the addition of a constant. However, this changes the energy function for the density model to $E(x) \rightarrow E(x) - g(x)$. Consequently, there exists a functional degree of freedom that can arbitrarily alter the energy function without impacting the discriminative loss optimized by a classifier. As a result, a classifier trained solely to distinguish between positive and negative samples is not inherently encouraged to learn a useful energy function for the density over $x$.
My interpretation of the results of [1], from which the authors seem to draw inspiration is that reinterpreting the logits of a classification model as an energy based model by itself accomplishes nothing if the training process of the original classifier is not changed to encourage a good density model to be learned and encoded into the free degree of freedom. [1] achieves this by optimizing the joint likelihood, $E_{x,y} p_\theta(y,x)$, instead of the conditional likelihood, $E_{x,y} p_\theta(y\vert x)$, used in training classifiers. This significantly complicates training by requiring sampling from the EBM during training. Therefore, it is unclear why the authors assume that the pre-trained PFN has learned useful energy functions for density modeling when it is derived from a simple classifier and this is not discussed in the paper.

- Another weakness of the paper is in the experimental setup where only default hyperparameters are used for all competing methods. For many of these methods, tuning hyperparameters can substantially improve their results and the default hyperparameters may not be optimal.
Personally, I would find the presented results more convincing if the paper compared with fewer competing methods but made a better effort of getting good results out of those as it is impossible to know how much the authors tuned the degrees of freedom available during experimentation with their own proposed method.

[1] Will Grathwohl, Kuan-Chieh Wang, Jörn-Henrik Jacobsen, David Duvenaud, Mohammad Norouzi, Kevin Swersky. Your Classifier is Secretly an Energy Based Model and You Should Treat it Like One, 2022 (https://arxiv.org/abs/1912.03263)

Limitations:
The authors aknowledge that due to the reliance of the method on a PFN it is currently constrained for use in small datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes TabEBM, a novel data augmentation method designed for low-sample-size tabular classification tasks. TabEBM generates synthetic tabular data using class-specific Energy-Based Models (EBMs) to learn the marginal distribution for each class. Experimental results on various real-world datasets demonstrate that TabEBM improves downstream performance via data augmentation, generates high-fidelity synthetic data, and strikes a competitive balance between accuracy and privacy in data sharing.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Novel approach using class-specific EBMs for tabular data generation.
- Comprehensive evaluation across multiple datasets, metrics, and downstream tasks.
- Strong performance, especially in low data regimes.
- Thorough analysis of statistical fidelity and privacy preservation.
- Open-source implementation provided.

Weaknesses:
- Scalability Issues: The reliance on TabPFN, which struggles with large sample sizes, limits TabEBM's scalability.
- Implementation Complexity: The need for class-specific surrogate tasks and the iterative nature of the sampling process may complicate implementation and increase computational overhead.

Limitations:
- Performance may degrade for datasets much larger than those tested.
- Inherits limitations of the underlying TabPFN model.
- May not be suitable for datasets with extremely high dimensionality.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors present a new method of tabular data augmentation, called TabEBM. The unique feature of TabEBM is that it creates distinct generative models for each class in a classification problem setting. With extensive and thorough evaluations, the authors prove that TabEBM sets the new state of the art.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The manuscript is well organized. The text is clear. The quality of the figures is high. 
2. The authors address a very common problem for the broad scientific community. Application of machine learning algorithms on small tabular datasets is limited by design. Introducing TabEBM as an effective data augmentation method, this work can have an extremely broad impact in the future.
3. The experiments are rich and rigorous. The method is compared against many existing methods on a variety of datasets. The evidence proving its effectiveness is convincing.
4. The authors claim TabEBM will be available as an open-source library.

Weaknesses:
Some important details on study design, model training, and practical considerations of TabEBM application are missing. See questions.

Limitations:
Briefly discussed in section 4.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces TabEBM, a class-conditional generative method for tabular data augmentation using distinct EBMs for each class. By modeling each class's data distribution individually, TabEBM generates high-quality synthetic data. Experiments demonstrate that using TabEBM for data augmentation improves classification performance across various datasets, particularly small ones.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper introduces distinct class-specific EBMs for tabular data augmentation. Moreover, it presents thorough experiments demonstrating effectiveness across various datasets, especially small ones. Additionally, the writing is well-structured and clear.

Weaknesses:
1. On the technical level, TabPFN, EBMs, and SGLD have all been introduced in TabPFGen [48]. The main difference in the proposed TabEBM method is the use of class-specific EBMs. However, this idea is straightforward and lacks novelty.
2. The most significant difference between tabular tasks and common tasks like images is that they include both continuous and categorical features. However, TabEBM treats all features as continuous, lacking targeted consideration for categorical features.
3. In TabEBM, although the choice of surrogate binary classifier is arbitrary, not using training-free models like TabPFN will result in training costs being positively correlated with the number of classes, significantly increasing the training overhead.
4. The datasets used in the experiments have a maximum dimensionality of 77, lacking experimental results on high-dimensional data (e.g., hundreds of dimensions).

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
ca2mABGV6p;"REVIEW 
Summary:
The authors analyze the encoder features of diffusion UNet and find that they share a lot of similarities across certain time steps. Based on this observation, the authors propose to reuse the encoder features and do parallel computation in sampling. This significantly fastens the generation process. Additionally, they also introduce a prior noise injection method to improve the generation quality. Extended experiments demonstrate their effectiveness across a wide range of generation tasks, including T2I, T2V, DreamBooth, controlNet. And their method can be seamlessly combined with various noise schedulers.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1 This paper is well-written and easy to understand. The analysis in section 1 is insightful. Fig.4 clearly shows the encoder propagation methods. 
2 The empirical evaluation is thorough. The authors validate their method on various datasets, architectures, and use various metrics. They also make a lot of comparison to the SOTA acceleration methods, such as DeepCache.
3 Sec. A4 shows that the parallel denoising leads to an acceptable memory cost, which addresses my concerns.
4 This approach can be seamlessly combined with other acceleration methods such as novel noise schedulers and distillations.

Overall, this is an excellent work that significantly reduces diffusion sampling cost in a training-free manner.

Weaknesses:
I think the manual selection of key time-steps may not be optimal. Is it possible to use some automatic strategies to determine the key time-steps in your work? Otherwise when we use a new architecture we would have to redo the encoder feature analysis experiment and manually define the key-time steps again. I think there are several works have explored the selection of key time-steps. Discuss on them will make your work even stronger.

[1] OMS-DPM: Optimizing the Model Schedule for Diffusion Probabilistic Models
[2] AutoDiffusion: Training-Free Optimization of Time Steps and Architectures for Automated Diffusion Model Acceleration
[3] Denoising Diffusion Step-aware Models

Limitations:
Yes, they discuss their limitation in section 5.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an approach to accelerate diffusion model inference by capitalizing on the minimal change in encoder features across time steps. The proposed encoder propagation strategy reduces encoder computations by reusing encoder features from previous time steps. The proposed method is comparable to existing acceleration methods and demonstrates effective acceleration across diverse tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1.	The proposed method offers a promising solution to speed up diffusion models.

2.	The experiments are conducted from various tasks, e.g., text to image; text to video; personalized generation and reference-guided generation.

Weaknesses:
1.	The evaluation metrics are insufficient, as concerns exist regarding the universality of FID in assessing generative models. It is recommended to validate performance on additional metrics, such as the newly proposed ImageReward [1] and Pick Score [2].

2.	It is unclear whether the latency is comparable when fewer sampling steps are used (Table 11 and Table 12). Moreover, the performance improvement is marginal compared to results with fewer sampling steps, and mainly stems from prior noise injection, which is not the core contribution of the paper. I am also curious whether fewer sampling steps, if equipped with prior noise injection, could achieve similar performance as the results presented in this paper.

3.	Comparing multi-GPU results with those single-GPU results from DeepCache seems unfair (Table 1). Besides, the acceleration outcomes on a single GPU are marginally improved when compared with other acceleration methods.

4.	The DiT does not inherently include concepts of Encoder and Decoder; thus, categorizing based solely on observations into Encoder and Decoder is somewhat imprecise.

5.	The layout of the paper is somewhat disorganized; the order of tables does not align with the sequence of their introduction in the text (e.g., Table 1 and Table 3), making it hard to follow.

[1] Xu J, Liu X, Wu Y, et al. Imagereward: Learning and evaluating human preferences for text-to-image generation. NeurIPS 2023.

[2] Kirstain Y, Polyak A, Singer U, et al. Pick-a-pic: An open dataset of user preferences for text-to-image generation. NeurIPS 2023.

Limitations:
The authors have discussed both limitations and potential negative social impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents an extensive study of the evolution of internal activations in diffusion U-Nets and uses their findings to motivate a training-free approach for accelerating sampling from diffusion models. The method is demonstrated to successfully speed up inference in a variety of settings, including different architectures (including non-hierarchical DiTs), different samplers, and some modified inference processes.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method is demonstrated to work well on a wide range of networks & tasks, including U-Net-based LDMs (SD), Imagen-style cascaded pixel-space U-Net diffusion models (IF), and DiTs. Especially DiT is notable, as previous methods do not work on homogeneous transformer architectures out-of-the-box.
- The extensive appendix provides a lot of useful additional information and experimental results and supplements the empirical study about the evolution of internal features in diffusion models well.
- The proposed method opens up avenues for further accelerating sampling speed as measured in wall clock delay for single samples by utilizing multiple GPUs to distribute the proposed parallel encoder propagation strategy.
- The proposed ""prior noise injection method"" helps alleviate artifacts incurred from the optimized sampling process.

Weaknesses:
- No details are provided about the user study, and false claims relating to it are made in the checklist [questions 14 and 15].
- The main method seems to only be a minor incremental evolution of DeepCache's methodology (encoder features are cached and reused without adaptation of features or network in following steps, optionally multiple times, with the main difference being what is defined as the encoder), and seems to provide a *reduced* speedup when compared in a fair single-GPU setting [Tab. 1, Sec. 4.1 l. 271: 41% speedup vs. 56% speedup, at comparable quality levels].
- The presentation of the paper needs some work. Font sizes in some figures are excessively small [cf. Figs. 1, 2, 3, 7], Tabs. 2 and 3 are excessively compressed up to a point at which readability is impaired. The paper would also benefit from a thorough re-read and corrections for the camera-ready version. Especially grammar errors that substantially affect the claims made in the paper (e.g., sampling time reduction ""to 41%"" [l. 271] instead of ""by 41%"") and typos that change the meaning of words (e.g., ""rudely"" [l.105]), but also names of methods (e.g., ""DeepFolyd-IF"" [l. 60], ""Stable Diffuison"" [l. 525]).

Limitations:
The authors have adequately addressed the limitations of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This method can be applied at inference time without requiring retraining to improve sampling efficiency while maintaining high image quality and can be combined with other approaches to speed up diffusion models. The approach is effective across various conditional diffusion tasks, including text-to-video generation (e.g., Text2Video-zero, VideoFusion), personalized image generation (e.g., Dreambooth), and reference-guided image generation (e.g., ControlNet). Using encoder features from previous time steps as input for decoding multiple later time steps allows for concurrent decoding, further accelerating SD sampling by 41%, DeepFloyd-IF sampling by 24%, and DiT sampling by 34%. Besides, a prior noise injection strategy is introduced to preserve image quality, maintaining texture details in the generated images.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Novelty: The three techniques to design efficient diffusion models are novel.

Significance: The problem of efficient diffusion transformer inference acceleration is significant in diffusion models.

Methodology: The proposed algorithm is well-formulated and well-explained. 

Results: The experimental results show significant improvements over existing methods over SD, DiT, and DeepFloyd-IF and are widely applied to text-to-image, text-to-video generation, and personalization.

Weaknesses:
The visualization results for smaller steps appear suboptimal, potentially due to the absence of consistency models or adversarial distillation models for verification. This raises concerns about the foundational assumption of maintaining high similarity throughout the process. It is crucial to explore whether incorporating consistency models or adversarial distillation methods could enhance the quality of results and ensure the assumption of high similarity holds even in smaller steps. Providing more robust verification methods would strengthen the validity and reliability of the visual outputs generated by the proposed approach.

Limitations:
Please refer to the weakness and questions part.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
mTAbl8kUzq;"REVIEW 
Summary:
The authors propose LiteVAE, a novel architecture for the VAE decoding step of latent diffusion. They show that LiteVAE can achieve comparative perfomance to SD-VAE, the default latent diffusion decoder, while using fewer parameters. The efficiency gain comes from using a more lightweight network, and a wavelet feature representation. The paper provides evaluation showing that LiteVAE outperforms the naïve approach of simply scaling down SD-VAE. This gives evidence that LiteVAE offers an architectural improvement.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper provides an extensive experimental investigation of how to improve the efficiency of the VAE upscaling step, and is clearly presented. To me the contribution is significant, due to the dominance of diffusion modelling for image generation. A performance gain in even one step can improve the efficiency many real world uses.

2. The use of wavelets is relatively underexplored in the literature. Using them to improve perceptual quality in generated images is novel to the best of my knowledge.

3. All claims are supported by experimentation and ablation studies.

Weaknesses:
1. Main concern is with the impact of this work. The motivation does not state how much more efficient their improvements would make the full diffusion pipeline. Is the VAE step such a significant performance bottleneck?

2. No code available for a paper whose main contribution is experimental. I would have liked to see how the wavelet features are handled, as Pytorch does not provide official modules that deal with wavelet decompositions. I am a bit concerned about how easy this model is to build and deploy.

3. I have some concerns about whether LiteVAE can offer a performance boost in domains other than natural images. If this is the case, maybe it should be mentioned as a limitation. My concern stems from the wavelet decomposition allowing to explicitly target high frequency content reconstruction, which is especially important for natural image upscaling.

4. Table 4 (difference between group norm and SMC) should ideally have error bars, as it claims an improvement in favour of SMC against group norm.

Limitations:
The limitations are not discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper provides a LiteVAE structure to replace the original one, aiming to reduce the computations when training on large-scale datasets, which can boost the performance of latent diffusion models with more potential augmentations during training.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The new encoder could reduce more parameters than the original one with less GPU memory.

2. The paper provides some interesting analysis of the feature map.

3. The first half of the paper is easy to read.

Weaknesses:
* There is less information about the decoder. If without the lite version of the decoder, the innovation of the work may be limited because the encoder would not be used during inference. What is the structure of the decoder? Will the decoder be trained with an encoder? Did the authors implement a new lite decoder? 

* Have the authors verify their structure in some known Latent diffusion models with fine-tuning strategies. Without such validation, this work may not contribute effectively to modern latent diffusion models. (There is no need to provide new experiments to verify this.)

* Did the paper have the ablation study on the structure modules in Fig.1?

* There is less visual comparison between LiteVAE and VAE to verify its effectiveness. (Fig.6 and Fig.9 didn't show the results of VAE)

Limitations:
The limitations of their work and any potential negative societal impacts should be strengthened.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents LiteVAE. LiteVAE is an efficient and lightweight modification to latent diffusion models (LDMs) that incorporate 2D wavelet transform into the encoding structure. It then uses a feature aggregating model (UNet-based architecture) to fuse multiscale wavelet coefficients into a unified latent code. It uses a decoder to transform the latent code into an image. This work also provides other modifications such as self-modulated convolution, pixel-wise discrimination, removing adaptive weight for adversarial loss, and additional loss functions. These modifications further enhance the training dynamics and reconstruction quality of LiteVAE.

These modifications lead to a considerable reduction in the computational cost compared to the standard VAE encoder without sacrificing the quality of image construction. Bse LiteVAE with a six-fold reduction in encoder parameter count matches the reconstruction quality of standard VAE encoders. Large LiteVAE models provide better reconstruction quality than the standard VAE.

Finally, this paper presents experimental results to support its claims. The result shows that large LiteVAE models outperform standard VAEs of similar size based on the following performance metrics: rFID, LPIPS, PSNR, and SSIM.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
ORIGINALITY:
The main focus of this work is to use the traditional signal processing method (2D wavelet transform) to improve the performance and reduce the computational cost of deep learning methods. It explores the fact that the latent code of Stable Diffusion VAE (SD-VAE) is itself image-like. So they opted for a traditional signal processing method or transformation that preserves the image-like structure of the latent code for SD-VAE. Although there are prior works that use 2D wavelet transform to improve the performance of generative models. This work incorporates 2D wavelet transform and adds some other useful modifications. 


PRESENTATION:
This paper is well organized and provides sufficient background information to understand the central claim of this work. For example, it section provides relevant background information to understand the main components of the new LiteVAE. 
The inclusion of relevant figures further improves the readability of this work.


QUALITY:
This is a high-quality. It clearly explains the motivation for this work and provides a detailed explanation and justification for the modifications presented in this work. It also provides a detailed explanation of how the modifications improve efficiency and scalability. Finally, it provides sufficient experiments to support the main claims of this work.
 

SIGNIFICANCE:
The reduction in computational cost with no tradeoff in reconstruction quality points to the significance of this work.

Weaknesses:
Table 3 only compares the throughput of VAE and LiteVAE-B. Please provide the throughput for LiteVAE-S and LiteVAE-L.

Also, the model did not discuss the additional computational cost for the 2D wavelet transform. Please provide some information about this.

There is a typographical error in line 207. I think ""Table 3"" should be ""Table 2"".

Limitations:
The checklist points to Section 7 for the limitations but the section only shows the conclusion. Please address this.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces LiteVAE, a novel approach that combines multi-scale VAE and discrete wavelet transform to reduce computational cost and enhance reconstruction capabilities. Both components are well-grounded and supported by experimental results. Additionally, the paper provides a detailed pipeline and ablation studies on training VAE for diffusion models, which will also benefit readers.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper integrates multi-scale VAE and discrete wavelet transform to reduce computational cost and boost reconstruction performance.
2. The paper offers a detailed training pipeline for VAE in diffusion models, along with ablation studies that readers will find beneficial.
3. The paper presents several interesting tricks for improving VAE training, including (1) removing group normalization from the decoder, (2) using a U-Net-based discriminator, and (3) eliminating the adaptive weight $\lambda_{reg}$.

Weaknesses:
1. The multi-scale VAE and discrete wavelet transform are two relatively independent improvements for VAE, so I recommend that the authors conduct an ablation study on these two components.
2. The new VAE training pipeline is a valuable resource for the community, so I suggest that the authors release not only the VAE checkpoint but also the entire training code.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
G0v0TxX01N;"REVIEW 
Summary:
The paper introduces Diffusion of Thought （DoT）, integrating diffusion models with Chain-of-Thought. The paper proposes two training-time sampling strategies to enhance self-correction during inference. Experimental results demonstrate the effectiveness of DoT in simple and complex reasoning tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents an initial exploration into the reasoning ability of current diffusion language models.
- In simple reasoning tasks, DoT achieves up to 27× speed-up without performance drop compared to CoT and implicit CoT.
- DoT showcases promising self-correction abilities in complex reasoning problems.

Weaknesses:
- The coupled sampling strategy, designed to rectify errors in previous thoughts, appears to assume that the noise added to the rationale $r_{i-k}, \cdots, r_{i-1}$ is the same as the potential errors in previous rationales during inference. This assumption is not intuitively obvious and lacks a clear explanation.
- Despite building upon the DiffuSeq framework, the paper does not include comparisons with the DiffuSeq model, which has the most similar model backbone.
- It would be better if the paper could provide a qualitative comparison of the reasoning paths between DoT and DoT$^{MP}$

Limitations:
The paper has adequately stated the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces ""Diffusion of Thought"" (DoT) to diffusion language models to improve upon their reasoning capabilities. 

The method adapts the implicit chain of thought methodology (iCoT) for autoregressive models, which relies on per-task fine-tuning to distill reasoning into transformer layers, while DoT encodes it into diffusion steps. 

The methodology includes a comparison between a single-pass and a multi-pass approach. The single-pass averages all rationales across all timesteps, while the multi-pass introduces a causal inductive bias between rationales by averaging each reasoning step at a time across all timesteps.

Similar to the iCoT paper, evaluations are conducted on tasks lmultiplication, boolean logic, and the grade school math (GSM8K) tasks.

The approach leverages self-correction, self-consistency, and the number of reasoning steps (T) to further improve accuracy, trading off some efficiency.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The fundamental idea of encoding reasoning rationales into diffusion steps seems an intuitive path to explore.
-  Due to the flexible timestep parameter (T), DoT offers greater flexibility compared to Implicit Chain of Thought (iCoT), which is limited by the number of transformer layers.

Weaknesses:
- **Direct Comparison Baseline** The paper lacks a direct comparison with answer-only and  traditional CoT technique applied to diffusion language models, which would provide a clearer benchmark for evaluating the effectiveness of DoT. The paper only provides a comparison with auto-regressive answer-only, CoT and iCoT results. This does not convincingly demonstrate that the additional complexity introduced is justified by performance improvements. 
  - The paper does not adequately separate between the specific contributions of the DoT methodology from the inherent advantages of using diffusion language models. 

- **Missing iCoT context** Section 3 does not clearly explain how DoT builds on the implicit Chain-of-Thought (iCoT) approach, especially regarding the training operations. Instead, it focuses mainly on additional complexities and mechanisms introduced to improve overall results. A detailed connection between iCoT and DoT is needed to better understand the modifications and their impact. The terms 'single-pass' and 'multi-pass' could be misleading as they typically imply batch processing. Here, they refer to how probabilities of different reasoning paths are handled, in parallel or sequentially.

- **Task-Specific Fine-Tuning Requirement**  DoT performs well on simple tasks like multiplication but requires fine-tuning with a larger number of reasoning steps t for grade school math. This contrasts with CoT methods in autoregressive models, which can adapt more flexibly using examples directly in the input. 

- **Throughput Comparison** The absence of a direct throughput comparison for fixed **T** across evaluation settings limits understanding of T's impact on performance and efficiency. The results in Table 1 summarized results for dynamic sampling timesteps T.

Limitations:
yes, the authors acknowledge the reliance on specialized training per reasoning task and the limited generalization capabilities. 
The need for more reasoning steps as tasks become more complex could be further elaborated upon.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a chain-of-thought technique for diffusion language models. They achieve this by diffusing a set of hidden representations (thoughts) through time. Different sampling techniques are introduced to enhance error recovery including looking forward and conditioning on multiple previous thought steps in predicting the current thought. They achieve competitive results in terms of throughput compared to chain-of-thought paradigms applied to small language models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The authors extend the chain-of-thought paradigm to language diffusion models, which is novel and significant.
- Their results seem to suggest that this is a promising direction.

Weaknesses:
- The presentation can be enhanced:
  - The transparent figure colors are very hard to read.
  - The figures do not render correctly on different PDF viewers.
- Comparison to larger open language models (e.g., Lama) would improve this contribution's placement in the literature.

Limitations:
The authors discussed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work introduces Diffusion-of-Thought (DoT), a method that combines diffusion language models with the Chain-of-Thought technique to enhance their reasoning ability. DoT uses the flexibility of diffusion processes to allow reasoning steps to diffuse over time, improving performance in several mathematical tasks, and demonstrating its self-correction abilities. The experimental results show DoT's effectiveness in many tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The work deals with an important problem in ML, verifying reasoning ability on a recently arisen diffusion language model.
- The proposed method is technically sound.
- The experiments show DoT's empirical effectiveness on many math benchmarks.

Weaknesses:
- Some of the recent work is not discussed [1]
- No standard deviation or confidence interval in the results.

---

[1] Can mamba learn how to learn? a comparative study on in-context learning tasks, ICML 2024

Limitations:
- Limited ablation study
- General performance improvement beyond mathematical tasks are not discussed

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
* The authors propose DoT, a chain of thought method for diffusion language models.
* DoT is applicable to both continuous embedding-based diffusion models and continuous-time Markov chain discrete diffusion models.
* DoT shows performance increase on digit multiplication, boolean logic, and GSM8K tasks, as well as tradeoffs in reasonability and efficiency.
* Overall, this is a relevant work in the growing field of diffusion language modeling that applies CoT reasoning from the AR literature.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* DoT is applied to both discrete and continuous diffusion language models. Given that there are various formulations of diffusion language models (embedding diffusion, simplex diffusion, masking state / absorption, continuous-time Markov chain), this is a plus.
* The authors also demonstrate DoT both by pretraining small models (standard 12-layer transformer with 6 encoder and decoder layers, respectively) from scratch, as well as leveraging pretrained diffusion models (Plaid, SEDD).
* DoT shows strong performance across multiplication, boolean, and GSM8K datasets, outperforming GPT-2 baselines.
* Empirically, the authors demonstrate that it is possible for diffusion models to have flexibly thought processes, where the model builds off of intermediate thoughts to arrive at the correct answer (similar to AR CoT) or jump to an answer, then correct its intermediate steps.

Weaknesses:
* The dataset explored in this work seems rather simple. Although the work understandably builds on top of previous work that employs the same dataset, the fact that baseline models achieve 100% or close to 100% makes it difficult to lucidly compare the baseline with the proposed approach. This applies to both multiplication setups as well as boolean logic, where GPT-2 models already reach 100% even without CoT.
* The authors use throughput as the basis for why DoT is superior to AR CoT when both methods achieve 100%. This is a slightly weaker argument because throughput for diffusion models critically depends on a number of hand-crafted parameters, such as the number of backward steps and the model context length. These parameters are orthogonal to DoT. It is possible that the particular setup of this work was favorable to diffusion, but not necessarily so in the general case. Moreover, AR models can leverage key-value caching to speed up generation, whereas diffusion models cannot. I am not sure if I am entirely convinced that DoT would generally be faster than CoT in the wild.
* Although CoT with diffusion models is a new area, the methodology itself is not entirely novel, as it appears to be an adaptation of DiffuSeq-style masking applied to CoT training data (i.e., give the model a question as its prefix context, and diffuse over the answer + CoT intermediate steps).

Limitations:
* The authors note that fully pretrained diffusion models are sparse, and that most diffusion language models remain at the small parameter regime (GPT-2). 
* Limitations are adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
5K3VeoBnqc;"REVIEW 
Summary:
The paper aims to address the adaptable error detection problem, i.e., detecting the error behavior of a robot policy in an unseen environment. Solving the problem ensures that the robot is stopped before performing behavior that causes disruptions to the surroundings.

The AED problem introduces three challenges: the abnormal behavior in the unseen environment is not observed in the training data and common anomaly detection method cannot be used; with complex background, there is no noticeable change between frames and this makes it difficult to indicate when the error occurs; AED is required to terminate the policy timely once the error occurs. The three challenges make it difficult to apply the current approaches on anormaly detection.

The paper creates a new benchmark specially designed for adaptable error detection, which consists of 322 base and 153 new environments encompassing six indoor tasks and one factory task.

The paper proposes a PrObe architecture consists of a pattern extractor and a flow generation equipped with a pattern loss, a temporal contrastive loss and a classification loss to predict the error probability.

The paper conducts experiments on the proposed benchmark and achieves the top 1 counts, average ranking, and average performance difference. Extensive ablative study justifies the effectiveness of the design choices.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Few-shot imitation learning learns a robot policy for a new environment with only a few demonstrations, which could accelerates the development of robot applications. However, with only a few demonstrations, there exist many corner cases that the robot could not observe in the demonstrations and the appearance and background of the scene are also different. This makes the policy perform error bahavior, which makes the surroundings fall into a dangerous situation. Thus, the problem of adaptable error detection is important to solve.

The three loss designs not only supervisedly learn the error prediction but also unsupervisedly regulate the intermediate feature to follow some human priors including close features for frames with high task relation and sparse feature to detect frame changes.

The experiment datasets and tasks are specially designed for the AED problems and selected baselines are strong error detection baselines.

Weaknesses:
One main contribution of the paper is that error detection could avoid the robots making disrupted behavior in the real-world environment. Thus, the authors may need a real-world environment that contains some disruption scenarios and test the method in such environments. Even the most realistic simulation may not simulate some real factors such as inaccurate control program of the robot arm.

Limitations:
None

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces Adaptable Error Detection (**AED**) within Few-Shot Imitation (**FSI**) tasks, a critical yet underexplored area in robotics and AI. The authors establish a novel benchmark for assessing AED methods and present **PrObe**, designed specifically for this task. Through comprehensive evaluations, the paper demonstrates that PrObe outperforms baseline models and shows robustness across different FSI policies.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
**S1. Well-written.** The paper has a coherent narrative and logical flow. The problem scenario is effectively motivated with real-life examples that illustrate the practical importance and applicability of the research. Relevant literature is extensively discussed and consistently referenced throughout the paper.

**S2. Illustrations.** The schematic illustrations provided in the paper are well made and enhance the comprehensibility of AED, PrObe, and their experimental analysis. The figures effectively delineate the structured protocol of the AED task across different phases, and the intricate architecture of PrObe, showcasing its components and their functions.

**S3. Unique challenges.** The unique challenges established by AED distinguish it from FSAD and other tasks in prior works. AED's focus on online anomaly detection in novel environments, where behavioral errors occur without clear visual cues, establishes it as a critical area of research.

**S4. Novel components.** PrObe establishes the usefulness of its design principles by outperforming other baselines. Augmenting rollouts, extracting patterns from policy embeddings, generating a pattern flow and fusing it with task-embeddings, and utilizing a temporal-aware triplet loss - all contribute to its effectiveness.

**S5. Comprehensive analysis.** The paper extensively explores PrObe's application in FSI policies for AED. PrObe achieves better visual separability of features in the latent space between successful and erroneous trajectories, thereby enhancing the interpretability of learned embeddings. Notably, PrObe excels in temporally precise error detection, outperforming other methods that detect errors too late or too early. An in-depth ablation study confirms the stability and essential contributions of PrObe’s components. Additionally, the paper explores PrObe’s failure scenarios, the impact of demonstration quality, and viewpoint changes.

Weaknesses:
W1. While the textual descriptions in the Preliminaries section provide a solid understanding of the DC policy, incorporating mathematical notation would significantly enhance clarity and comprehension. Specifically, the paper could benefit from the following mathematical formulations:  
 1. The contents of history $h$, detailing what it encapsulates
 2. The feature encoder and task-embedding network computing the history, demonstration, and task-embedding features
 3. Task-embedding network padding the sequences
 4. The actor policy, e.g, $\pi(a|o_t, f_h, f_{\zeta})$
 5. The inverse dynamics module
 6. NLL and MSE objective in this particular setting

W2. The average difference metric presented in Figure 4, which compares each method's performance to the second-best performing method, may be inadequate. This approach is only sensible when comparing two methods. A more informative assessment might measure performance gains relative to the worst-performing method, or performance losses relative to the best-performing method. Alternatively, using a standardized baseline or oracle for comparisons could provide a clearer and more meaningful evaluation.

W3. PrObe is only shown to be effective for evaluating policies trained on image data. As a primary example, the authors motivate the FSI ED problem setting on robotic tasks. Although sensor data cannot always fully capture the properties and locations of separate objects, policies for robotic manipulation tasks are often trained on proprioceptive data—such as joint angles and torques—to achieve better precision. Image data, though rich in environmental context, can sometimes be unavailable or impractical due to factors like occlusions, lighting conditions, and higher training costs. The inherent differences between image-based and proprioceptive data mean that the latent space characteristics and the patterns critical for error detection would vary. Image-based models capture visual features like shapes and spatial relationships, whereas proprioceptive-focused models emphasize dynamics and kinematic features, including the robot’s mechanical interactions. While there's no inherent reason to doubt PrObe’s capability with different data modalities, the effectiveness of its pattern extractor when applied to latent features learned from proprioceptive data remains uncertain without empirical validation. Conducting experiments with proprioceptive sensor data and demonstrating successful results would further validate the robustness, generality, and applicability of PrObe.

### Minor Improvements

1. The second and third contributions—introducing a novel benchmark and PrObe—are blended together; these could be separate for clarity.
2. Please add a reference in the main paper that the limitations are in Appendix F.
3. Line 125 “semantically” refers to language or linguistics, which doesn’t seem to be the intended use of the word here
4. Line 133 “learns implicit mapping” → “learns an implicit mapping”
5. Line 136 “current history” → “the current history”
6. Line 154 “and few” → “and a few”
7. Line 168 “high controllable” → “highly controllable”
8. Line 174 “and few” → “and a few”
9. Line 192 “from agent’s” → “from the agent’s”
10. Line 205 “take as input the history features $f^h$” →“takes the history features $f^h$ from … as input”
11. Footnote on page 4 “thet” →”they” 
12. Line 614 “are are” → “are”
13. Line 661 “*Pick & Plcae*” → “*Pick & Place*”

Limitations:
The authors have extensively discussed the limitations of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed a task Adaptable Error Detection (AED) that attempts to perform online behavior error detection for FSI policies, it advocate three main challenges comparing to FSAD: novel environment, no notable change for behavior error that AED tries to detect, and it has to be conduct simultaneously with policy execution to ensure timely error detection and damage control. The AED benchmark is provided, and a AED method, PrObe is proposed to solve AED through error prediction based on fusion of task embedding and flow pattern generated from history of rollout. The PrObe achieve good performance compare to baselines, and paper provided comprehensive studies analyzing PrObe error detection timing, it's embedding pattern, the importance of components, the stability of error detection, and provided pilot error correction to demonstrate practicality of the AED task.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. AED is a important task for FSI research.
2. The proposed PrObe is very effective in AED task for various tasks.
3. abundant analysis is provided on PrObe timing, embedding analysis, performance stability, ablation and more.

Weaknesses:
The writing has improvement room:
1. line 161 - line 162: I am confused: if frame level label is not available, how come the measurement is averaging over actions?
2. The function of demonstration data is not clearly described in section 5.2, more explanation is helpful.

The AED benchmark is not well described in the writing.

Limitations:
The limitation in terms of proposed PrObe is discussed through analysis experiments, which make sense to me.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
fMWrTAe5Iy;"REVIEW 
Summary:
The paper introduces R2-Gaussian, a framework for tomographic reconstruction using 3D Gaussian splatting (3DGS). This framework aims to address the limitations of traditional 3DGS in volumetric reconstruction, specifically for tasks like X-ray computed tomography (CT).

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Identifying and addressing the integration bias in standard 3DGS formulation for volumetric reconstruction
- The proposed R2-Gaussian framework is developed with tailored Gaussian kernels, rectified projection techniques, and a CUDA-based differentiable voxelizer.
- The paper provides simulated X-ray validation, comparing the proposed method against state-of-the-art techniques

Weaknesses:
- The proposed method shows results in experimental settings. However, its performance in real-world clinical or industrial scenarios is not thoroughly examined. For instance, real X-ray images are given to reconstruct the CT scan. 

- I am not sure if 75, 50, and 25 views of X-rays are considered sparse-view reconstruction and if it is practical to have 75, 50, or 25 views of X-rays for CT reconstruction. 

- In the proposed method, the kernel formulation removes view-dependent color. However, it cannot model the scattering effect in X-ray.

- In Fig. 7, it is unclear how to get X-3DGS slices and what ""queried from three views"" means in detail. Why not implement the voxelization on X-3DGS (or vanilla 3DGS) for a fair comparison?

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to achieve high tomographic reconstruction performance with a limited number of views in a time-efficient manner.
To this end, the paper modifies 3DGS for X-ray projection by adjusting the rendering equation, correcting 2D projection errors, and using voxelizers for regularization.
The experiments demonstrate the efficacy of the proposed method.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
First of all, the paper is well written.
The problem in the current tomographic representation is clearly stated, and the authors' objectives are sufficiently addressed.
The overall structure is easy to understand, and the ablation study covers most of the arising questions.

Weaknesses:
One critical weakness of this paper is the existence of prior work using 3DGS for X-ray projection.
Although the paper seems structurally well-written, the paper seems less novel due to the presence of X-Gaussian.
X-Gaussian, which has been accepted to ECCV 2024, employs a similar method.
More importantly, X-Gaussian achieved a PSNR of 43 in Human Organ reconstructions, whereas this paper (R2 Gaussian) achieved a PSNR of 36, despite slightly different experimental settings.
If the authors provide persuasive explanation on the novelty of this paper, I am willing to raise my score.

Additionally, it would be helpful to augment the related works and baseline models.
For example, C^2RV (CVPR 2024) is another recent tomographic representation model.

Limitations:
The paper clearly states the limitations and the potential societal impact of the work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
### Motivation
- The authors propose to adapt 3D Gaussian Splatting (3DGS) to sparse-view tomographic reconstruction, i.e., to recover a radiodensity 3D volume from a small set of X-ray images and corresponding sensor information. This is relevant for various clinical and industrial applications.

### Contributions
- Their _R2-Gaussian_ model iterates over existing 3DGS solutions tuned for XR/CT imaging, proposing a 3DGS initialization scheme better suited for tomographic reconstruction.
- The authors also correct an integration bias in 3DGS (meant for faster image inference but causing ambiguities in volumetric reconstruction) and provide the corresponding CUDA patch.
- The proposed _R2-Gaussian_ includes other adaptations (custom densification parameters, voxel-based regularization, simplified isotropic kernels), resulting in an end-to-end CT reconstruction system.

### Results
- The authors provide extensive qualitative evaluation and quantitatively compare to other NeRF-based and traditional CT reconstruction methods, showing that their method provides a better trade-off between volume accuracy and reconstruction time.

### Relevance
- Effort in applying 3DGS to XR/CT imaging has grown the past year [6, 27, 39], as 3DGS appears to be a well-suited representation for such applications (due to its compactness, fast convergence, etc.). This work is a meaningful iteration and could benefit the community.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
_(somewhat ordered from most to least important)_

### S1. Convincing Comparative Evaluation and Qualitative Results

- The quantitative comparison to state-of-the-art CT reconstruction methods appear convincing, with the proposed solution demonstrating a better trade-off between volume accuracy and reconstruction time.
- The authors provide a lot of meaningful qualitative results, to illustrate theoretical contributions, to highlight the benefits of their method, but also to showcase its limitations. This makes reading this paper the more interesting.
- An ablation study w.r.t. some of the key contributions and w.r.t. some hyperparameters is also provided.
- Though limited in number (15 volumes), the authors evaluate on different categories (animal, vegatal, and synthetic targets).


### S2. Iterative yet Meaningful Contributions Towards 3DGS for XR/CT

- The discussion w.r.t. the integration bias in vanilla 3DGS is interesting, and the technical solution brought by the authors appear valuable to the community. As mentioned in the paper, their corrected CUDA implementation could benefit other 3DGS works targeting volumetric reconstruction.
- The authors propose an initialization scheme dedicated to volumetric tomography, as usual 3DGS initialization techniques (e.g., SfM) are not applicable here. This is a relevant contribution, properly described and evaluated (qualitative + quantitative evaluation).
- The proposed system, tackling xrays-to-CT reconstruction in an end-to-end differentiable manner, is indeed novel. Existing 3DGS models [6, 27, 39] for XR/CT imaging rather focus on digitally-reconstructed-radiograph (DRR) novel-view synthesis (NVS) rather than CT reconstruction.


### S3. Sound Theory and Reproducibility

- Background theory is well described by the authors, and the scientific/technical insight of the authors w.r.t. the identified integration bias could benefit the community.
- The authors provide their model implementation, which appears sound and well-structured (note that I did not try to run the code, but had a look at key files).


### S4. Detailed Discussion of Limitations and Contributions

- The authors put significant effort in discussing and illustrating some of their method's limitations (needle-like artifacts inherent to 3DGS, varying convergence time, limited extrapolation ability, etc.), as well as summarizing its impact (possible clinical/industrial applications, benefit of their CUDA code to the CV community, etc.) in Appendices G and H.


### S5. Well-Written and Illustrated Paper

- Overall, the paper is nicely structured, written, and illustrated. E.g., Figures 2-4 are helpful to understand the methodology at a glance.

Weaknesses:
_(somewhat ordered from most to least important)_


### W1. Lack of Consideration for Real-World Noise and Anisotropic Effects

- The authors claim that ""X-ray attenuation depends only on isotropic density"" [L140] to justify their radiodensity-based model, but this is not entirely correct. While most models generating digitally reconstructed radiographs (DRRs) from CT volumes indeed consider x-ray attenuation as an isotropic phenomenon, this is a approximation. Some x-ray transport effects, such as Compton scattering, are actually anisotropic (but because CT volumes do not inherently contain the material information necessary to accurately simulate Compton scattering, DRR models ignore the anisotropy part) [a]. 
However, the authors do claim that they trained their model on DRRs generated using TIGRE [5] configured to simulate Compton scattering [L217-224]. Does it mean that the authors preprocessed the CT volumes to replace the attenuation values by material information (e.g., mapping HU values to a set of predefined materials)? More information on the data generation process would be helpful here.
If indeed the model was trained on DRRs containing anisotropic residual noise (c.f. Compton effect's residual impact on XR imaging), but the proposed algorithm itself only consider isotropic attenuation, how does it impact the results? E.g., it could be interesting to generate 2 sets of input DRRs (one generated with the approximated/simplified attenuation model and one more realistic) and compare the final accuracy of the reconstructed CT volumes.
- The fact that the method is only applied to synthetic inputs (DRRs generated by TIGRE) rather than real, usually noisier, X-ray images is also problematic. The paper would benefit from a real-world evaluation, or at least a discussion on why it was not performed.


### W2. Lack of References/Comparisons to SOTA on XR-3DGS

- The authors mention some of the existing 3DGS solutions applied to CT/XR imaging (e.g., X-Gaussian [6], GaSpCT [39], Li et al.'s model [27]) [L89-94] but do not perform any form of comparison with those. The lack of qualitative/quantitative comparison is fair (the authors argue that these models ""cannot generate CT models"" [L91-92], which is somewhat true ; though a comparison on the XR-NVS task could have made this paper stronger). But I would argue that the authors should have better referenced some of these works in the Methodology. E.g., even if less formalized, radiative Gaussians are already presented in X-Gaussian [6] ; and even though it is performed in the pixel domain rather than voxel one, GaSpCT [39] already proposes a total variation loss to regularize their XR-3DGS.
While I still believe that the proposed work is a valuable iteration over these works (by better formalizing and adapting the Gaussian properties and rasterization to CT data), I think the authors should be more transparent w.r.t. the SOTA.
- References w.r.t. total variation (TV) theory are also missing, making it hard to contextualize the scope of the authors' contribution w.r.t. the objective function.


### W3. Somewhat Overstated Contributions

- The CUDA implementation of the radiodensity voxelization seems to be more a technical feat rather than a scientific contribution. While possibly valuable to the community, I do not see any novelty in this module (maybe not as GPU-optimized, but differentiable point-cloud-to-voxel-grid tools already exist, e.g., in PyToch3D).
- Changes to the adaptive control are unclear/minor, according to the authors' descriptions [L210-214] (i.e., changing the size threshold w.r.t. pruning large Gaussians, editing the density of cloned/split Gaussians).
- The positive impact of the TV regularization does not appear that statistically significant (+0.32dB for PSNR, +0.009 for SSIM, +1m33s for convergence). Maybe some qualitative results could help grasp its contribution?


### W4. Limited Dataset Size
- The quantitative evaluation is performed on only 15 samples, even though varied. Larger CT datasets are available, e.g. CTPelvic1K [b]. 

### W5. Limited Impact of Integration Bias Correction (?)
- The authors claim that ""this integration bias, though having a negligible impact on imaging rendering, leads to significant inconsistency in density retrieval"" [L184-186], but they only provide qualitative imaging results (Fig. 6) to justify their un-biasing contribution. Additional results could better contextualize the corresponding claims.


----

### Additional Reference:

[a] Gao, Zhongpai, et al. ""DDGS-CT: Direction-Disentangled Gaussian Splatting for Realistic Volume Rendering."" arXiv preprint arXiv:2406.02518 (2024).

[b] Liu, Pengbo, et al. ""Deep learning to segment pelvic bones: large-scale CT datasets and baseline models."" International Journal of Computer Assisted Radiology and Surgery 16 (2021): 749-756.

Limitations:
Some limitations and societal impacts are discussed in detail (see **S4** above).

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a 3D reconstruction method for sparse-view computed tomography using 3D Gaussian Splatting. The core contribution is the reformulation of the volumetric rendering equation to include view-independent central density estimation. Additionally, the paper introduces a differentiable voxelizer that converts a set of 3D Gaussians into a voxel grid of densities, proving effective in computed tomography tasks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- The paper reveals the view-dependent integration bias in 3D Gaussian Splatting (3DGS), which, to my knowledge, has not previously been reported in the community. This discovery may have a high impact to computer vision. 
- While this is not the first paper to apply 3DGS to computed tomography, it is the first to accurately reconstruct 3D volumes with an image formation tailored specifically for this task. 
- The proposed method is well evaluated and compared against other baseline methods. 
- Exposition is clear. The paper really reads well.

Weaknesses:
I don’t see any particular weakness of the paper.

Limitations:
The only limitation I can think of is the scope of this work; computed tomography represents a relatively niche area in the fields of machine learning and computer vision.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
GxvDsFArxY;"REVIEW 
Summary:
The authors introduce PhyloGen, a new method that uses a pre-trained genomic language model to generate phylogenetic trees without relying on evolutionary models or aligned sequences. PhyloGen treats phylogenetic inference as a conditionally constrained tree structure generation problem, jointly optimizing tree topology and branch lengths through three modules: Feature Extraction, PhyloTree Construction, and PhyloTree Structure Modeling. A Scoring Function guides the model towards stable gradient descent. The method achieves state-of-the-art performance when evaluated on benchmark datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The framework is novel in that it leverages a pre-trained genomic language model  instead of relying on aligned sequences.
2. The method achieves state-of-the-art performance evaluated on real-world datasets.

Weaknesses:
1. Ablation Experiments Clarity

The ablation experiments are unclear. My main concern is the lack of clarity regarding why the proposed method significantly outperforms other methods. Specifically:

(1) Table 5: This table shows the average metrics across the eight datasets. However, Table 1 and Table 2 only shows the metrics for each dataset individually without any average metrics. It’s unclear what the performance gap is between the ablation model and the baseline methods when KL or S are removed.

(2) Figure 6: The ablation studies in this figure only show the metrics for the DS1 dataset. Please include the average metrics across all eight datasets.

To improve the clarity of the ablation experiments, I strongly suggest making the evaluation settings consistent in Table 1, Table 2,Table 5, and Figure 6. At the very least, include the average metrics across the eight datasets. Additionally, provide more explanations based on the complete ablation experiments to clarify why the proposed method significantly outperforms other methods.

2.  Claim on Aligned Sequences vs. Unaligned Sequences

The claim that unaligned sequences better reflect actual sequence variation than aligned sequences is questionable. Generally, aligned sequences with direct site-to-site comparisons are more effective in comparative sequence analysis. The authors need to provide more direct experimental evidence to support the advantage of using unaligned sequences.

3. Writing Issues

There are many writing issues. For instance,

a. line 141. ""m_ij"" -> ""h""

b. In the caption of table1, ""VBPI-GNNuse"" -> ""VBPI-GNN use""

c. line 213, ""3.1 Experiment Setup""  ->  ""4.1 Experiment Setup""

Limitations:
I have no concerns on the potential societal impact of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper propose phylogenetic tree inference by modeling it as a problem of conditional-constrained tree structure generation. Its goal is to jointly generate and optimize the tree topology and branch lengths. By mapping species sequences into a continuous geometric space, PhyloGen performs end-to-end variational inference without limiting the possible topologies. To maintain the topology-invariance of phylogenetic trees, distance constraints are applied in the latent space to preserve translational and rotational invariance. They have shown the proposed model  is  effective  across eight real-world benchmark datasets.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper addresses very interesting problem and propose novel approches.

Weaknesses:
I am not sure if the formulation equation 7, adding the term R mathematically make sense, as orignal ELBO in equation 6, is the lower bound, now adding the term log of R which is negative, and trying to maximize equation 7 would result in minimizing R?  Also in equation 7, q(\tao (z)) seems missing, or is there a kl between R and Q from module B?
I think the paper presentation needs to be improved. The figure 2 is very helpful but the text  and some notations are a bit confusing.

Limitations:
1. While PhyloGen outperforms many existing methods, it still requires substantial computational resources, particularly during the training phase. The complexity of jointly optimizing tree topology and branch lengths, along with maintaining topological invariance, sounds computationally heavy, for instance:
1.1  Monte Carlo simulations to sample from the posterior distributions of tree topologies and branch lengths.
1.2 The tree construction module computes distance matrices from latent variables and uses algorithms like Neighbor-Joining (NJ) to generate initial tree structures. 
1.3 The utilization of  a pre-trained genome language model (DNABERT2) to transform DNA sequences into genomic embeddings, 

2, Looking at the table 5, I am wondering how it works in inference time, would peformance drops when we have less or more specious than the one in the training data the model was trained on? Also how it translates to other specious? moreover, did you train one model per dataset, or is there a way to train all together?

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a new method, PhyloGEN, for phylogenetic inference. The method is able to perform end-to-end variational inference in order to jointly optimize the tree topology and the branch lengths. To achieve this, the authors propose using a pre-trained genomic language model to extract genome embeddings, and the embeddings are then used to form an initial topology which is refined iteratively using the proposed topology and branch length learning modules.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper makes a novel contribution to an important research topic. Unlike previous variational inference methods, the method does not require pre-generated topologies.

- The incorporation of a pre-trained genomic language model to the method is an interesting contribution and is something that will likely be increasingly explored in the future.

- The method is compared against a number of recent phylogenetic inference methods, and it achieves superior performance in terms of MLL and ELBO on standard benchmark datasets compared to the previous methods. In addition, experiments are performed to investigate the robustness of the proposed method and the diversity of the generated topologies, indicating that the method is consistent with the ""gold standard"" MrBayes approach. The authors also provide an ablation study to understand the impact of the various design choices in their method.

Weaknesses:
- The methods section (Section 3) is difficult to read in that variables are introduced without leading sentences, some notation is undefined, and design choices are introduced without clear motivation.
  - What is the motivation for $f_i$ and how is it defined in (2) if there are multiple children $f_j$?
  - The function $h$ is used before its definition.
  - Which function $h$ is the latter $h$ in the definition $h(x_i, x_j) = h(x_i, x_i - x_j, d_{ij}^2)$?
  - Why is max aggregation a good choice?
  - The scoring function $S$ is not described in enough detail. What does it output and how does it provide additional gradient information?

- Most experiments, in particular RQ2, RQ4, RQ5, RQ6, and RQ7 are performed on only one data set (DS1), making it difficult to draw meaningful conclusions. I would also consider the runtime an important metric, but it is measured only in conjunction with the robustness assessment on only one data set.

- The authors do not provide code for their method or experiments (although the code is promised to be released later). Without the code, considering the lack of detail in the methods section, it would be difficult to reproduce the proposed method.

- Minor:
  - The red and yellow colors are opposite to what they should be in the caption of Table 1.
  - There is a subsection numbered 3.1 in Section 4.
  - RQ7 is only referred to in the Appendix, and not in the main text.
  - Typos: ""TreeEnocoder"" (Figure 2 caption), ""Metrtic"" (Figure 3 title)

Limitations:
The authors do not explicitly discuss the limitations of their work. As I am not an expert in phylogenetic inference, it is difficult to judge the assumptions made in the paper. Are there conditions that are required to be met for using a pre-trained language model for extracting embeddings, and is the assumption of the tree topology and branch lengths being conditionally independent reasonable? It would be helpful for the authors to elaborate on what they think the possible limitations of their own work are.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents PhyloGen, a novel approach for phylogenetic tree inference using pre-trained genomic language models and graph structure generation. PhyloGen aims to jointly optimize tree topology and branch lengths without relying on evolutionary models or equal-length sequence constraints. The method demonstrates superior performance and robustness across multiple real-world datasets compared to existing approaches.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Novel approach combining pre-trained genomic language models with graph structure generation for phylogenetic inference

Joint optimization of tree topology and branch lengths without typical constraints

Superior performance on benchmark datasets compared to existing methods

Robust to data changes and noise

Provides deeper insights into phylogenetic relationships

Computationally efficient compared to baselines

Weaknesses:
Limited discussion of potential limitations or failure cases

Lack of comparison to some recent methods in the field

Source code not provided

Limitations:
While some robustness tests were performed (node additions and deletions), the paper doesn't explore all possible types of data noise or perturbations.

The paper doesn't deeply explore the interpretability of the model's decisions or the learned representations.

The paper doesn't extensively discuss how well the method generalizes to different types of genetic data or organisms not represented in the test datasets.

The method relies on pre-trained genomic language models (specifically DNABERT2), but doesn't explore how performance might vary with different pre-trained models.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
nRRJsDahEg;"REVIEW 
Summary:
This work develops a multi-task-masking (MtM) approach, based on a self-supervised Transformer, that masks and reconstructs activity across different dimensions for neural spiking data learning. Evaluated on the International Brain Laboratory dataset, the model improves tasks such as single-neuron and region-level activity prediction, forward prediction, and behavior decoding. This method enhances performance and generalization, advancing towards a comprehensive foundation model for the brain.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The deep thinking about foundation model in neurosciece.

2. The model significantly improves generalization by modeling across different brain areas, individuals, and sessions.

3. The multi-task-masking (MtM) strategy effectively enhances the capabilities of the Transformer model.

Weaknesses:
1. Lack of Qualitative Analysis:

There is a lack of qualitative analysis regarding the neural dynamics obtained from population neural activity across different individuals, such as visualizing features output by NDT. Are there similar patterns? Are there similarities between single-neuron and population-level activities?

2. Insufficient Ablation Studies:

a) What are the differences between training on multi-region data versus single-region data? When training with data from three brain areas, is there a phenomenon where the activity of one brain region dominates?

b) What are the performance differences when using multi-session data from a single individual versus cross-individual multi-session data?

Limitations:
see Weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes a transformer architecture (based on previously existing ones) and, more interestingly, a training procedure that, when applied to spiking neural data, should result in a foundation model for spiking neural data. The clever bit about the procedure is that the learning model is asked to reconstruct differently-masked portions of the data: activity of one held-out neuron, future activity of a neuron, activity of a neuron based on neurons in the same region, activity of a neuron based on activity of neurons in other regions. During training these tasks are presented to the model interchangeably, so it has to learn to do all of them (although with a help of an embedding vector that can decode for the type of the task).

The results show that the proposed model achieves better numbers in a variety of tests, however I found it hard to evaluate the significance of the result using the reported metric of bits per spike and how it translates to real-world performance. Please see my comments below and I hope to learn more about this from authors' rebuttal.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* In my opinion, this work is a step in a very important direction, indeed with the wealth of neural data being collected foundation models for various modalities can become very important tools for modelling brain dynamics

* Clearly and simply written, easy to read and follow

Weaknesses:
* I would like to see more about the architecture of the transformer (even though it is based on existing model), ideally a more detailed visualisation, this would help to imagine how exactly the data is going in, how the masking is applied, etc.

* I am not too happy about the bit-per-spike metric being the only one reported. It is fine to have one number to be able to characterize and compare the performance succinctly, but it's awkward to translate back to ""how well does it work, really?"". I think it would be great if the paper would include actual raster plots of good / average / worst reconstructions of the masked neuron / future / intra-region / inter-region regions -- this would allow a neuroscientist to decide with a quick glance whether ""wow I should start using this"" or ""nah I am gonna wait till NeurIPS 2025""

Limitations:
The authors have adequately highlighted some limitations, but I would like to hear a bit more about the limitations of the reconstructed signal from neuroscientific perspective: are they usable? Do they brake or maintain some important characteristics of spiking signal?

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a self-supervised approach to building a foundation model of single-trial neural population dynamics. The approach utilizes existing transformer architectures, which are trained using “multi-task-masking” (MTM), which alternates between several scales of prediction tasks, including co-smoothing, causal prediction, inter-region prediction, and intra-region prediction. The approach is demonstrated on the IBL repeated site dataset, where activity prediction, forward prediction, and behavior decoding using a fixed architecture are improved using MTM training relative to a baseline where training only utilizes temporal masking.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The work is original and forward thinking. The writing is admirably clear and well referenced.

Weaknesses:
The paper would benefit from:
- Comparing to a wider range of models of neural population dynamics (e.g., LFADS, GPFA, mDLAG, etc). Many previous approaches have reported co-smoothing performance (e.g., Neural Latents Benchmark). How does MTM-NDT1 compare against those previous approaches on co-smoothing? How does the proposed approach compare to multi-area models (e.g., mDLAG) for on the inter-region co-smoothing task?
- More fully describing the details of the “temporal masking” baseline. The details are referenced out to a citation rather than described within the text. Since this baseline appears throughout all of the results, it seems important to describe it within this paper.

Limitations:
The authors adequately state the limitations of their modeling approach, training paradigm, and datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a large-scale model pretrained on the International Brain Laboratory (IBL) corpus containing multi-region, multi-animal spiking activity of mice during a decision-making task. It introduces a self-supervised learning approach with a novel masking scheme called MtM which alternates between masking and reconstructing activity in timesteps, neurons and brain regions. The masking strategy was shown to be helpful in learning different structures in neural population activity and outperforms temporal masking scheme of previous models. The paper shows positive scaling results on held-out animals as the number of training animals increases.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The paper tackles an important problem of building “foundation models” for neuroscience at the level of spiking activity that can help with the understanding of the brain structure and facilitate the decoding of animal behaviors. 
2.	The proposed MtM method is novel and demonstrated effectiveness over the existing temporal masking method in learning different structures of neural activity.
3.	Thorough evaluation on multi-region, multi-subject IBL dataset provides valuable insights for the neuroscience community.  
4.	The manuscript is well-written. Text and figures are logical and easy to follow.

Weaknesses:
1.	The number of baselines is quite limited. Only two transformer architectures (NDT1 and NDT2) were benchmarked, and the proposed MtM method is mainly compared with one baseline of temporal masking. It appears that MtM could be used as the training objective for similar existing models that also assume a Poisson emission model, e.g. RNN-based LFADS (Pandarinath et al. 2018) and other models in the NLB benchmark (Pei et al. 2021). Evaluating MtM on more architectures could be helpful to gauge if MtM effectiveness is truly model-agnostic as the authors noted or is biased more toward certain type of architecture. 
2.	The dataset only consists of one behavior task (decision making), while existing pretrained models POYO and NDT2 referenced in the paper were trained/evaluated on multiple behavior (motor) tasks, even across species (monkey vs. human). It is unclear if and how the community could benefit from finetuning the proposed pretrained model on other datasets with a different behavior paradigm.
3.	Some experiment details are missing or not presented clearly (elaborated in Questions section below).

Limitations:
The authors have addressed the limitations in the manuscript.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
co7DsOwcop;"REVIEW 
Summary:
This paper proposes a novel approach for effectively capturing spatial and temporal correlations in multivariate time series forecasting and enhancing the interpretability of prediction models. The aim is to address the shortcomings of existing methods, which often fail to adequately reflect dynamic spatial correlations and exhibit high variability. To tackle this issue, the paper introduces a method that bypasses the traditional two-stage learning process and directly generates dynamic structures using a structured matrix basis. Furthermore, the basis matrices are parameterized through singular value decomposition (SVD), and all basis matrices share the same orthogonal matrices to improve the efficiency of model training.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The method of bypassing the two-stage learning process and directly generating dynamic spatial structures effectively overcomes the limitations of existing models.
2. By providing interpretability of the model, it offers users greater insights and increases the reliability of the results. This is a crucial aspect that is often required in many time series forecasting research papers.
3. This paper effectively addresses various limitations that arise in traditional time series forecasting problems.
4. This paper is well-organized theoretically.

Weaknesses:
1. The proposed model may be complex to implement due to the use of structured matrix bases and singular value decomposition, potentially leading to a steep initial learning curve in practical applications. Does the appendix indicate similar results for datasets other than Electricity?
2. While the model has been validated on various datasets, it remains to be seen how well it performs on very specialized domain data or data with unclear characteristics (i.e., weak temporal dependencies or spatial correlations). How does the model address these scenarios?

Limitations:
Including more analysis and comparison with graph-based spatio-temporal models would enhance the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a multivariate forecasting model underlined by a dynamic spatial structure generation function, enabled by SVD-based parameterization and theoretically-bounded output space. Experiments on six commonly benchmark datasets in comparison to several existing baselines demonstrated the overall improved forecasting performance of the presented method as measured by MAE/RMSE/MAPE. Additional ablation studies further demonstrated the benefits of dynamic coefficient generation and the structured parameterization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The concept of dynamic generation of spatial structure function is of novelty and has practical value for adapting to the varying spatial structure underlying time series data.

The SVD-based parameterization and low-rank approximation provides theoretical-based solutions to the challenge of identifying time-varying spatial structure functions. 

The experiments considered a large number of common benchmarks and representative existing forecasting models. The performance was overall favorable, and the ablation study thorough.

The interpretability results in section 4.4 and Fig 2 are interesting, especially in uncovering the dynamic patterns underlying a dataset.

Weaknesses:
The obtained performance gain (Table 1) was overall marginal (up to 2-3 decimal points). The practical implication of such margin of improvements is not clear and need to be clarified. The effect of hyperparameters and random initialization on such margin of improvements also needs to be examined — adding statistics to the results over different random seeds of experiments is important.

In switching dynamic systems, it is also common to model the transition matrix over time as a linear combination of several global matrices, with time-varying mixing coefficients (without considering the proposed SVD decomposition and parameterization). It’d strengthen the paper to add discussion about the relation with this line of works, as well as experimental comparison.


Based on the results in Fig 3, it appears that increasing M from 1 to 2 in general introduced small difference in performance except in the Electricity dataset. This again raises some question on the significance of the dynamic spatial structure introduced in this paper. Stronger clarification on performance improvement and adding error bars to the results in Fig 3 will be important for verifying the contribution of the paper.

Limitations:
The authors did not include sufficient discussion about the limitation or potential negative societal impact of the presented work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a method to learn dynamic spatial structures in spatio-temporal forecasting tasks. Specifically, it proposes to parametrize the dynamic structures with a convex combination of fixed matrix bases, and the bases are further confined to be in the same coordinate system. Beyond that, it also imposes low-rank assumption on the coordinates to further reduce complexity. Empirical evidences show the proposed method achieve impressive accuracy in a number of spatio-temporal forecasting benchmarks, and the found spatial structures are highly interpretable.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written with clear introduction of motivation behind major proposals and solid theoretical ground. 
- Extensive experiments are provided to showcase the advantages of the proposed method empirically that addresses both efficacy and efficiency concerns.

Weaknesses:
The forecasting horizon in the experiments is quite different from the setting in some baselines, such as PatchTST and FEDformer. While long-term forecasting is not major claim of the paper, I wonder if the method scales well with prediction length.

Limitations:
Limitations are discussed, and I agree that the selection of $M$, i.e. the number of bases, is highly empirical and ad-hoc.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
Dgt6sh2ruQ;"REVIEW 
Summary:
This paper considers the problem of dynamic decision making with resource constraints. They show that under certain conditions they can achieve O(1) regret with respect to the time horizon. However, I am not sure whether this is true regret (i.e. with respect to the best policy in hindsight) or regret with respect to a number $V^{FL}$. My final score is highly dependent on the author’s response to this question.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
I am wondering if in Theorem 3.1 the display equation should say $V^{ON}$ instead of $V^{FL}$. If it is indeed $V^{FL}$ then your goal is only comparing to $V^{FL}$ so what is the point of the “worst case” discussion comparing $V^{FL}$ to $V^{ON}$? If is is meant to be $V^{ON}$ in Theorem 3.1 then I think that this is an interesting result. However, if it is indeed $V^{FL}$ then the result is much weaker as you have not shown any improvement over $O(\sqrt{T})$ with respect to the optimal policy. My final score will be highly dependent on your answer to this question.

Weaknesses:
If Theorem 3.1 is written correctly then please see the “strengths” section for a major weakness.

In Theorem 3.1 I believe that the O(1) must be hiding problem-dependent constants. I don’t just mean constants like the sizes of the sets but constants that are based on the linear program itself. I.e. are there some non-degenerate linear programs in which the hidden constant factor blows up arbitrarily high - e.g. when we limit to degeneracy. This is the same concept as in stochastic bandits: the $O(Kln(T))$ factor hides the gap between the mean rewards of the optimal and second-optimal arms - which limits to infinity as the gap limits to zero (although the regret actually never goes above $O(\sqrt{KT})$). You should certainly include any problem-dependent constants in your bound (although I do understand they may be complex to bound - so at least point out that they exist (if they do)).

Line 226 (and line 78): You say that any LP can easily escape from degeneracy. Doe this mean that you can improve on the sqrt{T} bound when $J(\rho_1)$ is degenerate? Otherwise this statement (which appears twice) is highly misleading.

Line 171: Theorem 2.1 does not show an \Omega(\sqrt{T}) regret lower bound when you define the regret relative to $V^{FL}$.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies contextual decision making with Knapsack constraints assuming that the requests and the contexts follows some distributions. It studies the full information setting when each context is revealed after the decision is made and the partial information setting when the context is revealed only when a non-null decision is taken. The paper provides regret bounds under various settings.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Under the unique non degenerate LP assumption, the paper has provided regret bounds that are better than the worst cases in both full information and partial information setting, which is novel. It further shows that without this assumption, the regret bound is indeed $O(\sqrt{T})$ that matches the lower bounds. The paper further provides regret bounds when the request and context are continuously distributed. It greatly enhances the understanding of the problem.

Weaknesses:
1. I found the presentation can be further improved, particularly about why the unique non-degenerate solution can greatly improve the regret bound and provide a proof sketch. This is the main contribution of the work yet the intuition is not fully clear. 
2. See questions below.

Limitations:
Comparison to continuous optimization methods in NRM problems are not discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies an online contextual optimization problem with resource constraints. The paper provides a sufficient condition (worst case condition) under which the fundamental limit on the regret bound is reached. The paper further provides an algorithm that can achieve \tilde O(1) regret when the worst case condition does not hold. Numerical results are also provided to validate the theory.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well written. The relation between the worst case condition and the degeneracy of linear constraints is novel and inspiring. The intuition behind the proposed algorithm is clearly explained.

Weaknesses:
1.  Is $T$ known beforehand? If $T$ is unknown, how to compute the leftover budget $\rho_t$? If $T$ is known, how to design an algorithm with unknown $T$?

2. What's the difference between Rew and V^{on}?

3. if there is $\rho^i$ amount of resources per stage, why does the problem formulation only respect the resource constraint at stage $T$, instead of considering resource constraint at every stage $t$, i.e. total resources available to be used in stages $t$ <= $\rho^i t$ for all $t\geq T$?

4. In the simulation, how does the proposed algorithm compare with the existing algorithms proposed for this problem?

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers a new contextual decision-making model with knapsack constraints, which is highly related to the CBwK setting but features a different information feedback structure. Under this model, the authors nearly characterize the conditions under which $\tilde{O}(1)$ regret can be achieved based on the degeneracy of the optimal solution of the fluid LP. Specifically, the re-solving heuristic is proposed to achieve $\tilde{O}(1)$ regret under these conditions. Additionally, the $\tilde{O}(\sqrt{T})$ worst-case regret of the algorithm is provided.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The model considered in the paper is quite general to cover several interesting problems.

2. The results provided under the proposed model is relative sharp and complete.

Weaknesses:
1. The assumption on the randomness is stronger than those made in the contextual decision-making literature.

2. While I understand that this paper considers a new setting and that the works in CBwK are the most related, it is acceptable that the authors mainly compare their results with those in CBwK. However, since the information feedback assumption in this paper is strictly stronger than in CBwK, I suggest the authors provide more comments regarding this difference to make the comparison fairer.

Limitations:
N.A.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
woRFmNJiLp;"REVIEW 
Summary:
This paper proposed a new method for LLM alignment during pre-training. The proposed method is call ""native alignment"". This method include three steps: pretrain date duplication, alignment rewriting, and model training. They trained small size alignment expert model for alignment rewriting and use the model to rewrite large-scale pre-training data. The rewriting process suppose to solve format issue, value/fairness issue, unsafe content in pre-training data. They experimented with Arabic data and LLMs. Their experiments shows that the proposed method can help LLMs be more safe and helpful.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper proposed a new idea to align LLMs during pre-training. It seems an interesting topic. 
2. The paper writing is clear and well-organized.

Weaknesses:
1. Lack of comparison to existing post-alignment methods. The proposed method is a ""native alignment"" during pre-training. I wonder if this method can outperform the post-alignment methods. While the author acknowledged this limitation, I still feel it is important for strengthening their claim. 
2. Need more analyses to better understand their method's potential trade-off. For example, I wonder if rewriting pre-training data undermines the LLM's capacity to understand and learn Arabic dialects. The rewriting process may convert Arabic dialects into MSA. I also wonder if the rewriting data inherited hallucinations from LLM and deteriorated the trained model. 
3. The paper needs more clarification on experiment details. For example, they exploit Arabic data to investigate their method, however, the evaluation dataset, BeaverTails dataset, is an English dataset. I wonder how they evaluate and if they translate the samples.

Limitations:
I think that the paper needs more diverse analyses to understand the potential trade-off of their method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduce a method called ""native alignment"", which is a set of procedures to create data and train an LLM to rewrite raw text into ""useful"" texts for pretraining. They apply this technique specifically for Arabic LLMs and conduct experiments to show that this pre-processing of pre-training data helps produce better Arabic LLM down the line. As bonus, they release open-source Arabic LLMs for the communities

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper ideas are presented clearly and easy-to-understand

Weaknesses:
* As a proclaimed novelty, the paper draws itself between pre-alignment and post-alignment, indicating that previous work only focus on post-alignment but not pre-alignment. However, I afraid the paper misunderstands the concept of post-alignment (RLHF) and fails make an accurate comparison. Post alignment (RLHF) is finetuning technique to train the models to reward good-vs-bad response according to human values, and train the policy models to lean on the good behavior and stay-away from the bad behaviors gradually, often with the existence of a reference model (DPO and RLHF).

Meanwhile, the ""native alignment"" presented in the paper is a data-cleaning procedure, and it does not having any resemblance or contrast with ""post-alignment"". Furthermore, using LLMs or training LLMs to rewrite raw text to produce cleaner data is not new or novel, there are many techniques out there that do so, and there are abundant open-source data on huggingface which were produced in similar ways.
This confusion between data cleaning and alignment makes the paper less credible and the lack of novelty it the methodology itself, as a data cleaning method, is also troublesome.

Obviously as a result, the paper did not provide any necessary and required experimental comparisons with other data cleaning methods.

* Though I do appreciate the paper's effort for Arabic community, the scope of only Arabic LLM is small and generally inconclusive, that such method is not shown to generalize to other languages, domains. Perhaps, thus, the work is really not suitable for NeurIPS but more suitable for CL-type venues

* It is unclear from the writing whether the authors pretrained Llama-3 with Arabic from scratch or further finetune from Llama-3 checkpoint. In either case, there should be explanation and further ablation studies.

Limitations:
The authors discussed limitations

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a data augmentation pipeline which modifies the pre training data for large language models in key aspects such as formatting, values, content moderation and knowledge preservation. The resulting pipeline, termed native alignment, is applied Arabic LLMs due to the relatively small pretraining corpus available and the difference between Arabic and western culture. Experiments are conducted to test the performance on a few metrics including trustworthiness, knowledge, and Arabic localisation.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This is a well written paper targeting the important topic of llm alignment. It also addresses the relatively under explored sub question of how to improve alignment at pretraining. The resulting pipeline presents a reasonable idea, and the evaluations are clear and I find them comprehensive too. The author(s) should also be commended for their transparency regarding the limitations of the paper.

Weaknesses:
Although this might have become the norm of recent LLM papers, I still think it is important to include a discussion of the metrics used to measure things like 'trustworthiness' and 'knowledge', as these are qualitative metrics, whereas in the paper, it seems like the authors just quoted some existing evaluation pipeline.

Limitations:
As the authors are already open about, comparisons with other post alignment methods are not included. The authors attribute this to an absence of existing alignment evaluation benchmark, but I don't fully understand this - what is stopping the authors from using the same alignment benchmarks as ones they have already used to compare with other pretrained models?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on alignment of LLMs to human preferences and suggests to shift the alignment step from instruction-tuning (post-alignment) to the earlier stage of continued pre-training (native alignment). For that end it proposes an approach to creating aligned pre-training data, consisting of three steps: (1) seed data cleanup and rewriting with humans'/LLM help, (2) training a supervised cleanup model on that seed set and (3) processing the final pre-training dataset with that cleanup model. Presented experiments show that alignment data results in higher final quality compared to unprocessed pre-training data and that the performance gain does not reach a plateau at 12B tokens, suggesting that the amount of alignment data should be limited by the budget allocated to train an LLM. Experiments are performed on Llama-3-8B and Llama-3-70B and the Arabic language.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- A high-impact and efficient approach to pre-aligned model training is introduced

- Two pre-aligned LLMs for Arabic are released openly based on the experiments in this paper

- Related work is excellent, the paper is written very clearly and is easy to comprehend

Weaknesses:
1. No direct comparison between native alignment and post-alignment is reported

2. Minor text discrepancies are present:
- rows 16-18: partial sentence ""while.."" is not finished
- row 47: missing verb: ""LLaMA3-Tamed-8B could beneficial"" --> ""LLaMA3-Tamed-8B could be beneficial""
- row 326: typo: ""instruction tinning"" --> ""instruction tuning""
- row 150: ""pre-training"" should be called ""continued pre-training"" in this case

3. The created seed data and cleanup models are not released

Limitations:
Ok

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
UPxFYvHsyN;"REVIEW 
Summary:
The author proposed a novel, time-efficient, template-free NeRF-based method for 3D dynamic scene reconstruction, focusing on capturing detailed explicit geometry for each entity in the scene. Extensive experiments demonstrate the efficiency of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-organized, and the font size in the figures is appropriate, making it easy for readers to follow.
2. The method section is straightforward, providing a detailed introduction to each part, including sufficient details on the network, training, and loss weights.

Weaknesses:
1. The author mentioned that one of the contributions is time efficiency. Based on the description in lines 183-202, it seems the proposed method reduces computation to accelerate training. However, Figure 6 only shows the convergence speed. It would be more informative if the author could report the time consumed per iteration.

2. The information on how to obtain the semantic masks is missing. For example, in RoDynRF [1], the author uses optical flow and Mask R-CNN to obtain the segmentation masks.

3. The work focuses more on dynamic scenes with human subjects, but the title gives the impression that the method is designed for general dynamic scenes.

4. From Figures 2 and 3, it appears that the proposed method requires a 3D skeleton as part of the inputs. Does this mean that the proposed method does not work for scenes without humans or datasets lacking 3D skeleton information?

5. In the bottom part of Table 3, under the Dist. Acc column, HyperNeRF is the second-best method.

[1] Liu, Yu-Lun, et al. ""Robust dynamic radiance fields."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2023.

Limitations:
The authors have addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper propose TFS-NeRF to solve the semantic reconstruction of dynamic scenes.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
Experimental results on multiple entity and deformable entity reconstructions are good.

Weaknesses:
It is very hard to follow the storyline of the introduction.

Unclear contributions. In Lines 81 and 91, the paper claims it could reconstruct multiple dynamic entities with multiple complex interactions, but in the experiment parts, the presented results are mostly from one person/animal with a single object, it is hard to judge whether the proposed model could process multiple dynamic entities with complex interactions. Also, the authors claim the proposed model could address the occlusion challenge in Line 81, but there is no experiments about it. Besides, the paper claims that it could reconstruct semantics of dynamic scenes by using semantic masks. However, such semantic reconstruction has been solved by previous works like Semantic Flow [1], it is hard to judge the contribution of this paper in this situation.

Missing important method details. This paper proposes a semantic-aware ray sampling strategy and illustrates the points are separated in to deformable object set and non-deformable object set, but does not present any details about the separation process. After reading the paper, the reviewer guesses it separates the points on the rays based on the semantic labels of the pixels traveling through the rays. However, it may not be correct since different points in the same camera ray may belong to different set (like a ray travels through a dynamic person and a static wall behind the person). It is very hard to understand the entire pipeline of semantic reconstruction without such details.

Writing should be significantly improved. There are many informal and unclear expressions in this paper:

- Line 405: Takeaways -> Conclusion
- Figure 2: Overview of the system-
- Line 229: unclear expression of R^{1+256}
- Line 221: why ""it helps capture better articulation or deformation and surface reconstruction under motion?""
- Tables 5 and 6: what is the mearning of ""->""
- Line 216: Given,
- Lines 98, 112, 136: the usage of : should be the same

Reference:
[1] Tian, Fengrui, Yueqi Duan, Angtian Wang, Jianfei Guo, and Shaoyi Du, Semantic Flow: Learning Semantic Fields of Dynamic Scenes from Monocular Videos, ICLR 2024.

Limitations:
Unclear expressions and limited contributions

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses the problem of reconstructing dynamic environments for arbitrary rigid, non-rigid, or deformable entities. The authors propose a template-free 3D semantic NeRF for dynamic scenes, which employs an Invertible Neural Network (INN) for LBS prediction, and optimizing per-entity skinning weights based on semantic masks. The experimental results show high-quality reconstructions of complex interactions with improved training efficiency compared to existing methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The proposed method is able to reconstruct arbitrary non-rigid objects by utilizing TAVA framework.
- Disentanglement of objects improves the reconstruction quality by predicting LBS for each entity.
- The proposed framework outperforms existing template-free NeRF methods on various datasets containing different non-rigid and rigid objects.
- The paper is clearly written and easy to follow.

Weaknesses:
- The paper emphasizes semantic-aware ray sampling as a key contribution but lacks details on how the semantic masks are generated.
- The framework heavily relies on existing methods (TAVA, INN), limiting the novelty and distinctiveness of its contribution.

Limitations:
- The method is not scalable for the scenes more than two entities.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes TFS-NeRF, a semantic-NeRF framework leveraging no prior templates of dynamic scenes for 3D reconstruction. Guided by INN-driven LBS prediction and semantic-aware ray sampling, TFS-NeRF separately consider deformable and non-deformable parts during geometric learning but composite them to learn apperance to benefit from self-supervised RGB reconstruction loss. Experiments on human-object and animal cenric dynamic videos demosntrate the advantages of the proposed method against chosen baselibes.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
-The overall paper is well motivated and easy to follow. The qualitative reuslts and supplement videos demonstrate the promising reconstruction performance of dynamic scenes with multiple entities.

-The experiment are extensively evalauted on several public benchmarks and the ablation studies highlight the unqiue contribution of several design choices.

Weaknesses:
My main concerns comes from the lack of clarification on several key components:

(1)As a semantic-nerf framework, how abou the influence of quality of input masks? As to labels from an imperfect 2D predictor, what is the impact on final results, since the label quality may affect the sampling quality for deformable and non-deformable rays.
(2)Similarly, what is the robustness to pose accuracy as poses are involved with both ray generation and the pose loss?

Discussion on the above two points could better strenthen the practicalibility of TFS-NeRF.

(3)As a rapidly growing area, the chosen baselines do not include latest methods such as HexPlan,Tensor4D or GS-based ones to highlight the advantages of using INNs, compositional rendering as well as reconstruction quality.

(4)As to LBS predicton, what is the unique advantages of applying INN over other types of NN modules (e.g, lossy CNN auto-encoders)? What is the efficiency or performance boosts compared to standard CNNs?

(5)How to get the initial values of B_i to compute the initialization value of x_c for arbitrary objects?

(6)As the paper is claimed to template free and able to deal with generic scenes, it would be more exciting and convincing to see other types of interactions? For example, a common daily objects (e.g. a door or a cabinet, opening and closing) to really show the advantages of prior-free deformations of general scenes.

(7)It would be good to aldo add the appearance quality or view synthesis performanc, though I understand the focus of TFS-NeRF is on 3D reconstruction.

(8)Formatting needs to be improved. For example, it is wierd that in Sec3.B, authors put much related work descriptions. There are also improper indents like Line 282 and Line284,etc.

Limitations:
Limitations are properly mentioned in the main paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
Cqr6E81iB7;"REVIEW 
Summary:
Various results are proved about online learning of private learning algorithms, contrasting no DP, pure DP and approximate DP.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper present some interesting new results on online learning with DP. I read up to section 3 and the writing is very clear and the results are important and purportedly novel. (I don't have background or recent experience in the area of online learning so I cannot independently confirm their novelty.)

Weaknesses:
I can't point to any weaknesses, but this paper is outside of my area, and I was only able to follow up to Section 3, so it is certainly possible there is something I missed. I am basically taking the paper at its word on the claims made in Sections 1 and 2.

Limitations:
No potential negative societal impact.

Rating:
8: accept, good paper

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies online learning of concept classes under DP (differential privacy) constraint. The paper makes progress towards understanding mistake bounds (mostly) in the realizable case in a few settings. Concretely, The paper shows that:
1. If the adversary is oblivious, then PAC pure DP learnability implies online pure DP learnability.
2. On the other hand, if the adversary is adaptive, then PAC pure DP learnability does not imply online pure DP learnability.
3. In contrast with the standard online learning model, for every non-trivial hypothesis class, the mistake bound depends on $T$.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Assuming that the results are correct, the paper contains a significant improvement towards understanding mistake bounds in online learning under DP constraint.
2. The questions and results are interesting and in the scope of NeurIPS.
3. The proof techniques are explained in detail.

Weaknesses:
1.It is a bit hard to digest the results and understand the remaining gaps, because there are many settings considered in the paper (DP/non-DP, oblivious/adaptive, approximate/pure...), and no figure/meta-theorem that neatly explains all the relationships. Such a figure/meta-theorem would significantly improve the presentation of the paper.

2. As a consequence of the above, it is not exactly clear how tight the results are. If you prove a lower bound (as in Section 4), I think it is better to formally state, right after it, the best known lower bound (and the dual statement for proving a lower bound).

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper demonstrates that any function class that is offline PAC learnable with pure DP is also online learnable with pure DP against an oblivious adversary. In this context, a hypothesis class is considered online learnable in the realizable setting if there exists an algorithm with a sublinear mistake bound.

The paper also establishes a distinction between online learning with pure privacy for oblivious and adaptive adversaries. Specifically, it shows that the hypothesis class $point_N$ is privately online learnable against an oblivious adversary but not against adaptive adversaries. This finding also indicates a separation between pure and approximate private online learnability, as $point_N$ is online learnable with approximate DP against an adaptive adversary.

Additionally, the paper presents a general lower bound on DP online learning against an oblivious adversary for non-complementary function classes.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper offers general results on private online learnability, identifying conditions under which a hypothesis class is online learnable with pure DP against an oblivious adversary. This connection to Representation Dimension links to existing results on DP PAC learnability.
- It also explores different layers of separation using the function class $point_N$, contributing to a deeper understanding of the cost of privacy in online learning.

Weaknesses:
The proof of Theorem 4.3 is not clear

Limitations:
No significant limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies limits of pure DP and approximate DP in the context of online learning (with oblivious and adaptive adversaries).

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
The research questions are interesting and the results may have fundamental value.
I'm not an expert in all topics covered by the paper, but the contributions seem novel to me.
The text is in general well-written and understandable.

Weaknesses:
The paper has a lot of theorems and lemmas, which is interesting.  Still, the paper has no conclusions, discussion, further work, description of limitations or illustrative experiments or extensive examples.  This puts the task of understanding the value and applicability of the work to a large extent to the reader.

Limitations:
The authors don't discuss limitations or societal impact.
I believe there are no ethical concerns with this theoretical work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
S1fc92uemC;"REVIEW 
Summary:
This paper proposed a method about how to use a single LLM for both context reranking and answer in RAG tasks. Particularly, they finetuned a LLM with both ranking data and QA data with two stages. For inference, they use the trained llm to sample the top_k contexts at first, and then input them into llm to get the answer. Compared with normal rag workflow, this method choose the contexts by llm itself. Experiments in some QA tasks evaluated the QA effectiveness of this LLM.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Two stages of training enhance the reranking capacity of llm using fewer ranking data.
2. Using just a single llm for context ranking and answer at the same time.

Weaknesses:
1. The motivation is unclear. In lines 23 to 31, limitations 1 and 2 precisely explain the need for a reranker, which is also a challenge related to retrieval. Only with limited text in limitation 3  briefly mentions why the LLM is used for reranking contexts due to zero-shot performance, which has been proved with other rerankers in this paper. It is unclear about why using llm reranking itself rather than a separate reranker, maybe including the semantic gap between the reranker and llm or the larger length of reranking input at once.
2. Insufficient experiments in same reranking setting. It is lack of baselines with RAG methods also using a reranker, which can prove the core reranking effectiveness of RankRAG, such as rankgpt(Is ChatGPT Good at Search? Investigating Large Language Models as Re-Ranking Agents) and rankvicuna(RankVicuna: Zero-Shot Listwise Document Reranking with Open-Source Large Language Models). And also, the amount of top-k contexts after reranking is also hard to find in Table 2&6. How many contexts for reranking are input into llm at once? In addition, there are some blank spaces in Table 2 that are not convincing. 
3. Incremental techniques. Compared with ChatQA, RankRAG introduced the new training data RQA and RAR in stage 2th and reranking phrase during inference. But as shown in ablation study, each component contribute a little.

Limitations:
No potential negative societal impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce a novel approach to instruction fine-tuning large language models (LLMs) for ranking and answer generation tasks.
Their approach involves two main steps:
1. Supervised Fine-Tuning: Initially, the LLM is fine-tuned on a general instruction-following dataset.
2. Ranking Task Fine-Tuning: The LLM is then further fine-tuned using a mix of different instruction ranking datasets that contains multiple ranking-oriented tasks.

Incorporating ranking-based data during fine-tuning enhances the LLM's ability to rank documents retrieved through retrieval-augmented generation (RAG). The fine-tuned LLM is then used to rank the RAG-retrieved documents, and only the top-k ranked documents are added in the context.

The authors demonstrate that their method significantly improves the LLM's performance for knowledge intensive RAG based tasks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The authors introduce a novel method of fine-tuning LLMs for combined ranking and answer generation tasks.
2. The proposed method outperforms existing approaches on RAG benchmarks, especially on challenging datasets where RAG retrieval is suboptimal, due to the LLM-based re-ranking.
3. Their model exceeds the performance of re-ranking models trained on much larger datasets.
4. The fine-tuned LLM demonstrates strong generalization capabilities, showcased by its performance on medical benchmarks.
5. The paper is well-written and includes exhaustive experiments.

Weaknesses:
1. Scoring each document individually increases the latency significantly. Have you considered scoring multiple documents simultaneously? Similar to retrieval-augmented ranking dataset, you could input a group of documents and score them in a single pass. The group size could be tunable, balancing performance drop against latency improvement.
2. It would be helpful to include examples where RankRAG fails, particularly in cases where relevant documents are not ranked higher. Providing these examples can help understand scenarios where it might not perform optimally.

Limitations:
The authors have acknowledged the limitations, specifically in terms of latency and its lack of training on code or mathematical data.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes an effective approach that enhances existing RAG methods by introducing an additional context refinement step. This step filters out retrieved, but non-relevant contexts prior to including them as context in the input for answer generation.

The authors train context reranking alongside answer generation using a single LLM. They demonstrate that adding only a fraction of ranking data to the training blend yields superior ranking performance compared to training the same model in isolation on the ranking data, while outperforming other models that have been trained on 10 times more ranking data.

For evaluation, the authors compare their method on 9 general- and 5 specific-domain datasets, showing consistent improvements across LLM sizes for the Llama model family (Llama 2 and 3) over existing methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper introduces the novel idea of using the same LLM to first assess the relevance of individual contexts in a cross-encoder ranking style before using them as input for answer generation.

- The proposed RankRAG outperforms existing methods on various general and specific-domain datasets.

- Extensive experimentation and ablations covers a wide range of possible setups including LLM size, retriever, and efficiency and effect of different model components.

Weaknesses:
Main concern: 
- Reranking contributes only around 5% of the overall effectiveness on average (Table 4 RankRAG compared to Llama3-ChatQA-1.5-X), and the 7x computational overhead in inference time raises questions about its realistic application, given the computational demands of large models already without ranking contexts. The ratio of performance gained ( in combination with no sig. testing) and increase in computation is my main issue with this work. I am aware reranking fewer contexts decreases performance, however, similarity decreases performance gain over other methods. 

- No significance testing was done (for both generation on ranking ) to strengthen effectiveness claims as differences for most datasets are minor. Authors 
justify in their additionally provided checklist that sig. testing is not needed as generation and ranking is deterministic, this however, touches upon a different aspect and does not remove the need for testing whether the performance is significantly better than previous methods.
Further, improvements in Table 2 stem from NQ and PopQA which are relatively simple datasets that can be answered with a single context, therefore it is not apparent why RankRAG would particularly excel at those. Moreover, the ranking performance for these datasets in Table 6 only marginally improves over other rerankers, therefore it is to be expected to obtain similar gains when replacing the RankRAG reranker module  with other strong rerankers. As a side note averaging over different metrics - even though seen in many recent works - should not be done. 


Other points to improve upon:
- No information about the crowdsourced dataset is provided.
- In Section 4.2.3, the claim that LLM needs to be robust against irrelevant context contradicts the proposal to filter out irrelevant context beforehand.

Some experimental setups are unclear:
- It is not described how true/false tokens from context ranking are translated into a score that can be used for ranking.
- In Section 5.1, it is not clear if baselines use different retrieval setups; otherwise, effectiveness claims do not seem valid.
- It is unclear which number k is eventually used in RankRAG. The paper mentions optimizing for
k=5,10,20, the baselines. 

- The related work section could mention GRITLM as the first model jointly training answer generation and ranking for RAG: Muennighoff, Niklas, et al. ""Generative representational instruction tuning."" arXiv preprint arXiv:2402.09906 (2024).

Issues in Writing:

- Line 332: ""Observe that even with N = 20, We noted that"" – incomplete sentence.

- Line 41: ""in RAG framework"" -> ""in the RAG framework"".

- Caption Fig 1: ""the strongest RAG model,"" -> ""the strongest RAG models,"".

- Line 100: ""embedding model"" -> ""embedding models"".

- Line 111: ""As illurstraded"" -> ""As illustrated"".

- Line 112: ""one of the strongest model."" -> ""one of the strongest models"".

- Line 157: ""that, it is"" -> ""that it is"".

- Line 165: ""The LLM need"" -> ""The LLM needs"".

- Line 169: ""ranking data are"" -> ""ranking data is"".

Limitations:
The authors addressed the limitations adequately, however, could emphasize more on the dramatic inference time increase that results from the reranking step.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
IgU8gMKy4D;"REVIEW 
Summary:
The authors consider the problem of contrastive dimension reduction/estimation. In a nutshell, this problem involves a background dataset and a foreground dataset, and it is of interest to determine if the foreground dataset contains submanifolds/subspaces that are inherently different than those in the background. While there's past work on determining the differences of the foreground from the background, this paper asks if such a difference exists in the first place. This is posed as a hypothesis testing problem. An intuitively reasonable test is proposed and it's shown that the test is consistent. The authors also derive finite sample bounds on the error, resorting to commonly used sub-Gaussianity assumptions. They demonstrate the utility of the test through synthetic as well as real data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The manuscript identifies a new problem in an otherwise known sub-area of dimension reduction. A solution is proposed and it is justified both asymptotically and for finite sample scenarios, using available, and accessible mathematical tools. The paper is well written, motivates the problem well, and demonstrates the solution nicely on synthetic/real world examples. I liked reading the paper and I think it would be useful for future readers.

Weaknesses:
I think the manuscript has minor weaknesses. The most important one of those is that the proposed test itself is not very surprising, but I don't know if that's really a weakness -- after all it's backed up theoretically, and simple is good.
Other than that, I list a few questions below, which should be easy to address by the authors.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper deals with a contrastive dimension reduction (CDR) techniques which seeks to find a unique low-dimensional pattern that only exists in the foreground data compared to background data. Authors point out that despite of recent developments of related techniques such as contrastive principal component reduction (CPCA), there lacks a rigorous guideline to determine when and how those techniques should be utilized. Hence, they propose a hypothesis testing method to determine whether the foreground data actually have unique information which background data do not and an estimator of the number of CPCA PCs. The authors then investigate theoretical aspect of the estimator including the consistency and finite sample error bound. They also conduct simulation study and real data analysis to demonstrate the performance of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Contribution of introducing hypothesis testing and estimator of number of PCs to CDR literature.

2. Mathematically rigorous formulation to capture the amount of information which only exists in the foreground data.

3. Theoretical proof of the consistency and error bound of the estimator.

4. Extensive real data analysis with interpretation.

5. Well organized and well written manuscript.

Weaknesses:
1. Discrepancy between the p-value obtained from permutation test and the estimator happens in some cases. The authors might need to conduct additional simulation study to investigate the reason behind this discrepancy.

2. The simulation study seems to be too simple without repetitive iterations to check the stability of the proposed method. More extensive simulation is recommended. Authors need to at least provide the average of the repeatedly estimated value.


3. The proposed hypothesis testing relies on the permutation method which makes the result unstable and further theoretical investigation difficult.

Limitations:
The authors well addressed the potential limitations and future research goals of the study. They included the possibility of introducing likelihood ratio testing method to overcome the weakness of the permutation test pointed out as the 3rd weakness of the manuscript.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes to examine the existence and estimate the number of contrastive dimensions between the foreground and background groups. The authors provide a formal definition of contrastive dimensions and propose a hypothesis test method to examine the existence of contrastive dimensions. The authors also propose a contrastive dimension estimator to estimate the number of contrastive dimensions, and provide theoretical analysis on the consistency and finite sample error bound of the proposed estimator. Experiments are conducted on synthetic semi-synthetic and real-world datasets to evaluate the performance of the proposed hypothesis test and contrastive dimension
estimator.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The motivation is novel, and the proposed contrastive dimension estimator is theoretically sound.
2. The paper is well-written, and the experimental codes are made available, which make the paper easy to follow and reproduce.

Weaknesses:
1. The formulation of contrastive dimension seems impractical. For instance, consider the foreground group $X\in\mathbb{R}^3$ drawn from $\mathcal{N}(\mathbf{0}, diag(2, 1, \sigma_\varepsilon))$ and the background group $Y\in\mathbb{R}^3$ drawn from $\mathcal{N}(\mathbf{0}, diag(1, 1, \sigma_\varepsilon))$, where $\sigma_\varepsilon$ denotes the standard deviation of the noise terms. Both the foreground $Z_i$ and background $W_j$ occupy the same subspace, specifically the column space 
$\mathcal{C}\left(\left[ {\begin{array}{ccc} 1&0&0 \\\\ 0&1&0\\\\ \end{array} } \right]^\top\right)$
. The contrastive dimension is expected to be $dim\left(\mathcal{C}\left(\left[1\quad 0\quad 0\right]^\top\right)\right)$, yet according to the definition 1, the contrastive dimension is $dim(V_{xy})=0$ .
2. In section 2, the paper claims that a significant application of the proposed methods is to determine the number of dimensions for contrastive dimension reduction. However, the experiments provide limited discussion on the effectiveness of these methods in selecting hyperparameters for downstream contrastive dimension reduction tasks. This lack of detail undermines the practical utility of the proposed
approaches.
3. The paper mentions that the hypothesis test is conservative, and the experimental results indicate that the hypothesis test does not offer significant advantages over simply detecting whether $\hat{d}_{xy}=0$. However, the paper does not justify the need for a hypothesis test when the contrastive dimension estimator appears to be effective on its own.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors address two key challenges using two datasets, categorized as foreground and background: 1) Determine whether contrastive dimensionality methods are appropriate for application to such a pair of datasets and 2) Quantify the unique information present in the foreground data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The text is mostly clear including two theorems and two algorithms.
- The background section is well-written, offering a comprehensive overview of relevant literature and methodologies

Weaknesses:
--Hypothesis Test:
The assumptions, such as $V_x \subset V_y$ under $H_0$, lack proper justification.
The hypothesis test relies on estimating the intrinsic dimension, which is inherently challenging. The authors employ other methods for these estimates but fail to compare the accuracy of their estimates to these methods. These methods could also be used to test $d_{xy} = 0$.
The overall presentation of the hypothesis test is inadequate. The rationale for choosing this specific test and how calculating the singular values of the custom $\hat{V}_y$ improves the $p$ estimate remains unclear. The authors do not justify, explain, discuss, or comment on their choices.
--Experimental Evaluation:
The method lacks comparison to other approaches: For intrinsic dimension estimation, there's no clear way to compare against other methods due to the absence of ground truth. Alternative evaluations should be designed, such as training GANs to a priori upper-bound the intrinsic dimension of generated data by the dimension of the latent noise vector, as done in [1].
The authors don't compare their work to other ""contrastive dimensionality reduction"" methods. They could compare (i) expressed variance in the foreground group among different methods, (ii) expressed variance in the background group, and (iii) expressed variance common across groups. Alternatively, they could propose other ways to quantify performance and compare their method to existing literature.
The intrinsic dimension estimates are compared to those of (Pope et al., 2021), which aren't ground truth values. This makes it difficult to compare the inferred dimension to the actual one.
The practical implications of the algorithms aren't tested. How can these methods use dimensionality reduction to identify foreground sets? How can they be applied to related downstream classification tasks? How can they be used to compare different experiments?
Despite being motivated by ""large scale data,"" the experiments are limited to small dimensions (e.g., MNIST), with no consideration of scalability crucial in today's datasets.
There's no exploration of other datasets or discussion on how intrinsic dimensions were estimated.
--Terminology: The terminology is often unclear and potentially misleading. For example, ""unique information in the foreground group"" refers to low-rank components, and terms like ""contrastive information"" and ""contrastive dimension"" could be misinterpreted in the context of self-supervised learning. Alternative terms like ""complementary information"" may be more appropriate. Even the title is misleading as ""contrastive dimension reduction"" is closely related to contrastive learning, a self-supervised dimensionality reduction method.
Title: The title doesn't accurately reflect the paper's content and focus.
--Missing Conclusion: The paper lacks a conclusion section.
Language Issues: The manuscript contains several typos and linguistic errors, including repeated words in the abstract (e.g., ""with with"").
[1] Pope, P., Zhu, C., Abdelkader, A., Goldblum, M., & Goldstein, T. The Intrinsic Dimension of Images and Its Impact on Learning. In International Conference on Learning Representations.

Limitations:
Please see weakness section

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
preo49P1VY;"REVIEW 
Summary:
The paper introduces a matrix mixer framework for sequence models which linearly applies an LxL matrix M  to a sequence representation X of length L. Popular sequence models can be framed within this context e.g. softmax self-attention or SSMs, according to different properties of M. The authors use their framework to identify desirable properties of M, such as data dependence or extendability (where the parameterisation is such that the sequence length L can change for different sequences), or efficient matmul. The authors then use their framework to design sequence models with Vandemonde or Cauchy mixing matrices that perform comparably to attention. The main contribution of the paper is the introduction of Hydra, a bi-directional variant of SSM like Mamba. To do this, the authors let M instead denote a quasiseparable matrix (which is a bidirectional variant of the causal semiseparable matrices introduced in the Mamba-2 paper [6]). The authors show that Hydra (i.e. using quasiseparable matrix mixers) outperforms other matrix mixers (4.1), outperforms other approaches for bidirectional SSMs (4.2), and outperform other standard sequence model arhchitectures like transformers on Masked Language Modelling and ImageNet classification.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
1. The paper provides a general framework for matrix mixing and introduces desirable properties like Sequence Alignment.
2. Introduces a bidirectional version of SSMs, called Hydra, which is motivated through their framework.
3. Hydra seems to work really well, beats unidirectional Mamba and transformers across different settings where bidirectionality is desired.
4. Shows their framework can motivate new sequence models using e.g. Vandermonde or Cauchy matrices, and provides insights into why existing methods may perform well (like low rank matrices in linear attention).

Weaknesses:
1. The writing is unclear in large parts. This detracts from the flow and readability, and ultimately makes it somewhat a frustrating read, as it seems there are nice ideas here but communicated poorly. For example:
- What does “common data transformations” mean in definition 2.1.
- The term “Structured matrices” is not standard terminology and is not formally defined. In my reading the closest thing to a definition is line 117: “... a structured matrix, which are known to possess sub-quadratic matrix multiplication”. Is a structured matrix defined by the ability to do sub-quadratic matrix multiplication, because line 117 suggests there are other properties involved. Also the reference to established “mathematical literature” in line 120 but no citations are provided nor examples of structured matrices.
 - “Data dependence” also doesn’t seem to have a theoretical definition, but is part of the theoretical results (proposition 2.3) so should be defined. What does “canonical” mean in the context of line 138? The authors write ""Although data-dependency is a popular notion that is widely regarded as being crucial to performance of Attention, it lacked any formal definition in the literature."" but there also isn't a formal provided here in my reading. What does ""the third interpretation"" refer to in line 153?
- What are $\phi$ or $\hat{P}$ in definition 2.2 of sequence alignment, are there any constraints on $\hat{P}$, why/when is it necessary in the definition? I assume $\phi$ is the empty set but this is not obvious nor defined.
- The proof of proposition 2.6 does not prove the sequence alignment property.
- Are Quasiseparable matrices established in the literature or have you defined them? There is no citation provided afaict.
- Should the tilde be a hat on P in line 237 (over vice versa in line 131).
- The 2 paragraphs from line 190 to line 205 make reference to results which are a lot later (e.g. table 2). I would move the table earlier or at least refer to table 2.
- “Taming the Hydra” isn’t particularly insightful for a subsection title.
- The advantages of Hydra (lines 246 to 249) are not justified until later in the subsection, so on first reading it seems like overclaiming (also see my 2nd point). It is also not clear what is being compared to with these advantages (“heuristic alternatives” is cited for the first one but is this also true for the second and third benefit?)
- “It is known that quasiseparable matrices are closed under addition” - cite or prove.
2. It is not clear to me if the theoretical motivation of Quasiseparable matrices is actually a big factor behind the practical gains: in Figure 4 it seems like you are discretising the forward and backward SSD differently, which seems like it will break the symmetry between the A matrix in the forward and backward SSD which is necessary in for the equivalence in proposition 3.2, which to my understanding states in layman terms: “a QS matrix is just two SS matrices that *share parameters*  with flips/shifts”. I could be wrong, but discretise_A is not defined so it is hard to check and the zipped code is different. But if I am right this seems to go against one of the motivations of the paper that previous bidirectional SSMs are heuristic or ad-hoc.
3. Likewise, the arguments in line 250-258 that motivate Hydra as opposed to previous approaches for bidirectional SSMs make it seem like the only difference is the diagonal elements, which can be seen as skip connections (see e.g. https://arxiv.org/abs/2302.10322 in the example of diagonal elements of attention matrices replacing standard skip connections). Given that the previous methods presumably also use skip connections, this difference seems quite minimal. If anything, it might be better to pitch this framework as encapsulating and generalising previous works.
4. Ablations to understand the empirical improvements made in Hydra are missing. In general the results seem great, but as discussed a reader may have unanswered questions why Hydra seems to work so well.  For example, related to above, what to performance happens if you remove the extra diagonal part, which should boil down to the Add method in table 3 right? Also what if you change the convolution (e.g. remove it or make it a causal conv) because it doesn’t seem to be connect to the theory here (of quasiseparable mixers)? Also what happens if you remove the parameter sharing between the two SSDs (which again should get closer to the previous methods)? What if you place the different mixers in a standard transformer block instead of a Mamba block in table 2 (does table 2 give an unfair advantage to QS)?
5. The authors write that ""Hardware efficiency"" is a limitation in the appendix. Do the authors have throughput results for Hydra compared to transformers? If hardware efficiency is a concern then isn't ""fast implementations on hardware"" a more practical desiderata to design new sequence models as opposed to use ""sub-quadratic matmuls in theory"", which is the line taken by this work?

Limitations:
There is a discussion of some limitations in the appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents Hydra, an innovative framework that builds upon the Mamba model by introducing bidirectional capabilities. Hydra's approach centers on a matrix mixer perspective, which allows it to consolidate various sequence models, including Transformers and structured state space models, into a unified framework. The primary strength of Hydra is its capacity to surpass other sequence models in non-causal tasks while retaining efficiency and expressiveness. The study demonstrates how Hydra's novel bidirectional methodology and matrix parameterizations effectively enhance the performance of sequence models.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- The authors proposed a novel framework, Hydra, which extends mamba with bidirectional capabilities and presents an interesting perspective on improving sequence models.
- The proposed method of matrix mix offers a cohesive understanding of various sequence model architectures, which also offers valuable insights into how matrix parameterizations and sequence alignment affect model flexibility and performance.
- The paper is well-structured, making it easy for readers to follow the development of the Hydra framework and its contributions to sequence modeling.
- Abundant experiments covering both language and vision tasks illustrate the efficacy of the proposed method.

Weaknesses:
I don't identify a specific weakness, but I have a few questions regarding efficiency. Please see the following section.

Limitations:
See Questions.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces the concept of matrix mixer and sequence alignment for explaining recent sequence models including Transformer, linear transformer, and Mamba. It also proposes a quasiseparable matrix mixer (Hydra) as an alternative to the bidirectional SSM model. The experiments show that the quasiseparable matrix structure performs better than others including low rank (linear attention), attention, and dense. Also, Hydra outperforms other naive bidirectional approaches and some baselines for masked language modeling and image classification.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The idea of designing a structure matrix for a bidirectional case is reasonable and effective while the implementation is simple.  
2. The explanation of different matrix mixers, their relation to other methods, and the advantages/disadvantages are clear. This is very informative. 
3. The ablation study shows how different matrix mixers perform and the benefit of quasiseparable matrix structure.

Weaknesses:
1. The presentation of the paper needs to be improved. 
    1. It contains unnecessary details. The main contributions of the paper are matrix mixer, sequence alignment, and Hydra, but the focus diverges throughout the paper, especially in Section 2. Sections 2.3 and 2.4 can be moved as a side note after introducing Hydra.
    2. The purpose of introducing Cauchy and Vandermonde matrix mixers is unclear. How can this be useful or helpful in some ways? It's not clearly explained or shown in the experiments. 

Overall, the current representation makes it difficult for the readers to understand the core contributions. 

2. The experiments shown in the paper are limited. The main comparison table (Table 4) does not include any recent transformers or mamba-based models. Also, the experiments include one example of non-causal language modeling (masked language modeling) and image classification as applications for Hydra (bidirectional model settings). Could authors show at least one more example application that Hydra can be useful and better than other comparable models?

Limitations:
The limitations are discussed and are reasonable.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
Most of sequence models include the token mixers and the channel mixers, and this paper provides a detailed summary. They also identify the matrix parameterization is crucial for recent SSMs. Therefore, they extend the Mamba model by adding bidirectional quasiseparable matrix mixer. The experiments on GLUE benchmark and ImageNet data verify their method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper summarizes the previous relevant methods very well, as illustrated in Table 1. The authors also claim that the following two properties are important for sequence aligned matrices: data dependency and extendability. The former one is a well-known propoerty.

2. The proposed quasiseparable matrix mixer is simple and easy to understand. It is implemented through two Semiseparable matrixes.

3. The provided results show the advancement of their method.

Weaknesses:
1. The authors claim that the sequence model should be extended beyond their trained length, but they didn't provide the corresponding results.

2. I am curious about the computational complexity of Hydra compared with Mamba. Because they use two SSD operations as shown in Figure 4. The ablation studies are also necessary for the different variants.

3. Please compare with the most advanced methods, such as xLSTM. It also expands the hidden state into a matrix form.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
clAFYReaYE;"REVIEW 
Summary:
This paper provides a detailed application of ideas from fractal geometry to natural language data, using language models to compute the relevant information-theoretic properties.  They find, in particular, tell-tale evidence of self-similarity (common structure across scales) and long-range dependencies.  A particularly interesting result, in my opinion, is that using some of the estimated fractal parameters can help predict LMs' downstream performance over and above what can be predicted from raw LM performance (bits-per-byte), suggesting that some of this performance can be explained by their discovery of this fractal structure.  While the paper can be a bit dense and contains a huge range both of theoretical background and experimental results that make it a little hard to read, I do believe that it pays off to understand and makes a nice contribution that will be of interest to many people in the field.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
* Provides a thorough and detailed application of fractal geometry to natural language data.
* Use of the new fractal metric helps predict downstream task performance more than just BPB alone.
* Shows that the main findings are robust to the choice of LLM that is used for estimating information-theoretic properties of text.

Weaknesses:
* Could cite more literature about e.g. dependency lengths (https://doi.org/10.1073/pnas.1502134112) and other known properties of natural language.  For instance, ""duality of patterning"" (https://doi.org/10.1515/langcog-2012-0015) has been used to refer to the fact that morphemes->words and words->sentences exhibit similar structural properties, which is akin to self-similarity. 
* Although the paper is mostly self-contained, it can be very dense for readers not very familiar with the mathematics of fractals.

Limitations:
Yes; I appreciated especially the mention of the results being English-only

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduce a new perspective to that language is self-similar and predictability and self-similarity together imply long-range dependency, based on empirical analysis across different scales of LMs and information theoretic views. This new perspective may enable us to understand the strong capabilities of current causal LLMs. They studies three parameters, self-similarity (Hölder) exponent, the Hurst parameter, and the fractal dimension, as well as Joseph exponent. They also introduces a new metric that can more precisely approximately downstream performance than BPB.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- This paper provides some new interesting perspectives to advance our understanding of how LLMs acquire such strong capabilities after large-scale pre-training using simple autoregressive training objective. 
- They conduct large-scale analysis using LMs with different scale in three different model families. 
- They also empirically found that a median Hurst exponent can be a more reliable indicator of downstream performance than perplexity-based BPB. While perplexity has shown to strongly correlated with downstream performance, prior studies also show their limitations on predicting downstream performance, and this work may encourage future work to use this new metric instead.

Weaknesses:
- To my understanding, this work views language  as a  mere sequence of negative log probabilities while prior studies in linguistic / NLP often consider much more rich structures of languages and am not fully convinced the validity of analysis.
- While the analysis includes three models, none of the models’ checkpoints aren’t publicly available (i.e., PaLM and PaLM2 and their newly trained T5-decoder only models) and followup work may not be able to reproduce the results. I’m curious why the authors didn’t test other models with openly available model checkpoints such as Llama 2, 3 / OLMo / Pythia. 
- Overall I found the paper is a bit hard to follow (e.g., Introduction discusses prior literature in depth before providing high-level overview or motivation, Experimental setups come write after preliminaries). Some restructures of sections or writing may improves readability of the paper.

Limitations:
The authors provide limitation sections that include some of the limitations I was thinking about.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper draws connections between fractal patterns and language by evaluating properties such as self-similarity and long-range-dependency. Using a range of LLMs, they estimate the Holder exponent, Hurst parameter and fractal dimension for language from different domains, including web text, code, and math problems. They show that these parameters may have connections with LLM learning ability, demonstrating that using the median Hurst parameter can improve prediction of downstream model performance over only using bits-per-byte.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The authors estimate fractal parameters across a range of datasets and LLMs, showing that they are fairly robust to choice of LLM and domain. For domains with significant deviation, such as DM-Mathematics, the authors are able to attribute this to the dataset's lack of long-range dependency. 
- The authors show that the Hurst parameter captures useful information for predicting downstream performance that is not captured by BPB alone.

Weaknesses:
- Even though fractal parameters were found to be consistent across LLMs, I think their stated contribution (this 'establish[es] that language is self-similar and long-range-dependent') is far too strong a claim. As the authors mention, their method ignores many important facets of language, such as semantic nuance, and is reliant on the current state of LLMs' ability to model language. 
- I am unclear how future work can build upon these insights about language, especially that 'exploiting self-similarity more directly' could lead to further LLM optimization.

Limitations:
Yes, the authors discuss that their analysis is limited to English data and fails to capture the semantic meaning of language. However, as mentioned in Questions section, the possible connection between self-similarity and parameter-sharing is not obvious to me and could be further explained.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors try to reveal the existence of fractal structures to handle language in language modeling using recent language models based on the next token prediction. For that purpose, the authors rely on several aspects of fractal structures, which are self-similarity, long-range dependence, and information-theoretic complexity. The authors describe these aspects using metrics for typical phenomena under the assumption of fractal structure, which are self-similarity exponent, Hurst parameter, fragmental dimension, and Joseph effect. The experimental results show that the characteristics of modeling texts of ArXiv, Github, and Wikipedia data in The Pile validation split, whose tokens are longer than 4K, are along with their assumption. In the analysis of the downstream tasks, BBH, MMLU, and GSM8K also show that the task-solving performances are predictable from the language modeling performance based on their assumption, except for considering sequence lengths in training.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Providing an assumption to unify characteristics of interpreting language and language modeling under the variously used approach, next token prediction may help solving various kinds of tasks related to natural languages.
- The analysis is not only restricted to language modeling and the authors investigate the performance correlation between language modeling and downstream tasks based on their assumption.

Weaknesses:
- The target language is limited to English. Thus, the validity of expanding the insights in the paper to other languages is uncertain.
- The assumed baseline in this analysis is a random sequence. The authors can use finite-state automatons (FSAs) or Context-free grammars (CFGs) to make sequences that are random but close to languages. Moreover, to focus on the success of recent pre-trained language models, you should have prepared N-gram language models as baselines.
- How strongly the observation results fit the assumed distribution is not calculated mathematically, like model selection by Information Criteria such as Akaike's Information Criteria (AIC) and Bayesian information criteria (BIC).
- There is a gap between the ability to mimic the characteristics of language and showing the required knowledge for questions. Thus, discussing the correlation between the performance of language modeling and downstream tasks is limited from this viewpoint. Hence, targeting generation tasks like summarization, story generation, and machine translation are more suitable for deepening the analysis.

Limitations:
The limitation does not include the gap between language modeling and solving downstream tasks.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
PSubtZAitM;"REVIEW 
Summary:
This paper studies how to evaluate policies where source and target sites have distribution shifts. The authors introduce identification criteria for the effectiveness of policies, and develop doubly robust estimators that achieves fast convergence. The results are also generalized to multiple source datasets. Simulation results are provided to show the effectiveness of the method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well written and easy to follow. The setups are clearly introduced and motivated, the assumptions and methodologies are accurately stated. 

2. The proposed framework is general to cover both two domains and multiple domains.

3. The empirical results seem good and well aligned with the theories for both synthetic simulation and real world datasets.

Weaknesses:
I am not an expert in this field and not familiar with causal inference. I find no major weaknesses. Some minor weaknesses are on the empirical evaluations. For example, the non-synthetic experiments are only conducted on ACTG 175 clinical trial dataset. Experimenting on other different datasets will enhance the empirical results.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper develops novel graphical criteria and estimators for evaluating the effectiveness of various policies by integrating data from multiple experimental studies.  Through theoretical analysis and empirical validation via simulations, the paper demonstrates the robustness and fast convergence of the proposed estimators.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors develop nonparametric identification criteria to determine whether the effect of a policy may be expressed through an adjustment formula from two separate distributions induced by policy interventions collected from different populations. Further, they generalize the identification criteria and propose a general multiplier robust estimator applicable to the evaluation of policies from multiple source datasets.

Weaknesses:
The paper is difficult to follow. It is very hard to understand. Some grammatical mistakes and confused notations, e.g.,
1. line 71, ‘variable $\mathbf{Z}$ For example’ -> ‘variable $\mathbf{Z}$. For example’
2. unify the notations if necessary, try to avoid writing $\bigtriangleup_{ij}$ and $\bigtriangleup_{i, j}$ simultaneously.
For the experimental section, please give detailed descriptions on the simulated experiment and the empirical experiment (if the descriptions are too long, please move to the appendix), which allows reader to understand the advantages of the proposed framework.
For the theoretical section, the authors first give study when combining two experiments, followed by the study when combining multiple experiments. Honestly, the materials in the two studies are similar. Authors can unify the two studies: presenting the study when combining multiple experiments followed by the two examples (example 2 and example 3). This can release more space to discuss the core components.

Limitations:
The paper focuses on combining two experiments for policy evaluation only. It would be better if the authors can discuss the optimal policy when combining two experiments.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies off-policy evaluation in a transfer learning setting with multiple source datasets collected from observational and/or randomized studies. The objective is to evaluate the effect of a target policy on a possibly different target population. To achieve this, the author(s) assume at each time point, at least one source population shares the same distribution of the dynamic variable given the historical covariate-treatment pair as the target population. This ensures the identifiability of the target population's policy value, even no source population perfectly matches its entire distribution over time. The author(s) further propose doubly robust estimators, investigate their theoretical properties and finite sample performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
**Originality**: To the best of my knowledge, the proposed identification formula and the proposed estimators introduced have not yet appeared in the existing literature.

**Quality**: The theorems established seem to be correct, and the methodologies proposed are theoretically sound and potentially useful.

**Clarity**. Overall, the paper is well-organized and easy to follow.

Weaknesses:
I have two major concerns, regarding the technical assumptions and numerical experiments, along with several moderate comments. I detail them one by one below:

**Assumptions**. The assumptions might appear overly realistic:

* **Same number of studies to horizon**. In addressing a $T$-horizon off-policy evaluation (OPE) problem, the author(s) assume that there are exactly $T$ studies, each corresponding to a time point $t$, where exactly one study per time point shares the same outcome distribution (the distribution of $Y_T$ if $t=T$ and that of $W_{t+1}$ otherwise) with the target population at that time. These settings seem unrealistic in practical scenarios. It would be beneficial for the author(s) to provide examples of real applications that validate this assumption. The current analysis using the ACTG study seems artificial; I will elaborate on this concern in more detail later.
* **Knowledge of matching source population**. Additionally, the author(s) seem to require prior knowledge of which source population matches the target population’s outcome distribution at each time point. In practical scenarios, while we may have access to multiple studies, it is typically unknown which one aligns with the target's outcome distribution at each time point. A more realistic approach would adaptively learn which studies are similar to the target distribution at each time based on the data. This adaptive learning scenario, in my opinion, would better fit real-world applications.
* **Identical distributions**. Although the outcome distributions between the source and target populations may be similar at each time, they are not necessarily identical. Even if the distributions are not exactly the same, as long as the differences are minimal, it is reasonable to use source data for transfer learning remains. Recent studies have addressed such distributional shifts using regularization or adaptive weighting: 
    - https://arxiv.org/pdf/2112.09313
    - https://arxiv.org/pdf/2111.15012
    - https://arxiv.org/pdf/2406.00317

**Numerical experiments**. The experiments are overly simplified:

* **Single-horizon settings.** While the paper studies multi-horizon dynamic treatment regimes, the simulations are conducted in a single stage setting. I would suggest to use D4RL benchmark datasets or OpenAI Gym environments to evaluate the proposed methodologies in multi-stage studies.
* **Lack of competing methods.** There are some naive estimator to consider in these transfer learning settings. However, the author(s) did not include them in the experiments. For instance, one can ignore the differences in the outcome distributions and assume the multiple datasets come from a single population, with a mixture of multiple behavior policies that generates the action. 
* **Real data**. The ACTG dataset is generated under a single population. To evaluate the proposed methodology, the author(s) manually created a second population and a target population. It would be better if a real ""multi-source"" dataset could be used.

**Related works**. 
* Under the identical distribution assumption, this paper mainly considers settings with covariate shift. There are some recent works that studied similar problems, although in single-stage settings, e.g.,  https://onlinelibrary.wiley.com/doi/epdf/10.1111/biom.13583. 
* In the related work section, the author(s) argued that their work can be viewed as a bridge between causal inference and OPE by  leveraging formal theories in causal inference to solve OPE problems. In that sense, there are prior works that similarly integrated both fields. For instance, standard policy evaluation methods in the RL literature uses the backdoor adjustment to learn the Q- or value as a function of the state, to address the confounding effects (see e.g., https://web.stanford.edu/class/psych209/Readings/SuttonBartoIPRLBook2ndEd.pdf). Meanwhile, other studies have applied the front-door adjustment formula for OPE in the presence of unmeasured confounders (https://www.tandfonline.com/doi/full/10.1080/01621459.2022.2110878). Finally, some works have leveraged double negative controls for OPE (https://cdn.aaai.org/ojs/6590/6590-13-9815-1-10-20200520.pdf). 

**Typo**: $Z$ is used to denote the dynamic covariate in Section 2. However, this notation has been replaced with $W$ starting from Section 3.

Limitations:
The limitations have been discussed in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work presents a method for evaluating effectiveness of policies across multiple domains using a new graphical criteria and estimators  by combining data from multiple experimental studies. The authors report error analysis of the proposed estimators that gives provides fast convergence guarantee, and additionally share simulation results as well as empirical verification on real data.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The targeted problem seem to be of importance, and the proposed method provides theoretical error and convergence analysis, backed by empirical results both based on simulation and real data.

Weaknesses:
The the proposed method seems to have important potentials in real world applications, though the majority of the paper has been dedicated to theoretical analysis and the empirical evaluations are very limited.
There is little discussion on how this could impact realworld problems, and there is not a detailed analysis on the empirical results of the results on ACTG175 dataset.

From figure 3.c, it can be seen a contradictory message that the proposed DML method does not perform better than OM on all datasets, though there is no discussion on that. 

also authors state that ""simulations on real and synthetic data are provided for illustration purposes only."" which brings the question to what extend this approach is useful in real world scenarios.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

";1
w6q46IslSR;"REVIEW 
Summary:
This paper investigates the training dynamics of a single-layer transformer followed by a single MLP layer on a synthetic binary classification task, where the objective is to identify the co-occurrence of two specific tokens in the input sequence. They analyze the gradient flow dynamics for the case that all the attention parameters (key, query, value) and the linear layer all trainable and show that the model can achieve low loss despite the non-convexity of the objective. They identify two phases in the training, 1) the MLP aligns with the two target tokens at the start of the training and the model learns to classify all the samples correctly, 2) the attention and MLP parameters update to increase the classification margin and drive the loss to zero. They also run a small scale numerical experiment in their synthetic setup tp confirm their analysis.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper makes no restricting assumptions on the weights of the transformer model and performs the analysis on the joint optimization of all the parameters.

Although the paper and its proof are notation-heavy, the authors have broken down the complexity of the proof and notation in the main body to clarify the steps needed to prove the results.

Weaknesses:
There are some restrictive assumptions on the synthetic data model: The vocabulary set $d$ is considered to be larger than the number of training tokens, which is not the case in realistic setups. Thus, some tokens are not visited at training time. Also, they assume, apart from the two target tokens, the remaining tokens appear at maximum once in the training set.  

The proof outline in the main body helps in understanding the high-level steps involved. However, it could still benefit from additional clarifications on some intermediate steps. For instance, in phases 1 and 2, it's mentioned how the alignment of the MLP with the target tokens $G^{(t)}(\mu_{1,2})$ behaves during training. However, it's not clear how this connects to the evolution of the attention scores in phase 2 as stated in Lemma 4.7.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the training dynamics of a single hidden layer transformer network (self-attention + linear MLP) trained on a binary word cooccurrence task. Specifically, given a data matrix $X \in R^{d \times L}$ representing L ""words"" (each column of X is a word vector of dimension d), the model must output +1 if words 1 and 2 both occur in X, and -1 otherwise. The paper shows that a transformer layer is able to learn this task, and that the training occurs in two stages: First, the linear MLP layer learns to classify data points correctly by positively aligning with the embeddings for words 1 and 2 (but without making large changes to attention matrices). Second, it drives the loss down further by using the attention matrices to positively correlate q,k,v for words 1 and 2, and anti-correlate the q,k,v for a common word (denoted word ""3"" in the paper) relative to words 1,2.  After these phases, both the training and generalization losses go to zero (as long as embedding dimension is large enough).

Overall, I found the results interesting and insightful, though not very surprising, and the practical implications of these results were not very clear to me. Thus, I currently recommend weak accept. Importantly, my primary research area is not learning theory, so my knowledge of the related work is relatively limited, and thus my review confidence is relatively low.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- It is interesting to see that the training dynamics for this word cooccurrence task can be analyzed rigorously, with relatively few assumptions.
- The theoretical results are validated with a nice synthetic experiments, that demonstrates that the two phases predicted by the theory do occur in practice.

Weaknesses:
- This word cooccurrence task is very simple, and thus it is not surprising that a single transformer layer can easily learn it.
- Only full gradients are considered, whereas transformers are typically trained with mini-batch Adam(W).

Limitations:
Yes, limitations have been discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This article delves into the gradient flow dynamics for detecting word co-occurrence, demonstrating that the gradient flow approach can achieve minimal loss. The training process commences with random initialization and can be delineated into two distinct phases.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- This article noticed an interesting phase transition during training in this special setting and demonstrates it with solid calculation and experiments.
- A new property of gradient flow is noticed and contributes to prove near minimum training loss together with the analysis of softmax.

Weaknesses:
The setting of empirical experiments is also simple and ideal and readers may have no idea if this is a general phenomenon during training for detecting word co-occurrence.

Limitations:
The semantic mechanism this article focuses on is not general enough in NLP and the the results cannot give instruction useful enough to guide the training process of Transformer.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
B29BlRe26Z;"REVIEW 
Summary:
This work studies distributed model training with a parameter server; contributions are theoretical:
* **Assumptions**: $L$-smooth convex objectives, stochastic gradient estimates, heterogeneous worker distributions, bounded difference in expected gradients of local workers' objectives at a global minima, denoted $G^\star$.
* **Main result**: Given a fixed set of communication rounds $R$ with the parameter server, $M$ worker machines, and a budget of $K$ local gradient queries per worker in each communication round, this work presents a simple method for improving local SGD. Given $L$-smooth convex objectives with stochastic gradients and heterogeneous worker distributions, the local-SGD variant proposed in this work achieves more favorable convergence than mini-batch SGD (where the $K$ stochastic gradient queries are used to compute a larger mini-batch gradient at each worker). The proposed method converges in $\mathcal{O}(MK^{-1/2})$ rounds, whereas mini-batch SGD converges in $\mathcal{O}(MK)$ rounds.
* **Method**: The proposed method extends anytime GD (Cutkosky, ICML'19) to the distributed local-sgd parameter-server setting. In short, regular local-sgd with $K$ local steps, but where stochastic gradients are computed at an exponential moving average (ema) of the model parameters. The parameter server averages both the primal and ema parameters, and sends back the exact average to each worker at the end of each round.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
**Clarity**
* This work is extremely clear and well written. I have skimmed the main proofs in the appendix, but the proof sketches in the main paper provide enough detail to understand and follow the primary logic and intuition behind each result.

**Quality**
* I have quickly gone over the proofs in the appendix, and the work appears to be technically sound.

**Originality**
* To the best of my knowledge, this is the first such extension of anytime GD to the distributed stochastic local update setting, and the first result demonstrating the improvements of local sgd over mini-batch sgd in general smooth convex settings. Previous studies were either devoted to the non-convex setting or quadratic objectives, or assumed bounded second moments.

Weaknesses:
**Significance**
* No numerical experiments conducted to explore the convergence of the proposed method or generalization capabilities. To increase impact and adoption, researchers may be interested in the generalization performance of the method; i.e., going beyond the number of communication rounds required to decrease training error. Would design considerations (e.g., choice of $\alpha_t$ schedule) change in such a setting?
* Not a major issue for a theoretical paper, but current hyper-parameter choices requires knowledge of problem parameters ($\sigma$ and L-smoothness constant).

**Clarity**
* Minor complaint is that the proof sketches do not always correspond to a similar logic used in the actual proofs themselves. For instance, (16) in the proof sketch in the main paper was obtained by assuming a somewhat monotonic iterate sequence, but such a result is never explicitly proven in the appendix. Instead, Lemma 3 in Appendix H (which bounds the sum of the iterate sequence using the T times the starting error, plus additional terms depending on the gradient magnitude and variance; which should indeed converge) is used to arrive at (33) in the proof of Theorem 2 in the appendix, which corresponds to (16) in the main paper.

Limitations:
N.A.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new federated learning algorithm called Slowcal-SGD, which basically introduces anytime-SGD into federated learning setting. The authors provide solid convergence analysis and fruitful insights on the new algorithm. They show the algorithm can provably beat both mini-batch SGD and local SGD in the heterogeneous data setting. No experiments are provided, though.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- For each theorem, the authors provided insightful discussions, explaining why the algorithm works better.
- The proposed algorithm is novel and neat.

Weaknesses:
- It'd be better to define ""query"". It could be controversial. For example, why is computing the gradients wrt a mini-batch a single query instead of B (batch size) queries?
- No experimental results are provided. So it's hard to tell whether the proposed algorithm works in practice.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes SLowcal-SGD, which is a distributed learning algorithm that builds on customizing a recent technique for incorporating a slowly-changing sequence of query points, which in turn enables to better mitigate the bias induced by the local updates. Theoretical proof is given on the proposed algorithms.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The idea of combining local updates and mini-batch SGD to improve the data heterogeneity case is interesting.
* The author provides intuition on why slowcal-SGD is useful, and the comparison of proposed algorithm and other algorithms in table 1 is clear.

Weaknesses:
* While the paper is heavy on theory, it has no validation on any synthetic or real-world data. It should not be hard to verify this since this is convex problems and there are multiple ways and open sourced code to produce heterogeneous data.
* The rates shown in table 1 is a little confusing. Comparing to accelerated mini-batch SGD, what is the advantage of proposed slowcal-SGD?
* The assumptions of equation (1) - (3) are somewhat strong if we discuss the data heterogeneity. What is the main difference between proposed slowcal-SGD and general variance reduction SGD? (which doesn't need equation 1-3).

Limitations:
The paper is theory-driven and does not have negative societal impact.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
8W5ADJOKcv;"REVIEW 
Summary:
In this paper, the authors propose a variant of MDS that is able to deal with data with a non-Euclidean structure. The main idea is to generalize the inner product to a bilinear form, hence admitting some relationships corresponding to ""negative eigenvalues,"" interpreted as in an inner product form $u^TAv$. Although a closed-form solution is not possible, the authors optimize over a lower bound. The paper ends with empirical evaluations both on synthetic and real data, with promising results.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well written in general. The introduction and motivation are very clear. Although most of the bounds and some ideas are already in [38], the idea of including negative eigenvalues and its implementation is interesting.

Another strength is the availability of the code.

Weaknesses:
The introduction and problem definition are quite detailed (which is good), but I think that some of these facts may be considered well-known, like basic linear algebra facts. This could be used to include more specific material with the page restriction.

I also think that the experimental section needs a subsection to interpret the embeddings. Throughout the paper (and supplementary material) there are numerical results reported, but the embeddings are never plotted. At least for some toy example.

Limitations:
Although there is no specific section for this, some limitations are commented on throughout the manuscript.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces Neuc-MDS, a novel extension of classical Multidimensional Scaling (MDS) designed to handle non-Euclidean and non-metric dissimilarities in datasets. The goal is to create accurate low-dimensional embeddings while minimizing the STRESS (sum of squared pairwise error)

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	Neuc-MDS extends the concept of the inner product to a broader class of symmetric bilinear forms, allowing the incorporation of both positive and negative eigenvalues from the dissimilarity Gram matrix. This generalization helps capture the underlying structure of non-Euclidean data more effectively than classical MDS, which typically discards negative eigenvalues.
2.	Neuc-MDS is specifically designed to handle non-Euclidean and non-metric dissimilarities, making it versatile for a wide range of applications where traditional MDS falls short. The method is backed by a thorough theoretical analysis, providing guarantees for minimizing STRESS. This includes a detailed decomposition of the STRESS error and the demonstration of optimality in eigenvalue selection.
3.	Neuc-MDS is capable of working with a variety of dissimilarity measures that are commonly used in practice but are not Euclidean, such as cosine similarity, Hamming distance, and Jaccard index. This broadens its applicability to different fields and types of data.

Weaknesses:
1.	The theoretical guarantees provided by Neuc-MDS are based on certain assumptions about the data and dissimilarity matrices. If these conditions are not met in practice, the performance and guarantees may not hold.
2.	Some theoretical results rely on properties of random matrices (e.g., Wigner's Semicircle Law). The applicability of these results to structured or real-world datasets, which may not exhibit such random properties, is unclear. The use of Lorenzian distance or other non-standard measures may not satisfy traditional distance properties (e.g., triangle inequality), potentially leading to confusion or misinterpretation in applications that rely on classical distance metrics.
3.	The algorithm involves eigenvalue decomposition and optimization over eigenvalue selections, which can be computationally intensive for large datasets. The scalability of Neuc-MDS, especially for very large datasets, may pose a practical challenge.
4.	The implementation of Neuc-MDS involves several complex steps, including eigenvalue selection and optimization. This complexity may hinder its adoption by practitioners who require straightforward and easily implementable solutions.

Limitations:
See weakness

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce Non-Euclidean-MDS (Neuc-MDS), an extension of Multidimensional Scaling (MDS) that accommodates non-Euclidean and non-metric outputs, efficiently optimizes the choice of (both positive and negative) eigenvalues of the dissimilarity Gram matrix to reduce STRESS. The results seem to be promising and the error analysis looks solid.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. the error analysis seems solid.
2. the topic is important which can extend to non-Euclidean MDS
3. the experiments look promising

Weaknesses:
1. The overall writing is not easy to follow, for example from line 119-131, your statement is long, but I am still confused how to construct A. Apparently if A is Identity matrix, it degenerates into traditional one. Your contribution is to say A doesn't need to be Identity or even PSD, but how to construct A remains unknown to me.
2. I don't agree that A can be non-PSD, in which case the inner product of v with itself can be negative, I feel wired.
3. Line 137 you claim X can be recovered from B, however this is not precise. 
4. the formatting can be improved, for example the fontsize Table 3 can be reduced and figure 3 can be better.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Non-Euclidean Multidimensional Scaling (Neuc-MDS), an extension of classical Multidimensional Scaling (MDS) that can handle non-Euclidean and non-metric data. The key ideas and contributions are:

1. It generalizes the inner product to more general symmetric bilinear forms, allowing the use of both positive and negative eigenvalues of dissimilarity matrices.

2. Neuc-MDS efficiently optimizes the choice of eigenvalues to minimize STRESS (sum of squared pairwise errors). The authors set up the problem as a quadratic integer program but show that it has an optimal solution and that it can be found in a greedy fashion.

3. The authors provide theoretical analysis of the error and prove optimality in minimizing lower bounds of STRESS.

4. They introduce two algorithms: Neuc-MDS and an advanced version Neuc-MDS+.

5. Theoretical analysis is provided for the asymptotic behavior of classical MDS and Neuc-MDS on random symmetric matrices.

6. Experimental results on 10 diverse datasets show that Neuc-MDS and Neuc-MDS+ outperform previous methods on STRESS and average distortion metrics.

7. The proposed methods resolve the ""dimensionality paradox"" issue of classical MDS, where increasing dimensions can lead to worse performance.

8. The approach is applicable to both image and text data, and can handle various non-Euclidean dissimilarity measures.

9. The paper provides detailed proofs, algorithm descriptions, and experimental results in the appendices.

Overall, this work extends MDS to non-Euclidean spaces, providing both theoretical guarantees and practical improvements over existing methods for dimension reduction and data embedding tasks.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is well-written and it clearly outlines the problem and the solution. The analysis is non-trivial and the experiments are thorough.

Weaknesses:
While the paper presents a novel approach with several strengths, there are a few potential weaknesses or limitations that can be identified:

1. Computational complexity: For large datasets, the method may still be computationally intensive, as it requires eigendecomposition of the full dissimilarity matrix. While the authors mention the possibility of using approximation algorithms for partial SVD, they don't provide detailed analysis or experiments on very large-scale datasets.

2. Limited comparison with non-linear methods: The paper primarily compares Neuc-MDS with classical MDS and other linear dimension reduction techniques. A comparison with popular non-linear methods like t-SNE or UMAP could provide more context on its performance relative to the state-of-the-art in dimension reduction.

3. Interpretability: The use of general bilinear forms, while mathematically elegant, may make the resulting embeddings less interpretable compared to Euclidean embeddings, especially for domain experts not familiar with non-Euclidean geometries.

4. Downstream tasks: The paper focuses on the quality of the embedding itself (via STRESS and distortion metrics) but doesn't extensively explore how these embeddings perform in downstream machine learning tasks compared to other methods.

5. Negative distances: The method can produce negative distances, which may be problematic for some applications or require special handling in downstream tasks. While this is mentioned, strategies for dealing with negative distances are not fully explored nor is the impact on ""real"" data sets discussed.

6. A discussion of just how non-Euclidean ""standard"" data sets are would be useful. Is this problem of non-Euclidean distances a substantial one or not? 

7. This brings me to a more detailed discussion of the experiments: I'd be interested in seeing a quasi-synthetic example of a real data set with non-Euclidean distances used (say, graph distances from a nearest neighbor graph). What impact does that make on the results?

Limitations:
not applicable.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
yxOrSmS5wR;"REVIEW 
Summary:
The paper proposes AV-Cloud, a framework for high-quality spatial audio rendering in 3D scenes without relying on visual cues. AV-Cloud addresses issues in current audio-visual rendering methods, such as audio lag and dependence on visual rendering quality, by introducing Audio-Visual Anchors and the Audio-Visual Cloud Splatting module. These components facilitate the generation of viewpoint-specific spatial audio synchronized with visual content. The method demonstrates superior performance on multiple benchmarks, outperforming existing baselines in audio reconstruction accuracy, perceptual quality, and acoustic effects.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The concept of using Audio-Visual Anchors and Cloud Splatting to decouple audio rendering from visual rendering is interesting.
2. The paper demonstrates comprehensive experimentation and robust evaluation across multiple benchmarks.
3. The paper is well-structured and the presentation of the framework is clear. The figures and supplement examples help the readers better understand.
4. The proposed method addresses critical issues in real-time audio-visual rendering.

Weaknesses:
1. The mathematical formulation of the Audio-Visual Cloud Splatting module could be more detailed. For instance, Equation (2) introduces the softmax function applied to the relative vectors and visual features, but the reason behind this specific formulation and its implications are not sufficiently explained. Clarifying how the weights $a_{ki}$ are computed and how they influence the final output would enhance understanding.
2. The technical derivation of the Spatial Audio Render Head (SARH) lacks depth. Specifically, the process described in Equations (4) and (5), where the mixture mask $m_m$ and the difference mask $m_d$ are used to compute the left and right channel outputs, is not fully elaborated. The significance of these masks and their impact on the final audio quality are not clearly discussed. Additionally, the role and impact of the convolution modules within the residual structure (Figure 3) are not sufficiently explained.
3. While the method shows strong performance on benchmarks and some real-world examples, the provided examples are too idealized and lack challenging elements like interfering sound (e.g., crowd noise). I think the robustness of AV-Cloud in more complex and noisy real-world environments should also be validated.

Limitations:
The authors mention the limitations of their approach's challenges and potential drawbacks. The reliance on camera calibration and the potential issues with noise in real-world audio recordings are noted. Additional imitations can be found in the Weaknesses section

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
A novel approach for rendering high-quality spatial audio in 3D scenes, called AV-Cloud, is proposed. This method synchronizes with the visual stream without relying on or being explicitly conditioned by visual rendering, enabling immersive virtual tourism through real-time dynamic navigation of both audio and visual content. Unlike current audio-visual rendering methods that depend on visual cues and may suffer from visual artifacts causing audio inconsistencies, AV-Cloud overcomes these issues. It uses a set of sparse AV anchor points, forming an Audio-Visual Cloud derived from camera calibration, to represent the audio-visual scene. The Audio-Visual Cloud allows for the generation of spatial audio for any listener location. A novel module, Audio-Visual Cloud Splatting, decodes these AV anchor points into a spatial audio transfer function for the listener’s viewpoint, which is then applied by the Spatial Audio Render Head module to transform monaural input into viewpoint-specific spatial audio. This approach eliminates the need for pre-rendered images and efficiently aligns spatial audio with any visual viewpoint. The results are satisfying.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The AV anchors strategy seems to be interesting and effective for audio-visual scene representation. The Audio-Visual Cloud Splatting is novel for AV tasks but more likely to be a Q-former.
2. The experiment results are good and ablations are clear.

Weaknesses:
As I mentioned in the strengths, the Audio-Visual Cloud Splatting seems to be a Q-former like module.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper explores the problem of generating 3D audiovisual scenes – that is, generating 3D scenes with spatial audio. The proposed approach, AV Cloud, uses anchor points obtained from Structure-from-Motion (SfM) points. The anchors are then used with an AV Cloud splatting module which decodes the visuals and the audio. Experiments are done on RWAVS and Replay-NVAS with comparisons done with several prior works.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
– 3d audiovisual scene generation is a really interesting problem to solve. WHile there is considerable literature on visual scene generation, generating 3d visual scene is an interesting problem with real-world applications. 

– The model claims to be able to generate the audio and the visuals in parallel. Essentially unlike prior work it decouples the generation of two modalities by not using the generated visuals for generating the audio. 

– On objective metrics, the paper claims to make good improvements

---- 
increased score after rebuttal

Weaknesses:
– The paper is a bit difficult to follow – especially the key part of AudioVisual anchor points. 

– First, a short primer on SfM is desirable, even if it is in Appendix. More importantly though, it is not clear why it makes sense to use SfM points and clustering on top of them to model AV anchor points and generation of spatial points. Why does it make sense to use SfM points or anchors derived from them as the starting point for AV generation ? What relation the anchors have with audio which motivates the fact that these anchors can be used for audio generation ? 

– Second, the details of AV anchor points are fuzzy. The visuals are used for SfM points which are then clustered to get the anchors. Where is the audio into picture here ? Are these anchors visual only ? If so, why are we calling it AV Anchors ? 

– In prior works, for example AV-Nerf, there is an an explicit AV-Mapper which learns the audio visual relations through which the spatial audio generatio happens. Here Visual2Audio splatting transformer is expected to model that ? 

– For the subjective tests, it would be good to actually get proper subjective ratings on the generated spatial audio. The current preference numbers are not very informative. Getting the spatial audio rated with respect to their quality and spatial characteristics would be much more meaningful. 

– Since NAF, INRAS and other works are considered here - I think it would be good to reference NACF ([R1]) below. NACF specifically focuses on using visuals and is ideal for comparison. 

[R1] Neural Acoustic Context Field: Rendering Realistic Room Impulse Response With Neural Fields

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
HjeKHxK2VH;"REVIEW 
Summary:
This paper proposes a watermark technique called WaterMax to distinguish LLM-generated texts and human-written texts. WaterMax starts with the watermark detector and asks LLMs to generate a group of candidates from which the one with the lowest p-value determined by that detector is selected as the final output. This work offers a brandnew perspective of text watermarking as there is no formal watermark generator to embed watermark into LLM outputs. Watermax breaks the trade-offs between watermark detectability, quality and robustness, which is carefully discussed in this work with theoretical proof and experiment validation.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. WaterMax offers a brendnew research perspective in the field of text watermarking, where there is no official watermark generator.
2. WaterMax is almost distortion-free as it achieves high text quality on LLM outputs, yet the detectability is preserved.
3. By upgrading the detector, WaterMax can achieve high robustness as well.
4. The superiority of WaterMax is both theoretically proven and experimentally validated.

Weaknesses:
1. The dataset used for experiments is limited to high-entropy text generation, yet in reality there is need to watermark LLM-generated codes.

Limitations:
It would be better to include more datasets, and further improve the time complexity if possible.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a method for watermarking language models through the use of rejection sampling. By sampling and discarding ""chunks"" from the model until the p-value returned by an (arbitrary) detection rule is sufficiently low, the proposed method simultaneously preserves output text quality while achieving strong robustness to a variety of attacks. Crucially, the proposed method does not require any intervention within the model itself (e.g. via logit biasing).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The proposed method is both elegant and flexible.
* The paper is clearly written and adequately describes the proposed method.
* The authors demonstrate strong results in terms of text quality, detectability, and robustness against a reasonable selection of attacks.

Weaknesses:
* A previous LLM watermarking work, ""SemStamp"" [1], is similarly based on rejection sampling. Adding experimental comparisons to SemStamp might therefore strengthen the paper by showing how the proposed method fares against the most directly comparable existing method. Otherwise, the authors should probably cite it.
* The authors claim on line 62 that the method of Kuditipudi et al. is the only watermark explicitly designed for robustness; however, the unigram method of Zhao et al. [2] also appears to provide theoretical robustness guarantees.
* While the authors propose an algorithm to limit search over candidate generations, the computational complexity of the proposed method is somewhat concerning. Based on figure 7 (appendix G), it looks like WaterMax incurs ≤ 40% additional runtime on top of generation at a length of only 256 tokens; for the texts in the ""Mark My Words"" benchmark, the authors report in line 560 that WaterMax requires 5 times the runtime of KGW & Aaronson for watermarked generation. Additional computation is unavoidable with a rejection sampling watermarking scheme, but the authors could address this limitation more clearly within the main paper body.
* A very minor note -- given that the authors consistently compare three watermarking schemes (WaterMax, Aaronson, KGW), it might improve legibility to use a consistent color scheme to refer to these methods across figures.

[1] https://arxiv.org/abs/2310.03991 (NDSS '24)
[2] https://arxiv.org/abs/2306.17439  (ICLR '24)

Limitations:
* See comment on efficiency in ""Weaknesses"" section

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel watermarking scheme for large language models (LLMs). The proposed WaterMax scheme aims to achieve high detectability while maintaining the quality of the generated text, without modifying the LLM's weights, logits, temperature, or sampling technique. WaterMax balances robustness and complexity, distinguishing itself from existing methods that often trade off quality for robustness. The performance of WaterMax is theoretically proven and experimentally validated.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. WaterMax introduces a new detection mechanisms that improve the detectability of watermark in short text, thus preserving the original LLM's token distribution and sampling method.
2. The scheme maintains the quality of the generated text, which is a critical factor in practical applications of LLMs.
3. The paper demonstrates that WaterMax achieves higher robustness and detectability compared to other state-of-the-art watermarking techniques, even under various attack scenarios.
4. The theoretical and experimental evaluations are thorough, covering multiple LLMs and benchmarks. This provides a solid validation of the scheme's effectiveness.

Weaknesses:
1. The method involves generating multiple texts for a given prompt and selecting the most suitable one, which increases computational cost and latency. Although the paper suggests ways to limit this, it remains a potential drawback.
2. Based on the randomness in the sampling strategy, LLMs can generate diverse outputs from the same input. This method requires generating multiple outputs and selecting the most suitable one, which may damage the diversity and quality of the generated text.
3. The method's latency, especially in generating longer texts, could be a limitation in time-sensitive applications.
4. The scheme requires careful tuning of parameters to balance robustness, detectability, and computational cost. This adds complexity to its implementation.

Limitations:
This method may affect the quality, diversity and latency of the output text.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work proposes a new LLM watermarking scheme called WaterMax, that does not modify the distribution or the sampling procedure but uses rejection sampling on multiple generated segments of tokens to cause a high watermark score. A theorem is given to characterize the detector power under attack, given certain independence assumptions. The experiments attempt to demonstrate superior detectability under the constraint of high quality, independent of the temperature/entropy, as well as superior robustness.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- **The approach is novel**: the work investigates a fundamentally different approach from prior baselines which is a valuable contribution in itself. 
- **Thorough analysis**: the work analyzes the proposed method from several perspectives, and both theoretically and empirically, making it a well-rounded study. I especially appreciate the care taken to discuss the underlying assumptions and investigate their violations in practice. 
- **Good writing**: the work is generally very well written, discusses the prior work well, and introduces the method in an understandable way, splitting the different components across different parts of the paper.

Weaknesses:
- **Questionable experimental evaluation**: The experimental setup of the main experiments (Figure 5) is mostly reasonable and the observation about the instability of Aaronson holds up. However, the key claim seems to be that ""KGW used with common/realistic settings is both of lower quality and has lower detectability than WaterMax"". I have several observations here. 
1) This claim is made by observing 1.2x relative perplexity measured with a weak 3B model; it is not clear that this is a reliable estimate of text quality in pratical use-cases (see e.g. https://arxiv.org/pdf/2312.02382). The result would be more convincing if a larger model was used, but more importantly, a SOTA LLM (e.g. GPT4) was used as a judge of responses in at least one setting. The author's claim regarding this is ambigious, are they claiming that GPT4 is unreliable for this task? 
2) The watermarked model itself is a single 8B model. I see two more small models in Appendix J (7B and 4B). On Llama2-7B the variants of KGW that were tried actually have /higher/ quality that WaterMax, so it is not clear if increasing delta would significantly ruin quality but it may lead to high TPR. Regardless, at least one larger model (e.g., 13B) should in my opinion be tried to substantiate the claim.
3) Even in Fig. 5 a non-standard variant of KGW is used with h=6 and gamma=0.5. To make the claim stronger, can the authors repeat the experiment with more standard h=4 and with gamma=0.25 as well? 

- **High computational cost**: while the authors acknowledge this explicitly to some extent, no quantifiable measurement of the computational cost is shown in the main paper, and the appendix suggests that the full experiment took 5x more with WaterMax. This clearly makes WaterMax inapplicable in most real deployments where latency is critical. 

However, despite the 5x slowdown, I still believe the paper would have sufficient merit as an exploration of an interesting idea, if the authors could fill the gaps in the experimental evaluation and make the case regarding quality vs power fully convincing. My current borderline accept score is conditioned on the authors providing these additional results to remove my doubts.

Limitations:
The only comment I have is regarding quantification of the efficiency degradation which I believe should be explicit in the main paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
ZsxZ65YqL1;"REVIEW 
Summary:
This work introduces a benchmark for using LLMs are critics. 

The benchmark covers four settings:
1. Providing feedback
2. Correction of a response with/without feedback
3. Comparison of two responses for a given query
4. Providing meta-feedback (feedback on feedback)

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
Extensive experiments across models and setups.

Weaknesses:
For weakness I would repeat the limitations I list below.

Limitations:
Two minor limitations:
- As far as I can tell structured generation was not used which has been shown to improve performance
- As far as I can tell few-shot performance was not considered.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces CriticEval, a benchmark designed to comprehensively and reliably evaluate the critique ability of large language models (LLMs). It assesses critique capabilities across four dimensions (feedback, comparison, correction, and meta-feedback) and nine diverse task scenarios, using both scalar-valued and textual critiques for responses of varying quality. The benchmark was constructed using a human-in-the-loop pipeline, with initial critiques generated by GPT-4 and refined by human experts. CriticEval employs both objective metrics and subjective evaluation by GPT-4 with human-annotated reference critiques. Key findings from evaluating 35 LLMs include: GPT-4's high correlation with human judgments when using reference critiques, some open-source LLMs approaching closed-source models in performance, and insights into how critique difficulty varies by task type, response quality, and critique dimension.  CriticEval could be used as a comprehensive and reliable tool for assessing LLM critique capabilities.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- CRITICEVAL evaluates critique ability across multiple dimensions (feedback, comparison, correction, meta-feedback) and diverse task scenarios, providing a more holistic assessment than existing benchmarks.
- The benchmark combines GPT-4 generated critiques with human expert refinement and employs both objective metrics and subjective evaluation with human-annotated reference critiques. This approach ensures a more reliable and accurate evaluation of LLM critique abilities.
- The paper presents results from evaluating 35 open-source and closed-source LLMs, offering valuable insights into the current state of LLM critique capabilities. It reveals interesting relationships between critique difficulty and factors like task type, response quality, and critique dimension.

Weaknesses:
- The biggest concern of the dataset is that it relies heavily on GPT-4 for initial critique generation and evaluation. This could introduce a bias favoring models such as GPT-4 and models trained on GPT-4 distilled data. The human-in-the-loop process might not fully mitigate this bias, especially if annotators are influenced by GPT-4's initial outputs. A more diverse set of models or purely human-generated critiques for the benchmark could have provided a more neutral evaluation framework. 
- The paper doesn't adequately address the scalability of CRITICEVAL for evaluating future language models. Additionally, the reliance on GPT-4 and human experts for evaluation might make it challenging for other researchers to fully reproduce or extend the benchmark. 
- Insufficient analysis of failure modes: While the paper presents overall performance metrics, it doesn't delve deeply into the specific ways in which models fail at critique tasks. A more detailed error analysis could provide valuable insights into the limitations of current LLMs and guide future research more effectively.
- Lack of comparison to human performance: The paper doesn't provide a clear comparison between LLM performance and human performance on these critique tasks.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper addresses the need for a comprehensive evaluation of the critique ability of large language models (LLMs) for self-improvement and alignment with human outcomes. Current evaluation methods are critiqued for their limited scope and reliability. The authors propose CRITICEVAL, a benchmark designed to evaluate LLMs across four critique dimensions: feedback, comparison, correction, and meta-feedback, covering nine diverse task scenarios including NLP, alignment, and reasoning tasks. While CRITICEVAL incorporates human-annotated references to enhance reliability, it heavily relies on GPT-4 for evaluations, raising concerns about generalisability across other LLMs. Evaluations of 35 LLMs demonstrate CRITICEVAL's effectiveness but also highlight the inherent difficulties in critiquing complex tasks and the inverse relationship between critique and response quality. Although promising, the reliance on human annotation and the potential biases introduced by using a single LLM for baseline evaluations could limit its broader applicability. The release of datasets and evaluation tools is a positive step towards fostering further research.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The work is a very comprehensive study on the critical evaluation problem and provides insight for those who would be in situations where they would want to build better LLM pipelines for use-cases where accuracy is important for fit for use. 

The exploration of 35 different LLM providers more diversity and incorporating human feedback for quality ranking, it allows for wider evaluation than prior benchmarks. 

I believe the work is well presented, written and argued. It is a very daunting project to get all the pieces together. It is harder, ironically, to evaluate it just as a paper because of all the moving parts, but the authors evaluations across the LLMs are appreciated.

Weaknesses:
Even with the mention of incorporating Chinese, I believe there should be a serious discussion on how such evaluation pipelines and datasets would function for low-resource languages (Chinese is not one). Many of the large LLMs have multilingual capabilities and their generation of responses are more likely to have higher error rates and as such being able to do critical evaluation in such use-cases is important. 

The computational cost and real cost of setting up the CRITICEVAL pipelines should be spelled out.

It is not clear if IRB/Ethical clearance was obtained as the checklist response states that IRB *would* be easy to obtain. This is a concern as such I will be referring this for ethics review. It is appreciated that information was provided in Appendix B and G, but whether IRB was obtained or not should have a clear YES or NO statement.

Limitations:
The limitations are written in Appendix A. They are clear.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This study constructs a comprehensive framework for LLM-based evaluation, encompassing data construction, human/machine annotation, and result analysis. It defines a single evaluation framework that covers various tasks and response types. Although previous studies have evaluated different tasks and response types, they often lacked comprehensive analysis, making it difficult to understand the relationships between various factors during evaluation. This research aims to address this gap. Additionally, it examines the reliability of evaluations based on the types of judge models and target models, as well as the importance of human annotation data.
Through extensive experiments and comprehensive analysis, this study demonstrates consistent trends across various tasks and highlights the importance of utilizing human-annotated data in evaluations.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The proposed framework allows for the evaluation of various tasks and response types.
2. Through various experiments, the high evaluation capability of closed-source LLMs, regardless of response type and task, is confirmed.
3. The importance of human-annotated data is evident, showing consistency regardless of the evaluation model.
4. Additionally, extensive experiments analyze various aspects such as response quality, difficulty, and the capability of reward models as judge models.

Weaknesses:
1. One important metric for evaluation models, the ability to revise its generation with critique, is excluded. As mentioned in the paper, there is active research on improving generation performance through LLM’s self-feedback during the inference stage. It is necessary to evaluate each model’s ability to revise existing outputs when provided with critique.
2. Although this study addresses the translation task, it excludes the critique ability in other multilingual contexts. The critique ability of LLMs in various languages other than English is crucial for ensuring diversity, and additional data collection on this aspect seems necessary.

Limitations:
Yes, this study clearly mentions its limitations in the conclusion.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
gvg8pExqdd;"REVIEW 
Summary:
This paper proposes an entropy model that achieves higher compression rates than previously proposed models in the context of learned image compression. This model combines global, regional, and local information to make better (i.e., more accurate, and therefore requiring fewer bits) predictions with respect to the latent values (quantized) that the autoencoder produces. The authors discuss the method in detail and provide a visualization. The results show an improvement over the existing methods..

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper ran well designed experiments
- The authors do discuss runtime, which is great. I wish more paper did this.
- If I am reading the numbers correctly, the entropy model seems to provide a pretty good speedup over the existing models.

Weaknesses:
- This paper does not seem to be too theoretical, but it's clearly results-focused. I wonder whether a more appropriate venue would afford this a) more visibility to the correct (i.e., compression) audience; and b) be much more well received.

I don't really think that this will be of broad enough interest at NeurIPS.

- When discussing runtime performance, it would be great if the authors could break it down by sub-system. For example, it's useful to know how long does the autoencoder take to encode the image/decode the image. It would be great to know how much time does the entropy model take to predict all values needed to encode/decode. It would also be interesting to know (this is highly optional, but would make this paper much more interesting from a deployment standpoint) do discuss the runtime when ANS is also interleaved in the decoding / encoding process of the image.
- The proposed entropy model shows some slight improvement, but it's nothing exciting. I was hoping to see a 30% improvement given the added complexity, but we got a much smaller number than that.

The biggest issue:
- Since this paper is focused on entropy modeling, which I believe is a great topic, I would have thought that a primary concern would have been reproducibility. Entropy models are famously flaky, meaning that it's quite possible that an image encoded on a V100 GPU may not be decoded by any other device (including CPUs.). As such, I would have liked the authors to go into detail on what mitigations were put in place to ensure that their model can be used on variety of hardware. I truly believe, that this is the difference between a paper that slightly improves theoretical compression rates, and one which makes it possible to apply in practice.

Limitations:
They seem addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
**Disclaimer:** My research direction is not related to image compression. I will try my best to review this work but could be biased. 
The work studies neural image codec. The work proposes an entropy modeling framework that uses contents for forward adaptation without comprising the bit rate. The authors build upon the quadtree partition-based backward adaptation method and introduce additional forward context to address limitations in existing approaches, particularly in the first modeling step.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The work conducts in-depth analysis of the different components in the proposed method。 
2. The proposed method enjoys very efficient decoding while having a better BD-rate.

Weaknesses:
- There are some presentation issue, e.g. line 256-257, the links to citation are missing.

Please see the questions for methodological discussion

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors proposed a novel method to improve the entropy encoding in deep image compression. The key idea is to introduce three contexts, local, region, and global, to capture contextual information at different scales while balancing the compute cost. The authors show that the proposed method achieves SOTA compression rate while being computationally cheap.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* While introducing global context in entropy model has been explored in previous, how to efficiently capture these context information with minimal compute cost is still an open challenge. This work proposed a novel solution to fill the gap.

* Experiments on real-world high resolution images show significant improvements over baseline methods.

Weaknesses:
* There are quite a few deep image compression methods proposed recently. The authors only compared 4 baseline methods. It would be necessary to compare more STOA methods.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a method for learned image compression called DCA, which stands for ""Diversify, Contextualize, and Adapt"". DCA is a more sophisticated approach for building the entropy model compared to previous methods. The entropy model is a key component in compression models and is optimized to predict the distribution of quantized latents that represents an encoded image. This is crucial for compression since a more accurate entropy model directly leads to higher compression rates when used with an entropy coding algorithm.

The core insight of DCA is to use multiple (""diverse"") contexts for forward adaption. More specifically, the authors define a local, regional, and global context corresponding to a single spatial location in the latent representation (local), a spatial patch in the latent tensor (regional) and a compact summary of the full latent (and thus the full image) learned via cross-attention. The authors show that the three contexts are complementary (Fig. 7) and that a serial applications (architecture details in Fig. 2) outperforms a merged approach (Fig. 9 and 10).

The result is a SOTA model in terms of compression rate with a moderate model size and relatively fast decoding time (see Fig. 5).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
The primary strength of this paper is achieving a non-trivial rate savings over very strong existing image codecs: 11.96% average rate savings over VTM, the current best standard codec for lossy image and video compression, and 3.73% rate savings over a strong neural method [12]. This is achieved with a well-motivated, intuitive improvement over previous entropy models along with extensive experimentation and evaluation.

Furthermore, the rate savings are achieved with a moderate-size model (37.9M) and with a relatively fast decode speed. This is import because ultimately neural compression models compete with standard codecs with extremely fast decode speeds, and because it's already clear that incremental improvements to compression rates can be easily achieved at the expense of massively more compute and memory usage.

Finally, the evaluation in the paper is quite thorough in terms of comparisons with appropriate baselines, runtime analysis, and investigation of importance of the different contexts used, their order, and the architecture (CNNs vs. attention). I also appreciated that the authors showed *where* DCA helped (Fig. 6) by showing the difference in the scale of the normalized latents with and without DCA. Specifically, DCA (with a global context) shows much better prediction for the first step (\bar{y}^1) where there is no opportunity for backward adaptation and the baseline only utilizes regional context.

Weaknesses:
The primary weakness of the paper is modest gains (at least in the context of NeurIPS) on a version of the learned compression problem that is no longer the focus of the compression subfield.

In terms of the rate gains, 3.73% over the previous SOTA method is non-trivial (for reference, papers at conferences focused on improvements to standard codecs often show a gain of less than 0.5%), but it's still incremental progress. It's also not a large enough change to garner more interest from practitioners developing new standard codecs.

In terms of the problem itself, there is now more attention on modeling video and boosting subjective (perceptual) quality. Optimization for MSE can still be important since it allows clear, objective evaluation, but then we're back to the first point where I think a larger improvement is now needed to have high impact at NeurIPS.

Limitations:
Adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
njvPjG0BfK;"REVIEW 
Summary:
This paper presents a procedure for synthesizing hard UNSAT formulas from a given distribution of UNSAT formulas. Concretely, a formula is generated by 1) extracting a core from a seed instance; 2) adding random new clauses; and 3) refining the formula to become harder. In step 3), a GNN is trained to predict UNSAT core of a given formula and the predicted core is relaxed, potentially yielding a larger core. On two problem distributions, LEC Internal and random K-SAT, the proposed method can generate hard instances faster than previous approaches, and the generated instances exhibit similar hardness distribution.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The proposed method strikes a good balance between the hardness of the generated instances and the generation time. It can generate instances much harder than W2SAT, while in a much faster manner than HardSATGEN. 
- The combination of the two ideas, iteratively relaxing core to generate hard instances and learning to predict cores, is quite clever and elegant. 
- The fact that the solver performance on the synthetic instances resembles that on the original instances is a good indication that the generated instances are similar in some way to the original instances.

Weaknesses:
- It seems that the proposed core refinement technique can be viewed as a post-processing step that can be applied to any initial formulas (e.g., ones generated by W2SAT). Have the authors considered building on top of previous DL-generated formulas? Compared with formulas generated by randomly adding clauses to an original UNSAT core, conceptually, formulas generated by a method like W2SAT, which are trained to embed a formula distribution, seem to be a more justifiable starting point. 
- The method can still only handle relatively small formulas.
- Given that the LEC benchmarks are proprietary, only random K-SAT will be made publicly available. However, one could have considered using dataset such as SATLIB like in previous work (e.g., SATGEN, G2SAT, W2SAT).

Minor:
- x axis labels are missing in Figure 5. 
- Line 353: ""syntehtic"" -> synthetic

Limitations:
The authors adequately addressed the limitations. No potential negative societal impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a novel method for generating hard UNSAT problems. The method targets the ""core identification"" problem and iteratively performs refinement using a GNN-based detection procedure, which preserves the key aspects of the original instances that impact solver runtimes. The experimental results show that the method can generate instances with a similar solver runtime distribution as the original instances in a short time.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper introduces a novel method to generate hard UNSAT problems in a reasonable time frame, which alleviates the data scarcity problem in the SAT solving domain and improves the performance of deep learning methods in this area. Compared to previous methods, this method can generate SAT problems that are more similar to the original instances.

Weaknesses:
1. There is not enough support for the correlation between hardness and core size. For example, in the random 3-SAT problem, the UNSAT core can constitute a significant portion of total instances, even more than 80%, yet modern solvers can easily address these instances.
2. Lack of details about model training. There are no explanations on how to prepare the training dataset. 
3. The experimental setting is not convincing. Authors only perform the proposed hard case generation approach on LEC and random k-SAT problems, however, there are no details about how to construct these two datasets. Moreover, it is essential to show results on other datasets to showcase the generalization ability of the proposed method, such as the SAT competition benchmarks. 
4. There are some logical problems in the paper’s writing. In the “Core Refinement” part, the paper states that the addition of random new clauses is likely to create a trivial core. However, I find that in the core refinement pipeline, there is no addition of new clause. Instead, new literal is added. The writing here is kind of messy.

Limitations:
Yes, the authors have stated the limitations of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces HardCore, a novel method for efficiently generating hard Unsatisfiable (UNSAT) Boolean Satisfiability (SAT) problems, addressing the critical challenge of data scarcity. The approach combines a Graph Neural Network (GNN) for rapid core prediction with an iterative core refinement process, enabling the generation of thousands of hard instances in minutes or hours while preserving problem difficulty. HardCore outperforms existing methods in terms of generation speed and hardness preservation, as demonstrated through experiments on both Logic Equivalence Checking (LEC) data and K-SAT Random data. Despite limitations such as applicability only to UNSAT problems and potential scalability issues with extremely large instances, HardCore represents an advancement in SAT problem generation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The work provides a meaningful contribution to the SAT community by introducing a novel method that generates SAT instances within a reasonable timeframe while preserving the hardness of the original problems. The work has potential applications in various industrial settings, though the scalability limit of the approach might pose a challenge.
   * The authors conduct extensive experiments, comparing their method against multiple baselines and evaluating various aspects such as hardness preservation, generation speed, and similarity to original distributions. The paper compares against relevant and recent baselines (HardSATGEN, W2SAT, G2MILP), providing a comprehensive view of how HardCore performs relative to the state-of-the-art.
   * Innovative core refinement process: The iterative core refinement process (Figure 2) is a clever approach to gradually increasing problem hardness while avoiding the creation of trivial cores.

Weaknesses:
* The experiments primarily focus on two datasets (LEC Internal and K-SAT Random), with one being proprietary. This somewhat limits the generalizability of the results.
   * While the paper focuses on runtime distributions and solver rankings, it lacks a deeper analysis of the structural properties of the generated instances (e.g., clause-to-variable ratios, community structure) compared to the original ones.
   * The core refinement process (Section 5.1) involves adding a single literal to break easy cores. The paper doesn't explore more sophisticated strategies or justify why this simple approach is sufficient.
   * The paper mentions struggles with ""extremely large SAT problems"" but doesn't clearly define what constitutes ""extremely large"" or provide empirical evidence of where the method breaks down.

Limitations:
It would really strengthen the paper if the authors were to study the structural properties (e.g., clause-variable ratio, tree width, hierarchical community structure,...) of the generated instances and identify why they are hard for modern SAT solvers.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper addresses the scarcity of practical data (industrial satisfiability problem instances) for training deep learning methods for SAT solving. The existing data augmentation methods suffer from either the limited scalability or the collapsed hardness of the generated instances. Therefore, the authors introduced a fast augmentation method while preserving the original data's hardness. The primary technical contribution is a fast UNSAT core detection procedure using graph neural networks to monitor the hardness of UNSAT formulae during data augmentation. The empirical evaluation confirmed the new method’s fast generation speed and the preserved hardness of the generated instances. In the application of solver runtime prediction, the augmented data led to a 20-50 percent reduction in mean absolute error.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The proposed method achieved the best of both worlds. It has a similar speed to a fast generator while preserving a similar hardness of generated instances to a high-quality but slow generator.

The proposed idea is simple and easy to follow, yet effective. The paper is well-written.

The authors performed extensive evaluations to demonstrate the preserved hardness of the generated instances in various aspects, such as the preserved average hardness, the hardness distribution over instances, the runtime distribution per solver, and the best-solver distribution over instances.

The authors illustrated the effectiveness of their method in a practical application (solver runtime prediction). The augmented data successfully reduced the mean absolute error by 20-50 percent.

Weaknesses:
The authors don’t present how well the core predictor generalizes to other datasets. We may need to re-train the core predictor when applied to a new family of instances if it doesn’t generalize well, which would introduce extra overhead not presented in the current evaluation.

The technique to eliminate an easy core described in lines 176-191 has been proposed in [1]. The authors should cite the work and move the content to related work.

[1] Yang Li, Xinyan Chen, Wenxuan Guo, Xijun Li, Junhua Huang, Hui-Ling Zhen, Mingxuan Yuan, and Junchi Yan. 2023. HardSATGEN: Understanding the Difficulty of Hard SAT Formula Generation and A Strong Structure-Hardness-Aware Baseline. In Proc. of ACM SIGKDD Conf. Knowledge Discovery and Data Mining

In line 190, the satisfying solution should be (A=0, B=0, C=1).

The axis label is missing in Fig. 5.

Limitations:
The limitations have been well addressed. The authors acknowledged that the method can only be applied to UNSAT instance generation and is not suitable for the SAT case. They also mentioned that the results are limited to the current datasets and their method can’t scale to large SAT problems with millions of clauses.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
vo5LONGAdo;"REVIEW 
Summary:
This paper proposes Remix-DiT, which creates multiple experts by mixing fewer basis diffusion transformers, allowing each expert to specialize in the denoising task for corresponding timestep intervals. It achieves performance improvements by having each expert responsible for a larger number of timestep intervals with fewer total trainable parameters than previous multi-expert methods. Also, the paper analyzes the coefficients of how much each expert uses bases, demonstrating the denoising task similarity for adjacent timesteps, as well as the use of specialized bases for lower timesteps.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper is structured well, making it easy to understand and follow.

* The proposed mixing basis strategy is interesting as it achieves better performance with fewer parameters compared to existing multi-expert methods.

* Ablation studies on mixing methods are comprehensive.

Weaknesses:
* **Lack of experiments.** The authors have to validate the performance of Remix-DiT by reporting comparisons with previous methodologies on the FFHQ or MS-COCO datasets. It would make the manuscript more solid if Remix-DiT achieves consistent performance improvements on multiple datasets.

* **Lack of comparison.** There are two methods, DTR [1] and Switch-DiT [2], to address the multi-task learning aspect of diffusion training by designing distinct denoising paths for 1000 timesteps in a single model. These are more parameter-efficient methods where they use no additional parameters or 10%, respectively. The authors should analyze them with respect to Remix-DiT.

[1] Park et al., Denoising Task Routing for Diffusion Models, ICLR 2024.

[2] Park et al., Switch Diffusion Transformer: Synergizing Denoising Tasks with Sparse Mixture-of-Experts, ECCV 2024.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces Remix-DiT, a modification to the diffusion transformer architecture that incorporates the multi-expert denoiser framework during both training and inference. Unlike traditional multi-expert methods that train $N$ separate individual experts independently for each time interval, Remix-DiT employs $K$ base models combined with $N$ mixing coefficients to dynamically compute time-specific experts. This approach enhances efficiency and leverages task similarities between adjacent intervals more effectively. Experiments on ImageNet demonstrate that Remix-DiT improves the performance of DiT across various model sizes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-motivated and represents a valuable step towards integrating the multi-expert denoising framework into standard diffusion models. 

- The main idea of the paper (using global mixers to compute the final experts) is novel and interesting to me in this context.

- The method is simple and effective, making it more suitable for practical use cases. 

- The experiments are well-designed, and the ablations clearly illustrate the impact of various aspects of Remix-DiT. 

- The paper is well-written and generally easy to understand.

Weaknesses:
- While the authors show the benefits of Remix-DiT on finetuning a pretrained DiT model, it would be interesting to see its effect when training all components from scratch. If the compute budget allows, I suggest that the authors also add this experiment for better insights into what happens if one uses the remixing scheme from the beginning of training (perhaps after a small warmup)

- The performance gain seems to diminish as the size of the base model increases. Hence, a more detailed discussion on this issue is needed for the final version. For example, the performance gain is almost 30% for DiT-S, while it drops to only 15% for DiT-L.


**Minor comments:**

Please fix the following issues in terms of writing in your draft:
- L114 ""refer to"" -> ""refers to""
- L144 -> citation is missing
- L215 -> I assume 100M steps should be 100K steps
- L290 -> it seems that it should be written as N experts because K is the number of base models
- L295 -> ""can found"" should be ""can find""

Please also cite GigaGAN [1] as the mixing part of the paper is related to their method of mixing different convolution kernels during training.

[1] Kang M, Zhu JY, Zhang R, Park J, Shechtman E, Paris S, Park T. Scaling up gans for text-to-image synthesis. InProceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition 2023 (pp. 10124-10134).

Limitations:
The authors have mentioned this in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes Remix-DiT, a model architecture designed to enhance the capacity of a standard DiT model without significantly increasing inference costs. This is accomplished by training mixing coefficients to adaptively fuse multiple DiT models and developing specialized experts for multi-expert denosing. A key advantage highlighted in this paper is that Remix-DiT achieves better generation quality while maintaining inference speed comparable to that of a standard DiT. Experimental results on ImageNet-256 demonstrate favorable outcomes compared to baseline methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1.	The visualization results in Figure 4 are very interesting. It seems that the model has a certain preference in allocating the capacity of basis models, with clear segmentation across the timesteps. Additionally, a high coefficient is observed at early timesteps, such as 0-150. Does this imply that those steps are more challenging for the diffusion model to learn?
2.	The idea of mixing multiple basis models is clear and easy to implement. It does not requires the expensive training of independent experts for different steps.

Weaknesses:
1.	Using multiple base models may introduce more training costs. However, in Table 3, the GPU memory usage only slightly increases from 13G to 16G for DiT-B. Can the authors provide more details about the reason? Will Remix-DiT introduce a substantial backward and forward footprint?
2.	This method utilizes the pre-trained model as the initialization. This might make the mixed experts always the same after mixing since they are working on the same basis model initially. Will this be a problem?
3.	Why does the proposed method outperform naively training independent experts? In this method, the experts are crafted by mixing, which should theoretically be upper bounded by the naïve method mentioned above.

Limitations:
This paper discusses limitations such as sparse gradients and the training difficulty associated with a large number of experts.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To improve the generation quality of diffusion transformers, Remix-DiT proposes to enhance output quality at a lower cost and aims to create N diffusion experts for different denoising timesteps without the need for expensive training of N independent models. Remix-DiT achieves this by employing K basis models (where K < N) and using learnable mixing coefficients to adaptively craft expert models. This approach offers two main advantages: although the total model size increases, the model produced by the mixing operation shares the same architecture as a plain model, maintaining efficiency comparable to a standard diffusion transformer. Additionally, the learnable mixing adaptively allocates model capacity across timesteps, effectively improving generation quality. Experiments on the ImageNet dataset show that Remix-DiT achieves promising results compared to standard diffusion transformers and other multiple-expert methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novelty: Model mixers for efficient multi-expert diffusion model training is innovative and unique.

Significance: Addressing the challenge of efficient training of multi-expert diffusion transformers is significant in the field of diffusion models.

Methodology: The proposed algorithm is well-formulated and clearly explained.

Results: Experimental results demonstrate promising improvements over existing methods such as DiT.

Weaknesses:
1. Lack of Visualization Results: The paper does not include any visualization results. Providing visual examples of generated outputs is crucial for qualitatively evaluating the effectiveness of the proposed method.

2. Insufficient Motivation for Multi-Expert Training: The rationale behind adopting a multi-expert training approach is not fully well-motivated, particularly in the context of quantitative comparisons. A more detailed explanation of why multi-expert training is beneficial and how it compares quantitatively to other methods would strengthen the argument. Clarifying the advantages and potential trade-offs in performance and efficiency would provide a more compelling case for this approach.

3. High Training Cost: The training cost associated with the proposed method is substantial. It would be beneficial to provide a thorough analysis of the computational resources, time, and energy required for training compared to other existing methods. Discussing potential ways to mitigate these costs or offering insights into why the increased training cost is justified by the performance gains would add valuable context for evaluating the practicality of the method.

Limitations:
Please refer to the weakness and question part.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
LONd7ACEjy;"REVIEW 
Summary:
The paper addresses security concerns in cross-modality person re-identification systems, focusing on systems that use both RGB and infrared images. Traditional ReID systems have primarily focused on RGB images, but the differences between RGB and infrared modalities present unique challenges. The authors propose a universal perturbation attack method designed for cross-modality ReID, which optimizes perturbations using gradients from multiple modalities. Experiments on the RegDB and SYSU-MM01 datasets demonstrate the effectiveness of this method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.This work investigates vulnerabilities in cross-modality ReID models.
2.Proposes a cross-modality perturbation synergy (CMPS) attack using triplet loss to optimize perturbations that leverage shared knowledge across modalities.
3.Extensive experiments on the RegDB and SYSU datasets show the method's effectiveness and provide insights for future improvements in cross-modality ReID robustness.

Weaknesses:
1.The authors use random grayscale transformations to reduce modality differences. Are the three-channel grayscale images based on visible light, infrared, or both?
2.What is the intended meaning of the decision boundary in Figure 2?
3.The Figure 3 is difficult to understand.
4.Can the method be discussed on more datasets, such as the LLCM dataset?
5.Does the size of the adversarial boundary affect the experimental results?
6.What is the overall loss function used in the paper? How are the functions discussed in Section 3.2 and Section 3.4 related? Additionally, in Section 3.4, the sequence of formulas seems inconsistent with the context.
7. The SYSU-MM01 common tests are conducted in all-search and indoor-search modes. Which mode is the experiment in Table 1 based on? It is recommended to discuss both modes.
8.The authors should select more diverse types of baseline models to verify the generalizability of the method.
9.In Algorithm 1, the CMPS attack is mentioned to use grayscale images to update perturbations, but in the ablation experiments in Table 3, they are validated as two separate modules. What is the reason for this?

Limitations:
The authors explain the limitations of their work, and there is no negative societal impact.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper investigates adversarial attacks on cross-modality person re-identification (ReID) systems. It is purportedly the first study to investigate vulnerabilities in cross-modality ReID models, with the goal of evaluating the security of these systems. 

To this end, the paper introduces an innovative universal perturbation attack method specifically designed for cross-modality ReID. This new method includes a cross-modality attack augmentation technique, which helps the perturbation synergy attack to better bridge the gap between different modalities. Experimental results on the RegDB and SYSU datasets show that the CMPS attack significantly reduces the accuracy of ReID systems, outperforming existing traditional methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This paper introduces a novel perturbation attack specifically designed for cross-modality person re-identification, addressing a significant gap in the existing literature. The authors have made a notable contribution by being the first to explore vulnerabilities in these cross-modality systems. The importance of this work lies in its innovative approach to tackling the complex challenges posed by person re-identification systems that use different imaging modalities, such as RGB and infrared. The paper provides rigorous theoretical analysis and thorough experimental validation of the proposed method, demonstrating its significance both theoretically and practically. Additionally, the quality of writing is high, with the authors presenting their ideas clearly and concisely, making it accessible even to readers who are not experts in the field.

Weaknesses:
1. The difference between VI-ReID and regular ReID is that VI-ReID requires matching pedestrians across different modalities. Theoretically, it is sufficient to attack the RGB features, rendering them inadequate to match the infrared image features. Can focusing solely on attacking RGB features also fulfill the requirements?
2. The article states that infrared images are grayscale images, which might not be entirely accurate. Near-infrared images appear visually similar to grayscale images and lack color information.
3. While the paper demonstrates the effectiveness of the CMPS attack in bridging the gap between RGB and infrared modalities, extending this research to a broader range of modalities would provide deeper insights into the applicability and limitations of the proposed attack method. Additionally, the current experiments are primarily focused on the RegDB and SYSU datasets. Future studies incorporating more diverse datasets from various scenarios and conditions would help validate the robustness and generalizability of this method in complex environments.

Limitations:
Yes, the authors have adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper is the first to explore the security vulnerabilities of cross-modality ReID models and proposes a universal perturbation attack method for cross-modality person re-identification (ReID) systems, called the Cross-Modality Perturbation Synergy (CMPS) attack. This method innovatively utilizes gradient information from both RGB and infrared images to generate a universal perturbation, maintaining its effectiveness across multiple modalities. Experiments conducted on the RegDB and SYSU datasets demonstrate that the CMPS method significantly reduces the accuracy of ReID models, outperforming existing traditional attack methods. This study emphasizes the necessity of considering multiple modalities in the security evaluation of ReID systems and provides new perspectives for future research.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper makes a notable contribution by addressing security challenges in cross-modality person re-identification (ReID) systems with a pioneering approach. It stands out as the first to explore vulnerabilities in these systems, advancing the understanding and testing of their robustness against adversarial attacks, particularly in under-explored cross-modality scenarios. By focusing on this critical gap, the paper opens new directions for security evaluation in complex multimodal environments.

The novelty of the proposed attack method lies in its innovative use of aggregated feature gradients from different modality images to probe vulnerabilities in cross-modality ReID models. The writing quality is excellent, featuring clear, concise, and well-structured explanations that effectively communicate the paper's concepts. The insights provided are poised to drive improvements in the security of ReID systems.

Weaknesses:
1. Figure 1 would benefit from using the same set of gallery images for both the before and after comparisons.

2. The paper exhibits inconsistencies in referencing ""Table"" and ""Tab,"" as observed in lines 228 and 229. Standardizing these references would enhance readability and professionalism.

Limitations:
Yes, the authors have appropriately addressed the limitations of their study.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes an innovative strategy called Cross-Modality Perturbation Synergy (CMPS) attack, aimed at revealing security vulnerabilities in cross-modality person re-identification (ReID) systems. These systems are crucial in security applications, typically using RGB and infrared imaging to identify individuals under various lighting conditions and camera setups. The study highlights that current security research mainly focuses on single-modality (RGB-based) ReID systems, neglecting the complexities and potential vulnerabilities in cross-modality scenarios.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. By focusing on cross-modality security vulnerabilities, this paper fills a critical gap in the field of person re-identification (ReID), where previous research has primarily concentrated on single-modality (RGB-based) studies. This novel perspective is highly significant as it extends the understanding of ReID systems to real-world scenarios where different modalities are frequently used.
2. The proposed Cross-Modality Perturbation Synergy (CMPS) attack is innovative, leveraging gradient information from both RGB and infrared images to generate universal perturbations that remain effective across multiple modalities. This approach not only demonstrates theoretical originality but also provides practical value through robust experimental validation.
3. The experimental results on widely used datasets such as RegDB and SYSU convincingly demonstrate the superiority of the CMPS method compared to existing traditional attack methods, highlighting the method's effectiveness in reducing the accuracy of ReID systems.
4. This paper exhibits strong writing and organizational skills, with logically tight explanations that make complex concepts easy to understand. The comprehensive description and intuitive presentation of the methodology and experimental setup contribute to the paper's clarity and comprehensibility.

Weaknesses:
1. Although the paper is clearly written, some technical sections are still quite complex. Simplifying these parts would benefit a wider audience. For example, in the section ""3.2 Optimizing Loss Functions for Attacking,"" the derivation process from equations (4) to (11) is rather dense and might be difficult for non-specialist readers to understand. Adding more explanatory text between these equations would make it easier for a broader range of readers to follow.
2. The descriptions of ""Figure"" and ""Fig"" in lines 53 and 55 of the article are inconsistent, and they need to be carefully checked and unified.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
nxL7eazKBI;"REVIEW 
Summary:
This paper proposes a novel framework for model disassembling and assembling. A component locating technique is introduced to disassemble task-aware components from the models. And an alignment padding strategy and a parameter scaling strategy are also designed to assemble these useful components.

This work is meaningful to the interpretability of deep neural network models, as it combines the theory of biology and brain science theory to design human-interpretable models, which can uncover the black box CNN.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is clearly written and easy to follow.
The proposed task is very novel, and the whole pipeline is well presented and easy to understand, and makes sense.
The locating-then-assembling paradigm is well supported by the informative components identification and alignment padding strategy.
The authors tested their proposed method on various popular CNN models and obtained convincing results.

Weaknesses:
In section 3.1.1, the insight and rationale of the proposed metric is not clear. Why can this metric measure the contribution of features?

Also, in the 4.2, why the Parameter Scaling Strategy can balance the effects of different componens?

Limitations:
This paper only design CNN-based method, ignoring the popular Vit model.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper draws inspiration from the information subsystem pathways in biological vision systems and proposes a Model Disassembling and Assembling (MDA) approach.

- For model disassembling, the authors introduce the concepts of contribution aggregation and contribution allocation within convolutional filters and their kernels. The relative contribution of features is calculated and linked to the corresponding parameters for subsequent structure pruning.

- For model assembling, a new model is established by combining the disassembled, task-aware components derived from different source models without necessitating retraining.

To validate the effectiveness of MDA, the authors conduct extensive experimental evaluations across diverse architectures, including CNNs and GCNs. The results show that MDA can work effectively on these architectures. In addition to model creation, the authors also explore potential applications of MDA, such as model decision route analysis and model compression.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. The concept of Model Disassembling and Assembling (MDA) is novel, offering a new perspective on model architecture. It suggests that each part of a model related to specific tasks or categories can be considered an independent functional component. These components can be flexibly reused and assembled to achieve various combinations for multi-classification tasks.

2. The concept of MDA, along with the proposed methods and formula definitions, is clearly presented. The experiments are well designed, and the comprehensive results validate that MDA can work effectively for model reuse or creation, and other task scenarios. Moreover, the extensive experiments provide deeper insights into MDA, enhancing the overall quality of the paper.

3. The paper is written in a clear and accessible manner. The illustrations, methods, and formulas are easy to understand, and the language used is straightforward. The provided source codes are also well-structured and easy to read.

4. The proposed method demonstrates significant performance benefits. Besides creating models by reusing model components, the basic idea of MDA has the potential to inspire future research directions, such as model interpretability and model architecture design.

Weaknesses:
1. As the number of categories increases, the performance of disassembling decreases. This means that to achieve the best disassembling effect, it is necessary to decompose each category separately, which is time-consuming and may result in a heavier assembled model.

2. Manually setting thresholds (Eqns. 9 and 10) to obtain relative contributions may not guarantee consistently good results across tasks.

3. While the authors claim that model assembling does not necessitate retraining or incur performance loss (Line 204), the assembled models are often inferior to the baseline without retraining or fine-tuning (see Table 1).

4. In the assembled model, different categories fail to follow their own pathways extracted from the model disassembling. For example, a disassembled conv-filter of CIFAR-10-cat0 might generate a high response on CIFAR-100-cat0 because the conv-filter has never seen CIFAR-100-cat0, which can be regarded as inter-class interference. This is why the assembled model initially shows degraded performance. Ideally, model assembling should ensure that pathways between different categories are mutually exclusive.

Limitations:
The limitations have been discussed in the paper. Additionally, there is no societal impact from the work performed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes the Model Disassembling and Assembling (MDA) task for CNN classifiers, introducing techniques for extracting and reassembling task-aware components. Experiments show reassembled models perform comparably or better than original models. The approach offers new applications in decision route analysis, model compression, and knowledge distillation, with future extensions planned for Transformers and multi-modal models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The idea presented by this paper is novel and the technique adopted for solving the problem is new.

Weaknesses:
1. The definition of sub-task is strongly tied to category, severely limiting it to classification tasks. Additionally, the entire method seems complicated, appearing to be overfitted to classification tasks and the CNN architecture. Its inability to handle other types of tasks, especially self-supervised learning, which is widely recognized as the future, raises concerns about the value of this work.
2. There is a lack of systematic comparisons with other works, such as deep model reassembly[1], and the study only includes self-constructed specific tasks, making it hard to evaluate the technical soundness.
3. Given the widespread adoption of transformers in the field, the absence of experiments involving transformers is unfortunate.

[1] Yang, Xingyi, et al. ""Deep model reassembly."" Advances in neural information processing systems 35 (2022): 25739-25753.

Limitations:
As stated in the weakness part, the absence of the results on widely-adopted transformer architectures and its inapplicability to models trained with tasks other than classification (e.g., masked image modeling) may limit the value of this work.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces Model Disassembling and Assembling (MDA), a novel method inspired by the biological visual system to create new deep learning models without retraining. By disassembling pretrained CNNs into task-aware components and reassembling them, the approach maintains performance while enabling efficient model reuse. Experiments show that the reassembled models match or exceed the original models' performance, highlighting MDA's potential for applications like model compression and knowledge distillation.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. The proposed method allows for arbitrary assembly of new models from task-aware components, similar to building with LEGO blocks, which is novel and interesting.

2. MDA creates new models without requiring extensive retraining, saving significant computational resources.

3. The experimental results are impressive. The reassembled models can match or even surpass the performance of the original models.

Weaknesses:
1. The computation of contributions and component locating may introduce additional time and computational cost, especially when the number of classes or tasks are large. It is recommended to add a analysis of the complexity of this method.

2. Is this method applicable to transformer models and large models?

3. In some cases, the performance of MDA can even outperforms the base model, why?

4. Some typos in writing should be corrected. For example, in Section 5.2.1, the quotes `’sth’` are all right quotes.

Limitations:
The authors have adequately discussed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
AprsVxrwXT;"REVIEW 
Summary:
This work introduce MVGamba, a general and lightweight Gaussian reconstruction model featuring a multi-view Gaussian reconstructor based on the RNN-like State Space Model (SSM). The Gaussian reconstructor propagates causal context containing multi-view information for cross-view self-refinement while generating a long sequence of Gaussians for fine-detail modeling with linear complexity. With off-the-shelf multi-view diffusion models integrated, MVGamba unifies 3D generation tasks from a single image, sparse images, or text prompts. Extensive experiments demonstrate that MVGamba outperforms state-of-the-art baselines in all 3D content generation scenarios with approximately only 0.1x of the model size.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is of good written quality.
2. The inference speed is relatively fast compared to optimization-based methods.
3. This work introduces the new architecture, Mamba, into the field of 3D generative models, which is novel.
4. The computation cost of the training is lower than LGM and LRM.
5. The method is robust to multiview inconsistency.

Weaknesses:
1. In Fig. 6(b), the performance is continuously increasing as the token length increase. Although the authors' motivation is to lower the computation cost, Fig. 6(b) indicates the the performance grows as the computation cost grows.
2. No failure cases are shown to demonstrate the limitation of this work.
3. Fig 2(b) may be wrong. Mamba has linear complexity instead of constant complexity.
4. The video results in the supplementary materials are only marginally better than LGM's results.

Limitations:
See the weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces MVGamba, a feed-forward, sparse reconstruction model. This model takes a small number of images (e.g., 4 views) to infer 3D Gaussians. Basically, it is a Mamba version of multi-view large reconstruction model. The authors have implemented several strategies to ensure stable training and optimal performance.

Experimental results demonstrate that the MVGamba model can reconstruct 3D Gaussians with higher quality than the Large Gaussian Model (LGM).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1)	The experimental results are somewhat promising, showing better Gaussian reconstruction quality than LGM, although the quality is not as high as the concurrent GRM and GS-LRM works.

(2)	I like the experiment design in Section 5 Q1; it provides some motivation for why we need a non-pixel-aligned architecture. It seems this kind of architecture is more robust to multi-view inconsistency.

(3)	The paper is clearly written and easy to understand.

Weaknesses:
(1)	In the abstract and also in Lines 40-41, the authors state that the generated 3D models often suffer from multi-view inconsistency and blurred textures, which is why MVGamba is proposed. However, it seems that this issue is more likely due to whether the architecture is pixel-aligned or not. Therefore, I believe a non-pixel-aligned transformer model can also address these issues. Did the authors try this? How does a non-pixel-aligned transformer model perform?

(2)	I cannot find any tables or numbers in the paper to support the conclusion “0.1× of the model size.” Did the authors forget to include them?

(3)	The reconstruction results look in lower quality than the concurrent GRM and GS-LRM works.

Limitations:
The method is limited in object-level reconstruction with clean background. I encourage the authors explore on scene-level reconstruction as an interesting future work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose MVGamba, a general and lightweight Gaussian reconstruction model for unified 3D content generation. By replacing the previous LRM work’s transformer architecture with the recent model Mamba. MVGamba can generate long Gaussian sequences with linear complexity in a single forward process, eliminating the need for post hoc operations. Experiments demonstrate that MVGamba outperforms the current open-sourced LRM work in various 3D generation tasks with a smaller model size( 0.1× ). The authors also claim the proposed model has the potential to be applied to scene-level and 4D generation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and easy to understand, and the experiments are sufficient and well-analyzed.
2. The motivation of this paper is sound. The current LRMs papers have achieved impressive results in 3D generation, but their architecture is highly computationally intensive, especially when handling long-sequence inputs. The recent Mama has the potential to solve it. 
3. The proposed pipeline achieves better results than the selected baselines.

Weaknesses:
1. The paper’s experiments do not align with its claims regarding the inefficiencies of transformer-based LRMs in handling long-sequence inputs. The primary issue highlighted is that these transformer-based models are not efficient for long-sequence input. However, most experiments in the paper are for single or sparse image input 3D generation. In this context, the Mamba-based architecture does not demonstrate significant speed improvements over transformer-based architectures because the input tokens are relatively short. Table 1 shows no obvious speed enhancement compared to other feedforward methods, supporting this point. Therefore, I question the necessity of Mamba for most tasks discussed in the paper. The Mamba-based reconstruction model appears to be more suitable for dense view reconstruction tasks.

2. The performance of paper doesn’t achieve the state of the art. Although in Table 1, the paper achieves better quantitative results over all baselines, some previous LRMs papers have better performance according to their paper. For example, GS-LRM attains 8 dB higher PSNR than LGM according to its table. 1, while MVGamba is just 2 dB higher than LGM. I can understand this paper didn’t compare with GS LRM because GS LRM doesn’t release its code. I am just not sure if Mamba is important for quality improvement as claimed by the authors, when some transformer-based LRMs can also achieve good (or even better) quality.

Limitations:
The limitations have been well discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
w50ICQC6QJ;"REVIEW 
Summary:
This paper presents Causal representatiOn AssistanT (COAT), which introduces large language models (LLMs) to bridge this gap. LLMs are trained on massive observations of the world and have shown great capability in extracting key information from unstructured data. Thus, employing LLMs to propose useful high-level factors and craft their measurements is natural. COAT also uses CDs to find causal relations among the identified variables and provides feedback to LLMs to iteratively refine the proposed factors. This mutual benefit enhances both LLMs and CDs.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Interesting topic. Employing LLMs to propose useful high-level representations for causal discovery.
2. Develop two benchmarks for the unstructured causal discovery. AppleGastronome and Neuropathic.
3. Derives the first metrics that measure the causal representation learning capabilities of various LLMs.

Weaknesses:
1. ‘We will release an anonymous link during the discussion period.’ I will consider raising my score if the code is reasonable.
2. The contribution of LLM in COAT is a little small. I assume LLM is used as a representation tool to learn the conceptual level attributes, including iterative refining. The causal structural learning can still be considered as the downstream task.
3. COAT will inherit the shortcomings of downstream causal structure learning algorithms.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper tackles the problem of discovering relevant features for recovering the underlying causal graph in the absence and/or in lieu of a human domain expert. The proposed method, COAT, first queries an LLM through a prompt elucidating the task (for eg., discovering relevant features that affect a product review using a few text reviews), then the proposed variables are fed into another LLM that assigns a value to each of these variables, thus outputting structured/tabular data that can be used for causal discovery. Finally, the tabular data is used in conjunction with a traditional causal discovery algorithm (FCI and LiNGAM in this case) to retrieve a causal graph with respect to a target variable (for eg., review score) using the proposed variables. The process repeats until the proposed variables form a markov blanket for the target variable w.r.t the raw unstructured input data (for eg., the text reviews), progressively expanding the markov blanket in each iteration. Additionally, the LLM can receive feedback at the end of each iteration in the form of samples that the proposed variables cannot sufficiently explain. In particular, the authors propose clustering the samples w.r.t the latent variables induced by the LLM and picking the samples in the cluster with the largest conditional entropy.

Initial theoretical analysis of the proposed method implies that the proposed method is able to identify the markov blanket for a target variable using the proposed variables given that enough iterations of COAT are performed.

The authors evaluate COAT empirically over two synthetic datasets and three real-world datasets. They compare COAT against two simple baselines 1) factors being directly proposed by the LLM based on the prompt without further iterations 2) factors being proposed by LLM when queried using both the prompt and some samples of raw observations. The second baseline is essentially COAT without the LLM receiving any feedback after each iteration. The experiments are conducted using 10 different LLMs and primarily one causal discovery algorithm (FCI), with additional experiments on one dataset using LiNGAM. Additionally, the paper proposes two novel metrics for quantitatively assessing the performance of LLMs for feature proposals to be used for causal discovery.


Update: I moved my rating up in the hope that teh authors will add the experiments as they promised to the final version. We have no way of enforcing it but hopefully, the authors will follow up on their promise.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper addresses the important problem of causal discovery and employs an effective two pronged approach involving LLMs and traditional causal discovery algorithms. This approach leverages the strengths of both the LLMs and the causal discovery algorithms i.e, ability to respond to complex prompts and unstructured data with high-level and possibly noisy information, and robust causal discovery with strong theoretical guarantees although requiring strong assumptions on the faithfulness of the data and causal mechanisms, respectively. Overall, I believe this is a promising direction wherein the two components complement each other effectively.

The empirical evaluation is sufficient in terms of the large number of LLMs considered and the moderate amount of datasets evaluated. The results, based on the chosen metrics, sufficiently demonstrate the effectiveness of the proposed method over the simple baselines.

Finally, the paper is well-written and clearly explains the steps involved in each iteration. The further explanations provided in the appendix also aid in this.

Weaknesses:
The theoretical aspects of the proposed algorithm are exaggerated in the introduction. Given the strong assumptions of “sufficiently powerful” LLMs, “sufficiently diverse” examples and further assumptions pertaining to the chosen causal discovery method, the propositions, while appreciated, are rather straightforward. In particular, it would be far more interesting to theoretically analyse the impact of modules involving the LLMs themselves, such as the chosen prompt template, quality of factor annotations and responsiveness of LLMs to feedback regarding causal discovery, even though some of these are evaluated empirically. Also, an analysis on the rate of convergence of COAT would be beneficial.

Secondly, while the modularity of the proposed approach facilitates utilising a cross product of LLMs, causal discovery methods and feedback mechanisms, it also necessitates extensive ablation studies. In particular, the paper would be strengthened by a thorough ablation of the initial prompts and feedback. In particular, a discussion and ablation on the chosen prompt template and its effect on the proposed factors, or lack thereof, is needed. A robust template would allow more seamless adoption of the proposed method. Finally, the chosen baselines are far too simple to make any strong claims on the effectiveness of COAT. Comparing against some of the methods covered in the related work section would help bolster this claim.

Limitations:
See the weakness and questions above.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes COAT (Causal representation AssistanT), a novel framework to leverage LLMs to assist with causal discovery from unstructured data. COAT aims to combine the advantages of LLMs and causal discovery algorithms. To do so, COAT employs LLMs to identify high-level variables and parse unstructured data into structured data. On the other hand, causal discovery algorithms read the parsed data to identify causal relations. To improve the reliability of the results, COAT also constructs feedback from the causal discovery results to iteratively improve the high-level variable identification. The authors conduct extensive case studies ranging from synthetic data to realistic data, and find COAT effectively helps with discovering meaningful causal structures that well explain the target variable.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. This work identifies a crucial and timely problem for how to advance the causal tasks including causal learning and reasoning with foundation models likes LLMs;
2. COAT is novel, interesting and well-motivated. The authors also provide theoretical discussion to justify its soundness;
3. COAT is model-agnostic and robust to the choice of LLMs, and input data modalities;
4. The authors construct several benchmarks, present comprehensive case studies, and conduct extensive experiments to verify their claims. The improvements over direct prompting LLMs are significant.

Weaknesses:
1. The authors should provide more comparisons with advanced prompting techniques such as CoT.
2. More discussions should be provided on the hyperparameters used in COAT, such as the group size in feedback.
3. Model names are inconsistent. The name in Fig 4(c) is not the same with other names.
4. GPT-4 reasoning in Fig 7(c) is unclear in meaning.

Limitations:
No concerns regarding limitations.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper combines the power of LLMs with that of causal discovery by proposing a Causal representatiOn AssistanT (COAT) approach. Specifically, it considers datasets with textual descriptions, and tries to identify the Markov blanket with respect to a target variable (such as customer ratings and medical diagnosis). The key contribution is discovery of the causal factors through a pipeline that uses both LLMs and a causal discovery algorithm.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
I find this an interesting and practical paper that combines the advantages of LLMs – such as the vast amount of knowledge that they encode – and that of causal discovery approaches. The ideas around combination are generally simple but novel, and I believe the approach could potentially be valuable in a suite of applications, although the extent of the value is unclear from the paper.

Weaknesses:
A major limitation of the work is the empirical evaluation, even though it comes across on the surface as being extensive. I sympathize with the authors about benchmarks for causal discovery, but it seems they have used GPT-4 to generate the textual description of the data, and then used LLMs in their COAT procedure. This is clearly a synthetic dataset that can be problematic. Even the “realistic” benchmarks do not come across as sufficiently realistic, based on my understanding and the lack of details in the main paper.

I don’t understand why key aspects of the evaluation were moved to the appendix. I find it impossible to fully evaluate the work based solely on the contents of the main paper. I understand the need to make space and to move things to the appendix, but it’s never suitable to move key aspects such as the description of the benchmarks and the key results that show value of the work. This has impacted my assessment of this work and I have had to decrease my score because of the authors’ choices around appendix content.

A related weakness is the lack of any attempt at describing limitations, of which there are clearly many.

Limitations:
The authors have not suitably described limitations. I consider this a major weakness; please see my previous comments.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
qZSwlcLMCS;"REVIEW 
Summary:
In this paper, the authors propose a principled pipeline called Kaleido Diffusion for text-to-image generation with better mode coverage and diversity. The main intuition is that, conventional DM requires large CFG to make samples locate at high-likelihood modes, which would somehow constrain the diversity by only generating those limited modes. To solve this problem, the authors propose to first use AR to capture the distribution of such latent tokens, then use diffusion model to take them as extra conditions for generation. Experiments on both quantitative numeraical results and qualitative visualizations are used to evaluate the effectiveness of this work.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) The intuition of this work makes sense to me. The excessively large CFG would limit generated samples to certain modes, where using an extra model to capture such text-irrelevant information is a good practice.
2) The visual quality and especially the teaser of this work can well demonstrate the effectiveness of this paper.
3) The experiments are thorough to me.

Weaknesses:
1) I still don't well comprehend why choosing AR as the first-stage distribution learner. One of the reason I guess here is that, the CFG in AR model is only modulating the predicted logits, so that we can still sample across multiple reasonable modes for the next-stage DM through temperature parameter or top-k/top-p sampling. However, I'm still a little bit confused by this part: what if we use a diffusion model w/o CFG sampling to learn such an intermediate distribution? Is the model architecture design choice mainly for the fact that the authors use detailed textual descriptions as the latent token, so that it's natural to choose AR? If so, then this is a little bit ad-hoc to me.

2) Choosing these four specific types of latent tokens is also kind of ad-hoc to me. For example, somebody may also say the excessive CFG could result in similar artistic styles in generation. In this way, we have to additionally extract the style token as ground truth, then laboriously train the whole pipeline again to enable it. Another drawback of such design is that: the training cost is large and it's hard to scale up, every time we want to solve the mode collapse of new categories, we have to first extract/tokenize that new aspect, then jointly train the diffusion model and AR part, which is too ad-hoc and hard to scale up to me.

3) I hope the authors could also discuss about the diffusion decoder idea and the difference between that and this work. Specifically, AR + diffusion decoder would similarly take the latent tokens predicted by AR and feed into DM decoder as conditions. The major difference is that their setting doesn't need the ground truth for those latent tokens, which means the latent tokens are implicitly learned via end2end training. My question is what's the benefit of Kaleido compared to their pipeline, and won't their pipeline better at scaling up?

Limitations:
The limitations are discussed in the appendix.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
Diffusion models generate high-quality images from text but often lack diversity, especially with high classifier-free guidance. Kaleido addresses this by using autoregressive latent priors, which generate diverse latent variables from captions. It enriches input conditions, resulting in more diverse outputs while maintaining quality. Experiments confirm Kaleido's effectiveness in increasing image diversity and adherence to guidance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Due to the tendency of samples to converge towards the direction indicated by the condition under high CFG settings, the approach of generating abstractions first to expand the diversity of conditions and then proceeding with generation seems a very reasonable approach. Moreover, these abstractions are controllable, enhancing interpretability and customization possibilities, which is also favorable.
2. The qualitative results also appear promising.
3. The approach of constructing fine-grained prior via autoregression is novel to my knowledge

Weaknesses:
1. The formulation in Section 3.1 seems somewhat unintuitive. From my perspective, both text descriptions and the additional autoregressive priors you construct are forms of condition signals for diffusion models. Therefore, I do not see a compelling reason to modify or complicate the original classifier-free guidance formulation. Why not simply regard your method as an extension of the condition signal for classifier-free guidance, while still adhering to the existing CFG formulation?
2. The quantitative results are somewhat limited, with MDM being the only baseline. More comparisons with state-of-the-art diffusion models are encouraged.

Limitations:
More results on the efficiency and quantitative performance of your method are encouraged.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduced Kaleido Diffusion which leverages an autoregressive model to first model the latent mode and then generate latents based on the sampled mode. The proposed method is reasonable. The authors explain the insight from a classifier-free guidance perspective. Several experimental results can support the authors' claim.

In addition, the paper with the same contents is published as a workshop paper in ICML24.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper investigates how to improve the diversity of generated images with additional mode controls, which is interesting.
- The writing is clear and the mathematical explanation seems reasonable.

Weaknesses:
- The quantitative results are very limited (only Figure 5). 
- What is the context extractor (MLLM) used in the experiments? Although the authors claimed the mode selection is the major contribution of this work, the additionally enrolled pseudo labels perhaps also contribute to the performance improvements. 
- I am not convinced that the proposed approach is closely connected to the CFG explanation. Several previous studies, eg, ControlNet (ICCV23) and MaskComp (ICML24) have proven that dense controls will improve image quality. The proposed method is more like distilling knowledge from the pre-trained MLLM to obtain more detailed semantic information about the original dataset.
- It would be better to discuss more implementation details to enhance reproducibility.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper improves the diversity of diffusion generation by incorporating autoregressive latent priors. It leverages the autoregressive model to generate specific discrete latent features, and then concat them with the original extracted text features to serve as the condition of diffusion model. Experiments show that the method can achieve high quality and diversity even with high CFG.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The investigated problem is interesting and critical. By incorporating the intermediate ""mode"" representation, the method can apply CFG after the ""mode"" is sampled, which alleviates the ""mode collapse"" phenomenon. The author introduces four specific modes which can also support fine-grained editing.

Weaknesses:
(1) The writing lacks training and inference details. During training, whether all the modes are used in each step or they are randomly selected? For the visual tokens, whether the latent features from SEED are directly used or the sequence IDs are re-embedded? During inference, the autoregressive model is responsible for generating the latent modes if I understand correctly. Then how different latent modes are generated in control especially the combination of them?

(2) Evaluation is poor. In the experiment section, only a figure is provided without providing numerical numbers. The main experiment setting is confused (class conditioned or text conditioned). The construction of the toy example is also not clearly stated.

(3) Comparison with related works is not sufficient, making the position of this work unclear. There are related works also trying to improve the generation diversity of diffusion models such as [a].

[a] CADS: Unleashing the diversity of diffusion models through condition-annealed sampling.

Limitations:
See Weakness.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
bNDwOoxj6W;"REVIEW 
Summary:
The paper examines the computational complexity of Generic and Numerical Identifiability problems. It provides some reduction from these problems to R complete classes. Based on these results, the paper proposes an algorithm for Generic Identifiability, which improves the state-of-the-art double exponential time.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1- The paper proposed new findings on complexity of different various causal identifiability problems.

2- The results demonstrate that these problems can be decided in PSPACE.

3- The papers provided a comprehensive literature review in introduction.

Weaknesses:
1- The importance of results and their technical novelty are not clearly discussed.

2- The main part primarily comprises theorems proofs, which are not completely clear. It would be beneficial to move some parts to the appendix and include more intuition and discussion in the main part.

3- The paper lacks experimental results demonstrating the correctness of the proposed algorithm and the improvement in running time.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The submission studies the problems of generic parameter identification and numerical identification, both of which represent prominent tasks in causal analysis. The results include a PSPACE (i.e., single-exponential) algorithm for generic parameter identification, as well as ForallR-hardness and a single-exponential time algorithm for numerical identification. The submission also extends these results to the closely related setting of edge identifiability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The studied problems are computationally challenging and seem to be relevant for the NeurIPS community.

Weaknesses:
-The restriction to recursive models comes abruptly and is not motivated or discussed in the submission at all. A number of questions are left completely unanswered, such as: Are recursive models common? Have they been studied before? Do they occur in practice? Do the results generalize beyond recursive models? Is there a specific reason one should restrict one's attention to these models?

-The definition of the problems studied do not seem written in sufficiently accessible language and assume that readers are already familiar with several specialized concepts. As one example, ""Zariski almost all"" and ""Zariski closure"" is never defined. The submission would also be much more accessible if it included at least some high-level examples of problem instances and solutions.

-In terms of writing, several of the results are established using only semi-formal language and without what I would consider sufficient rigor. For instance, the construction for the proof of Theorem 2 is described using inline examples and descriptions, but given the crucial importance of the construction for the proof I would have expected the core properties of the construction to be established and formalized via lemmas or claims.

-The results seem to be obtained primarily by cleverly combining known techniques and results, with little new insights required. In particular, the main results in Sections 5 and 6 essentially follow via direct encodings into appropriate fragments of the Theory of the Reals.

Minor comments:
-row 48: ""using alone Sigma"" should be ""using Sigma alone""
-row 71: malformed sentence
-row 301: ""So we can check in..."" should be ""Hence, we can check in...""
-row 340: ""*the* generic identification problem"". Also, missing full stop at the end of the footnote.

Limitations:
The limitations are adequately discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper looks into the parameter identification problem in linear structural causal models using only observational data. Under the Gaussian linear SEMs, the paper provides a polynomial-space algorithm that runs in exponential time. The paper also provides the hardness of the problem.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
In my knowledge, the methods and resutls are new. The paper investigated this traditional problem from computational complexity theory's view, which seems interesting to involved in this problem. The paper linked this parameter identification problems to the NP/coNP and PSPACE and then provide hardness and bounds based on it. Through I could not entirely follow the steps in the proof, but the paper seems theoritically sound.

Weaknesses:
- The algorithm has some assumptions about the model itself. For instance, the model assumes normal noises and (known) topological sorting. But this does not weaken the paper's contribution. 
- The algorithm needs access to an oracle. 
- The notations can be further explained to enhance the reading flow; maybe give a few sentence definitions when the notions first appear in the paper.

Limitations:
The authors state all assumptions.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper is a theoretical examination of the problem of parameter identifiability in structural equation models for mixed graphs (which has applications in causal modeling), significantly improving upon known upper bounds and establishing a hardness result for the problem's complexity.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
**Originality**: The paper has originality (by using the existential theory of the reals to study complexity of parameter identifiability in probalistic graphical models), and this originality leads to an interesting new perspective and good results on the problem of study.

**Quality**:  The paper is of high technical quality (other than a few presumed typos).

**Clarity**: I find the proofs and technical writing to be rigorous and clear, and the positioning compared to previous complexity results is quite clear.

**Significance**: The paper should have at least moderate impact in the sub-areas Causal Inference and Graphical Models (and high impact in the sub-sub-area Algebraic Statistics). Specifically, this paper established foundational complexity results that have wide relevance in causal modeling. Additionally, the paper leads to natural follow-up work in the form of implementing the presented theoretical algorithm and approaching similar identifiability questions for other model classes from the same perspective.

Weaknesses:
The main weakness (other than those mentioned in the Questions and Limitations sections below) is the clarity of the non-technical writing. There is not much to guide the reader through the technical parts, so I worry non-experts will have too much difficulty to see the value of these theoretical results.

Limitations:
Limitations (such as restriction to the linear Gaussian setting and only having theoretical results) are clear from a detailed reading, but they could (and should) be more explicitly mentioned as limitations of the work in, e.g., the Intro.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tries to attach an important theoretic problem: the complexity of identification in linear SEMs. The author proposed an algorithm for generic identification with exponential running time while previous the best algorithm is in double exponential running time.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well organized and presented. For a reader who is not very familiar with the complexity theory, the writer make the conception such as $\forall \mathbb{R}$ easy to understand. 

2. The over all idea to connect the identification with ETR and QUAD is good and easy to follow.

Weaknesses:
Purely from complexity the current algorithm is better than state-of-the-art, however we do not have any knowledge about the constant computational overhead in the algorithm. When applying the theory in the paper to build a practical algorithm, this might also be important.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
uatPOPWzzU;"REVIEW 
Summary:
This paper proposes a novel spectral GNN architecture, the TFE-GNN, which integrates homophily and heterophily using a triple filter ensemble (TFE) mechanism. The model aims to overcome the limitations of existing polynomial-based learnable spectral GNNs by adaptively extracting features from graphs with varying levels of homophily. The TFE-GNN combines low-pass and high-pass filters through three ensembles, enhancing the generalization and performance of the model. Theoretical analysis and experiments on real-world datasets demonstrate the effectiveness and state-of-the-art performance of TFE-GNN.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper provides a fresh perspective on integrating homophily and heterophily in graph learning tasks;
* The presentation is easy to follow;
* Some experiments covering various datasets with different levels of homophily show significant improvements over state-of-the-art models.

Weaknesses:
* The paper does not address potential computational overhead introduced by the triple filter ensemble mechanism;
* The descriptions of the theorems are too lengthy and not concise.

Limitations:
The authors have discussed the limitations of their work adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a spectral GNN with triple filter ensemble (TFE-GNN). As the ensemble of two diverse and simple low-pass and high-pass graph filters, TFE-GNN achieves good generalization ability and SOTA performance on both homophilic and heterophilic datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
S1. This work is motivated reasonably by leveraging the diversity of graph filters to create an ensemble GNN. It is well-explained with clear definitions, theories, and results. The results are further presented with visual tools to create an easy-to-understand intuition.  

S2. The conclusions are supported by extensive experiments with a variety of baseline models and datasets.

Weaknesses:
W1. One major statement that TFE-GNN has better generalization then other adaptive filter GNNs is not fully explained - only empirical results on the loss curves are shown. As a major contribution emphasized in the abstract, more theoretical and empirical results are usually expected. 

W2. The relevance of the abstract and introduction and the later sections can be enhanced. For example, most theoretical analyses are about the expressiveness of TFE-GNN, which is not stressed in the introduction.

Limitations:
-

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed a spectral GNN with triple filter ensemble (TFE-GNN) that aims to retain both the ability of polynomial based spectral GNNs to approximate filters and the higher generalization and performance on graph learning tasks, when most existing GNNs can only retain one of the two capabilities. Theoretical analysis shows that the approximation ability of TFE-GNN is consistent with that of ChebNet under certain conditions, namely it can learn arbitrary filters. Experiments show that TFE-GNN achieves high generalization and best performance on various homophilous and heterophilous real-world datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The experiments are thoroughly conducted with 10 datasets and 17 baselines, which show that the proposed TFE-GNN achieve the best performance over all baseline models.
- The introduction of learnable spectral GNNs and the proposed TFE-GNN is clear and in details.

Weaknesses:
- My main concern is that the heterophilous datasets that are used in the experiments are not up-to-date. Specifically, the five heterophilous datasets in the experiments are a subset of those first adopted by Pei et al. 2020 (Geom-gcn).  However, recent works like Platonov et al. 2022 has identified several drawbacks of these datasets, including leakage of test nodes in the training set in squirrel and chameleon, which can render the results obtained on these datasets unreliable. I recommend the authors to run additional experiments on two more up-to-date heterophilous datasets, such as those introduced in the papers below:
    - Oleg Platonov, Denis Kuznedelev, Michael Diskin, Artem Babenko, and Liudmila Prokhorenkova. 2022. A critical look at the evaluation of GNNs under heterophily: Are we really making progress?. In The Eleventh International Conference on Learning Representations.
    - Derek Lim, Felix Hohne, Xiuyu Li, Sijia Linda Huang, Vaishnavi Gupta, Omkar Bhalerao,
    and Ser Nam Lim. 2021. Large scale learning on non-homophilous graphs: New benchmarks and strong simple methods. Advances in Neural Information Processing Systems 34 (2021), 20887–20902.
- The authors discussed that for existing GNN models, “some polynomial-based models use polynomials with better approximation than some other models when approximating filters, but the former’s performance is lagging behind that of the latter on real-world graphs.” The proposed TFE-GNN models seem to have overcome that limitation, but it remains unclear to me in a high-level, what are the key differences between TFE-GNN compared to prior models which make TFE-GNN able to achieve a better trade-off here?

Limitations:
The paper includes discussion regarding the limitations in Appendix A.2, most notably the reliance on setting hyperparameters $K_{lp}$ and $K_{hp}$ that are suitable for the homophily level of datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
7qBkADV4zD;"REVIEW 
Summary:
This proposed a DeltaDEQ method, which is designed to enhance computational efficiency for implicit models represented by deep equilibrium models. This method is inspired by the authors' observation of the heterogeneous convergence phenomenon prevalent in implicit neural networks, where differen dimensions of DEQ hidden states converge at varying speeds. The authors have tested DeltaDEQ across tasks involving implicit neural representation and optical flow, employing both RNN and CNN architectures. It seems that the proposed DeltaDEQ maintains accuracy while reducing computational demands across these network types.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1 It is interesing to propose heterogeneous convergence, which may exist in deep equilibrium models and other iterative methods and is found by analyzing the element-wise trajectory and dimensionality of hidden states update.
2 The proposed  method does not assume temporal correlation of inputs i.e. effective even for static, which is different from fixed-point reuse acceleration.
3  The proposed approach achieved an 84% reduction in FLOPs for the INR task and a 73% and 76% reduction for the OF task using the Sintel and KITTI datasets, respectively, without significant loss in task accuracy.

Weaknesses:
1. Although the paper provides some implementation details, more details about the implementation of the algorithm may be needed, especially for the sparse processing and threshold selection of DeltaDEQ.
2. The paper is not compared with the current state-of-the-art methods or enough other methods, it may be unconvincing and unable to fully demonstrate the superiority of DeltaDEQ.
3. Although the paper mentions the reduction of FLOPs, it does not elaborate on the actual running time or resource consumption, including memory usage.
4. The paper does not fully explore the sensitivity of DeltaDEQ to threshold parameters or other hyperparameters, this may affect the stability and reliability of the method.

Limitations:
1. DeltaDEQ may perform well on specific types of tasks, such as implicit neural network representation and optical flow estimation, but the paper may not provide sufficient evidence to demonstrate its applicability in other types of tasks or different fields.
2. Although the amount of calculation is reduced during reasoning, the computational complexity and convergence speed of DeltaDEQ in the training phase have not been fully evaluated.
3. If the paper does not provide or promise to provide code and data, this may limit the transparency and reproducibility of the study.
4. The experimental design of the paper may have limitations, such as the data sets used may not be diversified enough, or the experimental settings may not fully test all key aspects of the model.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work provides a finer-grained analysis of the dynamics involved in updating hidden states and identifies a phenomenon of heterogeneous convergence in implicit models, where certain dimensions of hidden states converge much faster than others. Based on this observation, the forward pass update of DEQ models was modified using a delta updating rule that stores intermediate results of computationally intensive linear operations and only recalculates dimensions when changes between updates exceed a threshold. This approach is complementary to other acceleration techniques and does not rely on temporal input correlation, making it effective even for static inputs. The method, primarily applied to DEQ-based models but adaptable to other iterative methods, was tested on implicit neural representation (INR) for images and optical flow (OF) estimation.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- I find the topic of pixel-level faster convergence in deep equilibrium models to be quite intriguing. Based on the experimental results presented in Section 5, this approach appears to have practical value in mitigating the issue of extensive computations during inference, thereby improving the approximation of the fixed point.
- The technical aspects of this paper are straightforward and easy to follow.

Weaknesses:
- While the proposed method appears unique in the context of accelerating deep equilibrium iterations, its underlying concept is akin to standard dynamic network pruning methods, such as dynamic gating or controllable layer skipping. Unfortunately, these related works are not discussed in Section 6. The key differences between the proposed method and existing dynamic pruning approaches seem to be the pruning pattern (element-wise in this paper versus the more common block-wise pruning along the spatial dimension) and the threshold selection strategy (learnable predictor networks versus handcrafted thresholds like $\tau$). Additionally, the current submission lacks information on real runtime speedup. As the authors noted, ""employing the delta rule might result in more overhead, offering no computational advantages,"" leaving me curious about whether the proposed method can achieve actual runtime speedup instead of merely theoretical reductions in computing cost.
- The ""cache update"" operation described in Equation (7) may introduce extra approximation error, in addition to the pruning error from Equation (5), since $C_z:=W_z\cdot\Delta z_t^i+C_z$ relies on $\Delta z_t^i=z_t^i-z_t^{i-1}$, which may not hold under the delta rule.
- The additional description of the ""Convolutional DeltaDEQ"" might be unnecessary, as the derivation from Equation (6) to Equation (8) is fairly straightforward. The main difference is that the sparse pattern should be compatible with the computational operations.
- Although the backward pass is independent of the forward pass, the value of $z^*$ determines the optimization process during backward propagation. I wonder if the learned parameters can still be used for model inference without applying the ""Delta rule.""
- The results in Table 1 show a noticeable performance drop with only a slight reduction in computing cost. Furthermore, the reported inference speedup seems inconsistent with the results under ""Inf. $\tau=0.05$.""
- The authors note that ""the backward pass FLOPs remain constant and are not influenced by the forward pass."" I would expect to see an overall reduction in training cost based on DeltaDEQ.

Limitations:
I found no potential negative societal impact of this work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors begin by presenting the observation that in fixed-depth networks, only three dimensions are sufficient to explain the trajectory of a dimension-20 hidden state. With this motivation in mind, they introduce the notion of heterogeneous convergence, which can be summarized as the concept that different dimensions in the hidden state converge at different rates. This is demonstrated for example in Figure 1d, where it is clear that some dimensions fluctuate significantly more than others which seem to converge rapidly. Hence, the authors re-formulate the fixed point iteration as in Equation (4), and apply what they call a “delta rule” whose goal is to save on computation by creating a sparse vector to multiply with a matrix. In experiments, they show that the training number of FLOPs greatly decreases.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper aims to decrease the number of FLOPs required for DEQs. This is a relevant problem given that DEQs are parameter-efficient models in terms of performance, and their main drawback is their computational cost. Hence the paper is well-motivated.
* The experiments section (5) is strong and provides a careful analysis of the benefits of the method. In particular, there is a clear reduction in training FLOPs. Figure 4 makes it clear that increasing the threshold for Delta decreases the number of inference FLOPs required, which is to be expected, while also decreasing the PSNR. It is nice to see how increasing tau affects the performance of the model in terms of*  FLOPs and PSNR.
* The method is novel and appears to be a very principled approach to reducing the FLOPs required by DEQs. It is clever to re-formulate Equation (4) and Equation (5) and to use caching along with the Delta method to speed up inference.. 
* The authors appear to be well-aware of the related literature and include relevant references. They include other methods as well for accelerating DEQs in order to maximize the acceleration.

Weaknesses:
* Section A.5 is very interesting but in a sense seems to discount the use of a DEQ. Classically a DEQ should find the fixed point, and should not halt until some threshold of being close enough to a fixed point is reached. It seems to me as though the DeltaDEQ method is only compatible with fixed-point point iterations due to the update method in Equation (3). Consequently, line 156 is lacking in precision. What exactly is defined as “good … convergence”? Furthermore, how many fixed-point iterations are there, for example, in the models in Table 1? I am curious to what extent convergence to a fixed point is necessary for your method to work, or if your method is more closely related to a weight-tied network as in “End-to-end Algorithm Synthesis with Recurrent Networks: Logical Extrapolation Without Overthinking” [Bansel et al 2022]. If it is the case that convergence to a fixed point is not necessary to obtain a good result with your method, I would truly hesitate in considering this a method for speeding up a DEQ.
* In Section 3, authors do not actually train a DEQ, they trains a recurrent neural network, or what some may call a weight-tied network. Furthermore, there is no noise in this setting and f(x) = sin(x) is quite a simple function. Consequently, it would be more interesting to (1) train a DEQ with fixed-point solving (2) either choose a more complicated ground-truth function or show many examples of this observation with other functions. Furthermore, although this is a toy example, it must be noted that this latent space of dimension 20 is far less than one used for a larger-scale DEQ. Does this observation of heterogeneous convergence hold in a much larger dimension? 
* Some details that are incorrect/ confusing: The caption states that the backward pass takes a constant amount of FLOPs, although this should not be the case when IFT is the backward method, since IFT also has a fixed-point iteration. Also, the authors write that the training method is Delta-DEQ in the first column, despite having also non- Delta-DEQ methods included as well.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes the DeltaDEQ framework to accelerate the forward pass of the Deep Equilibrium Model. Initially, the paper identifies the heterogeneous convergence phenomenon, which shows that different dimensions of the state converge at uneven speeds in the forward pass of DEQ framework. Inspired by the phenomenon, the DeltaDEQ framework stores the past linear operation results and only propagates the activation when its change exceeds a threshold. Experiments on implicit neural representation and optical flow estimation tasks show that DeltaDEQ improves the efficiency of DEQ framework without hurting performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Very clear writing.
2. The method is simple and easy to understand.

Weaknesses:
1. Lack of experiments. The paper only shows the result of implicit neural representation and optical flow estimation tasks. However, DEQ has a wide variety of applications, including language modeling [1] and generative modeling[2]. I think the authors should provide more experimental results to show that the method can be applied to different tasks. 
2. The method is limited to CNN and RNN architecture. In the paper, only CNN and RNN-based architectures are discussed. Can this method be applied to popular transformer-based architectures？

[1] Bai, Shaojie, J. Zico Kolter, and Vladlen Koltun. ""Deep equilibrium models."" Advances in neural information processing systems 32 (2019).

[2] Pokle, Ashwini, Zhengyang Geng, and J. Zico Kolter. ""Deep equilibrium approaches to diffusion models."" Advances in Neural Information Processing Systems 35 (2022): 37975-37990.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
te6VagJf6G;"REVIEW 
Summary:
In this paper, the authors present a fine-tuned LLM that reasons by generating pythonic code and executes it through the model rather than passing it to an external interpreter (CoGEX). They also use the model to generate a set of candidate programs from a training set, among which the top-k performing programs are selected and subsequently used to evaluate the test set (CoTACS). The model shows improved performance on several datasets against benchmark models without the pretraining, including with in-context learning and chain-of-thought reasoning. Several additional analyses are provided for model efficacy, including the number of training items and sampled programs.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. Paper is well-written and easy to follow. All the analyses and results are clearly presented.
2. The CoGEX model provides a useful datapoint on understanding how well LLMs can benefit from program-like planning without relying on an external program interpreter.
3. Allowing the model to use ‘primitive’ functions that are difficult to define programmatically offers greater flexibility than relying on interpreters and the ability to query for factual information.

Weaknesses:
1. The ideas do not strike me as particularly novel. As noted in the paper, using LLMs to generate code is not new, and neither is using language models to simulate program executors. CoTACS is simply a top-k search.
2. It isn’t clear to me what advantage this method has over just passing the code to Python Interpreter. Moreover, what advantage is there to using CoGEX to just using GPT4 given that the dataset was created using GPT4 in the first place?
3. While I appreciate the completeness of the presented results (for the sake of good science), even for experiments where the improvements are marginal, the results are often marginal at best, and it is unclear in what circumstances this approach is favorable and when it is not. Likewise, the authors use 0-shot CoT as their comparison model (Line 236), which is the weakest CoT baseline possible. Given that using an actual program interpreter is a viable alternative, I would be hesitant to adopt CoGEX without having a clearer understanding of when it outperforms simpler alternatives. In my opinion, the paper would strongly benefit from the following:
    A. A fair comparison with CoT, either controlling for the number of examples or showing at how many examples the models achieve parity. Since this is the simplest thing to try for most ML practitioners, even parity would have me favor CoT over most alternatives.
    B. A comparison, if applicable, to actual interpreters (e.g. Python). If this is not possible (e.g. due to calls to undefined functions), how often is that the case? Alternatively, if the model was forced to use predefined functions, how much better is it to use undefined functions?
    C. I really was hoping for a more extensive Qualitative Analysis section (S 3.4) to get a better understanding of when CoGEX strongly outperforms the baselines. The results in Table 1 demand too much prior knowledge about the individual tasks/datasets from the readers without some guidance on how to think about them.

Limitations:
The authors provide no guarantee of safety (this is acknowledged in the supplements), but not addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The proposed approach, Code Generation and Emulated Execution (COGEX), involves training LMs to generate pseudo-programs with some undefined functions and then emulate the execution of these programs. This allows the LM's knowledge to fill in the gaps while maintaining a program like reasoning framework. The COGEX model is adapted to new tasks through program search, optimizing performance across dataset instances. The approach shows improvements over standard in-context learning methods, demonstrating that code synthesis can be applied to a wider range of problems.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The concept of pseudo-program and LLM emulation of program is interesting.
- The authors evaluated the methodology on many different dataset and tasks.

Weaknesses:
I would consider raising my score if a fair number of the following weaknesses can be fixed by the authors. Many of them should be doable without extra experiments.

- Motivation: While in general I like the idea of pseudo-programs, but I don’t think it is very well motivated or explained in the paper. For this relatively new concept to be introduced, I would like to see a more precise definition that describes each type of reasoning, whether its intuition based with no reasoning, “soft reasoning”, and exact reasoning. State clearly what task falls into the camp of “soft reasoning”.
- Examples: I think the motivation is insufficient partly because of that the motivating examples (Fig. 1, 3, 10) do not seem to be requiring any “complex reasoning”. By “complex reasoning” I specifically mean things like composition of multiple sources of information, long range dependency of information, the use of logical operations like “and”, “or”, “not”, and “implies”, and etc.
- Task/Dataset selection: (also related to previous points) I would want some more clarification on why the authors pick the presented tasks and datasets. What makes these dataset special so that COGEX can be applied to them? What additional conceptual benefits do COGEX bring to them? This will also help motivate the methodology you proposed.
- Formalization: the symbol $f$ for COGEX model is unnecessarily abused. It can be invoked with either 2 or 3 arguments, which does not type check as a well-defined formalization. You should at least create two separate symbols, such as $f_{\text{reasoner}}$ and $f_{\text{enumlator}}$.
- Algorithm: The while loop in line 7-10 of Algorithm 1 does not seem to have termination guarantee. This is a potential flaw in the algorithm. What if the task and dataset is malformed and there exists no proper pseudo-program? Your algorithm should account for this.
- Comparison to GPT-4: I don’t want to suggest to do new experiments with GPT-4 as baselines, but I do want clarification on why GPT-4 is not used as a zero-shot baseline. I see that authors use GPT-4 to generate datasets used for training. So it’s definitely not the case that authors do not have access or hardware/financial limitations for doing so. Then why isn’t GPT-4 used as a baseline? I would assume that GPT-4 without training can still be used to synthesizing pseudo-programs, right?
- Claim in the experimental results: authors describes Fig.4 as “Figure 4 (blue vs gold) shows that CoT… can approach COTACS performance on some NLP classification tasks but generally performs substantially worse.” But to me when looking at Fig.4, I don’t see this claim accurate. I can only see significant difference on CoinFlip and Number Summing.
- Experimental setting: (related to the previous point) the direct comparison with zero-shot CoT might not be fair because your model has undergone fine-tuning, right?

Limitations:
- The author listed the limitations in the Appendix, which I would prefer doing in the main text. This also relates to the first listed weakness that I wrote: giving a proper overview would help people identify where and when your methodology could be applied.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a means of training language models to perform semi-programmatic reasoning, along with an adaptation method based on program search to specialize the resulting models for particular downstream tasks without updating their parameters.

The authors use GPT-4 to generate programmatic reasoning strategies from Alpaca instruction-tuning examples, allowing GPT-4 to include undefined function calls as the resulting code does not actually need to be executable, only used to guide the downstream language model's inference process.

The authors evaluate their method by fine-tuning Llama 2 models on their version of the Alpaca dataset, then comparing benchmark performance against few-shot, instruction-tuned zero-shot, and chain-of-thought baselines on several tasks picked to represent both algorithmic and non-algorithmic reasoning.

Experimental results indicate that the proposed method outperforms the considered baselines across most tasks by a good margin.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed method appears to strike a nice balance between the rigidity of code-structured reasoning and freeform CoT, as it performs well across domains where CoT excels and ones where it struggles.
- Being able to specialize reasoning for a particular task by selecting a pseudo-program is neat. It also appears to work well with a much smaller number of examples than are required to effectively fine-tune a model.
- The authors do a very good job of proactively answering natural questions with their ablations in section 3.3.
- As for clarity of exposition, all the writing is easy to follow and the paper is laid out intuitively.

Weaknesses:
1. The authors don't report statistical significance (e.g. through bootstrapping) or variance across runs with different subsamplings of training data.
2. As far as I can tell, the reported experiments don't really include domains where the correct solution can be reached by actually executing a fully specified program, besides Number Summing (accordingly, a program-of-thought baseline is also missing). In CoGEX the model is responsible for simulating program execution, so it seems likely that it would underperform in these settings. While actually integrating an interpreter into a system that can still handle underspecified functions would be out of scope, it would be good to address this issue at the conceptual level somewhere in the text.

Limitations:
The authors include a short statement to the effect that their models may make reasoning mistakes and the model artifacts are provided without any guarantees of safety. It might be worth including the fact that models may fail to execute generated code consistently, and thus generated code may be seen as an unfaithful explanation of model reasoning.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
8HwI6UavYc;"REVIEW 
Summary:
This paper introduces a method that can add a novel generated or customized object into a 3D reconstructed scene. To enable this, it proposes a Erase-and-Replace strategy. The first step is to remove an object queried by a text prompt: it segment out the target object using existing language based model and fine-tune existing NeRF model based on the inpainted images. The second step is to replace the object where it fine-tune existing NeRF model using the generated or customized images. The model compared with existing instruction based gaussian splatting and NeRF methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
1: poor

Strengths:
- The paper is very straightforward and therefore it is easy to follow.
- The quality is convincing and promising (without considering previous works, though).

Weaknesses:
]The main weakness of this paper is coming from the lack of novelty.]
- The scope of the previous works highly within the previous works, and it actually does not push and advance well the boundary in the field of the research area of the 3D editing field. More specifically, it does not address any challenge which has not been addressed yet. Since the challenge is blurry, the contributions are highly suppressed.
- The proposed method is quite obvious and too straightforward. The impression from this paper, this paper is just to combine existing 3D inpainting and replacement methods. Most components inherit from previous works (or more advanced LLM components) where this paper optimize the engineering parts of the framework (e.g., effectively combine the previous modules, and better and marginally handling the text prompt, data, and mask inputs). Therefore, it also leads to the impression of the product engineering documents not research paper. 
- Finally, the problem itself is not that novel and the proposed method does not even open extra-functionality in the field of 3D editing, which further suppresses the novelty of this paper.

[Potential unfairness]
- The way how existing methods use the prompt is different from the way how the authors used. Previous works have been using the prompts in a way of instruction style, e.g., turn the face to the hulk.
- To be more fair way, it would be nice if the authors can show the results from the same data with the same prompt as shown in the previous works. Since quantifying the performance of the 3D generative works is indeed challenging, this could be one of the ideal and most fair way for apple-to-apple comparison.

[Marginal quality improvement]
- Overall, the quality improvement from the previous works is not that promising. Based on the visual results, it is quite difficult to say which one is showing the better results. Based on the prediction results from the “original papers” (not this paper), it is hard to tell which ones are better. It also leads to the impression that the results shown in the paper is highly cherry-picked.

Limitations:
This paper discussed about the limitations.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel method that utilizes the Erase-and-Replace strategy for text-guided object replacement in 3D scenes. Given a collection of multi-view images, camera viewpoints, text prompts to describe the objects to be replaced and to be added, this method first optimizes the background scene with the original object erased, and then optimizes the foreground object to be added. Specifically, in the erase stage, it uses the HiFA distillation technique and optimizes a localized NeRF region, which covers the mask region of the original object and the surrounding nearby pixels. In the replace stage, it optimizes the foreground NeRF within the mask of the original object while alternatively substituting the background with constant RGB tensor during the distillation. The proposed approach enables localized 3D editing and obtains more realistic and detailed edited results.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The strengths of this paper include:
(1) It proposes a novel Erase-and-Replace strategy that optimizes the background and foreground NeRF separately as compositional scenes, which obtains more realistic and detailed results.
(2) It presents extensive experiments to compare with other SOTA methods and outperforms the others in terms of the qualitative and quantitative results.
(3) The ablation study validates the effective role of the key designs of the proposed approach.

Weaknesses:
The limitation of the proposed approach is also obvious, as described in the limitation section. Is is only suitable for the remove, add, replace editing. The added objects should be within the pre-specified mask region. And the training is time-consuming.

Limitations:
The paper includes a discussion about the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce a method for replacing 3D objects within 3D scenes based on text descriptions. They do this by using in-painting approaches for the set of  images that are used for down-stream novel-view synthesis.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper introduces an interesting and important application of distilling 2D diffusion models into 3D.

The use of a 3D implicit representation for both the erase and replace stages is well motivated for preserving 3D consistency. 

Adding a slight halo seems like a nice solution for getting slight interactions (shadows, reflections) between the generated objects and the background.

They demonstrate their method working on single and multiple objects.

The baselines they compare against seem sensible.

Weaknesses:
One big assumption from the way the method works is that the object you're attempting to replace is LARGER than the object you're replacing it with. SAM cannot necessarily anticipate the size difference between the two objects, and so during the erase stage the best it will be able to do is mask out the small area, in which case the area that you can inpaint over is very small. For instance, replacing the statue in Figure 2 with something larger, like a bus, will not work.

The opposite problem may also be true -- when the object being replaced is TOO much larger than the size of the object that is being added, the replace stage can end up over-sizing the object being added (perhaps encouraged by your foreground/background disentanglement, mentioned in Line 188 ). This can be seen in figure 3, where the cookies look (good but) all too gigantic in the scene.

Additionally, the Halo technique assumes that longer range interactions (further apart spatially within the image) between objects reflections don't exist, though this would easily not be true for reflective surfaces.

A table/quantitative results for Ablation studies would be more convincing. I'm especially curious about the impact of the depth reconstruction loss.

As you say in line 245, 3D scene editing is highly subjective -- in which case, I think a user study might make your claims stronger.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies instruction-guided scene editing. They introduce ReplaceAnything3D (RAM3D, not sure why using this abbreviation), a two-stage method to first erase the object to be replaced and in-paint the background in the scene, and then generate the replace-to object and compose it to the scene after stage one. Some relatively good visualization results are shown in the paper.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The proposed pipeline itself is novel and well-motivated.
- Some of the visualization results look good.

Weaknesses:
- The method highly utilized HiFA's loss and design in their pipeline, but no ablation study was provided about HiFA to indicate whether it is HiFA or the proposed method contributes most to the good results. More specifically, I would like to understand:
    - If adding HiFA's loss and design with Instruct-NeRF2NeRF or GaussianEditor, can they achieve comparable results as the proposed method?
    - If the proposed method removes HiFA's loss and uses simple loss - so it is natural that the results are worse than the full model, will it still get better results in GaussianEditor?
- Some of the editing results are not good enough. 
    - For clothes editing, there is always an obvious cutting edge (sometimes looks like the collar of a sweater) on the neck part.
- The replacement task is easier than standard instruction-guided editing, which explicitly indicates the replace-from and replace-to objects instead of requiring the diffusion model to infer. 
    - This makes some comparisons unfair, especially the ones in the model that struggle to infer the replace-from region, e.g., Fig.9. 
    - On the other hand, this setting also disables some editing, like more implicit instructions like ""turn him into a bald"" (i.e., remove hair).
    - Besides, removing an object and regenerating one prevents the model from keeping sufficient identities from the replace-from objects, which is not acceptable in some cases, e.g., color editing of an object may result in highly different objects.
    - The authors need to argue why this setting is reasonable and more/comparably reasonable compared with traditional instruction-guided editing tasks.
- The paper has multiple presentation deflects, especially the math formulas. 
    - For example, the subscripts should not be italicized. $\lambda_{HiFA}$ is used in the paper, which means $\lambda_{H\times i\times F\times A}$ instead of $\lambda_{\mathrm{HiFA}}$.
    - The same variable should be referenced consistently, but the paper contains $y_{\mathrm{replace}}$ at L127/129/203 and $y_{replace}$ (which means $y_{r\times e\times p\times l\times a\times c\times e}$) at L174/185.

Limitations:
The authors have included broader impacts and limitations in the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
NhqZpst42I;"REVIEW 
Summary:
This paper proposes a new measure of feature complexity based on an information-theoretic metric, the $\nu$-information metric. Utilizing this complexity measure, the paper shows (1) visualization of features of different complexities, (2) simple features are propagated through the residual connections to reach the final layer, (3) simple features are learned earlier than complex features during training, and (4) simple features usually have a higher importance (weight) to the output score of the model.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1.	The paper is well presented and is quite easy to follow. I appreciate the summary of different experiments into the words “what” “where” and ”when.”

2.	The paper provides abundant visualization to help readers grasp the intuitive behind simple features and complex features.

Weaknesses:
My major concerns for this paper are its coherence and significance.

1.	The paper lacks coherence because multiple components (e.g., metrics, algorithms) introduced in this paper do not come from the same theoretical framework. For example, the notion of feature complexity in this paper (the $\nu$-information metric) is drawn from information theory, while the method for extracting features (the Craft method) is based on non-negative matrix factorization and has little connection with information theory. The same issue applies to the feature visualization method for demonstrating features of different complexities, and CKA metric, and the importance metric $\Gamma(z_i)$. These metrics/algorithms are borrowed from different theoretical frameworks and contexts, so it is in doubt whether they can be used together. I would appreciate it if the authors are able to re-organize all components of the paper under a coherent framework, e.g., the information theoretic perspective (if at all possible).

2.	My second concern is about the paper’s significance, since many claims have been discussed in previous works and appears non-novel. The conclusion that simple features are usually color/edge detectors that are located in earlier layers is not surprising, nor do the conjecture that simple features tend to propagate through the residual connection. For the conclusion that simple features are learned earlier than complex features, there are many studies leading to this conclusion, from the perspective of spectral bias [cite1], game-theoretic interactions [cite2], or frequency [cite3]. Overall speaking, I do not learn new insights from the paper. Nevertheless, as mentioned in the 1st point in weaknesses, if the authors are able to re-organize the paper from the purely information-theoretic perspective, it will be a more intriguing work.

3.	Using K-means clustering to aggregate features into meta-features may lead to incorrect clustering results. K-means clustering is known for its poor performance for data clusters that are not circular-shaped and are of discrepant sizes. I am not sure how features are distributed in this paper’s experiments, but more advanced clustering strategies such as the spectral clustering or hierarchical clustering can be applied to mitigate this issue.

[cite1] Rahaman et al. On the Spectral Bias of Neural Networks. ICML, 2019.

[cite2] Liu et al. Towards the Difficulty for a Deep Neural Network to Learn Concepts of Different Complexities. NeurIPS, 2023.

[cite3] Xu et al. Frequency Principle: Fourier Analysis Sheds Light on Deep Neural Networks. ICLR, 2020.

Limitations:
NA

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method to measure the complexity of features extracted by deep learning models. This method is based on $\mathcal{V}$-information, an extension of Shannon's mutual information that takes the computational capabilities of a decoder into account. The proposed measure of feature complexity is inversely related to the cumulative $\mathcal{V}$-information across a model's layers, the intuition being that features that become available at earlier layers will accumulate a larger $\mathcal{V}$-information, while features that only become available late will have a smaller $\mathcal{V}$-information.
Features are extracted from a standard, ImageNet-trained ResNet50, from which an overcomplete dictionary is learned. These features are then clustered, and the mean complexity score for each feature is computed. Feature clusters with low, intermediate and high complexity scores are visualized, revealing that low-complexity features tend to be related to uniform colors or low-frequency information, intermediate-complexity features to local shapes such as eyes and noses and high-complexity features to highly structured shapes, such as insect legs.
Using the CKA between the features in the dictionary and the activations at different layers within the main branch and residual branch of the network, the authors find that Easy features tend to emerge in early layers and then copied to later layers through the residual branch, while Complex features steadily increase throughout layers in both branches.
Analyzing the dynamics of different features' emergence throughout training, complex features are found to emerge later than simple ones. Interestingly, analyzing the interplay between features' complexity and their importance in influencing the network's outputs, the authors find that (a) more important features tend to be less complex, and (b) the complexity of important features is reduced throughout training, suggesting that the network might be ""compressing"" the more important features.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper presents an elegant measure of feature complexity in deep neural networks, which takes into account the networks' computational expressiveness in different layers.
- The paper is extremely well written, with detailed methods and supplementary materials, exhaustive analyses and clear visualizations.
- The Related Works section is particularly exhaustive and thorough.
- It successfully combines several recently proposed interpretability tools.
- The analysis of the interplay between complexity and importance throughout learning was particularly interesting, suggesting a mechanism for the compression of important features across learning epochs in deep neural networks.

Weaknesses:
- The proposed measure is closely related to the accuracy with which a feature can be linearly decoded from the layers of a network. I believe the paper would benefit from an explicit discussion of what, exactly, is the additional information provided by the proposed method which cannot be gained from directly looking at the ""raw"" decoding accuracy.
- For simplicity, the authors restrict their analysis to a single ResNet model. While this is an understandable choice, I am not sure about what the implications are for models which do not include residual connections. The residual stream is found to be central in ""teleporting"" simple features to later layers. The authors should explicitly discuss what they predict the pattern of results would be for networks without residual connections. For example, would simple features be overall less relevant to the network's responses? Or would the network implicitly implement a residual-like stream? The limitations of using this architecture exclusively are briefly acknowledged in the Limitations section, but the specific role of residual connections is not discussed there.
- The proposed measure is based on a loose assumption that the decodability of features tends to increase across layers. However, it is possible that certain features are _only_ available in early layers (for example, certain low-level image features might be discarded). These features would have a low cumulative $\mathcal{V}$-information, and might thus be erroneously classified as high-complexity. The authors should explicitly discuss whether this is a concern at all, and for what reasons.
- The plot in Figure 5B shows that more important features tend to become less complex as training progresses. As the authors have access to the features which are subject to this process, and those which are not, it would be extremely interesting to visualize them, showing what exactly is happening - are the same features being extracted at earlier layers? Or are they actually changing, and starting to resemble simpler features such as colors or edges? The answer to this question would clarify more precisely what the proposed complexity measure is capturing. Is it possible that features we visually evaluate as ""complex"" can be extracted at early layers if they are important to the network's task? Or is layer depth always correlated with visual complexity?

Limitations:
Limitations are discussed adequately. Please refer to the weaknesses section for comments about limitations which I believe were not sufficiently discussed.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The current work proposes a method to assess the complexity of features in a representation space in terms of usable information.  The measure of complexity is related to how far back in the layers of a trained network one can find information about the feature that is recoverable by a linear decoder.  Equipped with a measure of complexity per feature, the authors grab 10k distributed features in the penultimate layer of a ResNet50 and then examine various relationships such as the growth in complexity over the course of training, the relative importance of simple features in determining the output, and the “flow” of simple features through the residual backbone of the network.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
The premise -- of a practical way to assess complexity via tiers of usable information, and then its employment as a route to a better understanding of learned features -- is very interesting.  To evaluate the complexity measure, the authors cleverly use the trained layers in the network to approximate a hierarchy of function classes, such that the amount of usable information between the input and the feature must increase as you move deeper into the network, and at each level all that is needed is a correlation measurement.  The complexity measure is thus reasonable and straightforward to measure in practice.  The authors are upfront about a central assumption related to optimal processing by the trained layers.  The writing is clear and the references to related literature are thorough and extensive.

Weaknesses:
Many of the analyses are questionable, in my opinion, making the work seem to have a strong premise with a lackluster follow up.  I will be happy to be rebutted on these points with justification and clarifications during the discussion period.

Specifically, the “what” and “where” analyses are odd to me.  My main gripes are below, with smaller points in the ""questions"" section.
- Regarding the qualitative exploration of dictionary vectors, and their grouping into clusters (for “what”): is there a good reason to think relative positions mean anything in feature space, and therefore that clustering is meaningful?  Were the dictionary vectors normalized beforehand, or else did the variable magnitude affect the UMAP and clustering?  Why were 150 clusters used?  Did a sweep over k or any other analysis indicate that the vectors are indeed clustered to some degree, and 150 is an appropriate choice?  Unless I missed it, it’s not explained how the meta-clusters were labeled -- was it just visual inspection, and the remaining 120 clusters simply weren’t interpretable?
- Regarding the use of CKA for “where”: The proposed complexity measure is already based on how far back in the processing the feature is still recognizable to a linear probe.  Why show CKA instead of the usable information as a function of depth for simple vs complex features?  It’s not intuitive to me to infer anything about the similarity of the “residual” branch aka simply the feature vec one layer deeper, compared to that of the delta (“main branch”), which is not used as a feature by the network.  It seems far more straightforward to look at any changes in the usable information as a function of depth as arising from the delta at that layer.  Why not just show the curves for each of the features?  The curves would also show the way in which usable information is growing with depth -- is it gradual, or stepped?  All said, it’s hard to find much insight from the results of Fig 4, and a straightforward alternative exists.

Limitations:
There is a reasonable discussion of limitations in the Appendices.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a metric based on V-information to measure feature complexity in deep learning models. Using ResNet50 trained on ImageNet,  it explores feature spectrum, training dynamics, network flow, and decision impact.

The study also highlights the role of simplicity bias and the evolution of feature importance in neural networks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1) Clear and well defined objective
2) Informative Figures
3) Comprehensive supplementary material

Weaknesses:
1) Not Easy To Follow On Math: The paper's mathematical sections are difficult to follow. Clearer and intuitive explanations would improve accessibility.
2) Limited Model Diversity: The study focuses solely on ResNet50, ignoring modern architectures like depth-wise separable CNNs.

Limitations:
The authors have addressed limitations in the appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel metric for quantifying feature complexity in deep learning models, specifically focusing on an ImageNet-trained ResNet50 model. This V-information-based metric captures whether a feature requires complex computational transformations for extraction.  The study addresses four key questions:
(1) The appearance of features as a function of complexity.
(2) The learning timeline of these features during training.
(3) The flow of simple and complex features within the network.
(4) The relationship between feature complexity and their importance in the model's decision-making.
The study reveals that simpler features dominate early in training and are transported through the network via residual connections, while more complex features emerge gradually and require more computational effort. Interestingly, the most important features tend to be simpler and become accessible earlier in the network, suggesting a sedimentation process.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1) Originality: The introduction of the V-information metric for assessing feature complexity is novel and provides a fresh perspective on understanding neural network behavior. The exploration of feature complexity across training epochs and network layers adds significant depth to existing knowledge.

(2) Clarity: The paper is well-organized, clearly presenting its goals, methods, and findings. Visualizations and qualitative analyses effectively illustrate the differences between simple and complex features.

(3) Significance: Understanding feature complexity and its impact on model performance and decision-making is crucial for developing more interpretable and efficient models. This work contributes to explainable AI by providing insights into how features are learned and utilized.

Weaknesses:
(1) Generalizability: While the findings are insightful, they are based on a single architecture (ResNet50) and dataset (ImageNet). The results might not generalize to other models or tasks without further validation.

(2) Complexity Metric: The assumption that each layer optimally represents features for downstream linear probes may not hold universally, potentially leading to overestimated complexity scores in some cases. More empirical validation across different models and tasks could strengthen the proposed metric's robustness.

(3) Temporal Analysis: The focus on specific epochs (e.g., epoch 90) may overlook important dynamics occurring at intermediate stages of training. A more granular analysis could provide a clearer picture of feature evolution, i.e., Can you analyze the changes in importance during the training process？

Limitations:
While the paper provides significant insights into feature complexity in deep learning models, there are several limitations that need to be addressed:

Model and Dataset Scope: The study focuses solely on a ResNet50 model trained on the ImageNet dataset. This raises concerns about the generalizability of the findings to other neural network architectures (e.g., Transformers, GANs) and different types of datasets. Future work should validate the proposed metric across a variety of models and tasks to ensure broader applicability.

Assumption on Optimal Representation: The complexity metric relies on the assumption that each layer in the network provides an optimal representation for downstream linear probes. However, this assumption may not always hold true, potentially leading to overestimated complexity scores. Further empirical validation and adjustments to the metric may be necessary to account for cases where this assumption is violated.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
u6XxyuD3Ro;"REVIEW 
Summary:
This paper considers the problem of tracking regret (aka. switching cost). The previous start-of-the-art result has a $\ln T$ dependence on the final regret bound. This paper claims to remove the extra $\ln T$ (more specifically, an $\sqrt{\ln T}$ factor) and achieve the optimal switching regret.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The solution is clear and intuitive. I checked the whole proof and it seems to be correct. The combination way of $z_t^i = \mu_t^i w_t^i + (1 - \mu_t^i) z_t^{i-1}$ is impressive as it is different from the traditional weighted combination of $z_t^i = \sum_i \mu_t^i w_t^i$. This novel combination method has proven to play an important role in achieving the optimal switching regret. However, Neither the reason why this weighted combination method is able to achieve the desired optimal switching regret nor the comparison with the traditional combination method are explained in the paper. I suggest that the authors could revise the paper to add corresponding discussions.

Weaknesses:
I found the solution impressive. However, the current presentation of the paper is not satisfying enough. As stated in the above part, the reason why this weighted combination method is able to achieve the desired optimal switching regret and the comparison with the traditional combination method are explained in the paper, which are thought to be necessary from my point of view. I believe that the proofs are correct and the method is of importance. However, I am inclined that the current presentation does not meet the acceptance bar of NeurIPS and thus another round of polishment and review would be better.

Limitations:
The authors have adequately addressed the limitations and, if applicable, potential negative societal impact of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides an algorithm which achieves a strongly-adaptive*
switching regret guarantee, removing the logarithmic penalty
typically associated with efficient algorithms achieving strongly-adaptive guarantees.

--- 
*Update after rebuttal: The results are only for switching regret, *not* strongly-adaptive switching regret guarantees

Soundness:
2: fair

Presentation:
1: poor

Contribution:
4: excellent

Strengths:
If the result is correct, it seems like it could be a breakthrough result,
removing a logarithmic penalty from strongly-adaptive guarantees that
seemed to be necessary.

Weaknesses:
It seems like the segment tree would still require $O(T)$ memory, would it not?
Whereas existing approaches achieve $O(\log(T))$ per-round memory and computation.
So it seems like the most interestin
So if the result is correct, perhaps the takeaway is that the log(T) penalty
is the cost of reducing the memory consumption of the algorithm.

The dynamic regret result doesn't seem particularly convincing since it only holds for
comparator sequences which do not move much, ie, for sequences that are essentially
just switching sequences. In fact it seems strange that any interesting
dynamic regret result could follow from a switching regret one, since the
optimal switching regret follows from the optimal $\sqrt{P_T T}$ dynamic regret
by throwing away information; if the comparator only changes at times
$\sigma_1,\ldots,\sigma_{S+1}$ then the path-length bound is $\sum_t \|u_t-u_{t-1}\|=\sum_i \|u_{\sigma_i}-u_{\sigma_{i+1}}\|\le S$ for $u\in\Delta_{d-1}$



The presentation is hard to follow. This work would also probably benefit from having diagrams
demonstrating the segment tree in the notation given here.

Limitations:
The limitations are largely unaddressed. It's clear that some compromises are made to achieve this result,
but it's still very unclear to me what exactly they are, and these should have been laid out more clearly in the main text.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies online convex optimisation under a non-stationary environment. The authors focus on the switching regret, which evaluates the regret on every possible segment of the trials. This paper demonstrates that the additional logarithmic factor $O(\log T)$ shown in previous results can be improved, and the authors obtain an algorithm with an optimal guarantee.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
This paper studies online convex optimisation under more challenging scenarios, and focuses on an important theoretical question: whether the additional logarithmic factor can be improved.

Weaknesses:
- As a theoretical paper that claims to make fundamental improvements, it would be beneficial for the descriptions of theorems to be more precise, rigorous, and comprehensive. For example, I would expect the authors to clarify under which assumptions ($\mathcal{X}$, loss functions) the theorems hold.
- The terminologies used in this paper are ambiguous. The term ""switching regret"" or ""tracking regret"" might often be used in a prediction with experts' advice setting, which could be treated as a degenerated version of dynamic regret, a measure considered widely for general convex optimization. The authors do not introduce the term ""adaptive regret"" [1, 2], which seems to be the same optimization objective in this paper, to my understanding. In [2], it is demonstrated that adaptive regret is stronger than tracking regret (or switching regret).
- There lacks a clear statement of the algorithm. It might improve the readability if the authors could explain the main idea and the key technique straightforwardly about how to achieve the optimal results.
- It would be better if the authors could provide more discussions about related works. See details in 'Questions'.

1. E. Hazan and C. Seshadhri. Efﬁcient learning algorithms for changing environments.
2. A. Daniely, A. Gonen, and S. Shalev-Shwartz. Strongly adaptive online learning.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the problem of switching and dynamic regret in online convex optimization (OCO). Given any segmentation of $[T]$, the switching regret is equal to the sum of static regrets on each segment. For the switching regret, the best-known bound obtained by prior works was $\mathcal{O}(\sum_{k}\sqrt{\Lambda_{k} \ln T})$, where $\Lambda_{k}$ is the length of the $k$-th segment. However, the lower bound is $\Omega(\sum_{k} \sqrt{\Lambda_{k}})$, which follows by independently applying the lower bound on the static regret of OCO with convex functions, on each segment. The current paper removes the extra $\sqrt{\ln T}$ factor by proposing an efficient algorithm RESET (Recursion over Segment Tree), which has a running time $\mathcal{O}(\ln T)$ per iteration (disregarding the cost associated with treating the feasible set). 

For the dynamic regret, it is known from Zhang et al. that a dynamic regret bound of $\mathcal{O}(\sqrt{\mathcal{P}T})$, where $\mathcal{P}$ denotes the path-length of the comparator sequence, is possible. However, the bound is not sensitive to variation in the path-length. The current work improves upon this by showing that RESET achieves $\mathcal{O}(\sum_{k} \sqrt{\Lambda_k})$ dynamic regret for any segmentation that has $\mathcal{O}(1)$ path length on each segment. When the segments have varying lengths, this bound is significantly better.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The considered problem is theoretically interesting and the presented algorithm is quite simple with a clean analysis. The writing is to the point. The idea of having a segment tree is natural since segment trees are known to handle interval updates, e.g. sum-query updates over intervals in an array efficiently. The subroutines used by RESET are Mirror Descent and Hedge. The algorithm can be viewed as updates over the (level, time) space, i.e. at each time $t$ we update the parameters $w_{t} ^ {i}, \mu_{t} ^ {i}, z_{t} ^ {i}$ by Mirror Descent, Hedge, a convex combination (with parameter $\mu_{t} ^ i$) of $z_{t}^{i - 1}$ and $w_{t} ^ {i}$. However, the analysis proceeds by translating these updates to a recursion defined over the nodes of a segment tree; the recursion is obtained by the updates and the regret guarantees of Mirror Descent and Hedge. The dynamic regret guarantee follows straightforwardly from the analysis of switching regret.

Weaknesses:
I do not see any immediate weakness regarding the contribution of the work. Although the presentation of the paper is good, there is a scope for improvement. I suggest the authors use appropriate punctuation marks to end/continue equations and sentences.

Minor Typo: Definition of Bregman Divergence in Section 3.1.

Limitations:
Yes, the authors have adequately addressed this.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
3uI4ceR4iz;"REVIEW 
Summary:
The paper ""SA3DIP: Segment Any 3D Instance with Potential 3D Priors"" presents a novel method for 3D instance segmentation by incorporating geometric and textural priors to generate 3D primitives. It addresses the limitations of existing methods that rely heavily on 2D foundation models, leading to under-segmentation and over-segmentation issues. The proposed approach includes a 3D detector for refining the segmentation process and introduces a revised version of the ScanNetV2 dataset, ScanNetV2-INS, with enhanced ground truth annotations. Experimental results on various datasets demonstrate the effectiveness and robustness of the SA3DIP method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The strength of the paper lies in its innovative approach to integrating both geometric and textural priors for 3D instance segmentation, which significantly reduces initial errors and enhances the overall segmentation quality. The introduction of a 3D detector for refining segmentation further strengthens the method by addressing over-segmentation issues. Additionally, the revised ScanNetV2-INS dataset provides a more accurate benchmark for evaluating 3D segmentation models, contributing valuable data to the research community. The experimental results across multiple challenging datasets convincingly demonstrate the robustness and effectiveness of the proposed method.

Weaknesses:
Despite its strengths, the paper has certain weaknesses. Firstly, it lacks sufficient innovation, as the framework closely resembles SAI3D [1], with the primary difference being the addition of a 3D detector. The entire Scene Graph Construction part is exactly the same as SAI3D. Furthermore, as shown in the ablation study, the overall performance improvement of the model heavily depends on the pre-trained 3D detector, which diminishes the originality and contribution of this paper.
 
[1] Yin, Yingda, et al. ""Sai3d: Segment any instance in 3d scenes."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.

Limitations:
The authors have addressed the limitations of their work. They highlight that using only 3D priors results in short execution times but can lead to an excessive number of superpoints, complicating the merging process.  Furthermore, they acknowledge that the affinity matrix based on 2D masking relies heavily on the accuracy of 2D foundational segmenters and suggest that a more robust merging algorithm or better utilization of various 2D foundational models could be promising future directions.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a pipeline to perform open-vocabulary 3D instance segmentation of scenes, incorporating geometric and RGB information. The method is based on constructing a super-points graph, which is then refined by SAM and a 3D detector (V-DETR). Also, the paper provides an enhanced version of ScanNetV2, correcting and extending the annotations. The method outperforms other SAM based baselines.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Given the popularity of ScanNetV2, releasing a more curated and fine-grained annotation seems a useful contribution. The proposed method is a careful combination of components, in a cascade of steps that seems effective.

Weaknesses:
- As a non-expert in 3D scene segmentations, I find it difficult to understand the novelty of the proposed approach, whereas other works also rely on SAM. From my understanding, the key difference seems to be the exploitation of 3D prior to incorporating the 3D classifier, producing the main impact in instance awareness by constraint. If this is the case, I am unsure about the significance of the technical contribution, and I would ask for further clarification on this.
- The paper does not provide enough documentation or analysis of the new annotations for ScanNetV2, which is critical since it is one of the core contributions of the paper. Figure 3 is insufficient to understand the quantitative statistics of the performed effort. On the new dataset, the methods perform worst, which could suggest that the new labels are more difficult/detailed, but more evidence is required to confirm this. To prove the dataset's usefulness, I would suggest comparing methods trained on ScanNetV2 and ScanNetV2-INS and incorporating them in the paper more statistics (e.g., the difference in the number of categories, how many instances for each of these, ...).

Limitations:
The method briefly discusses the proposed method's main limitation but not the dataset.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This study introduces SA3DIP, a novel 3D instance segmentation model based on SAM. SA3DIP leverages texture priors from point cloud color channels to generate complementary primitives and incorporates 3D spatial priors when merging 2D masks by integrating a 3D detector. These enhancements enable SA3DIP to generate superior superpoints and mitigate the over-segmentation problem found in previous SAM-based 3D instance segmentation methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1) Using SAM to extract 2D masks from RGB-D frames and merge them into a final 3D segmentation result is common in 3D OV segmentation methods. However, few previous methods are geometry-aware during the merging process, highlighting the significance of SA3DIP's incorporation of 3D priors.

(2) Constraints from 3D spatial priors substantially improve performance on both the ScanNetV2 and ScanNetV2-INS datasets.

(3) The low quality of ScanNetV2 ground-truth segmentation results has been a persistent problem. A 3D segmentation dataset with more accurate ground truth, like ScanNetV2-INS, is demanding.

Weaknesses:
(1) Using RGB values only as texture prior are not robust enough due to their susceptibility to variations caused by lighting conditions, shadows, reflections, and object materials.

(2) The ablation study shows that the performance gain from Complementary 3D superpoint primitives is not significant compared to other modules in SA3DIP.

(3) This paper reports only the class-agnostic instance segmentation results of SA3DIP. However, the previous benchmark method (SAI3D [1]) also uses semantic instance segmentation as a typical evaluation metric.

[1] Yin, Y., Liu, Y., Xiao, Y., Cohen-Or, D., Huang, J. and Chen, B., 2024. Sai3d: Segment any instance in 3d scenes. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 3292-3302).

Limitations:
The authors adequately addressed the limitations in section 4.4.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces SA3DIP, a novel method for 3D instance segmentation that leverages both geometric and textural priors to enhance the accuracy of segmentation tasks. The goal is to improve open-world 3D instance segmentation by addressing the limitations of current methods, which often result in under-segmentation and over-segmentation due to the limited use of 3D priors.

SA3DIP integrates both geometric and textural priors to generate finer-grained 3D primitives, reducing initial errors that accumulate in subsequent processes. ItIncorporates constraints from a 3D detector during the merging process to rectify over-segmented instances, maintaining the integrity of objects in 3D space. It also introduces a revised version of the ScanNetV2 dataset, termed ScanNetV2-INS, with enhanced ground truth labels for more accurate and fair evaluations of 3D class-agnostic instance segmentation methods.
Finally, extensive experiments on ScanNetV2, ScanNetV2-INS, and ScanNet++ datasets demonstrate the effectiveness and robustness of SA3DIP, achieving significant improvements in segmentation accuracy over existing methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Enhanced 3D Instance Segmentation Pipeline:
   - The SA3DIP pipeline incorporates both geometric and color priors to generate complementary 3D primitives.
   - Introduces a 3D detector to provide additional constraints during the merging process, addressing over-segmentation issues.

ScanNetV2-INS Dataset:
   - A revised version of the ScanNetV2 dataset with improved annotations, providing a more accurate benchmark for evaluating 3D instance segmentation methods.
   - Rectifies incomplete annotations and incorporates additional instances to better reflect real-world scenarios.

Robust Performance:
   - Demonstrated superior performance in 3D instance segmentation through extensive experiments on multiple datasets.
   - Achieved competitive results, significantly outperforming existing methods in terms of mAP (mean Average Precision), AP50, and AP25 scores.

The SA3DIP method addresses the limitations of previous approaches by fully exploiting the potential of 3D priors, leading to more accurate and reliable 3D instance segmentation results. The improvement of the prior dataset is cleverly done and the overall architecture is sound. The dataset is going to be valuable to the community for future work.

Weaknesses:
1. Obfuscation of Feature Contributions:
   - The enhancement metrics contributions by individual features are not clearly delineated. Ablation studies can be performed more meticulously to attribute the contribution of each feature individually.

2. Super Primitives Definition:
   - A more precise and comprehensive definition of super primitives could be provided to enhance understanding and reproducibility.

3. Progressive Region Refinement Examples:
   - Including examples of progressive region refinement could illustrate the process and its effectiveness more clearly.

4. 2D to 3D Space Integration:
   - While the method backprojects 3D space metrics into 2D space, the potential of lifting 2D space into the 3D object space was not fully explored. This approach could be considered and discussed to justify the design choices made.

These points highlight areas where the methodology can be further refined and expanded to provide clearer insights and potentially improve performance.

Limitations:
It is not clear what level of demarkation of 3d segmentation is considered. Can it work for subparts within a scene such as within a chair. A level of detail seems to be one of the main deficiencies. Additionally it is not clear what the complexity of the pipeline is in terms of computation. Only one scene is shown here. What would be metrics of how much improvement across the dataset can be - an estimate would be good across a randomly chosen dataset.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
G9OJUgKo4B;"REVIEW 
Summary:
This paper presents a method called aTLAS, which leverages task vectors to enhance transfer learning in neural networks. Task vectors represent the difference in weights between a pre-trained model and its fine-tuned variant. aTLAS introduces anisotropic scaling to these task vectors by learning different coefficients for each parameter block, which allows for more effective composition of knowledge from different tasks. The method is tested in various scenarios, including task arithmetic, few-shot recognition, and test-time adaptation, demonstrating improvements in performance and efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- An interesting way of using task vectors: the paper extends the concept of task vectors by introducing anisotropic scaling, which enhances the flexibility and effectiveness of knowledge transfer. This approach allows for fine-grained control over the composition of different task vectors, leading to better performance in multi-task settings.

- Evaluation and analysis: the method is evaluated across multiple tasks, including task arithmetic, few-shot learning, and test-time adaptation. The comprehensive set of experiments provides strong evidence for the effectiveness of the proposed approach. It provides insights on the behavior and importance of various parts of the neural network during task vector composition.

- Parameter efficiency: aTLAS is shown to be a parameter-efficient method for fine-tuning, which is particularly valuable in scenarios with limited data. The ability to achieve high performance with fewer learnable parameters is a significant advantage for practical applications.

Weaknesses:
- **Only CLIP?** the method is primarily tested on the CLIP model and might not directly generalize to other architectures without significant modifications. Future work should explore the applicability of aTLAS across a broader range of model architectures.

- **Computational complexity (is this scalable)?**  While the method is parameter-efficient, the computation of task vector compositions during training can still be resource-intensive, especially for large models. Strategies to optimize this process or reduce its computational footprint would be beneficial.  The method's scalability with an increasing number of task vectors is not fully explored. While the paper shows that performance improves with more task vectors, it is unclear how this scales with very large sets of task vectors or in more complex multi-task environments. Also, earning different coefficients for each parameter block introduces additional complexity. The benefits of this complexity should be weighed against the potential for simpler approaches that might achieve similar results with less computational overhead.

- **How exactly should one select the task vectors?** The selection of task vectors for composition can significantly impact performance. The paper discusses various selection strategies but does not provide a definitive approach. Further research into more sophisticated selection mechanisms could enhance the method's robustness and effectiveness.


- **How to leverage this in real world?** The experiments are conducted in controlled settings with well-defined datasets. Evaluating the method's performance in more diverse and real-world scenarios would provide a better understanding of its practical applicability and limitations.

Limitations:
See weaknesses section

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a method named aTLAS, which leverages task vectors and anisotropic scaling to enhance knowledge composition and transfer in pre-trained models. The authors investigate whether components of task vectors, particularly parameter blocks, exhibit similar characteristics and how these can be used to improve knowledge composition and transfer. The effectiveness of the proposed method is demonstrated in various tasks such as task arithmetic, few-shot recognition, and test-time adaptation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The introduction of anisotropic scaling at the task vector level is novel and offers higher controllability in model behavior, particularly for task addition and negation.
- The method is thoroughly validated across multiple tasks and datasets, showing significant improvements in performance.
- aTLAS demonstrates strong parameter efficiency, making it suitable for scenarios with limited data.
- The method complements existing few-shot adaptation techniques, leading to additional improvements in performance when combined.

Weaknesses:
- Knowledge composition and transfer are limited to the specific pre-trained model architecture, which may restrict its applicability across diverse model architectures.

Limitations:
- Knowledge composition and transfer are limited to the specific pre-trained model architecture, which may restrict its applicability across diverse model architectures.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper enhances the performance of task arithmetic, a recent model editing technique based on weight interpolation, in vision-language models. Instead of the original task- and parameter-independent scaling coefficients of the task vectors, it proposes to learn anisotropic scaling coefficients from validation data, resulting in significant performance improvements, particularly in task addition. The method also proves effective in few-shot learning and test-time adaptation scenarios and as a parameter-efficient fine-tuning (PEFT) method in low-data regimes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- **Originality:** While some parts of the method are not entirely novel (see Weaknesses section), overall, aTLAS goes beyond previous work. The few-shot adaptation application and its relation to PEFT (e.g., using LoRAs as task vectors) are also original contributions to the field of model editing/merging.
- **Quality:** The work is generally of good quality.
- **Significance:** Editing foundation models is an emerging field with promising real-world impact. The results obtained with the proposed method are very good. Specifically, the authors merge the parameters of 8 ViT-L-14 CLIP models while retaining 97.07% of the performance of the single models (see Tab. 2).

Weaknesses:
**Originality and references to previous work**

1. The idea of learning task-wise and layer-wise scaling coefficients for the task vectors is one of the main features of Adaptive Model Merging (*AdaMerging*, Yang et al., 2024). This work, which is not currently referenced, must be duly cited, along with a detailed discussion of the similarities and differences between aTLAS and AdaMerging. 
2. Similarly, the idea of using test-time adaptation techniques such as entropy optimization was also present in Yang et al. (2024).

**Clarity and missing details** 

3. The paper does not report the essential methodological details of aTLAS. Specifically, the loss, optimizer, learning rate, and hyperparameters for learning the scaling coefficients are missing.
4. The disentanglement error (line 143) must be defined and briefly explained. Moreover, as it seems to slightly differ from the original metric defined in Ortiz et al. (2024) – as likely it is computed only for the best set of scaling coefficients – this difference should also be mentioned.
5. In lines 153-155, some citations could be added, e.g., to the fact that the representation built by neural networks is often hierarchical and increases in complexity with the layer depth.
6. The experiments with ResNet backbones are never mentioned in the main text. 
7. A comparison of the computational costs compared to standard task arithmetic is missing and should be provided.

Yang, E., Wang, Z., Shen, L., Liu, S., Guo, G., Wang, X. and Tao, D., 2024. AdaMerging: Adaptive model merging for multi-task learning. The Twelfth International Conference on Learning Representations.

Ortiz-Jimenez, G., Favero, A. and Frossard, P., 2024. Task arithmetic in the tangent space: Improved editing of pre-trained models. Advances in Neural Information Processing Systems, 36.

Limitations:
The paper adequately discusses its limitations. I do not foresee any potential negative societal impacts arising from this study.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
2pgc5xDJ1b;"REVIEW 
Summary:
This paper introduces a method for inferring policy decisions based on randomized controlled trial data when applied to a target population with new covariate data. The method is nonparametric and makes no assumptions about the distributional forms of the data and certifies valid finite-sample inferences of the out-of-sample loss.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The problem of policy evaluation with covariate shifts studied in this paper is important.

Weaknesses:
1. The motivation for problem setup is not clear. Why inferring L_{n+1} for the additional (n+1)th data point rather than the new 1...n? This would be reasonable in clinical trial, n new patents coming in with unknown outcomes.
2. Discussion of theoretical guarantees is not thorough, especially when the method relies heavily on heuristics, e.g. strong assumptions about the correctness of the estimates of unknown distribution of p(S|X,U) are made, and division of D into D' and D'' seems arbitrary.
3. The experimental setup looks overly simplistic, a basic two-d covariate space and a simple quadratic model for the loss.

Limitations:
The authors have marked some limitations, such as the method requires independent samples and may not be suitable in scenarios like major virus outbreaks.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a method for constructing limit curves (upper bounds on the CDF) of an outcome, under a given policy, in a target population, using data from an experimental study.  The general goal is to certify that bad outcomes are unlikely in a target population, given experimental data and some knowledge of the strength of potential biases arising from selection on unobservable characteristics, model misspecification, and so on.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Full disclosure: I reviewed a previous version of this paper, and previously recommended rejection. Hence, my review is influenced both by the current version, and by my knowledge of what has changed since the previous version.

Without further ado, the strengths of this paper, as I see them:
1. This paper considers an important and significant problem at the intersection of several interesting lines of work (e.g., conformal prediction & risk control, generalization of policy evaluation from experimental to observational settings, etc).
2. With some minor nits (see below), the paper clearly presents their contributions, the motivation for their approach, and the assumptions required.  Moreover, the current version does a much improved job putting some of their work in the context of other papers that consider similar bounds on generalizing from experimental to observational data.
3. The approach is practical, given the incorporation of some informal methods for benchmarking plausible values of the $\Gamma$ parameter.  This is new since the previous version, and greatly improves the practical applicability of the method in my view.

Weaknesses:
The main weakness, in my view, is the claimed contribution that Gamma can incorporate finite sample and model misspecification error (see lines 77-78, ""includes all sources of errors...selection bias, model misspecification, estimation error)""), since it is not at all clear how those considerations enter into the selection of Gamma?  The informal benchmarking approach involves building intuition for plausible impacts of selection bias due to unobserved factors, but it didn't seem to speak to model misspecification or estimation error.  This is not a major issue necessarily, but it might be worth softening some of the claims that the proposed approach handles all these other types of error, without some corresponding method for benchmarking these.  For instance, for smaller sample sizes, I could imagine these factors being much more influential in the true value of $\Gamma$.

The second weakness, which I'm less inclined to weigh heavily, is that the novelty of technical contribution is slightly unclear: Mechanically speaking, there is little difference between sensitivity analysis considering unobserved confounding that impacts selection & outcome, versus confounding that impacts treatment & outcome, and there is similarly little difference between policy evaluation and evaluating average treatment effects.  It would be helpful if the authors could highlight the technical contributions they think are most notable compared to Jin et al. 2023, Ek et al. 2023, Huang 2024, etc, beyond the simple fact that the setting differs (e.g., considering treatment-outcome vs selection-outcome confounding, or considering ATE vs policy evaluation).

I also have some additional minor feedback that might be worth incorporating into a future version:
1. Regarding the benchmarking, it would be worth giving the caveat that this is an informal approach to benchmarking that can yield unintuitive results, see [0].  Part of the contribution of Huang 2024 (cited here) was to give a more principled approach under a different sensitivity model.  However, I don't view this as a major weakness, since this is generally an unresolved challenge as I understand it for Rosenbaum-like sensitivity bounds as used here (Huang 2024 & Cinelli and Hazlett 2020 use an R2-based sensitivity model where a more principled approach is possible in the first place).
2. There's a lot of work on combining observational & experimental data, and it might be worth highlighting differences to the settings considered. For instance, [1] deals with a similar setting, but one where outcome information is available from the observational data. There are other papers that deal with estimating causal effects in target populations from experimental data where observational data includes individuals not represented in the trial, e.g., [2], [3].
3. It's not entirely clear from the introduction what the ""loss"" $L$ entails, as the name suggests something like a prediction error.  I think it would be useful to highlight upfront that $L$ can represent e.g., an outcome of interest (more typically rendered as $Y$), as done in the application.  It is more intuitive to me to interpret the setup in light of the application, e.g., the goal is to bound the probability of extreme outcomes occurring.
4. As an improvement, it seems like it should be fairly straightforward to derive lower bounds as well, no?  E.g., replacing $L$ with $-L$ and applying the same machinery.  That might be useful in applications where you want to ensure that an outcome stays within a certain range.

[0] Making Sense of Sensitivity: Extending Omitted Variable Bias.  Carlos Cinelli, Chad Hazlett. https://doi.org/10.1111/rssb.12348 (JRSS B, 2020)

[1] Hidden yet quantifiable: A lower bound for confounding strength using randomized trials. Piersilvio De Bartolomeis, Javier Abad, Konstantin Donhauser, Fanny Yang. https://arxiv.org/abs/2312.03871 (AISTATS 2024)

[2] Removing Hidden Confounding by Experimental Grounding.  Nathan Kallus, Aahlad Manas Puli, Uri Shalit. https://arxiv.org/abs/1810.11646 (NeurIPS 2018)

[3] Falsification before Extrapolation in Causal Effect Estimation. Zeshan Hussain, Michael Oberst, Ming-Chieh Shih, David Sontag. https://arxiv.org/abs/2209.13708 (NeurIPS 2022)

Limitations:
The authors adequately discuss limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to use trial data to make valid inferences about policy outcomes for a target population. By incorporating additional covariate data from the target population, the sampling of individuals in the trial study is modeled. The authors develop a nonparametric method that provides certifiably valid trial-based policy evaluations, regardless of model miscalibrations, and ensures validity even with finite samples. The effectiveness of the certified policy evaluations is demonstrated using both simulated and real data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Paper has theoretical analysis.

Weaknesses:
1. paper is not well organized.

Limitations:
Yes. I have very limited knowledge for this area, my reviews for this paper should not be count.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the challenge of generalizing randomized controlled trial (RCT) results to a target population, addressing the potential issue of distributional shift from RCT participants to the intended population. Instead of estimating the expected loss on the target population, the paper proposes a nonparametric method that leverages covariates information from the target population and certifies valid finite-sample inferences of out-of-sample tail loss. The approach is validated using both synthetic and real data, ensuring its applicability to the target population.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well written. For me who work on causal inference but not very familiar with the methodology on handling distributional shift, I can easily follow the paper and get the gist of the idea. 
2. I think the method and the technical results are very clean and sound. 
3. I believe the inference on out-of-sample tail risks is of vital importance, which is often omitted in policy evaluation.

Weaknesses:
1. If I understand correctly, the policy Pi is given as an exogenous parameter. However, it is a common practice that we learn a policy using the RCT data and aim to know its performance on the target population. This definitely introduces the dependence issue. I suspect some type of cross-fitting may help but I doubt the empirical performance, or any chance to analyze the bias in this case?

Limitations:
The authors have adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
FOTMgW8w5t;"REVIEW 
Summary:
Clinical trials try to achieve the highest statistical precision using the fewest number of enrolled participants. One way to do this is by assigning more participants to the {covariate, arm} combinations with the highest outcome variance. We don't know the outcome variance before running the trial, which motivates the adaptive CARA method to adaptively assign more units with high outcome variance to treatment. This paper considers the setting where we don't directly observe the outcome, but instead observe a noisy surrogate along with a delayed outcome. The paper proposes a design objective (minimizing the semiparametric efficiency bound) and an explore-then-commit style algorithm for learning then applying a covariate-adaptive randomization using surrogate outcomes. The paper shows that this algorithm attains the semiparametric efficiency bound, and provides synthetic experiments based on an HIV study comparing this algorithm to complete randomization and CARA on observed outcomes only.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
I am not very familiar with real-world clinical trials. However, from what I can tell, the work is well-grounded in problems that clinical trial designers care about. The mathematical model of surrogacy and delay is simple to state and understand while capturing the important aspects of the problem. The problem is explained clearly with explicit examples.

Weaknesses:
The empirical results are missing error bars. These should be added to show variation in the results across many replicates. This is especially important for two reasons: (a) to support the claim that the proposed method has lower bias than complete randomization, when the two are extremely close on the plots, and (b) to understand whether the ""no surrogate"" method is really worse in both bias and variance than complete randomization, as suggested by figure 2. If the ""no surrogate"" method is actually worse than complete randomization, it would be interesting to have a comment on this in the text.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This article studies how to use interminate surrogate outcomes to estimate causal effects when the primary outcome is delayed in clinical trials. The author first proposes a novel Covariate-adjusted Response-adaptive (CARA)  design that supports efficient estimation of ATE using both surrogate and primary outcomes.  The authors prove the semiparametric efficiency bound for their ATE estimator and demonstrate the proposed design's efficiency through theoretical analyses and a synthetic HIV study.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
1. The delay outcome problem studied in this paper is well-motivated. The covariate-adjusted response-adaptive (CARA)  design allows us to modify the treatment allocation mechanism over time, providing an interesting scenario to address the problem. 
2. The authors consider improving efficiency in both design and estimation. They show that the proposed design strategy converges to an oracle design and proposes a semiparametric efficient estimator of the ATE.
3. The paper includes a synthetic HIV trial to illustrate the practical application and efficiency gains of the proposed design. The study demonstrates that the design reduces the standard deviation in estimating the ATE compared to other methods.

Weaknesses:
The main weakness of this article is that the presentation is too messy and lacks explanations in multiple places. 
1. In section 3, from line 159 to 179, the author introduces the efficient influence function (EIF) and variance bound in estimating $\tau$. However, it seems like the main motivation of CARA is that the expectation of the delay outcomes and the delay mechanisms are unknown.   It is unclear why EIF and variance bound are presented at the beginning of section 3.

2. Section 4 is also presented in a poor way. The authors go through the design steps with dense notation. The authors should provide more explanation of the steps taken in the design. The current Section 4 only shows that the design allows us to estimate the unknown quantity. But it is unclear how the design improves efficiency.

Limitations:
N.A.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces an approach to optimizing treatment allocation for variance reduction, in the context of adaptive clinical trials.  The particular setting is one where there are delays in observing outcomes. These delays that are independent of the outcomes themselves, but they impact efficiency of estimation.  Moreover, there are short-term surrogate variables that can be used to improve estimation efficiency.  In this setting, this paper proposes to estimate the variance of different treatment allocation schemes from an initial stage of experimentation, and then use those estimates to choose an optimal treatment allocation scheme for the remainder of experimentation.

---------------
REBUTTAL UPDATE: I have updated my score from 5->6

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Overall, I found the setup of the problem to be original, and while I did not have time to review the proofs in depth, the results seems correct based on my knowledge of semiparametrics.  The theoretical results are fairly thorough: For instance, I appreciated Theorem 3, which speaks to the convergence of the chosen strategy to the optimal one.  In terms of clarity, the presentation of the technical details is similarly thorough, and while a bit dense and harder to skim, I found it straightforward to follow as written.  I also appreciated the experimental results, which I found to be reasonably compelling.

Weaknesses:
I noted a few areas of the paper that appeared a bit weaker, or at least warrant some clarification.  I'll number my points to make it easier to respond during the rebuttal period, and put them in rough order of priority.

(W1) Significance / Novelty of Theorem 1: I understood Theorem 1 to be a necessary pre-cursor to the proposed adaptive strategy, but not the main contribution of the paper, so this concern is not as pressing as it could be.  However, it seems that Theorem 1 is essentially the same as Theorem 2.1 of Kallus & Mao 2020 [1], with the minor complication that different observations appear at different times, which leads to some additional notation to ensure the correct $e_{t}(X)$ is used in the relevant denominators.  Since the paper cites [1] (as [23] in the paper), it may be worth clarifying the similarity in the main text, and framing the contributions appropriately (e.g., my read is that Theorem 2 and 3 are the novel theoretical thrust of the paper).  I'm open to clarifications / corrections on this front.

[1] https://arxiv.org/abs/2003.12408

(W2) Significance of the proposed problem setting: From a practical perspective, I struggle to think of a setting where we would expect the causal graph in Figure 1 to hold, where the delay itself provides no information on the outcome $Y$, i.e., delays are not informative. E.g., in the HIV application, the outcome is viral load, and delay is in coming in for a visit (Lines 274-275).  It seems to be a classic case where delay could be caused by more severe illness, giving us some indication of $Y$.

(W3) I found it slightly hard to parse the optimality claim (Theorems 2 and 3 and subsequent discussion).  It's clear that the estimated treatment allocation converges in probability to the optimal treatment allocation, but does that imply that this approach yields the optimal variance?  Could there be other ways to estimate the treatment allocation that also converges to the optimal one, but has better variance properties along the way, or is that somehow ruled out?

(W4) Overall, I would have preferred to see more discussion of intuition and insights, rather than just going through the technical steps to derive the results.  It seemed that some parts could be made shorter to make space for this, e.g., Section 4 could be trimmed down a bit (e.g., define $\hat{\tau}^t(a, x, s), \hat{\tau}^t(a, x)$ and then use that notation in both Stage 1 and Stage 2 to $D^*+1$, or even just give the high-level comment that you estimate via empirical counts among the population where $Y$ is observed.)

In terms of missing intuition, I still lack some intuition for the ""signal"" being used in the optimization of the treatment allocation, and why we should expect it to vary across time periods.  I suppose the variance reduction comes from some understanding of which combinations of $X, A$ are likely to have missing data by the end due to delay, and so should be prioritized earlier in the treatment allocation process?

(W5) There is a slight mismatch between the discussion in the introduction and the actual setting where the algorithm can be applied, particularly the restriction that the delay must be upper-bounded (see 181-182).  In contrast, Line 75-76 refers to the contribution ""optimize the statistical efficiency in the presence of temporary or permanent missingness"", where I would have interpreted ""permanent missingness"" to be $D = \infty$. 

(W6) Interaction (or lack thereof) between delay and surrogates: This is a subjective / aesthetic point, so I place it last, but I was expecting more of a nuanced interaction between the delay and the surrogates.  In particular, the problem seems to factorize cleanly into (a) deriving an efficient estimator and variance lower-bound for any treatment allocation, (b) optimizing the allocation in a backward-looking period of data, and then (c) applying that allocation to the next period of data.  It's not clear the role of surrogates in this recipe, other than the influence they have on (a), because observing surrogates gives you no insight into future delays and therefore the optimal allocation going forward.

Minor nits:
* Algorithm 1 mentions ""Problem B"" on line 7, but I think that's really just Equation (2), Page 6, line 199.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the problem of covariate adaptive experimental design in the presence of time delayed outcomes. More specifically, we assume that participants are enrolled in waves, and the probability of treatment is given conditional on available user covariates. The target estimand is the average treatment effect for a long term/delayed outcome. To circumvent the issue of covariate adaptive randomization in the absence of observed outcomes, the authors consider a surrogate metrics. To derive the assignment procedure, the authors derive the influence function and semi-parametric efficiency bound, and then derive an objective based on them. A small number of empirical evaluations is provided which show the improvement in variance of the proposed estimator over complete randomization and approaches which do not incorporate surrogate information.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This is an interesting addition to the literature on adaptive experimentation, and covariate adaptive experimentation. The authors do a nice job of clearly describing the task, motivating and deriving the influence function and semiparametric efficiency bound, and introducing a relatively simple algorithm for optimizing the bound. The proofs, to my reading, are correct and well presented. The problem is of clear practical importance, the authors motivate their approach through drug trials, but there are also a number of applications in both social scientific and industrial settings where the problem of delayed and long term outcomes arises and adaptive design is desirable. The authors also do a nice job of clearly walking through each step of the proposed algorithm and describing it's function and intuition.

Weaknesses:
The paper focuses on asymptotic results, which is sensible and provides good empirical performance. However, it would be useful to have finite sample analysis as well, since many of the applications of experimental design are sample-starved. It would have also been nice to have seen a slightly larger set of empirical results. The authors provide a small demonstration, but ideally the behavior of the propose algorithm is more rigorously evaluated empirically.

Limitations:
Yes. See above for questions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper considers the covariate-adjusted response-adaptive randomization design for settings with delayed outcomes and surrogate outcomes. The paper first characterizes the efficient influence function for estimating the primary outcome under the delayed setting with surrogate outcomes. They then characterize the semiparametric efficiency bound of the estimate. Using the semi parametric efficiency bound they devise an optimization approach to construct an adaptive randomization design that optimizes the semi-parametric efficient bound. They provide a synthetic case study to demonstrate the effectiveness of their approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper overall feels well written and well organized. The problem set-up as well as the key results follow a logical flow and it is straightforward to understand how the adaptive randomization proposed in the paper is derived. The authors also provide insightful comments on the results in the paper which helped point out its main contributions compared to existing work. 

The paper's contributions are interesting as their adaptive randomization design in the delayed outcome setting with surrogate outcomes is a realistic setting seen in clinical trials performed by the FDA. The synthetic case study helps highlight the potential application of their work. They also provide theoretical results that give guarantees on the quality of the estimation approach

Weaknesses:
The paper leaves out some details which may be obvious but are worth including for clarity. For example, the unconfoundedness and arm-dependent delay assumptions are briefly mentioned in passing, but are not formally defined even though they are used in the statements of the theorems. The authors also do not comment on the tractability of optimization problem that determines the treatment allocation probabilities. Other details omitted include details of the cross-validation set-up used to tune the regularization penalty parameter and the the details of the ""No Surrogate"" approach in the case study. 

Discussion on the settings where the proposed method has the most benefit is also lacking. The numeric case study primarily studies the impact on sample size on different approaches for estimating the primary outcome. The results show that while there is benefit from using the proposed method, the most naive method with complete randomization also performs better than other adaptive randomization approaches. Adaptive randomization design may be more challenging to implement so a deeper study on settings that benefit the most from the proposed method would shed more light on the benefits and robustness of the proposed approach.

Limitations:
The authors have adequately addressed limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
dtPIUXdJHY;"REVIEW 
Summary:
This paper proposes a new vector contraction inequality and derives a tighter label-specific representation learning (LSRL) generalization bound based on it.
This paper derives bounds for general function classes of LSRL with a tighter dependency on c than the SOTA, which provides a general theoretical guarantee for LSRL.And it also derives bounds for typical LSRL methods, which reveals the impact of different label- specific representations on the generalization analysis.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. The paper proposes a new vector contraction inequality and derives a tighter LSRL generalization bound based on it, which has a more concise dependence on the number of labels compared to existing results.
2. The paper analyzes the generalization bounds of three typical LSRL methods, revealing the impact of different label-specific representations on generalization analysis, which helps better understand the good practical performance of LSRL. 
3. The projection function class improves the vector-contraction inequalities  and decouples the relationship among different components.

Weaknesses:
The description of the derivation of the general bound in the main text is too brief. For better reading experience, the paper should give more details and some more intuitive description (such as lemma 1 and Theorem 1).

Limitations:
Yes.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The article discusses the theory bounds of Label-Specific Representation Learning (LSRL) in multi-label learning. It highlights the need for a deeper understanding of LSRL's generalization properties and proposes novel bounds based on Rademacher complexity. This paper derives bounds for general function classes of LSRL with a tighter dependency on the number of labels than the state of the art, and these theoretical results reveal the impact of different label-specific representations on generalization analysis.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. This paper develops a novel vector-contraction inequality and derive the generalization bound for general function class of LSRL, which has not been explored in previous research.
2. By deriving tighter bounds and analyzing typical LSRL methods, the paper enhances theoretical guarantees and shed light on the impact of label-specific representations on generalization.
3. By exploring the effects of different label-specific representations, this paper provides insights into the generalization of LSRL and contributes to a deeper understanding of multi-label learning.
4. The assumptions posited in this paper are reasonable and aligned with real scenarios, with a logically coherent process of reasoning and argumentation.

Weaknesses:
1. Section 5.3 of this paper precisely assumes a fixed structure for the DNN-based LSRL method, which appears somewhat biased. It could be more comprehensive to indicate bounds for a subset of network structures sharing common characteristics.
2. Different label-specific representations exhibit substantial variations in their effect on Generalization Bounds. The paper solely undertakes theoretical analyses of a limited number of label-specific representations, potentially lacking comprehensiveness.
3. While the theoretical advancements are significant, the paper does not provide guidance on implementing LSRL in real-world applications.

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This work makes one step towards the generalization analysis for label-specific representation learning. Compared with previous generalization studies on multi-label learning, this work provides a generalization bound with a much weaker dependency on the number of labels and decouple the relationship among components of different classes. This work also analyzes the generalization bounds for three typical LSRL methods, including k-means clustering-based method, LASSO-based method and DNN-based method, and these results reveal the impact of different label-specific representations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) Overall, I think this is a nice work with significant contribution. The proposed generalization bound takes a much weaker dependency on the number of labels, and this is quite important since a large number of labels may occur in the multi-label tasks. Besides, previous theoretical works for multi-label learning preserve the coupling among different components, and this is invalid for LSRL. This work decouples the relationship among different labels during the analysis, which is consistent with the process of LSRL. Therefore, I consider this work as a milestone for the theoretical understandings of LSRL methods.

2) The major technical contribution lies in the vector-contraction inequality given in Lemma 1, and the core idea is to introduce a projection function to decouple the relationship among labels. This may shed lights on relevant studies on LSRL. This work also analyzes the generalization bounds for three mainstream LSRL methods, and some techniques may be of independent interest for the studies of KNN, Lasso and deep neural networks.

3) This work is well written and the structure is nice. Both the motivations and contributions have been stated clearly. Necessary analysis has also been presented for the proposed theoretical results. Besides, a detailed review of related work is also provided to help readers understand the background of this work.

Weaknesses:
1）As pointed out in lines 165-168, the Rademacher complexity cannot be directly applied to the class of vector-valued functions F. Therefore, this work considers the complexity of the loss function space with respect to F. Here, I suggest formally defining the Rademacher complexity of this loss function space to avoid any misunderstanding.
	
2）This work provides definitions for the covering number and Fat-shattering dimension in Definition 2 and Definition 3. However, these complexities are not mentioned again in the main text. I suggest explaining how these complexities are used in the main text or directly moving their definitions to the appendix.

Limitations:
See the above

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on the theoretical analysis of Label-Specific Representation Learning (LSRL) within the context of multi-label learning. LSRL aims to improve multi-label learning by creating representations with distinct discriminative properties for each label. While LSRL has shown empirical success, its theoretical underpinnings, especially regarding generalization bounds, have been less explored. The authors propose a novel vector-contraction inequality and derive tighter generalization bounds for LSRL. These bounds offer a better dependency on the number of labels compared to existing theories. The paper also discusses the implications of these bounds for various LSRL methods, emphasizing the mild assumptions that explain the good generalization abilities of LSRL.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Novel Theoretical Contributions: The paper introduces a new vector-contraction inequality specific to LSRL, providing a theoretical framework that was previously lacking.
2. Improved Generalization Bounds: The derived generalization bounds have a weaker dependency on the number of labels, offering better theoretical guarantees.

Weaknesses:
1. The main issue addressed in the paper is viewing the multi-label problem as multiple binary classifications and then identifying the most discriminative features for each label. However, it is well known that the key aspect of multi-label problems lies in label correlations, which are crucial for improving the effectiveness of the methods. If a model considers each label independently and ignores label correlations, it is no different from solving a multi-class classification problem. This paper does not consider label correlations, raising questions about whether the proposed theoretical framework can substantially enhance multi-label learning methods.
2. It is well known that in the era of big data, multi-label problems often face the issue of extreme multi-labels, where the scale of labels is very large. In such cases, implementing the LSRL framework is nearly impossible due to its high complexity. Therefore, although the authors propose better generalization bounds, the computational cost required to achieve this generalization is extremely high. The paper lacks an analysis of the complexity of multi-label methods under the LSRL framework. The authors are requested to provide further analysis in this regard.

Limitations:
Theory should serve practice or guide the design of better algorithms. However, this paper lacks experimental validation of the theory, particularly including a comparison of the effectiveness of multi-label methods under the LSRL framework with other methods that have larger generalization errors. The paper also lacks numerical validation of the theory and verification of whether the theoretical assumptions are easily met.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
168NLzTpw8;"REVIEW 
Summary:
This paper explores the Multi-modal Large Language Model (MLLM) based Referring Expression Generation (REG) task, which aims to generate unambiguous text descriptions for specific objects or regions in images. MLLM-based REG models tend to suffer from hallucination issues, and there is a trade-off between detailed descriptions and accurate targeting. To address this, a training-free method called ""unleash-then-eliminate"" is proposed, which elicits latent information in intermediate layers and uses cycle-consistency decoding to reduce hallucinations. Extensive experiments on the RefCOCOg and PHD benchmarks show that this method outperforms existing approaches in both semantic and hallucination-related metrics.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The observation that the intermediate layers of the current region-level MLLMs sometimes hold more descriptive regional information than the final layer is interesting.
2. The writing throughout the paper is clear and easy to follow. The authors have done a good job in presenting their ideas and methodologies in a manner that is both logical and comprehensible.
3. The experimental results are compelling and demonstrate that the proposed method outperforms existing methods on the newly introduced evaluation metric.

Weaknesses:
1. Since the proposed method requires multiple layers for inference to be described, both REG and RES models are required, resulting in a very low efficiency of the method and requiring a lot of additional calculations and memory.
2.  The ablation study is not sufficient. For example, subset used to select the optimal layer in Layer Importance Measurement. What is the Impact of subset size and quality on performance? What is the performance of using  cycle-consistency-based candidate ranking process for whole dataset, i.e., not using Layer Importance Measurement.
3.  What is effect of using different RES models for the proposed method.

Limitations:
The authors adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an approach for improving the accuracy and richness of referring expression generation (REG) by leveraging the descriptive potential of intermediate layers in Multi-modal Large Language Models (MLLMs). The method employs a cycle-consistency-based decoding strategy to reduce hallucinations and improve descriptive quality. The proposed method is evaluated on the RefCOCOg and PHD benchmarks, demonstrating superior performance over existing methods.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The ""unleash-then-eliminate"" strategy and the use of intermediate layers for generating more detailed descriptions is innovative to me.

Weaknesses:
The proposed method introduces additional complexity, particularly in the decoding process. While effective, the cycle-consistency-based approach may increase computational overhead, which could limit its applicability in real-time or resource-constrained environments.

The related work section lacks depth in its analysis and could benefit from a more thorough review of recent advancements, particularly in hallucination mitigation techniques.

Some terms and notations, such as Q, H and W in Section 3.1 , are not clearly defined in the context of the paper, which can lead to confusion. Providing explicit definitions and clarifications would improve readability.

Limitations:
The authors have not adequately addressed the limitations and potential negative societal impacts of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper aims to strike a balance between detailed description and precise captioning when using multimodal large language models (MLLM) in the task of referring expression generation (REG). A key observation is that the output of a Referring Expression Segmentation (RES) model should be consistent with the input of a REG model.

Based on this observation, a training-free method has been proposed, called 'unleash-then-eliminate,'  which adopts an 'elicit-then-eliminate decoding' strategy. Captions are generated using contrastive decoding (Li et al., arXiv:2210.15097) and then fed into a RES model to produce corresponding masks. The Intersection over Union (IoU) between the masks tagged by the RES model and the input of the REG model is leveraged to select the candidate layer along with the generated caption. Additionally, a Probing-based Importance Estimation method is proposed to accelerate the decoding process.

The generation quality of the model is evaluated using the METEOR score on the RefCOCOg dataset. Additionally, the model's performance in avoiding hallucinations is assessed with the CHAIR and PHD metrics. The model proposed in this paper outperforms both Osprey and Dora not only in terms of generation quality but also in terms of adequacy.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
1: poor

Strengths:
1. The problem definition and motivation are clear. The paper indicates that a balance between detailed description and accurate targeting is necessary when using MLLM.

2. The model architecture is clear. The proposed model utilizes contrastive decoding to unleash the information in intermediate layers for generating captions. It indirectly evaluates the quality of these layers by assessing the masks generated by a RES model in relation to the captions. Additionally, a Probing-based Importance Estimation method is proposed to expedite the decoding process.

Weaknesses:
1. The writing is disorganized and lacks clarity. For example, in the first sentence ""Referring expression generation (REG) Yu et al. [2016], Mao et al. [2016], Hu et al. [2016], Yu et al. [2017], Luo et al. [2020], Ding et al. [2021], Tanaka et al. [2019] is a task to"", the reference is mixed with words, making it difficult to follow.

2. The proposed approach is technically simple. The architecture seems simply a combination of REG and RES models.

3. The motivation is to achieve a trade-off between the granularity and accuracy of captioning by selecting different intermediate layers. However, the experiments only show that the proposed model outperforms other models, as indicated in Table 1. Furthermore, Table 2 reveals that the first bucket of layers achieves the best scores in both METEOR, which relates to generation quality, and nCHAIR_I, which relates to the avoidance of hallucinations. This finding contrasts with the initial proposal.

4. The answer to `Open access to data and code` is [Yes], but there's only a `placeholder` found in the anonymous repo,  noted as footprint in the first page of paper.

Limitations:
Yes.
The authors claim that since the model has not been tuned on a specific dataset, the generating performance is suboptimal compared to training-based methods.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses the Referring Expression Generation (REG) task using Multi-modal Large Language Models (MLLMs), identifying the key challenge as the trade-off between generating detailed descriptions and accurately targeting the referring objects, which often leads to hallucinations—the inclusion of incorrect or spurious information in the generated text. To address this issue, the authors propose a training-free ""unleash-then-eliminate"" method that leverages the intermediate layers of MLLMs and employs a cycle-consistency-based decoding strategy to mitigate hallucinations. The proposed approach is validated through extensive experiments on the RefCOCOg and PHD benchmarks, demonstrating superior performance compared to existing techniques.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is in well-written, which makes it easy to understand.

2. The proposed ""unleash - then - eliminate"" method is training-free, which can avoid the need for additional data and training, reducing the complexity and cost of the model.

3. I like the idea of utilizing the latent information in the intermediate layers of the current region-level MLLMs, which is often overlooked but contains more descriptive regional information.

4. The cycle-consistency-based decoding method helps to alleviate the production of hallucinations in the generated sentences, improving the accuracy and reliability of the model's output. The hybrid layer importance measurement strategy not only increases the decoding speed but also maintains the ability to mitigate hallucinations, achieving a good balance between efficiency and performance.

5. The method shows superior performance compared to existing methods on both semantic and hallucination - related metrics in the experiments, demonstrating its effectiveness.

Weaknesses:
1. Without tuning in the specific dataset, the generating performance of the method might be suboptimal compared to the training - based methods.

2. The methods used in the paper, such as cycle ranking, may introduce additional computational load, which could affect the per-sample decoding speed. Although the proposed strategy helps to alleviate this issue, it may not completely solve it.

3. The method assumes that the RES model can accurately estimate the region-aware descriptive performance of the captions generated by the candidate layers. However, the RES model may also have its own limitations and errors, which could affect the final evaluation results.

Limitations:
refer to weaknesses

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
Ss7l98DVvD;"REVIEW 
Summary:
Wild-GS proposes a heuristic appearance decomposition strategy to deal with arbitrary images captured in the wild. Specifically, the authors decompose the appearance of each Gaussian into three components: global appearance, local appearance, and intrinsic features. Compared to existing methods, this paper achieves the highest visual quality and the fastest training and rendering speed.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. the paper is well-written and easy to understand.
2. this paper proposes a novel hierarchical appearance decomposition method, achieving high-quality appearance transfer from arbitrary images (no matter inside and outside the training dataset).
3. Wild-GS achieves SOTA performance on three in-the-wild datasets compared with baselines (NeRF-W, Ha-NeRF, CR-NeRF)

Weaknesses:
1. **Lack of Novelty**. First, this task is boring because it has already been successfully addressed in NeRF, making it likely that it can also be applied to 3D-GS. Second, in line 67, the authors summarize the contributions into four points. However, I believe that the first, second, and fourth contributions can be considered as a single contribution, while the third contributions are inherited from 3D-GS. Therefore, this paper can be seen as having only one main contribution. Third, although the authors discuss the comparison with concurrent in-the-wild 3D-GS works, they do not directly compare with them due to the absence of released code. However, Scaffold-GS [1] and Octree-GS [2] can also handle in-the-wild images effectively due to their appearance MLP. Additionally, VastGaussian [3] also proposes an appearance embedding module by the CNN network. Considering this, I believe that Wild-GS may not surpass them in terms of visual quality if not consider the transient objects.

2. **Confused of Module Design**. I struggle to understand the motivation for using a triplane to represent local appearance. In line 14, the authors clarify that they aim to explicitly align pixel appearance features with corresponding local Gaussians. This raises a question: it appears that the authors require only a continuous volume representation. Therefore, any representation that provides a continuous volume, such as triplane, vector and plane components like TensoRF [4] or hash-encoding, could be suitable.

3. **Some sentences seem to overstate their claims.** In lines 160 and 182, the authors describe a local appearance design intended for physical interactions, such as distinct specular highlights and shadows. However, I have not seen any experiments that demonstrate this, and there is even no ablation experiment without the local feature. Similarly, in lines 161 and 216, the authors explain that they maintain a learnable intrinsic feature, which is said to represent inherent material properties. I am curious to know what the results would be if there were no global and local appearance features.


[1] Scaffold-gs: Structured 3d gaussians for view-adaptive rendering

[2] Octree-gs: Towards consistent real-time rendering with lod-structured 3d gaussians

[3] VastGaussian: Vast 3D Gaussians for Large Scene Reconstruction

[4] Tensorf: Tensorial radiance fields

Limitations:
the authors discuss the limitation in the appendix.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors propose a method that adopts recently introduced Gaussian Splatting to work in an in-the-wild setting. The major contribution introduces a decoupling between the global and local changes to the splats. A part of the framework shows how to leverage a given point cloud (from the camera calibration) to condition splats so that they can reproduce local variability in the scene. The experiment results show that the proposed approach improves over the past works, and the introduced components are necessary to obtain them.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed method is novel regarding the Gaussian Splatting applications,
- The qualitative and quantitative results show that the method performs better than the selected baselines.
- Additionally, the ablation study clearly shows that all the components are necessary to obtain the presented results.
- The extraction of features using a point cloud is an interesting novelty that may be used in future research.

Weaknesses:
- The model is complex in terms of the number of used components. It uses a pretrained Depth Anything model (for the depth prediction), a 2D UNet that encodes the reference image into a global descriptor, and a 3D UNet that processes the triplane representation. In such a case, how does a method that learns the representations (the global descriptor and triplane representation as local descriptors) in an auto-decoder version (as in NeRF-in-the-Wild) perform?
- As it is the first method (I am not including the preprints here or recently accepted) that applies Gaussian Splatting to the in-the-wild setting, a simpler baseline would strengthen the evaluation. How would a simple learnable per-image latent concatenated with each Gaussian would perform in such a setting?
 - All the chosen baselines have their codebases publicly available. I do not understand why the authors limited their evaluations to 3 scenes only.  In contrast, NeRF-in-the-Wild uses 6 scenes in total from the Phototourism dataset.
- Some parts of the paper need further explanation, notably:
    - Why is the cropping necessary? It would make sense in the case of triplanes that exceed 1024x1024 (for example) pixels in resolution. However, it seems that such a resolution would suffice.
    -  Is the compliment learnable vector $v$ the same for all gaussian outside of AABB? 
    - What does ""Efficiency"" in Table 1. denote exactly?

Limitations:
The limitations of the paper are clearly stated in the supplementary section. No negative societal impacts are mentioned. However, those do not seem to be of high importance. I suggest the authors mention that next to the limitation section.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new pipeline, which is based on 3D Gaussian Splatting, for in-the-wild rendering. Wild-GS decomposes the appearance into global feature vector, local feature encoded in triplane features and per-Gaussian intrinsic features. Wild-GS achieves best performance on three scenes, while keeping the training and rendering efficiency from 3DGS. However, the novelty is limited since Wild-GS mainly combines existing techniques and replaces NeRF with 3DGS. Besides, the triplane representation may be not suitable if the scenes further scale and become more complex, e.g., self-occlusions.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is overall well-written.

Wild-GS achieves better performance than existing methods, while keeping the training and rendering efficiency from 3DGS.

Weaknesses:
Limited Novelty. Wild-GS mainly replaces NeRF with 3DGS for the in-the-wild rendering setting and combines existing techniques. Global appearance encoding is estimated in a similar way as Ha-NeRF. Unsupervised visibility mask is also similar to Ha-NeRF. Depth regularization follows FSGS. Combination of triplane features and 3DGS is motivated by previous works, e.g., TriplaneGaussian. Projecting 3D point cloud to generate triplane features is similar to ConvOccNet [1*]. 

The triplane representation is mainly used in scenes that are not very complex. For example, EG3D works on human faces and objects, while LRM [2*] works on objects. Though the three scenes tested in the paper are buildings, their structure are still relatively simple. If the scenes further scale and become more complex, e.g., self-occlusions, using triplane representation may not be the best choice.

Learned visibility masks should be visualized as Ha-NeRF and CR-NeRF to understand to which extent the model removes transient objects.

[1*] Peng et al. Convolutional Occupancy Networks. ECCV 2020.

[2*] Hong et al. LRM: Large Reconstruction Model for Single Image to 3D. ICLR 2024.

Limitations:
Limitations are discussed in supplementary. For societal impacts, privacy should be concerned since in-the-wild images usually contain people’s faces.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
**Summary
The paper presents a method called Wild-GS, an adaptation of 3D Gaussian Splatting (3DGS) designed for creating realistic novel views from a collection of unconstrained photographs, such as those taken in varied tourist environments. The method addresses the challenges of dynamic appearances and transient occlusions by employing a hierarchical appearance modeling strategy that decomposes the appearance into global and local components, along with intrinsic material attributes for each 3D Gaussian. Wild-GS introduces an explicit local appearance control using triplane representation, which aligns high-frequency details from reference images to 3D space, and incorporates depth regularization and transient object handling to improve geometric accuracy and rendering quality. Extensive experiments demonstrate Wild-GS's superior performance in rendering efficiency and quality compared to existing techniques, with the promise of publicly available code post-review.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
**Positive points
1. Wild-GS achieves state-of-the-art rendering performance with significantly improved efficiency in both training and inference times.
2. The method uses triplane representation for explicit local appearance modeling allows for the transfer of high-frequency detailed appearance from reference views to 3D space.

Weaknesses:
**Negative points
1. How do you densify and prune the 3D Gaussians, what is the starting iteration and ending iteration and the interval iterations? will the inaccurate mask affect the procedure of the densify and pruning? How does the method perform on the in-the-wild-images? The authors can show more reconstructions in the wild and provide high-resolution images in the main text to make the results of the authors' method more convincing.
2. Will increasing L{m} increasing M{ir}, leading to masking everthing?
3. It would be better to introduce the details of evaluation, for example, NeRF-W, Ha-NeRF and CR-NeRF are test with only half of the images, does the authors uses the same evaluation implementation?
4. From the ablation studies, why does the performance of the baseline method, such as w/o depth in Table.1 also performs well, outperforming existing methods with a large margin?
5. how to ensure the extracted global appearance embedding controls the LF appearance changes?

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
jBf3eIyD2x;"REVIEW 
Summary:
This paper proposes a query-based adversarial prompt generation method. It eliminates the prior attack's dependence on adversarial transferability and local surrogate models. The attack can evade the OpenAI and Llama Guard safety classifiers with a near 100% success rate.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
## Originality
* The proposed improvement is interesting and novel.
* The related works are generally comprehensive.

## Quality
* Several practical considerations are included.
* The breadth of evaluations is good.

## Clarity
* The first three sections are generally well-written and easy to follow.

## Significance
* The proposed technique seems simple, but the overall contribution towards query-based attacks is important.

Weaknesses:
## Originality
I don't have major concerns here. The technical modifications of the existing attack GCG seem simple, but the good results can justify them. It is suggested to summarize and highlight the technical challenges.

## Quality

**Q: The evaluation setting is quite confusing.**

1. The paper was a bit vague regarding the attack's objective. Is it for a general jailbreaking attack (like GCG) or just eliciting the model to repeat a given harmful content? This also makes it hard to understand the attack success rate -- when do we count an attack as successful?

2. The evaluations do not match the major contribution. In Section 4, most of the evaluations are focused on the surrogate-based attack on various settings, including the actual GPT-3.5 model (a side question is why GPT-4 was not included since a black-box attack should work regardless of the underlying model). The actual contribution, the surrogate-free attack, was only evaluated on open models and then a different task of content moderation. The paper did not justify why the surrogate-based attack was not evaluated in the latter settings, and why the surrogate-free attack was not evaluated in the former settings. Ideally, the primary attack is supposed to be evaluated against all involved settings, and the surrogate-based attack is provided as a reference or ablation study.


## Clarity

* L33-45. It was a bit confusing when you first called out the surrogate-free property, then switched to a surrogate-based attack, and only finally, the further optimization that removes the surrogate model.
* L64-66. The high success rates in black-box attacks against classification tasks depend on two factors: the number of queries and the threshold perturbation. Most black-box attacks have 100% success rates at the first few queries but the inputs are either pure noise or heavily perturbed.
* Figure 1b. Please use different styles to distinguish curves of different groups of proxy or target. Right now, it is very hard to tell what the figure conveys and what we can learn when comparing curves with the same proxy size.

## Significance
N/A

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper modifies GCG, an attack on LLMs to elicit harmful responses, to create a query-based black-box attack with two primary goals: 1) Enable targeted attacks that are not possible with simple transfer-based attacks and 2) Enable attacks to still occur when no feasible surrogate model exists. To modify GCG, the key change is that the authors maintain a min-max heap of possible adversarial suffix candidates, searching for word replacements on the best currently known suffix candidate. The authors also experiment with prompt initialization, finding that including the target as many times as fits to be useful. Finally, as a query-optimizer in a proxy-free setting with random tokens instead of gradients, the authors experiment with trying a word replacement once in each position, and then taking the best position and trying more word replacements there. The authors find they are able to generate targeted attacks, which transfer attacks cannot, and can attack real-world models with proxy-free attacks.

Soundness:
1: poor

Presentation:
3: good

Contribution:
2: fair

Strengths:
Strengths include 1) enabling targeted attacks to elicit specific phrases, 2) enabling proxy-free attacks on real-world models without a surrogate, and 3) including optimizations to manage the query budget in the proxy free setting. Thus, the paper makes strides towards more practical attacks with adversaries with less power. The authors have also found their initialization strategy to help improve attack quality.

Weaknesses:
The primary weaknesses of this paper include a lack of comparisons against other black-box attacks (e.g., Andriushchenko et al., ""Jailbreaking Leading Safety-Aligned LLMs with Simple Adaptive Attacks"", and Lapid et al., ""Open Sesame! Universal Black Box Jailbreaking of Large Language Models"") on the proxy-free attacks. This also limits the technical novelty, as the attack as mostly a series of small changes to GCG. The other weakness is that this paper does not evaluate a wide variety of different types of LLMs - one of the themes is that the proposed attack is useful in settings where there is no good surrogate, so it would be nice to see how the attack does against transfer attacks across different families of LLMs.

Limitations:
Adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel query-based attack method designed to generate adversarial examples that induce harmful outputs in aligned language models. Building on the GCG attack by Zou et al. (2023), this method employs a query-based strategy that eliminates the need for transferability, resulting in a significantly higher success rate. The effectiveness of the attack is demonstrated on GPT-3.5, even in black-box scenarios. Additionally, the paper details universal and prompt-specific attack techniques that effectively circumvent OpenAI’s safety classifier, achieving near-perfect evasion rates.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
+ Despite the work from Zou et al. (2023) disclosing the vulnerability of production LLMs to adversarial examples capable of producing harmful strings and behavior, this paper overcomes some limitations related to attack transferability. The authors introduce a novel approach that leverages the API of production LLMs, like GPT-3.5, significantly improving the success rate of attacks and demonstrating the fragile behavior of LLMs in the presence of a smart adversary.

+ The paper is well-written, clearly describing each step of the attack, including strategies to bypass OpenAI’s API restrictions. For instance, the method for calculating the log probabilities is particularly smart and well-elaborated.

+ Overall, the experimental evaluation is comprehensive and provides valuable insights. For example, the results in Table 1 shows the limitations of previous attacks like CGC and AutoDan, whereas the attacks proposed by the authors are very effective both in white-box and transfer attack settings. 

+ The authors also analyzed the effect of universal attacks, revealing interesting results in terms of the trade-off between the budget/cost and the effectiveness of the attack.

Weaknesses:
+ Although it is a relevant attack targeting a commercial system like ChatGPT 3.5, the scope of the attacks is somewhat limited as it requires API access to provide the logprob and only works with GCG. Nevertheless, I think that the paper clearly shows the risks of providing access to the logprobs in the API. 

+ The paper only focuses on the attacker’s perspective. Defenses or possible mitigations are not properly discussed in the paper.

Limitations:
The limitations have been addressed appropriately in different sections of the paper, including, Section 3.2.1, and Appendix B, for example.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper delves into the topic of adversarial examples and prompt injection attacks on aligned language models. A new strategy (GCQ) for black-box adversarial attacks is proposed which does not need access to a surrogate model, but only uses black-box access to the target model. This strategy is an extension of GCG, the current state-of-the-art attack. The proposed method works against both generative models and safety classifiers, as the experiments show.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The experimental protocol covers a range of setups, including white-box, black-box, and universal attack.
- The main novelty of the paper is the no-proxy version of GCQ.
- The proposed attack GCQ shows good practical performance, in both white- and black-box setups, when compared to the existing state-of-the-art attack GCG and AutoDAN.
- Implementation included.

Weaknesses:
Significance:
- It would be great to include more attack baselines, e.g. AutoDAN attack from [Zhu et al., 2023].
- One of the most relevant qualities of the method is the independence of surrogate models. It seems relevant to compare against other attacks from the same category, e.g., PAL (cited as [28] by the paper, but not used as baseline).
- It also makes sense to include the GCQ without proxy access in Tab. 1.
- The topic of performance of the attack against existing defenses is not covered.

Novelty:
- For the GCQ version that still relies on a surrogate, the changes w.r.t. GCG seem minor.

Minor:
- Additional proofreading seems necessary.
- Sec. 4.2 and 4.6 are missing from the outline at the beginning of Sec. 4.
- L217 ""hamful"" -> ""harmful""

## References

- [Zhu et al., 2023] Sicheng Zhu, Ruiyi Zhang, Bang An, Gang Wu, Joe Barrow, Furong Huang, Tong Sun. AutoDAN: Automatic and Interpretable Adversarial Attacks on Large Language Models. https://openreview.net/forum?id=ZuZujQ9LJV

Limitations:
Ok

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
Tcft2V63Vd;"REVIEW 
Summary:
The paper studies the effects of layer widths on neural network performance. By studying the affects of the weight norm in different channels, the work discovers several distinct stages of training, which are apparent across different modalities and architectures. The authors also show how the insights could help in improving network performance with the same computational costs.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
The paper has great merit, and discovers interesting dynamics in neural network training. Further, the dynamics are consistent, and come from an intuitive intuition of NN training. It is exhibited across different architectures and datasets, with an ability to also improve networks using these insights.

Weaknesses:
The paper is really great! It could benefit from more experiments and archs, but the current experiments are convincing.

Limitations:
N/A

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a method to optimize neural network layer width by analyzing the variance of weight norms across channels during training. This approach helps determine if a layer is sufficiently wide. Empirical validation shows that adjusting layer widths based on these patterns can reduce parameters and improve performance across various models and datasets.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. This paper proposes a novel way to assess layer width sufficiency using weight norm variance.

2. The paper shows robust experimental evidence across multiple datasets and model architectures.

3. Besides the application on the model width optimization, the paper also offers a deeper understanding of training dynamics through the identification of distinct patterns in weight norm variance, which could be valuable for other related areas.

Weaknesses:
1. The layer width optimization is related to channel pruning and NAS-based channel number search, which have achieved significant success in finding optimal layer widths. I think these methods should be discussed and compared in the paper.

2. Measuring the weight norm would introduce additional computation cost, can the authors discuss the complexity of the method and report how much additional time does the method introduce?

3. Can the authors provide more detailed and theoretical analysis on the choice of the metric? For example, there are other metrics such as gradient norm, Hessian matrix, and absolute weight value to evaluate the weight importance and training statistics, why the proposed metric is better?

Limitations:
The limitations have been adequately discussed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tries to address an issue in deep neural networks: the trade-off between computational cost and performance, particularly focusing on the width of each layer. Traditionally, layer widths in neural networks are determined empirically or through extensive searches. The authors propose a novel approach by examining the variance of weight norms across different channels to determine if a layer is sufficiently wide.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The authors identify patterns regarding the variance of weight norm between different channels during training. Some layers exhibit an ""increase to saturate"" (IS) pattern and other layers show a ""decrease to saturate"" (DS) pattern. These patterns have some connection regarding the inter-channel similarities given a layer.
2. They redesign the width of classical CNN models like ResNets and VGGs according to their findings. And the empirical results provide some support for their arguments.

Weaknesses:
1. The empirical justification of the proposed method is somehow not convincing enough. From Table 2, we can see that channels within layers with IS patterns are also hard to merge, like layers 8 to 11 in Table 2. From Figure 12 in the Appendix, we can see that the similarity gradually decreased first and then increased for middle layers with DS patterns, and it is hard to say there is a hard cutoff.
2. Suppose the proposed argument is correct, then it seems challenging to use this method in practice. The width of each layer depends on the IS or DS pattern of the original model, so you need to train the original model first and then the model with the streamline width. This always increases the total training cost especially when scaling up the model. 
3. The authors claim that they have similar observations for ViTs in lines 58-62, but no results are given. Since the original ViT has a uniform width, I am wondering whether the arguments still hold for models with uniform width.
4. The experiment setup is not very convincing. For CIFAR-10/100 scales of datasets, ResNet-18 or ResNet-50 are large and they have many redundant parameters. In the original ResNet paper, they used different architectures for CIFAR datasets with a much smaller number of parameters (0.27M to 1.7M with ResNet-20 to ResNet-110), please see Table 6 in ""Deep Residual Learning for Image Recognition"". The performance of ResNets and VGGs is much lower than public baselines for CIFAR-10 and CIFAR-100. Please see these two repo: https://github.com/weiaicunzai/pytorch-cifar100 and https://github.com/kuangliu/pytorch-cifar.

Limitations:
Addressed in conclusion.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper investigates the relationship between the differences in weight norms across channels and the adequacy of layer widths. The authors suggest that knowing these patterns(IS/DS) can help set layer widths better, leading to better resource use, fewer parameters, and improved performance in different network designs. The experiment shows that narrow-wide-narrow streamline network would boost the performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: Yes, this paper is the first work that studies the width of network from the perspective of variance of weight norm. 

Quality. Good, clear figures and enough experiments.

Clarity. Good, the organization of the paper is well

Significance. Yes, it provide a new metric to adjust the width of the network.

Weaknesses:
To determine the appropriate width, your method should involve training the model and observing various patterns of weight norm variance to see if the width is sufficient. However, retraining the model to decide on the width can be time-consuming. Considering that IS/DS indicate whether layers learn similar neurons, could we use the cosine similarity of a pretrained model to directly assess if the width is sufficient?

Limitations:
No apparent limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
aXS1pwMa8I;"REVIEW 
Summary:
The paper addresses 3D surface reconstruction from point clouds. It proposes a patchwise rotation equivariant neural network to map query points to their 3D displacement to the surface. The local rotation equivariance allows weight-sharing between similar patches at different orientation, and  displacement fields have been shown to outperform occupancy and distance fields. Experiments show the method outperforms the baselines on several datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
S1) Using equivariant models to promote better use of model capacity is a great idea. Point clouds often present patches that are similar up to rotation so this design choice makes a lot of sense.

S2) Results are strong and seems state-of-the-art on surface reconstruction from point clouds.

Weaknesses:
W1) As far as I understand, the ideas in the paper are not novel, so the contributions are around  combining existing ideas. a) NVF [1] introduced the idea of using vector instead of distance fields, b) E-GraphONet [2] uses the idea of rotation-equivariant models for implicit surface representation, c) Zhao et al [3] uses the particular way of achieving equivariance through SVD on point sets. This might not be a deal-breaker since the results are good but more novel ideas would make for a stronger submission.

W2) I think the PCA-based alignment is not very robust. While it is perfectly rotation equivariant given the exact same point cloud patch on a different orientation, I think in practice we would see slightly different patches so the alignment is not guaranteed. Moreover, the way the ambiguity on the axis orientation is resolved seems to rely on the furthest point position so a moving a single point slightly might change the orientation drastically. There are other methods that are use equivariant layers which seem more appropriate such as Vector-Neurons [4] and SE(3)-transformers [5], why weren't they considered?

W3) Given W2 I found the design decision of using the PCA-alignment quite arbitrary. Given that the most related works are NVF and E-GraphONet, I believe a more natural choice would be to modify E-GraphONet to predict vector fields instead of occupancy fields, would it perform better than the proposed method? If the goal is to show that the PCA-alignment can be better than equivariant layers, a comparison against E-GraphONet on occupancy field prediction should have been performed. 

## References:

[1] Yang et al, ""Neural Vector Fields: Implicit Representation by Explicit Learning"", CVPR'23.

[2] Chen et al, ""3D Equivariant Graph Implicit Functions"", ECCV'22.

[3] Zhao, ""Rotation invariant point cloud classification: where local geometry meets global
topology."", 2021.

[4] Deng et al, ""Vector neurons: A general framework for so (3)-equivariant networks"", ICCV'21.

[5] Fuchs et al, ""SE(3)-Transformers: 3D Roto-Translation Equivariant Attention Networks"", NeurIPS'20.

Limitations:
The limitation regarding robustness of PCA-alignment should be addressed more clearly (see W2).

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies a simple task: input dense point cloud and output the implicit surface reconstruction of the geometry. To achieve this goal, the model uses an ""equivariant"" network to predict the displacement field. Since the input point cloud is dense, this paper crops the nearest patch on the surface point cloud to the query space point and uses PCA to canonicalize the patch. Once aligned, a transformer will predict the query points displacement vector to the surface. Since the PCA canonicalization is known, the displacement can be transformed back. This simple task is evaluated on shape and scene data.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The insight of reusing elementary shapes with different poses is good.
- This paper, although straightforward, considers using equivariance to model such elementary shapes/local intrinsic patterns.

Weaknesses:
- Heuristic baseline: I have a feeling that the model may depend very much on the dense KNN queries of the surface point cloud, in other words, the network is learning a potentially too easy task, just find the nearest point (or interpolate the nearest point) in the patch and compute a displacement to it if the point patch is too dense. A heuristic baseline could be just fitting a small parametric (polynomial, or even plane) to the nearest patch, and analytically computing, or directly finding the nearest point to produce the displacement vector.
- Noise/Sparse/Partial data? The task might be too easy in the current literature. Since the input is a completely dense point cloud, the geometry is almost given, and this work still depends on the surface point KNN queries to do the canonicalization, so what happens if the input observation is partial, sparse, or noisy?

Limitations:
Some limitations are discussed in the end.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce the 3D Patch-level Equivariant Implicit Function (PEIF), leveraging a 3D Patch-level Pose-Invariant Representation (PPIR) to address the surface reconstruction task. To overcome the limitation that existing Implicit Neural Representations (INRs) are not equivariant to 3D rotation, they develop PEIF to encode both equivariant and invariant information, thereby enhancing generalization to unseen 3D rotations. The SE(3)-equivariant implicit function is optimized using displacement optimization loss and patch discrimination loss with ground-truth 3D models. Experimental results on surface reconstruction datasets validate the effectiveness of PEIF.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Motivation: The study is well-motivated, addressing the redundancy in existing INR-based methods concerning local orientation-normalized patches. Moreover, the current methods are weak against unseen rotations of local shapes.

2. Technical Novelty and Soundness: The introduction of local pose-invariant representation for SE(3) equivariant implicit function is novel for 3D surface reconstruction. Patch-based pose normalization facilitates efficient training without the need for rotation augmentation.

3. Verification of Rotational Robustness: The authors demonstrate the rotational robustness of the proposed method, as shown in Table 4.

4. Performance Improvement: The proposed PEIF achieves superior performance compared to both equivariant and non-equivariant surface reconstruction methods on the ShapeNet, ABC, and SyntheticRooms datasets, as indicated in Tables 1 and 2.

Weaknesses:
1. Rotational Robustness: Further clarification is needed regarding the experimental settings in Table 4. Specifically, it is unclear whether ""w/o rotation"" and ""w/ rotation"" refer to rotation augmentation during training or testing.

1. Missing Citations: A similar approach exists in 2D pixel-level correspondence, as detailed in the work by Lee et al. (CVPR 2023). This study also utilizes local-level dominant orientation from rotation-equivariant features and normalizes the equivariant feature using the dominant orientation for an invariant descriptor. It would be beneficial to cite this work and discuss the similarities and differences. 

[A] Learning Rotation-Equivariant Features for Visual Correspondence (Lee et al., CVPR 2023)

1. Computational Cost: There is a lack of discussion regarding the computational cost of the proposed PEIF. Information on computation time and memory consumption, and a comparison with E-GraphONet would be valuable.

Limitations:
I recommend a weak accept score for this paper due to its strong motivation and technical novelty in addressing the limitations of existing INRs with respect to 3D rotation equivariance. The introduction of the 3D Patch-level Equivariant Implicit Function (PEIF) and its verification of rotational robustness demonstrate a significant advancement in 3D surface reconstruction, achieving state-of-the-art performance on multiple datasets. 

However, there are some limitations that should be addressed. The experimental settings regarding rotational robustness need further clarification. Additionally, the paper lacks citations to related works in 2D pixel-level correspondence that employ similar techniques, which would strengthen the discussion of novelty and prior art. Finally, the computational cost of PEIF, in terms of computation time and memory consumption, is not discussed, leaving questions about its practical applicability compared to existing methods. Addressing these points would enhance the overall contribution and clarity of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors address the task of surface reconstruction. They propose a patch-level pose-invariant representation of 3D objects, which is employed in the design of a patch-level equivariant implicit function. The proposed PEIF framework is composed of three modules: the spatial relation module, the patch feature extraction module, and the intrinsic patch geometry extractor. They authors demonstrate the effectiveness of the proposed framework for the surface reconstruction task through comprehensive experimental evaluations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The authors introduce a novel pose normalization scheme and a displacement predictor that employs the proposed pose normalization scheme, accompanied by rigorous proofs
- The proposed method shows the state-of-the-art performance in the surface reconstruction task, surpassing other equivariant method (i.e. E-GraphONet)
- The proposed method shows the state-of-the-art performance in the cross-domain evaluation setting

Weaknesses:
- The proposed method exhibits a significantly longer inference time compared to other equivariant methods (E-GraphONet). It appears that the majority of thie increased inference time results from the computation of the SVD. A detailed analysis of the inference time would be beneficial for a more comprehensive understanding of the proposed method
- The proposed method shows comparable performance compared to GeoUDF (which is not an equivariant method). This raises questions about the necessity of using an equivariant method for the surface reconstruction task
- The authors claim that the proposed pose-invariant property is intended to enhance the cross-domain generalization ability. To validate this, the cross-domain experiment should include the result of E-GraphONet (which is also pose-invariant)
- In table 4, it seems that other algorithms also quite robust to rotation changes. The authors are encouraged to provide further explanation on this observation.
- An ablation study concerning the three modules (i.e., the spatial relation module, the patch feature extraction module, and the intrinsic patch geometry extractor) is not provided. The inclusion of such a study would be valuable.

Limitations:
The authors have adequately addressed the limitations and societal impact in the main paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
7Mo1NOosNT;"REVIEW 
Summary:
This paper proposes the COLD (Causal reasOning in cLosed Daily activities) framework, aiming to bridge the gap between open-ended causal reasoning and symbolic representation-based question answering. 

The framework leverages human understanding of daily real-world activities to reason about the causal nature of events. The authors create a large set of causal queries and evaluate multiple Large Language Models (LLMs) on these queries. 

The findings show that causal reasoning is challenging for LLMs, even for activities considered trivial for humans. The authors also explore the causal reasoning abilities of LLMs using the backdoor criterion. 

The key contributions of this work are the development of the COLD framework, the creation of a substantial number of causal queries, and the evaluation of LLMs' performance on causal reasoning tasks. The findings highlight the need for further analysis using real-world events to properly validate LLMs' understanding of causality.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The COLD framework effectively bridges the gap between open-ended causal reasoning and symbolic representation-based question answering, utilizing human understanding of daily activities as a solid foundation.
- The paper addresses the crucial issue of evaluating LLMs' causal reasoning capabilities, emphasizing the significance of investigating and validating their intellectual capabilities.
- The paper is well-written, with clear explanations, logical flow, and concise language, ensuring effective communication of key points.
- The evaluation of multiple LLMs on a large set of causal queries reveals limitations in their causal reasoning abilities, while the exploration using the backdoor criterion provides valuable insights into causal strength between events.

Weaknesses:
Generally, I believe this paper makes good contributions. However, there are some minor issues that need to be addressed:

- Since the queries are mostly automatically generated, it is necessary to support them with human annotations or expert evaluations in order to confirm the reliability of the generated queries.
- The observational graphs are created through human annotations, which limits their capacity to cover a wide range of concepts. It would be better to discuss automated approaches for constructing such graphs in order to facilitate large-scale causal benchmarking.
- Can the synthesized queries be used for fine-tuning? I am interested in whether splitting the observation graphs into different sets and training them on queries synthesized from the training graphs would significantly improve performance. This could greatly enhance the comprehensiveness of the paper.
- Additionally, another set of baselines focusing on zero-shot commonsense question answering should be evaluated as well. It would be interesting to see whether transformations from commonsense knowledge bases can benefit causal reasoning tasks. I recommend checking these papers for reference.
  - Ma, K., Ilievski, F., Francis, J., Bisk, Y., Nyberg, E., & Oltramari, A. (2021, May). Knowledge-driven data construction for zero-shot evaluation in commonsense question answering. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 35, No. 15, pp. 13507-13515).
  - Wang, W., Fang, T., Ding, W., Xu, B., Liu, X., Song, Y., & Bosselut, A. (2023, December). CAR: Conceptualization-Augmented Reasoner for Zero-Shot Commonsense Question Answering. In Findings of the Association for Computational Linguistics: EMNLP 2023 (pp. 13520-13545).
  - Kim, Y. J., Kwak, B. W., Kim, Y., Amplayo, R. K., Hwang, S. W., & Yeo, J. (2022, July). Modularized Transfer Learning with Multiple Knowledge Graphs for Zero-shot Commonsense Reasoning. In Proceedings of the 2022 Conference of the North American Chapter of the Association for Computational Linguistics: Human Language Technologies (pp. 2244-2257).
- Lastly, there are some grammar typos that need to be corrected. For example, the caption of table 3 should not have ""Language"" capitalized.

Limitations:
The authors have dedicated a section explicitly discussing the limitations of their proposed COLD framework and offering potential solutions. The in-depth discussion contributes significantly to the paper, enhancing its overall quality.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes causal reasoning in closed daily activities, which combines the causal reasoning works of open-ended causal reasoning via causal commonsense reasoning and symbolic representation-based question answering for theoretically backed-up analysis. By creating a dataset containing about 8 million queries, it can estimate the causal reasoning performance of pretrained language models. Moreover, the paper introduces theroies of causal inference (such as backdoor adjustment) to conduct in-depth analyses of causal reasoning in pretrained language model.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The number of constructed dataset samples is large.
2. The usages of ATE and backdoor adjustment are novel
3. The analyses of causal reasoning in pretrained language models are thorough and in-depth.

Weaknesses:
1. The paper is not well demonstrated, readers might be confused when reading sequentially
2. Just for the dataset itself, there is no differences between cold and copa. Even copa may have a better quality.
3. The dataset is limited to six scenarios, many of them are just paraphrases of the original ones.
4. Lack of human performance as a reference.

Limitations:
None

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a dataset for evaluating the causal reasoning capabilities of LLMs by grounding evaluation in human understanding of real-world daily activities. The authors address the gap between open-ended causal commonsense reasoning and symbolic question answering by introducing the COLD framework, which generates causal queries based on daily activities. The paper describes the creation of causal graphs through crowd-sourcing. The authors test different LLMs using these causal queries, and show that even simple causal reasoning tasks remain challenging.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. Its approach to evaluating causal reasoning in LLMs by grounding the evaluation in real-world activities. 

2. The use of a large dataset to create extensive causal queries.

3. The detailed experimentation with various open-source LLMs and the plan to publicly release the framework and datasets.

Weaknesses:
1. The data used in the study appears overly simplistic, focusing on basic daily activities which may not provide a robust test for causal reasoning. If the goal is to evaluate how well models understand causal reasoning I would stay close to how we perform causal inference in science (i.e. gather data, understand whether we have an identification strategy, compute treatment effects/learn causal graph).

2. Causal Commonsense Reasoning is not causal in the sense of statistical causality. Almost all uses of causality in science are either: 1. To estimate a treatment effect in the real-world 2. To discover a causal graph, again in the real-world. This is done either from interventional or from observational data. This data doesn’t capture any of this, so I wonder what the usefulness of it is in relation to the causality literature. 

3. The insights drawn from the experiments are not clearly articulated. What exactly has been learned from this new extensive dataset and all of the experiments? 
The ATE experiments are particularly hard to understand. ATE measures the marginal effect of a treatment on an outcome. Comparatively, this dataset includes binary questions about causal triplets. In the results the authors show accuracy measures, I don’t understand how to interpret this as the ability to properly estimate the effect, and I wasn’t able to understand how to do so from the authors’ description.

4. The examples provided (e.g. Figure 3) often lack clarity, as both options seem plausible effects without additional context or a causal graph.

Limitations:
The authors acknowledge limitations of their work, such as the restricted set of activities and the challenge of creating causal graphs for more complex long-term tasks.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a new causal reasoning dataset for LLMs. The main motivation of the dataset is to bridge the gap between casual commonsense reasoning datasets and symbolic representation-based causal reasoning datasets. Specifically, the proposed dataset collects crowd-sourcing observations, and constructs related causal graphs and triplets. This paper evaluates multiple open-source LLMs using the collected datasets, benchmarking LLM's ability to predict causal relationships, and to estimate average treatment effect.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The construction of the causal graph and the estimation of casual relationships are strict and sound. The whole dataset provides high-quality causal relationship annotations for real-life events.

2. A large number of open-source models are evaluated. The evaluation on ATE estimation also involves multiple estimation methods using LLMs.

Weaknesses:
1. Human performance is not provided in the empirical comparisons. This makes it hard to understand LLM's performances. One can argue that the performances are far from perfect. However, due to the inherent ambiguity of natural languages, and inherent disagreement in people's opinions, I would assume human performance will also be significantly lower than 100%. Adding human performance in these tables will make the comparisons more informative.

2. Many important details are in the appendix. I understand this is a dense paper with lots of content, however, the presentation and organization can be greatly improved. Additionally, the current results section includes a significant part on how to estimate ATE, which should be included in previous sections.

3. While the dataset itself is huge, there are only five different events (shown in Table 2). Therefore, it is unclear how representative the model's performance on this dataset will be. This is a significant limitation, especially due to the ""Causal Parrots"" phenomenon mentioned in the introduction.

Limitations:
Limitations are discussed in Sec. 6.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
dfiXFbECSZ;"REVIEW 
Summary:
The paper proposes LoFiT, a procedure for *localized* fine-tuning of LLMs. LoFiT chooses a task-specific subset of attention heads by tuning head-wise learnable scales and selecting the heads with largest scales (by absolute value). After that, the algorithm tunes the biases for the chosen attention heads to solve the chosen task. The paper includes experiments on three benchmarks using LLMs from Llama-2 and Gemma families. Authors compare LoFiT against alternative head selection methods and against several PEFT algorithms (LoRA and RED). The paper also analyzes the impact of heads found by LoFiT in different scenarios.

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. Authors propose a very simple algorithm for selecting heads that seemingly works well (within the 3 chosen tasks). The fact that it is simple is a significant advantage: such algorithm would be easy to modify or reuse in other circumstances.

2. The paper includes several ablations and sub-analyses that answer the questions that arise when reading it. This is a sign that the experiments are well structured.

3. The paper is generally well written, well organized and easy to read.

Weaknesses:
My main concern about the paper is that the main evaluations are limited to 3 tasks (TruthfulQA, MQuAKE, CLUTRR). This makes it unclear if LoFiT is generally applicable in place of PEFT methods or if it is only competitive for a specific type of fine-tuning task. If latter is the case, the paper would be substantially stronger if it explained which tasks LoFiT is capable of and where it isn't. If latter is the case, it would be best to include more tasks from among PEFT papers and general LLM fine-tuning scenarios. For instance, the papers you compare against also evaluate on GSM8K, MMLU,  MNLI-m, RTE, (super)GLUE for smaller models and more.

Naturally, I do not suggest that you evaluate on *all* of the benchmarks, but the paper could be strengthened by either showing that LoFiT generalizes to more tasks or describing (and demonstrating) the types of tasks where it works poorly. This would help readers understand where to use LoFiT (or its components) instead of other intervention / peft methods.

Limitations:
Authors have sufficiently addressed the limitations of the proposed method. The work can be improved by describing the limitations of the evaluation methodology in more detail.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces Localized Fine-Tuning (LoFiT) - a two step method that involves (1) localizing attention heads that are important for a given task, and (2) learning an additive intervention for each important attention head. The authors evaluate the method over various tasks, and show that LoFiT outperforms other inference-time intervention methodologies, and is competitive with PEFT methods despite being much more parameter-efficient.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well-written.
	- I found the paper easy to read - it is clear and well-organized.
- Strong results (Section 5 & 6)
	- LoFiT outperforms other inference-time intervention methods by a very significant margin (Table 1).
	- LoFiT is competitive with PEFT methods, despite being more parameter-efficient (Table 3).
- Interesting additional investigations (Section 5 & 6)
	- The authors go beyond mere evaluation, and ask interesting follow up questions.
	- They show that localization is important by comparing to a baseline of selecting random heads, that the set of important heads are generally task specific, and that LoFiT shows promise in generalizing out of distribution compared to other methodologies.

Weaknesses:
- Could benefit from more thorough comparison with ITI
	- The methodology is very similar to ITI, and as such the difference in performance (Table 1) is surprising. It would be helpful if the authors could explore why LoFiT is so much more effective than ITI.
	- One possible way to explore this would be to investigate the learned bias vectors directly. Are the bias directions found by LoFiT similar to those found using ITI? Are they very different? Do they have similar magnitudes?
- Lacks detail in inference-time intervention baselines
	- Appendix C.2 and D.2 provide some limited details, but it seems important to give more detail, to convince a reader that these baseline methodologies were evaluated properly. Dataset construction is very important for contrastive pair methods, as are the hyperparameters $\alpha$ and the layer $l$ of the intervention (for RepE). Values for $\alpha$ are given, but it would be good to give more detail as to how these values were selected. 
- Head-selection baseline
	- I am curious to know if learning $A_{l}^{i} \in \mathbb{R}^{d_{head}}$ is necessary, or whether one could simply learn a scalar $A_{l}^{i} \in \mathbb{R}$ to weight the entire output of head $(l, i)$. This would seem to simplify the method considerably, requiring optimization over only one parameter per head in the first phase.
	- If the simplified version does not perform as well, then I think the method as presented would be better justified.
	- I am also curious to know how the method behaves as $K$ is altered. Is it increasingly effective as $K$ increases? Does the performance difference plateau once $K$ is increased past some value? Concretely, a figure which has $K$ on the x-axis, and performance on the y-axis would be informative.

Limitations:
- The authors acknowledge the following limitations:
	- The paper only focuses on English-language evaluation, with short contexts.
	- The paper only explores 3 particular benchmarks. I think this one is particularly salient as a limitation.
	- The paper only evaluates models up to 13B parameters, and results may not extend to larger models.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a lightweight fine-tuning method that trains bias offsets for only a subset of attention heads, achieving significantly lighter adaptation compared to methods that fine-tune all layers, with minimal performance loss. The proposed method involves two steps. First, attention heads to fine-tune are selected using a scoring scheme; in this paper, the norm of learnable scaling factors is used for scoring. Second, offset vectors for the selected layers are trained. Experimental results demonstrate that LoFiT outperforms representation intervention methods by a large margin and shows performance comparable to parameter-efficient methods such as LoRA, but with far fewer parameters.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The concept of LoFiT is interesting as it represents a middle ground between representation steering and fine-tuning. The proposed procedure effectively addresses the main challenges in representation steering: 1) selecting layers and attention heads, and 2) determining the steering direction. LoFiT introduces a novel two-step procedure to address these problems using labeled data.
- LoFiT is efficient and delivers performance comparable to LoRA and other parameter-efficient fine-tuning (PEFT) methods.
- The experiments are comprehensive, and the analysis of attention head transfer and localization is insightful.
- The paper is clearly written and easy to follow.

Weaknesses:
- The comparison with representation steering may be somewhat unfair, as LoFiT requires labeled data and an explicit training stage, while representation steering methods (e.g., RePE) do not.
- The technical contribution is minor—the localization step is the main difference from RED, which is not very significant. Moreover, the overall idea of fine-tuning transforms for representations is shared with RED and ReFT.
- Although LoFiT employs a two-step process, the learned parameters (scaling factors) from the first step are discarded after attention head selection. It is unclear why the learned parameters are not used—is it mainly to differentiate this work from RED?

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
Y4mBaZu4vy;"REVIEW 
Summary:
This paper introduces the Efficient Graph Attention Potential (EGAP), a new architecture for Neural Network Interatomic Potentials (NNIP) designed for scalability and efficiency. The authors investigate scaling strategies for NNIP models and propose a model that leverages optimized self-attention mechanisms. They evaluate EGAP on the MD22 and OC20 datasets, claiming state-of-the-art performance and improved efficiency compared to existing models.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper addresses an important challenge in the field of NNIPs by focusing on scalability and efficiency.
2. The presentation of the model illustration is clear and appealing, aiding in understanding the proposed architecture.
3. The authors provide an investigation into scaling strategies for NNIP models, which could be valuable for future research in this area.

Weaknesses:
1. Dataset split inconsistency: There is a significant issue with the dataset split on the MD22 dataset. The authors mention using a 95:5 train-test split, stating: ""We use an EGAP model with six blocks and 48M parameters on each system and evaluate the performance on the test set (train-test split 95:5)."" However, this deviates from the splits used in previous works [1]. In prior studies, the training data was predefined (e.g., the AT-AT target utilized 3k training data, which is 15% of the total data). The 95:5 split in previous works was applied to the training data for train-validation splitting, not for train-test splitting. For instance, the AT-AT target data was split into 2,850 train, 150 validation, and 17,001 test samples. By utilizing a 95:5 train-test split, the authors have made their results incomparable to baseline models.

2. Inconsistent dataset usage: The authors claim, ""The total dataset was used in the training and evaluation of EGAP, but the other models only used a subset of the dataset. We do not anticipate a significant difference in the performance of the models because the dataset is composed of very similar equilibrium structures, and improvement from increasing dataset size saturates quickly."" This statement is problematic for two reasons:
   a) The baseline models used the remaining data as a test set, not just a subset of the training data. By including more of the dataset in their training data, the authors have reduced the size of the test set.
   b) The authors have effectively used a different train/validation/test set compared to baseline models, rather than just using additional training data.
   c) The claim about performance differences with dataset size is not supported by evidence in the paper.

3. Limited experimental scope: Experiments on the OC20 dataset are conducted only on the 2M split of S2EF, rather than the larger S2EF-All or S2EF-All+MD splits. Given that the main motivation is efficiency, it would be more fitting to test on the larger set. This limitation significantly weakens the paper's claims about scalability and efficiency.

4. Inconclusive performance difference: On the OC20 2M split, EGAP underperforms on the force metric while outperforming on the energy metric. Since there is a regulation between these metrics during training, prioritizing one over the other can result in these outcomes even for the same model. This renders the performance difference unconvincing.

5. Lack of scalability experiments: The authors didn't provide any scalability experiments in terms of model size, which leaves their scalability claims unsupported. Without demonstrating how the model performs as its size increases, it's difficult to evaluate the true scalability of EGAP.

6. Result discrepancies: The results presented don't align with those provided in [1]. For example, in [1], the Tetrasaccharide target for VisNet-LSRM is reported as 0.1055 energy (kcal/mol) and 0.0767 forces (kcal/mol/Å), which equals 4.574914 energy (meV) = 0.05258521839 (meV/atom) and 3.326028 force (meV). However, the authors report these as 0.044 (meV/atom) and 5.0 force (meV). These discrepancies raise concerns about the accuracy of the reported results and the fairness of comparisons.



[1] Li, Y., Wang, Y., Huang, L., Yang, H., Wei, X., Zhang, J., Wang, T., Wang, Z., Shao, B., and Liu, T.Y. (2024). Long-short-range message-passing: A physics-informed framework to capture non-local interaction for scalable molecular dynamics simulation. In The Twelfth International Conference on Learning Representations.

Limitations:
The paper does not adequately address the limitations of their approach, particularly regarding the dataset split inconsistencies and the lack of scalability experiments. These limitations significantly impact the comparability and validity of their results.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
Following an investigation into the scaling of an attention-based NNIP model (EquiformerV2), this paper finds that it is better to increase the number of parameters in the attention mechanism than to increase the number of parameters using higher-order tensors. Based on these results, the paper proposes a model architecture that significantly reduces memory usage and accelerates training while scoring well on OC-20 and MD-22.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The model achieves good results despite restricting itself to fairly limited physical data. This is evidence in support of the paper's claim that scaling alone is sufficient to learn physical symmetries.
- BOO appears a promising method of encoding rotation-invariant bond information.

Weaknesses:
- The paper appears to treat ""efficient"" and ""scalable"" as synonyms throughout, but there would appear to be a meaningful distinction, one that gets to a significant limitation of this paper. A non-scalable model performs badly on large molecules. An inefficient model takes excessive resources to achieve its performance. There is no demonstration that this model improves the scalability of current architectures. Thus, the contribution of this paper appears to be limited to a potential reduction in training times. There is no enhancement to the accuracy or scope of pre-existent NNIP models. If the paper means to claim that its proposed architecture scales better, it should show that as its number of parameters increase, its performance continues to improve where the performance of similar models plateaus. Various implementations of this model with different sizes are not described, so it would appear inaccurate to assert the superior scalability of this model.
- Limited ablation studies in general. It is not clear which change gives the model its improvement in efficiency over EquiformerV2. Is it the use of BOO? Advancements in attention mechanism efficiency borrowed from NLP and CV? Simply that the model does not waste flops on higher-order tensors that do not improve its performance? Something else?
- The model marginally outperforms EquiformerV2 on OC20, but it has marginally more parameters. It is not clear whether there is any advancement in architecture, or whether the model is simply better because it has slightly more space to learn.
- It is not clear whether higher-order tensors are inefficient only as employed in EquiformerV2, or whether they are inefficient in general. Thus, the conclusion that scaling attention is superior to scaling the order of the tensors appears premature. In the current case, the investigation of scaling strategies is no more powerful than an extended ablation study of EquiformerV2.

Limitations:
See above.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Authors explore scaling strategies for ML interatomic potentials, which are alternatives to the increase of spherical order L, which is costly. By leveraging the insights they extract from ablations, and avoiding the inefficiency of equivariance, they build a new invariant model based on features containing angular power spectra. Experiments show that this new model achieves SOTA on MD22.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed model achieves SOTA performance on MD22, and comparable performance to spherical equivariant model such as EquiformerV2, by means of invariant features.
- Experiments and ablations are extensive
- The model stability is probed
- The investigation of alternatives to equivariance is a necessary topic

Weaknesses:
- Some other experiment showcasing the model accuracy of some other benchmark dataset with more models would have been beneficial
- Table 1 is a bit poor, perhaps authors could use a bit of extra space to include some other ways of dealing with 'bond directional features'.
- The authors could have used the extra space they have to perform some other experiment.

Limitations:
The authors say they discuss some limitations in section 3. However, and given that they have spare space, I would expect a small limitations paragraph.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel approach to predict properties of some 3d molecules based on efficient transformer-based graph neural networks. The paper investigates the computational bottlenecks of the previous approaches and addresses them by using simpler features and designing a novel graph attention architecture. The method shows good performance and is efficient, which is shown on two datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem of efficiently predicting complex molecule properties is important in practice.
2. The paper has interesting observations about scaling behaviors.
3. The paper is easy to follow and written and presented well.
4. The results are mostly convincing.

Weaknesses:
1. The self-attention described in L174 is computed over edges which seems unusual in the graph transformer methods that compute self-attention over nodes. The number of edges is much bigger so it's surprising that the model is still efficient. Does the model have to store a square E x E matrix at some point? What's the motivation of computing self-attention over edges? Why not to add edge features as some kind of biases in the self-attention matrix over nodes like in Graphormer?

2. In L206, ""The total dataset was used in the training and evaluation of EGAP, but the other models only used a subset of the dataset."" makes the comparison not fair even if the authors ""do not anticipate a significant difference"". Using a smaller dataset for EGAP or a full dataset for the baselines is expected. On the OC20 dataset, do all the methods use the same train/val/test splits?

3. Why EquiformerV2 is not compared to in Table 2:?

4. Can the authors provide runtime and memory comparison for different molecule sizes during testing? This can give practitioners important insights.

5. It would be important to ablate which components of the proposed method contribute to better runtime and memory. For example, how much FlashAttention helps, how much using a small order L helps, etc. I believe some of these components like FlashAttention can be easily incorporated to the baselines.

6. Other ablations such as EGAP model size and dataset size vs performance would be interesting to see to better understand method's behavior under different regimes.

Other minor issues:

- L62 - broken reference
- fig. 1 must be referenced in the text
- fig. 1 ""Right: Force MAE vs. Datasize. "" is not correct or the plot xlabel is not correct
- In Table 2, for Fatty acid h(r) is missing for EGAP

Limitations:
not discussed explicitly

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
dB99jjwx3h;"REVIEW 
Summary:
This paper considers the task of learning linear causal representation with data collected from general environments. Authors show that there exists a surrounded-node ambiguity (SNA) which is basically unavoidable in their setting. On the other hand,  identification up to SNA is possible under mild conditions in their considered setting. An algorithm, LiNGCReL, is further proposed to achieve such identifiability guarantee.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Learning linear causal representation using soft-interventions seem to be new.

- The surrounded-node ambiguity (SNA) identifiability is new.

Weaknesses:
- The placed assumptions do not seem to be weaker, compared with existing ones.

- Many places/claims are vague/incorrect, and presentation is poor.

Limitations:
See above.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper is about causal representation learning (i.e, learning the latent causal graph and the unmixing function) from high-dimensional observations in the case of linear SCMs where the mixing function is also linear. The paper defines the notion of surrounded node ambiguity (SNA) and then performs studies for CRL under assumptions of soft interventions (where there are K environments that share the causal graph). Theoretical analysis is performed and the authors also introduce a method, LinGCRel that can perform CRL upto SNA and is provably identifiable.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors introduce the idea of SNA and identifiability upto SNA which is novel and also important for understanding ambiguities in the CRL setup, even for the simple case of linear SCMs with a linear mixing function.

2. LinGCRel, a practical method is also proposed to perform CRL in such a setting, provably identifying upto SNA. The paper is also about soft single node interventions, in comparison to other previous work which has primarily dealt with hard interventions.

Weaknesses:
Since I am not aware of many theoretical results and proofs for CRL, which seems to be the main contribution of the paper (apart from LinGCRel), I do not have particular weaknesses to state.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates causal representation learning from low-level observed data across multiple environments. The authors address the surrounded-node ambiguity (SNA) in linear causal models and propose the LiNGCReL algorithm, which achieves identifiability up to SNA without relying on single-node interventions. Experiments on synthetic data show the effectiveness of LiNGCReL in the finite-sample regime.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.  It addresses the limitations of previous methods that rely on single-node interventions, providing a more practical approach to causal representation learning.
2. The proposed LiNGCReL algorithm achieves identifiability up to SNA under mild conditions

Weaknesses:
1. Could you provide an intuitive explanation or motivations for the assumptions? What do they represent in real-world data scenarios?

2. The authors mentioned that this work differs from Xie et al. [54, 55] and Dong et al. [11] because the latter requires structural assumptions. However, it seems that the proposed model in this paper also inherently assumes that there are no direct causal edges between observed variables.

3. In the first step of the proposed algorithm, ""any identification algorithm for linear ICA is used to recover the matrix $M_k$"". This may introduce some errors, as perfect identification cannot be achieved. These methods typically assume that the dimensions of $X$ and $Z$ are the same and other restrictions.

4. In Figure 2(e), the arrows on the edges could be made larger, as they currently look like undirected edges.

5. How would the proposed method perform when applied to real-world data?

6. The authors only provided the results of the LiNGCReL algorithm on simulated data. It would be more objective and validate the effectiveness of the proposed method if results of other baselines or classical methods on the same data were also provided for comparison.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies causal representation learning (CRL) in the linear setting, that is, linear latent SEMs and a linear mixing function. Key contributions include:
* Identifying an intrinsic surrounded-node ambiguity (SNA) that exists when the causal model and mixing function are linear. This ambiguity is unavoidable without hard interventions.
* Proving that identification up to SNA is possible for linear models under reasonable conditions with data from $O(d)$ diverse environments, where $d$ is the number of latent variables.
* Proposing an algorithm called LiNGCReL that provably achieves identification up to SNA in the linear case.
* Showing that $\Omega(d^2)$ single-node soft interventions would be required to achieve the same identification guarantee, highlighting the benefit of diverse environments.
* Demonstrating LiNGCReL's effectiveness on synthetic data in recovering the true causal model up to SNA.

The paper provides some new theoretical insights into the identifiability limits of CRL and an algorithm to achieve those limits in the linear case with general environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* New theoretical insights on identifiability limits for CRL, particularly connecting the SNA concept.
* The authors propose an algorithm which provides a concrete way to achieve the theoretical guarantees.
* Experimental results validate the theoretical findings on synthetic data.

Weaknesses:
* Limited to linear causal models and mixing functions. Nonlinear extensions not discussed.
* All environments share the same causal graph. My understanding on this is that soft interventions considered in this work do not allow for  removal of a subset of the parents.
* Experiments only on synthetic data. Real-world applications or datasets not tested. Perhaps a semi-synthetic experiment following Squires et al. 2023. Specially when a key point is that the assumptions are less stringent than prior work, an experiments like this might help.
* Computational complexity of LiNGCReL not thoroughly analyzed.
* Implications of SNA for downstream tasks not explored.

Limitations:
The natural limitations that come from the assumptions are discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
As91fJvY9E;"REVIEW 
Summary:
This paper introduces an end-to-end learnable clustering framework for intent learning in recommendation systems, termed ELCRec. The current intent recognition methods can be likened to the Expectation-Maximization (EM) algorithm, where the E-step involves clustering to obtain intents, and the M-step uses self-supervised methods to update embeddings. However, these methods suffer from slow clustering speeds and limited scalability, as well as performance issues due to the separation of clustering and optimization processes. ELCRec addresses these issues by integrating user behavior embeddings into a user embedding and introducing a differentiable clustering method (ELCM) that optimizes clustering and intent alignment. Additionally, the framework employs intent-assisted contrastive learning (ICL) and incorporates a next item prediction loss to enhance the recommendation performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The proposed ELCRec framework innovatively combines clustering and intent learning into a single end-to-end differentiable model, resolving the long-standing issue of separate clustering and optimization phases. This integration enhances efficiency and accuracy, representing a significant advancement in the field.
2.	The paper introduces Intent-Coupled Contrastive Learning (ICL), a groundbreaking method that significantly enhances user embeddings by incorporating intent information. This approach addresses the limitations of traditional contrastive learning methods and can substantially improve recommendation systems' performance.
3.	By providing complete experimental code and detailed descriptions of the experimental procedures, the authors ensure that other researchers can easily replicate and validate the results, contributing significantly to the research community.

Weaknesses:
1.	In the online A/B test section, the author mentions that this method can efficiently handle new users, but the paper seems do not provide detailed information on how embeddings or cluster centers are assigned to new users during inference.
2.	The paper employs a combined training approach using next_item loss, ICL loss, and cluster loss. However, it does not clarify the appropriate proportions for these three losses, particularly whether the ratio between next_item loss and ICL loss should be fixed at 0.1.
3.	The paper does not explain why the proposed framework results in increased latency on the sports dataset, as observed in Table 2, raising concerns about the consistency and generalizability of the method's performance across different datasets.
4.	The paper claims that existing intention-based methods typically rely on the Expectation-Maximization (EM) algorithm for stepwise training, which leads to suboptimal performance. However, many contemporary works based on Vector Quantization (VQ)[1][2] or Residual Quantization (RQ)[3] have already adopted end-to-end training approaches. The paper does not provide a comparative analysis or discussion of these VQ/RQ-based methods, which is a significant oversight given their relevance and effectiveness in the field.


[1] Hou, Yupeng, et al. ""Learning vector-quantized item representation for transferable sequential recommenders."" Proceedings of the ACM Web Conference 2023. 2023.
[2] Jiang, Gangwei, et al. ""xLightFM: Extremely memory-efficient factorization machine."" Proceedings of the 44th International ACM SIGIR Conference on Research and Development in Information Retrieval. 2021.
[3] Rajput, Shashank, et al. ""Recommender systems with generative retrieval."" Advances in Neural Information Processing Systems 36 (2024).

Limitations:
\

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to improve the optimization paradigm of the existing intent learning methods for recommendation. A novel intent learning method named ELCRec is proposed by unifying behavior representation learning into the end-to-end learnable clustering framework. Experiments, theoretical analyses, and application shown the superiority of ELCRec.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The research topic is practical and meaningful. Intent learning plays an important role in user understanding and recommendation. 

- The motivation of improving the alternating optimization is clear and the propose method ingeniously solved this problem by the end-to-end learnable clustering framework.

- The paper is very comprehensive, including experiments, applications, and theoretical analyses. The improvement is significant.

- The code is available, which guarantee the reproducibility.

Weaknesses:
- The author should provide more intuitions and insights of designing ELCRec before introducing this method, including but not limited to the challenge discussion, naive proposal, further improvement, and deep-in ideas. 

- The authors should provide the precise data and show them in the Figure 1 for the better understanding of the effectiveness of the proposed modules.

- The related work part is too short. The authors should make a more comprehensive survey and discuss on Recommendation in 2024. More details about the related papers are required. 
1. The authors assert the efficiency of the proposed method. However, details regarding the devices used and the improvements observed in the A/B testing have not been provided.

2. What’s the update rate of user group embedding? After the clustering, will the results be stored to the database?

3. How to determine the cluster number in the practical scenario? The cluster number seems fixed in this method but it is not reasonable for the large-scale users in practical app.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, the authros study the complex optimization  issue in the filed of recommendation.  It encodes users' behavior sequences and successfully unifies behavior representation learning into  a learnable clustering framework. Further, it uses cluster centers as self-supervision signals to highlight mutual promotion. Experimental results dmonstrate the effectivenss of the proposed method.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. The organization is easy to follow, and related work is fairly comprehensive. 

2. Motivation is strong, recommendation is an intersting and practical topic, and this study will facilitate this  field. 

3. The unification of behavior representation learning and clustering optimization is novel, and it is conducive to enhancing the optimization paradigm.

Weaknesses:
1. In section 5.2, the number of large-scale datasets is  relatively small. The authros are encouraged to add more large-scale datasets so as to further highlight its wide applicability.  

2. It would be better to add the summary about the devised loss Eq.(8).  

3. Some suboptimal experimental results should be discussed in more detailed, such as Fig. 1 (c).

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
tDMTwto6jv;"REVIEW 
Summary:
This paper explores the setting where a learner has a label budget that can be used to acquire labeled data from a user. However, differently from traditional active learning approaches, the paper assumes that the user may not want/be able to label some examples. That is, for specific (limited) examples, the user returns a label when queried, while for other examples it does not return anything. The key challenge of the paper is how to motify traditional query strategies to account for this new possibility of not getting any label from the user. The paper solves this challenge by modeling the user through a Bayesian network and including two additional terms in the AL score function: the first term scales the potential gain obtained by labeling an instance by the probability that the instance would get labeled when queried, while the second represents the gain obtained for modeling the user (i.e., where the user labels the data).

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
### Main Strengths

1. The problem setting is realistic, unexplored, and yet relevant for industry use cases: after all, some experts may not want, may not be allowed, or even may not be able to provide labels.

2. The paper is clearly written: claims are well supported, the notation is well detailed, and the flow is easy as well as the structure.

3. The proposed approach is reasonable and meaningful: the paper proposes multiple methods (such that one is the generalization of the other) and shows empirically how they perform. The Bayesian Active Learning with Disagreement approach is still quite used - despite being more than 10 years old. Thus, extending BALD to the new setting can have a positive impact on several industrial use cases.

4. The paper is technically sound and highlights whenever a quantity is an estimate and when is the true value.

Weaknesses:
### Main Weaknesses:

1. The experiments include MNIST, which is a little bit outdated and nowadays considered a simple dataset. I'd recommend using more elaborated multiclass datasets (e.g., SVHN, Waterbirds, ...). Using more common datasets would strengthen the work.

2. Some literature study might be missing: learning to defer is an area where a model defers the prediction to a human whenever the human is more likely to provide a correct prediction, whereas the model is preferred in other cases. Thus, this research direction also investigates how to model a human. Also, learning to reject is a similar area, and [1] uses active learning assuming that the user may not provide some labels. Finally, there is a connection with PU Learning (Positive and Unlabeled Learning), where the propensity score named e(x) represents the probability of labeling an instance. Including a brief connection to these areas would improve the quality of the paper.

[1]: https://www.ijcai.org/proceedings/2020/0403.pdf

Limitations:
Limitations are clearly discussed in Section 6.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a direct extension of BALD sampling criterion to address the need for active annotation rejection. The method assumes a rejection distribution over the candidate data samples in pool-based active learning and proposes an active sampling strategy that jointly considers the BALD informativeness and rejection cost.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The problem studied in this paper can be considered an instance of cost-aware active learning. The challenge of having samples that are difficult or even impossible to label is a very practical issue in active learning. This paper provides a valuable contribution to the field of active learning by explicitly estimating the rejection function, which offers better guidance for active learning and is beneficial for improving data efficiency.

2.The rejection component proposed in this paper is naturally integrated into Bayesian sampling. The final hybrid sampling criterion revolves closely around information gain and uncertainty reduction, which I find to be reasonable and effective. Approaching active sampling from a Bayesian perspective also makes it more interpretable.

3.The paper is written in a concise and fluent manner, with high readability. The core method is introduced clearly and is easy to understand.

Weaknesses:
1.The core of this paper's design is the human discretion function e(x), but there are some unclear aspects regarding this function in my view. First, the approximation of e(x) seems to be achieved through an additional Bayesian network, but the detailed features of this Bayesian network are omitted in the paper. As the authors mention, given the small sample nature of active learning, the estimation of e(x) is likely to have an underfitting problem. However, the two solutions proposed by the authors in line 171 do not seem to address this issue adequately, and the authors should provide more explanation on this. Additionally, since e(x) is estimated by a Bayesian network, it should be involved in the computation in the form of some posterior distribution. However, in the examples given in the experimental results section, the true e(x) is presented as an unnormalized piecewise function. Therefore, using a posterior to approximate an e(x) that is not a valid density function raises questions about its effectiveness and limitations. I believe this is directly related to the deeper reason for label rejection, i.e., the analytical form of e(x). If the authors cannot theoretically prove that the proposed method generally works for all types of rejection reasons, they should at least compare the model's prediction of rejection (instead of the final overall sampling score) with several typical true rejection distributions in the experiments.
2.As a cost-aware active learning method, I find the experimental section of this paper somewhat weak. The real-world dataset used does not record the true rejection behavior of human annotators, which means, to some extent, that the experiments on MNIST are still synthetic. The true reasons for human annotators rejecting to label might be much more complicated than the prototype e(x) used in this paper. Therefore, I believe the evaluation of this specific problem requires a dataset with actual recorded human annotator rejections.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces the Active Learning with Instance Rejection (ALIR) problem, addressing the issue of selective labeling where human discretion impacts the labeling process. In particular, humans might not always provide a label for a point returned by active learning; this abstention needs to be modeled explicitly. The authors propose new algorithms under the SEL-BALD framework to improve model performance using a human discretion model. The paper demonstrates the effectiveness of these algorithms through experiments on both synthetic and real-world datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper's focus on Active Learning with Instance Rejection (ALIR) focusses on a real-world problem where human discretion plays a critical role in the labeling process. This assumption seems to be realistic and well-motivated.
- The introduction of the SEL-BALD (Selective Bayesian Active Learning by Disagreement) framework effectively balances the dual objectives of selecting informative instances and accommodating human labeling behavior.
- The empirical results show the benefits of considering human discretion in active learning. This underscores the practical advantages of the proposed method(s).
- The paper is well-structured and clearly written.

Weaknesses:
- The integration of Bayesian neural networks for modeling both machine learning tasks and human judgment significantly increases computational complexity. These Bayesian models are computationally demanding to train.

- While the paper includes experiments with both synthetic and real-world datasets, the scope of real-world applications examined is relatively limited. Critical domains such as medical diagnostics or financial fraud detection present unique challenges. A more comprehensive evaluation across diverse applications would better illustrate the versatility and reliability of the approach.

- The paper's assumption that human discretion can be accurately modeled using predictive tools like Bayesian neural networks may be overly optimistic. Real-world human decision-making is influenced by numerous complex and often unpredictable factors. The efficacy of these models can vary significantly across different contexts. Accurate modeling of human discretion typically requires extensive, high-quality historical data on human decisions, which may not always be available. In cases where data is limited, noisy, or biased, the human discretion model's performance could be significantly compromised. Furthermore, human decision-making processes can change over time due to regulatory updates, new information, or shifts in organizational policies. The study does not address how the human discretion model would adapt to such changes, potentially leading to outdated or inaccurate predictions.

- The legibility of figure legends is consistently poor due to their small size. This aspect requires significant improvement for better readability and comprehension.

- Definitions 4.1 through 4.5 would benefit from more detailed explanations accompanying the equations, providing context and clarification for the mathematical formulations presented

Limitations:
See weaknesses above.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper considers an active learning scenario where the human annotation is done under the restriction that the annotator is biased in deciding the labeling. The Bayesian framework makes the e-BALD and various versions based on a posterior sample of labeling probabilities. The authors consider the three cases in human labeling probability, including the non-labeling of specific cases.  Three versions of the mean, quantile (UCB), and samples reveal different aspects in experiments. The trade-off between exploration and exploitation in labeling probability is observed. Using the quantile can have a detailed balance between examination and labeling (examination means checking the attribute and non-labeling).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The problem and task are novel. The BADL modified by the labeling probability is simple, clear, and well-defined. In experiments, the proposed algorithm performs better in the labeling rate for the candidates by the query.

Weaknesses:
There can be many issues. 
The first one is the applicability of various metrics used in active learning.  Usually, entropy, margin, and variation ratio are traditional measures. Especially, the BALD combined with deep neural networks can show poor performance. It requires the study of applicability to other metrics or advanced active learning algorithms such as BADGE.  
The second one concerns the datasets used in experiments. Three datasets are not sufficient; more datasets, such as Fashion MNIT or ImageNet-tiny, can be considered, and their statistical significance can be studied more thoroughly. 
The last issue is human behavior, the scenario is relatively simple addressed by the simple logistic regression. The behavior of human annotators can be diverse because of their complex properties. It requires human models, which can be complex. When the human annotator's behaviors are complex, what’s the performance? Also, models such as logistic regression can be mimicked to generate samples.       

Minor: 
bayesian neural  -> Bayesian neural
e-bald -> e-BALD

Limitations:
Sufficiently discussed in the last section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
fEYHZzN7kX;"REVIEW 
Summary:
This paper introduces Attraos, a new model for long-term time series forecasting (LTSF) that incorporates chaos theory and views time series data as observations from high-dimensional chaotic dynamic systems. Attraos utilizes attractor invariance, non-parametric Phase Space Reconstruction, and a multi-scale dynamic memory unit to effectively capture historical dynamics and forecast future states with substantially fewer parameters than existing models such as PatchTST. Empirical evidence demonstrates Attraos' superior performance across various LTSF and chaotic datasets, providing a new perspective on the underlying dynamics of time series data.

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
-	The paper presents comprehensive experiment results and provide rigorous theoretical results to support their claims.
-	Leveraging results from chaotic theories for LTSF tasks is a novel and interesting idea.
-	The method proposed in the paper needs much fewer parameters than previous methods.

Weaknesses:
-	Most of the descriptions for problem setup (section 2), methods, and theoretical analysis (section 3) are very confusing and hard to follow. There are always notations suddenly coming out without much information. I suspect there are also several typos that cause difficulties in understanding and assessments. See **Questions** for details.
-	If I understood correctly, the title is a little misleading since seemingly the key insight/perspective comes from the embedding theorem by Takens, a counterpart of Whitney theorem for attractors. Are there any other insights related to chaotic systems? If not, I would suggest concretizing this point in the title, abstract, introduction, and conclusion.
-	Section A.1 in appendix is entirely copied from Wikipedia, https://en.wikipedia.org/wiki/Takens%27s_theorem , except for some changes on notations. However the notations in this section are not coherent, e.g. $k$ and $N$ in L414.
-	Similar to the first point, the paper does not provide enough background introduction.

If the authors worry containing all the necessary details will exceed the page limit, I would suggest presenting a detailed version of problem setup, backgrounds, analysis and methods in appendix.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces chaos theory into a long-term time series forcasting (LTSF) model called Attraos (a play on the words ""attractor"" and ""chaos""). They propose a Multi-resolution Dynamic Memory Unit (MDMU) which is inspired by (and looks a lot like) the State Space Models (SSMs) used in the Mamba family of models. However, unlike SSMs, Attraos assumes additional structure on top of time-series signals by utilizing attractor behavior from chaos theory in the phase space. Attraos outperforms many existing architectures (e.g., Mamba, RWKV-TS, PatchTST) on time-series tasks at a fraction of the computational complexity and cost.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- S1: **Outperforms existing architectures at a fraction of the cost**. Attraos is able to outperform models like Mamba, RWKV, and Transformers on many different time series tasks at a fraction of the computational cost (in terms of training time and parameter count).
- S2: **Theoretically complete**. The paper provide rigorous definitions, theorems, and proofs throughout the main paper and appendix to justify their method. (Note: before this paper I was quite unfamiliar with chaos theory, though I have some experience in SSMs and time-series. I am unable to verify much of the math.)
- S3: **Comprehensive empirical results**. The paper compares Attraos to many alternative architectures for time-series data (e.g., RWKV, Mamba, Transformers, MLPs, etc.) and shows that on many tasks Attraos outperforms alternative methods at a fraction of the computational cost. Additionally, the paper includes an ablation study for the different components of Attraos.

Weaknesses:
- W1: **Presentation of results could be clearer**. I was often confused by the choice of coloring in the tables. See the following for my suggestions:
    1. [Table 1,2,4]. The ""Red/blue"" colorscheme is unfortunate, because most brains (including mine) associate the color ""red"" with ""poor performance"". I suggest using **bold** for best, <u>underline</u> for second best, as is often done in AI papers. 
    2. [Table 3,5]. For ""improvement/decline"" in performance, I suggest one of two options: (1) show ""better performance"" with blue, and ""worse performance"" with bold and red to emphasize decreased performance from an ablation, or (2) showing only the ""deltas"" with a ""stock ticker arrow"" (🔺 or 🔻, colored green for improvement, red for decline).
- W2: **Complete architectural description is missing**. Most of the paper focuses on describing how to implement the special components of Attraos, but the complete picture of the architecture is missing. Additionally, there is no section describing the hyperparameters for the training setup for each experiment. See Q1 for specific questions.
- W3: **Missing error bars**. Table 1 represents average results of long term forecasting across a swath of model classes, but these averages are not accompanied by reports of standard deviation. Thus, it is difficult to tell whether improved average performance is actually signficant. Improve by including error bars for all numerical results, which may require re-running some experiments under different random seeds.
- W4: **Grandiose, non-academic language**. The paper starts with ""In the intricate dance of time"" [L15] 😂 and makes statements like ""we can transcend the limitations of deterministic dynamical systems"" [L34]. I suggest reworking these lines to preserve the professionalism of the rest of the work.

These weakenesses are admittedly small and easily fixable by the authors during the review process. My overall attitude towards the paper is that it is of high quality and should be accepted. However, a lot of the theory was beyond my ability to evaluate, and I am willing to adjust my score positively/negatively as I become more familiar with this work throughout the review process.

Limitations:
Limitations are adequately described in the conclusion.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel approach, named Attraos, for Long-term Time Series Forecasting (LTSF) based on treating the observed time series as high dimensional chaotic dynamical system. The model first estimates an embedding of the data through Phase Space Reconstruction (Takens embedding) and then utilizes a memory unit through state space model that represents the dynamical system through polynomials which allows to evolve the dynamics into future steps and forecast the data. The model is evaluated on multiple LTSF datasets and additional experiments such as ablations and robustness under noise are performed.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
S1. The approach is proposing to forecast data evolution through learning the dynamical system that can generate such time series vs. to predict directly from data.

S2. Novel approach for polynomial estimation through a state space model is proposed.
 
S3. Fundamental properties of the approach are rigorously shown.

S4. Incorporation of Belloch algorithm for speed up is used.

S5. Experiments show that the approach performs better in many cases than existing approaches and ablation experiments are performed.

Weaknesses:
W1. There seems to be a detachment between the theorems and properties proved and the proposed system. There seems to be no discussion about how these properties lead to the particular setup in the paper. For example why this particular SSM was used? How the hyperparameters were chosen in light of the propositions? What are the cases that the model is limited and not expected to be effective? Also interpretation of the results is lacking.

W2. For non toy experimental results exposition and interpretation of the resulting dynamical system is missing. What is the dynamical system/s that are being obtained for the benchmark and what is the distribution of polynomials?

Limitations:
See W1-W2 and Q1-Q3. More extended discussion of limitations would contribute to better evaluation of the work vs generalization.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
gL5nT4y8fn;"REVIEW 
Summary:
# Summary

The paper presents Panacea, an innovative method for aligning LLMs with human preferences, by reconceptualizing the alignment task as a Multi-Dimensional Preference Optimization (MDPO) challenge to recover the entire Pareto front and adapt online.

# Contribution

Panacea marks a good advancement in LLM alignment by providing a scalable, efficient, and theoretically sound method for aligning models with diverse human preferences.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. Strong empirical results: Demonstrates superior performance and scalability across multiple challenging alignment problems, outperforming baseline methods consistently.·
2. Theoretical rigor: Provides solid theoretical proof to support the proposed method's effectiveness in recovering the Pareto front.
3. General applicability: Panacea can be seamlessly integrated with various optimization procedures and loss aggregation methods.

Weaknesses:
1. Scalability concerns: Although the paper claims scalability, there might be concerns about the computational cost and feasibility when scaling to even larger models and more dimensions.
2. Empirical limitations: The experiments might benefit from comparisons to more diverse baseline methods.

Limitations:
High Computational Demand: Despite claims of scalability, the computational cost of training and maintaining a single model that can adapt to a vast array of preferences may be substantial, particularly for very large models or when increasing the number of preference dimensions.
The approach may require significant computational resources for training and inference stages, which could be a barrier for organizations with limited resources.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the Multi-Dimensional Preference Optimization (MDPO) problem, which involves aligning multiple objectives that exhibit heterogeneity in preference in the population, such as the tradeoff between harmlessness and helpfulness. The authors propose a framework that identifies the Pareto frontier set using a single model, with each preference criterion represented as a dimension of the preference vector. Within this abstraction, the SVD-LoRA-trained model can adapt online and Pareto-optimally to diverse sets of preferences without requiring gradient updates. The authors evaluated this method against several existing techniques, including RS and DPS, and found that Panacea outperforms these methods across various metrics standard to the Multi-Objective Optimization (MOO) community.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed framework trains a single model (effectively one set of weights) to align with exponentially many preferences. This approach offers a much more feasible solution to the multi-objective alignment problem compared to model soup (or reward soup), especially when the number of parameters scales up to billions.

2. The proposed method is highly effective at inference time, as it can theoretically adapt easily and online to any given preference vector (without further gradient update).

Weaknesses:
1. The paper claims that the idea is theoretically sound. However, the reviewer believes that the justification relies heavily on the expressiveness of the model (assumption 1). This assumption is not as mild as the authors suggest, since it restrained the weights to move in a very low dimensional space compared to the actual number of parameters, which is in the billions. Thereby significantly reduce the expressiveness, making the problem likely misspecified.

2. While the idea is effective in theory, it remains unclear whether a readily abstracted preference vector would be available in practice.

3. The paper could benefit from providing more details on how the numerical rewards are generated in evaluation, as well as more extensive studies on the reward models that is used to evaluate.

Limitations:
The authors have addressed the limitations in appendix.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces Panacea for LLM alignment that reframes it as a multi-dimensional preference optimization problem. Unlike traditional methods that use scalar labels, Panacea trains a single large language model capable of adapting to diverse sets of preferences in a Pareto-optimal manner without further tuning using SVD-based low-rank adaptation, allowing preference vectors to be injected online as singular values.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper conducts thorough evaluation of the Pareto fronts obtained using metrics such as hypervolume, spacing, sparsity, and visualization for lower dimensional tasks. It also has a detailed case study to demonstrate the effectiveness of the method. 
- The method offers good scalability and scales up to ten dimensions. 
- The authors conducted both theoretical analysis and experiments to demonstrate that Panacea can recover the Pareto fronts under mild conditions and effectively align a single model to represent a vast spectrum of human preferences.

Weaknesses:
- The presentation of figure 1 is a bit confusing and hard to understand. It can be improved. Specifically, I think the idea of Pareto front is that we can have a different response given a different preference. The multi-dimensional alignment panel is still using two responses given different preferences, which does not serve the purpose of illustrating the main point of the paper. 
- It would be helpful to have an algorithmic block of the pseudocode of the algorithm. 
- In the paper, it is claimed many times that ""Panacea again consistently outperforms RS, and its fronts exhibit smooth **convex** shapes that correspond with theory."" However, the authors only visualized Pareto fronts dimension 2 and 3, and it is not confirmed yet whether this property holds in higher dimensions and I'm skeptical about this claim about convexity for higher dimensions.

Limitations:
The authors discussed the limitations as follows
- Curse of dimensionality as dimension scales up to more than 10
- The method requires a preference vector from the user, but the user might not know a specific preference vector until they try a bunch of evaluations
- No ground truth Pareto front available for evaluation

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
hUGD1aNMrp;"REVIEW 
Summary:
This paper aims to provide a unified framework for deriving lower bounds of interactive decision-making problems using an extended technique adapted from Chen et al., 2016 [11], which was established only for non-interactive estimation problems. The paper offers a quite general formulation of the minimax value of interactive decision-making that encompasses both previously known and novel formulations. In particular, the paper demonstrates that such a lower bounding technique can be used to rederive the lower bounds from Foster et al., 2023 [25] and provide tighter lower and upper bounds via a new complexity named ""decision coverage.""

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
I find the general formulation and the technique based on percentiles (instead of the expected value used in prior literature such as [25]) for deriving lower bounds to be quite appealing. Although it appears to me that the lower bounding techniques are essentially ""abstracted out"" from prior literature, such as [11] and [23, 25], the paper presents it in a quite clean manner. The paper also provides several new results, such as interactive parameter estimation and tighter bounds for interactive reward maximization. I generally enjoyed reading this paper and find it worthy of publishing at NeurIPS. I expect the paper to have a broad impact on the community.

Weaknesses:
My main complaint about this paper is that it contains many hand-waving arguments. I had to fill in many gaps in the proofs myself to verify and understand the results.

I outline some specific comments below (some are minor, while others need additional justifications):

1. I'm not sure if it is appropriate to call Corollary 2 a ""Generalized Fano’s inequality"", as it is not clear to me that it implies the original Fano's inequality (though they are ""qualitatively"" equivalent, i.e., both require $I(M,X) \ll \log |\mathcal{M}|$ to arrive at a constant lower bound).
2. You claim in line 186 ""Fano’s inequality, e.g., in the form of Corollary 2, cannot be used to prove Lemma 3."" Is there an argument why this is true?
3. Can you explain how Theorem 5 is instantiated from Theorem 1? It appears to me that its proof is completely self-contained without invoking any parts of Theorem 1.
4. Can you comment on how prior technique for deriving Corollary 9 differs from your Fano's inequality-based argument?
5. Can you explain how $T^*(\mathcal{M},\Delta)$ characterizes the regret? Specifically, it does not appear to me that there is a simple way to express the regret in terms of $T^*(\mathcal{M},\Delta)$. Moreover, why does Theorem 4 imply the statement in line 282?
6. The proof of Theorem 10 is too sketchy. It is not clear to me that (12) shares the structure as Proposition 8. Are you applying the minimax theorem here?
7. Theorem 10 looks quite trivial to me, as it does not depend on any information-theoretical structure of the model class (except Assumption 2).
8. Given point 5, I'm not sure how to interpret your Theorem 12 and how to compare it with [25]. It appears you are claiming this as a main result; can you provide any specific examples that demonstrate the improvement (i.e., explicitly computing the ""decision coverage"" for some classes)?
9. You claim that ""Theorem 12 provides (polynomially) matching lower and upper bounds for learning M."" Isn't $\max\\{a,b\\}$ can be arbitrarily smaller than $a \cdot b$? How should the ""matching"" be interpreted?
10. It appears to me the upper bound of Theorem 11 essentially converts the dependency on the size of the model class to that of the decision space. Given the similarity of the proof with that in [24], can you comment on if the results from [24] recover your upper bound (with possible replacement of decision coverage with $\log|\Pi|$)?
11. Typos (there are many, but I only include what I still remember):
    - $D_f(a,b)$ was never properly defined, though one can guess it is for $D_f(\text{Bern}(a),\text{Bern}(b))$.
    - What is the $\alpha$ in line 873?

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper develops the notion of Interactive Statistical Decision Making (ISDM), and a generic lower bound (Theorem 1) which can be instantiated to capture the standard Le Cam, Assaoud, Fano methods as well as recent lower bound results in interactive decision making. The authors further use Theorem 1 to derive new sample complexity bounds (Theorem 12, 13) on interactive decision making contexts (under some regularity conditions on the model class).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I think it's interesting to try to unify different lower bound approaches in order to gain new insights, so the premise of the paper is very intriguing to me. The submission further uses the general theorem to derive new bounds, based on the new notion of Decision Coverage, that tightens existing results in interaction decision making.

Weaknesses:
It may be due to my lack of expertise in interaction decision making literature, but I'm having trouble understanding and evaluating the new contributions, with intuition missing that I hoped the paper would give. Some of the contributions also seem a bit overclaimed. I hope the following constructive criticisms would help the authors improve the paper.

- ""Addressing remaining gap"" and ""complete characterization"": the authors say that the new results (Theorems 12/13) ""completely characterize"" the sample complexity of interactive decision making for convex model classes, but the left and right hand sides differ quadratically (and ignores log factors). Am I misinterpreting the results? They aren't even tight up to constants.

- Line 56: I don't quite understand why ""unifying two-point vs mixture-vs-mixture methods"" is a new contribution. Mixture-vs-mixture is clearly a generalization of singleton two-point methods, so why is the unification sold as a new contribution?

- Generally I find the bound in Theorem 1 challenging to interpret, as opposed to two-point or mixture-vs-mixture methods that make intuitive sense. I'm struggling to understand the insight gained by the Theorem 1 formulation through unifying two-point methods and Fano with interaction decision making. To me, ISDM reads like a very generic minimax game formulation, and then Theorem 1 tags on the reference distribution/ghost data $\mathbb{Q}$ in order to encompass existing techniques for interaction decision making. Lemma 3 then removes this extra component by declaring $\mathbb{Q}$ simply as the transcript of the algorithm, so what have we learned about standard statistical estimation through Theorem 1? In other words, my question is, why is Le Cam/Assaoud/Fano even part of the paper, instead of focusing only on the new bounds in interactive decision making? What am I missing?

- Theorem 12 is a bit too informal, hiding the log factors (especially without specifying log factors in what). It also wasn't actually proven -- I think the calculations using Assumptions 2 and 3 (and applied to Theorem 11) should be explicitly shown in the appendix.

- Relatedly, again on the topic of needing more interpretation, I wish the submission explained how DC is better than the $\log |\mathcal{M}|$ factor in Line 282.

Misc typos I spotted and other small comments:

- First page, should define Perf as cost, so that minimization is the correct direction.
- End of Line 123, should the asterisk in $M^\ast$ be removed?
- Line 177, $L:\Theta \times \mathcal{A} \rightarrow \mathbb{R}_+$ right? Instead of domain being $\Theta \times \Theta$? Just a consistency issue with the rest of the lemma.
- Line 594, I presume ""ISDM"" instead of ""ASDM""?

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a unified framework for lower-bound methods in statistical estimation and interactive decision-making. The authors integrate classical lower bound techniques (Fano's inequality, Le Cam's method, Assouad's lemma) with recent minimax lower bounds for interactive decision-making (Decision-Estimation Coefficient). The framework is based on a general algorithmic lower bound method and introduces a novel complexity measure, decision coverage. The paper also has a lot of other results, including the unification of classical and interactive methods, the generalization of Fano's separation condition, and the derivation of new lower bounds for interactive decision-making.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
* The framework developed in the paper is valid for a very general class of problems and unifies classical and classical lower bound techniques, Fano's inequality, Le Cam's method, and Assouad's lemma, and provides a comprehensive framework for statistical estimation and decision making.
* Lots of other contributions: incorporating the previous work on DEC into the framework of this paper introduces a new complexity measure called decision coverage, which quantifies the  complements of the previous DEC lower bounds.
* Except for a few typos, the paper is well-written.
* Overall, it is a solid paper, and contributions are significant.

Weaknesses:
* I dont find any major technical weakness in this work. The work is good and a bit hard to follow for someone who is a non-expert in this area. Most of the points I highlight below are bunch of typos that I found and clarification that I think its good to include. 
* Line 123, I think it should be $L(M,X)$ instead of $L(M^*,X)$
* The proof of corollary 2 is skipped. It involves 3-4 steps, and therefore I don't think it is trivial, especially for those who are not experts in dealing with these inequalities.

* There seem to be a lot of (minor) typos in Lemma 3, and it's proof.
1. Firstly, it's better to clarify whether sets $\theta_0, \theta_1$ are required to be distinct or not in the Lemma statement.
2. ASDM is not defined in the first line of proof.
3. I think the second equality in Line 602 will contain a factor of $1/2$. Can authors also clarify if the next step follows by data processing? 
4. In line 603, $ d_{3/4}(\cdot,\cdot)$ is not defined. May be authors mean $ d_{f, 3/4}(\cdot,\cdot)$. Please also explain why that inequality follows, is it by choice of $\Delta$ and so that Theorem 1 can be applied?  
5. There seems to be a typo in the subscripts of expectation in line 604, third inequality.

* I guess in Line 218 Eq. 9, $p_{out}$ is not defined.
* I don't think $\pi_{out}$  in Line 226 is defined before (in the statement of theorem 5).
* I would appreciate if the authors mention where the realizability assumption (Assumption 1) is needed and the issues that arise in the agnostic setting, i.e., when realizability does not hold. Also, please provide examples (or references) of well-posed model classes in Assumption 2.
* It is not clear to me why there exists $M$ in the model class, i.e., why is the supremum achieved here by some $M$ in proof of Theorem 5.
* Line 329, I don't think $\mathbb{V}$ is defined.

Limitations:
Essentially no broader societal impacts.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work provides a unified perspective of existing techniques for deriving lower bounds. This viewpoint covers techniques that are useful for traditional statistical estimation (e.g., Fano's inequality, Le Cam's method, and Assouad's approach) as well as the recently proposed approach using decision-estimation coefficients that concerns interactive decision-making. In addition, this work proposes a novel complexity measure called decision coverage. Using this novel measure, this work derives lower and a polynomially matching upper bound for learning convex model classes.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
S1. The paper discusses a unified perspective of techniques to derive lower bounds. It integrates classical techniques (Fano’s inequality, Le Cam’s method, and Assouad’s lemma) with contemporary methods for interactive decision-making, based on the Decision-Estimation Coefficient (DEC). 

S2. This work introduces a novel complexity measure called decision coverage. This measure facilitates the derivation of new lower bounds specifically tailored for interactive decision-making.

Weaknesses:
See questions

Limitations:
The work does not explicitly discuss the limitations of their work; at least, I could not find it anywhere.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
seYXqfGT0q;"REVIEW 
Summary:
This paper proposes a prototypical deep hashing framework to address the fine-grained on-the-fly category discovery problem. The proposed method includes two main loss functions: first, distance minimization between the encoded hash features to the category-representative hash coding after projection $\mathcal{H}_h$; second, enforcing minimal distance separation among hash encoding of different categories, which are enforced to be quantized to -1 and +1 for each dimension. The experimental results demonstrate the effectiveness of each loss function. The performance gains are considerable on all benchmarks compared with previous state-of-the-art baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The performance gains of the proposed approach are considerable.
- The proposed method is robust to the varying coding length.
- The identified ‘sensitivity issue’ problem exists for on-the-fly category discovery.

Weaknesses:
- Writing: It is informal to leave all the related works in the appendix, which will confuse the readers on the contribution of this work. Besides, some expressions are not
- Novelty: The technical contribution is limited in that the prototypical learning [3] is a mature practice in category discovery and the author seems to adapt the deep hashing method in [4] to this problem.
- Motivation: The motivation of achieving the balance between instance discrimination and class discrimination, especially with prototypical learning in the category discovery field is not new [2,3]. However, this is accepted to some extent since on-the-fly category discovery is a new problem.

Limitations:
As mentioned by the author, on-the-fly category discovery is a novel and challenging problem, therefore existing methods do not achieve satisfying performance on real-world tasks.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel framework called Prototypical Hash Encoding (PHE) for On-the-fly Category Discovery (OCD), which aims to discover both known and unknown categories from streaming data using labeled data of known categories. PHE first learns many prototypes for each category and then maps the learned prototypes to hash codes to distinguish samples from known or novel categories with a threshold.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written and easy to follow.
2. The proposed method addresses the limitations of previous hash-based OCD models by reducing sensitivity and preserving discriminative information through prototypes.
3. The proposed method achieves significant improvements in accuracy across various datasets.

Weaknesses:
1. The proposed method is an improvement from the previous work [1], sharing the same core idea of utilizing hash codes for OCD, which limits the novelty of the paper.
2. Despite the improved performance, the reasons why prototypes can address the sensitivity issue are not analyzed in depth.
3. Compared with feature-level prototypes, the advantages of hash code-based category prototypes have not been demonstrated or experimentally verified.

[1] Ruoyi Du, Dongliang Chang, Kongming Liang, Timothy Hospedales, Yi-Zhe Song, and Zhanyu Ma. On-the-fly category discovery. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pages 11691–11700, 2023.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the On-the-fly Category Discovery (OCD) task, which involves utilizing existing category knowledge to recognize both known and unknown categories in new data streams in real-time. To tackle the high sensitivity and suboptimal feature representation issues of existing methods when dealing with fine-grained categories, the paper proposes an innovative Prototypical Hash Encoding (PHE) framework. This framework uses a Category-aware Prototype Generation (CPG) module to represent each fine-grained category with multiple prototypes and employs a probabilistic masking strategy to encourage the model to fully capture intra-class diversity. The Discriminative Category Encoding (DCE) module maps the generated category prototypes to low-dimensional hash centers, optimizing image hash features to ensure intra-class compactness and inter-class separation. Additionally, a center separation loss function is designed to maintain a minimum Hamming distance between different category hash centers. Experimental results on multiple datasets confirm the superiority of this method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed PHE method demonstrates its superiority over existing state-of-the-art methods across multiple datasets. It effectively addresses the issues of large intra-class variance and small inter-class variance, while minimizing the information loss associated with dimensionality reduction. The paper employs visualization techniques to analyze the underlying mechanism by which PHE groups samples into known or unknown categories. Moreover, due to its optimization based on hash centers and inference process based on Hamming balls, the PHE method shows stable performance across different hash code lengths, effectively mitigating the ""high sensitivity"" problem. In contrast to the SMILE method, which exhibits significant accuracy degradation and instability with increasing hash code lengths, the PHE method maintains remarkable stability and consistency.

Weaknesses:
Although the proposed Prototype Hashing Encoding (PHE) framework outperforms existing methods in terms of performance, further research is needed to improve its accuracy in recognizing unknown categories. Additionally, due to the need to compute multiple hash centers and perform complex distance calculations, the computational cost of PHE is relatively high, especially on large-scale datasets.

Limitations:
As mentioned in the appendix of the manuscript, although the proposed PHE framework outperforms existing methods in terms of performance, further research is needed to improve its accuracy in recognizing unknown categories. Additionally, the PHE method involves multiple hyperparameters that require careful tuning to achieve optimal performance, increasing the complexity of model debugging and optimization. Moreover, the weights of the component loss functions in the total loss function need to be appropriately set to balance the optimization objectives of different components, which may require extensive experiments to find the suitable weight combination.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper focuses on On-the-fly Category Discovery (OCD), which is to determine novel categories during inference. OCD methods first compute the hash code of the image, this code then becomes a ""cluster index"", if not matching with existing code, then it is a novel category.
However, the problem with the previous OCD method (SMILE) is that it becomes highly sensitive when the code length increases (difficult to match a new category due to the high possible combinations of hash code). 
Instead of contrastive learning in SMILE, this paper uses cross-entropy loss adapted with a minimal hash distance scheme as regularization to improve performance. Further, the paper also adds cross entropy loss in the feature space to avoid information loss when compressing features into hash code, the feature representation used is a prototype-based model (e.g., ProtoPFormer). 
Overall, it enhances SMILE by using better loss functions and different representations.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
originality: the idea itself is clever and simple, propose/reuse some modules to mitigate the problem in SMILE (to reduce the sensitivity problem). the hamming ball-based inference utilizes the benefit of GV bound.

quality: analyzed the proposed method with many experiments.

clarity: the paper is well-written

significance: somehow significant as OCD is more practical in open-world scenario, and may help improve retrieval systems.

the one insight I got from this paper is that using hash code alone is sensitive when code length increases so it cannot be used for cluster index easily, and we need to make the hash code as compact as possible to reduce sensitivity (i.e., the same class must have the same hash code, by guiding the hash code into center, although already shown in multiple deep hashing papers such as CSQ [1], DPN [2], OrthoHash [3], [4]).

[1] Li Yuan et al. Central Similarity Quantization for Efficient Image and Video Retrieval. CVPR 2020.
[2] Lixin Fan et al. Deep Polarized Network for Supervised Learning of Accurate Binary Hashing Codes. IJCAI 2020.
[3] Jiun Tian Hoe et al. One Loss for All: Deep Hashing with a Single Cosine Similarity based Learning Objective. NeurIPS 2021
[4] Liangdao Wang et al. Deep Hashing with Minimal-Distance-Separated Hash Centers. CVPR 2023.

Weaknesses:
Comparison to deep hashing based methods are missing. There are many parts of the designed components similar to deep hashing such as the minimal hash distance formulation is adapted directly from [1]. Part of the loss objective is similar to cosine-similarity-based hashing methods like [2]. Another simple baseline is using ProtoPFormer + Deep Hashing method. From this paper, we do not know whether a naive application of deep hashing-based method is sufficient hence we cannot verify the novelty and effectiveness of the proposed method.

[1] Liangdao Wang et al. Deep Hashing with Minimal-Distance-Separated Hash Centers. CVPR 2023. 
[2] Hoe et al. One Loss for All: Deep Hashing with a Single Cosine Similarity based Learning Objective. NeurIPS 2021.

Limitations:
The author did mention limitations in the appendix which I also agree with.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
kf80ZS3fVy;"REVIEW 
Summary:
The paper introduces UniKE, a novel multimodal editing method that addresses challenges in knowledge editing for Multimodal Large Language Models. UniKE unifies intrinsic knowledge editing and external knowledge resorting by vectorized key-value memories. By disentangling knowledge representations into semantic and truthfulness spaces, UniKE promotes collaboration between intrinsic and external knowledge editing, enhancing the post-edit MLLM's reliability, generality, and locality. Extensive experiments validate UniKE's effectiveness.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	UniKE establishes a unified framework for intrinsic knowledge editing and external knowledge resorting.
2.	Extensive experiments demonstrate that UniKE consistently maintains excellent reliability, generality, and locality

Weaknesses:
1.	The presentation is confusing. For example, in intrinsic knowledge editing, how do you actual edit the intrinsic knowledge in FFN. Do you follow the same pipeline as T-Patcher or adding an additional neural network? Although intrinsic knowledge is considered as key-value pairs but they are different in the end.
2.	The unified framework and disentanglement of knowledge representations into semantic and truthfulness spaces introduces additional complexity to the editing process. If I understand correctly from the implementation details, it requires more than 15k additional triplets to train the encoders.
3.	In the experiments, I think /beta is another hyperparameter, but I did not find an experiment discussing it.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes UniKE, a novel multimodal editing method that establishes a unified perspective for intrinsic knowledge editing and external knowledge resorting. On this basis, the authors combine both types of knowledge editing methods, executing them in the latent space with a unified paradigm. Furthermore, this paper proposes to disentangle the knowledge representations into the semantic and truthfulness spaces, effectively enhancing the collaboration between intrinsic knowledge and external knowledge resorting. Extensive experimental results show that UniKE achieves promising results under various settings, ensuring that the post-edit MLLM maintains excellent reliability, generality, and locality.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
(1) I think knowledge editing for MLLMs is a relatively new topic. Previously, a few studies merely adapted existing knowledge editing methods from the NLP field into multimodal domain. To the best of my knowledge, this paper is the first to conduct a detailed and systematic analysis of the strengths and weaknesses of existing methods when applied to editing multimodal LLMs.

(2) The proposed method is very novel and effective. Previous efforts in knowledge editing show significant differences between intrinsic knowledge editing methods and external knowledge resorting methods. In this work, the authors ingeniously convert in-context editing into the format of feature shifting, achieving a unification of the editing paradigms that can operate simultaneously within the same transformer layer with synergistic correlation. I find this to be a very inspiring design. Moreover, the design of knowledge collaboration is closely integrated with this unified paradigm.

(3) The experiments are very solid and thorough, clearly demonstrating that UniKE effectively addresses multimodal knowledge editing tasks under various setups. Meanwhile, the authors have also provided the implementation code for the experiments.

(4) I commend the authors for conducting an extensive set of ablations and analyses, which are very helpful in understanding the impact of each component within UniKE.

(5) Additionally, I believe that the method proposed by the authors is not only applicable to knowledge editing tasks. By converting in-context learning into the representation space and avoiding the need to increase the context window space, it better synergizes with parameter update learning. I consider this to have significant implications for further studies on how to construct more powerful MLLMs.

Weaknesses:
(1) In the NLP community, some studies will discuss the resilience to overediting [1] of knowledge editing methods by adopting the contrastive knowledge assessment [2] (CKA). Unlike the locality property that measures whether LLMs forget previous knowledge, overediting can be understood as excessive generalized to seemingly similar but unrelated samples. Although there may be no current work on multimodal editing that discusses the phenomenon of overediting, I encourage authors to add relevant experiments for a straightforward comparison of the resilience to overediting among each method (UniKE, MEND, T-Patcher, and IKE).
(2) A more challenging task of knowledge editing is counterfactual editing, where the edited answer $y$ to the question $x$ can sometimes be counterfactual to the real world. A typical counterfactual editing dataset in the NLP community is called COUNTERFACT [3], which more accurately reflects the true effectiveness of knowledge editing methods by avoiding the effects of LLMs knowing this knowledge before editing. I encourage authors to construct multimodal counterfactual editing datasets and conduct more experiments to verify whether UniKE performs better in counterfactual editing scenarios compared to MEND, T-Patcher and IKE. 
[1] Zheng, Ce, et al. ""Can we edit factual knowledge by in-context learning?."" arXiv preprint arXiv:2305.12740 (2023).
[2] Dong, Qingxiu, et al. ""Calibrating factual knowledge in pretrained language models."" arXiv preprint arXiv:2210.03329 (2022).
[3] Meng, Kevin, et al. ""Locating and editing factual associations in GPT."" Advances in Neural Information Processing Systems 35 (2022): 17359-17372.

Limitations:
The authors have adequately discussed the limitations and potential negative societal impact.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
UniKE is a unified framework for multi-modal knowledge editing that includes three main aspects:
1.Knowledge Separation: UniKE divides knowledge into factuality and semantic spaces to manage and coordinate different types of knowledge more effectively.
2.Knowledge Collaboration: In the factuality space, UniKE standardizes new knowledge based on a learned factuality distribution, enhancing reliability and generality. In the semantic space, it adjusts the integration of external knowledge based on relevance to the input samples, maintaining locality.
3.Multi-step Editing: UniKE supports single-step, multi-step sequence, and cross-task editing while maintaining high reliability, generality, and locality.
Through these innovations, UniKE significantly improves performance in multi-modal knowledge editing tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The paper introduces UniKE, a novel framework that seamlessly integrates intrinsic and external knowledge editing, enhancing the model’s ability to handle complex multimodal information effectively. The motivation behind the work is clear, and the article is well structured, guiding the reader through the innovative approach and its benefits.

2.By disentangling knowledge into semantic and truthfulness spaces, the proposed method ensures robust collaboration between different types of knowledge, significantly improving the model's reliability, generality, and locality. The method's effectiveness is demonstrated through comprehensive experiments across various settings, consistently outperforming existing state-of-the-art methods.

3.UniKE's design allows for application across different multimodal models and editing scenarios, making it a versatile and robust solution for enhancing multimodal language models.

Weaknesses:
1.The paper does compare UniKE with other intrinsic knowledge editing and external knowledge resorting methods and highlights its efficiency in several aspects. However, it lacks a detailed discussion on computational speed and resource utilization.

2.The reliance on fine-tuning for knowledge updates could lead to overfitting, especially if the model is frequently updated. This could impact the model’s generalization abilities, making it less effective in unforeseen or less frequent scenarios.

3.The paper lacks detailed explanations for the evaluation metrics in Line 240. In Table 2, across multiple experiments, it is unclear why there is little difference compared to the SERAC method in the T-Loc metric, but a significant difference in the M-Loc metric.

4.The paper lacks detailed information about the parameter ""n"" used in the contrastive learning formula, making it difficult to understand its impact on model performance, and does not discuss how different values of ""n"" might influence the results and effectiveness of the method.

Limitations:
The limitations of the article have been discussed by the authors

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes UniKE, a novel multimodal editing method that establishes a unified perspective and paradigm for intrinsic knowledge editing and external knowledge resorting. Within such a unified framework, the authors further promote knowledge collaboration by disentangling the knowledge representations into the semantic and truthfulness spaces. Extensive experiments validate the effectiveness of UniKE, which ensures that the post-edit MLLM simultaneously maintains excellent reliability, generality, and locality.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and easy to follow and the motivation is clear and reasonable.
2. The paper gives a unified perspective on the intrinsic refinement of knowledge and the strategic reorganization of external knowledge, which can enhance subsequent research endeavors.
3. The experimental settings (one-step editing, sequential editing, cross-task editing) are fair.

Weaknesses:
1. The authors only edit Qformer style MLLMs (MiniGPT4, BLIP2), more foundation models should be compared. e.g., LLava. Besides, the improvement on BLIP2 (Tab.1) seems incremental.
2. More recent model editing baselines should be compared to prove the effectiveness of the proposed methods.
3. The paper focuses on MLLM editing, what are the main differences between MLLM editing and LLM editing? What are the specific designs for multimodal models? I notice that some compared methods are proposed for LLM editing. Can the proposed methods be used on LLM editing? The authors should give some explanations and experimental results if possible.

Limitations:
The paper briefly mentioned limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
xjyU6zmZD7;"REVIEW 
Summary:
The paper trains SNNs using surrogate gradient learning. In order to mitigate the gradient vanishing problem, the paper proposed the Shortcut Back-propagation method and utilizes an evolutionary algorithm framework to balance the training of shallow and deep layers. The effectiveness of the proposed method is demonstrated through many experiments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1)	The shortcut backpropagation method and the evolutionary training method are novel. 
2)	This paper can well handle the gradient vanishing problem.
3)	The paper is well-written.
4)	The paper shows the effectiveness of the proposed methods through many experiments.

Weaknesses:
1)	The author should add more mathematical proof to demonstrate that the mentioned residual structure in SNN is not very effective? The introduction of shortcut branches might add complexity to the network architecture, which could affect the interpretability of the model.
2)	Some recent SOTA works should be compared with too.  The authors can also compare with paper [1][2] which obtains really good results by MS-ResNet-18 backbone with 1 or 6 timesteps on large imageNet datasets.


[1]Yao M, Zhao G, Zhang H, et al. Attention spiking neural networks[J]. IEEE transactions on pattern analysis and machine intelligence, 2023.

[2] Qiu X, Zhu R J, Chou Y, et al. Gated attention coding for training high-performance and efficient spiking neural networks[C]. Proceedings of the AAAI Conference on Artificial Intelligence. 2024, 38(1): 601-610.

Limitations:
I find no limitation about the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a simple method to mitigate the gradient vanishing problem in the training of SNNs. This method introduces some early classification heads (including a pooling layer and a fully connected layer) to the SNN. Because the gradients from the early classification heads pass fewer surrogate gradients, this method aids the SNN in addressing the gradient vanishing problem. The authors also suggest an evolutionary training framework that changes the loss function to gradually adjust how important early classification head outputs are during the training phase. The proposed methods are only alive in the training phase and will not affect the inference phase of SNN.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This proposed method partially alleviates the gradient vanishing problem in the training of SNN with surrogate gradients. Furthermore, the method has demonstrated excellent performance across multiple datasets. The Short-BP method can be easily integrated into the SNN training process without introducing excessive computational overhead. Furthermore, the evolutionary training framework effectively mitigates the short-BP problem, which may make the network pay more attention to early classification heads than the final SNN output. The writing in this paper is clear and concise.

Weaknesses:
1. In this paper, the author only demonstrates a change in gradient distribution in the first layer. Presenting the changes in the men and variance of the absolute gradients for each layer would provide a more direct proof of their argument.
2. The author should provide a more detailed mathematical proof to explain why the use of surrogate gradients in deep SNN would lead to gradient vanishing, as well as why direct use of residual learning will not address the problem.
3. The author has not demonstrated their method on much deeper network architectures where the gradient vanishing problem is more severe.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes shortcut connections between layers to mitigate the gradient vanishing problem in SNNs. Additionally, the authors present a way to phase out the shortcut connections over training so that inference can be done without these additional connections. The experiments show that this method improves training performance in several image classification tasks.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1.The idea is small, but interesting and effective enough.

2.The performance improvement over the existing SNN methods is noticeable.

3.The paper is well-written.

Weaknesses:
1.The proposed method will increase the training time.

2.In the experimental section, some newer methods should be compared with this method.

3.Figure 2 lacks horizontal and vertical coordinates, and the readability and comprehensibility of the picture need to be improved.

Limitations:
None.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
eNCYpTCGhr;"REVIEW 
Summary:
This paper studies fully first-order methods for bilevel optimization with strongly convex lower-level objectives and linear lower-level constraints, including both inequality constraints and equality constraints. For linear inequality constraints, the paper used a penalty methods to construct hypergradient estimators and demonstrate the approximation error. Then combined with a gradient method for nonsmooth nonconvex optimization to obtain a Goldstein stationarity point. While for linear inequality constraints, the paper proposed to use zeroth-order approximation of the hyper-gradient and demonstrate complexity.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
1. The paper considered both linear inequality and equality constraints. 
2. The hypergradient construction in the linear inequality constraint setting is interesting and the paper provides approximation guarantees. It would be useful for follow-up papers. 
3. Obviously, the complexity bounds are not yet optimal. But the paper has contributed several interesting directions that worth further exploration. 
3. Algorithm 5 uses zeroth-order oracle is very smart as in many bilevel problems, the dimension of the upper-level is small such as hyperparameter optimization. 
3. The finite-difference gradient proxy for the linear equality constrained case is also interesting.

Weaknesses:
1. How to solve linear inequality constrained case with simple gradient methods to achieve optimal complexity bounds still remain unknown. The current algorithm is a bit complicated and hard to tune in practice. 
2. It is unclear if the complexity bounds are optimal for both linearly inequality and equality constrained settings. Following the conventional notation for stationarity such that $||\nabla F(x)||\leq \epsilon^2$, the complexity bound for Alg 1 is $O(\epsilon^{-8})$, for Alg 2 is $O(\epsilon^{-10})$ if further taking into account the complexity to get $tilde \nabla F$, and for Alg 3 is $O(\epsilon^{-4})$. Note that the paper consider deterministic bilevel optimization, it implies that there are a lot of room for improvements. The paper should mention comparison to unconstrained lower-level bilevel optimization so that readers know the results can be improved. Despite the complexity bounds, I still believe that this paper makes good contribution to the community of bilevel optimization.
3. The methods proposed in the paper are not aligning with each other well. Despite the constrained lower-level bilevel optimization setting, the paper lacks a coherent narrative in terms of algorithmic design.

Limitations:
The paper includes several settings. Yet why adapting a specific strategy to solve each setting is not sufficiently motivated. Such as why one needs to use Alg 2 and Alg 3 instead of directly using the hypergradient estimator with gradient descent. Why one needs to use zeroth-order oracle for the linearly equality constrained setting instead of first-order oracle?

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This works deals with constrained bilevel optimization problems where the lower-level has linear inequality or equality constraints. A set of algorithms is developed that does not require access to the Hessian, but only to zeroth and first-order information. In the case of inequality constraints, convergence is established to an $(\delta,\epsilon)$-Goldstein stationary point, due to the non-smoothness of the problem, using $O(\delta^{-1}\epsilon^{-4})$ or $O(d\delta^{-1}\epsilon^{-3})$ oracle calls (d: dimension of the upper-level).For equality-constrained problems, the convergence rate is nearly optimal with a rate $O(\epsilon^{-2})$.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
* This work makes progress in the topic of constrained (in the lower-level) bilevel problems, which is a challenging class of problems, much more difficult than unconstrained problems.
* The development of first-order algorithms for bilevel problems with linear constraints in the lower-level. On the contrary, typically implicit gradient methods for (unconstrained or constrained) bilevel problems require access to the Hessian. In addition, for the case of inequality constraints there are no assumptions involving second-order derivatives.
* The design of algorithms that deal with the non-differentiability of the hyperobjective F.
* Some of the proposed algorithms (e.g., Algorithm 3) appears to be simple and easy to implement.

Weaknesses:
* A major weakness of this work is the small number of experiments. There is only a single example bilevel problem over which experiments are performed. In addition, there are no experiments on real applications (e.g., in machine learning).
* In the experiments section the proposed method is compared only with a single baseline, which does not correspond to any published bilevel method. Why aren’t there any comparisons with other bilevel algorithms that deal with constrained bilevel problems? There are both value function and implicit gradient methods that can, at least in theory, deal with the example problem used here, e.g. [37,38]. 
* The algorithms require access to exact solutions of certain problems, such as $y^{\ast}(x)$ of the lower-level or $y_{\lambda^{\ast},\alpha}^{\ast}(x)$ of problem (3.6). This is not the case in practice.

Limitations:
* See weaknesses above.
* The authors discuss limitations of their work in section 7.
* No negative societal impact

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper provides algorithms for bilevel optimization with linear equality and inequality constraints. The main contribution is that the algorithms are fully first order and do not require Hessian computations. This is achieved by reformulating the linearly constrained bilevel optimization problem using the penalty method and assuming access to the upper-level variable $x$ and approximate dual optimal solution. In this setting an inexact gradient oracle for the problem is constructed. The authors provide stationarity guarantees for their algorithms under certain standard assumptions. They also describe how to use their inexact gradient oracle for non-convex non smooth optimization. The algorithms and theoretical results are backed by small set of proof-of-concept experiments.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1) Typically, hessian inverse computation is quite expensive and hence coming up with a fully first order method with guarantees for an optimization problem has both good theoretical and practical significance. The paper will be of interest to the community.
2) The paper is written very nicely. There is a lot of clarity regarding the related works, notations, main contributions and techniques. Proof sketches are also provided for some results in the main paper. Though the paper is heavy on technical material, the organization makes it somewhat easier for the reader to follow the arguments. I am not exactly from the same research area but could follow most of the paper. 
3) I could not check all the proofs in details, but the main claims of the paper appear correct.
4) Nonconvex non smooth optimization is encountered in many ML problems these days. The applicability of the inexact gradient oracle to these problems with guarantees may be of practical significance.

Weaknesses:
1) There are no experiments highlighting the effectiveness of Algorithm 2.
2) The experiments given here are also only of a proof -of -concept nature and are not comprehensive. Some more comparisons with existing algorithms (based on time) and ablation studies may provide more insights regarding the practical applicability of the methods.

Limitations:
Yes, the limitations have been clarified in the paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the linearly constrained bilevel optimization problem and provides a fully first-order method with solid theoretical analysis. To approximate hypergradient, the penalty method seems novel to me and it is applied in two settings where the LL problem is linear inequality or equality constraints.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The presentation is pretty good and I feel easy to follow.
2. Theoretical analysis is solid and rigorous.
3. Code is provided.

Weaknesses:
1. The experiment does not seem sufficient.
2. The checklist should be behind the appendix as I memorize.

Limitations:
Please check weaknesses and problems.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
Yz3wBKoK0K;"REVIEW 
Summary:
This paper proposes a new method to enhance the downstream generalization of CLIP by distilling knowledge from LLM- or human-generated text prompts. The proposed method involves training a prompt generator to predicts prompt embeddings (AAPE) based on images.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The method aggregates and distills task-related knowledge from LLM- or human-generated text prompts, leading to better downstream generalization of CLIP.
2.	The approach demonstrates good performance on various downstream vision-language tasks.

Weaknesses:
1.	The analysis on the aggregated text prompt is missing. There is no evidence to prove that Input-Adapted Prompt Aggregator can alleviate the influence from noisy text.
2.	The evaluation efficiency may be significantly impacted by the need to connect predicted prompts base text features.
3.	A minor error is found in line 181, where “a photo of a {class} + category type” should be “a photo of a {class}”.

Limitations:
The analysis on the aggregated text prompt is missing.
The proposed method should be applied on more vision-language models, such as SigLIP[1]. This limitation has been mentioned in the paper.

[1] Sigmoid Loss for Language Image Pre-Training.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The proposed method leverages language priors for better downstream adaptation and generalization of CLIP [37], which is similar to CuPL [36] that utilizes prompts generated by a LLM (e.g., GPT-3) for zero-shot image classification using CLIP. Unlike CuPL, the proposed method aggregates multiple LLM-generated or human-generated prompts to construct an aggregated text feature. Using the aggregated text feature as a guidance, the authors adaptively transform CLIP image features to tackle few-shot image classification, image-to-text retrieval, image captioning, or VQA. A notable difference from CuPL is that the proposed method does not run an inference of LLM at test time.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper has the following strengths:

(i) The proposed method is timely, and seems to be effective and efficient. By aggregating text features produced from LLM-generated or human-generated prompts and then using the aggregated feature as a guidance, the proposed method could effectively leverage language priors for better downstream adaptation and generalization of CLIP. After the distillation of rich language knowledge during the training time, the proposed method does not need to run an inference of LLM at test time. 

(ii) The authors validate the effectiveness of the proposed distillation method for downstream tasks (few-shot image classification, image-to-text retrieval, image captioning, VQA). It demonstrates that the learned adaptive transformation guided by language priors actually improves the adaptation and generalization of CLIP for downstream tasks.

Weaknesses:
This paper has the following weaknesses:

(i) This paper lacks the justification of how the adaptive transformation of CLIP image features, guided by an aggregated text feature, overcomes the image-text modality gap which exists in CLIP vision-language space [A]. Despite the modality gap, the proposed method simply encourages transformed CLIP image features to be located nearby an aggregated text feature (using L2 distillation loss).

[A] Liang et al., “Mind the Gap: Understanding the Modality Gap in Multi-modal Contrastive Representation Learning”, NeurIPS, 2022.

(ii) The proposed method might not be able to improve the model robustness to distribution shifts which are difficult to be explained by natural language. For example, the Terra Incognita dataset [B] contains distribution shifts caused by different camera locations in wild environments. The widely used benchmark of in-the-wild distribution shifts, WILDS [C], also contains such distribution shifts. Since the proposed method leverages only language priors, it might not be good at handling those distribution shifts.

[B] Beery et al., “Recognition in Terra Incognita”, ECCV, 2018.

[C] Koh et al., “WILDS: A Benchmark of in-the-Wild Distribution Shifts”, ICML, 2021.

(iii) This paper lacks experimental results obtained with different distillation loss coefficient values, although the distillation loss is the main contribution of this paper. According to L191-196, it simply states that the proposed model is not sensitive to different coefficient values. Without relevant experimental results, it is difficult to evaluate this statement.

Limitations:
Yes, in page 9 and 16.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new prompt embedding named Aggregate-and-Adapted Prompt Embedding (AAPE), which improves prompt learning by distilling knowledge about more detailed descriptions of classes into prompt embeddings. Concretely, the “summary” prompt is obtained by aggregating diverse reference prompts. Then, the prompt generator is trained to produce a prompt embedding that stays close to the aggregated summary while minimizing task loss at the same time. From the experiments, the proposed AAPE shows good performance on diverse tasks such as few-shot classification, VQA, and image-to-text retrieval.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- How to well train prompt embeddings is one of the important topics in fine-tuning Vision-Language Models.
- The paper is well written.
- The proposed approach shows good performance in multiple tasks.

Weaknesses:
- More details on using the CLIP reward are required. From my understanding, there is another loss to enhance the CLIP reward $\texttt{CLIP-s}\left(\boldsymbol{x}, \boldsymbol{p}^\alpha \right)$.
- It would be better to include the performance of the model with only Aggregate-and-Adapted Prompt Embedding (AAPE) $h\left(\mathbf{x}\right)$ in Table 1 without using text embedding $\mathbf{w}_i$ and projection $g$. This result clearly shows the performance gain induced by the Aggregate-and-Adapted Prompt Embedding.
- It would be better if the efficiency comparison was included in the paper compared to other prompt learning methods such as CoOp, MaPLE, and PromptSRC, etc.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This framework first aggregates textual knowledge from human or large language model (LLM) generated prompts into a summary aligned with each input image. This is achieved using a prompt aggregator. A prompt generator is then jointly trained to create prompt embeddings that are close to this aggregated summary while also minimizing task-specific loss. The method demonstrates improvements in performance on various downstream vision-language tasks, including few-shot classification, visual question answering (VQA), and image captioning without incurring LLM inference costs during testing.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The motivation is clear and the method is reasonable to me. The proposed method extends the application of downstream tasks from the common image classification to image-text retrieval, visual question answering (VQA), and image captioning. The performance on Flickr30k dataset is remarkable.

Weaknesses:
This paper misses one relevant research in prompt learning “ArGue: Attribute-Guided Prompt Tuning for Vision-Language Models.” ArGue introduces an attribute-guided prompt tuning approach that outperforms traditional prompt learning methods in specific tasks and datasets. The failure to reference and compare these latest methods may limit the comprehensiveness and advancement of the proposed method.

The comparison methods used for the image-to-text retrieval task in this paper are somewhat outdated and do not incorporate the latest research advancements. For example, BLIP-2, as a new image-to-text retrieval method, has demonstrated superior performance across various tasks and datasets. To more accurately assess the effectiveness of the proposed method, the latest comparison methods such as BLIP-2 should be included in the experiments.

The method in this paper shows overfitting on the base classes, with significantly lower performance on the novel classes compared to the base classes. This indicates a lack of generalization ability when dealing with new categories, potentially leading to poor performance on unseen data in real-world applications.

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a prompt learning method that distills the knowledge from pre-trained LLM while conditioning them on the image embedding. A Prompt Aggregator module combines the LLM generated prompts per image and the Prompt Generator module, generates a prompt from the image embedding. The modules are trained using a regularization loss between the aggregated and generated prompts, and the downstream task loss. Experimental results on 11 image classification datasets, four imagenet domain variants, two image captioning datasets and VQA dataset show the effectiveness of the proposed prompt learning method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The experimental results cover a broader spectrum that includes image classification, vision-language understanding, such as captioning and VQA. The results show the proposed method is effective across the range of tasks.

Weaknesses:
- The method is shares a lot if similarity to [1] which distills the LLM knowledge through prompts to the CLIP text encoder. It is important to compare and distinguish how this work differs from [1]. 
- See questions

[1] Khattak, Muhammad Uzair, et al. ""Learning to Prompt with Text Only Supervision for Vision-Language Models."" arXiv preprint arXiv:2401.02418 (2024).

Limitations:
none

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wAqdvcK1Fv;"REVIEW 
Summary:
This article, ""Energy-based modelling for discrete and mixed data via heat equations on structured spaces"", proposes to perform the training on EBM, using the Energy Discrepancy (ED) loss, in the case where having multi-modal dataset mixing eventually continuous inputs but also discrete (categorical) ones. The work describes into details how to parametrize in different setting the inclusion of discrete variables, and they apply it to various datasets.
The main contributions are the design of the continuous time Markov chain transition probability that lies at the heart of the ED approach and the application to tabular dataset for which generative approach is usually hard.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors show how their method can be efficiently used on Tabular dataset. In particular, they apply to several dataset and show that in average the EBM trained with Energy-Discrepancy using a discrete implementation of the Markov chain transition probability outperform the concurrent approach, ref[Xu et al 2019]. 

The authors also show experimental results on image modelling.

Weaknesses:
The authors extend the formalism of Energy Discrepancy to the case of including discrete state in addition to continuous features. Whether or not this justifies an entire publication can be debated, Although it should be emphasized that the datasets under consideration are quite original.

It might be because I'm not an expert on ED, but while traditional EBM relies on MCMC to compute the gradient, ED does not. However, it is not clear to me if sampling the EBM trained in such way need MCMC to be generative ? If so, the article should provide more details on the implementation. They should also check that the trained model is at equilibrium (the generated samples corresponds to the equilibrium properties of the model).

More importantly, the comparison for the tabular dataset is only done at the level of the AUC curves. Can at least the authors compare the frequency and correlations amongst the generated samples and the true ones ?

Limitations:
The limitations are correctly discussed in the article.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper extends the Energy Discrepancies framework introduced by Schroder et al. to the setting of discrete data. In order to do this, the authors first describe ways to perturb discrete data by modeling the perturbation process as a CTMC. They describe suitable perturbation choices for different types of discrete data (e.g. binary, categorical, ordinal, cyclical) and describe different considerations for the time parameter in the CTMC. They then propose an approach that performs a Monte-Carlo estimate of the contrastive potential needed for the Energy Discrepencies loss and compare their method to existing methods for training discrete EBMs.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: Energy discrepancy is a relatively new approach. While the original paper proposed some extensions to discrete data, this paper goes into extending energy discrepancy to discrete data in much more depth and includes new mathematical and experimental analyses. 

Clarity: Overall, the paper is well written.  

Quality: I believe the paper is technically sound. 

Significance: The authors show on toy examples that their method outcompetes contrastive divergence. The authors appear to generally outperform two methods proposed in 2019 along with contrastive divergence. While I have some minor concerns about these baselines, outperforming these baselines is at least demonstrating some empirical benefit of this approach.

Weaknesses:
Clarity: The clarity can be improved a bit (see my questions below). 

Significance: Despite demonstrating that the method can work empirically, I have some concerns with the overall significance. It seems that while the method works well on toy examples, the results are less impressive on real-world image modeling tasks. I am unfamiliar with the field of tabular data modeling and therefore, cannot properly assess the significance of the results. Beyond contrastive divergence the main baselines is a method from 2019 with 2,000 citations. Are there better baselines to compare against among these 2,000 citations?

Limitations:
I would like more honest discussion throughout the paper of the relative strength and drawbacks of their method. For example, while it is true that this method is “MCMC-free” it still requires Monte-Carlo estimation and the relative tradeoffs here are not adequately discussed. I would also like to see more comparisons of CD with large numbers of MCMC steps v.s. ED with large numbers of samples.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a suite of methods for training energy-based models for discrete and mixed data using the Energy Discrepancy loss, a recently proposed method for training EBMs. Compared to contrastive divergence, it does not require MCMC sampling from the model distribution during training, improving training efficiency. This is done by simulating a diffusion process on the discrete states and effectively using those noisier samples as the negative samples. The paper introduces a connection between the new method and maximum likelihood estimation, showing that energy discrepancy as applied to discrete state spaces can converge to the negative log-likelihood. In experiments, the new method behaves favourably compared to contrastive divergence-based methods on synthetic data sets, on average better than baselines on real-world tabular data sets, and comparably to many competing methods generation on discrete image modelling. An application of the trained EBM on classification and improving uncertainty quantification compared to a direct classifier is also shown.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper proposes a relevant extension to recently published work. Especially Theorem 1 does not seem obvious, and the paper may open up the use of the Energy Discrepancy loss to a much wider variety of use-cases. 
- The method is also quite simple, and seems simple to implement. 
- The paper connections to recent work on discrete diffusion models, and proposes a variety of methods to estimate the energy discrepancy loss. 
- The results are good compared to standard contrastive divergence based methods
- The paper is well written, and I found it easy enough to understand even without prior knowledge on the Energy Discrepancy method.

Weaknesses:
- As noted in the limitations, the application to data such as images seems to be challenging as the noisy negative samples may not give very useful training signal in this case. 
- Although the energy discrepancy method has already been proposed and published in previous work, I found the justification for the method slightly confusing while reading this paper. What is Theorem 1 exactly saying? (see questions) The loss also is, in practice, approximated with something slightly different than the proposed loss, which seems conceptually a bit confusing. However, this is not a major concern given that the base method has been proposed and published in previous work.

Limitations:
Addressed adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
: The paper introduces a novel method for training energy-based models (EBMs) on discrete and mixed data using heat equations on structured spaces. This method employs the Energy Discrepancy (ED) loss function, which eliminates the need for Markov chain Monte Carlo (MCMC) by using graph-structured perturbations. The proposed approach is evaluated on several applications, including discrete density estimation, synthetic data generation, and calibrated classification, demonstrating significant improvements in training efficiency and model performance.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper successfully extends the Energy Discrepancy method to discrete and mixed data with solid theoretical analysis, addressing the challenge of robust and fast sampling in these space. The designed experiments demonstrate the method's ability to accurately capture complex data structures and generate high-quality synthetic data, highlighting its practical applicability.

Weaknesses:
1.	Despite the method's solid contributions and experimental design, the motivations behind each step and their presentations are not very clear, making it hard to follow. For instance, in Section 3.1, the paper discusses different structured and unstructured categorical values, introducing the four types {cyc, ord, unif, abs}. However, it is not clear why these specific structures are chosen. Are they meant to cover all categorical values comprehensively, or are they the most common in tabular data? Providing a clearer rationale would help readers understand the choices made.

2.	The scalability of the proposed method in such scenarios is a significant concern. An analysis or discussion on how the method handles large categorical values would be beneficial. This could include potential modifications or considerations to ensure that the method remains efficient and practical when applied to datasets with large categorical variables. What’s more, I strongly recommend moving these algorithms from the appendix into the main body of the paper. This would make the paper easier to follow and more accessible to readers who need to understand the detailed workings of the method.

Limitations:
See weakness

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
ZoarR5QmFX;"REVIEW 
Summary:
This paper studies prompt optimization methods for finetuning language models. While previous methods mainly concern with in domain performance, this paper brings awareness of the domain generalization issue presented in existing PO methods, under the setting where the target domain is unknown. Two empirical findings that link the domain generality of the prompt to the behavior in attention map are presented regarding this setting. Building on top of these findings, the paper proposes new objectives that account for domain generability for both soft and hard prompt optimization settings. Empirical study on BERT-size transformers and standard NLP datasets reveals that the proposed method achieves fairly consistent and substantial gain over vanilla.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper presents a pioneering study of the domain generalization issue of prompt optimization for PLM finetuning under the more practical setting where the target domain is unknown.
2. The findings connecting the generalization ability of prompts to the attention patterns are interesting and of value to the community.
3. Translating the findings into loss objectives are nontrivial technical challenges, and the paper presents neat solutions to these problems.
4. Empirical gain are fairly substantial and consistent.

Weaknesses:
1. The main findings that motivate the proposed method are empirical. The paper can benefit from addressing the intuitions behind why such prompts are more generalizable.
2. The models of choice are mainly small-scale transformers rather than LLMs. It is unclear whether the findings on the attention patterns generalize to bigger models. The paper could benefit from further verifying them on open-source LLMs.
3. Presentation: the in-context citation formats are incorrect.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper focuses on improving the domain generalization of prompt tuning methods on LLMs. Specifically, this work claims that the concentration strength and concentration fluctuation of a candidate soft or hard prompt may indicate its generalization ability on new domains. By demonstrating the performance of various prompts with their concentration strength and fluctuation, the authors show that higher concentration strength and lower fluctuation may bring better prompt domain generalization. As a result, this paper proposes new objectives in soft and hard prompt tuning based on these observations. The experimental results show that the obtained prompts achieve better performance on new domains in NLI and sentiment classification tasks.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well organized, with very clear problem and methodology formulation, making the proposed approach easy to follow up.
2. The observation on prompt attention concentration and its potential correlation to domain generalization are interesting and may encourage further research on this topic.
3. The proposed objectives for soft and hard prompting methods are generalizable and are compatible with most recent prompting algorithms.

Weaknesses:
The main weakness of this work is the limited task types in the experiments. Only the sentiment classification and NLI tasks are considered in this work. It will be much better if more evidence or results on broader task types are obtained.

Limitations:
The authors have adequately addressed the limitations of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies the problem of prompt optimziation for domain generalization.  Through a pilot experiment, they find that the domain generalization capability is tied to the attention concentration in later layers of the network.  Based on this finding, the authors design a set of regularizers to improve both soft and hard prompt optimziation procedures.  Empirically, the method is tested on sentiment and NLI tasks.  Results demonstrated a reduced generalization gap, and improved performance with the added losses over several prompt optimization methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The exploration of prompt optimization is a highly important avenue for research as prompt optimization remains an efficient way to fine-tune language models. At the same time, it is improtant to consider the robustness of such approaches and how to improve this.

- The paper is overall well-written and flows nicely.  The paper highlights the core properties for the findings and main results in the introduction of the paper, constructs an experiment to validate this in Section 3, subsequently intorduces their regularizer, and demonstrates results across two tasks.  

- The proposed approach is simple to implement and can be applied irrespective of the architecture, as it is only an adjustment of the loss function thus having general applicability.

- Authors have conducted thorough ablations studies in the proposed approach in the Appendices particularly around stability, visualizations, and initalizations.

Weaknesses:
- My biggest concerns with this paper are around the experimental results.  The performance improvements in experiments Tables 1 and 2 are small covering only 1-2% improvements over baselines.  The paper also does not make comparisons to existing approaches for domain adaptation of prompts such as 
https://arxiv.org/pdf/2207.07087
https://arxiv.org/pdf/2210.02952
https://arxiv.org/pdf/2305.13954

- Further the paper only explores limited settings including a single architecture, task, and setting.  Exploring other modalities (such as vision transformers), additional tasks and settings, or other architectures would help solidify that the proposed approach is more general and would extend beyond the two tasks in this work.

- Some minor typos such as hypnosis at line 284, and some hyperparameters seem to be missing from the main text for replication purposes including the value for lambda and how this is selected for the regularizers.

Limitations:
- Proposed approach is only evaluated in fine-tuning settings.    This limits applicability to many settings where LLMs are evaluated such as in context learning, or zero-shot.  

- The proposed approach is evaluated on the Roberta model, whereas there are a number of language models that could be investiinvestigated including the T5 encoder-decoder models, and GPT decoder only models.  Do we expect different behaviors based on the architectrues and attention?

- The proposed approach is limited only to NLP applications, however could be applied for other variants such as vision transformers and robustness considerations.  Would we expect similar trends in other applications.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the domain generalization ability of prompts for pretrained language models (PLMs). The paper finds that prompts that receive higher attention weights from deeper PLM layers and those with stable attention distributions generalize better across domains. The authors introduce a novel objective called ""Concentration"" which implements a ""lookback"" attention from the current decoding token to prompt tokens, aiming to enhance both soft and hard prompt optimization methods. Their experiments demonstrate significant improvements in multi-source domain generalization accuracy—1.42% for soft prompts and 2.16% for hard prompts—while maintaining robust in-domain performance. These findings offer valuable insights into creating domain-generalizable prompts.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper is well-written.
* The authors started with an initial analysis to inform a novel training objective, which is both insightful and methodologically sound.
* The proposed method is simple and the authors demonstrate its effectiveness across several classification tasks.

Weaknesses:
I felt there were several obvious questions left unexplored, noted below, which raise concerns regarding the significance of the paper's contributions.

* The authors only experimented with a rather small model, i.e., 355M RoBERTa, which raises concerns about whether the proposed method works with larger model sizes.

* The authors focused solely on a classification tasks (sentiment classification and natural language inference). This raises concerns about the proposed approach's applicability and effectiveness for other tasks, like open-ended generation.

* Finally, the improvements were shown over rather weak baselines. For example, prompt tuning, particularly with small models and limited training data, is a rather weak approach. I also felt that the authors compared their method against a weak implementation of this baseline, using only 5 soft prompt tokens and a learning rate of $2×10^{−5}$. For reference, the original prompt tuning paper used 100 prompt tokens and a learning rate of 0.3, which they found to be critical for prompt tuning's strong performance and faster convergence. These differences raise concerns about the significance of the proposed method's improvements.

Limitations:
The author discussed several limitations of their approaches, including the limited variety of prompts, the focus on a few-shot setting, the restriction of discrete prompt optimization to the input level, and the inapplicability of their methods to generation tasks.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
r8M9SfYMDi;"REVIEW 
Summary:
This paper proposes to carefully segment the training data so that different documents won't be mixed together. They also propose a Grow-P2 curriculum that increases training efficiency and stability.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed Grow-P2 curriculum is useful to practitioners if they want to pretrain large language models.

Weaknesses:
This paper presents only empirical results.

Limitations:
None.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper explores dataset decomposition for LLM pre-training. 

The method decomposes documents into subsequences and organizes them into buckets. Sequences of similar lengths are grouped in the same bucket, and different buckets have different lengths. This amounts to more efficient training. The paper investigates various mixtures of lengths separately for their impact on performance. 

The paper also explores a length-based cyclic curriculum learning - treating smaller length buckets as ""easy"" examples and larger ones as ""hard"" examples. Curriculum learning can improve the results a bit.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
* The high-level ideas are reasonably motivated. 
* Reasonably extensive experimental analyses of the methods are provided along with baseline dataset structuring methods for pre-training.
* Results are generally promising. Generally, it results in better task performance while taking less training time.

Weaknesses:
On the one hand, the proposals can be seen as important in exploring some unorthodox training structures for the specific context of LLMs and informing future pre-training. However, on the other hand, the paper seems to be mainly an exploration of hyperparameter tuning. The main proposed techniques seem like extensions of existing strategies (bucketing and curriculum learning) for LLMs. Bucketing is already understood to make things efficient, and curriculum learning has some positive results in NLP in general in earlier papers. Cyclic curriculum learning was also used in earlier works [1].

[1] CYCLICAL CURRICULUM LEARNING - Kesgin et al. ArXiv 2022

Also, if I understand correctly, the training speed gain may be less significant with flash-attention-based LLMs and alternative models like (Mamba, Linear Transformers).

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces 'Dataset Decomposition' (DD), a method to enhance the pre-training of Large Language Models (LLMs). Contrary to the traditional 'concat-and-chunk' approach, which can lead to unwanted cross-document attention and computational inefficiency, DD organizes datasets into buckets with sequences of uniform length from individual documents, enabling variable sequence length training. This method is demonstrated to reduce training time, improve model performance, and scale effectively with the size of the dataset.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Comprehensive Experiments: The authors conduct extensive experiments across various datasets and LLMs, with observations and analyses that are noteworthy.

2. Simplicity and Effectiveness: The proposed method is straightforward and efficacious.

Weaknesses:
The paper's writing requires improvement, as exemplified by the following:

Line 14: ""Our proposed method incurs a penalty proportional to the actual document lengths at each step, resulting in significant savings in training time."" Even after careful reading, it remains unclear what is meant by ""penalty"" here.

Line 158 (and many other instances): ""We follow the exact setup as in [32]."" When using citations within sentences, it is customary to use the \citet command, resulting in ""We follow the exact setup as in Liu et al. (2024),"" rather than using \cite or \citep.
Table 1: It would be beneficial to clearly define the meanings of the elements in the first column within the main body of the paper to prevent confusion.

Additionally, while it adds value to the paper to provide extensive discussions of various aspects through experiments, the overall organization of the paper should be enhanced to make the main focus of the paper clearer.

In Section 3.6, DD shows a significant advantage over other methods in long-context scenarios. However, the authors do not offer a detailed analysis of why this is the case. In my understanding, with a fixed number of training tokens per gradient update, DD increases efficiency by enlarging the batch size and reducing sequence length (<8k). In contrast, the baseline and ICLM maintain an 8k sequence length. It is unclear why DD performs better, as most of its training samples are actually shorter in length.

Limitations:
No potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a method called dataset decomposition for training large language models (LLMs) more efficiently. Traditional LLM training processes use fixed-length token sequences, leading to inefficiencies such as unnecessary computational costs from cross-document attention. The proposed method tackles this by organizing the training dataset into various ""buckets,"" each containing sequences of a fixed length from a unique document. This allows for variable sequence length training, where different buckets can be sampled during training based on a curriculum that adjusts for sequence length. The approach significantly reduces the attention computation overhead, leading to faster training times and improved model performance across various language understanding benchmarks. This method enables efficient and scalable LLM pretraining on large datasets, with experimental results showing up to three times faster attainment of target accuracies compared to traditional methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1: The paper is well organized and easy to follow. 

2: The motivation is clear and the proposed method looks simple yet effective. 

3: This paper conducted massive experiments and provided many valuable empirical results, which can support both the claim of this paper as well as many long existing guesses in the community.

Weaknesses:
1. It might be more accurate to consider that ""the cross-document attention allocates significant computational resources to attending to unrelated tokens that may not directly contribute to learning."" However, it's also valuable for models to develop the ability to discern and disregard irrelevant information.

2. The concept of a length-based curriculum, while insightful, isn't entirely novel. It has been explored in previous studies, such as those detailed in references [1] and [2]. Furthermore, numerous models, including BERT, employ similar curriculum learning strategies, albeit without specific emphasis on this aspect.

[1] World Model on Million-Length Video and Language with RingAttention
[2] GrowLength: Accelerating LLMs Pretraining by Progressively Growing Training Length

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
hsgNvC5YM9;"REVIEW 
Summary:
The paper presents an extension to rectified flow by Liu et al., 2023, a method based on optimal transport that can match samples from two distributions. This class of methods may have different uses but is often employed for the efficient generation of new samples and for morphing one sample to another. As such rectified flow and the extension proposed in this paper may be seen as finding efficient trajectories for use during generation by diffusion models. The method solves the transport flow in a simple Euler approximation of the ODE describing the flow. The paper identifies crossing of flow trajectories as a major obstacle in generating efficient paths in rectified flow and proposes constant acceleration flow (CAF). The paper proposes Initial Velocity Conditioning to produce the initialize the acceleration field in solving the ODE. Rectified flow uses constant velocity to solve the ODE using a reflow procedure to optimize the paths in a second step. Similarly, the proposed CAF also uses reflow to improve the initial paths. The paper reports experimental evaluation of the model with synthetic distributions and with CIFAR-10. The experiments demonstrate improved flow between distributions for synthetic data and improved FID score on CIFAR-10. The paper also reports an increase in straightness of trajectories.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The constant acceleration flow is novel to the best of my knowledge. 

The paper describes the Initial Velocity Conditioning and related steps to make the approach work on CiFAR-10. The approach is suitably summarized in pseudo-code in two algorithms. 

The synthetic experiments demonstrate the paper's motivation visually. 

The ODE framework for the transport and the forward Euler solver makes the steps in the procedure easy to follow. The analogy makes it easy to see that constant velocity and constant acceleration lead to straight trajectories is the initial coupling of the samples x_0 and x_1 is correct.

A comparison with many alternative methods is provided in Table 1. 

The proposed CAF visually leads to improved quality and stability on CIFAR-10 over rectified flow.

Weaknesses:
The paper presents arguably a substantial improvement of results obtainable with rectified flow in terms of quality at the price of increased computation. The paper does not attempt any argument why this is significant, i.e., has the improved quality the potential to lead to wider adaptation of rectified flow type methods. The paper has no real-world use case to motivate the work. The claim that experiments with CIFAR-10 at 32x32 pixels is a real-world example seems far-fetched.    

The initialization of the method and its impact on the result is not well explored. The initial velocity calculation requires sample x_1 drawn from the target distribution. Both, the paper under consideration and rectified flow employ a pre-trained generative model to obtain that sample. The quality of initial samples may impact the results of CAF. The authors should provide evidence to the contrary or provide an experimental evaluation of the influence. 

There is no theoretical justification why constant acceleration is superior to constant velocity flow generation. While the experimental evaluation appears clear, it remains unclear if this behavior is due to the tasks considered or holds more broadly. 

The paper claims reflow as a contribution but it has been proposed by rectified flow. The same is true for the measurement of straightness of flow trajectory with Normalized Flow Straightness Score (NFSS) in Eqn 12. 

The paper does not discuss how CIFAR-10 is used. One can speculate based on earlier work such as GLOW that unconditional generation and conditional generation may refer to use of class conditioning but a concise description of the experimental setup is required. 

The use of a pre-trained generative model should also be taken into account in the required computational effort during training. It may be application-dependent if such a model is available or if it need to be trained.

Limitations:
The limitations section should be moved from the appendix into the main paper as the computational impact of CAF over rectified flow is essential.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes Constant Acceleration Flow (CAF), which, instead of learning a velocity-based flow model like in Flow Matching, jointly trains an initial velocity model together with a constant time-dependent acceleration model. This framework aims to learn straighter paths and thus enable better results for few-step generation.

Additionally, they propose to condition the acceleration model on the initial velocity and propose to improve the initial velocity using a reflow procedure. CAF is empirically validated on toy data and CIFAR-10, including ablations on the different elements of the proposed framework.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The Constant Acceleration Flow is a new formulation for achieving straighter trajectories in flow-based generative models derived based on the assumption of constant acceleration.
- Good empirical results on CIFAR-10
- Includes important ablations on the velocity conditioning as well as on the magnitude of the initial velocity

Weaknesses:
""Reflow"" for initial velocity:
- The reflow procedure was proposed to straighten the paths of an existing generative model. The authors propose to use a ""reflow"" procedure for training CAF. However, they don't generate the couplings with a model trained with CAF but instead with a pre-trained EDM model, which is very different from how the reflow procedure was proposed. Additionally, it is unclear how dependent CAF is on this procedure and, thus, from the pre-trained generative model. In the CIFAR-10 ablation, the setting with constant acceleration and velocity conditioning but no ""reflow"" procedure (and thus no pre-trained generative model) is crucial to answer the question of whether CAF also works without a pre-trained model. If it does not, then I would argue that the proposed framework is more of a new distillation technique rather than a new generative model. This contextualization should also be more clearly explained in the main text.

Missing discussion and contextualization of related work:
- Acceleration Generative Modeling has been proposed in [1]. While CAF is based on Flow Matching and a constant velocity is chosen, [1] considers Bridge Matching in a stochastic optimal control framework with a changing velocity induced by the acceleration prediction. However, their trained model is also a parameterized acceleration prediction that takes as input the current data point $x_t$, current time $t$, and the current velocity $v_t$ effectively conditioning on the velocity. This framework is very related to the approach proposed by the authors, and the connection should be discussed in detail in the main paper. Additionally, an experimental comparison illustrating the differences between the two approaches could be beneficial.
- Coupling preservation through initial velocity conditioning: The authors refer to the reflow approach as existing related work. However, the reflow approach was proposed to straighten paths, not preserve couplings. Moreover, the problem of coupling preservation has been thoroughly analyzed in [2], which proposes to use a source point conditioning, i.e. conditioning on $x_0$. How does Augmented Flow/Bridge Matching (Flow/Bridge Matching with $x_0$ conditioning) compare to CAF? As mentioned, velocity conditioning is also used in [1].
- A possible way to empirically compare to these competing methods would be to extend the ablation study on CIFAR-10 to include [1] and [2].

Minor Weaknesses:
- Experiments are only conducted on CIFAR-10. Including at least one more dataset, e.g. 64x64 ImageNet/CelebA/etc, could strengthen the empirical results.

[1] Tianrong Chen and Jiatao Gu and Laurent Dinh and Evangelos A. Theodorou and Joshua Susskind and Shuangfei Zhai. ""Generative Modeling with Phase Stochastic Bridges"". In ICLR 2024.

[2] Valentin De Bortoli and Guan-Horng Liu and Tianrong Chen and Evangelos A. Theodorou and Weilie Nie. ""Augmented Bridge Matching"". In Arxiv 2023.

Limitations:
- Again, how dependent is CAF on the pre-trained model?
- Training both an acceleration and a velocity prediction model effectively doubles the amount of total parameters. This could be mentioned as another limitation.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper develops a new flow based generative model whose vector field is constructed similar to rectified flows (straight path between source and target samples), but instead of using a path with a constant speed, uses a path with constant acceleration.  This requires learning a neural network to parametrize the initial velocity field and another neural network to parametrize the acceleration field.  To improve their model's performance, the authors also propose parametrizing the acceleration neural network using the initial velocity and also applying reflows to the initial velocity.  The proposed method helps mitigate the flow crossing problem at a model level, which should in theory avoid excess reflows to learn non-crossed flows.  The experiments section demonstrates the ability of the proposed method to learn high performing generative models on single step generation on the CIFAR-10 dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The idea is novel, solves a relevant problem and fits nicely into the current landscape of the flow based generative modeling research area.  The paper was easy to read and the method itself is simple enough to understand and implement after a single pass over the paper.  The main contribution of the paper, in my opinion, is the conditioning of the acceleration neural network on the initial velocity.  This gives the final trained model a non-markov path measure, which is something that is of current interest to the field (Augmented Bridge Matching, De Bortoli et al., 2023).

Weaknesses:
- The related work should include references to building diffusion models with non-markov path measures (see De Bortoli et al., 2023 and its related work).
- For completeness, the paper should include proofs to show that the learned distribution is the same as the data distribution.  This follows directly from interpreting Eq. 2 as a conditional flow matching objective (Lipman et al., 2023).
- In the initial presentation of the vector field $a(x_t,t)$ at the start of section $4.1$, it would make more sense to drop the dependence on $t$ to emphasize that the ground truth acceleration field is constant in time.
- The scope of the empirical evaluation is limited.  The authors should have evaluated on more datasets than just CIFAR-10.  There are other small scale image datasets, like ImageNet-32, that could have been evaluated.

Limitations:
The authors adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
AF32GbuupC;"REVIEW 
Summary:
The paper introduces an innovative approach to improve the generalization capabilities of Graph Neural Networks (GNNs) in few-shot node classification tasks. The authors propose a novel algorithm, Fast Graph Sharpness-Aware Minimization (FGSAM), which incorporates sharpness-aware minimization (SAM) techniques into GNN training but reduces the typical computational overhead by integrating multilayer perceptrons (MLPs) for efficiency. This method not only demonstrates superior performance on few-shot learning tasks compared to traditional techniques but also offers significant computational advantages, particularly in reducing training times.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
originality: medium
quality: medium
clarity: medium
significance: medium

Weaknesses:
Some experiments are missing and the proof need to be double checked carefully.

Limitations:
the authors adequately addressed the limitations

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper focuses on efficient graph neural network (GNN) training in few-shot node classification (FSNC) problem by extending sharpness-aware minimization (SAM) for reducing the computational cost and improving the generalization of GNNs on unseen classes. The training phase is accelerated by perturbing the parameters of GNN and then minimizing the perturbation loss of GNN without the message passing mechanism (MLP). Experiments have been conducted to verify the effectiveness and efficiency of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. It is interesting to incorporate SAM technique with GNN by removing MP during training and reintroducing MP in inference, which is reasonable to improve the generalization of FSNC.
2. The proposed method could be conveniently integrated into existing GNN-based few-shot node classification models, which is verified in the experiments.
3. The carefully designed experiments highlight the effectiveness of the proposed method in reducing the computational costs of GNN training.
4. The paper is well-organized and explains the method very clearly. In addition, the landscape visualization and toy case analyses also make the motivation and ideas easy to understand.

Weaknesses:
1. There are related work on applying SAM to few-shot tasks (Sharp-MAML) [1] and performing SAM every $k$ steps [2], and the innovation of this work is not significant compared with the above work.

[1] Sharp-maml: Sharpness-aware model-agnostic meta learning. ICML, 2022

[2] Towards efficient and scalable sharpness-aware minimization. CVPR, 2022

2. The experiments only compare with small-sample node classification models and variants of SAM from 2022, lacking comparisons with the latest baseline methods of FSNC.

Limitations:
The authors describe some limitations of the proposed method in time consumption section.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a method for few-shot learning on graphs leveraging sharpness-aware minimization (SAM) from the vision community. The paper explains SAM as a technique for gradient perturbation during training to push the parameters to ""flatter"" regions of the loss space in hopes of achieving better generalization in the few shot setting. However, SAM is inherintely slower, requiring two backward/forward passes during training; one to compute the loss gradient and a second to determine the perturbation direction. The authors propose multiple ideas to overcome the added computation and to leverage graph topology. The authors strategically substitute Graph Neural Networks (GNNs) with Multilayer Perceptions (MLPs) to avoid the burden of message passing in some parts of their algorithm while preserving graph topologies in others. The authors propose a second scheme that leverages iterations of an approximate perturbation scheme with periodic exact evaluations of SAM. The authors present ablation studies on multiple SOTA models and 9 datasets and show improvements in training times and accuracy.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors present multiple novel approaches to overcome computational challenges applying SAM techniques to structured data to achieve better generality in the few shot case. I only have a few style suggestions at the beginning of the paper but otherwise the paper does a good job of explaining multiple, complex ideas in a clear way. The paper does present a significant result, blending ideas from PeerML and SAM from the vision community to contribute to making structured models more generalizable.

Weaknesses:
I think the paper is very good and clear in most cases. The only weaknesses I would note (and this may be subjective) would be to make some style improvements in the beginning of the paper.

(1) In the abstract (line 14) Moreover, our method ingeniously reutilizes... is a bit grandiose; I would prefer a more scientific tone and delete ""Moreover"" and ""ingeniously""; ""Our method reutilizies...

(2) Remove ""for the first time"" in 20. It is clear that the ideas are novel.

(3) Line 50 is the first time Message Passing is abbreviated as MP. I would follow the convention used in the paper for the introduction of other abbreviations where the first letter of the abbreviated character is in bold and capitalized.

(4) Line 93 ""as follows"" is not followed by an equation but another sentence. Maybe the equation should be presented after the ""as follows"" text and the next sentence come after the equation?

Limitations:
No limitations.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
EdG59dnOzN;"REVIEW 
Summary:
This paper presents a novel algorithm, KATE, which demonstrates impressive scale-invariance properties for Generalized Linear Models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This paper presents a novel algorithm, KATE, which demonstrates impressive scale-invariance properties for Generalized Linear Models. The thorough theoretical analysis provided in the paper, along with the experimental results, showcases the effectiveness of KATE in various machine learning tasks. The comparison with existing algorithms like AdaGrad and Adam highlights the superior performance of KATE in terms of convergence rate and efficiency.

Weaknesses:
While the paper excels in presenting the theoretical foundations and empirical results of the KATE algorithm, there are a few areas that could be further strengthened. Firstly, the paper could benefit from a more detailed discussion on the limitations of the proposed algorithm, especially in scenarios where certain assumptions may not hold. Additionally, providing insights into the computational efficiency and scalability of KATE with larger datasets could enhance the practical applicability of the algorithm.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work proposes an optimizer that achieves the optimal convergence guarantee for smooth nonconvex settings and more importantly the scale-invariance property.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper is very cleanly written. It was very easy to follow.
The main results look sound, and the experimental results are well presented as well.

Weaknesses:
Overall, the paper presents the main scope and the results very well.
In other words, I do not have much concern for this paper by itself. 

My only concern is the importance of the problem it tackles. Scale-invariance is definitely a desirable property, but I'm not sure what advances it brings about for the ML optimization. What I mean is, in general, the important question in the community is whether one can design an optimizer that has a noticeable advantage over the previous popular ones. I'm not sure whether resolving scale-invariance will drastically improve our current technology for optimizing ML models. The experiments presented in this paper doesn't seem to justify this in a compelling way. (I think results are convex models have limited practical impact.)

Limitations:
As I said, the reason for the current score is mainly due to the main scope of this paper.
In my opinion, unless the authors have compelling experimental results or arguments, tacking the scale-invarance seems to have limited practical impact in the community.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel optimization algorithm which demonstrate scale-invariance property for generlized linear models unlike Adagrad. The authors analyzed KATE for smooth and non-convex functions and on generalized linear models to obtain the same convergence upper bounds (asymptotically) as Adagrad and Adam. However the scale invariant algorithm implies, the speed of the algorithm would be the same as for the best scaling of data. The authors also empirically verify this via numerical experiments on logistic regression and deep learning tasks.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1) The authors come up with KATE which is scale invariant, and yet admit a convergence upper bound of $\mathcal{O}(\log(T)/\sqrt{T})$, which is an interesting algorithmic finding.
2) The paper is easy to read and understand. 
3) Comprehensive appendix.

Weaknesses:
I think authors need to emphasize the first point in Strengths by comparing with the diagonal online-newton method (diag-SONew) [1] which is also scale-invariant algorithm and what part of the analysis can fail for diag-SONew. I wrote the algorithm without the EMA here, which is essentially adagrad without the square root but with varying $\eta_t$:

$$
w_t := w_{t-1} -\eta_t g_t /({\sum_{s=1}^t g_s* g_s})
$$

 schedules one can try is $\eta_t=1$ or $\eta_t=\sqrt{t}$. I think comparing with the latter schedule will give a good understanding of the novelty behind KATE, as the schedule simulates the square root in Adagrad while being scale-invariant. Similarly empirical comparisons (if possible) with this simple algorithm in logistic-regression can help understand which algorithm is better.

2) Comparison with Adam doesn’t make sense in neural networks as KATE lacks momentum and EMA in second-moments (which are key features of Adam). Devising KATE with these features similar to [2] would help the empirical performance in neural networks.

[1] Devvrit, Fnu, Sai Surya Duvvuri, Rohan Anil, Vineet Gupta, Cho-Jui Hsieh, and Inderjit Dhillon. ""A computationally efficient sparsified online newton method."" Advances in Neural Information Processing Systems 36 (2024).
[2] Defazio, A., & Mishchenko, K. (2023, July). Learning-rate-free learning by d-adaptation. In International Conference on Machine Learning (pp. 7449-7479). PMLR.

Limitations:
Authors have adequately addressed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a scale-invariant variant of AdaGrad, called KATE, particularly for generalized linear models. Theoretically, the authors proved a convergence rate of $\mathcal{O}(\log T/\sqrt{T})$ for KATE, matching the best known rates for AdaGrad and Adam. Numerical experiments are used to illustrate KATE on several machine learning tasks, which outperforms AdaGrad consistently and matches/outperforms Adam.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This work studies a crucial problem of developing a scale-invariant variant of AdaGrad which is particularly useful whenever data exhibit poor scaling or ill-conditioning. Convergence rates of KATE similar to those of AdaGrad and Adam under both deterministic and stochastic settings are established, with comprehensive numerical experiments to justify the efficacy of KATE compared to AdaGrad and Adam.

Weaknesses:
As the motivation of KATE is to develop a scale-invariant optimizer, the experiments (even the one with simulated data) do not seem to have demonstrated this.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
RA6rzOJ2zI;"REVIEW 
Summary:
**[Edited: My overall score has increased from 6 to 7 (Accept) after the authors' rebuttal.]**

Dynamic Sparse Training (DST) holds the promise of more efficient training and model robustness, but remains largely impractical due to the lack of inherent structure and severe amplification of training steps needed to converge to high quality.  The authors apply DST to extreme multi-label classification (XMC) problems and build on past work to avoid losing important gradient signals and achieve a practical speedup.  They use a structured fixed fan-in sparsity at the final, extremely large, classifier layer to provide benefit in practice, and an auxiliary classifier to enhance gradient signals to layer before the semi-structured sparse classifier.  This classifier is designed to provide useful signals early in training, but not become so important that it cannot be removed after that.  The proposed modifications to DST in XMC are evaluated across a variety of data sets and against a seemingly comprehensive set of baseline techniques, and the results suggest that the method reduces memory consumption compared to the most baselines and gives a model with comparable quality to the best-performing baseline.  The authors also show that the auxiliary objective not only helps with initial training, but also with high sparsity in the final classifier.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
**Originality**
There seem to be two novel aspects: applying a past semi-structured fixed fan-in sparsity to the final classifier, and moving the low-rank intermediate layer to become an auxiliary classifier, solving a standard label shortlisting task, that can be removed by gradually decaying its importance over time.  The authors also provide new insight into the importance of the commonly-used meta-classifier and demonstrate their approach is effectively robust to label imbalances.

**Quality**
I believe the claims have been supported by experimental results in all cases.  The ablation study in Figure 3 makes the benefit of the auxiliary classifier clear.  The authors also provided a sweep of rewiring intervals to support their experimental settings, and the baselines seem fair to me; kudos to the authors for thinking of including a ""smaller, dense"" version of the fixed fan-in sparse classifier with the same number of parameters.

**Clarity**
In general, I found the paper easy to read with a fine layout, and I appreciate the effort the authors spent in explaining the XMC problem for readers, like me, who are not familiar with its unique aspects.  

**Significance**
Though I'm not familiar with XMC, if it is as widespread as some other tasks, successfully training a sparse model from scratch will be a huge advancement.

Weaknesses:
**Originality**
It really seems like [32] used the same type of semi-structured sparsity on their classifier.  What is the difference?  Section 2.2 makes it appear as though this were your contribution.

**Quality**
The authors claim, in line 142, that 2:4 structured sparsity results in deteriorated model accuracy, but there was no reference (other than the 2:4 whitepaper showing only high model qualities), and I couldn't find evidence in the submission that 2:4 sparsity behaved poorly for XMC tasks.

**Clarity**
Though the authors provided context for XMC, it still took me a couple reads to really understand the task, but once I did, the difficulty of the problem the authors set out to solve was clear.

I'm confused about the authors' intent to claim novelty for applying semi-structured fixed fan-in sparsity to the final classifier.

**Significance**
This is my first time hearing about the XMC problem, but I've heard of many other tasks and problems that I do not work on directly.  This makes me think that perhaps the applicability of the work may be limited, but I'll be happy to be wrong.

The improvement over past work is generally only in memory savings; there are often past techniques that give a higher quality model (though with higher memory requirements).  It'd be more compelling if the method matched or exceeded the accuracy of past work with a smaller memory requirement.

It's hard to judge the benefit of one of your primary contributions, the python bindings for CUDA kernels, without seeing them.

Limitations:
Yes, the authors have discussed the limitations and potential societal impact of their work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the application of Dynamic Sparse Training (DST) methods to the domain of extreme multi-label classification (XMC), where the label space can be very large, on the order of millions. The authors propose several enhancements to standard DST approaches to address the challenges posed by the highly skewed label distributions and scarcity of training data typical in XMC datasets. These include using semi-structured sparsity with fixed fan-in connections, adding an intermediate layer between the encoder and sparse classifier, and incorporating an auxiliary training objective to improve gradient flow, especially in the early phases of training. Empirical results on various large-scale datasets demonstrate that the proposed approach can significantly reduce GPU memory requirements while maintaining competitive performance compared to dense models and specialized XMC methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well-organized and clearly written, providing a comprehensive overview of the challenges in XMC and the proposed modifications to DST to address them. The technical contributions are well-motivated and supported by empirical results.
- The paper tackles an important problem of scaling DST to the domain of XMC, characterized by enormous label spaces, label imbalance, and label noise. It demonstrates DST's applicability beyond the typical benchmark datasets used in sparsity research.
- The authors present a well-motivated set of modifications to standard DST algorithms to handle XMC-specific issues. The semi-structured sparsity, intermediate layer, and auxiliary loss are supported by empirical analysis showing their impact on performance.
- The results show substantial memory savings on large-scale datasets with minimal loss in predictive performance. This enables end-to-end training of XMC models on commodity hardware.

Weaknesses:
- Limited novelty in core techniques: The main components (DST, semi-structured sparsity, auxiliary objectives) are existing methods, though their combination and application to XMC is novel. More discussion and intuition on how these components interact and complement each other in an overview would be beneficial.
- The paper is primarily empirical and lacks rigorous theoretical justification for why the proposed modifications work. Some insights into the underlying mechanisms that make the proposed approach effective would be valuable.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes the application of DST(Dynamic Sparse Training) to XMC(Extreme Multi-label classification) by employing an intermediate layer and adding an auxiliary training objective to enable end-to-end training of XMC problems with millions of labels on commodity hardware.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well-written and easy-to-follow.
2. The paper solves an important problem of democratizing access to the state-of-the-art XMC model training.
3. The paper provides a thorough analysis of challenges and solutions in applying DST to XMC.
4. The conclusion is supported by experimental results on 4+1 datasets.

Weaknesses:
1. The paper addresses the memory efficiency, however the training time still remains a concern, specifically for time-sensitive real world application datasets. 
2. The paper does not present results on datasets with label features except one(LFAT-131K). Many modular techniques such as NGAME, DEXA etc. perform much better on these datasets and are memory efficient by using negative sampling and optimizers such as SparseAdam.
3. The introduction of new hyperparameters, such as sparsity levels, intermediate layer sizes, and auxiliary objectives, adds to the complexity of the model. The sensitivity of the model's performance to these hyperparameters is not thoroughly explored.

Limitations:
1. The authors have discussed some limitations.
2. It would be good to comment on the scalability of the proposed approach given that the real-world datasets have much more than 3M labels.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
zDaD8zv8tG;"REVIEW 
Summary:
The paper introduces a novel teacher-teacher framework named LIghtweight kNowledge alignmEnt (LINE), which facilitates knowledge exchange between two pre-existing large language models (LLMs) to enhance clinical language representation. By leveraging complementary knowledge from general-purpose and domain-specific models, LINE aims to harmonize their knowledge within a unified representation space. The framework is validated through downstream tasks showing that the LINE model outperforms individual pre-existing models in understanding and processing clinical language. This approach allows for more efficient sharing of clinical pretrianed models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. **Clarity and Structure**: The paper is well-written and structured, offering a clear motivation for the study. This makes it accessible and engaging for readers, facilitating a deeper understanding of the proposed framework.

2. **Novelty and Utility**: The proposed teacher-teacher framework, LIghtweight kNowledge alignmEnt (LINE), is innovative, providing a pragmatic approach to integrating the strengths of different pre-trained models. This methodology is particularly notable for its potential to enhance clinical language representations without the need for developing new models from scratch.

3. **Usability and Efficiency**: The framework is user-friendly and does not require retraining of the original models, which significantly reduces computational overhead and simplifies its adoption in real-world applications.

4. **Empirical Validation**: The experimental results demonstrate stable and significant improvements over existing methods, substantiating the efficacy and value of the proposed framework in practical settings.

Weaknesses:
**Data Requirements and Availability**: A notable limitation of the proposed LINE framework is its dependency on well-aligned and specific types of data sources, which may not be readily available or commonly found in practical settings. For example, integrating data from disparate modalities like CT and MRI requires the availability of cases that include both types of data, which may not always be feasible. This requirement could limit the framework's applicability across different clinical or real-world scenarios where such aligned data sets are scarce.

Limitations:
No.
The authors discuss the potential improvement instead of the limitation of the current work. Bring more information and try other situations cannot be counted as an adequate discussion of limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents an interesting topic on LLM but the importance of this problem is not convincing and the methods here is not novel.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The teacher-teacher concept is novel to some extent.

Weaknesses:
1. The problem's importance is not significant.
2. There lacks the inclusion of SOTA models like llama, gpt, etc.
3. The results improvement is limited as shown in Tab. 4,5.
4. The Fig1 lacks details of the proposed method.

Limitations:
See details in weakness.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors look to address the question representational alignment between language models trained on different textual domains to improve performance of potentially both models on their out-of-domain text. The authors propose to specifically investigate this in the context of EHR text, and choose as their models for this CODER and BGE. They propose a contrastive loss, and additionally propose to train an alignment module/project layer rather than end-to-end training of the teacher models.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The concept is solid and well implemented and motivated. I wonder if it would be possible to further generalize it beyond medical text - which it is restricted too due to the reliance on alignment with extracted medical concepts by NILE. The discussion mentions this possibility, but it would be exciting to see it in action.

The clinical NLP benchmarks are particularly appropriate for the task.

Weaknesses:
Some of the benchmark tasks are older, and the comparisons could be more robust. Some ablations are missing.

The project's scope is incredibly narrow: encoder models on extractive medical tasks. While the authors claim that the technique is broadly generalizable, it would be nice to see proof-of-concept. 

The work seems to me to fit more into the realm of domain adaptation rather than learning by alignment. We aren't learning novel models here via alignment (like CLIP), but rather, pushing the learned representations of two different models into a common space. I'd strongly consider citing and discussing DA literature for this paper.

Limitations:
No discussion of limitations. Without additional experiments at the minimum a stated limitation should be the highly restricted domain of application (purely encoder models on medical topics).

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduce a teacher-teacher framework for clinical language representation learning. The framework uses a lightweight knowledge alignment module to harmonize the knowledge of both models within a unified space, which including two steps: The first step involves initial training to define residuals and capture complementary information. The second step focuses on refining the alignment by recovering residual information. The framework was validated using the MIMIC-IV database, where the LINE model outperformed baseline models in aligning concept and text representations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The main contribution of the work is proposed teacher-teacher framework, and training strategy.

- Originality: The teacher-teacher framework is very interesting as it enables mutual enhancement between two pre-existing LLMs, a unique departure from traditional approaches that typically involve training a new model or continual pre-training of existing models. This innovative method opens new avenues for leveraging existing resources to achieve superior performance.

- Quality: The paper demonstrates high quality through its validation using the MIMIC-IV database, a well-known and respected dataset in the clinical domain, adding significant credibility. Additionally, the LINE model's performance is compared against several strong baseline models, showing clear improvements across various downstream tasks, thus underscoring the robustness and reliability of the proposed framework.

- Clarity: The paper is well-written and clearly structured, making it accessible to both domain experts and those new to the field. The introduction provides a comprehensive background and motivation for the proposed framework, while the methodology section offers detailed descriptions of the teacher models and the LINE module.

- Significance: The practical applications and potential impact on the clinical domain shown the significance of this work. The teacher-teacher idea has substantial implications for more advancing NLP applications in other filed.

Weaknesses:
1. Figure 1 is somewhat confusing. From my understanding, Teacher 1 should be a strong LLM, while Teacher 2 should be an LLM with existing domain-specific knowledge. However, Figure 1 gives the impression that Teacher 2 serves merely as a database, making the framework resemble a RAG framework.

2. Although the paper compares the LINE model against several strong baseline models, it lacks a detailed comparison with the latest strong general LLMs, such as GPT-4, which should be considered a strong baseline. Consider adding a small comparative analysis or stating the advantages of the framework over simply using GPT-4.

3. The paper underscore the practical value of the framework, but it does not sufficiently address potential practical implementation challenges, such as computational requirements and scalability when applied in real-world clinical settings.

Limitations:
The paper has limited discussion on the broader implications of implementing the teacher-teacher framework in clinical settings. Consider to add the assessment of how the framework could impact patient care, data security, and trust in AI systems in healthcare.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a mutual learning framework, called LINE, between two pre-existing LLMs in the healthcare domains. By harmonizing the knowledge of two distinct LLMs into a unified representation space, the model achieves better performance on intrinsic and extrinsic downstream evaluations of clinical tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Clear motivation. Overall well written.

The methodology was reasonably designed to map representations from two distinct LLMs into a unified representation space.

The method achieves better performance on downstream clinical tasks.

Weaknesses:
1. Only two LLMs (BGE and CODER) were aligned by LINE. It is unclear if LINE will work on combinations of other LLMs. 

2. LINE make downstream predictions based on clinical concepts only, rather than the full context. The concepts themselves can be negated, historical and hypothetical in context, but the proposed method does not seem to consider this.

Limitations:
See weakness

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
Jf40H5pRW0;"REVIEW 
Summary:
The paper critiques the practices of privately adapting closed(-source) LLMs to private data by demonstrating that these techniques are potentially unsafe and do not yield the required quality of resulting model in terms of accuracy. The authors conclude that a focus on open LLMs should be preferable in sensitive fields.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I agree with both the motivations and the premise of this paper. It is important to think about adapting LLMs to private data holistically, i.e. on a systems level, and not just on the level of individual techniques. I like that the authors investigate a multitude of techniques, contrast open and closed LLMs, and discuss the costs of each method. The paper is also written clearly and includes a good overview of prior works. I also approve of the detailed exposition on hyperparameters, costs, etc. in the appendix.

Weaknesses:
I would have liked to see a more detailed investigation on the effect of privacy levels beyond $\varepsilon=8$ (for most techniques excluding PATE, where there is a note on different privacy levels and the performance plateauing). Perhaps I missed this, but I didn't find a concrete justification for $\varepsilon=8$ either, although I recognise this as a ""folklore/default"" privacy budget which many of us have come to think of ""automatically"" when working with DP-SGD due to the multitude of works which use it. 
Moreover, it would have been nice to see more dedicated instruction fine-tuning tasks, since this arguably has become one of the most important tasks for contemporary LLMs.

 On a more conceptual level: There is no ""methodological or theoretical novelty"" in this work in the traditional sense: It is a (thorough and thoughtful) comparison of techniques. I don't personally view this as a big weakness, to the contrary, I disapprove of the, usually fairly arbitrary, notions of ""novelty"" used to argue against papers, and I would like to see this paper published in some form, as I believe it discusses important points. However, I am unsure whether it would not have been a better fit to the benchmark track, rather than the main conference track.

Limitations:
The authors discuss some limitations, such as cost and performance, which I appreciate. I would be interested (see Questions above) in a discussion on the interplay between privacy and other aspects of trustworthiness, especially safety and alignment.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper compares the performance and privacy leakage between private adaptations on closed and open-source LLMs. The authors conclude that adaptations on open-source LLMs result in better performance, lower training costs, and enhanced privacy protection

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The paper presents extensive experiments on adaptations of both closed and open-source LLMs, including four private In-Context Learning (ICL) methods for four closed LLMs and three private tuning methods on four open models.

Weaknesses:
1. The work lacks novelty in its methods. All the private adaptation methods used are pre-existing, with the authors only extending two methods from classification tasks to generation tasks. This makes the paper more of a benchmark work rather than original research. Additionally, as noted in section 4.2, ""Previous work [31] has shown for non-private settings that gradient-based tuning methods (used for open LLMs) offer better accuracy and significantly lower computational costs than ICL (used for closed LLMs) since the adaptations can leverage the internal behavior of the LLM."" Thus, it is intuitive that the performance would be similar under DP scenarios, making the conclusion to use open LLMs less impactful.

2. It seems unnecessary to use private tuning methods on open LLMs since the trained model will not be shared and only queried. As the authors state, ""Looking at Figure 1, it becomes obvious that any private tuning method executed on that open LLM would, conceptually, improve privacy protection since the LLM provider would neither be involved in the adaptation nor in the use of the adapted LLM, yielding absolute privacy against them."" If a company locally fine-tunes the model and then preserves it locally for queries, direct fine-tuning would be more efficient.

3. The comparison in the experiments is unfair and lacks proper baselines. The experiments compare task performance between various adaptations on closed and open LLMs. However, the closed and open models are different and show varying performance even without fine-tuning. The zero-shot performance of these models is not provided, leaving no baseline for reference. Additionally, DP-based ICL methods and fine-tuning methods offer different levels of privacy protection. As the authors mention, ""Yet, the threat model of multiple private ICL methods for closed LLMs does not include providing privacy against the LLM provider."" It is not fair to compare their performance with the same parameter $\epsilon$ as shown in Figure 2, which could mislead readers into thinking they offer the same level of privacy protection.

Limitations:
Please refer to the weaknesses and questions sections.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper compares private adaptation between closed LLMs and open LLMs, and the authors find that adapted open LLMs always perform better than closed ones at much lower cost.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper presents a comprehensive overview of privacy-preserving adaptation techniques for LLMs. It thoroughly examines various existing methods and compares them across key factors like performance and cost.
2. The results are striking: open-source LLMs consistently outperform their counterparts in virtually every aspect.
3. The paper also discusses the private data flowing and point out that the private data might still be leaked to LLM providers in 3 out of 4 adaptation methods studied.

Weaknesses:
My main concern is that the claim that open LLMs are essential for achieving high-quality, privacy-preserving adaptation might be premature. It could be that the field is still new, and we simply haven't yet developed sufficiently powerful algorithms for privacy-preserving adaptation of closed models. I suggest tuning down this claim slightly, acknowledging that the field is relatively new and rapidly evolving, and that the current findings may only be applicable to the privacy algorithms currently available.

Limitations:
As above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper compares and contrasts the privacy protections of open vs closed LLMs through conceptual threat models and experimental evaluation. The authors give experimental evidence on local gradient based adaptations performing better than their closed discrete prompt-based counterparts in the areas of privacy, cost-efficiency and performance. The paper concludes that the use of open LLMs not only yields more privacy, but also higher performance at lower costs.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well-written and clearly explains what the contributions are, the background for both DP and LLMs, as well as their experimental evidence. The authors make a compelling case behind the reasons for privacy-preserving adaptations for closed LLMs not being as effective. The problem addressed in the paper is important and the contribution seems novel.

Weaknesses:
I think the paper is well-written and don't have any specific feedback.

Limitations:
Limitations were addressed

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
Cjnirz5pan;"REVIEW 
Summary:
This paper proposes a slow and fast parameter-efficient tuning method for continual learning. Slow learner is learned on the first session and fixed with a transfer loss. Fast learner is continually updating for new tasks. Slow learner and Fast learner are further restricted to avoid forgetting.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The idea of slow and fast learning has been interesting for continual learning, such as in SLCA. This work leverages pre-trained models to design the slow and fast learner, which is novel. And the experimental results compared to other baselines are significant.

Weaknesses:
Although the main idea is sound, it is not well supported by the experimental results. For instance, In Table 2, it shows that with only Fast learner the performance is already as good as the proposed method SAFE. In Table 3, the gain of the aggregation method is marginal compared to some straightforward baselines.  In Table 5, without using $L_{f2s}$ and $L_{s2f}$, the Fast learner obtains the similar results. All these components are claimed as main contributions.

Limitations:
Yes, it is addressed in the appendix.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a novel method named SAFE (Slow And Fast parameter-Efficient tuning) to tackle challenges in continual learning. SAFE introduces a unified framework that combines slow parameter-efficient tuning (S-PET) for inheriting general knowledge from pre-trained models (PTMs) and fast parameter-efficient tuning (F-PET) for acquiring task-specific knowledge in incremental sessions. SAFE demonstrates state-of-the-art performance on six benchmark datasets, showing significant improvements over existing methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. SAFE introduces an innovative approach by integrating both slow and fast parameter-efficient tuning within a unified framework. This dual-tuning mechanism effectively addresses the trade-off between stability and plasticity in continual learning.
2. The paper presents a thorough and well-structured evaluation on six benchmark datasets, clearly demonstrating the effectiveness and superiority of SAFE over existing methods. 
3. The approach does not rely on storage class distributions for data replay and maintains constant computational and memory overheads.

Weaknesses:
1. Training the slow adapter only in the initial session may restrict its ability to adapt to new tasks, potentially limiting the overall flexibility of the model.
2. Complex Loss Functions and Hyperparameters: The proposed method involves multiple loss functions and hyperparameters, making the optimization process complex and potentially challenging to tune in practice.
3. While the method shows promising results, additional experiments or ablations specifically addressing the impact of each loss function and hyperparameter choice would strengthen the validation of the approach.

Limitations:
The authors acknowledge that SAFE's training process involves complex loss functions and hyperparameters, which could be a barrier to practical implementation.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a novel paradigm for continual learning called SAFE, which utilizes both slow and fast parameter updates. The method focuses on continual learning with pre-trained models, using slow updates to preserve the generalization capability of the pre-trained model while employing fast updates to adapt to new downstream tasks. During inference, the two branches are aggregated to achieve more robust predictions. Experiments on 6 different benchmarks demonstrate that SAFE consistently outperforms existing methods, achieving state-of-the-art performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper focuses on continual learning with pre-trained models, a highly valuable topic. By designing a slow learner, it retains the generalization capability of the pre-trained model.
2. The paper also designs a fast learner to balance stability and plasticity, enabling the model to continually adapt to new downstream tasks.
3. The paper conducts extensive experiments on multiple benchmarks, providing strong evidence for the effectiveness of the proposed method.

Weaknesses:
1. The slow learner assumes that the data distribution of the first task in the continual learning scenario is roughly similar to subsequent tasks, which is not always true in practical applications. When the data distribution of the first task diverges significantly from subsequent tasks, constraining the fast learner to the slow learner may yield negative results.
2. The design of the method introduces too many loss function terms, which could make the model training overly complex. Additionally, the stability of these hyperparameters is not analyzed in the experimental section.
3. The main experimental section lacks comparisons with the latest methods, such as DAP[1].
4. The ablation studies show that the introduction of many components results in only marginal improvements. According to Occam's Razor, these components should be removed.

[1] Jung D, Han D, Bang J, et al. Generating instance-level prompts for rehearsal-free continual learning[C]. Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023: 11847-11857.

Limitations:
The method proposed in the paper is only applicable to scenarios where the data distribution of the first task is roughly similar to that of subsequent tasks.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces the SAFE (Slow And Fast parameter-Efficient tuning) framework for continual learning using pre-trained models (PTMs). The proposed approach combines slow parameter-efficient tuning (S-PET) to inherit general knowledge from PTMs and fast parameter-efficient tuning (F-PET) to adapt to new tasks in each incremental session. The paper validates the effectiveness of the SAFE framework through experiments on multiple datasets, demonstrating its superiority over state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is generally well-organized and easy to follow. 
- The method is well motivated and contains clear novelty comparing previous methods. 
- The paper provides extensive experimental validation on various benchmark datasets, showing improvements over state-of-the-art methods.

Weaknesses:
- The motivation and effects of the proposed techniques are unclear. 
    - For example, what are the specific effects of the cross-correlation matrix? What is the specific motivation? This design is quite similar to the de-correlation operation process in RanPAC, although with a different formulation. More solid analyses are required. 
    - The issue is still with the fast learner part. Although ablation studies are conducted for the components, the insights of the designs are unclear. For ablation studies, not only the terms should be ablated, but the designs, such as the usage of W_slow in the loss should be clearly discussed and analyzed. 
- Although the paper is generally well written, some parts have tedious descriptions and blurry presentations, such as line 58 - 81 in the introduction, which need to be improved. 
- The proposed method contains many hyper-parameters, such as the weights for the loss terms. Ablation studies for them are necessary and required. 
- The importance of aggregation for the performance should be discussed based on the ablation study. 
- The aggregation process requires weights for slow and fast results. Although they can be calculated automatically, it should be analyzed and demonstrated how the distribution of these weights for different data samples and datasets. 

The paper can be a good paper. However many details are absent in the papers, making the delivered message unclear and the techniques not justified well. I will reconsider my score based on the rebuttal.

Limitations:
The paper includes a limitation section in the appendix. But it does not cover actual limitations of the work. Instead, the discussed limitation is mainly the characteristics of general pre-trained model-based continual learning (i.e., using the pre-trained model), which is actually not a limitation. The authors may discuss the limitation related to the robustness of the hyper-parameters.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
dJUb9XRoZI;"REVIEW 
Summary:
This paper addresses this limitation by rethinking diffusion without training loss guidance from an optimization perspective. They formulate a series of constrained optimizations throughout the inference process of the diffusion model. In each optimization, they allow the sample to take multiple steps along the gradient of the surrogate constraint function. The termination conditions come from two aspects: one is the accuracy of the approximate surrogate, and the other is the estimation of the manifold.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The motivation for this paper seems reasonable to me, and they believe that the one-step guidance of DPS is suboptimal.

2. The proposed direction has improved the performance compared to the two baselines DPS and DSG.

Weaknesses:
1. First of all, I have doubts about the theory of this article. Equation 12 in the article seems to be written incorrectly. As far as I know, $\epsilon_\theta(\mathbf{x}',t)=\int(\mathbf{x}'-\sqrt{\alpha_t}\mathbf{x_0})/(\sqrt{1-\alpha_t})p(\mathbf{x}_0|\mathbf{x}')\mathrm{d}\mathbf{x}_0$, it should be conditional expectation here.

2. The motivation for this part is also not very clear. Why does the above integral correspond to a multivariate Gaussian (Line 156)?

3. The number of test samples used in the image experiment is a bit small. Why didn't the author use a data set similar to DPS?

4. In addition, the proposed method still seems to require more than 500 NFE, similar to DPS, and I am a little worried about its efficiency and the need for improvement.

5. DPS itself requires careful tuning of hyperparameters, such as the guidance rate. The proposed method introduces new hyperparameters $m·t+c$ and $\epsilon_max$, which raises scalability concerns.

Limitations:
The authors discuss the limitations of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a method to enhance training-free loss-guided diffusion sampling. The key contributions are:

1. Introduction of Trust Sampling: A novel method called Trust Sampling is proposed, which diverges from the traditional approach of alternating between diffusion steps and loss-guided gradient steps. Instead, it treats each timestep as an independent optimization problem, allowing multiple gradient steps on a proxy constraint function at each diffusion step.

2. Early Termination and State Manifold Estimation: The method includes a mechanism to estimate the state manifold of the diffusion model, enabling early termination if the sample starts to deviate significantly from the expected state. This ensures the proxy constraint function remains trustworthy.

3. Optimization Perspective: The paper reformulates training-free guided diffusion as a constrained optimization problem, providing more flexibility and robustness across various domains and tasks. This approach addresses the limitations of previous methods, such as sensitivity to initialization and performance degradation with fewer neural function evaluations.

4. Experimental Validation: The efficacy of Trust Sampling is demonstrated through extensive experiments in different domains, including image and 3D motion generation. The method shows significant improvements in generation quality and constraint satisfaction compared to existing techniques.

5. Generalization Across Tasks: The paper demonstrates the generality of Trust Sampling across various image tasks (e.g., super-resolution, inpainting, gaussian deblurring) and motion tasks (e.g., trajectory tracking, obstacle avoidance), highlighting its versatility and effectiveness.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper proposes Trust Sampling, a method that diverges from traditional guided diffusion approaches. Instead of alternating between diffusion and gradient steps, Trust Sampling treats each timestep as an independent optimization problem. This innovation provides a new perspective on training-free guided diffusion, addressing some of the key limitations of existing methods.
2. The quality of the paper is reflected in the thoroughness of its methodology and the robustness of its experimental validation.
3. The paper is well-written and clearly structured, making it accessible to both experts and those new to the field.
4. The significance of the paper lies in its potential to impact a wide range of applications in generative modeling.

Weaknesses:
1. The Trust Sampling algorithm, which takes multiple gradient steps of constraint guidance on each predicted $x_{t}$, is similar to the  corrector stage of PC sampling [1]. 
2. Training-free loss-guided diffusion sampling have been applied not only on inverse problem but also on variable tasks [2-6], such as refined text-to-image and layout-to-image. The authors need to further verify the effectiveness of Trust Sampling on these tasks. 

[1] Score-based Generative Modeling through Stochastic Differential Equations

[2] Counting Guidance for High Fidelity Text-to-Image Synthesis

[3] Fine-grained Text-to-Image Synthesis with Semantic Refinement

[4] Attend-and-Excite: Attention-Based Semantic Guidance for Text-to-Image Diffusion Models

[5] BoxDiff: Text-to-Image Synthesis with Training-Free Box-Constrained Diffusion

[6] LoRA-Composer: Leveraging Low-Rank Adaptation for Multi-Concept Customization in Training-Free Diffusion Models

Limitations:
The authors have adequately described the limitations and potential negative societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a trust sampling scheme which incorporates given constraints loss function as guidance for constrained generation. This approach conducts early stop while detecting a mismatch between the predicted noises magnitude of sample and the noisy level at the current state manifold at each diffusion step. In experiment section, this approached is applied to image generation and human motion tasks both across various applications.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This approach can deal with given constraints using the pre-trained unconditional diffusion models without more training.
2. Paper is easy to follow.

Weaknesses:
1. It seems that there is no guarantee that the final samples satisfy the constraints since the reverse process will go to the next diffusion noisy level from the early stop if the predicted noise is higher than the threshold while the loss is not 0 yet, which is also reflected in the human motion experiment.
2. I get the idea that early stop is determined by the norm of predicted noise from the section 3.2: if the norm is large, then $\mathbf{x_t}$ does not reside in its supposed manifold. But is the reverse direction valid: if the norm is small than some threshold, than $\mathbf{x}_t$ is on the manifold? Is this necessarily true?
3. To summarize above, I found the explanation from methodology section tends to give intuitions about why this algorithm would work instead of guarantees or theoretical analysis, such as a bound on the distance between the $\mathbf{x}_t$ before and after applying loss gradients multiple times, or an upper limit of the probability of samples being out of constraints, etc..

Limitations:
Based on the checklist guidelines, the authors claimed the positive and negative societal impact are discussed, but I only found the ``justification`` below the question in checklist is the only place they talked about the impact and I assume this should be mentioned somewhere in the main text or appendix. Limitation is also claimed being discussed in the section 6, and I assume that's mentioned in the second paragraph.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper tackles the task of sampling from diffusion models with additional inference-time constraints. In this setting, synthesis needs to simultaneously follow the diffusion model-defined generative prior as well as a constraint objective. The paper proposes two techniques to achieve this in a robust fashion. First, trust schedules define the number of optimization steps with respect to the constraint that are carried out between each diffusion model generative denoising step. Moreover, state manifold boundaries prevent the model from falling off the diffusion model-defined data manifold during optimization. These techniques enable more robust and stable constraint-guided sampling of diffusion models. The proposed methods are validated on constrained image modeling tasks (superresolution, deblurring, inpainting) as well as several human motion synthesis benchmarks, and favourable results compared to baselines are achieved.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
**Clarity:** The paper is overall written well, clear to read, and easy to follow.

**Quality:** Overall, the paper is of good quality. The paper is clear, has a detailed discussion of related work, as well as mostly appropriate experiments and ablation studies. I do not see any major technical flaws.

**Originality:** The specific proposed techniques are new and original, to the best of my knowledge. They do represent simple heuristics, though, and I have some concerns discussed below.

**Significance:** I think the paper tackles an important task, generation with inference-time constraints, and it achieves strong performance compared to the baselines. This makes the work generally significant.

Weaknesses:
I think there are two main weaknesses:

First, both the trust schedules and also the approach to estimate state manifold boundaries are merely well-motivated heuristics and they both require hyperparameter tuning (how many steps in the trust schedule, $\epsilon_{max}$). For the trust schedules, there is an elaborate derivation that relates approximation errors to state variances (Eq. (9)). But in the end, none of this is tractable and it just motivates the use of a step-dependent optimization schedule, which needs to be chosen fully heuristically. And this schedule also depends on the optimization step size, which needs to be chosen manually, too. The state manifold boundary constraint follows a similar $\epsilon_{max}$ heuristic. In the end, the paper proposes some useful and well-motivated optimization heuristics, but not an overly novel and rigorous framework for constrained diffusion model sampling. This makes the paper somewhat less original.

Second, as the authors pointed out in their related work section, there are many previous papers in the area. However, the authors only compare to DPS as well as DPS+DSG, although there should be further applicable baselines. Most importantly, the authors argue that LGD-MC has no code available. However, I believe reimplementing the LGD-MC method should be quite easy, and a comparison to this work would be very appropriate here. Moreover, maybe LGD-MC could even be combined with, or enhanced by the techniques proposed here. This possibility is not discussed.

**Conclusion:** In conclusion, even though I see some weaknesses and evaluation could be expanded, the proposed heuristics seem useful and the paper is of overall satisfactory quality. Hence, despite my concerns, I am carefully leaning towards suggesting acceptance.

Limitations:
The paper does not have a detailed discussion of its limitations or its societal impact. This did not significantly influence my paper rating, but I would strongly suggest the authors to add a more detailed discussion on both of these aspects to the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
kZpNDbZrzy;"REVIEW 
Summary:
This paper tackles the challenges of Offline RL, which involves learning effective decision-making policies from static datasets without online interactions. The authors introduce Generative Trajectory Augmentation (GTA), a novel approach that uses a diffusion model to enhance offline data by augmenting trajectories to be both high-rewarding and dynamically plausible. GTA partially noises original trajectories and denoises them with classifier-free guidance based on amplified return values. The experiment results indicate that GTA, as a general data augmentation strategy, enhances the performance of widely used offline RL algorithms in both dense and sparse reward settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1) Interesting and important topic: The guided data augmentation for offline RL is interesting. The success in this domain will further benefit the real-world applications of offline RL algorithms.

(2) Extensive experiments: the authors provide extensive experiments to validate their proposed method.

Weaknesses:
(1) Lack of theoretical guarantee to support the algorithm: The authors only provide empirical results, lacking theoretical analysis of the performance of the proposed method.

For other potential weaknesses, please see my questions.

Limitations:
Yes, the authors discussed the limitations in their paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To improve the quality of offline datasets, in this paper, a generative data augmentation approach is proposed by leveraging the diffusion models. Moreover, with the adoption of partial noising and denoising framework with amplified return guidance, the trajectory can be guided towards high-rewarding region. Finally, with the generated trajectories, existing offline RL methods can be utilized to learn the optimal policy.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is clearly organized and the experimental results show the effectiveness of the proposed approach. Also, the separation of the data generation with diffusion models and the policy learning mitigates the possible time and computation cost in the policy learning. Moreover, the trajectory-level data generation can capture the transition dynamics, which is beneficial for environments with sparse rewards.

Weaknesses:
The proposed approach is mainly a direct combination of current techniques, e.g., the diffusion model for data generation, the adding noise for the exploration and the existing offline RL techniques for policy learning, the theoretical contribution of the paper is trivial. Also, experimental results are still not sufficient, e.g., the lack of the comparations of time and computation costs between different methods, and only two data augmentation baselines are compared. Moreover, the proposed approach is limited to trajectories with reward signals, which is not the case in many real-world applications. Furthermore, it is unclear to me whether there are any theoretical guarantees to push the generated trajectory toward the high-rewarding region.

Limitations:
The authors have addressed some limitations, but for some other limitations, e.g., the computation and time cost of the proposed approach, the application of the proposed approach in some real-world applications with no reward signal provided should be further stated.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper builds on ideas from SynthER but adds classifier free guidance to boost the returns of the generated trajectories. This makes sense as the resulting data has higher quality and the paper is a totally sensible next step in the series of works building upon SynthER, which is an exciting direction for research. The paper is well executed and a solid contribution.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* A highly relevant area building on synthetic data generation for offline RL. Out of all the current active areas in RL research this one benefits the most from the current foundation model/large data regime, and scaling offline RL has been shown to be highly impactful in areas such as Robotics (E.g. RTX). 
* The method is not overly complicated and builds upon the recent SynthER paper, presented at NeurIPS 2023. This is an example of a simple idea that makes a great deal of sense, and it is well executed. 
* The paper reads well.
* The experiments go beyond what was done in SynthER, including some new environments. It is great to see this - as RL needs to keep pushing for more complex benchmarks and not just sticking to D4RL and Atari ""because thats what the previous paper did"".

Weaknesses:
There are no major weaknesses here, it is a solid paper taking a nice step in an exciting general direction of research. The below points are fairly minor:
* The authors could cite Ball et al 2021 ""Augmented World Models"" as another example of data augmentation in offline RL.
* The authors could discuss how this relates to another paper building on SynthER, Policy-Guided Diffusion by Jackson et al. This is very recent work so makes sense it is not needed as a baseline, but the comparison may be worth including in text.

Limitations:
Nice to see this in the main body rather than the Appendix. It would be good to add a discussion on scalability if possible.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces Generative Trajectory Augmentation (GTA), a data augmentation approach for Offline Reinforcement Learning (RL) that enhances the quality of static datasets by generating high-rewarding and dynamically plausible trajectories using a conditional diffusion model. GTA partially noises original trajectories and then denoises them with classifier-free guidance via conditioning on amplified return value. The authors demonstrate that GTA improves the performance of various offline RL algorithms across several benchmark tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- GTA integrates seamlessly with existing offline RL methods. It builds on previous advancements, and new offline RL algorithms could also benefit from it, given its agnostic nature towards the specific RL method used.
- The ability to generate high-return trajectories that do not exist in the logged data is a significant advantage. This feature potentially improves the performance of offline RL algorithms by enriching the dataset with valuable transitions, as supported by the experimental results.
- The experimental results presented in the paper show significant improvements across various offline RL algorithms and environments. Additionally, thorough ablations are provided, highlighting the impact of different components and hyperparameters on the overall performance. This comprehensive evaluation demonstrates the practical effectiveness of GTA.

Weaknesses:
- The claim that GTA-generated data adheres to the dynamics of the environment (lines 145-148) seems unfounded. Is there a principled argument on why diffusion models would learn the dynamics of the environment well? Especially where the goal is to create transitions outside the dataset. Figure 5 does not conclusively show that GTA is dynamically plausible, especially for tasks like Cheetah compared to methods like S4RL.
- Compared to S4RL, the augmented trajectories appear less plausible. This issue is evident in tasks like HalfCheetah, yet GTA still achieves better rewards. It is unclear why this does not affect the final results.
- Based on Appendix B.2, the performance of GTA appears to depend heavily on finely-tuned hyperparameters, as suggested by the different values used for different environments in Table 6. This raises concerns about the generalizability of the method, indicating that it might rely on online manual tuning.

Limitations:
The authors discuss the limitations and potential impacts in their paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents Generative Trajectory Augmentation (GTA) that is aimed at improving offline reinforcement learning (RL). GTA uses a diffusion model conditioned on high returns to generate high-rewarding trajectories. These trajectories are used to augment the static datasets used to train offline RL algorithms. The augmentation process involves partially adding noise to existing trajectories and then refining them using a denoising mechanism guided by an amplified return value. The authors demonstrate that GTA improves the performance of popular offline RL algorithms across tasks in the D4RL benchmark. They also analyze the quality of the augmented trajectories, showing improvements in both data optimality and novelty over two baselines (S4RL and SynthER).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
**Originality:** The use of a conditional diffusion model for trajectory augmentation is a novel approach that adds computation overhead on the data creation stage instead of the policy learning stage.

**Quality:** The paper includes experiments across tasks from the D4RL benchmark, demonstrating the effectiveness of GTA in various settings, including dense and sparse reward tasks, high-dimensional robotics tasks, and a pixel-based observation task.

**Clarity:** The paper is well-organized and presents the methodology, experiments, and results clearly and logically. The authors provide an anonymized link to their code enhancing the reproducibility and transparency of the proposed method.

**Significance:** GTA is shown to be compatible with offline RL algorithms, making it a flexible solution for data augmentation in offline RL. By adding novel high-rewarding trajectories to the datasets in the D4RL benchmark, GTA shows improvement in the performance of offline RL algorithms.

Weaknesses:
1. As shown in Figures 4 (a)(b) and Table 25, the performance of GTA is sensitive to the choice of hyperparameters, such as the noising ratio ($\mu$) and the multiplier for conditioned return ($\alpha$) which might require extensive tuning for different tasks.
2. While empirical results show that the generated trajectories using GTA are dynamically plausible (Figure 5), it is not explicitly enforced in GTA and might be a byproduct of generating high-rewarding trajectories. The reviewer believes that further evaluation of GTA on real-world data is needed to make claims regarding GTA's dynamic plausibility such as in lines 6-9 (“In response, we introduce … and dynamically plausible”), lines 39-40 (“GTA is designed … dynamic plausibility”).
3. Related to weakness #2, while the paper shows GTA's effectiveness on standard benchmarks, it lacks validation in real-world applications where the dynamics and data distributions may differ significantly from simulated environments. This poses a question on the real-world applicability of GTA.
4. The paper does not include any experiments comparing GTA with model-based RL baselines. Although the authors state that both approaches perform data augmentation at different stages (data generation vs policy learning), the reviewer believes that a comparative study should be included. This is because both approaches learn a separate model to generate augmented data.

Minor:
- Line 48: “... any offline RL algorithms …” should be “... any offline RL algorithm …”.
- A pink curve is present in Figure 4 (b), but its legend is missing. The blue curve denoting $\mu$ = 1.0) is missing in this figure,

Limitations:
1. As the authors have mentioned, while empirical results on the D4RL benchmark tasks show that GTA has low dynamic MSE, in tasks where dynamic violations have a critical impact, the performance boost may not be significant.
2. The current evaluation is limited to simulated tasks, and the effectiveness of GTA in real-world offline RL tasks remains to be validated.
3. As shown in Figures 4 (a)(b) and Table 25, the performance of GTA is sensitive to the hyperparameters ($\mu$ and $\alpha$) which might limit its applicability.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
KT6F5Sw0eg;"REVIEW 
Summary:
The paper analyzes the block drafts generated by multiple independent prediction heads of blockwise parallel language models and observes three key features: consecutive repetitions, confidence of different heads, and efficiency gap with oracle top-k block. To address these issues, the paper proposes two algorithms to leverage the top-k predictions at each head: local rescoring via (small) neural LMs and global rescoring via n-gram LMs with multi-drafts. Experimental results show that the proposed two algorithms can improve block efficiency.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper identifies a weakness in the existing blockwise parallel decoding algorithms: the predictions are made independently. The paper proposes two algorithms: local rescoring via neural models and global n-gram rescoring to address the weakness.
2. The paper analyzes the block drafts and gives several observations. The observation of strong correlation between the index of the largest head such that the average entropy of each head increases monotonically to that point and block efficiency is especially interesting.

Weaknesses:
1. The experiments are only conducted on a 1.5B LM pretrained on 200B tokens of C4, without the alignment stage. This is far from the common practice in current LLMs. The authors should consider adding results on more open LLMs with different sizes.
2. The medusa paper [1] already proposes using top-k predictions for different heads. The contribution of the paper mainly focuses on the two rescoring algorithms. However, the algorithm of local rescoring with a small LM is very similar to speculative decoding with a small LM [2]. The contribution of the global rescoring algorithm with n-gram models is not sufficient for a NeurIPS paper.

[1] Medusa: Simple LLM Inference Acceleration Framework with Multiple Decoding Heads. http://arxiv.org/abs/2401.10774
[2] Fast Inference from Transformers via Speculative Decoding. https://arxiv.org/abs/2211.17192

Limitations:
The authors have adequately addressed the limitations

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes new ways to improve blockwise parallel decoding (BPD), a method to reduce inference latency in large language models. It first analyzes the token distributions produced by multiple prediction heads and then leverages this analysis to develop algorithms to improve BPD inference speed by refining the block drafts using n-gram and language models.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ The paper thoroughly studies BPD's behavior, including issues like consecutive token repetition and varying confidence levels across prediction heads, providing new insights for efficiency improvement. It further introduces the oracle top-k block efficiency as a useful metric for understanding the potential headroom for improvement in block drafts.

+ The proposed refinement algorithms (local neural rescoring and global n-gram rescoring) demonstrate improvements in block efficiency across multiple tasks, with gains of up to 21.30% in some cases.

+ The evaluation considers a variety of tasks (language modeling, question answering, and summarization) and datasets, 

+ The paper is well-structured and easy to follow, with helpful illustrations and examples.

Weaknesses:
+ The evaluation mainly compares the proposed improvements over the existing BPD baselines but doesn't compare them with other approaches for reducing inference latency, such as quantization or model pruning. It's suggested to make a more thorough comparison.

+ The evaluation is conducted mainly on a 1.5B parameter model. It's unclear how well these findings and improvements would generalize to larger, state-of-the-art models. Further, while the paper focuses on improving block efficiency, there's limited discussion on the additional computational cost of the rescoring methods. It's unclear whether the efficiency gains outweigh any increased computational requirements.

+ The improvements in block efficiency vary significantly across tasks, with some showing little to no improvement. A deeper analysis of why certain tasks benefit more than others would be valuable.

Limitations:
The paper has discussed the limitations sufficiently.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper provides an improved solution for block-drafting, which is a potential useful way to improve the inference efficiency of LLMs. The work begins with observations of the problems of current block drafting, reveals that the consecutive repetition and drafting confidence are related to the quality of the draft. Rescoring methods are employed to improve the drafting process accordingly.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The observations are persuasive and the solution is intuitive.

The experiments show impressive improvement (up to 20%) of block efficiency, which is potentially useful.

Weaknesses:
The rescoring phase uses yet another model to scoring the candidate. I am wondering whether using different models affect the final performance. Because different models have different token generation distributions.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
aujnNnIiiM;"REVIEW 
Summary:
The paper introduces PASTA, a framework designed to address the challenges of training end-to-end transformer-based trackers in heterogeneous scenarios, specifically negative interference and poor domain generalization. PASTA leverages Parameter-Efficient Fine-Tuning (PEFT) and Modular Deep Learning (MDL) to define and train specialized modules based on key scenario attributes like camera viewpoint and lighting conditions. These modules are then combined using task arithmetic to enhance generalization to new domains. The framework's effectiveness is demonstrated through extensive experiments on MOTSynth and zero-shot evaluations on MOT17 and PersonPath22, where it outperforms traditional monolithic trackers. The primary contributions include the introduction of PASTA, the use of PEFT and MDL for better domain adaptation, and the validation of its superior performance in diverse tracking scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper introduces an innovative framework that combines Parameter-Efficient Fine-Tuning (PEFT) and Modular Deep Learning (MDL) to tackle challenges faced by end-to-end transformer-based trackers in heterogeneous scenarios, setting it apart from traditional approaches. The research quality is robust, with extensive experiments on MOTSynth, MOT17, and PersonPath22 validating the framework's effectiveness. The paper is well-structured and clearly explains key concepts and methodologies, making it accessible and easily understood.

Weaknesses:
The paper has several weaknesses that should be addressed. 
1. The authors do not compare their framework to ByteTrack and other trackers in zero-shot settings, limiting their evaluation's comprehensiveness. 
2. For a fair comparison in Table 1, the authors should provide detailed information on the different detection thresholds used, as varying thresholds can significantly affect ByteTrack's performance. 
3. The authors need to discuss their choice of attributes in more detail, explaining why specific attributes were selected and their relevance to the framework. 
4. It remains unclear if the performance could be improved without using LoRA, as the authors do not explore or report the impact of omitting LoRA in their experiments.

Limitations:
The authors have claimed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new framework called PASTA (Parameter-Efficient Scenario-specific Tracking Architecture), which aims to improve the generalization ability of multi-object tracking (MOT) in diverse scenarios. The main contributions of this paper include: 1) proposing the PASTA framework to achieve efficient query tracker fine-tuning through PEFT technology; 2) improving domain transfer and preventing negative interference by introducing expert modules; 3) validating the method's effectiveness in zero-shot tracking scenarios through a comprehensive evaluation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1，The paper thoroughly verifies the effectiveness of the PASTA framework through extensive experiments on multiple datasets (MOTSynth, MOT17, PersonPath22). These experimental results clearly demonstrate the significant advantages of PASTA in reducing negative interference and improving domain generalization ability, particularly its excellent performance in zero-shot settings. On the MOTSynth test set, PASTA shows improvements in multiple key metrics (such as HOTA, IDF1, MOTA, DetA, AssA) compared to MOTRv2-MS, proving the advantage of its modular design in handling complex scenarios.
2，The paper is structured tightly and logically, with each part (introduction, methodology, experiments, and conclusion) clearly laid out, making it easy for readers to follow and understand. The methodology section, in particular, provides detailed descriptions of the design principles and implementation steps of the PASTA framework, allowing readers to clearly grasp its working principles and innovations.
3，The paper uses professional and accurate academic language, clearly expressing the research objectives, methods, and results. The concise and clear description of technical details and experimental results enhances the paper's persuasiveness and credibility.
4，The figures and tables in the paper are exquisitely designed, visually presenting experimental results and method structures. For example, Figure 1 shows the modular architecture of PASTA, allowing readers to intuitively understand the combination and application of different modules. The experimental result tables are also clear, facilitating the comparison of different methods' performances.
5，The PASTA framework combines Parameter-Efficient Fine-Tuning (PEFT) and Modular Deep Learning (MDL) technologies, proposing a new multi-object tracking solution. By defining key scenario attributes and training specialized PEFT modules for each attribute, PASTA performs excellently in handling heterogeneous scenarios, demonstrating significant innovation and research value. The paper introduces the concept of combining modules using task arithmetic, significantly reducing negative interference and enhancing domain generalization ability, showcasing the potential of modular design in deep learning.

Weaknesses:
1，The PASTA framework relies on manual selection by domain experts to determine the appropriate modules for the current scenario. Although effective, this method has limitations in practical applications. In some scenarios, it may not always be possible to obtain support from domain experts, or the judgments of domain experts may be influenced by subjective factors, affecting the overall performance of the model. Additionally, this manual selection method may lead to consistency and repeatability issues and is challenging to automate in large-scale applications.
2，Although the PASTA framework has been extensively tested on the MOTSynth, MOT17, and PersonPath22 datasets, these experiments mainly focus on pedestrian tracking and surveillance scenarios. There is a lack of validation in other important application fields (such as traffic monitoring, smart retail, autonomous driving, etc.), limiting the generality and promotion of the results. Further experiments can help verify the application effects of PASTA in broader scenarios, enhancing its generality and practical value.
3，The paper mentions that PASTA reduces computational costs through parameter-efficient fine-tuning, but it lacks detailed analysis of specific computational resources and time costs. For example, in practical applications, how much computational resources (such as GPU/CPU time, memory requirements, etc.) are specifically needed, and how much time is saved compared to traditional methods. Detailed analysis of resources and time costs can help evaluate the practical feasibility of PASTA in different application scenarios.
4，Although the paper mentions the advantages of modular design, it lacks an in-depth study of the potential synergy effects between different module combinations. There may be complementary or conflicting effects between different modules, which have significant impacts on the final performance. Conducting relevant experiments and analysis can help understand which module combinations are most effective, thereby optimizing the design and application strategies of PASTA, further enhancing model performance.
5，The paper does not provide a detailed discussion on the robustness of model parameters in the PASTA framework. There is a lack of experiments testing the model's performance under different noise levels, data quality, and data volumes. These tests can verify the model's stability and robustness in practical applications, further improving the credibility and practicality of PASTA. These experiments can demonstrate the adaptability and stability of PASTA in various real-world environments, ensuring it maintains high performance under various conditions.

Limitations:
1，Complexity of Modular Systems
The modular design of the PASTA framework increases the complexity of the system, especially in terms of training, managing, and combining modules. Each module requires separate training, optimization, and validation, increasing development and maintenance costs. In practical deployment, modular systems require more complex architectural support, involving dynamic selection and combination of modules, which may increase implementation difficulty and system maintenance complexity.
2，Generalization Limitations
Although PASTA performs well on specific datasets and scenarios, its design and optimization target specific scenario attributes (such as camera angles, lighting conditions, etc.). PASTA's generalization and adaptability may be limited when dealing with new scenarios that do not belong to these predefined attributes. The framework's performance in completely different application environments or new domains has not been fully validated, potentially affecting its promotion and widespread application.
3，Dataset Dependency
The experiments in the paper are mainly based on the MOTSynth, MOT17, and PersonPath22 datasets, which may have certain biases or characteristics. The excellent performance of the PASTA framework on these datasets may not be directly generalizable to other datasets or more diverse data sources. The lack of experimental validation on more diverse datasets limits the comprehensive evaluation of PASTA's generalization ability in various scenarios.
4，Dependence on Pre-trained Models
The overall performance of the PASTA framework largely depends on pre-trained backbone networks and detectors. If pre-trained models perform poorly in certain tasks or scenarios, the performance of the PASTA framework will also be significantly affected. This means that the application and effectiveness of the PASTA framework are, to some extent, limited by the quality of pre-trained models, requiring high-quality pre-trained models to achieve optimal performance.
5，Lack of Evaluation of Real-time Performance
The paper does not provide a detailed evaluation of the real-time performance of the PASTA framework, such as processing latency and efficiency in real-time video streams. Real-time performance is critical for many practical applications (such as video surveillance and autonomous driving). The lack of evaluation in this aspect makes it difficult to judge the feasibility and effectiveness of PASTA in real-time environments, potentially limiting its application prospects in scenarios requiring high real-time performance.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper aims to address the domain gap across different multi-object tracking datasets. The proposed method is inspired by LoRA and introduces parameter-efficient fine-tuning for state-of-art end-to-end trackers. Specifically, these trackers are first trained on a large-scale dataset MOTSynth. After that, by shifting several expert attributes and training partial parameters, these trackers can achieve impressive performance on MOT17 and PersonPath22.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and easy to follow.
2. The proposed method is novel and interesting. I like it very much!
3. The experiment details are well provided.

Weaknesses:
The idea of this paper is attractive for me. However, I still have several minor concerns about the experiments:
1.This paper does not perform zero-shot learning on another popularly used dataset MOT20.
2.The employed attributes in this work, such as lighting, viewpoint, occupancy, location, and camera, seem “weak”. How about the transferring performance on “strong” attributes, like scenes and categories? In other words, how about the zero-shot evaluation performance on other datasets like DanceTrack and KITTI?
3.In Table 1, both model parameters and trainable parameters should be listed for a better comparison. Besides, the table captions in experiments are suggested to provide more descriptions.

Limitations:
Please refer to the weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces a fine-tuning framework, denoted PASTA, for multiple-object tracking that is aimed at reducing the cost of tine-tuning large models while mitigating negative inference to improve zero-shot transfer and domain generalization.

During training, the authors independently fine-tune per-domain modules/experts on pre-defined scenario attributes (e.g. lighting, static or moving camera, ...). 
Modules only adapt a sub-set of the hyperparameters and the adaptation is done by relying on low-rank adaptation (LoRA). This allows for fine-tuning with a low computational and memory footprint.
More specifically, the authors only adapt batch norm parameters in the backbone and linear layers in the transformer modules.

During inference, the method relies on ""expert knowledge"" to select the right module for each attribute (e.g. select the ""bad lighting"" module for the ""lighting"" attribute). A convex combination of the attribute's modules' weights is then applied (with the selected module having a higher coefficient, defined by a hyper-parameter).

For the zero-shot (transfer) setting, the model is simply set to be a weighted average of all modules.

Experiments demonstrate the effectiveness of the method by comparing performance with the same model trained without the framework. PASTA outperforms the baseline on both the synthetic dataset and on zero-shot transfer (from synthetic to real-world datasets). The authors further demonstrate the reduced forgetting in the ablation studies.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is easy to read, provides a good overview of the methods it relies on (i.e. LoRA and modular deep learning), and includes a good overview illustration.
- The experimental section and the ablation studies demonstrate the effectiveness of the approach and thereby validate the main claims.
- The authors discuss both positive and negative possible societal impacts of the approach.
- ""Novelty"" is limited as it is mainly an implementation of existing concepts. Nevertheless, the application to a new domain and the good execution make it a useful contribution to the community.

Weaknesses:
- The approach is claimed to reduce fine-tuning costs (reduced time and memory footprint), but no time or memory comparison to full tine-tuning is provided.
- The approach is related to domain adaptation and zero-shot transfer, but does not discuss similarities and differences to those approaches in the related work section, and does not compare to such methods in the experimental section.

Limitations:
Yes, the authors discuss the potential positive and negative societal impacts of multiple-object tracking, as well as the main limitation of the method (i.e. the reliance on an external ""domain expert"")

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a multiple object tracking framework that can generalize to new domains by training specialized modules for each scenario attributes.
These modules are trained using Parameter-Efficient Fine-tuning and modular deep learning techniques on a transformer-based tracker. 
This tracker is build on the Deformable DETR framework with a ResNet backbone for image feature extraction.
Extensive experiments are performed on both synthetic and real datasets to compare with some recent track-by-detection and track-by-attention approaches.
It shows that the proposed approach can generalize well on unseen datasets and reduce negative interference during training.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposed modular architecture is significant and origin in solving MOT problem.
2. Experiments support and prove the proposed architecture's ability to generalize on new domains, i.e. real dataset from synthetic dataset.

Weaknesses:
1. The authors only conduct a synth-to-real zero-shot experiment. It would be better to see real-to-real experiment where one can train on one dataset and test on a new dataset.
2. It would be great to have some statistic on the training dataset in terms of the attributes, e.g. how many frames / videos have high/low occupancy, etc. 
3. One missing aspect in the ablation studies is how the task vector is incorporate to the pre-trained tracker contribute to the final results. For examples, incorporating on the backbone, encoder or decoder only and the whole network can have different results.
4. The proposed approach seems to be limited by the availibility of the attribute data for training each scenario-specific module.

Limitations:
Yes. The authors provided some analysis on limitations and social impacts.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
hGgkdFF2hR;"REVIEW 
Summary:
As the paper clearly outlines in the introduction, Optimal Transportation (OT) is widely used in various fields of machine learning, however, the cost for computing OT is computationaly expensive (quadratic scaling), even after Sinkhorn algorithm employing entropy regularization significantly alleviated the cost. The purpose of Low-rank Optimal Transport (LOT) is to reduce the complexity by constraining the transportation plan to possess a certain low-rank structure.

The key contribution of the paper is (1) new parametrization of LOT using a Latent Coupling (LC) factorization, (2) The eponymous ""Balance FRLC"" algorithm for resolving the optimization procedure for computing LOT with respect to various objectives associated with (LOT), (3) Theoretical guarantees for the smoothness of the regularized objectives, as well as upper bounds for the convergence criteria.  The algorithm is applied to illustrative toy examples and then to a more realistic example (spatial transcriptomics alignment) and the results indicate improvements over previous LOT.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The contributions of the paper is significant. The writing is excellent. The LC factorization, adapted to the LOT framework from a previous work, is well-motivated. The Balanced FRLC algorithm appears a sound and non-trivial extension of previous approaches for LOT (Sinkhorn, semi-relaxed OT). The algorithm is particularly simple and shown to be effective. The experimental results are clear and illustrative, improvements are convincing.

Weaknesses:
What this reviewer found missing is discussion of the case the optimal transport map is itself not low-rank. It is well-known that the optimal transportation plan is not necessarily low-rank (as indicated e.g. by their connection to differential equations with non-regular solutions), so LOT may converge very slowly to the full-rank optimal transportation plan. In section 4.2, the authors provide a simple illustrative example, but this is when the transportation plan is known to be low-rank - in this case the transportation plan seems to stabilize after some threshold rank - however, what happens when the full-rank plan does not have an inherently low-rank structure?

Limitations:
This work builds on the LOT framework - so perhaps the authors comment on some scenarios where the LC factorization can potentially yield no better results, when compared to previous LOT?

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work introduces a new low rank formulation of optimal transport based on the latent coupling decomposition introduced in https://arxiv.org/pdf/2012.11589 . Compared to previous formulations of low rank OT, this formulation allows easier extensions to unbalanced and Gromov Wasserstein settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I found that the presented FRLC method is a quite elegant approach to low rank OT. It decouples optimization into entropy regularized semi relaxed OT problems for $Q$ and $R$, and $T$ is updated solving entropy regularized  OT of size $r$. This decoupling is enabled by the form of the inner coupling which is no longer assumed to be diagonal. This construction alleviates the need to use the Dykstra machinery as in (Scetbon 2021).
With the proposed approach, $Q$ and $R$ are not required to have the same inner marginal. From what I understand, the optimization over inner marginals is handled by the semi relaxed steps and does not require a separate step as in previous approaches. 

Another strength of the approach is that it generalizes smoothly to the unbalanced setting. 

Other strengths include:
* I carefully looked at the presented method and the appendix and the algorithmic approach made sense. 
* non asymptoptic convergence bounds are derived for the method. 
* the appendix includes a complete review of the necessary technical background presented in a clear way, easy to follow.
* limitations are clearly stated (section N in appendix).
* a new initialization scheme for LC decomposition is proposed. 
* there is a gain in interpretability compared to (Scetbon 2021) as the inner coupling captures the coupling between clusters in the data.

Weaknesses:
* from section 3.1 to 3.2, the new aspects of the approach compared to https://arxiv.org/pdf/2012.11589 should be stated more clearly. The contributions of the paper should be separated more clearly from previous works. I had to read the other paper to better understand each contribution. 
* In algorithm 1, it would be interesting to have the computational complexity of each step (presented in the algorithm directly).

Limitations:
/

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel framework called Factor Relaxation with Latent Coupling (FRLC) which is based on coordinate mirror descent to compute the low-rank LC factorization.The algorithm decouples the optimization into three sub-problems, offering greater flexibility, interpretability, and linear space complexity. FRLC is applicable to various OT objectives (Wasserstein, Gromov-Wasserstein, Fused Gromov-Wasserstein) and marginal constraints (balanced, unbalanced, and semi-relaxed). Theoretical results support its effectiveness, and empirical tests demonstrate superior performance in applications like graph clustering and spatial transcriptomics.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The introduction of the LC factorization and its integration into the FRLC algorithm represents an innovation in reducing the complexity of optimal transport problems.

The FRLC framework is versatile, handling multiple OT objectives and marginal constraints, making it applicable to a wide range of practical problems.

The latent coupling provides a interpretable description of the transport plan, which is beneficial for understanding and visualizing the results.

Weaknesses:
While the empirical results are good, the paper could benefit from more extensive comparisons with additional baseline methods to solidify the claims of superiority.

The theoretical results are limited in that, even though $\Delta_k(x_k, x_{k+1})$ is small, it cannot be guaranteed that the iteration points converge to the optimal solution or even a stationary point.

Limitations:
N.A.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents an approach for low-rank optimal transport (OT) leveraging a latent coupling (LC) factorization and solving it with mirror descent. This approach offers a new parameterization of the low-rank OT problem, providing advantages such as decoupling the problem into three OT problems and enhancing flexibility and interpretability. The authors introduce the Factor Relaxation with Latent Coupling (FRLC) algorithm, which utilizes coordinate mirror descent to compute the LC factorization. FRLC accommodates various OT objectives and marginal constraints. The paper includes theoretical results and demonstrates the performance on selected tasks like graph clustering and spatial transcriptomics.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Generality: handles different OT objective costs and relaxations of the marginal constraints.
- Significance: the proposed method, and improving OT using factorizations, has the potential to address the scalability issue of OT with large datasets, making it highly relevant for applications in machine learning and data science.

Weaknesses:
**Presentation and logical organization of the ideas**

I found it difficult to discern the paper's particular contribution, especially concerning previous art like Latent Optimal Transport (Lin et al., 2021), Forrow et al. (2019), and Scetbon et al. (2021). For example, the main contribution is said to be ""compute a minimum cost low-rank transport plan using the LC factorization,"" where LC factorization is precisely the factorizations proposed by Lin et al. (2021), with the further contribution being the computation of this factorization using mirror-descent. I wondered how much of the description also applies to Lin et al. (2021) and which part is particular to the novel method. 

Furthermore, according to Lin et al. (2021), there is an algorithm for computing low-rank factorized plans based on factorized costs using a projection method and an algorithm to factorize the costs and select anchor points without the need to have a similar number of anchor points at source and target latent space. How does your approach compare with the approach? 

**Comparison and empirical validation**
The first empirical comparison analyzes LOT and FRLC, capturing how both methods behave as the factorization rank changes. This is a good way of understanding the effect of the rank on each case. Nevertheless, in the second setup, the comparison with LOT is not present anymore, which begs the question of how LOT performs in the second empirical evaluation. Furthermore, an extensive empirical study with the structured approach of Lin et al. (2021) would help clarify the particular contributions of the type of factorization and factoring algorithm used. 

I found some other issues lacking in a more in-depth discussion of the computational complexity, possible implementation difficulties, and overall limitations in the proposal.

Limitations:
I found it necessary to discuss in more depth the limitations and the tradeoffs present in the proposal compared with previous art.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
zqLAMwVLkt;"REVIEW 
Summary:
This paper works on node anomaly detection in the novel semi-supervised setting where few labeled normal nodes are given and proposes to generate new anomaly nodes to boost the training data. The anomaly generation algorithm is inspired by the empirical observation that:

(1) Anomaly nodes have lower affinity score than normal nodes
(2) Feature distribution of anomaly nodes are similar to normal nodes if they share similar neighborhood patterns.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
(1) The setting is novel and aligned to the real-world situation where normal nodes are typically known compared with anomaly nodes.

(2) The motivation for the proposed two regularization losses is very intuitive and clear.

(3) The experimental results are very impressive.

Weaknesses:
(1) The proposed two regularization losses are heavily based on the empirical analysis, which might not transfer to other anomalies in other datasets. 

(2) For the second prior, its assumption that anomaly nodes sharing similar local structures would share a similar feature distribution has not been empirically verified.

(3) Experiments miss the comparison with diffusion-based generative anomaly detection baseline.

Limitations:
In addition to the limitations mentioned by the author, there are some other limitations worth addressing:

(1) The currently proposed anomaly generation method is still operated in the embedding space. As admitted by the author anomaly behavior is heavily based on interactional behaviors, therefore, it is also helpful to consider directly characterizing/generating anomaly in the graph space.

(2) The comparison misses one generative-based baseline [1]

[1] Liu, Kay, et al. ""Graph diffusion models for anomaly detection."" (2024).

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel approach called GGAD aimed at improving anomaly detection in graphs under a semi-supervised framework. GGAD generates pseudo anomaly nodes that serve as negative samples for training a one-class classifier. This method is built on two
key priors: asymmetric local affinity and egocentric closeness, which help in generating reliable outlier nodes that mimic real anomalies in terms of both graph structure and feature representation. Extensive experimental results demonstrate the effectiveness of the method across diverse graph anomaly detection datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The method is innovative. The proposed graph anomaly detection method can exploit the feature and structure information of normal nodes more effectively in the studied semi-supervised scenario compared to existing methods.  The proposed two priors provide a meaningful characterization of desired properties of outliers in this semi-supervised setting and can be utilized to explore other beneficial priors further. 

2.The experiments in the paper are comprehensive and thorough.

Weaknesses:
1. The model relies on prior knowledge to generate anomaly points. This prior knowledge can limit the model’s application scenarios. The model performs best only when the real anomalies align with this prior knowledge. For anomaly types that do not conform to the prior knowledge, the model may not effectively detect them.

2.The model does not perform best on the Photo dataset in Table 1, and the article lacks an explanation of the results at the overall data level.

3. This model employs a semi-supervised approach that uses some positive samples for training. However, it does not consider the issue of noise interference within the positive samples, namely, how the model overcomes interference when some positive samples are mislabeled.

4. During the initialization step, only the initial feature of outliers are obtained while the connections between the outliers and normal nodes are not well illustrated in the paper. From Figure 2, one outlier is connected to more than one normal node while the feature of the outlier is generated according to single normal node. The neighborhood of outliers is important since the it involves the computation of node affinity score of outliers.

Limitations:
yes, the authors point out that some anomalies whose characteristics may not be captured by the two priors used

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel generative-based GAD approach, named GGAD, tailored for the semi-supervised scenario. Unlike existing GAD frameworks, the authors highlight the feasibility and importance of a semi-supervised setting where labels for normal nodes are relatively easy to obtain during training, but labeled abnormal nodes are very limited. In this context, the paper proposes generating pseudo-anomaly nodes to serve as substitutes for real anomaly nodes in training, thus aiding in anomaly detection. These pseudo-anomalies are generated through two unique loss-guidance mechanisms. Experimental results demonstrate the effectiveness of GGAD.

However, the description of the semi-supervised setting in this paper lacks clarity and unconvincing. Additionally, there is minimal differentiation between the proposed method and existing works that generate pseudo-anomaly samples for data augmentation. I think this paper's novelty is limited. I still think that doing unsupervised GAD is more necessary, and if the authors can prove that the pseudo-outlier proposed by GGAD can benefit unsupervised GAD as a general module, I can up my score.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.The complete experiment shows the effectiveness of the method and the necessity of each component.

2.Some visual illustrations help the reader understand, although the shapes of the images seem to be compressed.

Weaknesses:
1. I am still confused about the motivation for performing semi-supervised GAD. Why do most methods emphasize unsupervised scenarios? The cost of labeling normal nodes seems too expensive, as the authors themselves state on lines 268 to 269, yet they assert again on line 31 that labels for normal nodes are easy to obtain.This inconsistency hinders a clear understanding of the necessity and practical applications of semi-supervised GAD, which significantly undermines the motivation for this work.

2. While the first loss function proposed by the authors appears intuitively valid, the second loss function aims to generate outliers similar to normal nodes. In my opinion, optimizing these two losses together is unreasonable because they conflict with each other. It seems that they should correspond to different outlier generation processes

3. The paper validates the improvement of unsupervised GAD using labeled normal nodes and claims that GGAD remains superior. I think the authors ignore the fact that unsupervised methods do not obtain this outlier like GGAD and this comparison is not reasonable.

Limitations:
No limitation need to discuss

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper explores the problem of semi-supervised graph anomaly detection (GAD), where some nodes are known to be normal, in contrast to the typical unsupervised setting with no labeled data. The authors show that even a small percentage of labeled normal nodes can improve the performance of existing unsupervised GAD methods when adapted to the semi-supervised scenario. The paper proposes a novel Generative GAD approach (GGAD) to better exploit normal nodes by generating pseudo anomaly nodes, called 'outlier nodes', to provide effective negative samples for training a one-class classifier. GGAD generates these outlier nodes using priors about anomaly nodes, such as asymmetric local affinity and egocentric closeness, to mimic anomalies in structure and features. Experiments on six real-world GAD datasets show that GGAD outperforms state-of-the-art methods in both unsupervised and semi-supervised settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ This paper studies a new problem of semi-supervised GAD that has not been widely studied. 

+ The proposed method is simple and effective from the empirical perspective.

+ The experiments are extensive including effectiveness and efficiency analyses and the method has been tested on real-world large-scale graphs to verify the scalability.

Weaknesses:
- The two priors that are used to generate outlier nodes are heuristic or based on empirical evidence. There is no theoretical analysis provided to better guarantee the effectiveness of the proposed method.

- It will be more interesting and helpful to show the generated outlier nodes can capture the characteristics of anomalous nodes in addition to comparing their representations.

- The experimental settings of anomaly contamination are not very clear: how the contamination is introduced?

- Overall experimental settings. What hardware has been used in the experiments, e.g., memory, and why are the experiments conducted on CPUs?

Limitations:
The authors have adequately addressed the limitations

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies an under-explored graph anomaly detection problem where the detection models have access to a set of labeled normal nodes. To tackle this problem, it introduces a generative approach namely GGAD that generates pseudo anomaly nodes, called outlier nodes, to support the training of a discriminative one-class classifier. The key idea underlying this approach is to generate the outlier nodes in a way that can well simulate real anomaly nodes in both graph structure and feature representation perspectives. To achieve this, GGAD defines and incorporates two priors, including asymmetric local affinity and egocentric closeness, into its optimization objectives, with the former prior focusing on the alignment on the graph structure aspect and the latter on the feature representation aspect. The method is evaluated on six large real-world datasets and shows impressive detection performance compared to existing state-of-the-art methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is generally well-written and easy-to-follow.
- The problem setting is practical since labeled normal samples are easy to obtain in many real-world applications. Compared to the commonly studied unsupervised setting, this semi-supervised setting often results in better detection performance.
- The proposed method GGAD is novel. There have been many generative anomaly detection methods, but as far as I know, they are unable to consider the graph structure and the neighboring nodes’ representations. By introducing the two new priors, GGAD addresses this issue well. Fig.1 and Fig. 3 help demonstrate this effect.
- The method is compared with a range of unsupervised and semi-supervised methods on 6 real-world datasets with diverse genuine anomalies, and gains largely improved detection performance over these competing methods.
- The ablation study is plausible and justifies the contribution of each proposed prior.

Weaknesses:
- The outlier node generation in GGAD may cause non-trivial computational overhead.
- Despite better performance than the competing methods, GGAD gains an AUC of only around 0.6 on some datasets, such as DGraph and Reddit.
- In Fig. 4 (b), GGAD shows a fast AUPRC growth with increasing training size, but the other methods have a flat performance trend. What would be the reason behind?

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
JK728xy8G7;"REVIEW 
Summary:
The paper presents a condition-free guidance method for diffusion models. The guidance is generated from the self-attention mechanism to perform guidance from an energy-based perspective as an alternative to classifier-free guidance. With this, the work aims to train the models for improved quality performance in conditional and unconditional image generation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The model presents an alternative guidance method for the diffusion model that is independent of an explicit condition, allowing it to work with conditional and unconditional generation.

Weaknesses:
As a suggestion by the paper is to refer to a qualitative assessment, what would be the criteria to consider a model or ablation generates a better quality?

Limitations:
The paper discusses potential limitations and societal impact of generative models in the conclusion section.

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a technique to improve unconditional sampling from diffusion models. The main idea is to translate the notion of classifier-free guidance (CFG) to the case in which there is no condition available. To this end, the paper notes that the conditional prediction is ""sharp"", while the unconditional prediction is ""smooth"", or more simply put, the unconditional prediction is smoother than the conditional one. The paper also notes that applying Gaussian filtering on the attention weights of the model's unconditional prediction yields an (even) smoother prediction. The paper then combines these two observations, and proposes SEG, a version of CFG that is applicable to the unconditional case: CFG requires the conditional and unconditional predictions; SEG replaces the conditional prediction with the unconditional, while replacing the unconditional with its smoothed version. By doing this procedure, SEG manages to preserve the sharpness-smoothness relation that CFG has between the factors (the conditional and unconditional predictions).

The paper then demonstrates that this procedure translates into improved performance compared to reasonable competitors on standard benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper proposes a simple yet effective procedure to improve unconditional sampling in diffusion models. The idea, in my view, is elegant
- The paper is, over all, well written and clear
- Both qualitative and quantitative evaluations seem to demonstrate the paper's point on the empirical side
- I think the paper's proposal could be used for improving the efficiency of conditional sampling, which is arguably an even more useful case

Weaknesses:
## Weaknesses that don't affect my rating
- The Method section could strongly benefit from a figure illustrating on what object, exactly, is the Gaussian blur being applied. The paper currently doesn't have a ""pipeline"" figure
- I understand the value of Lemma 3.1, but I don't think it's necessary to provide a proof in the main paper. As a side note, I don't even think it's necessary to provide a proof: as far as I understand, this fact is widely known, since the Gaussian filter is normalized and symmetric, no?
  - For instance, I think a well-known intuition in the computer vision community is that (infinite) successive applications of Gaussian blurring on an image result in an image with a single color, that corresponds to the average color of the initial image (i.e. same mean, and zero variance).
- L188 mentions PAG. It would be useful to remind the reader what it stands for. (I think it was originally just mentioned in the introduction)
- Unless I'm misunderstanding, L211 claims that, contrary to two other methods--SAG and PAG--the presented method is both training- and condition-free. I'm not sure I follow: aren't those two methods both also training- and condition-free?
- I understand what the authors mean in L231 by ""FID has been found not to be a strict measure of image quality"". However, FID in itself does not measure image quality, but distance w.r.t. some distribution
------------
## Weaknesses that don't affect my rating, but should be addressed:
- Eq. (5): please consider explaining that ""lse"" standards for logsumexp a bit before L85: by then it's a bit confusing already
- Isn't L86 redundant (since the same statement is made--and referenced--in L75?)
- L95: please consider denoting the LSE with some convention: just writing ""lse"" lends itself to confusion
- L97: a negative prediction of what?
- L108: to make which operation tractable?
- L124: increases the LSE or decreases (according to L127). My intuition is that it decreases, since LSE approximates the maximum of the tensor
- The proof of proposition 3.1 could be made shorter, or sent to the appendix
- L177: ""Note that the Gaussian blur can also be applied to K"" I presume this is true because of the commutation property of the convolution operation. Perhaps explicitly stating that in the paper would be better

Typos and such:
- L9: actually?
- L39: why ""actually""?
- L70: conditioning on
- L72: demonstrates its prevalence? sounds weird
- L111 and equation at L 113: boldface ""a""?
- Minor comment for all over the paper: use \eqref for equations
- L230: ""how much guided images are altered from unguided ones"" sounds a bit weird
- L232: ""being more favorable to users"" sounds weird
- Fig. 2 and 11: I think there's a repeated sample? (the one indoors)

Limitations:
Yes, they have

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a method for unconditioned image generation based on Diffusion Models, specifically using Stable Diffusion. The proposed method offers an alternative to classifier-free guidance (CFG), eliminating the need to train a classifier for adding conditions. Traditionally, the CFG denoising function incorporates both conditional and unconditional terms, while unconditional generation only involves an unconditional evaluation of the U-Net. Instead, this method performs unconditional diffusion by introducing an additional term to the traditional reverse process.
This new term is an energy-based component calculated as a proportion of the Gaussian blurring of the self-attention layers in the U-Net. The core idea is to use a blurred version of the unconditional self-attention layers to shift the mean distribution of the original unconditional prediction, thereby ""smoothing"" the prediction.
The paper employs the standard SDXL model and compares its results against the Perturbed Attention Guidance (PAG) and Self-Attention Guidance (SAG) methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper presents an interesting way to use the unconditional prediction of the network as another way to do guidance generation. 

This paper presents a method that aims to improve traditional unconditional prediction in diffusion models by smoothing the distribution through the addition of a term to the conventional unconditional image generation process. The proposed idea is analogous to the original CFG methodology in that it combines two different distributions to enhance the generated image. In this case, the smoothing of the distribution is achieved by incorporating a blurred version of the self-attention layers.

Weaknesses:
-  Technical correctness of the paper

The claim in lines 191-192, where the paper asserts that $\sigma \rightarrow 0$ remains the same as the original [image], is not entirely clear, and the intuition behind this is not thoroughly presented. The concern arises because Eq. 7 indicates that the proposed method involves adding another term to the original unconditional generation. According to the literature [10], what this paper refers to as $s_\theta(x,t)$ corresponds to a distribution, as does $\tilde{s_{\theta}}(x,t)$. However, having $\sigma$ approaching to 0 does not imply that this term becomes 0; rather, it means that the Gaussian filter affecting the calculation of the self-attention is 0. Therefore, the inference drawn from this reasoning is that if the filter's variance approaches 0, then $s_\theta(x,t)$ approximates to $\tilde{s}_\theta(x,t)$.

Since this paper relies on a strong theoretical background, it is crucial to provide a thorough demonstration and explanation of all claims. For instance, in line 150, what does it mean to be under a ""reasonable"" assumption to demonstrate the attenuation of the Gaussian curvature? Additionally, what does the paper refer to when discussing the property of linear mapping in the blurring of the queries between lines 167 and 169?

As the method uses a Gaussian kernel for the blurring process, it involves a sigma parameter derived from the Gaussian filter. However, there is insufficient discussion on how this parameter significantly affects the saturation of the generated image. From line 211, it can be interpreted that sigma controls the saturation, but between lines 274-276, the paper indicates that saturation can be altered by using the guidance scale for this method. The question is how sigma and gamma can be balanced to avoid over-saturated images and whether other characteristics of the image (brightness, darkness, vibrance, etc.) can be controlled through these parameters.

- Experimental validation

This paper limits its exploration by not providing further comparisons using other diffusion models as backbones. This omission raises questions about the proposed method's generalization capability.

It is not clear why the paper does not report the perceptual metric for the Vanilla SDXL in Table 1.
The lack of a metric to compare the perceptual performance of the Vanilla SDXL against the proposed method raises a concern, as the validation is then limited to just a few image examples presented in this manuscript. Without this measure, it is difficult to fully understand the overall performance of the proposed method compared to the baseline and other state-of-the-art methods.

The disclaimer in lines 231–232 regarding the FID is acknowledged. However, when it comes to image generation, these are the metrics that the community has adopted and learned to interpret. Of course, this does not limit new research from proposing new ways to measure image generation to provide a ""strict measure of image quality"" and other attributes.

This paper claims (line 14-15) that its implementation does not significantly increase computational cost. However, it is unclear how the computational requirements should be adjusted to use it. Moreover, the paper directly compares its method against unconditional generation, which only requires a single network evaluation, while this method (according to Eq. 7) requires at least two network evaluations. Furthermore, using a conditional input would necessitate at least three network evaluations, likely leading to a considerable increase in computational cost. It is suggested that the paper include an analysis of the trade-off in terms of the number of function evaluations (NFEs) to fully understand how many network evaluations are needed and how this method impacts computational cost.

It would have been interesting to see how other filter kernels perform besides Gaussian blurring.

- Presentation 

Although this paper is well-written and presented, its intelligibility could be improved by adding a few more sentences to clarify some of the intuitions described above.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The manuscript introduces SEG, a novel training- and condition-free guidance method for enhancing image generation with diffusion models. The method leverages an energy-based perspective of the self-attention mechanism and introduces a technique to reduce the curvature of the energy landscape of attention, thereby improving the quality of generated images. SEG controls the guidance scale by adjusting a Gaussian kernel parameter, offering a flexible and theoretically grounded approach to unconditional and conditional image generation. The authors validate the effectiveness of SEG through extensive experiments on conditions, showcasing its superiority over existing methods like SAG and PAG in terms of sample quality and reduction of unintended effects. My detailed comments are as follows:

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
This article proposes a novel training- and condition-free image generation method, SEG, which significantly improves diffusion models through the theoretical foundation of smooth energy landscapes and the introduction of Gaussian blur on attention weights and the development of efficient query blur techniques. The quality of the images generated and their superiority verified on multiple conditions.

Weaknesses:
The method proposed in this manuscript, SEG, relies heavily on the quality of the baseline model and may amplify biases or harmful stereotypes in existing data.

Limitations:
There is a lack of detailed discussion, such as how to determine the value of σ and why increasing the value of γ does not improve the sample quality in terms of FID and CLIP scores.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The method (SEG) discussed in the paper mainly applies an energy-based optimization on the emerging values in the self-attention to reduce the curvature of the energy landscape of attention, leading to improved image quality and less structural change from the original prediction compared to previous approaches. SEG is training- and condition-free and can be used for both unconditional and conditional sampling strategies. The authors validate the effectiveness of SEG by evaluating generated images with and without text conditions, as well as with ControlNet.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper looks at the refinement of image generation through emerging values in attention layers, which is very interesting. The paper investigated multiple conditions for the diffusion model they used, and the visualizations are insightful.

Weaknesses:
The quantitative evaluations needed to be presented in a more detailed manner.
The number of works they compared against could be much more.
e.g. imagic (Kawar, B., Zada, S., Lang, O., Tov, O., Chang, H., Dekel, T., Mosseri, I. and Irani, M., 2023. Imagic: Text-based real image editing with diffusion models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 6007-6017).), LEdit++ (Brack, M., Friedrich, F., Kornmeier, K., Tsaban, L., Schramowski, P., Kersting, K. and Passos, A., 2024. Ledits++: Limitless image editing using text-to-image models. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 8861-8870). ) or collaborative diffusion (Huang, Z., Chan, K.C., Jiang, Y. and Liu, Z., 2023. Collaborative diffusion for multi-modal face generation and editing. In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition (pp. 6080-6090).),

Limitations:
The validation can be improved.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
f63DKIpx0I;"REVIEW 
Summary:
The paper titled ""Self-Healing Machine Learning: A Framework for Autonomous Adaptation in Real-World Environments"" introduces the concept of self-healing machine learning (SHML). This framework aims to address performance degradation in machine learning models due to distributional shifts. Unlike traditional concept drift adaptation methods, SHML focuses on diagnosing the reasons for model degradation and proposing targeted corrective actions. The paper presents a theoretical foundation for SHML, an algorithm (H-LLM) that uses large language models for diagnosis and adaptation, and empirical evaluations demonstrating the effectiveness of the approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The SHML framework is a novel approach to handling model degradation by diagnosing and addressing the root causes, rather than using reason-agnostic methods.
2. The paper provides a solid theoretical foundation for SHML and demonstrates its practical viability through empirical evaluations.
3. The concepts are well-explained, and the structure of the paper is logical and easy to follow. Figures and tables enhance the understanding of the proposed methods.
4. SHML has significant potential in high-stakes applications where maintaining optimal model performance is critical, such as healthcare and finance.

Weaknesses:
1. While the paper demonstrates promising results, the empirical evaluation is limited to a simulated diabetes prediction task. Additional experiments in diverse real-world environments would strengthen the claims.
2. The comparison with existing concept drift adaptation methods is not exhaustive. A broader set of baseline comparisons would provide a clearer picture of the advantages and limitations of SHML.
3. The paper lacks detailed information on the implementation of H-LLM, especially regarding the practical challenges of deploying such a system in real-world scenarios.
4. Although the authors mention the availability of the code upon acceptance, more details on the experimental setup and data used would help in reproducing the results.

Limitations:
The authors acknowledge the limitations of SHML, including the challenges in accurately diagnosing the root causes of performance degradation and the potential computational overhead of the approach. They also discuss the need for sufficient parallel processing capacity to handle the increased demands of multiple diagnostic and adaptation actions. The broader societal impact of SHML, including potential misuse in high-stakes applications, is also briefly addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new concept of self-healing machine learning, or SHML. The idea is based on understanding and addressing the reasons of performance drops in ML systems, thereby going beyond most common approaches that are labelled as reason-agnostic. The approach is based on a pipeline well illustrated in Fig 2 that includes four steps: monitoring, diagnosis, adaptation and testing. Formal definitions are provided. Experimental support is provided to demonstrate some aspects of the new idea and its potential advantages.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
I have identified the following strength: 
- The paper starts from an interesting assumption that the reasons for performance drop should be considered and investigated. 
- The paper is well organised and easy to read. 
- The method is described with precise formalism.

Weaknesses:
I have identified the following weaknesses: 
1. While the main assumption is move on from reason-agnostic methods to a system that attempts to understand the reasons, the paper does not offer a good classification of such reasons, mainly because it main a meta-level approach, in which reasons are to be learned. This could be less effective than pre-defining monitoring and diagnosis methods according to known causes.  
2. As a follow-up from the previous point, the diagnostic part that generates hypotheses via a LLM may or may not be correct and the adaptation may or may not be effective. I find it hard to get a feeling of how well the system would work on a large set of benchmarks and problems from the evidence and experiments presented in the paper. 
3. Most of the interesting details of the paper are actually in the appendix.
4. I disagree that this is the first non-reason-agnostic method, as the authors overview and list other approaches that do focus on the causes of performance drop. I agree that this may be the first approach to tackle the issue comprehensively. Nevertheless, the novelty, knowledge gap, and improvements of the approach with respect to existing approaches is not well summarised in the introduction/related work, which is rather short and it refers to the appendix for further detail.

Limitations:
I find that the limitation part (before the conclusion and in 5.2)  is too short . Yes, I agree that the correct identification of the root causes of performance drop is challenging in real work scenarios, but the issue needs to be expanded and discussed more. For example, why is it difficult? Are some reasons that are more difficult than others to identify? Monitoring could also be challenging. Adaptation could be challenging. 

The impact is sufficient covered.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a self-healing framework for machine learning models called Self-Healing Machine Learning (SHML). Unlike previous methods, SHML autonomously diagnoses the causes of model degradation and suggests corrective actions based on these diagnoses. The authors formalize SHML as an optimization problem, aiming to minimize expected risk by adapting to changes in the data-generating process (DGP).
A theoretical framework for self-healing systems is introduced, exemplified by H-LLM, which leverages large language models for self-diagnosis and self-adaptation. Empirical analyses of SHML's components demonstrate its potential and effectiveness.
The paper underscores the importance of ensuring optimal performance in algorithms used in high-stakes applications. By enabling systems to autonomously adapt to new environments, SHML aims to advance self-healing systems, benefiting both the machine learning community and society. The theoretical framework lays the groundwork for developing optimal adaptation and diagnosis methods. The authors hope this work will stimulate further theoretical developments and encourage the adoption of self-healing systems in critical fields such as medicine and finance.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
From the perspective of originality, this paper lowers the barrier for others to implement adaptation actions in machine learning models. Its importance lies in addressing the growing challenge of maintaining machine learning models, especially given their increasing usage. A framework capable of diagnosing drift or degradation and autonomously solving these issues is crucial. This paper effectively addresses this need, outperforming existing approaches in both presentation and results.
The organization of the paper is good, with a well-structured presentation. The detailed inclusion of examples, definitions, assumptions, theoretical explanations, and results in the main text and appendix is thorough and effective (but the appendix is probably required to really understand)

Weaknesses:
The main weakness of this paper is its tendency to use overly long sentences and complex phrases, which can hinder readability and clarity. Another significant weakness lies in the viability section; many crucial details have been relegated to the appendix. This makes it difficult to fully understand and trust the framework based on the main text alone. Additionally, while the concept of ""self-healing machine learning"" is compelling, the section dedicated to it could be more concise and focused. Overall, improving the clarity of these sections would greatly enhance the paper's impact.

Limitations:
Authors mention a limitation in section ""SHML’s success relies on accurate root cause identification and finding effective adaptation policies which could pose challenges in some complex, real-world settings"" (Sec. 5.1).
This could be further explored.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Model performance degradation on unseen data is a classic problem. Existing approaches solve the problem through a deterministic strategy: change model, retraining, etc. This paper proposes an adaptive way to decide the action after model degradation automatically and introduces a self-healing framework. The evaluation thoroughly analyzes the intuition of SHML and its limitations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Autonomous healing for model performance degradation is an important problem.
2. The automatic adaptation idea is novel and interesting.
3. The evaluation is strong and thorough, which covers the details to help readers further understand the scope and the logic of the proposed method.

Weaknesses:
1. The writing is hard to follow in section 3. What is the practical meaning of f? How is it related to the policy? 
2. The assumption of the requirement for optimal adaptation actions is strong. Is there any practical case to support this assumption?

Limitations:
Limitations are well-discussed in the main text.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
i3me9bCSCy;"REVIEW 
Summary:
This paper proposes SNE, or Set-based Network Encoding, a method of encoding the weights of arbitrary neural networks in order to predict properties such as performance, generalization gap, training loss, number of epochs, etc.  Specifically, using a chunking mechanism as well as layer-wise and block-wise positional encodings to generalize across different types of neural networks. Transferability experiments are conducted where a predictor is trained on one network model zoo, then evaluate on another, demonstrating an ability few predictors have. Additional experiments and ablations are performed, producing convincing results. Finally, the paper is decently well-written, but has a few fixable presentation flaws.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The contribution of the paper, using a network's weights to determine attributes about it, are quite novel. 
- The Methodology section is quite clear and easy to follow. Specifically, the aggregation mechanism in Eq. 9 is similarly analogous to graph aggregation in GNNs. 
- The experimental results are generally convincing.
- Some visualization of the network representations from different methods are provided.
- There are ablation results included in the appendix.
- A limitations section is provided, highlighting reasonable restrictions on the experiments.

Weaknesses:
- First, the motivation of this paper, specifically lines 29-30, is quite weak and fails to really provide examples of the application of this research other than asserting that they must exist.
- Overall, the presentation is a weak spot, and should be revised. Some examples: 
	- E.g., in line 31, the authors mention ""Implicit Neural Representations (INR)"" - what are these? No citation is provided. By contrast, ""b) the performance of CNNs"" is very easy to understand.
	- ""modelzoo"", this should be ""model-zoo"" [1] or ""model zoo"" [2-4].
	- Citations in the paper have issues. For NeurIPS its generally just [num], or if you want to be fancier, ""Author et al., Year [num]"" but not ""Author et al. [num]"" without the year.
	- Float captions need work, e.g., Tabs 2/3 need longer captions to fully explain the performance metric Kendall's Tau, etc. Should be longer than 1 sentence.
	- Figure 2 needs work. The plots should be bigger and legend is clipped.
	- NeurIPS checklist should be placed after the references and supplementary.
- The evaluation is done on simpler datasets/networks, however, the limitations section highlights why.

Limitations:
Yes, limitations and future work is discussed in App. B. The authors explain that their experiments are on simpler/smaller/older networks as they have limited computational resources. I find this to be an acceptable justification.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work introduces a set-based neural network encoding that processes each chunk of an input model independently to predict network properties. The proposed model, SNE, is trained using Logit Invariance instead of weight tying to maintain generalizability to unseen architectures. Unlike previous approaches, SNE can be trained and tested on different architectures and it demonstrated superior performance in out-of-distribution experiments involving both datasets and architectures.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
SNE is more general than its predecessors and can be applied to any input architecture. Results indicate that it outperforms other encoding methods.

Weaknesses:
While SNE shows improvements in novel tasks such as cross-architecture and cross-dataset scenarios, its comparison on traditional tasks is limited to predicting INR frequencies. Expanding Table 1 to include SNE's performance on additional outputs, such as accuracy, would enhance the paper.

Limitations:
Limitations were properly addressed in appendix.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work tackles an original and interesting challenge: predicting neural networks properties from their trained weight values. To do so, the authors propose to leverage set to set and set to vector transformations in order to encode weight values. Furthermore, they propose to account for the deep neural network architecture and operation order through the addition of positional encoding at several stages of the property predicting model. Ultimately, they anticipate the problem of various model sizes with a padding/chinking mechanism.
The proposed method is then evaluated on small scale datasets in a cross-dataset fashion against method of the field, published in recent years.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Such research studies can have a significant in other fields. For instance, a good accuracy estimator from model weight values can be leveraged as a proxy objective in DNN quantization or weight pruning.
The approach is well motivated and the solutions to the highlighted challenges are sound.
The paper is well written and simple to follow.
The authors provide a thorough evaluation and ablation study of the proposed method and its components.

Weaknesses:
From my understanding of this work, I identified two major drawbacks:
1. the authors insist on the generalization across different model architectures and tasks. From my perspective, the former is the most important one. However, as it stands, there is little variation between Arch1 and Arch2 as presented in this study: both are simple CNNs. I wonder if the authors could provide numbers w.r.t transformer architectures against CNNs.
2. Since these experiments can be expensive (require multiple training of DNNs to create the predictor training set), it would be interesting to measure the impact of the dataset size w.r.t the predictor's performance, in order to estimate how far the community is from being to reasonably work on large scale datasets such as ImageNet.

As it stands, I believe this work aims at providing empirical results which are not fully convincing yet. I look forward to the authors' response and other reviews and will update my rating accordingly.

Limitations:
The authors made a relevant attempt at highlighting the limitations of their work

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The study introduces Set-based Neural Network Encoding (SNE) without weight tying, allowing for encoding network information into a compact form. SNE utilizes Logit Invariance Regularization to ensure the correct functional equivalence of network permutations, departing from traditional weight tying methods. The choice of Set-to-Set and Set-to-Vector functions, including attention mechanisms and pooling layers, are crucial in implementing the SNE framework. Experimental results demonstrate the effectiveness of SNE in predicting generalization performance for Convolutional Neural Networks (CNNs) and Incremental Neural Networks (INRs). SNE is compared with various baselines such as MLP, Deepsets, HyperRep, and others, showcasing its superiority in certain aspects.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- New evaluation benchmarks of cross-dataset and cross-architecture evaluation.

- Solid empirical performance. SNE significantly outperforms HyperRep by very large margins in the cross-architecture task. Also, SNE demonstrates true agnosticism to architectural choices compared to NeuralGraph, and shows robust performance without being trained on the training set of a specific model zoo.

Weaknesses:
- The proposed method did not ensure weight-space equivariance by design but use a way that is more like an augmentation plus a consistency loss. The theoretic contribution is therefore a bit weak.

- The padding and chunking designs look tricky and it is not clear for me how they help with the performance.

- Fig 1 is not really clear. (1) the legend is not sufficient to understand the computation diagram presented in the figure. (2) the input is shown as neurons but from the method I think it should be the weight matrix that connecting neurons in adjacent layers.

Limitations:
The limitations of the method are discussed in Appendix B

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

";1
wlcm21C4nk;"REVIEW 
Summary:
Recent research indicates that rate-coding is crucial for information representation in deep Spiking Neural Networks (SNNs) trained via Backpropagation Through Time (BPTT). Building on this insight, a new training strategy called rate-based backpropagation has been developed to leverage rate-based representations, reducing the complexity of BPTT. This approach focuses on averaged dynamics to simplify the computational graph, thereby lowering memory and computational requirements. Theoretical and empirical analyses demonstrate that this method closely approximates BPTT's gradient optimization, maintaining comparable performance while surpassing other efficient training techniques. This advancement is poised to enable more scalable and resource-efficient SNN training, particularly in environments with limited resources.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.	The paper is very well written and documented.
2.	The contributions have been discussed comprehensively.
3.	The experiments have been conducted on multiple benchmarks.

Weaknesses:
Some important details (such as the top-level algorithm of the proposed rate-based backpropagation method and details of the experimental setup) are reported in the appendix, while, due to their importance, they should be moved to the main manuscript.

Limitations:
The limitations and societal impacts have been discussed in Appendix D.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel rate-based backpropagation method for spiking neural network (SNNS) training, which effectively separates the time-dependent backpropagation (BPTT) process and thus reduces computational and memory costs. The method employs a rate-encoded approximation to capture the basic information and is validated by empirical experiments on various datasets, demonstrating that it is superior in terms of training efficiency and accuracy when compared to the traditional BPTT.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Empirical results on multiple datasets (CIFAR-10, CIFAR-100, ImageNet, CIFAR10-DVS) support the theoretical claims and ensure accuracy while reducing memory and time costs.
2. The paper is well-written, clearly explaining the proposed method, theoretical underpinnings, and experimental validation.

Weaknesses:
1.	In lines 53-55, this paper mentions that the proposed method reduces training time, but there is no relevant experimental proof in the experiments section.

Limitations:
The authors fully explain the limitations and potential social implications of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work falls into the category of efficient SNN training methods. This paper proposes a reduced computational graph to reduce the memory and computational demands of SNNs training. This work has the potential to train SNNs on resource-limited devices. The paper evaluates the methods on CIFAR-10, CIFAR-100, ImageNet, and other datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper addresses the issue of high time and memory costs in training spiking neural networks. 

This paper provides solid theoretical insights into the error bound and its relation to SNN BPTT training. 

The results of this work are comparable to the performance of the BPTT counterpart.

Weaknesses:
Not a clear comparison of the differences with existing e-prop methods in terms of methodology. 

No generalization results on hyperparameters (e.g., $\lambda$) are presented in this work. I raise this question because most work on SNNs uses large values of $\lambda$, but this work used 0.2 as $\lambda$.

Limitations:
See weakness and questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a rate-based SNN training method, which can effectively reduce memory and time cost during training. They proved the efficiency of the rate-based back-propagation training and demonstrate that the rate-based training outperforms other back-propagation methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The rate-based method achieves better performance and uses less computing resource compared with BPTT, which is impressive.

This paper is well-written and well-organized.

Weaknesses:
The novelty is weak. There are two previous works that share similar idea with this paper, since they all use rate-based backpropagation [1,2]. The author needs to briefly explain the differences between these papers.

The rate-based backpropagation is not suitable for sequential tasks.

[1] Li, Yuhang, et al. ""Differentiable spike: Rethinking gradient-descent for training spiking neural networks."" Advances in Neural Information Processing Systems (2021).
[2] Bu, Tong, et al. ""Rate gradient approximation attack threats deep spiking neural networks."" Computer Vision and Pattern Recognition (2023).

Limitations:
See in weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

";1
r5nev2SHtJ;"REVIEW 
Summary:
The paper focus on recover human-interpretable concepts from observation. It proposes a concept based representation learning method, which relax causal notions with a geometric notion of concepts. Experiments on synthetic data, multimodal CLIP models and  large language models supplement their results and show the utility of their approach.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.The authors hope to find a middle ground where they can simultaneously identify a smaller set of interpretable latent representations, which is an interesting idea.
2.This work can be interpreted as a new direction for identifiable representation learning in order to study when interpretable concepts
can be recovered from data.
3.Experiments on synthetic data, multimodal CLIP models and large language models supplement the results and show the utility of the approach.

Weaknesses:
1.This approach sacrifices causal semantics. This can be particularly problematic in situations where a deep understanding of causality is crucial, such as in root cause analysis, where the goal is to identify the fundamental reasons behind a problem or an event. Without causal semantics, one might only address the symptoms rather than the core issues, leading to temporary or ineffective solutions.
2.The author has made numerous assumptions within the article, which could potentially impact the universal applicability of the theory presented.

Limitations:
The authors adequately addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a theory to identify latent concepts from model representations. In contrast to previous work in the concept-based models' field, concepts are expected to be linearly represented in the model representations, and a linear transformation A is associated with such concepts. The paper is theory-driven and presents rigorous results for the identifiability of such concepts. The authors also present some experiments related to the theory, one of which verifies in a synthetic setup the validity of the proposed theory, one on CLIP and one on LLMs.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
### **Originality** 

The paper proposes a new perspective on concept-based learning by providing also identifiability guarantees. The notion of concepts as linear subspaces in the representation space is interesting although not entirely new, see [1]. The results on identifiability are new and potentially useful for follow-up works: it is a valid idea to learn such concepts from conditional distributions. 

### **Quality and Clarity**

The paper is of high quality, providing new important results for identifiability of latent concepts, and addresses the important problem of learning high-level concepts from data. The presentation is somehow clear, although it can be improved.

### **Significance**

Bridging identifiability in (causal) representation learning and concept-based learning is an open problem. The main contribution by the authors is indicating a viable route to achieve it by relating with theory. It also can be related to visual-language models, like CLIP, that are typically learned without any concept supervision but, due to interpretability concerns, it is often of interest to know whether concepts are linearly represented and used by the model. The theory seems solid and is of interest for advancing research in concept-based models.


[1] Concept Whitening for Interpretable Image Recognition, Chen et al. (2020)

Weaknesses:
## Major 

### **Experiments**

One thing that is particularly dangling in the paper is the experimental section and the supposed evidence in support of the theory: 

1) The experiments on CLIP and LLMs seem to me unrelated to the theory devised by the authors and rather support on one hand that concepts can be found in the representations in CLIP to some extent (which was previously observed, e.g. [2,3]), on the other hand, that it is possible to improve the steering of LLMs predictions with some matrix operation rather than vector addition. How is this related to the theory the authors propose? How is it that CLIP training aligns with the assumptions that would lead to the identifiability of latent concepts? LLMs like LLaMA or GPT are next-token predictors learning with a different objective, see [4], how are even these models related to the theory proposed?

2) It would have been more useful to provide experimental evidence of the proposed contrastive learning method on semi-synthetic datasets like MPI3D or Shapes3D. 
 Some real-world datasets are of particular interest to the community in concept-based models, being more tackling, like CUB200 and OAI [5], CelebA [6], and many more [7,8].

3) The synthetic experiment is impossible to understand from the main text and many details concerning the data, the learning procedure proposed (which is also very detailed), and the metrics are confined to appendices rendering it necessary to consult them in length. Nonetheless, it seems that under the working assumptions the model trained in a contrastive manner on the synthetic data and environments captures the right latent concepts. How do you evaluate $A^e$? 

### **Assumptions**

It would be beneficial to present the assumptions and discuss the intuition behind them. Assumption 1 is fine and is common in studying identifiability, but if the aim is to identify concepts, it seems unnaturally restricting to consider only invertible functions $f$. One could hope to extend the results also to non-invertible functions. However, this is not a serious limitation given the novelty of the results.  \
Assumption 2 takes the concepts to be linearly independent, what happens if for some of them is not the case? \
Assumption 3 requires a Gaussian distribution for the noise, it is remarked that other distributions work as well (which distributions?) but there is no citation. \
How should Assumptions 4 and 5 be understood, what data are expected to be collected? Are these expensive to obtain in practice? They seem to presuppose a lot of knowledge on what latent concepts should be identified, which may not be the case when concepts are not known apriori.


## Minor

### **Conditional distributions in practice**

It is a bit puzzling how data should be gathered for the theory to work, and thus the scaling of the proposed method. The synthetic experiment offers a proof of principle but it does not show how the model behaves in settings where more concepts (and structured ones, like the color of an object) and conditional distribution are to be considered.  


### **Related work**

Causal Representation Learning seems more of an inspiration to the paper rather than having a tight connection to the theory. The authors consider conditional distributions for latent concepts and the proof techniques are inspired by the iVAE work (2019), so there is no clear link to what should be the causal aspects that should be taken into consideration. 

It seems more that works about identifiability in Causal Representation Learning become relevant if one wants to extend authors' theory to causal variables, not being essential to support their claims and the connection. 

I was also expecting to see a comparison to Concept-based models' current practices, which require dense supervision of the concepts in cases [1, 7, 8],  partial [6], or language-guided [2,3]. Some approaches aim to learn concepts only by leveraging supervision on the classification task, see [9], and seemingly related work on continuous latent variable identification [10]. 

## Summary

The experiments do not complement the theory and dilute the message by showing two post-hoc analyses on CLIP and LLMs. I struggle to see how both constitute valid evidence for the theory proposed. On the other hand, additional experiments on known semi-synthetic datasets or real ones would highlight the extent of the theory to the community in concept-based interpretability. 

The presentation of the material also requires some clarification around the assumptions the authors make and their validity in practice.

[2] Label-free Concept Bottleneck Models, Oikarinen et al. (2023) \
[3] Language in a Bottle: Language Model Guided Concept Bottlenecks for Interpretable Image Classification, Yang et al. (2023) \
[4] On Linear Identifiability of Learned Representations, Roeder et al. (2021) \
[5] Concept Bottleneck Models, Koh et al. (2020) \
[6] GlanceNets: Interpretable, Leak-proof Concept-based Models, Marconato et al. (2022)  \
[7] Concept embedding analysis: A review, Schwalbe (2022) \
[8] Concept-based explainable artificial intelligence: A survey, Poeta et al. (2023) \
[9] Provable concept learning for interpretable predictions using variational autoencoders, Taeb et al., (2022) \
[10] Synergies between Disentanglement and Sparsity: Generalization and Identifiability in Multi-Task Learning, Lachapelle (2023)

Limitations:
All assumptions are natural limitations of the proposed theoretical results. It is not clear whether foundation models are trained under the conditions that the author found for assessing identifiability of the concepts
and the connection to LLMs seems a bit weak.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work takes a step toward learning human-interpretable concepts while relaxing the restrictions of (interventional) causal representation learning, and they do so inspired by the linear representation hypothesis.

The authors claim that learning the generative process and the ""true"" causal factors $f^{-1}, Z$ from observations $X$ using interventions has important caveats: 1) One needs many interventions ($\Omega(d_z)$) for identifiability of $f^{-1}, Z$ which might be too much of a requirement in many cases, 2) There's no reason that a priori such latent representations are interpretable, 3) Interventions in many examples might not be possible at all 4) One might not need the whole $f^{-1}, Z$, and there are cases where we can seek only a handful of interpretable concepts for an application without learning the full encoder and latent representation.

The authors then introduce the geometric notion of concepts as linear subspaces in the latent space of $Z$. This is inspired by the abundant evidence on the linear representation hypothesis. Based on this notion, they define the concept conditional distributions as a source of supervision for learning concepts which will replace interventional distributions as the source of supervision for learning causal representations $Z$. Concept conditional distributions are simply defined by filtering the dataset with samples that are $\textit{perceived}$ to satisfy a concept (see eq. 1)

The problem then becomes whether given an observational distribution $X^0$ and a set of concept-conditional distributions $X^1, \dots, X^m$ corresponding to $m$ concepts, one can identify the linear subspaces $A^mf^{-1}(x)$ corresponding to those concepts.

The main theorem then proves the identifiability (according to definition 4) of those concepts given linear independence of concepts, as well as some diversity constraints on the environments.

The authors then try to validate the claim using 3 experiments:
1) Synthetic experiments with various linear and non-linear mixing $f$, and different dimensions for $Z,X$.
2) Evaluating the linearity of the valuations of the concepts learned via multi-modal CLIP (inspired by the similarity of CLIP objective to their contrastive algorithm
3) Showing that concepts can be used to steer LLM outputs.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- I find this work original and novel. There have been attempts at learning concepts recently, but from my understanding (as well as the author's mention of the related work) such attempts have been limited to specific domains, while the work at hand seems to be addressing that challenge in a broad way. 
- Moreover, I believe they have nicely translated the linear representation hypothesis to concept learning, and relaxing the restrictions of interventional causal representation learning is an important endeavor (see questions though) as is also nicely motivated in multiple places in the paper.
- I find the theoretical result insightful and important; not only does this work move away from interventions as a not-so-ideal tool, but also clearly demonstrates the theoretical advantage of concepts vs. interventions (only theoretically though).
- The experiments touch upon different modalities showing the versatility of the claims.
- The presentation and arguments are generally well-constructed (up until page 8)

Weaknesses:
- In the synthetic experiments, I was expecting to see large $d_z$ and small $n$ to match the claims that had been made earlier as to the advantage of concept conditional distributions, but the dimensions are quite small. I can see that they show a proof of concept, but still, it would have been nice to be consistent with the claims made earlier (unless there's a reason why the authors didn't do so)

- Could the authors think of any experiment to contrast concept learning to CRL? Maybe with simple datasets like CLEVR or 3d-shapes? Isn't there a way to try to learn causal representations and concepts, and show empirically that one is easier to be achieved? I understand that the premise was that $Z$ is not always interpretable in the first place, but I think there would exist situations where it will be. If I understand correctly, do the authors think such an experiment would add to the empirical evidence for their method?

- I'm also a bit unclear about where this work is taking us, and would have liked it if it was explained better. In particular, are we hoping to change our representation learning towards learning concepts? If so, are you proposing this for reasoning tasks? For alignment? Interpretability? Or what else? The reason I'm asking is that wouldn't we probably still need some causal representations in some reasoning tasks, say in vision? Basically a short discussion of when we would prefer concepts over causal representations (or else) would be helpful.

----
Writing and Clarity:

- The notion of environment in the context of concepts appeared out of the blue on page 7 (same with its notation that followed)
- Not a weakness of the method - but the learning method seems like an important component of the paper which is deferred altogether to the appendix, i.e., one would learn about the identifiability, but there is no mention of the actual method to learn such (identifiable) concepts which might hide the difficulties and challenges associated with it.



Please also see the questions.

Limitations:
See questions.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors argue the shift from causal representation learning (CRL) to concept-based representation learning since the current CRL framework relies on strong requirements such as interventional datasets and stands far from realistic, practical use-cases. The paper formalizes the notion of concepts and establishes a theoretical foundation on the identifiability of concepts. The experiments demonstrate the utility of the framework.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The motivation is convincing and the framework is novel. It also provides a rigorous foundation for the notion of concept and its identifiability.
- The writing is clear and easy to follow. The paper provides a thorough literature review which makes it very helpful to understand the paper positioning and key contributions.
- Experimental results on CLIP and LLMs are interesting. It supports the paper’s motivation to move from CRL to concept-based representation learning.

Weaknesses:
Datasets $X^e$ from each environment is associated with different concept $C^e$ and corresponding valuation $b^e$. The proposed method using contrastive learning requires how the dataset is partitioned into each environment, i.e., $X^0, \cdots, X^m$. This implies that the framework is naturally more useful for **discrete** concepts (i.e., discrete valuation), as showcased in the experiments where the authors use discrete labels. However, as the motivation suggests, concepts could be continuous in many cases (e.g., intensity of the color). Therefore, I have doubt on the practical utility of the proposed framework since it cannot handle such continuous concepts. In other words, the requirement of data partition $X^0, \cdots, X^m$ goes against the motivation of the proposed framework of handling continuous concept valuations.

Limitations:
.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

";1
