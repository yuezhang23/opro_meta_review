id;text;label
DBz9E5aZey;"REVIEW 
Summary:
The authors propose a implementable stochastic approximation algorithm based on Stein Variational Gradient Descent, which the idea is based on a stochastic approximation at the probability measure level.  The efficiency and convergence analysis of the proposed algorithm, i.e. VP-SVGD and GB-SVGD,  are presented. A better convergence rate for the proposed algorithm is proved given a weaker assumption for the potential function

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
The authors propose a new algorithm, a stochastic approximation at the probability measure level according to the authors, to study the SVGD. The focus is to improve the convergence rate and make the algorithm more efficient. The idea of the proposed algorithm in (3) seems to be new. The intuition and design of the algorithm is interesting, and it seems to work as shown in the numerical experiments. 

Weaknesses:
The reviewer feels that the intuition and the theoretical results have some gaps. To be more precise, some of the arguments are only explained at the intuition level without rigorous proof. It might be possible the reviewer missed the key point of the proof, if this is the case, please correct me. 

Limitations:
see above

Rating:
5

Confidence:
3

REVIEW 
Summary:
This study analyzed the finite particle behavior of Stein Variational Gradient Descent (SVGD) in an asymptotic manner. For that purpose, the authors designed two computationally efficient variants of SVGD, namely VP-SVGD and GB-SVGD, and prove their convergence rates. In those methods, the authors introduced a new concept of ""virtual particles"" and develop a new stochastic approximation of the population-limited SVGD dynamics in the probability measure space. The proposed algorithm shows a significant improvement of the convergence rates under  looser conditions than the usual SVGD.

Soundness:
4

Presentation:
2

Contribution:
3

Strengths:
- The convergence rates in this paper are significantly better than existing methods.

- The idea of virtual particles introduced for the analysis is very novel and very interesting in itself.

- Numerical experiments, although simple,  suggest that the proposed method is indeed an improvement over existing methods.

Weaknesses:
- Notation in Sec 4 is very confusing, especially about $l, s, t$

-  In the explanation of Sec 4, only the case of K=1 is clearly stated, and I had to spend a lot of time thinking about what happens when K>2. I think an additional explanation of K > 2 is needed in the Appendix.

- The algorithm of GB-SVGD is difficult to understand. In particular, the third line mentions ""l"", but ""l"" does not appear at all in the fourth line. This may be because the ""l"" in the third line may be a mistake for ""s"".

- I am not sure if the theoretical comparison of the convergence order with the existing analysis is justified since the VP-SVGD is no longer the original SVGD. The GB-SVGD is close to the existing SVGD, but it is also difficult to compare with the existing SVGD because of the approximation errors as shown in Theorem 2.

Limitations:
Theoretical limits are discussed in detail.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper studies Stein Variational Gradient Descent (SVGD) which is a popular variational inference algorithm that simulates an interacting particle system to approximately sample from a target distribution. The authors propose two variants of SVGD, Virtual Particle SVGD (VP-SVGD) and Global Batch SVGD (GB-SVGD), that differ in their implementation details and computational efficiency. Both variants use the so-called ""virtual particles"", which are additional particles that evolve in time but are not part of the output, to compute information about the current population-level distribution of the real particles and enable exact implementation of the stochastic approximation using only a finite number of particles. The use of virtual particles allows for faster variants of SVGD with provably fast finite-particle convergence.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
This paper introduced two variants of SVGD, VP-SVGD and GB-SVGD, that use virtual particles to enable the exact implementation of the stochastic approximation using only a finite number of particles. The authors present a non-asymptotic analysis of the convergence of VP-SVGD and GB-SVGD to the target distribution, which demonstrates a double exponential improvement over the best-known finite-particle analysis of SVGD. The paper proposes a new algorithm, VP-SVGD, that achieves provably fast finite-particle convergence and is computationally more efficient than ordinary SVGD.

The paper is well-organized and clearly written, with detailed explanations of key concepts and technical details that make it accessible to a wide range of readers. Literature review of related work on SVGD and its variants was conducted comprehensively, which helps to contextualize the proposed algorithms and their contributions.

I went through the proofs of Theorem 1 and Corollary 1 or 2 in Section E in detail, and believe they are technically sound. The KSD metric satisfies the triangle inequality where the upper bound is in two additional parts. In (34), the KSD metric satisfies a triangle inequality, and the first term (probability kernel vs population limit) is polynomial with a polynomial decaying exponent that is independent of dimension $d$ ($d^{1/3}T^{-1/6}$ when subgaussian). The second term appears dominating, where the polynomial decaying exponent in Wasserstein-1 distance is $\Theta(1/d)$, which is due to Lemma 8 by Lei [29].

Weaknesses:
Despite its significant contributions, this work assumes a certain level of familiarity with the concepts of variational inference and particle methods and is relatively specialized and may make it difficult for readers who are not already familiar with or not working on the topic of SVGD and its variants to follow.  Despite the technical contributions of proposed SVGD-style algorithms and their convergence properties, the authors did not provide a detailed discussion of the limitations of the proposed algorithms or potential avenues for future research, which may limit their impact and relevance in the long term it and make it less accessible to readers who are more interested in the broader implications of the work. Lastly, the work does not provide sufficient details on the proposed algorithms' empirical evaluations of real-world datasets, which limits the ability to assess their practical usefulness. These weaknesses did affect my current rating, but I reserve the possibility to raise the score if any of the above points can be resolved in the authors' rebuttal.

Limitations:
The work is purely theoretical and admits no negative social impacts.

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper propose two variants of stein variational gradient descent (SVD) named as Virtual Particle SVGD (VP-SVGD) and Global Batch SGVD (GB-SVGD) with provably fast finite-particle convergence rates to a target distribution. Introducing the notion of virtual particles, the authors show that VP-SVGD is an exactly implementable finite-particle stochastic approximation of population-limit SVGD dynamics in the space of probability measures. Viewing VP-SVGD as a specific random-batch SVGD algorithm, the authors argue that  the output particles from GB-SVGD after T time steps with batch-size K are as good as sampling iid from a distribution whose Kernel Stein Discrepancy (KST) to a sub-gaussian target distribution is at most $O(d^{1/3}/(KT)^{1/6})$. Furthermore, under mild growth conditions, the authors show that the empirical distribution of the n-output particles from both VP-SVGD and GB-SVGD converges to the target in KSD at a rate of $O(d^{2}/(n)^{\Theta(1/d)})$. The authors note that this is a doubly exponential improvement over the state-of-the-art finite-particle analysis of SVGD under weaker assumptions. Finally, the authors provide numerical experiments to show fast convergence of GB-SVGD which is computationally more efficient than SVGD.








Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
1)Originality: This paper is introduces two new variants of SVGD appealing to a novel construction involving countably many virtual particles.  Although the analysis involves infinitely many particles, a finite-number of particles exactly suffice for the actual algorithm VP-SVGD. This enables the authors to claim that VP-SVGD performs a finite-particle stochastic approximation of the population (infinite-particle) dynamics of SVGD over the space of probability distributions. Finally, they show VP-SVGD and the computationally efficient GB-SVGD behave very closely.

2)Quality: I think this paper is technically quite solid. The theoretical claims are all sound and the assumptions are reasonable.

3)Clarity: This paper is well written. It is easy to follow the main ideas including the problem statement, technical challenges, relevance to past works, and the contributions.

4)Significance: It is quite significant to improve the SOTA result doubly exponentially under weaker assumptions. Besides, the virtual particle method is worthwhile studying by itself ( I am not aware of a similar method discussed somewhere else).

Weaknesses:
It wasn't easy for me to parse some of the theoretical results, which can be expected as this paper is theory heavy.

A table of past works comparing the results in terms of assumptions, method, and the convergence rate might be a helpful addition. 

Limitations:
Limitations and future works are discussed.

Rating:
8

Confidence:
3

";1
9NzC3PjpAt;"REVIEW 
Summary:
The authors present a new reinforcement learning objective, dubbed Reinforcement Learning from Human Gain (RLHG), that explicitly incorporates an understanding of human performance with an intervention into the objective function. They show that training with this added component improves outcomes in a MOBA, both on the overall objective of winning and on subobjective related to satisfaction.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
They present an important research problem (ML systems working as collaborators) and make a good attempt at overcoming it.
They did experiments with human participants.
The domain they trained models in is non-trivial and showing results shows good ability


Weaknesses:
I don't believe the main claim. The authors say that by explicitly adding their more complex models of human wants/behaviours they can get better performance, but they don't compare against a model that attempts to optimize for those things directly. A perfect RL agent that has correct values for the different objectives should be able to learn the optimal policy without using their proposed more complicated methods. They only test against a model that is trained to win, and a model that is trained on short term value optimization. How does the method compare to another model trained with similar data and objectives? The lack of comparison makes this feel like they shows that PPO and deep learning work, not that there new methods are better.

Limitations:
The authors do not explain why their decomposition of the loss/training loop is _theoretically_ better than any other, they only provided some _empirical_ evidence to suggest it.
The authors only discuss the RL literature in their framing/discussion. In other domains, such as recomender systems, the issue of over optimizing for a single metric to the detriment of other objectives is a major area of research. Consider looking over some data science papers from KDD as a starting point

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper proposes a new method called Reinforcement Learning from Human Gain (RLHG) to effectively enhance human goal-achievement abilities in collaborative tasks with known human goals. The paper evaluates the RLHG agent in the widely popular Multi-player Online Battle Arena (MOBA) game, Honor of Kings, by conducting experiments in both simulated environments and real-world human-agent tests.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
The problem setting considered by the paper is tightly connected to some real-world problems (e.g., assistive agents in MOBA games). 
Experiments are performed in a real-world application (Honor of King).

Weaknesses:
It is difficult to evaluate the major contribution of the paper (the two-step training process) because
1) the major contribution of the paper, as the authors claimed in the paper, is orthogonal to many of the complications in the paper (e.g., multiple agents, multiple goals, partial observability). These complications are not contributions and they make it hard to understand the contribution of the paper clearly. Maybe the authors added them because their experiments are in Honor of Kings?
2) probably because the paper focuses too much on these complications, the paper fails to explain why RLHG is a good idea and provides clear evidence. For example, why do the authors propose estimating primitive human performance rather than primitive human+agent performance? What is making estimating primitive human performance helpful? Can this idea be used in environments without humans?
3) the writing of the paper is unsatisfying. I was completely lost when reading the paper. Please see Questions for my questions about the paper.
4) in experiments, the new method achieved a worse winning rate compared with the baseline method. I can understand this performance drop given that there are improvements regarding other metrics. What should I learn from this indeterminate result?

Limitations:
N/A

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors propose Reinforcement Learning from Human Gain, an RL algorithm that explicitly optimizes for enhancing human abilities in cooperative human-AI settings. Given a predefined set of human goals, the main approach first learns a value network to estimate the primitive human performance at achieving said goals. Then, a secondary Gain network is trained to estimate the enhancement the human return receives under interactions with the cooperative agent. The cooperative policy is trained with a combination of a traditional agent value network and the proposed human gain-based value network. The authors test the RLHG framework in a cooperative game and find that across experiments with real humans, the RLHG agent is preferred over an agent without the human gain objective, despite having a lower overall win rate.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The problem setting is interesting and relevant, and the proposed solution of optimizing for human gain is intuitive — it makes sense that a cooperative agent should account for improvements in human behaviours, rather than having the agent directly optimize for its own reward or what it perceives human rewards to be (which may reduce human enjoyment and overall autonomy in a task, as noted by the authors).
- The experiments use a complex multiagent task and test both human models and real human participants. The results show a significant improvement in human preference for the RLHG agent across the predefined goals as well as various subjective metrics.

Weaknesses:
- The method seems sensitive to the choice of partner $\pi$ while collecting the primitive human episodes. If the initial partner is already very good, will its success be attributed to the human? And vice versa -- if the partner is very bad, would that lead to a false representation of the base human skill? The paper would be strengthened with additional studies on how sensitive the method is to different pretrained policies.
- The method relies on knowing the set of human goals beforehand. It would be interesting to have additional analyses of how the method is affected in the case where some of the goals are missing or misspecified, which is more representative of a real-world scenario.

Limitations:
The authors adequately discuss the limitations of this work. Additional discussion on societal impacts would be helpful, as these agents are trained to interact with people directly.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper focuses on the fine-tuning of a pre-trained agent to assist and enhance the performance of a given human model in achieving specific goals. The authors assume access to a human model and a pre-trained agent. The authors propose a two-step approach.

1. The human model's initial performance is determined by training a value network to estimate its effectiveness in goal attainment using episodes generated through joint execution with the agent.

2. The is trained agent to learn effective behaviors for enhancing the human model's performance using a gain network that estimates the improvement in human return when compared to the initial performance.

The algorithm is evaluated on a Multi-player Online Battle Arena (MOBA) game.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The idea of developing algorithms to assist humans to solve tasks is interesting and of great practical interest, and this paper makes headway in this direction. The authors conduct extensive experiments which includes evaluating using real players to test their algorithm in a game.

Weaknesses:

1. The algorithmic contribution in this paper appears to be relatively modest, as it mainly builds upon the existing Proximal Policy Optimization (PPO) approach. The authors introduce a gain function, which essentially computes an advantage by comparing it to another state-dependent baseline ( $V_\phi(s)$)
 
2. The assumption of having a human model is justifiable; however, the strong reliance on assuming knowledge of human goals, in my opinion, limits the direct applicability of this research (as acknowledged in the limitations section).

3. There is a lot of notational ambiguity in the paper (Section 2.2), which makes reading a little hard. For example, 
 
3a. Advantage function generally depends on state/observation and action. In this setting, the Advantage is independent of both. 

3b. Is V value of a state or the infinite horizon discounted reward? It is unclear as its used in both contexts. 

3c. G is used as return-to-go, which should be a state dependent function. 

4. I do not understand the exact need for the gain network. What would happen if in line 12 of the algorithm, you drop the - Gain(s) part? This essentially means that you are computing advantage with respect to the human primitive baseline. 

5. The authors have invested significant effort and computational resources in conducting their experiments, making it extremely challenging to reproduce or recreate such experiments due to the demanding compute requirements. Although this does not diminish the value of their work, it would be beneficial if the authors could incorporate simpler environments into their experimental setup. This addition would aid in evaluating the algorithm's performance and further validate its quality.

Limitations:
The authors discuss limitations and future work. 

Rating:
6

Confidence:
3

";0
wjqT8OBm0y;"REVIEW 
Summary:
In this paper, the authors formally define five anomalies for an
explainability score and prove that for every n >= 4, there exist
Boolean classifiers defined over n features that exhibit one or more
of these anomalies for the SHAP score. In this way, the authors
provide evidence of the inadequacy of Shapley values for
explainability.

The aforementioned anomalies are defined by considering the concept of
abductive explanation. More precisely, given a binary classification
model M : {0,1}^n -> {0,1} and a tuple v in {0,1}^n, a subset X of {1,
..., n} is said to be a weak abductive explanation of (M,v) if for
every y in {0,1}^n such that y[i] = v[i] for every i in X, it holds
that M(y) = M(v). In other words, the values of v for the features in
X are enough to obtain the same result as M(v), so they are enough to
explain the output of M for v. Moreover, a subset X of {1, ..., n} is
said to be an abductive explanation of (M,v) if X is a weak abductive
explanation of (M,v), and there is no weak abductive explanation X' of
(M,v) such that X' is a proper subset of X. In other words, X is an
abductive explanation for (M,v) if X is a minimal weak abductive
explanation for (M,v). Then a feature i is said to be relevant for
(M,v) if there exists an abductive explanation X of (M,v) such that i
belongs to X, and otherwise i is said to be irrelevant for (M,v). With
this notion of irrelevance, the anomaly I5 for the SHAP score is
defined as the existence of a feature i such that i is irrelevant for
(M,v), but the absolute value of the SHAP score of i is greater than
the absolute value of the SHAP score of every other feature. Thus,
this can be considered as an anomaly of the SHAP score, as i is an
irrelevant feature that is considered more relevant according to the
SHAP score that all the other features (some of which are
relevant). The other four anomalies considered in the paper (I1, I2,
I3, I4) are defined in a similar fashion.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
1. The five notions of anomaly studied in the paper clearly represent
anomalies for explainability scores. These notions are properly
formalized in the paper.

2. The paper provides valuable insights into the SHAP score,
specifically providing a formal framework to assess its adequacy as an
explainability score.

3. The paper provides one of the first formal results of the
inadequacy of Shapley values for explainability.

4. The paper is well written.

Weaknesses:
1. The results of the paper show that a tiny proportion of the Boolean
classifiers defined over n features exhibit some of the anomalies I1,
I2, I3, I4 or I5. For example, the paper proves that at least
2^{2^{n-1} - n - 3} Boolean classifiers exhibits anomaly I1, which is
a tiny proportion of the 2^{2^n} possible Boolean classifiers defined
over n features. Hence, it could be the case that the vast majority of
Boolean classifiers do not exhibit the anomalies studied in the paper.

2. In practice Boolean classifiers are given in some specific
formalism, such as decision trees or binary decision diagrams. The
authors do not provide any results about the formalisms that are
suitable to express the Boolean functions exhibiting anomalies. For
example, is it possible to express the Boolean functions in the proofs
of Propositions 3, 4, 5 and 6 as decision trees of polynomial size in
the number n of features? If this is not possible, can these functions
be expressed as FBDDs (or d-DNNFs) of polynomial size in the number n
of features?

Limitations:
The following are the main limitations of this work (see Weaknesses), 
which are not addressed in the paper. 

- The results of the paper show that a tiny proportion of the Boolean
classifiers defined over n features exhibits some of the anomalies I1,
I2, I3, I4 or I5. 

- The authors do not provide any results about the practical
formalisms (such as decision trees) that are suitable to express the
Boolean functions exhibiting anomalies. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper reviews previous work on ideas of feature importance and hi-lighted inconsistencies with Shaley values. It defines ideas of importance and irrelevance of features in a Boolean ML model. These definitions are based on the idea of a minimal set of inputs needed to freeze an model output. necessary inputs are in every minimal coalition that can freeze the output, relevant inputs are in at least one minimal coalition, and irrelevant inputs are in no coalitions. They then go on to show that, among other issues, there exist Boolean models and certain inputs where irrelevant inputs are given large Shapley values, while relevant inputs are given a Shapley value of zero. Thus, the logic goes, Shapley values do not track importance.

The paper's original contributions are to prove that model/input pairs with issues exist/can be found for models of any input size. Previously, only small models were exhibited to have these issues, but it was unknown if larger models also had these issues. They also give lower bounds on the number of models that have these issues.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Generally clear and straightforward exposition.
- Good background and presentation of previous results.
- Results are easy to understand.
- idea of necessary, relevant, and irrelevant is intuitive.



Weaknesses:
- Paper is based on a comparison of apples to oranges, without an in-depth analysis of the issue. It is possible that the whole paper is based on a misunderstanding. Further analysis is needed.
- Some grammatical issues.
- Contributions are not very significant.

Limitations:
The author has not discussed the limitations of the claim that Shapley values are refuted. This statement seems not entirely supported. See questions.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper demonstrates / constructs functions with features whose Shapley values (i.e., attributive importance in a prediction) is misaligned with their true relevance.

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
- Addresses a theoretical gap in our understanding of Shapley values.

Weaknesses:
- I find the problem being investigated to be mostly a mathematical curiosity that so happened to be open and has now been addressed. 

Limitations:
n/a

Rating:
3

Confidence:
3

REVIEW 
Summary:
Based on definitions of feature necessity, relevancy, and irrelevancy from previous work,as well as systematic issues with Shapley values for explainability on boolean classifiers (e.g. non-zero Shapley values assigned to irrelevant features, zero Shapley values assigned to relevant features, among others) identified in previous work, the authors offer proof for their existence in functions with an arbitrary number of variables. They conclude that the existence of such systematic issues is cause for concern in using Shapley values for explainability, as misleading information about feature importance can induce errors in human decision making.
 

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
- Originality: The work offers proof for the existence of issues with Shapley value explanations on boolean functions with an arbitrary number of variables that were previously only studied empirically.
- Quality and Clarity: The theoretical framework, preliminaries, and proofs are described in a very concise manner. Despite the theoretical nature of the paper, the authors are able to concisely state to the reader what is described in each formula (e.g. lines 125-127: Thus, given an instance (v, c), a (weak) AXp is a subset of features which, if fixed to the values dictated by v, then the prediction is guaranteed to be c, independently of the values assigned to the other features). Similarly, the main idea for each proof is described in a very intuitive manner, increasing readability of the paper significantly.
- Significance: The present work proves systematic issues exhibited by Shapley value explanations on boolean functions. Shapley values are one of the most popular solutions, as they are based on clearly defined axioms, i.e. properties deemed desirable for explanations. For boolean functions, the present work shows that these axioms (which Shapley values do fulfill) may be lacking for treating irrelevant and relevant features as would be expected.

Weaknesses:
I am a bit concerned with the novelty, as the present work only provides proof for observations about unexpected behavior of Shapley value explanations for boolean functions that were already observed empirically in previous work (however, the authors also state themselves that these issues have been identified empirically in previous work). To raise concern about e.g. I1, it would be sufficient to simply identify a case where irrelevant values are assigned nonzero Shapley values. 

I also believe the title promises a bit more than is provided by the paper. The proofs and resulting claims are restricted to boolean functions, however, it would be interesting to see how and if the described issues occur in continuous settings, e.g., when explaining DNNs.

Limitations:
restriction to boolean functions, as described in ""Weaknesses"" section. I think a paragraph of how the described proofs and observations may impact Shapley value explanations in more real-world settings would go a long way here, as well as suggestions on how to mitigate the proven issues.

Rating:
6

Confidence:
4

";0
xxfHMqNcum;"REVIEW 
Summary:
The authors propose a novel feature selection algorithm, aiming to detect interactions between features at instance-level, contrary to the usual feature selection algorithms that selects the same features for every sample. Initially, the authors propose a highly memory-demanding approach, requiring an $m \times m$ matrix, being m the number of different values the features can take. Later, a less memory-demanding approach is presented, using matrix decomposition. The experimental results are slightly better to the state-of-the-art.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- **Quality:** Several DSN methods were included in the state-of-the-art section. The proposed algorithm is able to obtain very similar results.
- **Clarity:** The paper is easy to follow and to understand. The decisions made are clearly motivated.
- **Significance:** The idea of selecting, per each sample, the most important interactions between features is very interesting and it can provide a good explanation about the decision making.

Weaknesses:
- **Originality:** The algorithm is a combination of well-known techniques. The innovative part is focused on how to merge all of them.
- **Quality:** There exists other field of methods that also address feature interaction: the so-called *dynamic feature selection'. Techniques like L2X [1] are focused on the same goal, without the need of using DSNs, which highly reduces the memory consumption. Some information regarding these techniques should be included in the paper.
- **Clarity:** Fig. 3 is clearly misleading. Although it constantly suggest the proposed method outperforms the state-of-the-art, the granularity of the y-axis is almost non-existent. There are very little differences amongst all algorithms.
- **Significance:** I have concerns regarding two critical aspects of the experimental results:
    1. The experimental results show very little improvements against the baseline methods. An statistical analysis is mandatory, in order to establish whether the obtained results provide a real improvement over the state-of-the-art or not.
    2. Although I agree with the authors that feature interaction selection can provide insightful information regarding the decisions provided by the network, the authors do not mention anything related to this in the experimental section.

[1] Chen, J., Song, L., Wainwright, M., & Jordan, M. (2018, July). Learning to explain: An information-theoretic perspective on model interpretation. In International Conference on Machine Learning (pp. 883-892). PMLR.

Limitations:
Not applied.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This work proposes a hybrid-grained feature interaction selection approach for deep sparse networks, which targets both feature field and feature value. The proposed approach uses a decomposed space that is calculated on the fly to explore the expansive space of feature interactions. The work also introduces a selection algorithm called OptFeature, which efficiently selects the feature interaction from both the feature field and the feature value simultaneously. The proposed approach is evaluated on three large real-world benchmark datasets, and the results demonstrate that the proposed approach performs well in terms of accuracy and efficiency. The work concludes that the proposed approach can effectively select feature interactions in deep sparse networks, and it has the potential to improve the performance of prediction tasks with high-dimensional sparse features.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The hybrid-grained feature interaction selection approach goes beyond traditional field-level selection, and the decomposed space and sparsification-based selection algorithm make the work appear to be a cutting-edge method to some extent.
2. This work ran the repetitive experiments with different random seeds five times and reported the average value for each result, and provides information about the parameter setup, metrics, datasets, baseline and parameter setup, so the experimental results appear to be reliable.

Weaknesses:
1. Novelty: The proposed approach that targets both feature field and feature value levels and introduced a decomposed space and a sparsification-based selection algorithm to explore the selection space, which appears to be a novel contribution to the field. But it does not provide a comprehensive review of related Feature Interaction Selection work in Section 2, and the novelty is not so obvious.

2. Experiments: What GPU was used in this work? How many were used? Were all the experiments conducted on the same GPU? Why formulate the hybrid-grained feature interaction selection as a binary selection according to Equation 6? Taking either 0 or 1 doesn’t seem to reflect ‘hybrid’. 

3. Writing: the introduction does not summarize the main contributions of this work, so readers cannot intuitively get the advantages of this work. In addition, the content of Section 2.1 introducing Neural Architecture Search seems to be not very relevant to this paper. Furthermore, Section 3.3.2 does not explain how to determine the parameter α in Equation 6, which makes one wonder how to choose between value-grained and field-grained.




Limitations:
See the weaknesses.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper tackles the problem of modeling fine-grained feature interactions in high-dimensional sparse features.
A hybrid-grained feature interaction selection method is proposed, which operates on both field and value for deep sparse networks.
To handle the increase in computation, a decomposed form of the selection space is done, which greatly reduces the computational requirements of modeling.
Results on deep sparse networks benchmarks show that the proposed method achieves SOTA results while being more computationally efficient.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- Strong results in terms of performance on established benchmarks and computational efficiency, demonstrating the effectiveness of the proposed method.
- The proposed method seems generalizable and can be applied to other methods.
- All experimental parameters are provided, making reproduction straightforward.
- The writing is fairly clear and easy to understand.

Weaknesses:
- Experimental results:
  - The proposed method is a simple tensor decomposition for improved efficiency and the additional consideration of more features. Such choices are generalizable to other architectures (as mentioned in lines 109-112) but this is not demonstrated in the paper. I would like to see the application of the proposed components to other existing approaches.
- Significance of results:
  - The AUC and Logloss scores differ by less than 0.001 between the proposed method and the previous SOTA. Is this significant? I suggest the authors add confidence intervals to Table 1 and 2 for easier comparison.

Limitations:
The limitations are discussed at the end of the paper.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper introduces a hybrid-grained feature interaction selection approach that targets both feature field and feature value for deep sparse networks and decomposes the selection space using tensor factorization and calculating the corresponding parameters on the fly. 


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Extending the selection granularity of feature interactions from the field to the value level.

Introduce a hybrid-grained feature interaction selection space, which explicitly considers the relation between field-level and value-level. 

The tensor decomposition and the sparsification are combined to perform selection on the shrinking space.


Weaknesses:
1.The evaluation datasets are pretty small (the feature number is around 11-26). For recommendation systems optimization work, it is usually better to show the results in large-scale datasets like industrial datasets to demonstrate the scalability and performance.


2.Missing several references:

AutoFAS，

NAS-CTR，

AutoIAS,

GAIN: A Gated Adaptive Feature Interaction Network for Click-Through Rate Prediction

Maybe adding some discussions or comparisons to them is better. 





Limitations:
Yes, the authors discussed some limitations.

Rating:
6

Confidence:
4

";1
qHrZszJSXj;"REVIEW 
Summary:
This paper proposes the use of nonmonotone line search methods to speed up the optimization process of modern deep learning models, specifically Stochastic Gradient Descent (SGD) and Adam, in over-parameterized settings. The proposed method relaxes the condition of a monotonic decrease in the objective function and allows for larger step sizes. The authors introduce a new resetting technique that reduces the number of backtracks to zero while still maintaining a large initial step size. The proposed POlyak NOnmonotone Stochastic (PoNoS) method combines a nonmonotone line search with a Polyak initial step size. The paper proves the same rates of convergence as in the monotone case. The experiments show that nonmonotone methods outperform the rate of convergence and also generalization properties of SGD/Adam.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- **Originality:** The use of nonmonotone line search methods to relax the condition of a monotonic decrease in the objective function is a stochastic generalization of [Zhang and Hager 2004] which was proposed initially for deterministic optimization. The initial step size is chosen on the basis of previous work [Vaswani et al 2019]. The paper also introduces originally a new resetting technique that reduces the amount of backtracks to zero while still maintaining a large initial step size. Overall, the paper's originality is a significant strength. 

- **Quality:** The paper provides rigorous proof that the proposed nonmonotone line search method has the same rate of convergence as in the monotone case despite the lack of a monotonic decrease. The experiments show that nonmonotone method has a larger speed of convergence and better generalization properties of SGD and Adam. Computational time comparison experiments also show the outperformance of the proposed method. The theory is solid and the experimental results are strong.

- **Clarity:** The paper is well-written and easy to understand, with clear explanations of technical terms and concepts. Qualitative explanations of the theorems are provided to help the readers understand the main messages. The authors provide detailed descriptions of the proposed method and the experiments conducted to evaluate its performance. Comparisons with other methods are presented clearly.

- **Significance:** The proposed method shows the outperformance of existing state-of-the-art algorithms in both computational time and generalization properties. 

Weaknesses:
- The proposed method includes many parameters to be chosen artificially, such as $\eta_{\rm max}$, $c$, $c_p$, $\delta$, and $\xi$. Although the ranges of them are provided in the theorems, influences on the performance of the proposed method due to different choices of these parameters are not clear. Are the specific values used in a real experiment not so important? If so, to what extent?

Limitations:
The limitations of the proposed method are stated not sufficiently. Only future perspectives are stated. For example, considering the local PL assumption. The claims that the proposed method outperforms many other the-state-of-the-art methods from several perspectives are quite strong. Are there any drawbacks, or points to be improved, of the proposed method?

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a non-monotonic line search method for choosing step sizes in stochastic optimization. Convergence rates are proved for strongly convex, convex, and PL functions, and the rates match those of previous work. Experimental results show that (1) for MLPs and CNNs, the proposed algorithm outperforms SGD, Adam, and previous line search methods, and (2) for kernel models and transformers, the proposed algorithm outperforms SGD and previous line search methods, and is competitive with Adam.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
1. The question is significant. Given the observations of the ""edge of stability"" and non-monotonic decreases in loss when training deep networks, it seems natural that incorporating non-monotonicity into line search methods may yield significant performance improvements.
2. The presentation is clear and easy to follow.
3. The theoretical results (Theorems 1, 2, 3) can recover convergence rates from previous work.
4. The experimental evaluation is very broad, covering many datasets and neural network architectures.

Weaknesses:
1. The proposed algorithm appears to be a direct combination of existing techniques (non-monotonic line search with Polyak initial step size). While this isn't necessarily a problem in itself, as a result the technical novelty of the paper is not very high.
2. The theoretical results recover the previous convergence rate, but they do not exhibit any improvement over baselines. Recovering the previous convergence rates is natural and the proofs don't appear to contain any new techniques. Therefore, the theoretical contribution is not significant.
3. The main text contains no information about the tuning procedure for baselines or for the proposed algorithm, and the appendix contains very little information about hyperparameters. It's uncertain whether the experimental comparison is fair, and since the theoretical results do not exhibit improvement over baselines, the experimental performance is the only substantial contribution. Some previous baselines require additional hyperparameters (e.g. SPS with $\gamma$), but there is no assurance that this parameter was properly tuned. It is also difficult to see whether PoNoS was tuned more extensively than baselines, which would of course not be a fair comparison.
4. Evaluation for RBF kernel models and transformers does not compare by wall-clock time, and does not include results for test loss. Since PoNoS is competitive with Adam when measured by epochs, it is natural to assume that PoNoS lags behind Adam in terms of wall-clock time, which begs the question whether existing line search methods are useful for training Transformers. Line 344 says that only the training procedure is considered (following previous work), but this is not completely satisfying to me. If we compared test performance for MLPs, and CNNs, why not also for RBFs and Transformers?

Limitations:
The authors include some discussion of limitations and future work in the conclusion, though it would be nice to see some more discussion of the weaknesses of the proposed method instead of just directions for future work. Discussion of potential negative societal impact is, in my opinion, not necessary for this paper.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper presents a non-monotone line search method for optimizing over-parameterized models. The method is equipped with some theoretical support for strongly convex, convex and the PL condition. Furthermore, experimentally, the method is shown to have favorable performance when optimizing various deep learning models of practical interest.

==> post rebuttal: increased score from 4 to 5.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Obtaining convergence results with non-monotone line search strategies appears to be a novel contribution - though, I don't know if some variant of this result has appeared in existing literature dealing with non-monotone line search.

Weaknesses:
- The proposed approach seems incremental compared to existing approaches.
- The theory doesn't adequately capture why the proposed method outperforms existing approaches. The bounds suggest identical convergence rates as ones that use monotone line search methods. This suggests all these bounds are fairly worst case (loose upper bounds) that do not help quantify why these methods work well in practice in the first place.

Limitations:
Yes.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This submission proposed a new linear search method to ensure convergence without the monotone decrease condition of the (mini-)batch objective function. The method is quite suitable for the modem DNN training, which prefers the larger training learning rate.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
- The explanation of motivation is very clear.
- The related work has been extensively discussed.

Weaknesses:
- Discussing the difference between the proposed and the previous methods is inadequate, especially since the existing methods inspire some steps.
- The theoretical benefits of the proposed method are not shown/discussed in the convergence rate results. The proposed method seems to share a similar rate with the previous results. However, since PoNoS prefers a larger learning rate, its rate at least can demonstrate the advantage of a constant level.
- There is no numerical comparison in the main part, and just having the curve figures cannot fully display the results.

Limitations:
See weakness.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper proposes a new line search method for determining the step size in SGD within the interpolation regime. In contrast to the previous approach called SLS, which relies on the monotonically decreasing Amijo condition, the proposed method adopts the non-monotone Zhang & Hager line search. The authors establish the convergence guarantees for the proposed Stochastic Zhang & Hager line search when an upper bound is placed on the initial step size, considering strongly convex, convex, and PL problems. Additionally, they introduce several enhancements to improve empirical performance: (1) utilizing the Stochastic Polyak Step (SPS) to set the initial step size of the line search and (2) introducing a new resetting technique to reduce the number of backtracking steps.








Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Numerical experiments are extensive and the performance of PoNoS looks quite promising. It is great to see a provably convergent optimization algorithm (with other heuristics) working well in large-scale experiments while incurring only a minor computation overhead.
- The proof techniques look new and interesting. 

Weaknesses:
- There are certain expressions within theorems that may lead to confusion. I have elaborated on these concerns in the ""Questions"" section of my review.
- The Polyak step and the resetting technique are only heuristics. It would be valuable to provide further analysis regarding how these heuristics influence the convergence properties of the algorithm.

Limitations:
N/A

Rating:
8

Confidence:
3

REVIEW 
Summary:
The paper presents a proposed Polyak nonmonotone stochastic (PoNoS) method which combines a nonmonotone line search with a Polyak initial step size. It builds on the work of Vaswani et al. [2019] by modifying the monotone line search to incorporate a nonmonotone approach. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Originality:
The introduction of nonmonotone line search applied to Deep Learning is noteworthy.
Clarity:
The paper is generally well-written. However, the authors should emphasize their specific methodological innovation and contribution to the conclusion section to highlight the significance.

Quality: The overall quality is good. The paper demonstrates the outperformance of the proposed methods over other methods in regard to runtime.

Significance:
The experiments show the advantages of the methods in terms of convergence speed and runtime when applied to Deep Learning. 

Weaknesses:
The authors admitted the adverse impact on convergence speed of the proposed method. This matter should be further investigated to have a more comprehensive intuition of the robustness of the method.

Additionally, the authors should conduct a further comparison with the state-of-the-art line search approaches and the based model of Vaswani et al. [2019]
?a Figurethis

Limitations:
The authors do address some of the limitations of their work.

Rating:
5

Confidence:
3

";1
G6yq9v8O0U;"REVIEW 
Summary:
This paper proposes a factorized tensor network (FTN), that adds task/domain-specific low-rank tensors to shared weights, to perform lightweight MTL/MDL. Specifically, the low-rank tensors can form a weight additive to the shared backbone weight so that each task can have its own parameters to achieve good accuracy.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is easy to follow.
- The basic idea of adding additional task-specific low-rank vectors is clear to understand.

Weaknesses:
- About the method:
    - My high-level concern about this idea is not truly an MTL/MDL work in my understanding.   
    If I understand the proposed method correctly, it is equivalent to fine-tuning a completely shared feature extractor on each task. After fine-tuning, we can get the weights offset $\Delta W = W_t - W$ between the fine-tuned weights $W_t$ for task $t$ and the original backbone weights $W$. The low-rank tensors are just a parameter-efficient way to store $\Delta W$ by three separate vectors.
    What I believe is that the proposed method is still a traditional transfer learning method with some low-rank storage design for the weight difference.

    - If it is an MTL work, it fails to obtain the two basic advantages of using MTL.
        - First, in MTL, we believe a good parameters-sharing may improve the generalization ability, i.e., the task accuracy, when training multiple tasks jointly. However, in this paper, the shared backbone weights will be frozen (as shown in Figure 1d) when training the additional low-rank tensors for each task, which means there is no truly joint training/parameter sharing across the multiple tasks. As I described in the first bullet point, each task is just fine-tuned from the shared backbone and has no communication with other tasks during training, and the final model weights for each task will be independent to each other at the end.
        - Second, the wide adaptation of MTL relies on its advantage of lower computation cost and latency. However, the proposed method in this paper has no latency saving since all the tasks still have their own parameters and have to execute sequentially even if they share the same input images.

    - About the method itself, I feel it lacks good motivation. There is no reason why we can have three low-rank tensors to get $\Delta W$ for each task in each layer. The authors may consider adding some literature that can support this choice, or have some ablation studies. For example, what will happen if we use the full-rank $\Delta W$ directly? How about using PCA or SVD on $\Delta W$ to get low-rank matrics?
    
    - More importantly, this paper has a very similar idea as [1]. The only difference is the choice of low-rank representation.    
    [1] Hu E J, Shen Y, Wallis P, et al. Lora: Low-rank adaptation of large language models[J]. arXiv preprint arXiv:2106.09685, 2021.

 - About the literature:
    - Missing papers and comparisons with adaptor-based MTL methods. Adaptor-based MTL is very close to this paper, in which task-specific adaptors are parameter-efficient and plug-in modules on top of an all-shared feature extractor. For examples,    
    [2] Rosenfeld, Amir, and John K. Tsotsos. ""Incremental learning through deep adaptation."" IEEE transactions on pattern analysis and machine intelligence 42.3 (2018): 651-663.     
    [3] Zhao, Hanbin, et al. ""What and where: Learn to plug adapters via nas for multidomain learning."" IEEE Transactions on Neural Networks and Learning Systems 33.11 (2021): 6532-6544.

    - Missing recent papers in MTL/MDL.    
    [4] Zhang, Lijun, et al. ""Rethinking hard-parameter sharing in multi-domain learning."" 2022 IEEE International Conference on Multimedia and Expo (ICME). IEEE, 2022.     
    [5] Zhang, Lijun, Xiao Liu, and Hui Guan. ""Automtl: A programming framework for automating efficient multi-task learning."" Advances in Neural Information Processing Systems 35 (2022): 34216-34228.
    
 - About the experiments:
    - The authors may want to compare with adaptor-based MTL methods, which are the most related works to this paper.
    - It's necessary to have more experiments for MTL, including more baselines, more backbone models, and more datasets. More baselines could be the representative and new methods like MTAN and AutoMTL, rather than just RA and ASTMT. Backbone models could be some efficient backbone like MobileNet or EfficientNet, rather than just ResNet variants. Datasets could be CityScapes and Taskonomy, rather than just NYUD.
    - AdaShare is a typical MTL model. It's weird to compare it in MDL setting only.

Limitations:
The social impact is not mentioned either in the main paper or the supplementary materials.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper presents the Factorized Tensor Network (FTN) as a solution to the challenge of learning multiple tasks/domains using a single unified network. The authors claim that FTN achieves comparable accuracy to independent single-task/domain networks with a smaller number of additional parameters. The method incorporates task/domain-specific low-rank tensor factors into a shared frozen backbone network. Experimental results on multi-domain and multi-task datasets demonstrate the effectiveness of FTN.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. `Efficient Approach for Knowledge Transfer`: The Factorized Tensor Network (FTN) proposed in the paper offers an effective mechanism for transferring knowledge across multiple tasks and domains. By incorporating task/domain-specific low-rank tensor factors into a shared frozen backbone network, FTN allows for efficient utilization of existing knowledge, reducing the need for extensive retraining or independent models.

2. `Minimal Parameter Requirement`: FTN demonstrates the ability to achieve comparable accuracy to independent single-task/domain networks while requiring a smaller number of additional parameters. This parameter efficiency is advantageous when dealing with a large number of tasks or domains, as it helps reduce storage costs and computational complexity, making the approach more scalable and practical.

3. `Strong Validation Results`: The paper presents experimental results on widely used multi-domain and multi-task datasets. These results showcase the effectiveness of FTN, demonstrating similar accuracy compared to single-task/domain methods while utilizing only a small percentage (2-6%) of additional parameters per task. The validation experiments provide empirical evidence supporting the proposed approach and its potential for practical application.

Weaknesses:
1. `Limited Novelty`: The paper falls short in highlighting the novelty and distinctiveness of FTN compared to other parameter-efficient tuning methods, especially KAdaptation [A] and SSF [B] . This missed opportunity leaves readers wondering about the specific advantages and unique contributions of FTN. A more comprehensive analysis of its novelty would have evoked excitement and clarified its standout features.

2. `Limited Comparison`: The paper fails to sufficiently contrast FTN with other parameter-efficient fine-tuning (PEFT) methods, such as adaptors and low-rank approximation (LORA). This omission leaves unanswered questions about FTN's comparative strengths and weaknesses. A thorough evaluation against these methods would have added depth and fostered confidence in the proposed approach. Additionally, the paper's baseline for multi-domain image translation appears relatively weak, limiting the scope of the evaluation. 

3. `Overlooking Recent Trends in Modular Learning`: The paper overlooks recent advancements in modular learning highlighted in references [C] and learning factorized knowledge with different modules [D]. 

[A] He X, Li C, Zhang P, et al. Parameter-Efficient Model Adaptation for Vision Transformers[C]//Proceedings of the AAAI Conference on Artificial Intelligence. 2023, 37(1): 817-825. 

[B] Lian D, Zhou D, Feng J, et al. Scaling & shifting your features: A new baseline for efficient model tuning[J]. Advances in Neural Information Processing Systems, 2022, 35: 109-123. 

[C] Pfeiffer J, Ruder S, Vulić I, et al. Modular deep learning[J]. arXiv preprint arXiv:2302.11529, 2023. 

[D] Yang X, Ye J, Wang X. Factorizing knowledge in neural networks[C]//European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022: 73-91.

Limitations:
The author should discuss some of the limitation of FTN in the paper

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper tackles multi-task learning (MTL) and multi-domain learning (MDL). The author proposes to a general method to add task/domain-specific weights to the common shared backbone. Specifically, they add the factorized task/domain specific tensors which introduce very low storage cost for each task and domain while maintaining the original performance. They experiment in three different settings: multi-task learning with dense-prediction tasks, multi-domain learning for image classification and multi-domain learning for image generalization. They also ablate on the relationship between the rank of factorization and the accuracy. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
(1) The paper is well written and easy to understand.

(2) The experiment section is well designed and they show the effectiveness of their proposed methods in three different scenarios including multi-task learning / multi-domain learning, deterministic and generative tasks. 

(3) The design of the proposed method is reasonable, easy to implement but effective to preserve the performance of each task/domain and save the storage cost. It scales well with more tasks and domains

Weaknesses:
The paper is based on the convolutional network and the author uses big gan in the generation tasks. These designs are somehow in the old fashion. Even though it is mentioned in the conclusion that the method can be extended to transformer based architecture, it is lack of experimental proof.

Limitations:
They have included limitation in the main paper.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper tackles the problem of adapting a single pretrained network to multiple tasks or domains while introducing as little parameters as possible. More specifically, first a backbone model is trained which contains a shared backbone. Then,  the proposed method (Factorized Tensor Networks, **FTN**) is applied to further adapt the shared backbone to each task with a few added parameters. All task specific parameters are trained alongside each other (i.e. the BatchNorm parameter, the low-rank FTN parameters as well as the task heads)

Given a weight $W$ from the shared backbone, FTN adds low-rank parameters $\Delta W_t$ for each task/domain $t$ such that the forward pass for task $t$ uses the shifted weights $W + \Delta W_t$. Setting the rank $r$ of the low-rank parameters allow to control the trade-off between parameter efficiency and model capacity. In addition, FTN also tune batch norm layers parameters (scale and bias) for each task independently.

The proposed FTN is compared to previous methods on adapting weights during finetuning (e.g. residual adapters) on mainly three benchmarks:
- Multidomain classification from a pretrained ImageNet model to the 5 domains in ImageNet-to-Sketch
- Multidomain classification from a pretrained ImageNet model to the 6 domains in DomainNet
- Multitask from a pretrained ImageNet model to the 3 domains in NYU

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* The proposed method is simple to implement and compared to other adapter methods it is extremely parameter efficient

* The method is experimented on both multi-domain (different inputs) and multi-task (different outputs) settings

* While the method is only evaluated on convolutional networks, there is no strong assumptions on the backbone architectures, hence it could also be used on transformers for instances 

Weaknesses:
- **Missing important related work**: As far as I can tell, the proposed FTN is quite similar to the low-rank finetuning method from ""LoRA: Low-Rank Adaptation of Large Language Models"". While LoRA is only evaluated on NLP/transformers in the original paper, it should be mentioned and discussed in related work

- **Unclear how the number of parameters is computed** The original $BA^2$ paper reports a parameter cost of ""1.03x"" in their Table 3. While this paper reports a cost of ""3.8x [1.71x]"". It seems like the baselines results reported in this paper are directly taken from the Table 1 in the  ""Task Adaptive Parameter Sharing for Multi-Task Learning"" paer:   It would be nice to clarify how baselines evaluation was conducted and where the discrepancy may come from

- **Missing results from Visual Decathlon**: Manyworks on adapters report results on Visual Decathlon introduced in the original residual adapters work where an ImageNet pretrained model is adapted to a wide variety of very different downstream tasks 

Limitations:
Limitations are explicitly mentioned in an independent paragraph and fairl ydescribe some drawbacks of the methods.
Although I partially disagree with the statement that *(line 73) ""The proposed method does not affect the computational cost because we need to compute features for each task/domain using separate functional pathways""*: This is fine for the MDL applications, however most MTL works operate under the ""feature -extractor"" paradigm instead, where the multiple tasks can be solved with a single forward pass on the fully shared backbones, followed by lightweight task heads: In comparison, FTN is much less efficient as it requires individual forward passes of the encoder for each task.

Rating:
6

Confidence:
3

";0
2z8noau98f;"REVIEW 
Summary:
A long withstanding problem in offline RL is the distribution shift issue, i.e., the query of the action values for out-of-distribution state-action pairs.
This paper proposes to consider the mutual information (MI) between states and actions.
Specifically, the authors view state and action as two random variables, and constrains the policy improvement direction by the state-action MI.
For the practical implementation, the authors introduce the MISA lower bound of state-action pairs and adopt MCMC techniques to construct an unbiased gradient estimation for the proposed MISA lower bound.
The authors also unify TD3+BC and CQL under the proposed MISA framework.
Empirically, the proposed method performs well on several datasets from the D4RL benchmark.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The paper is clear and easy-to-follow in the derivation of the MISA lower bounds.
2. The practical implementation and hyperparameter choice are clearly discussed.
3. The discussion on the connection with TD3+BC and CQL is informative.

Weaknesses:
1. The discussion is generally vague on why the proposed regularization method MISA is better than the prior approaches. The authors do discuss it in the paper, such as in the paragraph titled ""Intuitive Explanation on the Mutual Information Regularizer"". Those discussions are, however, general subjective and hard-to-follow. Some examples of the vague statements includes: 
    * ""directly fitting the policy on the dataset is short-sighted"" (what is ""short-sighted"" and why?), 
    * ""optimization direction"" (is it the gradient?), 
    * ""make sure in-distribution data have relatively higher value estimation"" (not sure where does this statement come from).

In Section 4.4, the author also mention that the propose method can ""give a better mutual information estimation"" (and thus better than CQL). But why does a better estimation of MI lead to better policy performance?

2. Experimental results: Both Table 1 and Table 2 (main table) do not contain the error bar. This make it hard to judge the significance of the improvement over prior methods.

3. The proposed method is highly-related to well-established methods in bounding/estimating the MI and KL (e.g., the $f$-divergence and DV representation of KL). The novelty of the proposed method is therefore less shining, but obviously is not a major weakness. 
4. The paper may need re-organization so that the algorithm box for the main algorithm (Algo. 1) can show up in the main paper. For example, is MISA-$f$ really needed since in Eqn. (10) MISA is based on the DV representation?

Limitations:
I do not find a discussion of the limitations in the paper.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors propose a new offline RL method called MISA. Similar to prior work in offline RL, MISA constrains the learned policy to lie within the offline data manifold and does so by maximizing a lower bound on the mutual information between states and actions in the dataset. The authors consider three different bounds on lower information and generally find that a tighter bound leads to better performance. The authors connect their work with prior methods in offline RL. They evaluate MISA on a wide variety of different environments and find that MISA generally achieves superior performance compared to prior methods.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- The paper is well-motivated and theoretically sound, deriving three different lower bounds on mutual information between states and actions.
- The authors connect their work with prior work in offline RL (TD3+BC, CQL).
- The experimental evaluation is very thorough. The authors run experiments on a large number of environments and compare to a large number of baselines. The authors also conduct informative ablation studies on factors such as choice of mutual information lower bound, biased vs. unbiased gradient estimation, and number of Monte-Carlo samples.


Weaknesses:
- To support the authors' claim in line 272 that tighter mutual information bounds lead to better performance, it would be nice to show a plot of numerical values of the different mutual information estimates (Ba, MISA-$f$, MISA-DV, MISA) to see if the bounds in line 274 hold in practice.
- Are the values of $\gamma_1$ and $\gamma_2$ in Equations (12) and (13) specified anywhere? Are these hyperparameters that need to be tuned for each environment? Or is the proposed method robust to choice of $\gamma_1$ and $\gamma_2$. I'm curious how the authors trade off maximizing the RL objective and the Mutual Information objective, and how different choices of $\gamma_1$ and $\gamma_2$ affect performance.

Limitations:
yes

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper integrates two distinct methods in the offline RL domain: the KL regularized method and the conservative Q-learning method. It achieves this by incorporating mutual information in both the value loss function and the policy loss function. To accurately approximate the mutual information between states and actions in the offline dataset, this paper employs the Donsker-Varahdan representation. The experimental results demonstrate the effectiveness of the proposed MISA algorithm and highlight how accurate approximation of mutual information can significantly enhance performance.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. This paper introduces a significant novelty by combining two distinct offline RL algorithms, namely the KL regularized method and conservative Q-learning method. This is achieved by introducing mutual information regularization terms in both the value loss function and policy loss function.
2. The experimental results demonstrate that the proposed MISA algorithm significantly improves performance across various environments.

Weaknesses:
1. performance in offline settings, but this may not always be the case. For instance, as derived in this paper, BA is a lower bound for MISA-f and MISA-DV. However, the results presented in Figure 1 do not support this statement, as BA outperforms MISA-f and MISA-DV in most cases within the MuJoCo medium-replay environments.
2. To achieve a better approximation of the mutual information, it is crucial to find $T_\psi (s,a)$ that maximizes the right-hand side of the Donsker-Varadhan representation (as outlined in Lemma 3.2). Providing results using such optimized $T_\psi (s,a)$ would strengthen the validity of this statement.

Limitations:
All limitations are addressed.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors of this paper introduce a novel framework called MISA, which aims to optimize the lower bound of mutual information between states and actions in the dataset to direct the policy improvement. They provide a theoretical explanation for MISA's superior performance over CQL and empirically demonstrate that MISA attains state-of-the-art results on D4RL when compared to different baselines.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The motivation behind this study is logical and sound.
- MISA successfully integrates TD3+BC and CQL, and subsequently deduces an improved variant from a theoretical standpoint.
- The experiments conducted within this study are extensive and thorough, and MISA achieves SOTA on D4RL.

Weaknesses:
- It appears that there is a confusion between the true Q function, denoted as $Q$, and the estimated Q function, represented as $\hat{Q}$, in the theoretical derivation provided by the authors. This confusion is evident in equations 5 and 6, where the update rules should have used $\hat{Q}$ instead of $Q$ (as correctly utilized in the CQL paper). Additionally, the Q function should be $\hat{Q}$ in Section 4.3. Therefore, the term $\pi^{*}_{\theta,\phi}\propto \pi_\theta (a|s)e^{Q_\phi(s,a)}$ in Line 199 doesn't hold true since $Q_\phi(s,a)$ should be $\hat{Q}_\phi(s,a )$. This implies that the ""Explanation on the Mutual Information Regularizer"" is incorrect. I may not have spotted all errors due to time constraints.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
4

Confidence:
4

";1
hNpedVWwoe;"REVIEW 
Summary:
Consider the $d$-dimensional unit sphere $\mathbb{S}^{d-1}$, and any function $f:\mathbb{S}^{d-1}\rightarrow\mathbb{R}$ defined on the sphere with bounded L2 norm $\|\|f\|\|_{\mathbb{S}^{d-1}}$. This function $f$ can be evaluated at any point $\vec w \in \mathbb{S}^{d-1}$ on the sphere, but it is expensive to evaluate $f$ so we wish to do this as little as possible.

The goal of this paper is to recover a near-optimal polynomial $\tilde f$ approximation to $f$, where $\tilde f$ is constrained to be a multivariate polynomial where the sum-of-degrees of each term is at most $q$. (For example, the sum-of-degrees of each term in $\tilde f(x,y,z) = x^2y^3 + z + x^5$ is at most 5).

Letting $\beta_{d,q}$ denote the number of free parameters in any $d$-variate polynomial with terms whose sum-of-degrees is at most $q$, this paper shows that $O(\beta_{d,q} \log \beta_{d,q} + \frac{\beta_{d,q}}{\varepsilon})$ evaluations of $f$ chosen uniformly at random on the sphere suffice to recover such a near-optimal $f$. This is proven in three parts:
1. The leverage function associated with this polynomial recovery problem is **exactly** the uniform distribution on the sphere
2. The randomized linear algebra toolkit shows this near-linear sample complexity suffices
3. A small kernel ridge regression problem can be solved to recover this $\tilde f$ from uniform random samples on the sphere

They also prove an $\Omega(\beta_{d,q})$ sample complexity lower bound. Lastly, they include some synthetic experiments.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper is a nice application of well understood tools from randomized linear algebra (RandNLA) to spherical harmonics, which has not been explored by the RandNLA literature afaik. The real core strength of this paper is its novelty in connecting these two literatures in a simple and elegant way.

Simplicity and elegance really are words that mark this paper. Almost everything is extremely clear and well written. There are basically no typos even! The flow of logic in the paper is very clear, the problem they solve seems important in the spherical harmonics literature (though I'm no expert in that domain), and the proofs are even pretty clean. To that last point, the theorems in this paper are proven either by very clean techniques that are now standard in the RandNLA literature, or by using well established classical facts about polynomials and spherical harmonics. I verified a decent amount of the math, and it all struck me as rather nice and clean.

Since I'm not an expert in spherical harmonics, I can't perfectly speak to the significance of the result. Taking the authors' words at face value, it seems that prior work in spherical harmonics did not use such simple algorithms (Kernel Ridge Regression) and did not achieve optimal sample complexities.

The paper may not carry a huge amount of new technical ideas to get their sample complexity and simple algorithm, but this result would only exist if someone who knew enough about both spherical harmonics and RandNLA decided to sit down and figure out if this all works together. For that view of novelty, in addition to the simplicity and elegance of the paper, I recommend accepting this paper.

The experiments are perfectly fine for a theoretical paper. Nothing particularly strong about them, but nothing I'm left looking for.

Weaknesses:
The novelty, simplicity, and elegance is strong. But, the proof techniques are not especially novel. The proof techniques are standard relationships between operators and algorithms as explored by the RandNLA community for a good few years now.

The only proof that doesn't seem directly tied to something already proven in the RandNLA literature is the proof that the leverage function for the regression problem is uniform on the sphere. Even then, this claim is pretty intuitive, since the problem statement is rotationally invariant. There's no reason for a sampling algorithm to care more about one point of the sphere than another. (This is in contrast to learning on an interval, where an algorithm may prefer to sample near the edges of the interval.) So, even this most novel proof is not terribly surprising, and the proof is short and simple.

That said, I hesitate to really call this a ""weakness"". This isn't clearly a downside of the paper. While it's certainly nice for a paper to overcome new technical issues and proof difficulties, it's also nice for a paper to show that spherical harmonics smoothly fit into the RandNLA framework.

All this is to say that the technical weight of the paper truly is in understanding two literatures and connecting them. The effort to connect the literatures seems to be low, but that's given that someone actually understands both literatures well. I'd say this overall comes out as a slight strength for the paper, but it's certainly a tradeoff in the reviewing process.

Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
### Result ###

The paper studies the problem of recovering a function from a finite number of noisy observations, for the class of ""square-integrable functions on the unit sphere"" (denoted by $L^2(\mathbb{S}^{d-1})$ when the sphere in question is of degree $d$).

The paper shows, by developing an algorithm, that the number of samples required for recovery up to an $\epsilon$-multiplicative error is proportional to $\beta_{q, d}$, where $q$ is the degree of spherical harmonics desired and $\beta_{q, d}$ is the dimension of the space of degree $q$ spherical harmonics on the sphere of dimension $d-1$. This sample complexity is optimal, as shown by the paper via lower bounds. 

---------------

### Broader Contributions ###

To achieve the result above, the paper makes the following conceptual and technical contributions. 

First, the problem at hand is a regression problem with potentially infinite-dimensional continuous cost functions. Inspired by the approach of Avron, Kapralov, Musco, Musco, Velingker, and Zandieh (AKMMVZ19), which discretizes this problem via the leverage scores of the regression matrix. 

Second, the authors utilize connections between spherical harmonics and zonal harmonics to enable the implementation of leverage score sampling of the regression matrix in question. This leverage score sampling result is novel. 



Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
### Result's Strength ### 

I think the optimality of the stated result of the paper makes the paper mathematically strong.  

----------------------------------------

# During the Rebuttal Phase # 

The author's explanations, repeated readings of the paper, and Reviewer BcP8's review and questions have helped me understand and appreciate the paper better. I'm therefore raising the score from 4 to 6 and confidence from 2 to 3. 


Weaknesses:
### Difficulty in reading. ### 

I found the paper quite difficult to read though I could tell it was written well. I think this is simply because of the deeply technical nature of the problem and my unfamiliarity with the topic (at least to the level of generality of this paper) within the context of machine learning. 

I am not sure if I have any concrete feedback to this end but due to this particular reason I feel this paper might be a much better fit (in terms of reaching a wide audience who'd actually understand and appreciate the results) at a mathematical/optimization journal. That said, I acknowledge, based on my unfamiliarity with the topic, that my judgement could be completely misplaced. 

I'll be grateful to the authors if they could situate their work more in the context of machine learning. I'll also be happy to keep reading the submission and improving my understanding through the rebuttal period.

----------------------



Limitations:
N/A 

Rating:
6

Confidence:
3

REVIEW 
Summary:
A technique is proposed to recover spherical harmonic expansions for functions defined on a d-dimensional sphere from a set of function evaluations.  Spherical harmonic expansions are recovered by solving an optimizaiton problem via a kernal approach, which is accompanied by theoretical guarantees.  Numerical experiments demonstrate phase transitions close to the theoretical bounds.

Soundness:
4

Presentation:
2

Contribution:
2

Strengths:
A solid theoretical analysis is presented to derive a new approach to recovering spherical harmonic expansions from the evaluation of functions on the d-dimensional sphere.  It is shown that functions should be evalued uniformly randomly over the sphere.  The approach presented is accompanied by numerous theoretical results and guarantees.  Experimental results validate the theory presented, with phase transitions in success probabilities close to the theoretical bounds.

Weaknesses:
While the method is interesting, it does not seem that NeurIPS is the appropriate venue for this work.  While the field of deep learning on the sphere is an active area of research (e.g. [Cohen et al.](https://arxiv.org/abs/1801.10130)), this contribution would appear to be somewhat orthogonal to that body of literature.  No connection is made in the submitted manuscript beyond the final, somewhat cryptic, comment: ""We believe our finding would appeal to the readership of the community"".

Furthermore, the connection to previous literature of the topic of fast spherical harmonic transforms and efficient sampling on the sphere is very poor.  No reference is made to the central works of [Driscoll & Healy](https://www.sciencedirect.com/science/article/pii/S0196885884710086) and [McEwen & Wiaux](https://ieeexplore.ieee.org/document/6006544) and the follow-up articles of their groups.

The use of the term ""near optimal"" in the title and throughout the article is somewhat overstated.  The proposed method is ""near"" optimal up to a logarithmic factor.  I would typically expect near optimal to be up to a constant factor. 

Limitations:
No special negative societal impacts.

Rating:
4

Confidence:
5

REVIEW 
Summary:
The paper studies the approximation of a function $f \in L_2(\mathbb{S}^{d-1})$ from its evaluations, via a degree-q spherical harmonic expansion. To this end, an efficient kernel regression based algorithm is proposed which recovers such a degree-q expansion of f, from the evaluations of f on $\mathbb{S}^{d-1})$. In particular, the number of evaluations needed scales nearly linearly in the dimension of the space of spherical harmonics of degree at most $q$. The main idea is to exploit connections between spherical harmonics and zonal harmonics, and the fact that the zonal harmonics are the reproducing kernels of the space of degree $l$ spherical harmonics. Some numerical simulations are provided on synthetic examples to demonstrate the performance of the algorithm.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is written well overall with a well-defined problem statement, and a clear description of related work.


2. The results hold for any dimension $d$ which does not seem to have been handled previously. As noted in the related work, previous results typically applied to small, fixed values of $d$. In that respect, I think the results are quite strong.


Weaknesses:
1.  It’s a bit unclear to me whether the results hold only in the noiseless case, or is the method actually robust to noise. For instance, if we the function evaluations are corrupted with iid centred Gaussian noise, what can be said about the recovery error? The abstract mentions that the algorithm provides robust recovery, but I am not sure if this is what is proven.

2.  While I understand the main contributions are theoretical, are there any real examples on which the method can be evaluated? At the moment, experiments are only conducted on synthetical examples.


Limitations:
I do not see the limitations discussed anywhere but this probably does not apply to this paper since it is essentially theoretical, and the sample complexity bounds are shown to be nearly optimal. Perhaps the running time which is super-quadratic in the number of samples could be improved upon?

Rating:
6

Confidence:
3

REVIEW 
Summary:
To develop kernel regression based algorithm to recover degree-q expansion of f \in L^2(S^{}d-1| - by only evaluating on uniformly sampled points on S^{d-1}.


Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The ideas used in this paper is deeply technical and ideas are complicated. It first re-formulates the problem as least squares regression and then uses the sampling based on leverage scores to decide which samples to pick. Then the rest of the paper is proving the estimated bound is satisfied.

Weaknesses:
motivate the problem - how it could be applied in real life

** after rebuttal -- thank you for providing all these papers regarding contributions of this workstream in terms of application side. I am happy to increase my score regarding soundness of the paper.

Limitations:
-

Rating:
6

Confidence:
3

";1
TrcpLUcYfL;"REVIEW 
Summary:
The present work proposes a block-wise learning strategy, whereby the architecture is split into several blocks, with each block receiving an error signal stemming from a local (block-wise) loss. As this technique makes use of a parametrized twin network to compute these error signals, it also bypasses the so-called ""weight transport problem"" of the standard backprop algorithm. The motivation of this work is the hardware-friendliness of the resulting algorithm.

More precisely:

- Section 3.1 formulates the learning objective in a probabilistic fashion (likelihood maximization) and how to do it (variational inference)
- Section 3.2 explains the parametrization of the base model $p(z|x) := \alpha(z)$ to be trained and that of the variational posterior $q(z|x, y)$. The variational posterior itself decomposes into the product of  bottom-up top-down messages: $q(z|x, y) \propto p(z|x) p(y|z) := \alpha(z) \times \beta(z)$. The base model and the variational posterior are parametrized within the *exponential family* and the neural networks are made to output the natural parameters associated with these distributions. No stochastic quantity is ever propagated through the nets -- e.g. it does not operate as a VAE for instance.
- Section 3.3 grounds the previous idea in an encoder-decoder setting where the last layer of the decoder is augmented with extra channels to output pixel-wise variances in the image space. Training this architecture on F-MNIST with the proposed approach (the learning objective is not yet introduced at this stage), it is shown that the resulting uncertainties are good proxy to the reconstruction error. 
- Section 3.4 introduces the proposed local (block-wise) learning objectives -- the derivation is in Appendices 1.2 -> 1.3.4. Heuristically: the learning rule reads as the forward/posterior mismatch backpropagated into the feedforward block (first term and second term of Eq. 7) and its feedback counterpart (second term of Eq. 7) through the natural parameters gradients. Importantly, an heuristic called ""data mixing"" is introduced to midly take into account the residual terms of the upper bound of the loss (details in Appendix).
- Section 4.1 presents results obtained on MNIST, F-MNIST and CIFAR-10 with the proposed technique on ResNet architectures, benchmarked against feedback alignment and standard backprop. The proposed approach performs comparably with backprop on MNIST and F-MNIST, slightly outperforms FA on CIFAR-10 but is considerably degraded with respect to backprop.
- Section 4.2 finally presents results on a 20-layers deep transformer architecture on a toy task (reverting a random permutation of numbers ranging between 0 and 9) where the proposed approach is shown to perform comparably with backprop. 

Soundness:
2

Presentation:
1

Contribution:
1

Strengths:
- The paper tackles an interesting problem: block-wise local training casted into a probabilistic setting.
- The idea of the paper is interesting: the starting point is the same as Predictive Coding (Whittington & Bogacz, 2017), but: i/ picking a different variational family ii/ amortizing inference with a single forward pass (rather than minimizing energy for the E-step at each batch iteration) iii/ having a whole block of layers (where backprop applies) to compute the parameters of the distributions. I like the idea of stitching several algorithms together to solve a problem. 

Weaknesses:
- The writing and the structure of the paper make it extremely difficult to deeply understand the proposed approach. To my eyes, ideas do not appear in the right order in the main, important ideas are in the appendix and the notations are confusing, important details are missing.
- It is *not* true that the algorithm parallelizes the forward and backward pass (L.40: ""forward and backward propagation can work in parallel""). The underlying algorithm is just variational inference applied to a generative model conditioned on a given input $x$. I do not see any valid (theoretically grounded) argument as to why the first block could start processing a novel input $x'$ while the upper blocks process an input $x$. **The block-wise locality of the learning rule does not suffice as an argument here**.
- In the same vein, it is *neither* true that the algorithm allows for the parallelization the backward pass *across different blocks*. Here again, as the underlying algorithm simply is variational inference, I see no valid argument as to why each (feedforward) block could do without having a top-down error signal. However, it is true that the training of the *feedback* parameters can be parallelized.
- The derivation of the learning rule -- which should be, to my eyes, the *central piece* of the paper -- is unfortunately not presented in a sufficiently clear and detailed fashion. 
- Section 3.3 on uncertainty estimation is weak and orthogonal to the scope of the paper in my eyes. The uncertainty estimation is carried out on a single simple task, and does not abide by the standard of the uncertainty estimation literature: are the uncertainties well calibrated? Can they be used for anomaly detection? What is the quantitative performance of the anomaly detection in terms of binary classification metrics? A good task to consider (and not too difficult) is the MVTech dataset (https://www.mvtec.com/company/research/datasets/mvtec-ad).
- The baselines chosen in the experimental section are not very relevant. The proposed technique applies to block-wise training, but not a single block-wise training baseline (e.g. Belilovsky (2019)) is considered. Why? A relevant choice would have been to consider a VGG-11 architecture with 3 layers per block and try to reach $\approx 67.6 \\%$ top-1 performance on ImageNet (Belilovsky et al, 2019: https://arxiv.org/pdf/1812.11446.pdf).
- Given that the use of backprop is allowed within each block and that ResNet architectures are considered, the experimental results are disappointing: the performance obtained on CIFAR-10 is very poor ($\approx 70 \\%$ accuracy on CIFAR-10 is achievable by a 4 layers-deep convnet), ImageNet32 (or even ImageNet) has not been considered, and some results are surprising (see one of the questions below).


Limitations:
To summarize my points of advice above:
- In terms of presentation, I would recommend:
   + you clarify the derivation of your learning rule along the lines suggested above,
   + state clearly the intractability of $\ell^{(2)}$ for the ELBO gradient and better explain the heuristic used (""data mixing""),
   + distinguish between the feedback and feedforward block parameters, e.g. $\theta_f$ and $\theta_b$,
   + remove that your approach allows for forward/backward pass parallelization and backward pass parallelization across layers. While you *can* do this in practice, the theoretical approach itself (e.g. variational inference) does not prescribe doing this. This might explain why the penultimate and upstream blocks don't learn.
+ **Please write a detailed pseudo-algorithm** for the proposed procedure for a given training batch. That would be extremely helpful.

- Try to check if the previous blocks are really learning. My hypothesis is that it is not and that it requires fixing the algorithm itself. 

- If uncertainty estimation matters to you, I would suggest considering the MVTech dataset. 

  

Rating:
2

Confidence:
5

REVIEW 
Summary:
The authors present a block-local learning rule as an alternative to end-to-end gradient backpropagation to train neural networks. They present a probabilistic view of neural network representations and assuming an exponential family of distributions, derive a learning rule that can be understood as forward and backward message passing between blocks. Notably their message passing interpretation allows them to formulate auxilliary local losses that can be then optimized using gradient descent at the block-local level. Furthermore, they claim that their algorithm is a more principled way of performing algorithms like Feedback Alignment and Target Propagation. Finally, they demonstrate that their algorithm can be used to train ResNets (ResNet-18 & ResNet-50) on certain vision datasets and Transformer architecture on a sequence prediction task. Overall, the proposed algorithm seems a promising alternative to backpropagation with local learning properties that enable better memory footprint and neuroscientific realism than backpropagation.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
1. The paper does a good job in explaining the probabilstic interpretation and message passing view of forward and backward phase in neural networks.
2. The authors also empirically demonstrate the efficacy in training neural networks in Autoencoder setting as well as image classification and sequence prediction settings. 
3. The proposed probabilistic interpretation of activations allows uncertainty estimation in neural network predictions. Although these uncertainty estimates are not benchmarked in the paper, I feel this is a strength of the proposed method.
4. Owing to its block-local nature, the proposed algorithm can be thought to be a biologically-plausible credit assignment algorithm for hierarchical neural networks. In doing so, this paper also offers a potential solution to memory-efficient distributed training of neural networks. 

Overall, I think it's a strong submission and is very relevant for the NeurIPS community.

Weaknesses:
1. The writing and presentation in the paper is sometimes hard to read and understand, thereby leading to lack of an in-depth understanding of the proposed algorithm.
2. The current version of the paper does a great job in introducing the probabilistic interpretation of the neural network activations and the message passing view of forward and backward passes. However, their main learning rule is described in Section 3.4 which is not as clearly described. Unfortunately, the reader is deferred to the Appendix for key details of the derived learning rule, which adds to the lack of clarity regarding their proposed learning rule. 
3. In contrast to the Feedback alignment algorithm, the block-local learning algorithm seems to overfit significantly to the Cifar-10 dataset. Although this is an interesting finding in itself, the paper doesn't offer an explanation into this phenomenon or which components of their algorithm contribute to this. 
4. The discussion section offers more conjectures and falls short of discussing the implications of their results. For instance, the discussion section doesn't revisit the issue of overfitting or dive deeper into strategies around choosing the blocks & their backward counterparts and how these choices could affect the performance of the algorithm. The discussion section could also potentially highlight the potential biological plausibility of the proposed algorithm.

Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper introduces a novel framework for block-local training of deep networks. It proposes a twin network design that propagates information backwards from targets to the input to provide auxiliary local losses. This design allows forward and backward propagation to occur in parallel preventing the problems of weight transport and locking across blocks. This design is applied to training ResNets and transformers on several tasks.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
- Overall this paper is clearly written and proposes a novel interesting idea. I think this is an exciting direction that others will be able to build upon.
- The proposed twin architecture and the treatment of block outputs as uncertainty is novel
- The empirical results are strong and show a clear advantage of the block-local learning method particularly on CIFAR-10

Weaknesses:
- One selling point of this work is improved training efficiency. I would like to see an analysis even theoretically of what type of speedups can achieved with the proposed block-local training method.
- Although a few different architectures were evaluated the effect of block size on performance was not discussed. This seems like a important parameter to address as it effects both how parallelized/distributed training can be and biological plausibility. 

Limitations:
I think the limits of the biological plausibility of the twin architecture where the backwards network requires the same number of parameters as the forwards network should be discussed more. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this work, the authors address the problem of weight transport and weight locking issue in backprop by introducing a new bio-plausible algorithm known as block-learning to train NNs. The model uses different forward and backward weights, creating a twin network-like scheme to learn efficient signals via local losses. The proposed learning algorithm is tested on convolution and transformer-based architectures to show that the proposed framework can scale to complex architectures. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. Well-written paper
2. Experiments on convolution and transformers show that the proposed work can scale to complex architectures.

Weaknesses:
1. Novelty is limited, given current framework shares several similarities with other approaches, such as Local representation alignment.
2. Related work is missing several key citations.
3. Experimental setup is restricted, as several SoTa bio-plausible approaches, including PC based approaches, are not compared

Limitations:
1. Comparison is needed with relevant methods.
2. Analysis and ablation study missing.

Rating:
5

Confidence:
5

REVIEW 
Summary:
The authors propose a novel approach to the estimation of deep neural network parameters using block-localized backpropagation in conjunction with belief propagation. This approach is much more parallelizable, thus should help for distributed training, enabling horizontal scaling across devices.

The proposed approach works using a twin *backward* network, and by incorporating the belief messages into the block-local losses which are optimized using gradient descent.


Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
- The proposed approach is an interesting combination between belief propagation and back-propagation. I particularly appreciated the view of a neural network as a Markov chain.
- The proposed approach is significant, especially considering the current state of deep learning research. Large neural networks are steadily becoming the norm, and the development of specific learning algorithms for this kind of models is a valid and important research direction.

Weaknesses:
- The paper is sometimes not clear. I personally had difficulties understanding the following sections;
    - 3.1, especially after equation (2)
    - 3.4, it seems it requires the supplementary material to be correctly comprehended.
- I think with the current state of the paper it might be difficult to reproduce the reported results. The algorithm itself is not completely clear, and I think the submission would improve from a clear explanation (maybe provided in the supplementary material).
- A more in-depth analysis of the BLL algorithm convergence would improve the paper's strength.

Limitations:

- The algorithm requires to train an additional twin network, thus the number of total parameters optimized is greater than with standard gradient descent.
- The message-passing operation could affect convergence time, Fig. 3 seems to suggest otherwise, but it may not be always holtrue for other dataset or hyper-parameters.

Rating:
5

Confidence:
2

";0
NGiq8qCQNk;"REVIEW 
Summary:
Cai (2016) proved that in normal-form polymatrix games the set of CCEs corresponds with the set of NEs. This paper provides a similar result for polymatrix, switching controller, Markov games.

After introducing the formalism used throughout the paper, the main result is presented for both fixed horizon and infinite horizon. A counter example is also offered regarding the necessity of the switching controller condition.

The crucial aspect of the paper from a theoretical perspective lies in the fact that the polymatrix + switching controller assumptions allow to switch from a correlated distribution to a product distribution in the formulation of a CCE, thus proving the equivalence to a NE.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- Clear step by step structure of the paper
- Solid and intuitive proofs

Weaknesses:
The clarity of the paper can be improved in multiple points:
1. Sometimes the notation is confusing and feels heavy
	- the $\cdot^\dagger$ for best responses is unintuitive and does not feel natural
	- appendix A.1 is missing a cartesian product $\times$ in the Best responses symbols (after line 525)
	- in many points of the paper, some symbols are silently redefined to avoid explicit expectation terms, like $r_{k,h}(s,a,b)$ suddenly accepting a probability distribution as in $r_{k,h}(s, \pi, b)$. At a first glance, this formalism bugged me. Having those shortcuts defined in line 167-169 would help in avoiding surprises in the formalism.
 2. The definition of the linear program $P_{NE}$ would be easier to comprehend if the program was accompanied by a short textual description of the variables' meaning and of the constraints. This will greatly improve the smoothness of the read and ease the comprehension of the proofs.
 3. The relation between CCEs and Best response policies defined as product distribution should be explicitly explained to allow the connection between the definition of the coarse correlated and its practical meaning (i.e. CCEs are stable to policy deviations which renounce to see the correlation signal). It is to be noted that at the moment there is no proper intuition behind the idea of a CCE.

Limitations:
The main limitation of the paper is that specific assumptions have to be made on the reward structure (polymatrix) and transition function (switching control). These are clearly addressed and explicated in the paper. However, I do not agree with the authors regarding the practical real-world importance of polymatrix games (Section 1.1)


Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper defines a class of mulit-agent Markov games called zero-sum polymatrix Markov games, which is a generalization of the zero-sum polymatrix normal-form games. Specifically, it defines a class of Markov games where each state is a zero-sum polymatrix game. The main results of the paper shows that: in both the finite-horizon and the infinite-horizon setting (1) when the switching control assumption holds, i.e., the transition is determined by one player on each state, then the marginal policy of any (approximate) coerce correlated equilibrium (CCE) is an (approximate) Nash equilibrium (NE). This equilibrium collapse results ensures efficient computation of NE by reduction to computation of CCE. (2) when the transition is controlled by more than two players, equilibrium collapse does not hold. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper introduces an interesting class of Markov games where efficient computation of NE is possible, which covers switching control zero-sum Markov games and zero-sum polymatrix games as special cases. The results contribute to a better understanding of equilibrium computation in Markov games. 
2. This paper is fairly well-written and easy to follow. The results are complete in the sense that the authors showed equilibrium collapse with the swtiching control assumption and also provided counterexample when the assumption does not hold. 

Weaknesses:
1. The swtiching control assumption is kind of strong and limits the significance of the results. This paper would be much stronger with a more general sufficient condition of efficient computation of NE in zero-sum polymatrix Markov game. Currently, computation among  zero-sum polymatrix Markov game is rather unclear. 

Some minor comments:

1. Check the notation of $V_{k,h}^{\pi}$ in line 161, 164 and some other places. As the text suggests, it means the cumulative reward of player $k$ after timestep $h$ but what is written refers to $V^{\pi}_{k,1}$? 
2. Adding a formula might be helpful for readers to understand Assumption 2 
3. Line 324: ""can (be) modelled""

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper considers the problem of computing approximate Nash equilibria in Markov (aka stochastic) games. It is known that such equilibria can be computed efficiently in the non-stochastic setting when the game is zero-sum polymatrix. The authors show that approximate equilibria can also be computed efficiently in zero-sum polymatrix Markov games, when the game has the switching controller property (i.e., a single, but possibly different, agent influences the transition at each step).

The result is proved by showing that the coarse-correlated equilibria (CCE) collapse to Nash equilibria in this setting. As a result, it suffices to compute a CCE, which can be done efficiently using prior work. The same approach of equilibrium collapse was also used by Cai et al. (2016) in the non-stochastic setting. The authors also show that equilibrium collapse fails to hold if the switching controller assumption is removed.

Soundness:
3

Presentation:
2

Contribution:
4

Strengths:
- the paper studies a very natural computational problem
- the results are obtained by a non-trivial generalization of the techniques used by Cai et al. (2016)

Weaknesses:
- the writing is quite sloppy in some parts

Limitations:
n/a

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper explores a specific class of Markov games called zero-sum polymatrix Markov games, a stochastic generalization of one-shot polymatrix Markov games, and shows how an ϵ-approximate Nash equilibrium can be efficiently computed. The authors show equality between the set of coarse-correlated equilibria and Nash equilibria for a subset of  these games with switching control. This implies that Nash equilibria in switching control zero-sum polymatrix Markov games can be computed efficiently. The paper also discusses open questions related to using a policy optimization algorithm to converge to an approximate Nash equilibrium. Overall, the paper provides a theoretical framework for reasoning about strategic interactions over dynamically changing networks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper tackles a well-defined question, which is of interest to the community and which I would also find interesting. Overall the authors seem precise in their analysis. Writing is also often clear and and results seem reproducible.

Weaknesses:
The paper is not entirely self-contained (for instance when it comes to the references to Daslakakis et al’s algorithm), and lacks important intuition for the programs provided and theorem proofs. Given that the authors seems use traditional tools to obtain their results, and the paper is entirely theoretical I would expect a much more thorough explanation of theoretical result and would have liked a discussion of the algorithm introduced by Daslakakis et al. since the paper is recent and the reader might not be familiar with it.

For a purely theoretical paper, I would expect more intuition to be provided and more explanations of results. As it is, the reader has to spend its entire time decoding the results without any help from the authors. To be entirely clear, I think that the results, although they seem to rely on standard theoretical tools, are interesting and valuable to the community but the paper would be in much better shape with either experimental evaluation or with additional explanations. I remain open to changing my score and would appreciate it if the authors provide intuition and explanation of their results to this end.

Limitations:
NA

Rating:
7

Confidence:
4

";1
2SScUiWUbn;"REVIEW 
Summary:
The paper introduces an empirical study for visual pre-training. By focusing on the pre-training, the authors conduct wide-range experiments through (i), data quantity, (ii) label granularity, (iii) label semantics, (iv) image diversity, (v) data sources. The empirical study considers how to use pre-training datasets (both real and synthetic datasets), fine-tuning (data distribution), and test tasks (image classification, out-of-distribution detection). In the experimental section, the study disappears several findings for visual pre-training.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
- The novelty of the project is experimental findings with a wide-range of visual pre-training. The paper reveals five items in effect of data quantity (Section 4.1), label granularity (Section 4.2), label semantics (Section 4.3), image diversity (Section 4.4), and pre-training with different data sources (Section 4.5).

Weaknesses:
- How about implementing vision transformer (ViT) architecture? According to the experimental setup in Section 3, almost all of the network architectures are based on CNN architecture. Although some additional results are reported CLIP, one of the current visual architectures is undoubtedly ViT architecture. Since many architectures are implemented in the experiments, it doesn't seem that there is a lack of computing resources for the experimental conduction.

- The reviewer would touch the results shown in Figure 4. Does the graph show the highest score in the upper right corner? The claimed point with 25k pre-training samples (green line) is better than the baseline with 129k iWildCam images, but still wouldn't it be better to have more data such as 100k and 150k in terms of robustness? With this results, can we conclude tha ""we do not need a lot of pre-training data to see significant robustness gains"" (l.46-47)? 

- Related to Section 4.5.1 and Figure 10, the PixMix framework [Hendrycks+, CVPR22] with the combination of real and synthetic images improves a robustness score. Therefore, for robustness performance, the present paper should implement PixMix framework rather than that of single usage of fractal images.

[Hendrycks+, CVPR22] Dan Hendrycks et al. ""PixMix: Dreamlike Pictures Comprehensively Improve Safety Measures"" in CVPR, 2022.

Limitations:
There are no negative limitations and societal impacts.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper delves into the impact of pre-training data construction on fine-tuning robustness, encompassing aspects such as dataset size, class granularity, in- and out-class diversity, class similarity, and the use of synthetic data for pre-training. The evaluation metric employed is the in- and out-of-distribution testing performance, assumed to follow a linear trend. The study investigates changes in the slope of this trend to assess the influence of the mentioned factors. The findings suggest that pre-training data quantity and label granularity significantly affect fine-tuning robustness.


Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
This paper presents a comprehensive exploration of the relationship between pre-training datasets and fine-tuning robustness. The novel perspective on dataset construction offers valuable insights into deep learning models.

Weaknesses:
A potential limitation lies in the reliance on a single metric to measure effective robustness, which might not provide a fully comprehensive evaluation of the connection between pre-training datasets and fine-tuning robustness.

The figures' lines and points lack sufficient definition, which makes it somewhat challenging to grasp their full meaning. Some lines appear to be extrapolated, raising concerns about their validity beyond the tested environments, as observed in Figure 4, 5, 6, 7, 8, 10, and 11.

Limitations:
The authors have addressed some limitations of their work, and there are additional suggestions for improvement in the 'Paper Weakness'' part and ""Questions"" part.


Rating:
6

Confidence:
4

REVIEW 
Summary:
In this paper, the authors investigate the influence  of pre-training data on the robustness of fine-tuning. The authors design several experiments by following a common pre-training, fine-tuning and evaluation pipeline. They  found that the quantity of the pre-training
data and the granularity of the label set  are two most influential factors on the robustness of downstream fine-tuning. The  authors further leverage synthetic data from Stable Diffusion to increase the effectiveness of the pre-training distribution along these two ablation axes.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is  well-written and the motivation is clear.
2. The authors considers many factors that will possibly  affect the fine-tuning robustness including data quantity, label granularity, label  semantic, etc.
3. The conclusions of the paper are clear which is useful for follow-up research. 

Weaknesses:
1. The  authors mainly focus on the impact of the  pre-training data on the fine-tuning robustness, it is not clear if different fine-tuning methods (only fine-tune the last layer, etc) would change the conclusions of the paper.
2. The authors considers out-of-distribution generalization in this paper, it is unclear whether the conclusions also apply to other concepts of robustness, such as adversarial robustness.

Limitations:
Yes.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper investigates the role of pre-training data diversity on fine tuning robustness. They vary various factors like label space, label semantics, image diversity, data domains, and data quantity of the pre-training distribution to investigate how these factors impact the robustness of the models. Some interesting insights include similar label semantics doesn't necessarily improve the robustness of the model and increasing per-class examples doesn't necessarily improve the robustness of the model.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Provides insights on pre-training models like how various factors like data diversity, label space, label semantics etc affect the robustness of the models. Might be helpful for ML practitioners trying to decide what kind of data is best suited for pre-training 
- Experiments are thorough and clean. Paper is also easy to read highlighting main results.



Weaknesses:
- Major section of the work has been focused on supervised pre-training which is becoming less and less common with the advent of self-supervised learning methods. It would have been interesting to look at these of pre-training strategies in much more depth.
- Only one downstream task considered in the experiments (iWildCam-WILDS). Hard to quantify if these results generalize to other datasets.

Limitations:
addressed.

Rating:
6

Confidence:
3

";1
2MRz5bSnan;"REVIEW 
Summary:
The authors proposed the permutation decision trees method, which uses Effort-To-Compress as the impurity measure to model the order dependencies of data instances, and extended the proposed permutation decision tree to a variant of random forest. They also did some experiments to compare the performance of the proposed methods with random forests. 

Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
The proposed structural impurity can actually capture the order dependencies of data instances, as shown in the examples in Table 1. 

Weaknesses:
The paper exhibits several weaknesses, which are outlined below:

1. Insufficient clarity regarding the chosen setting: The authors' intended focus appears to be on time series data; however, the task discussed pertains to multi-class classification, which is an i.i.d. setting. The authors are recommended to formalize the problem setup.
2. Inconsistent use of notation: The paper demonstrates inconsistencies in notation usage. For instance, the features presented in Table 3 are denoted as $f_{1}, f_{2}$, whereas in Figures 3-7, they are represented as $x_{0}, x_{1}$.
3. Unfair experimental setup and insignificant results: The experiment setup lacks fairness, and the obtained results do not exhibit statistical significance. In the only out-performing dataset, the random forest model employed only one tree, while the proposed permutation decision forest utilized five trees, indicating an apparent unfairness in the comparison. Furthermore, the hyperparameters' n_estimators vary across different datasets, which is deemed unreasonable. 

Limitations:
The authors adequately addressed the limitations in Section 4.

Rating:
2

Confidence:
4

REVIEW 
Summary:
In traditional decision tree algorithms such as CART and C4.5, impurity measures are used to split internal nodes. The paper proposes a decision tree induction method by using effort-to-compress (ETC) measure, which can capture order dependencies in the data. With ETC’s ability to capture order dependencies, permuting the data can result in different trees, thereby constructing a forest without the need for bagging. This proposed decision tree induction method can be used for datasets with temporal order. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper uses ETC as a new impurity measure for constructing decision trees. Since ETC is sensitive to the order of data points, the tree built using this measure may be well-suited for temporal datasets. And it also provides a different way for constructing diverse trees and thereby getting a forest. Overall, the paper is clearly written and easy to follow. 

Weaknesses:
- What about the bias and variance in the permutation decision forest? Random forest uses bagging and random feature selection to make trees in the forest uncorrelated, thereby reducing variance. But trees in the permutation forest are not uncorrelated. Using the ensemble of these correlated trees may not reduce variance. 

- In the toy example, some leaf paths are shown in different trees. I am wondering if there will be a significant number of duplicated leaf paths within the permutation forest. 

- Experiments only show the comparison between random forest and permutation tree forest in terms of F1-score. How about other evaluation metrics, e.g. misclassification loss? The results don’t show that the proposed method outperforms random forest. And there is no comparison between the performance of single trees, such as CART vs. ETC tree. 


Limitations:
The authors have addressed the limitations of the paper and propose future directions. 

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper ""Permutation Decision Trees using Structural Impurity"" introduces a novel split criterion for the training of decision trees that also takes the order of labels inside the training data into account. This way, to obtain a forest, one only needs to shuffle the data before training individual trees. Moreover, the novel split criterion supposedly works better for data that includes (temporal) dependencies, although der are no experiments to support this claim. 

Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
- I think the idea of tackling non-iid data with a novel split criterion is nice, and Effort-To-Compress as an impurity measure seems like a good choice

Weaknesses:
- The experimental evaluation is very weak. The authors compare their method on 6 small real-world datasets and one artificial dataset and compare it only against Random Forests. Moreover, their method seems to be worse compared to RF. Hyperparameters are also incomparable, as the RF uses smaller trees than their method although it is well-known that RF benefits more from larger trees. In addition, the number of estimators changes for every experiment. There is no clear experimental protocol, and the authors do not use random repetitions and/or cross-validation but resort to a single train/test split. The experimental evaluation is hence borderline useless and can only be seen as a first test-experiment.  
- The paper contains limited valuable information. While the Effort-To-Compress (ETC) measure seems to be of central interest here, the authors do not present a formal mathematical explanation of it. They mention the NSRPS algorithm to compute ETC, but also do not explain it mathematically, and only offer a single example. A thorough mathematical explanation and the typical explanations of the notation (Model function f(x), samples X, labels Y, etc.) is missing entirely. 
- The authors deal with the case in which the order of samples is important. This is completely against the typical IID assumption we have in Machine Learning. Unfortunately, the authors neither discuss this (certainly interstring) difference in more detail nor do they really present any real-world example of this. 
- A dedicated Related Work section is missing, although there is plenty of space left in the paper. The authors decided to waste roughly two pages by printing different DTs, which does not add any new information to the paper. This space would have been used better to highlight related work or pinpoint the novelty of this work in more detail. 
- Eq. (1) and Tab. 4 do not fit the page width

Limitations:
The authors acknowledge that their method is worse compared to the state of the art and intend to perform more testing. As this paper presents a novel method I don't see any immediate negative societal impact.

Rating:
2

Confidence:
5

REVIEW 
Summary:
The paper proposes a novel in Decision Tree literature splitting criteria based on Effort To Compress (ETC) gain. Use of this criteria is justified by a desire to work with data that doesn't conform to i.i.d. assumption about the generating distribution. There is an experiment on a synthetic data that shows that different decision trees are generated when different orderings of the data are used for training. A permutation voting forest is introduced, that allows using random permutations of full data to obtain multiple different decision trees for use in the final ensemble of trees. There is an evaluation of Permutation Voting Forest against regular random forests on multiple real world datasets that however show slightly lower results when using proposed method. 

Soundness:
2

Presentation:
2

Contribution:
4

Strengths:
- The paper opens a novel line of research about using Decision Trees for modeling data, that comes in a sequence and does not follow i.i.d. assumption. 
- There is a novel application of ETC measure as a splitting criteria in decision trees.
- A generalization of the proposed model: Permutation Decision Forest is introduced, that uses a novel idea of shuffling the data in the context of a splitting criteria that generates different trees for different permutations of the training data. 


Weaknesses:
- It is noted that usage of ETC allows to get rid of i.i.d. assumption. But this claim needs more thorough theoretical analysis. If we want to keep sequential structure of the data, the sequence still gets destroyed upon split: split does not split examples in a consecutive way; some examples may go to the left split, then some to the right, then again to the left part of the split and so on. So, new left and right sequences after the split will have completely different properties. Considering an example from introduction, where ETC is used: musical compositions. Splitting the musical composition according to some feature, like presence of some range of frequencies at a given moment, will result in an unpleasant music on both sides of the split, because instead of hearing half of the musical composition, we will hear a ""fractal"" - small parts of original sequence with small gaps inbetween, that got assigned to left or right parts of the split
- Related to the previous point: at the testing phase there is no ""memory"" in the model, and the model still predicts elements by looking at them one by one. So, shuffling the testing set will result in exactly same predictions. Can we say that the problem of non-i.i.d. distribution is solved, if the behavior on the testing set is equal to the behaviour of the i.i.d. models?
- Testing of regular Decision Trees with proposed splitting criteria on real data is needed (only the forests were tested on real data, but proposed forests work differently due to the proposed shuffling of the input data, so regular trees must be evaluated separately as well). It would be nice to both test on regular datasets (that are not sequentially ordered, like the datasets from section 3.2), and also to find at least some example real datasets where ordering is important, and where proposed model (regular permutation decision tree) is both practically and theoretically better than the baseline decision tree models.
- In section 3.2 a more thorough experimental design would be more convincing. (a) If we compare proposed model to the baseline, why hyperparameters are different for same dataset? If hyperparameters tuning was done, that should be thoroughly described. (b) Experiments should be run several times on different train-test splits and mean scores and standard deviations should be reported to allow fair comparison in the presense of noise.


Limitations:
Limitations are well described in the paper, which is good. Since there were identified significant limitations in terms of accuracy of the proposed permutation decision forests (that may also affect proposed single permutation decision trees), it would be more convincing to include additional experiments that will clarify the extent of such limitations right away without deferring them to the future work. 


Rating:
4

Confidence:
5

";0
Se71ks7Mfz;"REVIEW 
Summary:
This paper proposes a Diffusion Transformer for 3D shape generation (point cloud), named DiT-3D, which conducts the denoising process on voxelized point clouds. Technically, it introduces 3D positional and patch embeddings, as well as 3D window attention. The main experiments are done on ShapeNet. In addition, the authors empirically show that the pre-trained DiT-2D checkpoint on ImageNet can significantly improve DiT-3D on ShapeNet.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is clearly written and easy to follow.
- The authors extend the 2D window attention operator to 3D.
- The authors empirically show the benefit of leveraging a  2D transformer pre-trained on natural images for 3D generation.

Weaknesses:
1. It seems that an important baseline or reference is missing: ""Point-E: A System for Generating 3D Point Clouds from Complex Prompts"". In L172-L173, the authors claim that ""We tried to train the diffusion transformer on point coordinates, but it did not work since point clouds are sparsely distributed in the 3D embedding space"". However, given Point-E, it sounds not that convincing. Can the authors explain why Point-E is not mentioned or compared in the paper?
2. It is hard to tell whether the generated shape is of high fidelity if the number of points is only 2048 (L245). It is more convincing if the number of points is larger than 4096 (Point-E) or 16384 (usually used in high-fidelity point completion). In addition, it will be visually better if the authors can present the 3D shapes in the format of mesh (like Point-E, MeshDiffusion) or colored point clouds. Currently, it is hard to tell whether the point cloud is of high fidelity. Visually, GET3D and Point-E look better than this work.
3. Only ShapeNet is used. Currently, there are more and more 3D datasets. It will be better if the authors can show results on more categories of ShapeNet, ABO, or (a subset of) Objaverse.

Limitations:
The limitation is not adequately addressed.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes DiT-3D, an extension of DiT for point cloud data to achieve point cloud generation. With the design of patch embedding and 3D window attention, the proposed model is able to reduce the computation cost. Also, the proposed model is able to directly leverage pre-trained 2D DiT model by fixing most layers and only finetuning several layers, which can reduce the required training time. The results quantitatively verify the effectiveness of the proposed method.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The proposed method out-performs existing SOTAs on the unconditional point cloud generation task in different evaluation matrices.

2. The paper is easy to follow and understand. 


Weaknesses:
1. The proposed DiT-3D looks like the original DiT with several modifications (simply changing existing 2D techniques into 3D version). Specifically, it seems like an ECCV 2022 paper [1] has already addressed a similar design of window attention, this paper is just using such a technique on a different task. Therefore, I think the novelty is limited, and more insights should be explained.

2. Previous point cloud generation works such as PVD and LION conducted experiments on multiple conditional generation tasks to verify their design. However, this paper only shows the results of unconditional generation. I believe more experiments such as point cloud completion or 2D image-to-3D point cloud should be included.

3. The qualitative results shown in the main paper only contain examples generated from DiT-3D and the supplementary material only additionally contains results from PVD, DPM, and SetVAE. As two methods that are quantitatively most comparable to DiT-3D, the visualization of LION and MeshDiffusion are not shown in this paper. So it is not clear how DiT-3D outperforms these methods. 
A minor issue: the computation resources should be detailed in this paper. 

[1] SWFormer: Sparse Window Transformer for 3D Object Detection in Point Clouds (ECCV 2022)


Limitations:
Yes

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper tackles the task of 3D generation. Inspired by the recent progress of utilizing transformers in 2D image generation with diffusion processes (DiT [1]), this paper proposes to replace the common U-Net in 3D diffusion models with a plain transformer. To adapt the DiT to the 3D scenario, authors make several modifications to the vanilla 2D DiT. Experiments on ShapeNet demonstrate improvements over baselines.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
This paper tackles an important task of 3D generations that is important for many downstream tasks.
The paper is generally well-written and easy to follow.
Experiments are thorough and convincing.

Weaknesses:
My main concerns are about the incremental development of the proposed DiT-3D based on DiT [1]. Specifically,

a. **Diffusion on voxelized point cloud** has been studied in [12];

b. **3D positional embeddings** is a natural/must-have modification from DiT's 2D positional encodings;

c. **3D window attentions** may not be necessary. Actually, I am quite confused why we need the following procedure: point cloud -> voxel (Sec. 3.2 Voxelized point clouds) -> patches in voxels (Sec. 3.2 patch embeddings) -> reshape patches in voxels into 3D window (Sec. 3.2 3D window). I think these hierarchical steps essentially just change the **actual voxel size**. Then why don't we just have a voxel with a resolution of $(p \cdot R)^3$ at the very beginning? Here $p$ is the patch size the author used in ""3D Positional and Patch Embeddings""(Sec. 3.2) and $R$ is the number of patches for ""3D window"" (Sec. 3.2). And this does not prevent authors from applying 3D convolution to exchange information (L179).

If the above abstraction/simplification is correct, it seems like the major modifications to 3D are just changing from image patches to voxels.

d. Authors state
> These results indicate that our DiT-3D can support flexible transferability on modality and domain, which is different from previous 3D generation methods [12, 13] based on U-Net as the backbone of DDPMs (L325-327). 

Can the authors explain whether there are some experiments to support this statement? As in Tab. 3, we only have results from DiT-3D without any baselines.

Limitations:
No limitations are provided by the authors.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes to adapt Diffusion Transformer [1] to class-conditional 3D point cloud generation task. In order to achieve it, the authors propose (1) to apply diffusion/infusion directly to point clouds rather than work in latent space; (2) transform input point clouds to voxel grids to apply transformers to tokens extracted from 3D grids in a straightforward way; (3) to reduce the complexity voxel features are processed in patches to produce patch tokens and self-attention in the transformer is modified to aggregate tokens in a window of a predefined size; (4) final voxel features are devoxelized back into points for per-point noise prediction in the infusion process.

Experimental results show promising results in class-specific point cloud generation on ShapeNet dataset, various ablation studies demonstrating the importance of different components and some additional transferability studies that use selective fine-tuning to adapt models pretrained on a data modality/domain to another one.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The main strength of the paper lies in experimental results beating state of the art with an novel approach, not based on prior 3D point cloud generation works. Although it is an adaptation of Diffusion Transformers working with images to 3D point clouds, such a transition from one data type to another is not trivial, so it is remarkable that the authors made it work in this setting.

Weaknesses:
In my opinion the main weakness of the paper is the quality of the presentation. Text can still be polished to improve readability and correct some mistakes. The authors claim across the paper (e.g. L120) that they apply diffusion for the voxelized point clouds, and I think it is misleading. In fact the diffusion is applied to regular point clouds, it is the infusion network that is designed to operate on voxelized 3D features, but as far as I understand, the denoising is applied on a per-point basis. Clarity of explanation can also be improved, since some details are missing (see questions).

The authors show a lot of ablation studies, but comparisons to external approaches are limited by the main single-class generation experiment. Qualitative comparisons are moved to supplementary materials, while it is better to show it in the main paper (quality of these comparisons could be improved by decreasing point sizes and sampling more points per shape, so some finer details could be examined).

Limitations:
The authors do not provide any statements about such limitations.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper introduces DiT-3D, a groundbreaking diffusion transformer for 3D shape generation. It addresses the limitations of previous 3D diffusion methods that mainly relied on the U-Net architecture. DiT-3D leverages the power of Transformers to directly operate the denoising process on voxelized point clouds, resulting in superior scalability and high-quality generations. The authors incorporate 3D positional and patch embeddings to aggregate input from voxelized point clouds and mitigate the computational cost of self-attention by employing 3D window attention in Transformer blocks. The proposed DiT-3D achieves state-of-the-art performance on the ShapeNet dataset, showcasing its ability to generate diverse and high-fidelity 3D point clouds.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. DiT-3D achieves state-of-the-art performance in single-category point cloud generation, demonstrating its effectiveness in generating high-quality 3D shapes.
2. The utilization of pre-trained DiT-2D checkpoints from ImageNet to improve DiT-3D on ShapeNet showcases the transferability of 2D diffusion models to the 3D domain, which is an interesting and promising approach.
3. The model is concise, and the paper is well-written, accompanied by clear and visually appealing illustrations.

Weaknesses:
1. The authors only conducted unconditional generation experiments for single-category and three-category cases, limiting the application fields.
2. Although the window attention technique is employed to mitigate computational costs, there are concerns regarding the generation speed, when operating on 32 * 32 * 32 (even 64 * 64 * 64) voxel grids.
3. Most DiT-3D models, except for DiT-3D-S, have parameters exceeding 100 million, considerably higher than other existing 3D generation methods.

Limitations:
N.A

Rating:
5

Confidence:
4

";1
SycQxJaGIR;"REVIEW 
Summary:
This paper addresses the multi-agent pathfinding problem by proposing an approach that utilizes a combination of a planning algorithm for constructing a long-term plan and reinforcement learning for reaching short-term sub-goals and resolving local conflicts. The results show that the proposed method outperforms decentralized learnable competitors and centralized planner. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The method is straightforward and concise.
2. The writing is clear and easy to understand.

Weaknesses:
1. The proposed method follows a hierarchical reinforcement learning framework, which has been extensively studied in previous works. There are limited contributions to the design of sub-goal selection. 
2. In the heuristic sub-goal decider, A* is used to construct a path, which requires global information. As the sub-goal decider will be used multiple times during the episode, the overall method seems not fully decentralized. 


Limitations:
This work has little negative societal impact. 

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper introduces a decentralized hierarchical approach without agent-to-agent communication for Lifelong Multi-agent Pathfinding (MAPF). The framework adds a congestion-based heuristic to an A*-planner and a low level Reinforcement Learning (RL) - based controller to follow the provided sub-goals. Experimental results show that the proposed method has a higher throughput (or rate of reach of new goals) for a range of maps.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- Easy to understand. Uses the well-studied A* planner with additional heuristics and a low level RL-based controller simply trained to reach goals promoting long term performance.
- Hierarchical framework is simple and could be effective way for decentralized control in an MAPF problem.

Weaknesses:
- The change of the heuristic in the A* planner seems weakly substantiated. While empirical results are promising, the need for hyperparameter tuning for the score and lack of guarantees on behavior may impede the use of this new heuristic.
- Confidence intervals for higher density experiments may be too large to claim better performance. (E.g. Table 1, 16 agents, proposed approach has throughput $ 0.56 \pm 0.34$ vs Primal2 having $0.31 \pm 0.14$ ). This may point to noisier behavior in the presence of more obstacles.

Limitations:
- Access to global map is assumed for use of the A* algorithm.
- Several empirically determined reward function portions may hinder generalizability to different maps.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper considers a decentralized multi-agent pathfinding (MAPF) problem. The main idea is to combine heuristic-based search and reinforcement learning. This work first determines subgoals and uses this information as intrinsic rewards. Empirically, it outperforms two baselines in the literature,  PRIMAL2 and PICO, in domains with different sizes and different numbers of agents. The method also demonstrates the ability to generalize to domains unseen during training.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The main observation is that pure heuristic search would not have a good performance in complex domains, where collaborative behaviors like congestion avoidance may not emerge. 
This work shows an inspiring combination of heuristic-based search and reinforcement learning.

The proposed algorithm also outperforms a centralized control algorithm (RHCR) when the number of agents is large or when the computational budget is small (so the centralized algorithm is expensive).


Weaknesses:
Some concerns about the practicality of the algorithm: 1) It requires some hyperparameter selection. 2) It requires pre-train a neural model. When it outperforms RHCR in some settings when RHCR is run for 10s, we need to consider the computational overhead of running the RL algorithm during training.
In terms of performance, the performance between FOLLOWER and PRIMAL2 is very close in some domains.

**Minor points.** Line 203, duplicate “node.”


Limitations:
The authors have mentioned the limitations of this work to be the assumptions of static environment, perfect perception.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a novel method for decentralized lifelong MAPF. The method consists of two components: a heuristic sub-goal decider, which assigns sub-goals for each agent using a heuristic (e.g., A*), and a learning-based policy network, which outputs actions for achieving the short-term subgoals. The paper compares the proposed method with both learning-based decentralized methods and the search-based centralized method on extensive setups and demonstrates the proposed method's superiority. The paper also provides insightful ablation studies.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The idea of using heuristics to solve long-term planning and utilizing learning-based policies for achieving short-term sub-goal is reasonable and also commonly used in many other tasks.

2. The paper compares the proposed method with both learning-based decentralized methods and a search-based centralized method on extensive setups and demonstrates the proposed method's superiority and generalization capability. 

3. The paper also provides insightful ablation studies and verifies the necessity of each proposed component. 

4. The paper is well-written and easy to follow.

Weaknesses:
1. Since I'm not active in the MAPF field right now, I am unsure if there is literature sharing similar ideas in the MAPF tasks. But at least I know that the idea of using heuristics for long-term sub-goal assignment and learning-based policy for low-level sub-goal achievement is quite common in the RL field.

2. How the RL policy handles the collision and deadlock is unclear to me. What will happen if the agent (or two agents) choose the action(s) that will cause a collision? What will happen if there is a deadlock (e.g., two agents want to pass a narrow corridor)?

3. I am a little confused about what the RL policy can learn if the K is set to 2, which means the sub-goal is just two steps away from the current location. Are there many candidate paths to a sub-goal, which is just two steps away?

4. Lines 211-218: Since the agent doesn't know the future locations of other agents, how does the method count the ""number of times the other agents were seen"" in a future step? Does the method use a static heat map (for only the current step) to count that?

5. Lines 242 and 247: the symbol H was used twice with different meanings.

6. Figure 1 is not mentioned in the text.

7. Lines 175-176: ""as the ratio of the episode length to the number of goals achieved"" -> ""as the number of goals achieved to the ratio of the episode length""?

8. Line 203: ""node node""



Limitations:
Yes

Rating:
5

Confidence:
3

";0
HwWkIwzzKF;"REVIEW 
Summary:
This work studies the CBwK problem beyond the worst case scenario, presenting two results regarding the worst-case locations and log rate respectively. The proposed algorithm utilizes a re-solving heuristic that achieves $O(1)$ under full-information and $O(\log(T))$ regret under partial-information with some regularity conditions. The worst-case guarantee is also presented.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This is a well written paper. The key ideas and main contributions are clearly presented. The proofs seems complete and easy to follow. 

Weaknesses:
This reviewer has doubts on the exact contribution of this paper. 
Firstly, the problem studied (CBwK) is well-established in the literature. And the setting that the authors choose might be not so realistic and sort of marginal. For example, one major concern from my understanding is that the budget is ""soft"", since the budget constraint is only required in expectation, and in summation. However, in reality, ""hard"" constraints should be considered prevalently, where the constraint should not be compensated, and the expectation might also be unnecessary. The assumption of null action might not be met in reality as well.

Secondly, the algorithm and the re-solving heuristic are from literature. There's something new in the proofs since extending from BwK to CBwK requires a more complicated estimation.

To add to these, there are existing work on constrained reinforcement learning, e.g. [1] that considers similar setting. Since RL is typically considered more general a setting than CB, the results there should also be at least comparable.

Lastly, the assumption for full-information seems overly strong, and the results does not seem very pertinent to the main claims. Maybe it could be better moved to the appendix to make the contribution more clear.

[1] Sobhan Miryoosefi, Chi Jin, A Simple Reward-free Approach to Constrained Reinforcement Learning

Limitations:
n/a

Rating:
4

Confidence:
4

REVIEW 
Summary:
This works considers a general setup of Contextual Bandits with Knapsacks (CBwK). The authors identify sufficient conditions under which constant of logarithmic regrets are possible (while previous authors were mainly dealing with $\sqrt{T}$ worst-case regrets). The studied algorithm is rather intuitive and is based on a sequential refinement of the underlying best static LP problem. 



Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. Identification of the conditions for $o(\sqrt{T})$ regret is a great contribution.
2. The algorithm does not need to know whether the conditions are satisfied---it is adaptive.
3. The main body exposition is rather clear, but more details would be appreciated.
4. The approach is intuitive and computationally tractable.
5. Overall I liked the proofs, they are a bit hard to read due to heavy notation, but once the reader is comfortable, the presentation is rather good.

Weaknesses:
Here are the weaknesses in no particular order

1. Bounds are in expectation (which is fine per-se, but from my experience the results are often formulated with high probability)
2. Little to no comments on the actual stopping time of the algorithm, is it much smaller than T? (I only found this information in the appendix)
3. O(1) is great, but what kind of 1 it involves is not present in the main body.
4. Regret is not the only part of the story: e.g., Agrawal and Devour 2016 allow o(1) budgets, instead of constant budgets. In this case comparing to their result is not really fair. 
5. While computationally tractable, the algorithm is not very efficient, one needs to solve a potentially huge LP problem at each step. 




Limitations:
I was not aware of the work of Chen et al 2022, but upon skimming it looks very similar to the current submission (tools and techniques-wise). A broader discussion would be appreciated.

While being honest and correct, the extension to the continuous case, is not very satisfactory (but maybe it is a matter of taste).



Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper studies stochastic contextual Bandits with Knapsacks. The authors provide an algorithm that guarantees regret smaller than the worst-case $\tilde O(\sqrt{T})$ in non-degenerate instances. In particular, they provides an algorithm that achieves $\tilde O(1)$ regret when the fluid LP has a unique and non-degenerate solution. Moreover, in the worst-case the algorithm maintains $\tilde O(\sqrt{T})$ regret.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The main paper is clear and well written. The problem under study is interesting. The technical results are not straightforward.

Weaknesses:
Most of the results in the paper are based on the existence of an unique and non-degenerate solution (Assumption 3.1). The formal definition of this assumption is missing. Moreover, while this assumption is commonplace in the linear programming literature, it is not clear whether it makes sanse in the bandit with knapsack problem. Finally, previous work suggests that large regret is unavailable in many standard cases.

The main paper does not give a clear intuition of the techniques used to prove the theoretical results and the appendix is difficult to follows. Moreover, the appendix is heavily based on results in other papers, e.g., Chen et al. [2022]. This makes the proofs hard to follow. Moreover, it is not clear which are the main technical contributions of the paper, and to what extend the results follow directly from the application of known techniques.

Limitations:
Yes

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper considers the problem of contextual bandits with knapsacks and provides an algorithm that goes beyond worst-case (i.e., provide logarithmic regret) under the mild condition of unique optimal solution and non-degenerate solution. This also simultaneously enjoys an optimal worst-case regret bound when the conditions aren't met. The paper also shows a Omega(sqrt(T)) lower-bound when the instance has a unique optimal solution and a degenerate solution.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
+ The paper considers a significant generalization of the results in Sankararaman and Slivkins 2021, in that it extends the logarithmic regret from two resources and best-arm optimality to arbitrary resources and unique best solution.

+ The paper also proves that the new algorithm derived from resolving the program enjoys worst-case regret bounds that are optimal (in the interesting regime of B ~ O(T)).

+ The results also applies to full policy-based contextual bandit problems, as opposed to prior work with logarithmic regret bounds which only applies to the linear contextual bandits setting.

Weaknesses:
- The algorithm needs to (a) solve a program at each round (can this be removed?) unlike UCB type algorithms where the program needs to be solved only when the ucb of any single quantity changes by a constant factor and (b) even in a single round the run-time of solving the program is unclear (I might be wrong, please correct me). Although the proposed results are interesting, this seems like a major downside from the algorithm front.

- [Some rewording of comparison with prior work] The proposed algorithm is very similar to that of [Flajolet and Jaillet, 2015] (the paper mentions this, although the wording could be more generous to the prior work). Likewise, the paper mentions that the setting considered in Sankararaman and Slivkins '2021 is ""and
almost surely excludes all problem instances"" which is not technically true. Note that the prior work paper uses the same argument of perturbing LPs in the case of d=2. Thus, the key difference is extending to d > 2 and not about the number of instances. 

- Although the lower-bound presented in this paper is interesting, I think some of the lower bounds are implied from the instances presented in Sankararaman and Slivkins '2021.

---

EDIT: 

I still don't know how to implment this algorithm apart from the trivial way of having one arm per (context, arm) pair. This is important, since then the algorithm is trivial, and the results then likely follow from prior works, in particular [1] which implements this for the k-armed bandit setting. Thus, the paper needs to explain clearly why this is better than that trivial reduction, and as a result, the bounds are significantly better. I believe multiple reviewers get at this same point, and i am not able to see a  convincing argument. Please make sure to incorporate this detailed comparision in the next version of the paper.


[1] Logarithmic regret bounds for Bandits with Knapsacks - Arthur Flajolet, Patrick Jaillet

Limitations:
This is a mathematical paper and no societal impact.

Rating:
4

Confidence:
4

";0
FwmvbuDiMk;"REVIEW 
Summary:
The paper analyze several theoretical property of the expert demonstration distribution to the performance of the learnt policy in imitation learning. The paper claims that people should provide consistent and transition and state diversity.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
The paper defines several quantity to clear express the meaning of the properties. The theorems are sound to me.

Weaknesses:
The results claimed by the paper are straightforward to me, which makes the contribution less significant. The theoretical results are basically quick derivation of standard machinery of MDP analysis, which are trivial and straightforward.

The paper claims that expert should provide consistent demo based on BC analysis. However, the paper does provide experiment of the types of imitation learning methods like transformer architecture to handle multi-modal distribution.


Limitations:
See weakness.

Rating:
2

Confidence:
5

REVIEW 
Summary:
This paper considers how to evaluate  the quality of a given dataset for an imitation learning task.  It is well-known that IL suffers from distribution shift. While much work on IL focuses on improving algorithms, the authors propose that another important approach to making IL more effective is to curate better datasets and demonstrations from experts. Existing approaches to evaluating the quality of datasets focus on state coverage / diversity, trying to ensure that the expert demonstration contains instances of a variety of different states.
The authors discuss in some depth how state diversity is an incomplete measure of dataset quality, specifically because it ignores the actions taken in the dataset; the goal of the paper is to find an improved metric for evaluating dataset quality. he paper proposes action diversity and transition diversity.  The authors then consider  the distribution shift from the expert policy to the learned policy under a given f-divergence (KL divergence by default), as having a large distribution shift can cause problems in IL.
They then show a number of properties of their measure and relate it to action diversity and transition diversity , and discuss how these metrics can be applied to improve IL in general (4.3, Implications for Data Curation). They claim that their work suggests that action consistency in the expert demonstrations leads to better outcomes, while state diversty and system noise in the dataset are good or bad depending on context.
Finally, they perform experiments on two simple simulationtasks, comparing the effects of adding policy noise to adding system noise in the training set and comparing how demonstration datasets of varying (known) quality correlate with various metrics.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The main idea of finding improved metrics for dataset quality, above and beyond state space coverage, is interesting. Furthermore, the dichotomy that their analysis exposes between expanding state space coverage via higher action divergence versus higher transition divergence is an interesting one, showing that not all state space coverage is created equal. The theoretical results linking Q with action divergence are also interesting and well-presented.
While there are questions left by the experiments (see Weaknesses and Questions) one extremely interesting finding is that the additon of a small amount of system noise vastly improves the effectiveness of training with policy noise (compare Figure 2 (c)(d) with Figure 2 (i)(j)), especially when the number of demos is low. The effect of this is so strong that it would have been better to focus more on it when discussing the results.

With a few minor exceptions, the paper is also very well written. The essential ideas are clearly stated and easy for the reader to absorb.

Weaknesses:
This paper would be greatly improved by reducing repetition and adding more experiments.  The points made in the Intro are repeated at least 5 times.  The paper would be much better if the authors agree to remove this redundancy and add more thorough experiments.

he biggest weakness is a key property of the proposed measure (equation (6)), which is tied to a strength of the work (""data quality is heavily tied to the algorithm A that leads to the learned policy \pi_A as well as the expert policy \pi_E""): since it takes the learned policy \pi_A as well as the expert policy \pi_E, actually measuring the quality Q may require running A, which somewhat diminishes the value of the data quality metric since the it may not be usable when collecting the data (note that most common metric, state diversity in the training dataset, can be measured without running A). Even one of the proposed proxies for Q, action divergence, requires having the learned policy to measure against the expert policy.
There is an inconsistency in lines 196 and 199-200 with Theorem 4.1: it says the data quality is ""lower bounded by the action divergence"" but Theorem 4.1 shows that action divergence provides an upper bound to the data quality. Since upper and lower bounds for dataset quality have different implications this is very important to fix.
While Figure 2 (i)(j) represents the most interesting finding (in my opinion) of this work, it doesn't escape notice that there is no corresponding Figure 2 (k)(l) showing the same plots for the Square nut task. These should absolutely be included to show whether or not the finding in (i)(j) generalizes to at least one other task (otherwise there is the possibility that (i)(j) is simply a property of one particular setting).
The authors do not include a limitations section.
Minor weaknesses and typos:
In (3), the sum should go for t=1 to T, not H
Figure 2 caption: In ""X-axis corresponds…"" should probably write ""injected system noise"" twice (rather than ""injected Gaussian noise"" the first time and ""injected system noise"" the second time). They're both system noise and both Gaussian, right? Writing it differently may convey the mistaken impression that they might not be both Gaussian, whereas the difference is just when the noise is added. Notation is also confusing, it seems \sigma_s is used both to refer to system noise at evaluation (in the figure itself) and to system noise added in the training set (line 342).
Table 1: instead of merely stating that state similarty is the ""opposite of state diversity"" (line 360), it would be much better to have a more precise definition. Additionally, it is claimed that ""state diversity once again is not correlated with performance on the task"" (line 370, just above Table 1) but Better/Okay/Worse all seem to have similar state similarity values while Proficient Human (PH) has lower state similarity (i.e. higher state diversity) and has significantly better success rates – doesn't this mean that it is correlated? Claims of no correlation should probably come with the actual correlation number, since it isn't obvious from the table that there really is no correlation.
Equation (6) should probably be in a Definition since it's one of the key contributions of the work.
4.3 Implications for Data Curation might be better placed after the experiments section.

Limitations:
There is no Limitations section in the paper.

The limitations of the work are only very briefly addressed (lines 390-392) and need more expanding; in particular, the weakness mentioned (that the data quality metric proposed is very hard to measure in real datasets) is a serious one and deserves a much broader discussion.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies imitation learning (IL) from a theoretical perspective. It proposes the idea that we should curate the data to improve performance, rather than solely relying on designing more robust algorithms, which is a more common practice in the field. The authors propose data quality for a particular IL algorithm, which is the negative of the divergence between the expert policy and the learned policy using that algorithm. They then show theoretically how two properties, action divergence and transition diversity, are linked to data quality. The authors then study the finite-data regime under a very simplistic setting, and finally show some results of studying their metrics on PMObstacle, Square, and Robomimic domains.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The presentation / quality of the writing is top-notch. I really appreciated the clear descriptions throughout the paper, starting from the well-written introduction to the explicit problem setting to the way the authors use English to describe and contextualize all the math.

Related work section is comprehensive. I like how it addresses both algorithmic and data robustness.

I also really like the framing of Equation 6. The insight that data quality is a function of the learning algorithm is very nice.

Weaknesses:
I am unfortunately not well-versed in IL theory, so I cannot comment on the novelty of the work with respect to the rest of the field. Therefore, I will only evaluate the paper on the merits of the contributions stated by the authors.

My only major weakness is about the unrealistic conditions underlying some of the theory.
* One of the conditions of Lemma 4.2 is that states in the support of the expert policy would have low divergence between actions under the expert policy and the learned policy. The authors say in L226 that with a good learning algorithm, this bounding constant \beta would be small, but I don't see why this would be true in general. For BC, maybe it makes sense, but for other IL algorithms, a ""good"" algorithm could sacrifice exactly matching the expert at some states, for the sake of getting better interpolation/generalization to states outside of the support of the expert policy.
* Theorem 4.3 studies only a very simple setting where the environment dynamics are fixed Gaussian with diagonal variance. I'm not sure how much we can take away from this Theorem as this condition will almost never hold in practice. As a consequence, I find takeaways like that in L254 to be misleading because one cannot draw general conclusions from such a restricted Theorem.

Other comments:
* One more broad concern I have is that most of the robotics domains studied in the RL literature have deterministic dynamics, including Robomimic (ref: https://arxiv.org/abs/2305.14550) and Mujoco. So how can we obtain transition diversity in such settings?
* Theorem 4.1 only makes sense with stochastic policies due to the KL divergence. It's not clear how to apply these findings to deterministic policies, which are quite common (and in fact, are used in the experiments of this paper).
* L230, L290: It's unclear what the authors mean when they say the data should maximize system noise. That's a property of the environment dynamics, not the expert policy; we don't have control over it!
* It is hard to gain too many insights from Table 1. A simpler explanation for the observed success rates is that as task horizon increases while data size stays fixed, BC simply has less data for each state, leading to worse policies. I'm not really sure why action variance and transition diversity come into play here.

Minor things:
* The authors use the word ""expert"" throughout the paper. I would rather say the ""behavior policy"" or ""data generation policy"", because the whole point of this paper is that we are trying to curate the dataset, e.g. by introducing more diversity, which means the data may not end up looking like what an expert would do.
* It would be good to say in the preliminaries that you're in a continuous state/action space setting.
* L73: remove ""how""
* L167: ""\pi_A should match \rho_{\pi_E}(s)"" --> I think you mean ""\rho_{\pi_A} should match \rho_{\pi_E}""?
* L167: ""learns only"" --> ""learns only from""
* Lemma 4.2 RHS, subscript of the expectation: \in should be \sim?

Limitations:
Yes, addressed adequately.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper addresses a key issue in Imitation Learning (IL) called as distribution shift. The paper formalize data quality and data curation for IL. The paper propose two fundamental properties to evaluate/measure data quality as ""action divergence"" and ""transition divergence"". The authors investigate these two properties in context of data quality for IL and presented theoretical and empirical results. They conducted an experiment on human/expert demonstration data and provide analysis on data noising and data measuring.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is well-written
- The idea presented in the paper is very important for IL

Weaknesses:
- The work doesn't seems to fit for a high quality conference like NeurIPS
- On measuring the data quality and its effect on success rate, a lot of research is being done on learning from mixed data as its really hard to classify expert vs non-expert data.
- The paper seems more like a white paper or can be submitted to AI Safety conferences but not for NeurIPS
-

Limitations:
- Authors are encouraged to explicity mention the limitations of the paper

Rating:
3

Confidence:
4

REVIEW 
Summary:
Data quality is an important aspect of offline robotics because there is not a large quantity of data. In particular, in imitation learning, it has been shown that naive training policies on low-quality data can lead to distribution shift issues. Most researchers focused on dealing with this issue using algorithm-centric approaches. Instead, this paper advocates for a data-centric approach for addressing this issue. The data-centric approach modifies the data collection process, where the data collection can be broken down into two categories: action divergence and transition diversity. The authors formalize these two categories of the data collection process and provide empirical evidence of their significance. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Formalizing and addressing the data quality issues in imitation learning is challenging. This paper proposed interesting ideas to this challenging problem and has the following strengths:
- Clarity: The overall paper is well written, and the problem the authors would like to address is clear. The two categories of data collection are also very clear and make it easy to understand the underlying problem dataset attributes.
- Empirical Justification: The author's performance experience using synthetic noise, single and multi-human datasets. All of the experiments performed across these datasets provide insight into various aspects of the data collection discussed in the author's paper. These insights helped enhance the author's discussion of data collection attributes proposed by the authors.

Weaknesses:
Although the authors proposed an interesting perspective to address a difficult problem in imitation learning, there exist several weaknesses of the paper:
- Lack of novelty: Theorem 4.1 was already proposed and proven in [1] (see theorem 3 appendix). That means the action divergence interpretation was already discussed in the literature. Furthermore, transition diversity was already proposed and discussed to address distribution shift issues in [2].
- Lack of explanation: The authors emphasize that past work only focuses on state coverage while ignoring that action's role. But if the policy providing the demonstrations is an expert,  then the states provided from the expert should inherently have low action divergence because this is an expert;  and an expert provides the optimal action for a given state.
- Lack of citation: The section on distributions shift in IL needs proper citation.


[1] Imitation Learning as f-Divergence Minimization by Ke et al.
[2] Sequence Model Imitation Learning with Unobserved Contexts

Limitations:
Yes

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper systematically studied the data quality assessment problem in imitation learning, proposing new metrics based on test-time distribution shift. 


Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
The paper is generally well-organized and well-written. The analysis seems to be in fair detail.

Weaknesses:
One of the drawn conclusions seem straightforward to me (""increasing state diversity can often come at the expense of action divergence""). Increasing state diversity will bring more challenges for the learning and thus may cause increased action divergence. 
    
The paper lacks practical insights, e.g., how the proposed data quality evaluation metrics can help guide the data collection and thus help the imitation learning is unclear. Currently I dot not see the benefits of introducing such data quality evaluation metrics. 

Limitations:
Not discussed in the paper.



Rating:
5

Confidence:
2

";1
WBXYGBQXiB;"REVIEW 
Summary:
This paper generalizes common machine learning operations from Cartesian lattices to other regular lattices, such as the hexagonal lattice. The authors argue that the Cartesian lattice is a sub-optimal representation for important natural signals, and that operating on their non-Cartesian structure natively leads to more efficient implementations and better results.

The authors further promise to release a software library for non-Cartesian deep learning, and include an experimental section with implementation details and efficiency arguments, as well as experimental results on various computer vision tasks.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
The contribution is very novel in the sense that the field of deep learning on lattices is under-explored. The author's work is likely to have a big impact on this area of machine learning, as providing a complete optimized library with non-Cartesian version of common lattice operations with speed-up future research significantly.

Weaknesses:
The paper is short, with the authors being allowed one more page of content. They could improve the discussion by expanding, for example, on the computation of the derivatives.

Are the backward passes of all operations naturally handled by PyTorch or did it require manual implementations? What about numerical stability?

Limitations:
Some aspects are lacking, as explained above:
- Derivatives and numerical stability
- Comparison with other non-Cartesian models such as GNNs and group equivariant neural networks

Rating:
8

Confidence:
4

REVIEW 
Summary:
This works introduces a framework as well as software for computing convolutions on non-Cartesian lattices. The method is compared to existing software for hexagonal lattices as well as on image data.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The method is put in a strong theoretical framework that also explores the very important up and down sampling operations on non-uniform grids. The authors also make available open-source software that is more general and whose performance seems much better than anything available. 

Weaknesses:
In the context of scientific machine learning, there have been similar ideas explored on how to make architectures which work on arbitrary grids, for example, https://arxiv.org/abs/2207.05209, https://arxiv.org/abs/2305.19663, and I am sure there are others. Furthermore, graph neural networks can also perform convolutions or arbitrary grids, for example, https://arxiv.org/abs/1704.01212, among many, many others. It would be good to mention some of these and even include some comparisons.

Limitations:
The authors have adequately addressed limitations and potential negative societal impact. 

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper introduces a high-quality software extension for PyTorch that enables seamless computations with non-Cartesian lattices for 1D, 2D and 3D images. The key observation made by the authors of this work is that **non-Cartesian lattices can be decomposed as ""sums"" of Cartesian lattices**: up to some clever refactoring (which makes up the core numerical code of the proposed software package), efficient computations on non-Cartesian lattices can directly leverage the standard (and highly optimized) PyTorch/cuDNN implementations of e.g. convolutional layers. 

This implies that **the proposed software package is both efficient and easy to maintain in the long run**. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- The authors tackle an interesting and **very original topic**. Non-Cartesian lattices are indeed fundamental to low-level image processing but essentially absent from the machine learning literature.

- The paper is **extremely well written**, with clear figures, attention paid to details and a satisfying evaluation. I haven't tried to run the code provided in the supplementary materials (too many papers to review at once!), but this is **clearly a high-quality software package** with a clean structure, a full test suite and extensive documentation.

- This package targets successfully one specific and interesting operation in image processing, providing a **useful extension to our common toolbox via a neat and clever software package**. This is more than what most papers (never mind submissions) can provide and, in my opinion, clearly warrants publication at NeurIPS.


Weaknesses:
Reviewers can always ask for more (outstanding deep learning experiments, better run times, support for all types of attention layers and niche hardware architectures, etc.)... But realistically, I am very happy with the paper as submitted.

Limitations:
 

Rating:
8

Confidence:
4

REVIEW 
Summary:
he authors claim that the concept of tensors, a fundamental cornerstone in machine learning, assumes data are organized on Cartesian grids. They further suggest that alternative non-Cartesian representations may be more beneficial in certain situations. One case is when the data is inherently non-Cartesian — for example, the raw R/G/B image data from most imaging sensors are follows a quincunx (checkerboard) structure in the green channel. Another case is when non-Cartesian grids show superior performance in certain aspects — for example, the hexagonal lattice is known as the optimal sampling lattice in 2D.
The authors hereby propose a framework and a software library that introduces standard machine learning operations on non-Cartesian data. The new data structure is called lattice tensor and the software library is called Non-Cartesian Deep Learning (NCDL).


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. In the introduction section, the authors provided sufficient context on why non-Cartesian grid structures may be superior to Cartesian counterparts under certain circumstances.
2. We shall appreciate the effort in designing the memory-efficient coset representation for operations on the hexagonal lattice (shown in Figure 2).


Weaknesses:
While custom tensor definitions and operations for non-Cartesian data representation are theoretically pleasing, the authors have not shown clearly the potential applications and impact. It is not obvious which datasets and/or standard machine learning tasks will directly benefit from the non-Cartesian representation. A table detailing some typical use cases will be utterly helpful. 
The experiments/comparisons performed are not convincing enough. The two main results shown in the submission are (1) runtime of convolution operation at different grid sizes, and (2) loss curves of a Cartesian vs. Quincunx auto-encoder on CelebA (celebrity faces) dataset. 

In the first experiment, while the authors compare the runtime against the standard Cartesian Conv2D and another non-Cartesian baseline (HexagDLy), they have only investigated the convolution operation but skipped the other operations such as pooling, downsampling, upsampling, gradient computation, and back-propagation. Further, no comment has been made on numerical correctness or precision.
In the second experiment, the authors aim to show superior performance of Quincunx auto-encoder for image reconstruction. It is a bit weak as the experiment is only performed on one task over one dataset. It may be helpful to include a few more datasets — they don’t even need to be big one, e.g., STL-10, SVHN, LSUN will be sufficient. Besides, the only metrics shown are L1 and SSIM on the validation set. I would recommend including other metrics such as L2, PSNR, FID, and perceptual distance.


Limitations:
While it has been covered in previous sections, the main limitations are:
-Unclear potential application and impact.
-Insufficient of experiments and comparisons.

Rating:
4

Confidence:
3

";1
lSbbC2VyCu;"REVIEW 
Summary:
The paper proposes rewarded soup (RS), a simple technique for combining policies trained for different rewards into a single policy performing well on a particular convex combination of those rewards. The technique consists in linearly interpolating weights of individual policies, using the fact that they share the same initialisation and remain linearly connected during finetuning on different rewards. The approach is well-motivated theoretically and thoroughly evaluated on a diverse array of tasks ranging from language generation, image captioning and image generation to robot locomotion. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The paper addresses crucial problems of accounting for diverse preferences and adapting to changes in reward specification that frequently arise in the emerging and important field of aligning generative models with human preferences.
2. Rewarded soup is well-motivated theoretically as a Pareto coverage set of policies for linear combinations of individual reward functions. I found working hypotheses 1 and 2 to be very helpful in understanding how RS works. I’m also convinced by empirical evidence for these hypotheses being true.
3. The fact that linear mode connectivity also holds for RL policies trained for different rewards is an interesting finding about deep learning overall. I found it somewhat surprising that interpolated weights outperform the interpolated rewards so consistently.
4. The paper is well-written and easy to follow despite being very dense. The theory part connects with experiments very well. I also appreciate the plots (e.g. Figure 2) being easy to navigate while conveying a lot of information.
5. The experiments are very thorough and diverse and I find them compelling.

Weaknesses:
I think the discussion of reward misspecification could be more nuanced. I think the claims that RS “mitigates reward misspecification” (line 75) and “If Hypothesis 2 is true, then RS can mitigate reward misspecification” (line 161) should be framed a bit more cautiously, making it clear that it’s a very particular kind of reward misspecification: when the real reward is linear in a set of proxy rewards. I don’t think this is representative of most kinds of reward misspecification that we see and that we should me worried about such as context-dependence of human preferences or biases of data workers providing feedbacks.

Limitations:
Limitations and societal impacts are discussed thoroughly. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper presents reward soups (RS) which is the idea of starting with a pre-trained network, which is finetuned to multiple proxy rewards (say, multiple different criteria), and at test time, infers a reward as a linear combination of these proxy rewards and uses this to linearly combine the corresponding weights which is then used for prediction/generation. In contrast to naive variants of multi-objective RL which trains many different policies (far greater than the number of proxy rewards) to obtain a high fidelity policy for preferences encountered at test time, the proposed approach ends up working while training a number of policies that is equal to the number of proxy rewards while showing reasonable empirical performance.

==> post rebuttal: updated score.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The problem formation and proposed approach are topics of increasing interest and relevance to the community. The paper presents interesting results for many practically relevant and useful benchmarks.

Weaknesses:
- There are not much comparisons to approaches in multi-objective RL, which makes it unclear as to how one can imagine this paper's result to be approaching notions of pareto optimal trade-offs. While the paper acknowledges this issue, it leaves open huge gaps as to what can be achieved using a suite of approaches that exist in multi-objective RL (for e.g. even starting from reference 130 cited in this paper).

Limitations:
yes.

Rating:
4

Confidence:
4

REVIEW 
Summary:
In this paper, the authors propose a multi-policy strategy called ""rewarded soups"" to fine-tune any foundation model, embracing the heterogeneity of diverse rewards. The method combines multiple networks through linear interpolation in the weight space, despite the non-linearities in the network, which efficiently yields Pareto-optimal solutions after training. The authors demonstrate the effectiveness of the approach for text-to-text, text-image, and locomotion control tasks, showing that ""rewarded soups"" can mitigate reward misspecification. The proposed approach aims to enhance the alignment of deep models and how they interact with the world in all its diversity. The authors highlight the issue of aligning AI systems to specific and diverse needs while making the process more transparent and limiting the cultural hegemony of a few individuals.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
This paper addresses the reward misspecification problem caused by single priori rewards in current RLHF frameworks for foundation models. In order to solve the problem, this paper proposes a relatively complete framework called rewarded soup (RS). RS combines multiple networks (fine-tuning on different proxy rewards) through linear interpolation in the weight space and selects relative coefficients according to the user’s preferences, yielding Pareto-optimal solutions. The content of the whole paper is complete, and the experiments are sufficient.

Weaknesses:
1. The writing logic of the article could be more coherent in the reviewer's opinion. For example, in 3.3, ""Moreover, RS gives a better front than MORL, validating Hypothesis 2."" and in 3.5, ""Moreover, the front defined by RS indicates an effective balance between risk-taking and cautiousness, providing empirical support for Hypothesis 2, although MORL with $\mu$ = 0.5 (i.e., $\alpha$ = 0.5) slightly surpasses RS's front."" 
2. Validations on Hypothesis 1 and 2 are based on empirical results. The authors are encouraged to give some theoretical analysis. 
3. Lack of experiments on computational costs. According to the results, the proposed method has no strengths compared to MORL. The authors state that RS can reduce computational costs. However, no relative experiments showed in this paper.
4. Most experiments are assigned an N=2 reward model, which is not aligned enough with the ""diverse"" in the title.

Limitations:
The authors have adequately addressed the limitations.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper explores a model-soup strategy to efficiently adapt to diverse reward functions from various real-world users. By fine-tuning a pre-trained LLM multiple times each with a specialized reward function and interpolating their weights linearly, the proposed method is able to adapt to various reward functions without having to train a new LLM per user.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The proposed method is much more efficient than the baselines, which have to train a separate model when a new reward function is given.
- The evaluation is through and conducted on a variety of LLM tasks.
- The performance of the proposed method does not fall behind compared to the more costly baselines.

Weaknesses:
- The novelty of the paper is weak. It seems the main contribution of the paper is applying the weight interpolation (model soup [1]) technique, which was well-explored in supervised learning, to RLHF. I suggest the authors clarify the paper's novelty (other than applying the model-soup technique to another domain) more clearly.
- The authors point out that in RLHF, the reward function is different per each model, unlike supervised learning where the training objective is the same. I agree with the authors on this point, but the rewards used in the paper's experiments do not seem very heterogeneous to back up the authors' claim. A more realistic scenario would be to experiment with a set of rewards that contradict each other directly (e.g., different reward functions learned from users with conflicting interests).
- The main strength of the proposed method is that it is more efficient in terms of training (fine-tuning) cost and inference cost. However, the paper does not provide a quantitative comparison of these costs between the proposed method and the baselines. Therefore, it is hard to assess how much efficiency gain the proposed method will provide.

[1] Wortsman et al., Model soups: averaging weights of multiple fine-tuned models improves accuracy without increasing inference time, ICML 2022.

Limitations:
Noted above.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The authors present a new strategy to address the heterogeneity of diverse rewards in reinforcement learning. Specifically, they propose 'rewarded soup,' which involves individually training multiple networks, each assigned to a different proxy reward, and then linearly combining these networks. Compared to the multi-objective reinforcement learning baselines, the proposed rewarded soup demonstrates its superiority on several benchmarks, including text-to-text, text-to-image, and control benchmarks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The authors presented a comprehensive study on the topic and the research field, as most arguments in introduction are supported by some references. The motivation is strongly supported and the path to the proposed method is reasonable.

- Presentation is clear, concise, and easy-to-understand, and the idea is simple yet effective. 

- The authors conducted extensive experiments to verify the effectiveness of the proposed rewarded soup, and this includes multiple text-to-text tasks (shown in Section 3.1), image-to-text tasks (Section 3.2), text-to-image tasks (Section 3.3), and control tasks (Section 3.5). For most of the experiments, the improvement against MORL is obvious.

Weaknesses:
Although there are many strengths in the paper, there is a weakness that can be further enhance the overall quality. 

- Ablation studies could be added: While the authors presented many reinforcement learning applications and also showed the improvement, it would be better to include more fine-grained ablation studies, such as how the difference of rewards affects the effectiveness of MORL baseline and the rewarded soup or how the number of networks affects the results.  


Limitations:
The authors have provided sufficient information of limitations and societal impact in the paper. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
This manuscript studies a way to interpolate trained networks' parameters for diverse rewards in a reinforcement learning manner. To be specific, the proposed method introduces a way to achieve Pareto-optimal solutions through linearly weighted parameters after training. Extensive experiments showed the effectiveness of the proposed method on various domains such as text2text, image2text, and text2image.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The manuscript is well-written and organized overall.
- The proposed idea is effective yet efficient as it does not require additional training.
- Extensive experimental results demonstrate the effectiveness of the proposed method on various domains; text generation, image captioning, and diffusion model.

Weaknesses:
Interpolating weights for better performance is not a new concept; model soups, which is mentioned in the manuscript. However, the authors did not provide any comparison with it. When we have N fine-tuned models, rewarded soup can perform better than model soup? 
For example, in the case of an image captioning task, the experimental setup assumes only two rewarded models, differently fined-tuned models on AVA and cafe datasets, respectively. I think there is no reason to hesitate to apply a way of model soup.


Limitations:
I think there is no potential negative societal impact.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes a method of using linear interpolated weight finetuned on different rewards instead of using linear combination of rewards to finetune weight, which is a solution to applying model under different and multiple preference scenarios. The idea is intuitive but works well, it (the RS) can achieve similar performance compared with MORL, while RS reduces the computational cost significantly. The author makes some mathematical hypotheses which empirically hold when all the weight to be interpolated is finetuned from a pretrained model. Also, the author has done a lot of experiments to show the feasibility of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is clearly written. The experimental work is sufficiently done.

The author did a lot of experiments on different task which shows that this interpolating strategy is universal under different application scenarios, while with good performance.

The proposed method reduces the heavy computational requirements for pretuning compared with previous work, which makes it much more applicable and flexible to complex application scenarios.


Weaknesses:
Novelty: The most heavy workload in this paper is applying the strategy to different tasks and testing their performance, while less novel concepts or theories is presented.
 
Condition for Hypothesis: The hypothesis used in the method, such as the linear mode connectivity which states that the combined reward is concave to the weights requires research on the model structure and activation function. There should be some limitations for the design of networks which ensures that the hypothesis holds.

Limitations:
All right in total.

Rating:
5

Confidence:
3

";1
uz7JaQgAJb;"REVIEW 
Summary:
The submission considers McKean-Vlasov Stochastic Differential Equation models, which are a generalization of the more-familiar Ito processes. The difference is that the former additionally feature the evolution of the law of the process $X_t$ (denoted $p_t$) as well as $X_t$ itself. Such processes are the limit as the number of particles in a system approach infinity, and their finite-particle approximation (where $p_t$ becomes some empirical distribution) give rise to a model that exhibits temporal as well as between-particle interactions. 

A key aspect of the work is to posit a model whereby $p_t$ enters only through a term $\mathbb{E}_{y_t \sim p_t}[\varphi(X_t, y_t)]dt$ for some *interaction function* $\varphi$. The advantage of such a formulation is that the transition density of the process then reduces to the solution of a PDE.  The work discusses the properties of MV-SDEs of the aforementioned form that make them a desirable class of models (e.g., non-local interactions between paths, and the ability to incorporate jumps). 

Three neural architectures are proposed, differing in particular for their approach as it relates to the modelling of $p_t$: 
* Implicit Measure: which recasts the empirical measure as a single layer of a neural network (though I did not completely understand how this happens). The implicit measure architecture has the advantages of being able to cope with the missing data setting.
* Empirical Measure: where $\varphi(\cdot, \cdot)$ takes the form of a trainable neural network. 
* Marginal Law: which involves learning a generative model.  

Methods for parameter estimation are presented (Section 4), including methods for the missing data setting based on Brownian bridges. A result is proven (Proposition 5.1) regarding implicit regularization properties. A numerical study is conducted on a number of simulated data examples, real data (for which forecasting is also explored), as well as a generative modelling task. 



Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
* The ideas contained in the submission are highly non-trivial, yet explored in impressive depth from a number of different angles in a sophisticated manner.
* The study of such techniques is impressively comprehensive in terms of both theory and different approaches (giving a number of methodological approaches, as well as some theory). The submission appears to represent the outcome of a significant body of work and investigation. 
* The submission is very well written and presented. The numerical experiments appear well-executed. 

Despite some familiarity with concepts in the submission, I am not an expert on SDE modelling, so can not particularly comment on the novelty in that regard. However, the ideas in the submission appear like ones that are very natural to explore, have been explored well, and are worthy of publication in my opinion. 


Weaknesses:
* As much as I enjoyed the abstract presentation, it would be beneficial to have additional clarity as to settings where MV-SDE modelling may be of interest, so to contextualise the work, or where different approaches would be preferred. It is mentioned that few works have considered such models in machine learning tasks, and it would be beneficial to have a small discussion what the contribution could potentially be there (at a high level, that is). 

* Related to the above, a clearer motivation of the task at hand would be beneficial. The paper takes an SDE-first viewpoint, but isn't the overall goal to fit some sort of particle approximation to the SDE? Some additional discussion and background would be beneficial. 
 
* Some applications are mentioned at present, but it would be nice to have a small discussion something along the lines of ""MV-SDEs are most useful when the goal of interest is to model... "".

* The derivation of the implicit measure architecture was something that I could not properly parse as currently written. 



Limitations:
A little additional discussion would be beneficial (see other parts of this review for specifics). 

Rating:
8

Confidence:
3

REVIEW 
Summary:
This paper considers the problem of parameter estimation from data when the underlying dynamical system is modeled by the MV-SDE. To represent the target MV-SDE, the authors propose two strategies: (i) expressing a layer in a neural network as an expectation with respect to a density and (ii) using generative models to capture distributions that generate observations at different time stamp. With these strategies, the authors then propose to conduct parameter estimation via 1. maximum likelihood estimation, 2. using Brownian bridge for estimation, 3. explicit marginal law estimation. 


Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
Please see the discussion below.

Weaknesses:
1. Poor presentation. Overall this paper is obscure and hard to follow. Here are some detailed examples.
* When stating the underlying MV-SDE model in Eq.(2), it is not clear which terms are known and which terms are to be learned. Specifically, do we know $f$ and $\phi$? Since we are interested in the problem of parameter estimation, both terms should be learned from the observations.
* It is now clear how the proposed implicit measure architecture can be carried out: In Eq.(6), it is not clear what $\mathbb{P}_t$ and $\mathbb{P}_0$ are and why we can estimate the Radon–Nikodym derivative $d \mathbb{P}_t/d\mathbb{P}_0$. Moreover, the authors are motivating the implicit measure architecture as an alternative to the standard empirical measure approach since it can handle the situation when only samples from irregular time stamps are available. However, it is not clear why this is the case.
* In section 4.2, where the authors propose to estimate parameters using the Brownian bridge, it is not clearly how the Brownian bridge comes into play. 
* The maximum likelihood estimation in section 4.1 is proposed in previous work [Sharrock et al. 2021], but is presented as a strategy proposed by this work, which is misleading.

2. Lack concrete contributions. Partly due to the poor presentation of this work, this paper presents no clear contributions. For example, the proposed implicit measure architecture and the marginal law architecture are just two ways to rephrase the standard empirical measure.  The results presented in section 5 are well-known results in the literature, e.g. the Wasserstein gradient flow structure of the MV-SDE.

3. A contradiction between the goal of this project and its fundamental assumption. While the authors motivate the research of MV-SDE to model jump (discontinuous behavior) in time sequence data, they also assume that the drifting term of the MV-SDE is sufficiently regular. This is a clear contradiction. For MV-SDE with regular drifting term and interaction term, it can be proved that the characteristic flow is also regular.



Limitations:
Please see the comments above.

Rating:
2

Confidence:
4

REVIEW 
Summary:
The authors proposed two new methods of modelling McKean--Vlasov SDEs using a neural network, and studied its empirical performance. 

Since I'm not an expert this exact topic, I would like to ask the authors some questions first. I would be happy to raise my score further once I understand the paper better.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
The authors proposed methods that do not model a finite population of the particles, which is a very interesting alternative.

Weaknesses:
Several parts of the paper seem unclear to me at the moment, and I will ask specific questions next.

Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a methodology for simulating McKean-Vlasov (mean-field) equations using standard function approximation techniques, e.g. neural networks. It provides mathematical intuition for these algorithms and evaluates them on a broad suite of benchmarks.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper is very well written with clear exposition of its main points. The proofs of key claims seem broadly correct as well.

The methodology is broadly well justified and uses very intuitive ideas from stochastic analysis. 

In particular, the adaptation of standard neural network techniques for simulating ODEs/SDEs is not entirely applicable here, and so the derived techniques need to account for the estimation of the particle density. The resulting algorithm is novel and an important independent contribution.

The experiment evidence, especially in the Gaussian case, seems to vindicate the intuition of this algorithm and clearly outperforms the chosen baselines.

Weaknesses:
I would say that the ultimate idea is rather simple, i.e. approximating both the drift function (both interactive and interaction-free), and possibly the particle density with some kind of learned approximations.

I appreciate the inclusion of standard deviations in the Tables, however these values seem, particularly in Table 1, to be quite large relative to the proposed gains.

I have some additional questions about the methodology. To summarize, I think this paper makes fairly solid contributions to the simulation of McKean-Vlasov equations, which is an important problem. If my questions are addressed, I would be amenable to raising my score.

Limitations:
None

Rating:
7

Confidence:
2

";0
dnEFueMZ43;"REVIEW 
Summary:
This work presents Q-switch Mixture of Policies (QMP) that identifies shareable behaviors and incorporates shareable behaviors. The authors propose to utilize each task’s learned Q-function to evaluate shareable behaviors, and incorporate helpful behaviors from other tasks to aid the exploration of the current task. Experiments on manipulation and navigation tasks are done to validate the proposed method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1, The paper is written well and the MTRL framework has the potential to generalize to kinds of RL tasks because of its simplicity. The idea of Q-switch is straightforward, but seems to work in the experiments.

2, The analysis is comprehensive, and validates the effectiveness of the behaviors identifying and incorporating. Moreover, behavior sharing seems to be a good complement to parameter sharing. By combining them, the sample-efficiency would get improved further.


Weaknesses:
Though compared with many benchmarks, the experiment environments are sort of over-simplified. I would suggest testing the framework in the meta-world environment with more tasks, like insert peg, pick&place, to make the results more convincing and reliable.

Limitations:
As discussed above in the weaknesses, it's still a good paper to accept.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper studies sharing behaviors between tasks in multi-task reinforcement learning. In the proposed method, each task maintains an independent policy network. During online exploration, it selects the action maximizing the task's Q function among actions proposed by all the tasks' policies. Experimental results show the method improves multi-task performance in three continuous control environments. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well written. The motivation of behavior sharing for MTRL is clear.
2. In experiments, figures and results are clearly presented.


Weaknesses:
1. The paper makes a strong assumption that tasks are only different in reward functions. Many complementary methods, like parameter sharing, tackle a wide problem setting where the transition functions and state spaces can be diverse.
2. In baseline methods: the Fully-Shared-Behaviors baseline, a policy without any task information as input for multi-task RL, is weird. A fully-shared baseline with task identifier input makes more sense.
Two ablation methods seems unnecessary, since the proposed method is simple enough. 
3. In experimental results, the proposed method does not outperform baselines very significantly.

Limitations:
Authors discussed some of the limitations and they should be addressed in future work.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper introduced a new exploration mechanism for MTRL. They suggested training a different policy for each task and “sharing the behaviors” between them. In order to do so, each policy, in a certain state, chooses the most suitable action using its own Q function. The author evaluates several MTRL benchmarks and shows increased sample efficiency and final performances over behavior-sharing baselines.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
$\underline{\text{Clarity:}}$

1. Overall, the paper is coherent and easy to follow
2. The introduction is well-written and the motivation for the work is clear

$\underline{\text{Significance:}}$

1. The method seems quite general and might be useful in many cases

2. To the best of my knowledge the idea of using the Q function as a metric for policy selection is novel

Weaknesses:
My main issue is with the technical soundness of the paper. Throughout the paper, the level of the formulation was relatively low, and I spotted some inaccuracies in notation and claims. In general, I understood the motivation for the Q-switch, but there lacks some theoretical analysis or empirical study to support it. I think this method might be suited for some set of tasks, but probably have a limitation, due to the generalization capabilities of the Q function, that was not discussed in the paper. Here are some more specific examples regarding the technical quality of the paper:

$\underline{\text{In the problem formulation section:}}$

1. The MDP components are not defined. Are the state and action spaces continuous or discrete? Should state that $\gamma \in \mathbb{R}$

2. In line 104 you state that $T$ is a number of tasks in the task set and in line 109 you denote the i’th task in the set as $T_i$. This is a confusing abuse of notation.

3. In line 107 - “We parameterize the multi-task solution as…” what does the solution for a multi-task mean? 

4. In line 109+110 - the objective is not formulated. “the tasks are uniformly sampled during training” - what does that mean? From which distribution the tasks are being sampled? Does the sample accure at the beginning of the training phase once?

5. In line 112 - what is $\pi_i^*$? It is not defined. 

$\underline{\text{In section 4.3:}}$

1. line 173 - “over all the task policies $\pi_j$” -> “over all the task policies $\left[\pi_j\right]_{j=1}^T$” 

2. In line 173 - how does $\pi_i^*$ defined? I believe it is an abuse of notation from section 3.

$\underline{\text{Related work and baselines:}}$

1. Limited coverage of skill learning and intrinsic reward literature. I don’t think the statement in line 101 (“.. assume that the optimal behaviors of different tasks do not conflict with each other”) is true for many skill-learning methods

2. Although this work approach is quite different than intrinsic reward/skill learning, I believe that a standard state visitation bonus should be competitive (or at least a good baseline) for the evaluation benchmarks

$\underline{\text{Experiments:}}$
1. In section 6.2, the chosen baselines (QMP-uniform and QMP-domain knowledge) are too simplistic, please provide a more solid baseline, e.g. the one you suggested in line 178 (a probabilistic mixture). 

2. In Figure 6(c) you show that the best performances were achieved when incorporating both parameter sharing and behavior sharing, and showed that using only parameter sharing beats using only behavior sharing. This raises the question of what would happen if we used different kinds of exploration mechanisms together with your method (e.g. intrinsic exploration). Overall, I feel that this evaluation is limited, both in the variation in testing environments and exploration combinations.

Limitations:
Although the authors raised a valid point in the Limitation section of the paper, I believe other limitations of the method exist and aren’t addressed (please see the weaknesses section for further details).

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes Q-switch Mixture (QMP) for identifying shareable behaviors over tasks and incorporating them to guide exploration. QMP identifies shareable behaviors from other tasks and incorporates them to make exploration efficient. The proposed framework is tested on three different multi-agent tasks and compared with other methods. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper introduces the problem of selective behavior sharing for improving exploration in multi-task reinforcement learning requiring different optimal behaviors. The proposed method consists of a Q-switch for identifying shareable behaviors and is used to guide an exploration scheme incorporating a mixture of policies. The Q-function of each task is used to assess the quality of other task policies’ behaviors when applied to the task. The Q-switch acts as a filter to evaluate the potential relevance of explorative behaviors from other tasks.

Weaknesses:
1. The proposed method aims to simultaneously learn multiple tasks. Do they share the same observation space and action space? If it is true, the contribution of the work is limited. If not, the author should consider how to measure the similarity of two tasks. If the two tasks are quite different, it is hard to transfer samples from one task to the other.
2. For incorporating shareable behaviors, the number of training samples from other tasks may be much less than the number of training samples generated for the current task. It would be hard to learn from training samples from other tasks.
3. The scenarios used in experiments are simple tasks. It would be better to see the performance of the proposed method in complex  problems.


Limitations:
The author has discussed the limitations of the proposed method.

Rating:
4

Confidence:
2

";0
QSJKrO1Qpy;"REVIEW 
Summary:
In this paper, the authors first use Dirichlet energy minimizations on simplicial complexes (SCs) to interpret their effects on mitigating the simplicial oversmoothing. Then, through the lens of spectral simplicial theory, they show the three principles promote the Hodge-aware learning of this architecture, in the sense that the three Hodge subspaces are invariant under its learnable functions and the learning in two nontrivial subspaces are independent and expressive. Moreover,  the authors prove it is stable against small perturbations on the strengths of simplicial connections, and show how three principles can affect the stability. Lastly, we validate our findings on different simplicial tasks, including recovering foreign currency exchange (forex) rates, predicting triadic and tetradic collaborations, and trajectories.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well-written. The introduction and background give a nice overview and motivation for the problem.
2. The problem of simplicial complexes learning is really interesting and in many aspects understudied.
3. In the simplex prediction, the proposed SCCNN achieves promising empirical performance. 
4. The proposed SCCNN is supported by a theoretical analysis.


Weaknesses:
1. Limited applications and examples. It would be interesting to see more applications of the proposed SCCNN over widely used datasets (e.g., citation networks - Cora, CiteSeer, PubMed) for node and graph classification tasks. Moreover, although SCCNN achieves promising performance compared with SC-based model, can the authors compare it with other the state-of-the-art graph neural network (GNN)-based models?
2. Can the authors provide the running time of SCCNN and compare it with the state-of-the-art baselines?
3. How to select the dimension of $k$-simplex in the SCCNN model?

Limitations:
In general, I think this is a good paper with tackling a well motivated task. It would be helpful that this paper explores more datasets and compares with more advanced graph neural network-based models. 


Rating:
4

Confidence:
3

REVIEW 
Summary:
The authors identified the limitations of non-Hodge aware learners on simplicial complex (SC) data and proposed a convolutional structure that 1) decomposes the upper and lower k-Laplacian and 2) takes the inter-simplicial couplings into account. The paper has presented a justification for the performance by analyzing the Dirichlet energy and oversmoothing. Additionally, they provided theoretical perturbations bound to study the robustness of the proposed convolution layer. The claims are supported by experiments on synthetic and real SC datasets.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. [Originality] Incorporate the well-known Hodge theorem into the learning task on simplicial complex. The Hodge theorem provides a good intuition and explanation for the learning of the simplicial signal on SC. 
1. [Quality] Empirical examples on the synthetic datasets on Dirichlet energy and stability bound to support the theoretical claims. 
1. [Clarity] Great overview of the simplicial complex and Hodge decomposition. The authors also provided a motivation/justification for why the proposed layer works with Dirichlet energy minimization. 
1. [Significance] Being able to learn the simplicial signal in different Hodge subspaces is an important task.

Weaknesses:
1. If this framework needs to be applied to graph only having edges (i.e., SC of order 1), one usually can apply something like clique-complex (or any other methods to fill in the Simplicios) from that graph. In this case, $n_k$ is generally large (worst case $n_k = \mathcal O(n^k)$), resulting in a huge $L_k$ matrix. How practical is it to use the proposed method under this scenario?
1. Can you provide a definition/discussion or citation of Hodge-aware? It is not clear to me where it is defined throughout the manuscript. I can get some high-level ideas by reading Theorem 7, but I think it would be nice if you could explicitly call it out at the beginning (e.g., in introduction or background).
1. The discussion for preventing “over-smoothing” in Section 3 is great, it provides some high-level motivations of the choices you made. However, I am not sure if that it can support the claims.  Specifically, to really prevent “over-smoothing” of the Dirichlet energy, shouldn’t we bound the $D(x_k^{\ell+1})$ in other way around, i.e., with a lower bound rather than an upper bound? If we can show that $D(x_k^{\ell+1})$ can be lower bounded, the claim can be more convincing. 
1. Consider adding some high-level intuition on what harmonic flow is using the edge space example (e.g., flow cycling around global topoplogical structures); this will give readers having no background in Hodge decomposition a better understanding of what a “harmonic flow” is.
1. [Typo] L186 there is typo/grammatical issue, do you mean “$\tilde{h}_k = \text{diag}(...)$ is the frequency response of $\mathbb H_k$”?


Limitations:
The paper discusses some of its limitations and requirements/assumptions. No significant social impact is identified from this work. 


Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper aims to propose a convolutional architecture which incorporates the Hodge theory. Specifically, the proposed architecture incorporates the following three properties: uncoupling the lower and upper simplicial adjacencies, accounting for the inter-simplicial couplings, and performing higher-order convolutions.




Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
The paper claims a new architecture that incorporates the following three properties: uncoupling the lower and upper simplicial adjacencies, accounting for the inter-simplicial couplings, and performing higher-order convolutions, but the clear differences with respect to existing simplicial neural models are unclear.

Weaknesses:
- Errors: The paper is very poorly written. The typographic, punctuation, and grammatical errors throughout the paper make it hard to follow. Lines 21, 25, 31, 32, and 37 (in the first page of the paper) are some examples of lines that containing such errors. Many abbreviations are used before defining them. For example, NN (line 30), SCCNN (line 51), MLP (line 65), SCF (line 264) etc., were never defined. Many variables, like \mathcal{V} in line 70 and most of the variables in eq. (4), and terms, like alternating map in line 87, were never introduced.
- Contributions: The contributions of the paper are unclear. In line 104, for example, the authors say, “we inherit the names of three edge subspaces to general k-simplices”, while the Hodge theory is already in place for simplices of all dimensions. The main contribution of the paper, which is supposed to be an architecture that incorporates the Hodge theory, is also not clearly presented. 



Limitations:
Overall, the paper is very hard to follow. The motivation and contributions of the work are not clear. The errors in the paper make it more difficult to follow

Rating:
2

Confidence:
3

REVIEW 
Summary:
This paper introduces a novel architecture designed to operate on simplicial data, drawing its foundation from the Hodge decomposition. This decomposition ensures that features associated with a simplicial complex of order $k$ can be represented by three distinct quantities: a curl-free quantity, a divergence-free quantity, and a harmonic quantity.
The authors develop SCCNN, a new architecture for simplicial data that abides by these decomposition principles. The authors demonstrate the relevance of these principles by examining the Dirichlet energy, serving as a measure of oversmoothing within simplicial networks. The authors show that this new architecture reduces oversmoothness.
Theoretical guarantees for the stability of the proposed method when subjected to small perturbations are proven. The proposed approach is benchmarked against two exploratory tasks: a forex test and simplex prediction. The findings indicate superior performance of the SCCNN over previous architectures in these tasks.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- This paper delivers a substantial theoretical advancement to the field of neural networks operating on simplicial complexes. It introduces compelling theorems within a well-structured framework. Theorem 6. on the expressiveness of the SCCNN model is a result of potentially good interest for this subfield.
-  The authors have thoroughly compared their approach to existing methods, including standard graph neural networks (GNNs) and other simplicial neural networks. The results on toy examples are compelling.
- The paper does a good job of introducing the key concepts in a clear and precise way. I appreciated the background on Hodge decomposition. 
- Due to the simplicity of the overall principle behind SCCNN,  it has the potential for broad application within the field and formalizes new fundamental design principles for future architectures.

Weaknesses:
- The paper will benefit from enhanced clarity. Currently, it contains abundant results, which, while potentially insightful, obscure the core message of the research. Streamlining these results and focusing on the most salient points would aid in transmitting the core of the research, which is the SCCNN architecture, more effectively. The discussion around stability, while interesting, feels out of place. There are no clear motivations for studying it so thoroughly in the main text. On the other hand, the critical Theorems, such as Theorems 6. and 7., will benefit from a lengthier exposition. In particular, giving intuition behind the proofs of Theorem 6 would be appreciated. While I appreciate it is a theoretical paper, the results go in every possible direction with no clear target. In such a short paper, one should focus on a few key ideas and move as much as possible to appendices.

- The forex example is somewhat tailored to align with the proposed method. While it proves the paper's point and should be kept, it feels too synthetic. I am happy to be contradicted by the authors on that.

Limitations:
Limitations are discussed. Some points need to be clarified. See my question above. 

Rating:
7

Confidence:
2

REVIEW 
Summary:
The authors propose a general convolutional architecture with principles of uncoupling the lower and upper simplicial adjacencies, which accounting for the inter-simplicial couplings, and performing higher-order convolutions for learning of higher-order structure and simplicial signal.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The authors show that the proposed SCCNN structure demonstrates awareness of the Hodge decomposition and performs efficient learning on simplicial data
- Effect on mitigating simplicial complex oversmoothing is explained with Dirichlet energy minimization
- Comprehensive experimental results 
- Stability against robustness is also studied in this work

Weaknesses:
See questions

Limitations:
n/a

Rating:
5

Confidence:
2

";0
Of0GBzow8P;"REVIEW 
Summary:
The paper explores the in-context learning (ICL) phenomenon in transformer networks and investigates its transience during training. It presents empirical findings that ICL if emerged in transformers, may not persist as the model continues to be trained. 
The paper highlights the importance of evaluating appropriate validation metrics and suggests stopping training before a computing budget is reached to preserve ICL. 
Strategies to mitigate ICL transience, such as data distributional properties and regularization, are explored. The paper also discusses the competition between ICL and in-weight learning (IWL) circuits and raises questions about the emergence and fading of ICL. 
The experiments demonstrate that ICL in transformers is often transient. Model size, dataset size, and data distribution can influence the transience of ICL, but persistent ICL is not guaranteed. Wider models with larger embedding sizes show improved ICL, possibly due to reduced competition with IWL circuits. Skewed data distributions mitigate ICL transience but necessitate longer training.
Future directions, limitations, and implications for large language models are discussed. Overall, the paper provides valuable insights into the nature of ICL and its implications for transformer networks, contributing to the understanding of in-context learning and its behavior during training.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Originality and Contribution:
- Empirical findings on the transience of in-context learning (ICL): The paper presents novel empirical evidence that ICL if emerged in transformer networks, may not persist as the model continues to be trained. This finding sheds light on an important characteristic of transformer models and highlights the need for appropriate evaluation and validation metrics.
- It provides insights into how ICL emerges, disappears, and gives way to in-weight learning (IWL) during training.
- Discussion of the coexistence of ICL and in-weights learning (IWL): The paper explores the competition between ICL and IWL circuits for resources in the transformer residual stream. It raises intriguing questions about the emergence and fading of ICL and offers potential explanations for the coexistence of ICL and IWL in large language models.
- Exploration of strategies to mitigate ICL transience: The paper investigates various strategies to mitigate the transience of ICL, including increasing the number of classes, introducing Zipfian data distribution, and using L2 regularization. These strategies provide valuable insights into potential methods for preserving and enhancing ICL capabilities.
- It is interesting that ICL is transient even under conditions that are deemed conducive to the emergence of ICL.
- Comprehensive and thought-provoking discussion, posing clear and relevant questions: discussion of future research directions and acknowledgment of the limitations of the conducted experiments. It identifies the need for experiments on large language models, exploration of other datasets and context lengths, and investigation of additional mechanisms in transformers, encouraging further advancements in the field.
- Consideration of the impact of training duration and stopping criteria: The paper emphasizes the importance of appropriate training duration and stopping criteria to preserving ICL. It suggests that training large transformer models beyond a certain point may result in the gradual disappearance of ICL, even if training losses continue to decrease. This insight has practical implications for model development and training practices.

Research Problem and Motivation:
- The research problem is well-defined and clearly stated, focusing on the transience of ICL in transformer networks.
- The paper provides a convincing motivation for the chosen problem by highlighting the importance of evaluating appropriate validation metrics, preserving ICL, and understanding the factors influencing its persistence.
- The relevance and importance of the research problem are clearly explained, emphasizing the potential impact on training large-scale language models.
- The paper raises numerous additional questions, stimulating further scientific inquiry and exploration.

Methodology:
- The methodology is clearly described and well-founded. The paper outlines the experimental setup, including the models, training procedures, and evaluation metrics used.
- The algorithms, models, and techniques employed are appropriate for addressing the research problem of studying ICL transience in transformer networks.
- The paper adequately explains the experimental setups and data collection processes, providing sufficient details for reproducibility.

Experimental Evaluation:
- The experimental results are presented clearly and concisely, with appropriate figures and tables.
- The evaluation methodology is rigorous and appropriate, considering various metrics and comparing experimental conditions.
- While the statistical significance of the experimental results is not explicitly mentioned, the paper conducts a thorough analysis and provides empirical evidence to support the presented findings.
- The rigorous analyses demonstrate a commitment to scientific rigor and methodology.

Related Work:
- The paper comprehensively reviews the relevant literature, discussing previous works on ICL, language models, optimization, and initialization methods.
- The paper differentiates itself from existing works by focusing on the transience of ICL and investigating strategies to mitigate it.
- The references and citations are appropriate and up-to-date, incorporating relevant studies in the field.

Clarity and Organization:
- The paper is well-structured with clear sections and subheadings, making it easy to follow the flow of ideas.
- The ideas and concepts are presented logically and coherently, building upon each other to support the main arguments.
- The language used in the paper is clear and concise, minimizing excessive technical jargon and enhancing readability.

Presentation and Visualization:
- The plots are visually appealing, utilizing scalable vector graphics and an attractive gradient color scheme. The figure captions are concise and informative.
- The writing is of high quality, with minimal errors or inconsistencies.
- The overall presentation is visually appealing and well-designed, enhancing the readability and comprehension of the paper.

Weaknesses:
- Lack of experiments on large-scale datasets: The experiments mainly focus on controlled synthetic datasets, and the impact of ICL transience on real-world, large-scale datasets is not explored. Including experiments on diverse, natural language datasets would enhance the practical relevance of the findings.

- Limited scope of experiments: The experiments were not performed on large language models, one of the most important applications of transformers. Conducting experiments on larger models would provide a more comprehensive understanding of the phenomenon and its implications.

- Narrow definitions of ICL and IWL: The definitions of in-context learning (ICL) and in-weights learning (IWL) used in the paper are relatively specific and make for a clear-cut distinction that in reality - as acknowledged in a footnote - may not be that clear. Exploring additional mechanisms and interactions between context and learning could contribute to a more comprehensive understanding of transformer models.

- Limited exploration of alternative explanations: The paper acknowledges possible explanations for the emergence and transience of in-context learning but does not thoroughly explore alternative hypotheses. Further investigation into optimization quirks, initialization schemes, or alternate optimizers could provide deeper insights into the phenomenon.

Overall, the above-mentioned points can be seen more as ideas for future work rather than genuine weaknesses of the paper. 

Minor points:
- Lack of statistical significance analysis: Although the experimental results are presented clearly, the paper does not explicitly address the statistical significance of the findings. Including statistical tests or discussing the significance of the observed differences would strengthen the conclusions.

- (minor) Maybe explain in-weight learning in the abstract, contrast it to in-context learning
- (minor) second to last sentence in the abstract (l. 13), reformulate: how does that question logically follow from the observation?
- (minor) l. 18 Reformulate definition to be clearer, order of words
- redo the references, missing information (ls. 373, 394, 408), inconsistent formatting (e.g., ls. 307, 315)
- Capitalize ""In-Context"" instead of ""In-context"" in the title.
- Clearer motivation for why researching ICL is interesting and relevant?


Limitations:
While the potential negative societal impacts are not explicitly discussed in the provided excerpt, the authors have identified areas for further investigation and improvement in their research, which addresses some limitations of their work. 

Since this work focuses primarily on analytical rather than directly enhancing capabilities, there are no immediate negative effects to be considered.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper investigates the emergence and persistence of in-context learning (ICL) in Transformer models. Training Transformer models on tasks by Chan et al. [10], the authors find that ICL does not persist, despite an initial emergence. Loss reduction suggests a preference for in-weight learning after some training steps. Through specialized experiments, the authors explore model size, training dataset statistics, and L2 regularization to understand and mitigate this phenomenon. The findings emphasize the importance of L2 regularization in Transformers during extended training. This research provides valuable insights into ICL dynamics, contributing to better understanding of this phenomenon and offering insights into potential improved training strategies.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. The paper builds upon the established dataset by Chan et al. [10], ensuring reproducibility of results. The phenomenon of interest is consistently observed across different scenarios, allowing for comprehensive examination. The inclusion of small-scale experiments facilitates further investigation by researchers. The availability of the code, given that it will be released, will help with reproducibility and transparency of the study.
2. The authors adopt a systematic approach to address the problem at hand, systematically eliminating various factors that contribute to the lack of persistence in in-context learning (ICL). Through controlled experiments, they explore and analyse multiple aspects that may impact ICL persistence.
3. The paper has a well-structured and coherent storyline, ensuring clarity of the results. The authors effectively motivate each experiment, providing a clear rationale for their choices. I enjoyed reading the narrative presented by the authors.

Weaknesses:
1. The phenomenon is observed for a single small dataset. Although some experiments are made with respect to aspects of the training data, i.e. number of classes and number of exemplars per class, the small relative size of the training data in conjunction with the large number of training steps tested for here, may potentially lead to conclusions that do not generalize past this setting. Some attempts were made to match data distribution with naturalistic data, but any conclusion that generalize to large language models have to be taken with a grain of salt.
2. Although a lot of different effects are explored and some correlations explored, no clear answer is given in the end. 

Limitations:
The paper discusses a limited scenario where ICL is not persistent. Experiments are performed but always in the context of the provided dataset and for smaller models. More experiments are in general required to verify the existence of this phenomenon elsewhere. The study is nonetheless very interesting and useful for the community.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper explores In-Context Learning (ICL), a model's ability to adapt its behavior during inference without weight updates, and In-Weights Learning (IWL), where the model relies on information stored in its weights. While early neural networks displayed ICL in controlled scenarios, research changed with the transformer model, particularly with GPT-3, which exhibited ICL without deliberate training efforts. However, it was found that ICL isn't guaranteed when training transformers, with language data properties like burstiness and skewed distribution playing a crucial role. Additionally, ICL and IWL often seem to oppose each other, with ICL emerging more readily when training data has a larger number of tokens or classes and is bursty. Despite the challenges of relying on large models, overtraining is currently a favored approach for training small yet high-performing transformers. The paper challenges the assumption of persistence (that once ICL emerges it remains), showing that ICL can be transient, with the potential to disappear if networks are trained carelessly for longer periods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
On the positive side, the paper is certainly quite interesting and relevant to the major open question these days, which is the emergent capability of large language models. The paper raises a number of interesting open questions (see Discussion in section 7). In fact, I would argue that that is the most interesting section of the paper.

Weaknesses:
On the negative side, the paper can be viewed as ""half-baked"" at this point. It relies heavily on some empirical results, derived mostly from small models and visual tasks, and it raises a number of questions that are interesting but.. open. I wish the paper could go a bit more deeply to actually answer at least one of those questions. At this point I cannot say with confidence that the paper is ready to be presented at the premier conference of deep learning. 

Limitations:
The paper is quite open about its limitations, mentioning clearly that it has not utilized very large models and so it is not really certain whether the major claims of this study would be actually still seen in large models such as GPT3 or 4. Additionally the paper does not try to ""oversell"" what it does -- and it identifies many open questions instead of claiming that it already has answers to those questions.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper presents empirical evidence illustrating the transient nature of in-context learning (ICL) in transformers, and it shows that the models may shift to in-weights learning (IWL) when overtrained. Furthermore, the transience of ICL is studied across diverse settings such as model size, dataset size, data distributions, and training regularization. Results reveal that adequate L2 regularization can mitigate this transience.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. This paper reveals a characteristic of transformers performing transient ICL.
2. The proposition that regularization can eliminate the transience is noteworthy.
3. The provided experiments are clearly organized and easy to follow.

Weaknesses:
1. The paper lacks clarity in its notations, which makes them hard to follow without reference to previous literature. For instance, the term ""in-weights learning"" needs to be related to supervised learning; in the Zipfian distribution equation $p(X = x) \propto x^{-\alpha}$, the terms $X$ or $x$ should be explained as representing the rank of input classes.
2. While the authors provide numerous empirical evidences, they scarcely discuss potential underlying reasons. Some interpretations or conjectures would enrich the discussion around the observed outcomes.
3. The experiments are constrained to a single dataset and transformer (with different sizes), which is not convincing enough in my opinion. It would be interesting to see if the similar phenomenon is observed with other datasets (e.g., NLP dataset) or other auto-regressive models. The usage of the Omniglot dataset—a stationary supervised dataset—where in-context samples can be regarded as independent entries, makes the existence of IWL unsurprising.

Limitations:
yes

Rating:
5

Confidence:
2

";1
g6We1SwaY9;"REVIEW 
Summary:
This paper proposes  BLIP-Diffusion, a new subject-driven image generation model with multimodal encoder that supports multimodal control which consumes inputs of subject images and text prompts.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
The proposed method is novel and enables subject-driven generation under efficient fine-tuning and zero-shot setups.

The proposed model can serve as a foundation model and combined with previous methods such as prompt-to-prompt and controlnet.

Solid experimental results and ablations.

Weaknesses:
The contribution is a bit lacking. It seems to me that the work is just a combination and the multimodal encoder from CLIP and the t2i model.

The improvement under efficient fine-tuning setup seems to be incremental.

Limitations:
Yes.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper aims to solve the subject-driven text-to-image generation with a pre-trained subject representation that is derived from a vision-language encoder, the BLIP2 model. The obtained subject representation captures rich information of the visual input while being aligned with the textual space. The text-conditioned diffusion model is pre-trained to produce images based on this subject representation and the text input. The derived diffusion model can produce novel impressive renditions of a given subject with respect to different text prompts, in both zero-shot and few-step finetuned scenarios. Beyond that, the paper presents fancy applications like controlled generations and subject-driven stylization. Compared to the state-of-the-art approaches, the proposed BLIP-Diffusion demonstrates preferable subject-driven generation results both qualitatively and quantitatively while offering significant speed-up.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper explicitly leverages the text-aligned image feature and the diffusion model trained on this subject representation is able to generate subject renditions with compelling quality. The novel combination with the BLIP2 model forms an effective solution to subject-driven text-to-image generation. The method presented is neat and elegant.

Moreover, I like many technical details authors propose to achieve the final quality.  For example, authors carefully design a synthetic procedure to form the synthetic training pairs to tackle the tendency to produce trivial solutions. Also, it is found that randomly dropping the subject prompt with some probability is beneficial to text-to-image generation.

The paper demonstrates fancy applications in various applications and presents impressive generation results. the significant speed-up for the subject-driven generation makes it promising in practical usage. The quantitative study further proves the superiority of the method over prior leading approaches. 





 


Weaknesses:
- One possible way to explain the method is that this paper uses a detailed text prompt instead of a rough description like ""an image of [V]"" as used in Dreambooth. That is, the features captured from the BLIP2 may not be the key. One simpler baseline is ""Encoder-based Domain Tuning for Fast Personalization of Text-to-Image Models"". The paper should mention such baseline and add more discussions regarding this.

- To avoid the distraction of the background, the authors propose to replace the background of the target image with random background. Then why not just remove the background during training?

- One primary usage of subject-driven text-to-image models is to generate images related to humans. However, the paper avoids this scenario by explicitly removing human-related images during training, why is that? It is suggested to have a discussion regarding this, otherwise, it is suspicious that the method does not perform well on portraits. 

- While the paper qualitatively measures the fidelity to the input image and the text prompt respectively, there is no measure for image quality. While in the chosen samples of Figure 6 the BLIP-Diffusion shows apparent quality advantage over prior arts, it still needs a quantitative measure to reflect the image quality on a large amount of image results.T

Limitations:
The limitation is sufficiently discussed in the supplementary material.

Rating:
8

Confidence:
5

REVIEW 
Summary:
The paper introduces ""BLIP-Diffusion"", a new subject-driven image generation model that supports multimodal control using subject images and text prompts. The model introduces a pre-trained multimodal encoder to provide subject representation and enables zero-shot subject-driven generation and efficient fine-tuning for customized subjects. This model can be combined with existing techniques to enable novel subject-driven generation and editing applications. 

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. Originality: The paper introduces a novel subject-driven image generation model, BLIP-Diffusion, which supports multimodal control using subject images and text prompts. This model is original in its approach as it combines a pre-trained multimodal encoder for subject representation, enabling zero-shot subject-driven generation and efficient fine-tuning for customized subjects.

2. Quality: The quality of the paper is evident in the detailed explanation of the model and the comprehensive experiments conducted to validate its performance. The paper includes qualitative results that demonstrate the model's capabilities, such as zero-shot subject-driven generation and high-fidelity fine-tuning. The model also shows high subject fidelity and prompt relevance, requiring significantly fewer fine-tuning steps compared to other methods.

3. Clarity: The paper is well-structured and clear in its presentation. The authors provide a thorough explanation of the model, its implementation, and the experiments conducted. The use of figures and tables further enhances the clarity of the paper, providing visual representations of the model's performance and capabilities.

4. Significance: The significance of the paper lies in its contribution to the field of image generation. The BLIP-Diffusion model presents a new approach to subject-driven image generation, offering potential for novel subject-driven generation and editing applications. The model's ability to perform zero-shot subject-driven generation and efficient fine-tuning for customized subjects is a significant advancement in this field.

Weaknesses:
Limited Zero-Shot Performance and Dependence on Fine-Tuning: The paper claims that the proposed BLIP-Diffusion model can perform zero-shot rendering of images across various categories of subjects. However, the results presented do not fully substantiate this claim. Both qualitatively and quantitatively, the zero-shot results are not as impressive as one might expect. Furthermore, the model's performance seems to heavily rely on fine-tuning. While fine-tuning is a common practice in machine learning, the extent to which the model depends on it raises questions about its practicality and efficiency. The necessity of fine-tuning to achieve good results could be seen as a limitation, especially in scenarios where rapid or on-the-fly generation is required. This dependence on fine-tuning could limit the model's applicability and ease of use in certain contexts.

Limitations:
The paper's main claim is that the proposed BLIP-Diffusion model can perform zero-shot rendering of images across various categories of subjects. However, the results presented do not fully substantiate this claim. Both qualitatively and quantitatively, the zero-shot results are not as impressive as one might expect. Furthermore, the model's performance seems to heavily rely on fine-tuning. While fine-tuning is a common practice in machine learning, the extent to which the model depends on it raises questions about its practicality and efficiency. The necessity of fine-tuning to achieve good results could be seen as a limitation, especially in scenarios where rapid or on-the-fly generation is required. This dependence on fine-tuning could limit the model's applicability and ease of use in certain contexts.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper addresses the issues of lengthy fine-tuning and preserving the subject fidelity in subject-driven text-to-image generation models. Different from existing models such as Textual Inversion and Dreambooth that invert subject visuals into text embedding space, the paper introduces a new multimodal encoder which is pre-trained to produce visual representation aligned with the text. New subject renditions are then generated using such visual representation. The paper highlights the utility of the proposed approach in zero-shot subject-driven generation and editing applications, and demonstrates 20x speedup in fine-tuning for customized subjects.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well-motivated and well-written. The key idea of the work to deeply align subject embedding and the text embedding is interesting. The proposed model is compatible with ControlNet and Prompt-to-Prompt, which has potential to unleash several important editing capabilities. The experiment results are solid, with sufficient evaluations.

Weaknesses:
I see the key novelty of the proposed approach in section 3.2 where the paper introduces the subject representation learning. The paper mentions that output of the BLIP-2 multimodal encoder is passed to CLIP Text Encoder by combining the text and subject embeddings. I suggest authors elaborate this to provide more concrete details. We get embeddings as output from CLIP Text Encoder right? How is it possible to combine them before passing as input to CLIP Text Encoder. I might be missing something here, would be great if the paper clarifies this in rebuttal. Also, it is not clear if the paper additionally finetunes the CLIP Text encoder. In lines 164-164, it is mentioned that the Text encoder is also fine-tuned. But wouldn’t that bring language drift issues? Are there any specific strategies used by the paper in fine-tuning text encoder that aid in avoiding the language drift problem? Please clarify. Also, in section 3.2, how many synthetic pairs are used? I am happy to revise my final rating based on the clarifications in the rebuttal.

Limitations:
Authors adequately addressed the limitations.

Rating:
6

Confidence:
5

";1
dQLsvKNwZC;"REVIEW 
Summary:
Paper considers the generalized safe exploration problem (problem 4) and compares it to the other safe exploration problems in the literature, leading to Thm 3.1 that concludes that it is more general than the others. The authors then introduce MASE, a meta-algorithm for safe exploration, that attempts to solve the GSE problem with the ability to execute an emergency stop ""beforehand"" as opposed to others that have done it afterwards. Section 6 then presents a more practical algorithm using GP models, which is then used in section 7 to compare the new approach with several baselines.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well presented, with several technical results about the safety analysis, culminating in Thms 5.6 and 5.7 which show that GLM-MASE guarantees safety with high probability for every time step.

Several numerical experiments are presented to compare with unconstrained and SOA constrained RL algorithms.

Weaknesses:
The development in the paper assumes access to an emergency stop action that enables the agent to avoid violating a safety constraint. This assumptions seems difficult to achieve in practice as for many agents of interest, stopping is not a safe state. Further, the action would typically be state dependent, requiring at a minimum a emergency action policy (rather than action). Finding either action or policy seems non trivial and could possibly be overly simplifying most problems of interest.

Given the comment about [33] at the bottom of page 4, I would have expected to see a comparison of these approaches to imposing an action to avoid an unsafe state before/after a training epoch in the numerical results. If that is included in section 7, suggest highlight that discussion point more.

The footnote on page 3 makes reference to this being a conservative approximation of that in safe RL problems with chance constraints, and consigns the discussion Appendix B. It would seem like more clarification than that is needed here. Also, the more recent work by Pavone in this area will be of interest:
* Lucas Janson, Edward Schmerling, and Marco Pavone, “Monte Carlo motion planning for robot trajectory optimization under uncertainty.” In Robotics Research, pages 343–361. Springer, 2018.
* Anirudha Majumdar and Marco Pavone “How Should a Robot Assess Risk? Towards an Axiomatic Theory of Risk in Robotics,” https://doi.org/10.48550/arXiv.1710.11040

Fig 2 shows that MASE satisfies the constraints, but if I understand the plot correctly, this is achieved with very conservative margins (constraint at 20, values are typical ≤ 5). This is similar to Saute TRPO, but perhaps suggest why the performance (episode return) is so weak compared to the unconstrained solutions (that don’t violate the constraints by much). Is there a way to trade off  this conservatism to achieve better performance?

Very hard to see the frequency with which TRPO Lagrangian violates the constraints given the lines/colors on fig 2.

Limitations:
See little discussion of this point in the paper.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies the safe RL problem with a generalized stepwise safety chance (probability) constraint, essential for many safety-critical systems with RL. The authors propose a meta-algorithm to solve this problem (MASE), by combining unconstrained RL with an uncertainty quantifier to guarantee safety with probability. They study two variants for MASE, one for the linear model and the other for GP. Experiments on grid-world and safety gym show that MASE with GSE formulation achieves SOTA compared to the baselines. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well-written and easy to follow. The proposed GSE problem is more general than the CMDP formulation with an additive expectation constraint. Especially, the GSE problem is an important problem to study for many real-world safety-critical systems, such as autonomous cars and cars. 
The proposed method looks sound and correct to me. 
The MASE algorithm essentially builds on Assumption 3.4 (uncertainty quantifier) which could also be a potential bottleneck or limitation, but the author did a good job by proposing a general linear model and GP for it. 
The experimental results show better results in terms of safety violations, compared to other CMDP-based approaches. 

Weaknesses:
1. The author may want to at least discuss this ICML paper in their related work. The hard safety chance constraint is highly relevant to the GSE problem in this paper, although it is indeed using different approaches to solve the problem. 
Wang, Y., Zhan, S. S., Jiao, R., Wang, Z., Jin, W., Yang, Z., ... & Zhu, Q. (2022). Enforcing Hard Constraints with Soft Barriers: Safe Reinforcement Learning in Unknown Stochastic Environments. arXiv preprint arXiv:2209.15090.

2. The authors should discuss the limitations of this work

3. In Method, MASE needs to compute a safe action set, how complex this computation is? In order to compute it, what kind of assumptions of the underlying environments do you acquire? 

Limitations:
I would like to see the authors' opinions on the limitations in their responses. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors present a novel safe RL algorithm for safety constraints with probability one. First, the authors present a problem formulation that can be used to derive different common safe RL formulation (state constraints, accumulated constraints etc). Then the authors leverage a sophisticated technique to reshape the reward accounting for safety information. In particular, they predict if the following actions will be safe by learning the action safety state similar to safety shield techniques. Their shields are learning using Gaussian processes and offer safety predictions with uncertainty estimates.`


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The algorithm is technically sound, novel and quite interesting as an idea.
1. The authors present an extensive analysis for the case of generalized linear CMPDs. The theoretical results seem to be correct, but I have a few minor concerns regarding the statements 
2. The numerical results are impressive but shown only for one environment. 


Weaknesses:
1. The statement of Theorem 3.1 reads there are instances of the GSE problem that are not equivalent and cannot be transformed into Problem 1, 2 or 3. This is not shown. 
1. The fact that Problem 2 can be transformed into the GSE problem does not mean the GSE problem is more general than Problem 2. If the GSE problem can be transformed to Problem 2 as well, then they are equivalent. Furthermore, if the accumulated cost is used as a state as in Lemma A.1, then the problems cannot truly be equivalent. I think the authors should rephrase these results and make more accurate claims. 
1. The method does remind me of safety layer techniques [Dalal 18] and [28] with a more sophisticated reward-shaping approach. Can the authors provide a short discussion on the relation to these papers?
1. The numerical results can be improved 
     * I recommend adding boxplots to the simulation results to see the distributions of the traces. For instance the boxplots of the trajectories for the final epoch as in [28]
     * I recommend providing more epochs in the experiments. I suspect that the algorithms generally achieve similar performance, but the algorithms with probability one constraint simply converge slower.

* [Dalal 18] Dalal, G., Dvijotham, K., Vecerik, M., Hester, T., Paduraru, C., & Tassa, Y. (2018). Safe exploration in continuous action spaces. arXiv preprint arXiv:1801.08757.
* [Yu 22] Yu, Haonan, Wei Xu, and Haichao Zhang. ""Towards safe reinforcement learning with a safety editor policy."" Advances in Neural Information Processing Systems 35 (2022): 2608-2621.

Limitations:
There are two limitations of the approach: 1) the scalability of the algorithm and 2) the restrictiveness of the problem definition. While the authors discuss the latter limitation and show that this problem formulation is important, I didn’t find a discussion on scalability. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper addresses the problem of safe reinforcement learning, in particular safe exploration. In a nutshell, the authors provide an algorithm that is supposed to go beyond standard 'safety' measures in safe RL, such as the constrained MDP setting. There, an agent is bound to satisfy an additional (expected) cost constraint. Here, the authors postulate that an agent must satisfy a constraint almost surely or with high probability. As a key feature of their approach, the authors assume that a so-called  'emergency stop action' is available that allows the agent to always have a fallback. The authors provide a general framework in the form of an algorithm, prove its theoretical guarantees, and evaluate the method on a set of standard benchmarks.

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
The authors tackle a very important problem, safe exploration in RL. Moreover, they identify correctly that the standard constrained RL (or constrained MDP) setting is generally insufficient to ensure safety during exploration. In principle, the constrained RL setting provides just an incentive to act safely during training, and even after training, safety depends on an expectation to satisfy a safety constraint. Generally, the paper is well-written and easy to follow.

Weaknesses:
I like the paper in general, but in its current state cannot be accepted, in my opinion. The reason is a severe lack of related work. Most importantly, the authors seem unaware of a flavor of safe RL that is often referred to as 'shielding.' In these settings, a so-called shield 'blocks' unsafe actions according to some pre-defined safety measure. [1] was the first paper to introduce this aspect, [2] introduced shields for almost-sure properties in partially observable environments, [3] provides shields that satisfy a property with a certain probability, [4] provides a shielding mechanism for multi-agent settings. There are many more relevant works. The general shielding framework depends on varying assumptions, but the general procedure looks like the MASE algorithm. Note that safety during exploration/training is the most important motivation for shielding. I encourage the authors to thoroughly compare these (and more) works to explain the contribution and novelty better.  

[1] Alshiekh et al.: Safe Reinforcement Learning via Shielding. AAAI 2018

[2] Carr et al.: Safe Reinforcement Learning via Shielding for POMDPs. AAAI 2023

[3] Jansen et al.: Safe Reinforcement Learning Using Probabilistic Shields. CONCUR 2020

[4] Melcer et al.: Shield Decentralization for Safe Multi-Agent Reinforcement Learning. NeurIPS 2022

Moreover, I am not convinced by the experimental evaluation. The emergency action seems central to the approach, but I fail to see how it has been integrated into the standard benchmarks. Then, how often is it called during training by the agent? How does it impede an agent's exploration rate? 


minor comments

l92: In the definition of a policy, it seems to be non-stochastic. In such a multi-objective setting, it might be beneficial to use stochastic policies. Have you considered this? 

l 125/126: in a probabilistic setting, there might be states with a high probability of violating a safety constraint but do not already violate it. It could be interesting to consider such information in the value function.

Limitations:
The limitations are not properly addressed, see my earlier comments on the evaluation.

Rating:
5

Confidence:
4

";1
EmYWJsyad4;"REVIEW 
Summary:
The paper mentions that current disentanglement techniques in RL require features to be independent and cannot disentangle correlated features. Therefore the paper proposes an auxiliary task for RL methods based on conditional mutual information (CMI) which can learn to disentangle observations with correlated features. The authors propose a conditional set which makes the features independent and details the process of learning conditionally independent representations. Since the conditioning set is based on the entire history of actions and past features, the authors claim that using the last one time-step of action and features is enough for good generalisation performance. The results section shows that the proposed method works well when reverse correlation is encountered during testing and also provides ablations to show the efficacy of different design choices.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The paper is very well written, and I found it very easy to follow through with detailed figures highlighting the conditioning set and the architecture.
- The paper very clearly explains how to minimise the proposed CMI with explanations of how to sample from the joint and product of marginals distribution.
- The proposed auxiliary task can be used on top of existing RL methods with just adding a discriminator and an adversarial loss.
- The authors have shared their code for reproducibility.
- There are clear ablation experiments to showcase the efficacy of different design choices made (correlation %, conditioning set, history length, CMID loss coefficient).


Weaknesses:
- The experiments seem a bit limited as the proposed method is evaluated on four DMC tasks with only a couple of colour/size variations which raises concerns regarding the generalizability/scalability of this approach. The paper could have been more insightful with another domain with more complex correlations where longer history is required for the conditioning set. That would also have helped with understanding if the kNN permutations used would work well with increasing dimensions (N) of the feature space.


Limitations:
Yes, the authors have pointed out limitations of their approach.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper proposes an auxiliary task based on minimising mutual information in the learned latent representation. While several approaches have adopted similar loss functions for learning representations, the difference here is that the mutual information is conditioned on the history of past (latent) states and actions. This has the effect of disentangling correlated features, which improves performance when the agent interacts with a new task where those correlations no longer exist. Experiments on continuous control tasks illustrate that the method better generalises to tasks where these correlations are not present, whereas other approaches (also based on mutual information) fail to zero-shot transfer. 


Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
The paper is well-written and easy to follow, while the experimental results clearly show the advantage of the method over other auxiliary tasks that have previously been proposed. 

Although I am not overly familiar with the literature in the space, I believe that the proposed method, while a variation of existing approaches, is novel.

The auxiliary task is well motivated, and the implementation of it within a deep RL framework is elegant, which could likely make it easy for others to incorporate the idea into their own learning algorithms.

Weaknesses:
Too much of the experiment section is in the appendix. As it stands, the main paper is not readable without reference to the appendix. I realise that space is an issue, but it would be best if the main text served as a standalone document. For example, there is no explanation of what reversed vs no correlation means (although I can guess). In the main text,  only the pendulum task is described - the rest are not even briefly mentioned. To free up space, I recommend moving the discussion of history length and coefficient into the appendix instead. These can be briefly mentioned, but they serve as additional ablations and are not key to the story. 

Given Figure 13, the saliency maps in the main text seem a bit cherry-picked, since many of the other ones are not as clean. While the RL performance clearly shows a win, it would be nice to see more evidence that the auxiliary task promotes the kind of disentanglement that is expected. Would it not be possible to investigate this in the non-RL setting? For example, collect data and then simply apply the representation learning part alone to that data. The resulting representations could also then be evaluated using existing metrics from the representation learning literature (e.g. MIG score, D/C/I, FactorVAE, etc.)

While the tasks are engineered to demonstrate the point of the paper (by synthetically causing entanglement), it would have been nice to see the approach tested in a task that wasn't quite as synthetic and hadn't been formulated precisely to showcase the strengths of the method. 

Minor:

The intro contains many different examples; it would be clearer to stick with one running one consistently.

The discussion in Section 4.1 may be inaccessible to someone in RL who is not familiar with graphical models, causal learning, etc. Given that space is an issue, it might be helpful to expand on some of these ideas in the appendix (e.g. explaining the concept of a collider). Perhaps Figure 1 could serve as a diagram where various concepts could be illustrated.

Limitations:
The limitations of the method with regard to simple, synthetic correlations are discussed well. I wonder if one further limitation could be the reliance on the Euclidean distance metric. While it will likely be fine in the case of the state representation and continuous actions, I'm wondering specifically about the case of discrete actions, since the IDs attached to each action are arbitrary and so Euclidean distance between actions is meaningless.

Rating:
7

Confidence:
2

REVIEW 
Summary:
This paper presents a novel approach to disentanglement in RL, focusing on the challenge of spurious correlations in high-dimensional observations.  
The authors argue that conventional disentanglement techniques, which rely on minimizing mutual information between features, can struggle with the task of disentangling correlated features, leading to a generalization failure when these correlations shift.  
To address this issue, they propose an auxiliary task termed Conditional Mutual Information for Disentanglement (CMID) which can be added to any RL algorithms.  
CMID constructs disentangled representations by minimizing the conditional mutual information between dimensions in the latent representation.  
The authors experiment with continuous control tasks and show that CMID enhances training and generalization performance of base RL algorithm SVEA, and outperforms other state-of-the-art baselines, supporting its efficacy in handling correlated features and correlation shifts.


Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
This paper exhibits several significant strengths that enhance its value:
* The paper is meticulously written with a clear and organized structure that allows for easy comprehension of its contents. Each claim is sufficiently justified and is coherently interconnected to form a compelling narrative.  
* The authors present a versatile and elegant method, CMID, that can be conveniently integrated with any model-free RL algorithm. This broad applicability enhances the practical relevance of the proposed method.
* The presented results offer persuasive evidence for the effectiveness of the proposed method. Notably, improvements in both training performance and zero-shot generalization are demonstrated, indicating successful disentanglement of information. 
* The analysis of the agent's attributions provides a valuable qualitative insight into the learned representations. This analysis adds depth to the study and helps to illustrate the inner workings and benefits of the proposed approach.


Weaknesses:
* One of the main drawbacks of the proposed method is the computational overhead, particularly due to the loop over the features during the computation of the adversarial loss. While this is a natural consequence of the chosen method, it would be beneficial for the authors to quantify this overhead.

* The paper could also benefit from a more direct qualitative or quantitative evaluation of the disentanglement effectiveness. The provided experiments successfully showcase improvements in terms of reward and zero-shot adaptation, but they do not directly assess how well the disentanglement itself is working. For instance, measuring the impact of changing specific factors (like the color or length of a cartpole) on the N feature would provide a clearer picture of the disentanglement capability of the proposed method and reinforce the authors' claims. This additional experiment, while not necessary for rebuttal, could greatly strengthen the paper's contribution and its implications in practical settings.

Limitations:
The authors adequately addressed current limitations of this work.

Rating:
8

Confidence:
4

REVIEW 
Summary:
Reinforcement learning often suffers from feature correlations from the limited number of data and feature coverage which hinders the generalization to other environments. To this end, this paper proposes to add an auxiliary task for RL algorithms to learn a disentangled representation of high-dimensional observations with the correlated features by minimizing conditional mutual information between features in the representations. The conditional mutual information is minimized by the density ratio trick using batch permutation and discriminator. This method improves robustness of RL algorithms.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
* The motivation of this paper is very clear and the idea is easy to follow.

* The experiment setting is quite reasonable.


Weaknesses:
* The experiment results only focus on SVEA. Authors should add experiment results with other RL algorithms.

* The related works for RL algorithms with disentangled representations are not enough.




Limitations:
Yes

Rating:
5

Confidence:
3

";1
zCFfv49MjE;"REVIEW 
Summary:
Kernel methods are important in many ML applications, but they suffer from scalability issues. In the Euclidean setting, random features methods (Rahimi & Recht NeurIPS 2017) can be used to ""sketch"" the kernel matrices and speed up computations. In the graph setting, however, no such random feature seems to have been available until a 2023 preprint by Choromanski. 

The present paper present an improvement over Choromanski's method. To explain it, note that the basic idea in the 2023 preprint is to consider random features for the regularized graph Laplacian, and to obtain an approximation of an operator of the form $(I-U)^{-d}$ via importance-weighted random walks. These random walks have geometrically distributed lengths. 
 
The main theoretical contribution of the present paper can now be explained: a simple modification of Choromanski's method, where the the geometrics of pairs of random walks become correlated, has the same zero bias and a smaller variance than the above method. 
This requires an elementary, but long proof with a long case analysis.

Experimentally, the gains observed were:

* visible, but not too big for approximation of the kernel;
* impressive for graph diffusions (which is natural because this requires exponentiating the Laplacian);
* fairly significant for clustering and node attribute prediction methods.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper is clear and seemingly original. In general it is fairly well-written. Moreover, the ""anthitetic"" coupling is a simple little idea that could be applicable more broadly.



Weaknesses:
The present paper has been submitted before Choromanski's preprint appeared. As a result, applications of graph kernel random sketches are quite incipient still. It is hard to say whether the present paper will have a significant direct impact. _[Edit on 08/11: this was partly addressed in the rebuttal.]_

The proof of variance reduction is very long and difficult to check. (Though I believe it.)  (An ""actionable version"" of this point is that it would pay off to make this paper a bit less dependent on Chorumovski's preprint by eg. adding more experiments or theory.)  


_Less important comments_

The notation $t_{1,2}$ is a bit weird as it represents a pair of RVs. Moreover, if I understand correctly, these r.v.s are resampled at each step of the walk. 

In terms of exposition, it seems that all the authors need to do is to correlate the coin flips for the two walks at each step. Could you write their joint distribution purely in terms of the probabilities of 00, 01, 10 and 11?  

Limitations:
These are properly discussed. 

Rating:
7

Confidence:
3

REVIEW 
Summary:
The author(s) extend the work of Choromanski [2023] and introduce quasi monte carlo graph random features.
To estimate the 2-regularized Laplacian kernel, they now use random walk features for each node which have coupled probabilities of terminating after a certain number of steps.
This increases diversity in the features and interestingly leads to provable reduction of variance of the global estimates.
An empirical evaluation supports the theoretical findings by improved practical variance, as well as in a clustering and supervised node learning scenario.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:

The paper is well-written and mostly easy to follow. 
While proofs are moved to the appendix, a sketch of proof of the main result is included in the paper. I like that.

The introduced way of increasing diversity in random walks is (up to my knowledge) novel and very easy to implement. While it is evaluated in the context of estimating the 2-regularized laplacian of a graph, I agree with the authors that this method is of independent interest. 
An interesting question is if this kind of sampling improves results of other random walk based approaches, as well, or if the parctical impact is restricted to just a few scenarios (GRF being one of them).
As a result, I see potential impact for further research beyond this immediate application.



Weaknesses:
The introduction is structurally very similar to that of Choromanski [2023]. I'm assuming now that I know one of the authors of this submission. Otherwise, I would recommend the authors to revise the introduction to decrease similarity. 


Limitations:
The authors provide a dedicated section discussing potential (far away) societal impacts, as well as open questions with the current approach. 
The discussion is adequate.


Rating:
7

Confidence:
4

REVIEW 
Summary:
This work proposes an efficient random-walk sampling approach that accelerates the estimation of the feature mapping for 2-regularized Laplacian kernels. The random walk sampling is based on antithetic termination, which is a variance reduction technique and sounds novel  when being applied to the estimation of graph random features. The technique sounds solid. The authors also conducted experiments from different angles to verify the effectiveness of the proposed approach in practice. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Using random walk with antithetic termination to estimate graph random features is novel, to the best knowledge of me. 
2. Although I did not check the appendix, the technique and the benefits it may bring look reasonable to me.
3. The authors did a great job to explain the key insights behind. The paper is very well written. 
4. The experiments are extensive. The benefits of the proposed approach is justified from various angles. 

Weaknesses:
1. The biggest weakness of the work is the limited scope of this work. It is unclear why the 2-regularized Laplacian kernel is important and why we need to compute it. It seems that 1-regularized Lap kernel can be efficient computed by Pagerank algorithms, why we may need 2-regularized Lap kernel. 
2. It is unclear how much the estimation of the 2-regularized Laplacian kernel relies on the random walk with antithetic termination. Can this random walk approach be only applied to 2-regularized Laplacian kernels or a broader range of graph kernels? This question is kind of related to whether the paper has over-claimed the contributions. Neither the title nor the introduction reflect that the improvement is only for  2-regularized Laplacian kernels, and they claim a broader range of contributions. I expect the authors to make the actual contributions more clear in the revised version.  

Limitations:
N/A

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper introduces a method for graph random features (GRFs). Graph random features is the graph equivalent (i.e. kernels defined on graphs) of kernel matrix approximation using Random Features (Rahimi & Recht). Recently Chromanski et al. had proposed a method for GRF which uses a series of random walks. 

In the Euclidean setting, the quasi-Monte Carlo variants of the Random Features methods have been developed since they provide better convergence properties. So the goal of this paper is to devise an algorithm for the quasi-Monte Carlo GRFs setting. 

The idea is to correlates the length of the random walks by imposing 'antithetic termination' which is effectively a procedure to obtain a more diverse ensemble of graph random walks. 

The paper provides several theoretic results related to the error guarantees as well as a brief empirical study.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
+ Original idea with strong practical impact and wide ranging downstream applications as time-efficient approximation of the graph diffusion process is used in many scientific fields. 
+ The paper is of very high quality: Well written and clear.  
+ Good literature review and problem justification, as well as empirical analysis. 



Weaknesses:
- Some typos/grammatical mistakes (esp. definite/indefinite articles), please proof read before publishing
- Some questions below as well.

Limitations:
The authors address the limitations and potential downstream impact in a dedicated section at the end.

Rating:
9

Confidence:
4

";1
WVmus8NWE8;"REVIEW 
Summary:
This paper motivates the mixture weight estimation problem using Multi-source Multi-target Domain Adaptation problem. More specifically, this paper considers how to estimate the optimal mixture of sources, given a target domain; also, when there are multiple target domains, how to solve empirical risk minimization (ERM) for each target using a possibly unique mixture of data sources in a computationally efficient manner. This paper tackles both problems by constructing new efficient algorithms with a convergence guarantee.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
This paper provides a rigorous theoretical analysis of the optimization algorithm proposed in the paper and considers both offline and online settings for the problem.

Weaknesses:
This paper focuses more on the theoretical analysis of a specific optimization problem instead of addressing the Multi-source Multi-target Domain Adaptation (M2DA) problem.

1. The bound in Theorem 1 can be quite loose; therefore, minimizing the right-hand side does not necessarily result in good weights in the target domain. As the right-hand side of this bound is minimized using empirical data, will there be an extra overfitting issue? Is there a way to bound this gap?

2. If some training data from the target domain is available, i.e., $\hat{\mathcal{T}}$, why not use these target data in the training of $h$ so that you have N+1 dimensional domain weights?

3. The relaxation from (1) to (2) seems arbitrary, especially the removal of the concave function of the square root.

4. There are no experimental results to verify the effectiveness of the proposed algorithm, not even on synthetic data.

Limitations:
There are no experimental results to verify the effectiveness of the proposed algorithm, not even on synthetic data.

Rating:
4

Confidence:
3

REVIEW 
Summary:
Authors formulate the problem of optimizing mixture weights given the target domain as a compositional convex-concave minimax optimization problem. Then, they propose a stochastic descent ascent algorithm for solving the problem, which improves upon previous method of [31] by allowing stochastic updates. Then, authors address the second problem of: given a large number of target domain distributions, how to efficiently find model weights for all of target distributions? This is discussed in both offline and online setting. In the offline setting, a two-layer ReLU neural network is trained to output model weights; the model is essentially a ""hypernetwork"" that outputs model weights. In the online setting, nonparametric online regression method is adapted for the problem. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
Authors identify an important instance of a convex-nonconcave minimax problem. The learning of mixture weights has become both theoretically and empirically important topic of research. For example, methods like Group DRO (Sagawa et al https://arxiv.org/abs/1911.08731 ) and DoReMi (Xie et al, https://arxiv.org/abs/2305.10429 ) have been drawing attention. The reduction of this learning problem to an abstract convex-nonconcave optimization problem will facilitate the adoption of techniques from a broader optimization literature.

Authors also push the state of the art on convex-nonconcave minimax problem by developing a stochastic version of the algorithm, adopting recently developed techniques such as stochastic corrected gradients.

Weaknesses:
The first half of the paper (mixture weight optimization) and the second half (model weight prediction) are related, but the connection is not very strong. These two ideas could've made good two separate papers. While making these results a single paper made this paper very rich with technical content, but on the other hand, readers of the main body of the paper shall learn much less than what they would with two separate papers. The rationale of authors seems to be that mixture weights for the second problem could be found from the algorithm from first half, but in this case, we already know model weights; thus, the second problem is not very well motivated from the first problem.

There is no numerical experiments in this paper, and hence results in the paper are not numerically validated. Also, this makes the practical utility of proposed algorithms less clear. 

Limitations:
What are alternative formulations for mixture weight estimation (1), and how would convex-nonconcave formulation compare against them? Practically, wouldn't it be too pessimistic to consider the supremum over $\mathcal{H}$?

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper is about the multi-source multi-target domain adaptation problem. The authors formulate a minimax algorithm to find the mixture weights of source domains. Furthermore, the authors extend it to the scenario of multi-target domains and introduce the co-component ERM problem. For this problem, this paper proposes algorithms to efficiently solve co-component ERM problems, in offline and online fashions.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
This paper extensively and theoretically studied the domain adaptation problem from the minimax optimization perspective. 

The paper gives the convergence analysis of the proposed algorithms.

For the multi-target domain scenario, the authors provide solutions to both the offline and online settings. The solution is more efficient than training each target domain adaptation independently. 

Weaknesses:
This paper is fully theoretical. The research focus of this paper, i.e., domain adaptation, has many open benchmark datasets and thus it would be better to experimentally evaluate the proposed methods. 

In line 148, it is mentioned that a deterministic algorithm exists in the literature, while this paper is focused on the stochastic one. It would be good to elaborate more on the benefits of the stochastic one and meanwhile experimental compare them.

Limitations:
No potential negative societal impact is found in this work.

Rating:
5

Confidence:
1

REVIEW 
Summary:
Authors propose a new way to compute mixture coefficients for combining multiple empirical risk minimization objectives (w.r.t. different sources in domain adaptation) in a way that takes into account the relation to a new target domain. As an application, the authors consider the multi-source multi-target domain adaptation scenario and solve it by predicting (the weights of) new target classifiers from the mixture weights.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The considered phase transition is interesting.
- The weights could be provided by a domain expert who is, e.g., certain about a physical relation between the domains.
- Convergence of the algorithm as extension of [31] is interesting.


Weaknesses:
- No empirical intuition if the algorithm can be implemented with reasonable effort. I consider the result as of purely theoretical interest.
- The error in the computation of the weights $\alpha_i$ is not taken into account in the error rates results. This could dominate the convergence rates results.
- My impression is that, if we are able to solve Eq. (1) efficiently, then also a separate training for each target domain should give us a comperable accuracy. I don't see any argument, neither theoretical or practical, which guarantees that the proposed lagorithm improves the separate learning (which is possible with labels in the target domain).

Limitations:
- Influence of error of weight estimation should be discussed.

Rating:
6

Confidence:
3

REVIEW 
Summary:

Summary:
The paper addresses the problem of multi-source multi-target domain adaptation, where the goal is to learn a model from multiple sources in such a way that it performs well on a new target distribution. The context for this problem includes scenarios like learning from data collected from various sources (e.g., crowdsourcing) or in distributed systems with highly heterogeneous data. The two main unsolved problems in this context are: 1) how to estimate the optimal mixture of sources for a given target domain, and 2) how to efficiently solve empirical risk minimization for each target domain when there are numerous target domains, which can be computationally expensive. The paper proposes solutions to both of these problems using convex-nonconcave compositional minimax and overparameterized neural networks with provable guarantees. Additionally, an online algorithm for predicting parameters for new models given mixing coefficients is proposed.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Theoretical Contributions: The paper proposes novel approaches to tackle the problems of mixture weight estimation and empirical risk minimization, utilizing convex-nonconcave compositional minimax and overparameterized neural networks, respectively. These contributions are supported with provable guarantees, which add rigor to the proposed methods.

Efficiency and Scalability: The paper emphasizes the efficiency of the proposed algorithms, particularly for mixture weight estimation and empirical risk minimization for multiple target domains. The avoidance of individual ERM for each target domain in certain cases helps reduce computational overhead.



Weaknesses:


Complexity: The proposed methods, such as convex-nonconcave compositional minimax and overparameterized neural networks, might be complex and difficult to implement for practitioners who are not familiar with these advanced techniques.

Applicability to All Domains: The paper may not clearly address the limitations or specific domains where the proposed techniques might not be directly applicable or might require additional adjustments.

Empirical Evaluation: The paper lacks details about empirical evaluations, such as experiments on real datasets or comparisons with other state-of-the-art methods. This could raise concerns about the practical effectiveness of the proposed algorithms.

Limitations:
Limited Empirical Validation: The lack of empirical evaluation or real-world case studies might raise questions about the practical effectiveness and applicability of the proposed methods to real-world scenarios.

Rating:
4

Confidence:
3

";1
lSZXSDwvGv;"REVIEW 
Summary:
The paper studies the ability of LLMs to improve in a negotiation game. The find that only a subset of language models can self improve from AI feedback, a model's ability to learn from feedback depends on its role in the game, and stronger agents can go through more rounds of negotiation.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is generally well written; it was more or less easy to understand the entire paper.
- The experiment methodology is sensible. It was nice to narrow down the set of models by eliminating models based on their ability to respond to feedback or understand the problem. 
- The results are interesting and could be valuable to the community on understanding the role of LLMs as agents.

Weaknesses:
- It seems like incorporating feedback is done by providing the feedback as context for the LLM. The paper could be made stronger by utilizing the fine-tuning APIs for the given models. 
- Using GPT3.5 as the fixed agent is interesting. It's clear GPT-4 is the strongest agent in this scenario and it'd be interesting to see how well these results would hold against a stronger agent. In a similar vein, seeing how well these results would hold when actually negotiating with a human. 
- Could buyer's and seller's be prompted better? E.g. would it be possible to prompt claude instant v1.0 more effectively to respond to multi-turn negotiations or prompt Cohere command or AI21 j2 better to respond to bargaining and feedback respectively?

Limitations:
The authors partially address limitations of their work, and address societal implications of their work.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper investigates the intriguing possibility of autonomous improvement among multiple large language models through a negotiation game. By assigning various LLMs to distinct roles and allowing them to engage in iterative improvement, the paper aims to enhance their negotiation strategies without human intervention. The study uncovers some insights into negotiation problems, encompassing the assessment of model capabilities and their responses to AI feedback. However, after careful evaluation, I am inclined to reject this paper due to its limited contribution and insufficient experimental results, which fail to adequately demonstrate the effectiveness of its findings.

Soundness:
2

Presentation:
3

Contribution:
1

Strengths:
This paper studies an interesting problem: Improving large language models with each other and demanding only black-box access is a promising direction.
This paper is largely clear and concise. It is easy to follow the problem setting and the negotiation process.
This paper compares capabilities (especially the continue learning ability using in-context learning) of some advanced large language models in the proposed negotiation problem, which is interesting.

Weaknesses:
One of the main technical novelties is the AI feedback technique used in their method. However, this technique seems like a result of random attempts plus some intuition. More detailed though on how did the authors develop this technique or the comparison between other possible candidate techniques is needed. Moreover, this technique is similar to CoT. It would be better for the authors to discuss on the relationship between these two techniques. I am curious about the performance of guiding LLMs to think step by step without relying on additional critics in negotiation problems. More thorough explanations or experimental results of this aspect would provide deeper insights into the effectiveness of the proposed approach.
This paper aims to improve the ability of large language model by self-play with each other. However, to strengthen the paper's claims, it would be better to provide additional evidence regarding the transferability of the proposed framework to different types of games. Suggestions on the specific implementation process would also be valuable in applying this framework to different domains.
The stability of the environment is also a concern since the results appear to heavily rely on the reliability of the moderator. This dependence raises doubts regarding the individual contributions of various components in the system. Furthermore, the claim of proposing a technique for prompt optimization for generic classification tasks in Line 120 lacks sufficient details and evidence. Elaborating on this technique and demonstrating its effectiveness would enhance the paper's credibility and address this specific weakness.
It is not clear whether there exists an upper limit of improvement using ICL. It would be better for the authors to also discuss ICL with, e.g., fine-tuning or other trainable parts.

Limitations:
The authors discuss the limitations of the work in the last section. 

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper studies whether multiple large language models (LLMs) can improve each other in a negotiation game by playing, reflecting, and criticizing, with minimal human intervention. Two LLMs play the roles of a seller and a buyer, and a third LLM plays the role of a critic who provides feedback to one of the players to improve their negotiation strategy. The authors report several intriguing findings, such as the different abilities and behaviors of various LLMs in the game, the trade-off between deal price and success rate, and the evidence of improved language complexity and strategy from iterative AI feedback.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Interaction between LLMs is quite interesting and potentially important for future AI research.
2. The authors experimented with a variety of LLMs, including open-source (Cohere) and proprietary (GPT, Claude) ones.

Weaknesses:
Major Issue:
1. The negotiation setting is quite contrived and not well-grounded. Specifically, there is no context about the goods being discussed. Also, the feedback and overall conversation seems very generic (Figure 2). There is no value/intrinsic motivation for the buyer to get the goods, and there is only one choice to go for. Typically, negotiation in real-life doesn't happen this way.
2. It feels like the current LLM negotiation setting can be viewed as `predicting the most probable outcome/text' given the negotiation contexts.
3. AI feedback is not the same as human feedback, the behavior may be completely different in real-world situations since there are a lot of other concerns including 'value', 'time', 'personal preference' for negotiation. This makes me wonder what this simple negotiation has to offer in terms of understanding/future research.


Minor Issue:
1. The title is a bit misleading. It seems to me from the title that the paper is about an algorithm, but it is more about evaluations and understanding.


[After Rebuttal]

My concern still lies in the 'motivation' behind the setting for the experiment. In my understanding, negotiation requires a setting, e.g., both seller and buyer will have a stake on the product (buyer want to pay lower than the *utility* of the product and seller want to earn more than the *cost*). However, such basic setting is not present in the experiments. 

This makes me concerned if the observed effect is really negotiation or just the effect of instruct fine-tuning.

I decided not to change my score.

Limitations:
I was unable to find any information related to limitations.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper studies the strategic multi-agent problem setting of two LLM Players interacting in a negotiation (or bargaining) game and proposes to use feedback from an LLM Critic to improve each Player’s expected behavior and performance in the game.  Importantly, the paper aims to study how AI Feedback can enable Player improvement when playing competitive games under well-defined rules.  The proposed method is to use the player dialog history and critic feedback as “in-context demonstrations” for players to self-improve over the course of the game.  Experiments are conducted on a bargaining game instance negotiating the price of a balloon, investigating several LLMs as base models for the Players (and their Critics).

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
Identified strengths of paper:
 - I really like the premise of this work: that agents playing in a strategic game may each be able to benefit and improve their negotiation strategies, not only by observing their opponents, but additionally by heeding the advice of personalized critics whose aim is to help (cooperate with) them.  Like other AI feedback methods, this type of approach has the potential to scale better than using Human feedback for the same type of assistance, presuming feedback is helpful and thus desirable for this negotiation task.
 - The topic is timely and very relevant to the multi-agent and game theoretic communities, as it takes a long-existing and well-studied problem (multi-agent negotiation) and investigates how rational player strategies can be improved using AI feedback and LLMs.


Weaknesses:
Identified concerns and suggestions to improve the paper submission:
- It would be extremely helpful to see examples of what the *instructions* and *in-context learning* looks like for the Critic agents? Examples of their few-shot prompts (or instruction data for Finetuning) are important, as this is a key contribution of the work (adding AI Feedback to strategic reasoning/negotiation settings). Notably, where are the Buyer/Seller Critics getting the suggested negotiation strategies in Figure 1B  from (e.g. the flinch technique, the anchoring technique, etc)?  They seem to be good, general strategies to have in the player’s toolbox, but require the Critic to have knowledge of effective negotiations and how/when to employ them.  Thus, how are the Critics coming up with general negotiation strategies and reasoning about when to apply them?  This is important for opening the black box of the AI Feedback component of the proposed method.
 - While I like the overall premise, the novel technical contribution of this work seems tenuous.  With that, the problem setting is strongly inspired by AlphaGo Zero, but is there any traditional learning in this setting (i.e. any updates to the agent policies based upon the AI feedback given)?  That is still unclear to me.  Currently the paper seems to simply add a small amount of additional context in the prompt for the Player LLMs.  If there is no traditional learning, this setting is also critically different from AlphaGo Zero in that the proposed feedback signal (from Critic Agents) is **not** used to update the Player policies.  This should be explicitly clarified.  Also, if there are model updates, it would be useful to see the learning update rules.  If not, why not do any finetuning of the model?  At least as an experimental condition to compare against and understand if in-context learning is ""sufficient"".  Perhaps more motivation and context for this design decision would be helpful.
 - This work explores a potentially interesting research direction (negotiation games or strategic gameplay more generally + AI Feedback for improved player strategies) but it’s not clear to me how well motivated AI feedback is for this negotiation game or how interesting the problem is in its current instantiation.  In particular, it would be helpful if the paper provided some type of performance analysis to show that the negotiation game used (an instance of balloon purchasing) is interesting and challenging to solve *before* ever adding in LLMs or a Critic.  How would existing multi-agent negotiation/bargaining approaches solve this game?  What equilibria do the Players generally converge to without LLMs?  How and why does using LLMs change the *expected* solution?  Given two Player LLMs, why is a Critic *necessary* to have?  In other words, if the Critic is being prompted with examples of bargaining conversations, could the Buyer/Seller agents not simply see those same examples and then converge upon the same solution (they are currently finding) *without* a Critic?  Or is there some value that the Critic role *uniquely* adds (e.g. more efficient or robust convergence on an equilibrium in this negotiation game)?  Motivating the game selected as an interesting and challenging problem in its own right is critical.
 - Furthermore, I still question the generality and significance of the paper’s empirical findings.  **RE Significance:** How significant and meaningful are the differences in Seller performance in Table 1?  The numbers don’t seem that different to me, but perhaps with more context, the significance becomes more clear.  With that, I see a distributional shift in Figure 4, but how meaningful is this shift?  What are the implications of it, regarding how good or bad these solutions/equilibria are before and after receiving AI feedback?  Do the solutions to the negotiation game simply change a small amount but are comparable in terms of how preferable/desirable they are or do they get qualitatively better in some way?  **RE Generality:** The paper seems to use only one evaluation domain (balloon purchasing).  It is not clear to me which empirical findings/trends from the use of AI Feedback on this one, seemingly simple bargaining domain, are expected to generalize. In particular, regarding the effectiveness and impact of AI Feedback in other multi-agent negotiation domains and settings.  This limited evaluation seems more like an interesting case study than general findings that transfer.
 - In Subsection 4.3, the paper provides analysis of each of the LLM models (gpt-3.5, gpt-4, claude-v1.0, etc) versus a fixed gpt-3.5-turbo opponent.  However, a more thorough investigation examining the cross product of each LLM model as Buyer against every other LLM model as Seller (a *complete* Payoff matrix with all pairwise model comparison) could potentially provide more general insights. Why was this not done? Can it still be done to show a resulting Payoff Matrix?  Not doing a pairwise comparison of *all* pairs of models might unnecessarily constrain the results and thus the insights that can be extracted.

Limitations:
Please see Weaknesses section.

Rating:
4

Confidence:
4

";0
DvRTU1whxF;"REVIEW 
Summary:
The authors make two claimed contributions:
1. They propose an architecture for an encoding model. This architecture consists of three key components.
* A frozen DINO trained ViT-B backbone + learnable convolutional layers on top of ViT feature maps
* A differentiable spatial sampling layer (implemented via pytorch grid_sample, similar to [1]), where the 2D coordinates are predicted from 3D voxel coordinates
* A differentiable softmax based layer selector
2. They propose an ""All-for-One"" training recipe. Which incorporates the following:
* ""dark knowledge distillation"" (typically referred to as knowledge distillation or network distillation in most other machine learning works), where they use ROI specific networks to train larger networks
* They propose a new parcellation across brains which they call ""veROIs"", which is extracted via k-means clustering of voxel weights.

To validate this method, the authors visualize the learned spatial sampling grids and how the preferred layer varies across voxels. The authors further perform an ablation study of the encoder, and perform image retrieval using their network.

The authors provide an illustration of how the sampling grid evolves during training in the supplementary.

[1] Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. ""Spatial transformer networks."" Advances in neural information processing systems 28 (2015).

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is novel in the combination of techniques, which uses online-learned sampling grids with soft backbone layer assignment to predict the brain activations, this design is reasonably biologically motivated.

The authors proposal of clustering based ROI assignment is also novel in the context of decoding (image retrieval).

The paper provides definitions and dimensions for most variables where appropriate, and the figures are illustrative.

My view is that the proposed decoder architecture is moderately novel in the context of encoding models. There is insufficient information (lack of detail) to judge the all-for-one training. 

Weaknesses:
The paper is interesting, however my key concern lies in the lack of details for the all-for-one training scheme, and the lack of evaluation in comparing against simple linear-regression + backbones (ResNet, CLIP ViT [1])
* On the clarity of the paper
  * The high level clarity of the paper regarding the all-for-one training is poor and could use significant improvement. 
  * It is not clear how exactly the veROIs are used. Is the encoding model ultimately at the voxel-level? If so, are the veROIs used for the network distillation and image retrieval steps? Do you share the backbone and just use independent linear weights for the voxels?
  * It is not clear which subjects you use for Stage 1/2/3. Do you train with one subject's ROI? Or do you train on all subjects' single ROI (using stage 1 as an example). If so, do you take veROI-1 for example, then train a model on all subjects from all modalities for veROI-1?
  * It is not clear how you get the all-ROI model to derive the veROIs. Is this model trained on all subjects and all modalities? Do you train just the last linear weights? Or you use the DINO features? Do you train the conv weights?
  * It is not clear how you train across subjects and datasets where the number of voxels are not the same. This is a central claim in your paper.

* On motivation
   * Currently the justification for repeated distillation is weak. Your experiments show that distillation does help from a performance standpoint, however your motivation differs from the typical use of distillation (which is to accelerate inference using a student model). Could you better motivate this? 

* On prior work
  * For the topic of brain encoding models that utilize differentiable spatial sampling, I recommend citing [2,3] which are similarly brain motivated, as well as [4] which is one of the more significant papers that uses differentiable sampling.

* On the soundness of the baseline
  * For the ""FrozenRM"" encoder, you mention that every voxel is mapped to the center. Since you use DINO based on ViT, there is an extra classifier token (usually the first token). Typically when using ViT based architectures for spatially invariant tasks, this is the embedding you use. I recommend modifying the FrozenRM baseline, or adding an additional baseline when you use this token
  * The same criticism applies to the GlobalPool token, I recommend adding/replacing a baseline where the global pooled representation is replaced with the classifier token

* On the lack of baselines
  * Currently the authors perform ablation studies, but do not perform qualitative or quantitive comparisons against other works [1]. The paper would be strengthened by adding comparisons to linear-regression based single subject voxel-wise encoding models based on different architectures (global pooled ResNet features , VGG, Gabor features, GIST features) trained on ImageNet, or different objectives (CLIP/OpenCLIP/EVA-CLIP, DINO, any of the SSL/Masking work) adapted to a single subject. I don't expect their multi-subject network to necessarily perform better (nor would it be a negative if they perform worse), but some evaluations are still necessary. 

* Minor
  * Figure 4 was quite confusing to me. In the retinagrid case, the colors indicate spatial extent. However in the retinamap, the colors indicate layer selection. I would ask that you provide additional clarity here.
  * The notation in Table 3 is quite confusing, you do not specify what veROIsX is. It is implied that it corresponds to what stage of network distillation you use. Please clarify this. 
  * Line 199 ""regulirazation"", minor misspelling. 

On balance, the paper is interesting from an architectural standpoint. I would be happy to take another look at the paper if the authors can clarify their training scheme and add additional baselines. 

[1]  Conwell, Colin, et al. ""What can 5.17 billion regression fits tell us about artificial models of the human visual system?."" SVRHM 2021 Workshop@ NeurIPS. 2021.

[2] Mahner, Florian, et al. ""Learning Cortical Magnification with Brain-Optimized Convolutional Neural Networks."" Conference on Cognitive Computational Neuroscience. 2022.

[3] Jun, Na Young, Greg Field, and John Pearson. ""Efficient coding, channel capacity, and the emergence of retinal mosaics."" Advances in neural information processing systems 35 (2022): 32311-32324.

[4] Jaderberg, Max, Karen Simonyan, and Andrew Zisserman. ""Spatial transformer networks."" Advances in neural information processing systems 28 (2015).

Limitations:
The authors adequately address the limitations of their model.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper develops an all-for-one training model to address the challenge of one big-model problem by converting it into multiple small models, in which the small models aggregate the knowledge while preserving the distinction between the different functional regions. With the proposed method, biological knowledge of the brain, particularly retinotopy, is used to introduce inductive bias into a 3D brain-to-image mapping that ensures a) each neuron knows which regions and semantic levels to gather information, and b) no neurons are left out. Overall, it is an interesting paper, however, there are several concerns about the machine learning novelty, validating the empirical studies, and the clear presentation of the proposed method.


Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
Please refer to the question section


Weaknesses:
Please refer to the question section


Limitations:
Please refer to the question section


Rating:
4

Confidence:
4

REVIEW 
Summary:
The manuscript addresses the challenge of generating brain encoding models (specifically for visual stimuli) - which seeks to predict brain responses at the voxel level to visual stimuli. A challenge facing brain encoding is heterogeneity in data modality, individual variability, and functional differences across brain region. Existing models often address the problem of heterogeneity by fitting separate models for different brain regions. However, the authors seek to to fit a single model that encompasses the entire visual brain by leveraging a dark knowledge distillation method in which each ROI distills the dark knowledge present in the other ROIs.  The authors evaluate their method on a variety of functional imaging datasets spanning fMRI, MEG, and EEG.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Originality: The authors developed a novel training pipeline utilizing dark knowledge distillation in order to allow ROIs to collaborate during training. The authors also develop a new method for incorporating retinotopy, a biologically realistic feature, into their encoding model.

Quality: The model seems to work across different data modalities (e.g. EEG, MEG, and fMRI), making it broadly applicable.

Clarity: The manuscript provides several ablation studies that investigate the limitations of the approach and which features of the architecture are important for improvement.

Significance: The study introduces a new approach for creating visual brain encoding models from functional imaging data. Their approach could be employed by other groups studying non-visual stimuli as well.

Weaknesses:
Unless I am mistaken, the authors do not evaluate their model against the state of the art in  computational speed, making it hard to evaluate whether their model represents a significant improvement, as the improvement in correlation appears to be modest. Furthermore, the message of the paper is a little unclear - what is the main breakthrough the authors are trying to present: the all-for-one training, retinotopy, or ability to work with multiple data modalities. Evaluation against state of the art should occur for each of these topics. Finally, the model is trained on data across fMRI, MEG, and EEG modalities, but the held out data consists only of fMRI data, making evaluation of model generalizability difficult.

Limitations:
Limitations are addressed except for points raised earlier about comparison to state of the art.

Rating:
4

Confidence:
2

REVIEW 
Summary:
The authors tackle the brain encoding task, which predicts brain voxel-level responses to image stimuli. The authors aim to train a comprehensive brain encoding model using the vast amount of public data from diverse imaging modalities and numerous participants. The proposed method, the All-for-One training recipe, divides the one-big-model to multiple small models and aggregates the knowledge together in inference time. An intriguing technique the authors propose is to use retinotopy to introduce inductive bias to learn the mapping.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
1. The figure quality (especially aesthetics) is not often seen in NeurIPS submissions. Take a look at Figure 2, 4, 5 and 6. Those figures are on par with Nature family submissions. 
2. Decent ablation studies.

Weaknesses:
1. The design choice is not the most straightforward to process. I have some trouble understanding the logic behind the three-staged design of the proposed All-for-One recipe — specifically, why is it necessary to have these 3 stages?
2. RetinaGrid and RetinaMap seems a bit far-fetched. By far, my impression is that the authors are simply trying to draw an analogy from image formation process in the retina and provide a fancy visualization. The authors are welcome to defend with explanations.
3. Lack of other alternative baselines.

Limitations:
Nothing that I am aware of.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper focuses on the task of brain encoding model, which aims to predict brain voxel-wise responses to stimulus images. First, the paper proposes the All-for-One (AFO) training recipe, which enhances interactions among multiple ROI models to handle the large diversity within the data. Second, the paper introduces the RetinaMapper to learn a 3D brain-to-image mapping and the LayerSelector to selectively merge features from multiple layers. Finally, the paper trains the model on a very large scale of data and demonstrates the effectiveness of the model through extensive qualitative and quantitative analysis.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
1. The overall motivation is clear, and the techniques used are straightforward and reasonable. For example, enforcing cross-model interaction/distillation is a reasonable way to enhance each model, and selectively merging information from multiple layers with different receptive field sizes is sensible.

2. The experiments are extensive, and adequate qualitative and quantitative analysis is provided. For example, Table 2 demonstrates the effectiveness of TopyNeck, and both Figure 4 and Figure 5 show the effectiveness of the LayerSelector.

3. The work pre-trains the model in large-scale data, which is impressive.

Weaknesses:
1. need to provide more details and insights into specific techniques
2. missing more explanations/references
3. some writing issues

Limitations:
The authors have addressed the limitations

Rating:
6

Confidence:
2

";0
mookk2nLO9;"REVIEW 
Summary:
This paper studies the problem of efficiently sampling from a SDE given a drift function and diffusion tensor. It proposes a solution based on computing a PSD model that satisfies the Fokker-Planck solution associated with the SDE, and then sampling from the resulting PSD model.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
The paper seems to have a lot of technical content, and it is clear that the authors have put in a lot of effort.

Weaknesses:
This paper is extremely difficult to read. The theorem statements (and the paper overall) need to be drastically simplified to be understandable. The problem doesn't seem to be well motivated, and it's not clear from reading the paper how the solution compares to others in the literature. For instance, the [LCL+21] paper is mentioned in related work, but it's not at all clear how the solution proposed of learning a PSD model compares to their work. There are no experiments to support this idea of fitting a PSD model. Overall, I think this paper needs to be rewritten to be more easily understandable.

Limitations:
Yes

Rating:
3

Confidence:
2

REVIEW 
Summary:
This article investigates the approximation of the Fokker-Planck equation and the fractional Fokker-Planck equation using positive semi-definite (PSD) models. The authors demonstrate that for a given accuracy $\epsilon$, there exists a PSD model with a dimension not exceeding $\epsilon^{-(d+1)/(\beta - 2s)}(\log 1/\epsilon)^{d+1}$ that approximates the corresponding partial differential equation (PDE). By utilizing this result in conjunction with the PSD model-based sampling method proposed in [1], the authors are able to provide samples of the stochastic differential equation (SDE) corresponding to the Fokker-Planck equation or fractional Fokker-Planck equation at any given time $t$.

[1]Marteau-Ferey U, Bach F, Rudi A. Sampling from arbitrary functions via psd models[C]//International Conference on Artificial Intelligence and Statistics. PMLR, 2022: 2823-2861.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This article demonstrates clear and logical writing, making it easy to understand the problem formulation considered by the authors as well as the fundamental concepts and properties of the PSD model adopted.
2. As a newly proposed model, this paper presents a novel application scenario for the PSD model, which has not been previously explored in related works. This contributes to its significant novelty.
3. From the perspective of sampling based on SDEs, the authors provide a new design approach for sampling by combining existing works, offering a fresh perspective on sampling techniques.

Weaknesses:
1. Although the author mentions it in the paper, the influence of initial conditions on the solution of the PDE can sometimes be significant. It would enhance the completeness of the theoretical analysis in this article if the author could provide an analysis of whether the PSD model can satisfy the initial conditions or, in cases where exact satisfaction is not possible, characterize the approximation of the initial conditions' impact on the final solution's approximation.
2. Personally, I have limited knowledge of non-log-concave sampling and numerical modeling of PDEs. However, it would be beneficial if the author could compare the effectiveness of the PSD model-based sampling method with other traditional methods (such as Langevin Monte Carlo) in terms of sampling performance. Additionally, comparing the approximation methods of the PSD model for PDE solutions with other existing approaches, either from the perspective of complexity analysis or purely experimental results, would provide valuable insights.

Limitations:
Despite the theoretical guarantees provided by the author, I believe the biggest limitation of this article lies in the lack of clear demonstration of the advantages of the PSD model. It would be beneficial if the author could explain how the PSD model stands out compared to other similar models in terms of approximating FPE and subsequent sampling tasks. This would significantly enhance the persuasiveness of the article.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The authors introduced a new method of sampling solutions of the Fokker--Planck equation (FPE) using a positive semidefinite (PSD) model. While the algorithm used for the PSD model is not new, the authors created a framework to sample from the FPE, which is quite original based on my knowledge. 

While I believe the results are very neat, I would like to clarify several questions with the authors before providing a confident score. For now I will recommend borderline accept, and will raise the score once my questions are adequately addressed. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The approach to drawing samples for the Fokker--Planck is very unique and original. I believe alternative methods that hold promise should be welcomed and more carefully studied, regardless of how practical it is at the moment. 

Weaknesses:
There are a few clarifications I would like to have regarding the implementation of the algorithm. 

Limitations:
N/A 

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper considers numerical methods for the Fokker-Plank equation and its ""fractional Laplacian"" version. These partial differential equations describe the evolution of densities of certain random processes: in the case of standard FP, the process is a regular diffusion (an SDE driven by Brownian motion), whereas the fractional version corresponds to a process driven by $\alpha$-stable noise. Accordingly, the authors focus on initial conditions given by probability densities. 

The idea behind the paper is to adapt the fairly recent methodology of positive semidefinite models (PSD) to this PDE problem. PSD models estimate nonnegative functions via feature maps $\phi:X\to H$, where $X$ is the domain of the function and $H$ is a Hilbert space. Given $\phi$, the nonnegative function of interest is estimated as $\langle \phi, M\phi\rangle$, where the PSD linear operator $M$ is chosen from the data. The whole method is based on the same idea as the well-known kernel trick: there is no need to explicitly deal with the Hilbert space $H$; only the kernel matrix over data points will matter.  

In a nutshell, the paper gives a method for approximately solving the two aforementioned PDEs via PSD models. The salient features of this approach are as follows. 

1) The PSD estimator can be obtained via a semidefinite program (as shown in previous work). 

2) The method will output probability densities, and one can sample from them via methods from previous work. 

3) The complexity of the problem grows like $\approx \epsilon^{-d/\beta}$, where $\epsilon$ is the target accuracy, $d$ is the dimension and $\beta$ measures the degree of smoothness of the solution. The exponent is not quite right, but the point is that sufficient smoothness will avoid the curse of dimensionality.

One may note that this method has rigorous error guarantees. This sets it apart from other PDE solvers based on machine learning ideas, such as Physics Inspired Neural Nets (PINNs). 
 

Soundness:
4

Presentation:
2

Contribution:
4

Strengths:
The paper gives strong convergence guarantees for a new PDE solving method. Although several ingredients come from previous work, they are combined with new ideas into an impressive package. As mentioned above, this is a ML-based PDE solver with rigorous guarantees. Importantly, these guarantees do not require coercivity or hypercontractivity properties that are usually required from Langevin samples for solutions.  

Weaknesses:
* No experiments are performed to compare the method with other PDE solvers (though this is understandable in a heavily mathematical paper). 
* The writing is a bit sloppy in places: I found quite a few typos and have several minor comments on writing (I could probably find some more typos if I kept looking). See the list below. 

NB: were it not for the sloppiness, I would have given this paper a higher score.

---

_List of typos and comments on writing_

(Line numbers refer to the full paper in the SM.)

Lines 146/7: ""It was"" and ""It has"" should be ""They were"" and ""They have"".

Line 180: why define $j=\sqrt{-1}$ if you could have just written $\sqrt{-1}$ in the exponent? (You never use this notation again, do you?)

Line 185: Shouldn't the condition for infinite $p$-th moment be $p\geq \alpha$?

Line 201 and elsewhere: I find it a bit odd that the transpose notation is used for vectors in Hilbert space. If you are going to use it, at least explain here what is going on.

 
Line 251: why is this sensence needed here?

Line 205: $\eta$ was treated as a vector before, and will be treated as a vector later in the text. Here, however, it seems to be a scalar. 

Line 223: ""The cost of the algorithm is..."" -- per sample point, right?

Line 254: ""Hence, $p^\star(x,t)$""

Line 255 should end with "":"".
 
Line 270: the definition of $\psi$ should be in a numbered display for easier refererence.

Display (8): if $p^*$ is the true solution, why do we have it in here? (Its contribtion vanishes.) Indeed, the definition of the loss later does not involve $p^*$; see eg. (9). (On the other hand, $p^\star$ shows up again in the display right after line 989.)

Line 294 and elsewhere: when you are mentioned a numbered theorem or assumption, start the corresponding word with a capital letter. 

Line 295: $\mathcal{H}_X\otimes\mathcal{H}_T$

Line 318: ""by Gaussian kernel approximation"" -- should this be here?

Line 365: ""Similarly to the previous section, two steps are required to obtain""

Line 327 and elsewhere: I think $\mathbb{E}_t$ was not defined anywhere.

Line 352: ""If the kernel satisfies the Bochner...""

Line 355 and elsewhere: this is a matter of taste, but couldn't you replace ""utilize"" with ""use"". 

Line 376: ""The major difficulty""

Line 391 (taste): ""does require"" could be ""require"".

Line 395: I think ""approximated"" should be ""approximating"".

Proposition 4 is also in Evans's book, which was cited previously (I am saying this from memory, so please check. $W_2$ is replaced by $H$ in that book.)

Many spots of the SM: the inequality $\|f\star g\|_p\leq \|f\|_p\|g\|_1$ should be mentioned, as it is used in several steps. 

Lemma 2: it seems that $\tilde{p}$ is replaced by $f$ in the statement of the Lemma and at several steps of the proof. (Eg. the display right after 665.) This also seems to take place in the next Lemma.

Line 680: Since you are repeating a definition from the preceding proof, please say so. 

Line 682: ""We use the result of Lemma 1""

Line 746: Should ""The following result holds"" be here?

Display (34): this ""fill distance"" is the smallest value of $h$ such that $\tilde{X}$ is an $h$-net of $[-R,R]^d$.

Display below line 777: how should I interpret the norm in the RHS?

Lines 873/4: why are you spaeking of conditional probabilities here?

Line 891: what is $f(\delta)$?

Line 910: ""redefine"" should be ""define again"" (they are not the same).

Line 928: capitalize ""Holder"".

Limitations:
They have not addressed limitations explicitly, but I do not think that that would have been necessary. 

Rating:
7

Confidence:
4

";1
THDGuhN7LA;"REVIEW 
Summary:
This paper studies a new task named semi-supervised few-shot object detection, where both of base and novel classes are supposed to be scarce.
The author first finds the vanilla supervised FRCN trained on base classes has a low recall on novel classes, and trained with extra unlabeled novel data can effectively improve the novel recall.
Then the author follows the SSOD framework Soft Teacher to do semi-supervised base training, where only partial base data is available. However, the original Soft Teacher has a low recall of small and ambiguous objects. Thus the author proposes a new proposal learning method to improve it.
Finally, the pre-trained model is then semi-supervised fine-tuned on a balance set comprised of both base and novel samples.
Experiments show Softer Teacher achieves a good GFSOD performance.

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
* The idea is straightforward and has good soundness.
* The method has promising results in the GFSOD setting.

Weaknesses:
1. The proposed task of semi-supervised few-shot objects is similar to semi-supervised object detection. Since both base and novel classes are scarce, what's the meaning of splitting classes into base and novel? I can't see any practical significance in this task.

2. From my point of view, the setting proposed in the paper is closer to a semi-supervised than a few-shot object detection problem. Particularly, one of the key properties of few-shot learning is that the model does not know the novel classes in training, so it can adapt quickly to new classes with only a few examples in testing either using meta-learning (e.g., meta-RCNN) or small-#step finetuning (e.g., TFA). 
Another common sense in FSOD is that the novel classes are authentically rare, and we cannot find more images about that class, regardless of label or unlabeled.
Therefore, the proposed approach works best for semi-supervised object detection rather than few-shot object detection, and it is not fair to compare it with FSOD works.

3. The proposed method SoftER Teacher is an incremental improvement based on existing work SSOD Soft Teacher. The only improvement seems to be that the author appends a new loss to constrain outputs from the teacher and the student should be close to corresponding proposals, but the method is more likely to be only related to semi-supervised learning, it seems nothing about few-shot learning.

4. Section 3.1 is named ""What makes for Effective FSOD"", but 3.1 studies unlabeled data can improve the novel recall of the FSOD model. The title is not very accurate.

5. In line 252, the author argues, ""we are the first to incorporate external unlabeled data with few-shot fine-tuning"", I don't think it is a contribution or anything good.

6. There is no component analysis or ablation experiments to demonstrate the effectiveness of the proposed method. For example, the performance comparison of w/wo  the proposal learning loss.

7. The performance improvement is minor in the FSOD setting (not GFSOD). The novel performance is actually bad. The superior GFSOD performance may attribute to the strong baseline Softer Teacher on base performance.

8. The authors do not show any numbers related to the training resources (memory and time).

Limitations:
N/A

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper focuses on Semi-Supervised Few-Shot Object Detection, where both base classes and novel classes have few labeled training set, along with abundant unlabeled data. For model architecture, the softer teacher is proposed to train with unlabeled data and a teacher-student framework. Experiments demonstrate strong performance using only 10% base labels.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The idea of introducing unlabeled data for few-shot object detection is interesting and has great value for real-world application. The idea of  reducing the number of labeled data for base classes in few-shot object detection is also interesting.

Weaknesses:
My major concerns are as below:

1. What is the sources of unlabeled images? Do the unlabeled images have both base-class and novel-class instances? In this way, it is no surprise that adding abundant images could improve the proposal recall and detection results of few-shot novel classes.

2. The training framework in figure 2 and Section 3.3 are not exactly the same. In Figure 2, the unlabeled images are used for both base-class pre-training and few-shot fine-tuning. But in Section 3.3, it seems that the second stage of few-shot fine-tuning do not use additional unlabeled image. Clarifications are need. If the second stage also use unlabeled images, can we merge the second stage into the first stage because both base classes and novel classes are few-shot? We do not need to have two stages for training in that case, and the problem becomes semi-supervised object detection and each class has very few labeled images. Thus, what is the difference between traditional semi-supervised object detection?

3. This work has improved overall performance of base and novel class. Although the performance of novel class improves compared some baseline model (e.g., Faster RCNN), but has far worse performance compared to the SOTA [1,2]. Does this mean that the additional unlabeled images only work well with base classes, but not for novel class? Using unlabeled image is perhaps a right way for semi-supervised object detection. But does using unlabeled images is the right way for few-shot object detection?

[1] Qiao, Limeng, Yuxuan Zhao, Zhiyuan Li, Xi Qiu, Jianan Wu, and Chi Zhang. ""Defrcn: Decoupled faster r-cnn for few-shot object detection."" In Proceedings of the IEEE/CVF International Conference on Computer Vision, pp. 8681-8690. 2021.

[2] Kaul, Prannay, Weidi Xie, and Andrew Zisserman. ""Label, verify, correct: A simple few shot object detection method."" In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition, pp. 14237-14247. 2022.

4. The Table 1 and Table 2 are not complete. Table 1 lacks the comparison of Faster R-CNN (Our Impl.) and Soft Teacher (Our Impl.) as in Table 2. Table 2 lacks the comparison of the latest method for few-shot object detection (e.g. LVC, DeFRCN) as in Table 1.

5. The Figure 3 (b) and (c) are very confusing. I can only find one red box. Does it mean that vanilla FRCN-base only have one proposal? This is weird.

6. What is the difference between soft teacher and softer teacher? In L201-L207, the author mentions that soft teacher has an aggressive threshold of 0.9 which is not good. How did the authors address this problem? I do not find the answer in the main text. L208-L225 seems to be a simple extension of soft teacher without big changes.




==========================================================================================

After reading author's rebuttal and other reviews, some of the concerns about technical details are clear. But the major concern about the significance of doing few-shot base/novel partition is still there. I would suggest the authors make the problem setting simpler to get broader impact.

Limitations:
Please see the weaknesses above

Rating:
4

Confidence:
5

REVIEW 
Summary:
This article has done a meaningful work, which is a object detection method that combines few-shot with semi-supervised learning. The author introduces a SoftER Teacher for semi-supervised object detection in few-shot scenarios. SoftER Teacher enhances the quality of region proposals to substantially boost semi-supervised FSOD. Compared with LVC, DeFRCN and other methods, the performance has been improved.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- This task is very meaningful. As far as I know, the traditional few-shot object detection task is difficult to be directly applied to the industry, and the method combined with semi-supervised target detection will be a good solution. (Although the method in this paper is not the first to consider the combination of few-shot and semi-supervised.)
- It is good to see that the author provides the source code in the supplementary material, which provides a guarantee for the reproducibility of this article.
- The authors present rich experimental results in the article and supplementary material.

Weaknesses:
- To my acknowledgment, the current mainstream FSOD method is verified on MS COCO 2014, not MS COCO 2017. ""Consistent with the current literature on FSOD"" may be ambiguous.
- In Table 1, since the author did not report the results of LVC in 5-Shot, the experimental results of LVC are from the original article? But considering that our method has unlabeled data for additional testing, it seems unfair to compare the results with the LVC experimental setting.
- In addition to the comparison with the FSOD method, it would be better to add some comparisons with the SSOD method (under the Few-shot setting).
- In Table 1, I found that the results of the novel class seem to be relatively weak, what is the reason? Because the method in this paper utilizes additional data.

Limitations:
The authors describe the limitations of this paper in the supplementary material.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper approaches the task of Few-shot Object Detection (FSOD) from a semi-supervised perspective, where in addition to base classes data it uses unlabeled data during the base-pretraining phase, and then fine-tunes on the combination of base and available novel data using the best design choice of freezing appropriate layers (backbone, FPN and RPN) following the past works. The benefit of the approach comes with a higher bar on fully-supervised base class performance, which is attributed to training on additional unlabeled data. This higher performance then translates to a better overall (base + novel) classes performance in the fine-tuning phase, and establishes the effectiveness of semi-supervised learning on the FSOD task. The authors propose a SoftER Teacher approach which, in addition to the Soft Teacher loss, adds a consistency loss between teacher and student using at proposal-level. The authors show that this leads to better proposals (using recall) in low-data regimes.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- The paper is well-written with extensive experiments and ablations, and the semi-supervised exposition in the FSOD setup is much appreciated with a potential for realistic low-shot setups
- The paper is well-positioned with respect to prior related works 


Weaknesses:

- Table 2 on VOC07 and lower performance of Novel classes in low-shot setup
    - Retentive R-CNN compared to SoftER exhibits a trend that its 1-shot performance is much higher that the proposed SoftER, and this trend gets reversed with more shots (such as 10-shot)
    - The authors acknowledges the phenomenon in lines 295-296, and in line 297 mentions that Retentive R-CNN “generally falls behind on novel class performance”. However, this isn’t true in low-data setup (1-shot).
    - This trend, however, doesn’t appear in the COCO dataset where novel class performance in 1-shot case is low for both Retentive R-CNN and the proposed SoftER approach
    - My question is:
        - Do the authors have any intuition for this behaviour?
        - The authors leverage COCO-20 and COCO unlabeled2017 as he unlabeled data source for VOC (Table 2) and COCO (Table 1) experiments. Do the authors think that the domain mismatch between VOC and COCO is reflected in low-shot novel class performance in the case of VOC (Table 2)?
        - In general, the proposed approach does better with relatively higher-shot regimes, which also appears in the claims made in the paper - such as Fig 1 (30-shot). But as a reader, there seems to be little explanation about low-shot regimes, which runs counter-intuitive since the approach uses additional unlabeled data compared to prior approaches, and is expected to perform well especially in low-data regimes

- Presence of novel classes in unlabeled data
    - Line 277-278 mentions the use of COCO-20 for VOC and COCO unlabeled2017 as the source of unlabeled data for VOC and COCO respectively
    - This makes the assumption that novel classes in the plots of VOC and COCO experiments are necessarily present in the unlabeled set
    - In general scenarios, such assumption may not hold true. Do the authors have some intuition of how the proposed approach would work if the percentage of novel classes in the chosen unlabeled set is low / absent?


### Minor concerns
- Figure 3a for more percentage of base labels
    - Not a head-to-head comparison between FRCN-Base and FRCN-Base + Unlabeled, since the latter assumes more data
    - Does the difference narrow with more percentage of base labels?

- Conflicting claims
    - Fig 1: exhibiting less than 7% in base degradation
    - line 54: exhibiting less than 9% in base forgetting


### Justification of the rating
My main concern is highlighted above. In general, explanations towards the above mentioned concern would help the readers


Limitations:
Yes

Rating:
6

Confidence:
4

";0
RI6HFZFu3B;"REVIEW 
Summary:
This manuscript understands the expressive power of GNNs from a new perspective of subgraph aggregation, and reveals the potential reason for the performance degradation of traditional deep GNNs due to the overlap of aggregated subgraphs. The authors propose a sampling-based generalized residual module SNR and theoretically proves that SNR enables GNNs to more flexibly utilize information from multiple k-hop subgraphs, thereby improving the expressive power of GNNs. Extensive experiments show the effectiveness of the proposed SNR module. 

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
1）The idea of rethinking the expressive power of GNNs from the perspective of subgraph aggregation is novel and interesting.

2）The paper is well presented. The motivation, steps to construct a node-level, more flexible and general residual module to enhance the expressive power of GNNs while alleviating overfitting issue are clearly introduced.

3）The experimental results are convincible.


Weaknesses:
1）More related works on dealing with oversmoothing and overfitting issues in deep graph neural networks should be reviewed.

2）Further analysis of the experimental results is needed. For example, in Table 4, the possible reason why the proposed SNR module is weaker than InitialRes on Citeseer dataset should be discussed. 

3）There are some typos, e.g., row 221: “four data sets” should be “six datasets”. 


Limitations:
Yes.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper propose a sampling-based module to enhance the expressive power of Graph Neural Networks (GNNs), which traditionally assume that information from the subgraph of the same hop is equally important for all nodes in the graph. They argue that this rigid assumption restricts the models' ability to capture complex relationships. With their proposed module, different nodes can assign varying levels of importance to their neighbors at different hops during information aggregation, which is shown to improve the performance of the GNN variants.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1.	The proposed idea is interesting.
2.	The paper is well written and clear.
3.	The method is described in sufficient detail and easy to follow.


Weaknesses:
1.	Some reported baseline methods’ performances are questionable. For example, GCNII [1] shows alleviation of over-smoothing on the benchmark datasets with even 32 and 64 layers and achieved much higher accuracy than reported in Table 3. 
2.	The experiments are mainly performed on small-scale graph datasets. It’ll be interesting to test the method on larger graph datasets (e.g., the OGB datasets).
3.	Some detailed analysis of the learned mean and variance of the normal distribution would better support the proposed idea. It would be interesting to see how the distribution for nodes of different degrees changes with the increasing number of layers.
4.	To evaluate the method’s robustness to oversmoothing, it will be better to see a plot about how the performance changes as the number of layers increases than presenting the performance in a table. What’s more, the proposed method’s performance significantly dropped as the layer number increased. It suggests it still suffers from oversmoothing and/or overfitting.
5.	It’ll be interesting to see the over-smoothing analysis in Figure 2 to be done for different GCN variants with and without the proposed module.

References:
[1] Chen, Ming, et al. ""Simple and deep graph convolutional networks."" International conference on machine learning. PMLR, 2020.


Limitations:
yes

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper studies the problem of over-smooth with GNNs. It is argued that the over-smooth problem is caused by the increased overlap of the sub-graph when the respective field of GNNs becomes larger and larger.  In order to alleviate this problem, this paper proposed a method that random weighting the nodes within each layer by node-wise and layer-wise learnable parameters. The proposed method is limited evaluated on several datasets.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- Introducing randomness by using re-parameterization trick is interesting and makes sense to alleviate overfitting.

- Formulating different types of residual GNNs from a unified view is good.

Weaknesses:
- Some statements from this manuscript do not stand well. For example, it is argued from the 98-th line that as the k increases, the overlap between k-hop subgraph rises, making the aggregation from the k-hop subgraph from different nodes indistinguishable. It is not evident and the overlap is not sufficient for the over-smooth issues. Think about the transformer architecture where the self-attention within each layer has access to all other nodes/tokens while performing well on pixel-level tasks with several layers. 

- Additionally, the experiment that nodes with higher degrees tend to have more similar representations seem can not support the claim of sub-graph theory, as the overlap of the sub-graph does not influence by its degree.

- As for the over-smoothing problem, there are many other methods like drop-edge, which also introduce randomness when training the GNNs. It is highly recommended to include comparisons in the main results with methods along this line to make the contribution clearer (rather than just some comparisons under the setup of missing vectors). 

- It is unclear how the proposed method can help under the setup of missing vectors rather than other methods.

- As the sampling parameters are node-wise, I'm wondering how the proposed method can extend to inductive learning.

Some minor issues:

- Missing punctuation at the end of all equations.

- The quality of the figures can be improved a lot. It is highly recommended to use vector graphics for all the visualization.

Limitations:
There is no limitation discussion. The authors are encouraged to include as least the discussions about the limitation for inductive learning.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper focuses on the alleviation of overfitting and over-smoothing of deeper GNNs. It is an interesting topic and the solution seems promising with a sample-based node level residual block. Extensive experiments on public datasets verifies the applicability.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper analyses in a new perspective on the performance drop of GNN with the increase of layers, with motivating examples. It proposes a new residual block with the parameters randomly sampled from a natural distribution. The block alleviates the performance drop and seems applicable in general to all GNN models.

Weaknesses:
1. The presentation is in great need to improve. There are some long sentences that hinder the understanding of the authors's ideas, such as L54. A research article should describe facts with a 3rd party stand, but not subjective notations.
2. The paper should be self-contained even without the appendix, but it does not provide enough explanations such as on L83.
3. The figure is not reflecting exactly what is written such as L90 for figure 1, nothing shows the overlaps of aggregations via the lines and nodes.
4. Abbreviations should come with the full spellings on the first occurrence, even if it may be obvious in a specific research domain, such as GCN, but not GCNII on L194.
5. The citations should go to clear items such as a formula or a reference, but not a long section as L202 to Sec. 4.
6. It seems to be unnecessary to have Sec. 3.1 if there is only 1 sub-section.
7. Theorem 1 does not mean anything to me as it gets only formulas, but no conditions neither a conclusion.
8. It should be consistent to have the name of the proposed method or model.

Limitations:
N/A

Rating:
7

Confidence:
4

";0
9buR1UFCDh;"REVIEW 
Summary:
This work proposes an extension to DQN aimed at improving projection steps in Q value updates. There are two main contributions of the paper:

- The paper intuitively explains the Q-value learning characteristics of DQN variants caused by a mismatch between the optimal Bellman operator and the set of representable Q functions.
- The authors propose the iterated DQN (iDQN) method, which keeps track of a collection of K online Q-functions. When updating these Q networks, the previous Q function is used as the target network.

Experiment results show that iDQN outperforms a collection of DQN variants on the standard Atari benchmark. Further ablation studies explore the effect of K and sampling strategies for iDQN and conclude that bigger K and uniform sampling of Q networks are in general preferable.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The figures explaining the projection characteristics of existing DQN variants are very intuitive and provide excellent motivation for the proposed method.
- The iDQN method, the newly introduced hyperparameters, and the experiment settings are communicated clearly and transparently.
- The results on Atari are convincing.
- The ablation studies on the choice of K and sampling method are insightful.

Weaknesses:
- The figures used to explain the projection behaviors of DQN variants are created for illustration, but not from actual experiments.
- As discussed by the authors, iDQN doesn't outperform more recent DQN variants which employ other tricks to improve performance.

Limitations:
The authors discussed how iDQN is not able to outperform more recent DQN variants using other tricks. It would also be nice if the authors can discuss the training stability of iDQN.

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this paper, a new variant of DQN algorithm, iDQN, is proposed by replacing the classical Bellman iteration with several consecutive Bellman iterations and using multiple Q networks.
Intuitively, this new Bellman operator propogates reward sigals more efficiently thus speeds up learning, with the cost of more computation and memory.
As the number of consecutive Bellman iterations increases, it is shown that the learning performance of iDQN in Atari games is also increased.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
As far as I know, the proposed method is novel. The new algorithm, together with several baselines, are tested in 54 Atari games.
Many illustrative pictures are included to help understand the new Bellman operator.
The paper is generally well-written.

Weaknesses:
1. It will make this work much better if a theoretical analysis of the proposed Bellman operator is provided, such as convergence speed, the affect of the number of Q networks, etc.
2. The performance of iDQN is only slightly better than baselines, such as DQN(Adam). A summarized result (e.g. the first figure in [DQN Zoo](https://github.com/deepmind/dqn_zoo)) can make the comparison in Atari games much clearer.
3. Missing related works about ensemble methods + DQN, e.g. Averaged DQN and Maxmin DQN.
4. Misssing baselines: DQN + n-step return. Both iDQN and this baseline try to accelerate the propogations of reward signals. Furthermore, although it is mentioned that ""We do not consider other variants of DQN to be relevant baselines to compare with."", more explanations are necessary.
5. It is claimed that ""Our approach introduces two new hyperparameters, rolling step frequency and target parameters update frequency, that need to be tuned. However, we provide a thorough understanding of their effects to mitigate this drawback. "". However, I don't find the understanding thorough enough.

Limitations:
The limitations of iDQN are that it takes more memory and computation than DQN. Jax is used to parallelize the computation, making the training time increase acceptable.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper focuses on learning the projection of the empirical Bellman operator's iteration on the space of function approximator (neural model). This being done through increasing the number of gradient steps using multiple heads with a certain form of update. While retaining the same total number of gradient steps and samples compared to common approaches, the proposed method seems to provide better results (at the cost of retaining multiple heads and more computation).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The idea is interesting, novel and practical. The paper also **experimentally** shows noticeable improvement over various baselines.

Weaknesses:
- The presented method is quite simple and could have been presented much more efficiently with simple math and direct explanation rather than lengthy discussions over multiple figures. 

- The choice of $K$ seems to have a significant impact on the behaviour, which also varies depending on the domain. Suggestion: some formal analysis (e.g., the algorithm's variance) could be useful to provide better insight about what to expect from larger $K$ in terms of other characteristics such as the transition kernel.

Limitations:
While the authors mentioned at the end of Introduction that ""We conclude the paper by discussing the limits of iDQN ..."", they apparently forgot to do so! No discussion of limitations is provided.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper presents Iterated Deep Q-Network (iDQN), a new DQN-based algorithm that incorporates multiple Bellman iterations into the training loss. The paper highlights the limitations of traditional RL methods that only consider a single Bellman iteration and proposes iDQN as a solution to improve learning. The algorithm leverages the online network of DQN to build targets for successive online networks, taking into account future Bellman iterations. The paper evaluates iDQN against relevant baselines on 54 Atari 2600 games and demonstrates its benefits in terms of approximation error and performance.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The proposed iDQN algorithm introduces a novel approach to incorporate multiple Bellman iterations into the training loss, addressing the limitations of traditional RL methods.
2. The paper provides a well-structured review of relevant literature, discussing the behavior of various Q-learning algorithms in the space of Q-functions. This analysis helps in understanding the motivation behind iDQN and its relationship with other methods.
3) The empirical evaluation on selected Atari games demonstrates the superiority of iDQN over its closest baselines, DQN and Random Ensemble Mixture. This provides empirical evidence of the effectiveness of the proposed approach.

Weaknesses:
1. It would be helpful if the paper included more comparisons with widely-known baselines in the field. While the paper compares iDQN to DQN and Random Ensemble Mixture, it would be valuable to see how iDQN performs against other popular RL algorithms like R2D2.
2. Some parts of the paper could be further clarified to improve the reader's understanding. For example, the explanation of the loss function and the graphical representations of DQN variants could be made more concise and intuitive.

Limitations:
It would be helpful if the paper included more comparisons with widely-known baselines in the field. This paper has no negative social impacts.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper considers the problem of how to get accurate approximations of optimal Q-functions. The paper introduces a new algorithm called iterated DQN (iDQN). iDQN incorporates multiple consecutive Bellman iterations into the training process, which aims to allow for better approximation of optimal action-value functions. It uses the online network of DQN to build a target for a second online network, and so on, for considering future Bellman iterations. The authors conducted several experiments based on Atari games by comaping iDQN with baseline methods, including DQN and Random Ensemble Mixture. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Significance and Originality: The topic that the paper studies - how to learn the Bellman iterations efficiently, is an important topic in reinforcement learning. The authors propose a simple yet effective method, called iterated DQN, to improve the learning efficiency. Specifically, iterated DQN uses a second online Q-network for learning the second Bellman iteration simultaneously, where the target for the second online Q-network is created from a second target network according to the first online network. In this way, the loss can include K-1 more terms compared to the original loss used in DQN. The way for using such kind of ensemble seems novel, which allows for improved efficiency.

- Clarity: The paper is well-written and easy to follow, with very clear illustrations for the update for DQN, REM, and iterated DQN as in Figures 1-4.

Weaknesses:
- Quality: The paper presents a simple yet effective idea, but it could be further strengthened particularly in theoretical and empirical analysis. First, the authors could provide a theoretical guarantee for iterated DQN by examining its convergence speed, in addition to the intuitive explanation given in Section 4.1. This would lend credibility to their claims. Second, the empirical validation raises concerns, as iterated DQN's performance is only marginally better than that of previous baseline methods such as DQN (Adam), C51, and REM. This modest improvement does not strongly support the paper's assertions. Lastly, it would be beneficial for the authors to include a memory comparison, as employing more Q-networks may lead to increased memory costs, which is an important consideration for practical applications.

Limitations:
The authors have discussed some of the limitations by considering other value-based methods.

Rating:
4

Confidence:
3

";0
rzu41O7us0;"REVIEW 
Summary:
The paper presents a new approach to identify task-specific building blocks of neuronal activity from fMRI data by using supervised matrix factorisation. The identified patterns generalise from the train to a test set and match expectations on the brain activity for the different tasks from the neuroscience literature. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well written and addresses an important problem in the analysis of fMRI data in a novel way that achieves impressive performance. 

Weaknesses:
While the paper criticises that existing methods cannot be applied to large, diverse datasets, the paper lacks a study of the computational efficiency of the proposed approach and a comparison with existing methods. 

Limitations:
Limitations of the approach are not openly discussed. 

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper addresses the challenge of identifying elementary functional neuronal networks and their combinations in the context of complex tasks, using task-specific functional MRI (fMRI) data. The central problem it tackles is the deconvolution of task-specific aggregate neuronal networks into elementary networks. These elementary networks can then be used for functional characterization and mapped to underlying physiological regions of the brain. Due to the high-dimensionality, small sample size, acquisition variability, and noise inherent in this task, the authors propose a deconvolution method based on supervised non-negative matrix factorization (SupNMF). The results demonstrate that SupNMF can uncover cognitive ""building blocks"" of task connectomes that are physiologically interpretable, predict tasks with high accuracy, and outperform other supervised factoring techniques in both prediction accuracy and interpretability. Overall, the proposed framework offers valuable insights into the physiological foundations of brain function and individual performance.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The paper presents a valuable effort to implement a supervised decomposition method in a novel way, showing the potential for this approach in a complex context, such as the analysis of neuronal networks.
2. The authors provide fascinating results indicating that each task has unique markers within these learnable networks. This insight could contribute significantly to understanding how tasks are represented and processed within the brain. Observing that some networks are shared across tasks also provides a meaningful direction for future research.
3. The alignment of the findings with existing physiological research is a good sense for future study.

Weaknesses:
1. Reproducibility: The study could be enhanced by applying the proposed method to other datasets or by resampling the existing dataset. This would help to assess the generalizability of the method and the robustness of the findings, which is currently a limitation of the work.
2. Baseline Comparison: It would be beneficial if the authors had compared the proposed SupNMF method with other supervised decomposition methods, such as Partial Least Squares regression. This lack of comparison limits the understanding of how their proposed method stands in relation to existing methodologies regarding performance and effectiveness.
3. Parameter Study: The authors need to provide an in-depth analysis or sensitivity study concerning the weight parameter \lambda. As this parameter likely plays a significant role in balancing different loss terms, this omission constitutes a substantial weakness, potentially leaving readers unclear about the effectiveness of supervision signals.

Limitations:
Yes

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper presents a decomposition method for task-functional connectivity. It proposes canonical task connectomes which derives sub-structure of functional brain connectivity which group connectomes which identify elementary components of the overall connection. The authors use supervised non-negative matrix factorization to factor connectome matrices and show that the derived features are suitable to predict tasks for functional MRI and robust dimension reduction of the original representation. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is clearly written. 
- It construct a clear optimization problem whose results are easily interpretable. 
- It demonstrates a solid dimension reduction for connectome data. 

Weaknesses:
- Motivation for including supervision in the connectome decomposition is very weak. 
- There are some missing details on variable descriptions, e.g., $d$ in line 96 is missing, $\hat y$ in eq (2) is missing (although can be inferred that it is a prediction for a class), and derivation of $X$ from $C$ should be better explained. 
- Experiment is performed only on one study. It can be excused if the dataset is rare, but there are so many publicly available fMRI data. 
- Lack of baselines. It is missing the most fundamental baseline, i.e., LDA. Moreover, just typing in ""supervised dimension reduction"" in google scholar yields various literature but this paper demonstrates only SupSVD as a supervised baseline. 



Limitations:
The paper does not discuss any limitation of its own. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
This contribution presents a novel method to find a functional basis for a database of task fMRI acquired from different subjects. The functional basis, dubbed canonical task connectomes, is shard across large cohorts; can be composed into task-specific networks; and is predictive of task efficacy.

The authors produce this functional basis through supervised and non-supervised NMF and SVD. For this they propose an objective function (in equation 3) which is compatible with these methodologies.

To implement this approach the authors use the HCP100 database which has 6 cognitive tasks. To show that the obtained basis is task-specific the authors use UMAP plots of different resulting decompositions showing good separability of tasks in the embedded UMAP space. To show that their basis is generalizable across cohorts they use 80/20 splits of the HCP 100 database and use the basis to inform a classifier that predicts the task performed by an unseen subject given the fMRI acquisition. To claim physiological and anatomical grounding for their basis, the authors compare qualitatively the basis against known anatomical traits and the involvement of different basis components as important features for each task.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
This contribution is a high-quality application of known methods to the significant problem of understanding how functional connectivity in the human brain (as measured  by fMRI) is centric to cognitive tasks.

The manuscript presents a well-justified formulation of the problem as a deconvolution case and solves it through different approaches, supervised and non-supervised. This formulation and resolution are original and well presented. Even if the methodological contribution is not at the center of this manuscript, the application of known techniques is well-justified and evaluated.

The evaluation of the results are a good balance of qualitative evaluation (e.g. with UMAP embeddings), and quantitative (e.g. with the clustering approaches) in the case of task-specificity of the connectomes. The generalisation experiment using a downstream classification task is also well conceived. Finally the relation with anatomy and physiology is well organised.

In all, this contribution presents a very good application of known methods to an important problem in neuroimaging. So it's a paper that will have impact in one area, the neuroimaging one.

Weaknesses:
I find two weaknesses which are related to claims of cohort generalisability. In short, with the availability of public datasets of fMRI, it's hard to justify a cohort generalisation claim while staying in one 100-subject database. Specifically when the used 100-subject set is a subsample of a 1,200 total database. In light of this, second weakness I find is the lack of an analysis of the stability of the functional basis across datasets, including a study on dataset size.

Limitations:
The authors have not explicitly mentioned the limitations in the manuscript.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper presents a novel framework for fMRI analysis that aims to deconvolve complex neuronal networks into task-specific elementary networks called ""canonical task connectomes."" The proposed method utilizes supervised matrix factorization to identify these task-specific networks and demonstrates their interpretability and generalizability. The study showcases experimental results on the Human Connectome Project dataset, highlighting the ability of the framework to capture the natural task-specific structure in neuroimages. 

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- The paper presents a new problem formulation and introduces the SupNMF method, which is a novel approach to identifying task-specific networks. The authors demonstrate the usefulness of the proposed framework in identifying canonical task connectomes that have a strong physiological basis and can be mapped to regions of the brain to identify physiological underpinnings of tasks.
- the authors present the problem formulation and the proposed method in a clear and concise manner.
- The proposed interpretable framework has the potential to advance understanding of complex cognitive processes and to identify biomarkers for predicting tasks. 
- The authors also provide a comprehensive discussion of relevant methods and materials.

Weaknesses:
- While the authors present comprehensive experimental results, they could provide more details on the performance of the proposed framework in comparison to other state-of-the-art methods. Additionally, the authors could provide more details on the interpretability of the identified canonical task connectomes and how they relate to existing literature in neurosciences. 
- While the authors briefly mention the potential applications of the framework in understanding shared and unique functional networks across different pathologies and how task-specific networks can get dysregulated due to the onset and progression of diseases, a more in-depth discussion of these applications and their potential impact on the field would be helpful. 
- The authors did not explain much on why the “unrelated set” of subjects in the Human Connectome Project is selected. Also, more datasets are expected to be included to demonstrate the generalizability of the proposed method. 
- The authors could provide more details on how they determined the optimal number of latent connectomes and how this choice impacts the results. One potential concern of the proposed framework is that it relies on the assumption that the observed connectome matrix can be represented as a linear combination of a small number of latent matrices. While this assumption may hold for some datasets, it may not be applicable to all fMRI datasets, especially those with high levels of noise or variability. Additionally, the choice of the number of latent connectomes (i.e., the dimensionality of latent space) is critical and may impact the performance of the proposed framework. 
Another potential weakness of the proposed framework is that it requires task-label vectors for each connectome in the dataset. While the authors provide details on how they obtained the task-label vectors for the HCP dataset, it may not be feasible to obtain such labels for all fMRI datasets. Additionally, the choice of the task-label vectors may impact the performance of the proposed framework, and the authors could provide more details on how they selected the task-label vectors and how this choice impacts the results.

Limitations:
While the proposed framework has the potential to advance our understanding of complex cognitive processes and to identify biomarkers for predicting tasks, it is important to consider the potential ethical implications of this research. For example, the use of fMRI data for predicting cognitive states or identifying biomarkers could raise concerns about privacy, informed consent, and potential misuse of the data. 

Rating:
4

Confidence:
4

";0
i2H2sEiq2T;"REVIEW 
Summary:
The paper provides a unification of ad-hoc analysis and interpretations of the ghost clipping algorithm under a single framework. It also shows that for certain operations, such as fully-connected and embedding layer computations, further improvements to the runtime and storage costs of existing decompositions can be deduced using certain components of the proposed framework.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- **Originality**: The paper proposes a novel framework for fast gradient clipping used in DP-SGD. This is a significant theoretical contribution to the field of privacy of machine learning.
- **Quality**: The paper is well-organized. A thorough explanation of the proposed framework, including mathematical derivations and experimental results is provided.
- **Clarity**: The paper is easy to follow and understand, even for readers who are not experts in the field. The authors provide clear explanations of the background, motivations, concepts, and techniques used, and the experimental results are presented in a concise and understandable manner.
- **Significance**: The paper has significant implications for the specific field of privacy of machine learning. It provides a unified perspective on the ghost clipping algorithm in DP-SGD.

Weaknesses:
The paper focuses only on a specific technique of gradient clipping, i.e., ghost clipping. Whether this ghost clipping lies at the heart of the big field of privacy of deep learning or not is unclear to me.

Limitations:
Limitations are not discussed by the authors. Perhaps the possible relation or extension to other clipping algorithms can be mentioned in the final as a separate concluding section which is now missing.

Rating:
6

Confidence:
1

REVIEW 
Summary:
This paper provides a unified framework for efficiently computing the gradient norms of individual examples for a wide range of neural network architectures, which significantly decreases runtime and storage costs for implementing DP-SGD.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The considered problem is important: Computing per-example gradients is a computation bottleneck for previous implementations of DP-SGD.
2. Theoretical and empirical improvements over baselines are clearly presented and significant. Tables 1 and 2 summarize the time and storage complexity for previous work and the proposed algorithm, and Figures 1 and 2 summarize the observed improvements in experiments. The theoretical and the empirical improvements are both convincing.
3. The presentation is rigorous and clear.

Weaknesses:
1. The experimental evaluation does not provide a big-picture idea of the end-to-end savings provided by the proposed framework. Figures 1 and 2 exhibit the runtime/storage costs only for the gradient computation operation, and it does not show how the overall costs are affected. If gradient computation accounts for a small portion of the overall runtime, then the overall time savings will not be significant. As it stands, the experimental results do not give a concrete idea for the overall savings in runtime, which is the metric of interest for practitioners.
2. It would be nice to see an empirical evaluation for the more popular layer types mentioned in the text, e.g. convolution and attention.

Limitations:
There is no discussion of societal impact beyond the underlying (implied) importance of user privacy. However, I don't think any further discussion of societal impact is necessary for this work.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper unifies a framework that enables us to apply ghost clipping algorithms to new layers of neural networks, apart from fully connected, feed-forward layers. In particular, the paper discusses efficient implementations of computing norms for running DP-SGD to train neural networks. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The contributions of this paper are as follows: 

1. It shows how to utilize the proposed framework to apply ghost clipping techniques to new layers for DP-SGD on training neural networks. Three main examples are included to discuss how to implement efficient algorithms to compute the gradient norm and to run DP-SGD. 

2. Experimental results evaluate the effectiveness of their proposed framework over the classical ghost clipping framework in run time and memory requirements. 

Weaknesses:
Literature on the importance of DP-SGD and clipping operators is lacking in this paper. It is better to include them in Section 3 (Previous work) or in the introduction section to motivate the study of this paper in a broader impact.  

Limitations:
N/A 

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper studies the memory/time complexity of the per-sample gradients norm computation step in the DP-SGD algorithm. The authors present a general theoretical framework that generalizes the idea of ghost norm computation (computing the norm of the persample gradients without having to store the gradients), and can be applied to arbitrary layers of a DNN (if an ad hoc decomposition can be found). The authors show early experiments on the benefits of their methods compared to previous ghost clipping techniques.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
originality: - Computing persamble gradient norms is a real bottleneck when training DNNs with DP-SGD. Typically, the Opacus library (Pytorch for privacy) stores the gradients per sample before computing the norms, which can be memory intensive, especially for large architectures. Other methods have been proposed to compute the norm without storing the gradients (ghost norm), but no general framework has been proposed yet. This paper gives a clear problem formulation and a general theoretical framework to approach this problem. I was very happy to see a theoretical work that tries to tackles this issue.
quality: The math are correct
clarity:  ok
significance: using the theory of linear operator to improve time and space complexity of DP-SGD is good novelty that could benefit the community

Weaknesses:
- Typos: l108 “authors”
- Overall, the experiments are very insufficient. They are performed with batches of size 1, for which the vanilla per-sample gradient storage method has a better running time, but with which it is never compared. The authors should investigate in detail how their method would work with GPUs to see how it would be beneficial in practice: the complexity bounds given are linear in the batch size, but in practice we benefit greatly from parallel computation/optimized matrix multiplication. 
- Experiments should be performed on whole (even small) networks used in practice, with batches of size greater than one. In Li et al (2022), they compare ghost norm to opacus when training a GPT2-like architecture and find similar runtimes; a similar comparison could be done here (even with much smaller versions of GPT2 when computational power is limited). 
- It is not the same abstract in OpenReview and in the article. The first states that savings can be as high as 150x: this could be the case in very specific situations chosen for the experiments, but the authors do not elaborate on why these choices are realistic in practice. 
- The section on related work is sparse, with very few papers cited (no mention of why gradients need to be clipped, no mention of the whole deep learning literature where this is a real bottleneck (Opacus))
- No code for reproduction


Limitations:
The paper does not include a limitation section. 
Suggestions for improvement would be to perform a rigorous empirical study of the proposed method, with entire architectures and using micro batches, and see if can easily used with an existing DL library (pytorch, JAX...). 

Rating:
5

Confidence:
3

REVIEW 
Summary:
This work unifies the analysis of the ghost clipping algorithm, which was previously analyzed for specific architectures only. The proposed framework extends previous results to a wide class of network architectures including new applications and provides asymptotically faster computation times than the previous case-by-case approaches. Some numerical simulations confirm the effectiveness of the proposed algorithm demonstrating faster computation time in practice. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is nicely structured and well motivated. The considered family of functions (NN architectures) is quite general and the proposed approach introduced by Proposition 4.1 seems to be very principled. Despite of the generality, the strategy proposed in Proposition 4.1 can be implemented for a number of important special classes of functions such as fully-connected networks, embedding layers and low rank approximation layers. The authors demonstrate improved runtime and storage costs for this algorithm in these three special cases. 


Weaknesses:
-- The main motivation of the work is that when the batch sizes of DP-SGD is large, the runtime and storage can be large. Why this problem cannot be simply solved by using parallel computing (assigning different batches to different workers)? If this is one possible approach, I believe this should be mentioned and discussed in the related work. 

-- It seems that the assumptions on the loss function and the layers $\phi_x$ are not explicitly stated. It is unclear to me if it is sufficient to have all $\phi$ and the loss function to be Frechet differentiable or any additional technical assumptions are needed for Proposition 4.1. Why the sub gradient of $Z_x$ might not be unique while $\nabla h$ and $g$ in this proposition are unique? 

Limitations:
-

Rating:
6

Confidence:
3

";1
yw1v4RqvPk;"REVIEW 
Summary:
The authors proposed a learning-based framework to solve for optimal equilibria in n-player general-sum extensive-form games. The key idea is to leverage Lagrangian relaxation and convert such games to 2-player zero-sum extensive-form games which can then be solved using known learning methods that solve for minimax equilibria. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* significance
I believe this work could be a significant contribution to the field as it proposes a computationally practical method that solves for optimal equilibria in n-player general-sum games. This is significant as it offers a language to speak about optimality of equilibria in addition to stability and could make game theoretic methods attractive in wider classes of practical applications. 

* originality
I believe the proposed method is novel. 

* clarity
The paper is well written and relatively easy to follow. 

* quality
The theoretical derivation of the method appears sound; the empirical results seem extensive though it would be interesting to understanding additional details which I have commented below.  

Weaknesses:
The presentation of the empirical results seems to be primarily focused on the efficiency and feasibility of converting the problem of solving for optimal equilibria in n-player general-sum EFGs to solving minimax equilibria in two-player zero-sum games. For instance Table 1 presented runtime results of the learning-based method compared to solving LP directly. 

Would it be possible to provide additional details on optimality as well especially in general-sum games such as Sheriff where we know the value of max-welfare equilibria (e.g. https://www.cs.cmu.edu/~gfarina/2020/efcce-aaai20/coarse-correlation.aaai20.pdf)? Would the proposed method scale up to these instances of sheriff game? 

Limitations:
The authors have discussed practical limitations of the method in snippets of the main text.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper focuses on addressing the challenge of computing optimal equilibria in extensive-form games. The authors introduce the revelation principle, which transforms the problem into a linear programming (LP) task. 

They propose using Lagrange relaxations to solve the LP, treating the resulting saddle-point problem as a Nash equilibrium in a zero-sum two-player game. The authors explicitly construct such zero-sum game. To efficiently find the solution, the authors employ regret minimization over conic hulls. Additionally, they highlight the flexibility of their approach by showing that other algorithms for zero-sum two-player games can be utilized, offering different convergence rate guarantees or achieving last-iteration convergence. 

The paper concludes with thorough experimental evaluations, demonstrating the effectiveness and scalability of their proposed methods across various simple games.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper addresses the significant problem of computing optimal equilibria in game theory. By reducing this problem to zero-sum games, the authors provide a framework for finding optimal equilibria. The key contribution lies in the fact that existing algorithms for zero-sum extensive form games can be directly applied to this problem. This reduction greatly enhances the understanding and applicability of the proposed approach.

- The experimental evaluation conducted by the authors is robust and contributes to the strength of their work. They provide evidence of the scalability of their methods through preliminary experiments. This not only showcases the effectiveness of the proposed techniques but also demonstrates their potential for real-world applications. The inclusion of experimental results further reinforces the practical relevance of the research and adds value to the overall paper.

Weaknesses:
My only concern is about the revelation principle. The reduction in the paper relies on the revelation principle, which is a fundamental concept. However, it is not clear regarding the specific conditions under which a fixed pure strategy d_i exists for different players and within which games and equilibria this concept applies.  Additionally, how to effectively find such d_i strategies is also a problem.

Limitations:
The authors have partially addressed the limitations of their work, though there is space for improvement (see the section Weaknesses and Questions).



Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper studies the computation of optimal equilibria in multi-player extensive-form games via no-regret learning algorithms. The key idea is to take the constrained LP formulation of the optimal equilibrium problem proposed by Zhang and Sandholm and consider the saddle point formulation, which can then be solved using no-regret learning algorithms. In order to alleviate the large $\lambda^*$, a binary-search based algorithm is proposed. Finally, the algorithms are compared with a LP-based method on several tabular games as well as for an auction-design problem.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is well-written. The results presented clearly and the writing is easy to follow.

- The experimental results seem compelling as it has an order of magnitude advantage in wall-clock time over the LP-based method. This advantage is consistent across a set of diverse instances.

Weaknesses:
- I have some concerns about the LP formulation (G). Consider a $N$ player normal form game with $A$ actions per player for instance; it seems that for the correlated equilibrium in such a game, the $\mu$ in (G) would need to have dimension $A^N$. However, no swap-regret algorithms can find an approximate correlated equilibrium with $\tilde{O}(NA^2/\epsilon^2)$ samples. It seems that the LP formulation (G) can lead to suboptimal dependence on the number of players.

- Perhaps related the previous point, while in Line 79 the authors claim that ""Our algorithm significantly outperforms existing LP-based methods"", in the experiments it is only compared against the LP based solution to (G) proposed by Zhagn and Sandholm. Is (G) the only known LP formulation of the optimal equilibrium problem? If not, wouldn't it make sense to compare against those as well?

Limitations:
This paper does not have foreseeable negative societal impact.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper presents a novel approach to compute optimal equilibria in multi-player extensive-form games through the use of Lagrangian relaxation as a two-player zero-sum extensive-form game. Building upon the mediator augmentation game framework, the proposed computing approach significantly contributes to the reduction of zero-sum games, thereby holding substantial implications for mechanism design and information design. Furthermore, it plays a pivotal role in fostering a well-balanced hierarchy of concepts.



Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
-- The investigation of computing minimax equilibria in extensive-form games is an interesting and well-motivated problem.

-- Most correlation equilibrium notions pose challenges in finding optimal equilibria, whereas the framework presented in this paper offers a learning-based algorithm to compute them.



Weaknesses:
-- The paper lies heavily on notation that lacks intuitive explanations. To enhance reader understanding, a schematic representation of the mediator-augmented game framework should be included in the main body. 

-- The approach proposed in this paper resembles a transformation of the original game into a mediator-led Stackerberg game. It would be beneficial to discuss and compare existing research on equilibrium solutions for zero-sum games in Stackelberg games to provide a contextual background for the paper's contribution. Lack controlled experiments involving additional algorithms for equilibrium solutions in Stackelberg games.


Limitations:
N/A

Rating:
6

Confidence:
3

";1
KfOUAlraMP;"REVIEW 
Summary:
This paper presents a novel adversarial attack methodology premised on Wasserstein distributionally robust optimization. This approach serves as a generalized technique that encompasses many established adversarial attack methods such as FGSM and TRADES. To ensure computational tractability, the authors introduce a first-order approximation of the attack loss. This approximation is particularly suited for gradient-based optimization methodologies, offering a much faster computational speed compared to SOTA techniques such as AutoAttack. Theoretical evidence is provided for certified bounds on adversarial accuracy and out-of-sample performance. Experiments on the CIFAR-10 dataset are conducted to demonstrate the effectiveness of the proposed method.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
+ The proposed ReDLR loss (10) is both simple and effective. It is built on clear intuition, that the attack should perturb images classified far from the decision boundary more aggressively while leaving misclassified images unchanged. The theoretical analysis and guarantees provided further validate the proposed approach.

+ The method is presented using robust proofs and in-depth analysis. I didn't notice any substantial unjustifiable assumptions or logical inaccuracies. The section on ""Bounds on Out-of-Sample Performance"" is particularly intriguing. Despite substantial parts of the proof being left in the appendix due to page limits, the analysis still provides invaluable insights for future research on empirical distributions of test sets and broader applications of Wasserstein balls.

+ The empirical results demonstrate notable improvements over robust baseline methods like DLR. It is worth noting that while the proposed ReDLR method may appear very similar to DLR at first glance, the empirical results show significant improvements, attesting to the effectiveness of the theoretical analysis.

+ The paper is well-structured and clearly written. Even the first part of the method section could serve as a comprehensive introduction to DLR-based methods, making it approachable and comprehensible even for novices in the field.

Weaknesses:
- One weakness of the paper is that the number of experiments provided is limited (only results on CIFAR-10 with a limited number of baselines compared). However, considering the major contribution of the paper is to provide a comprehensive theoretical analysis and the existing results already demonstrate the effectiveness of the method, I would say the weakness of experiments does not affect the overall quality of the paper.


Limitations:
See Weakness and Questions.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper focuses on the issue of distributional robustness in deep neural networks and introduces a novel approach to address it. The authors present a tractable distributional attack within a Wasserstein ball, which allows attackers to perturb inputs in a non-uniform manner. They propose a first-order approximation of the problem and introduce a new loss function called ReDLR. This framework encompasses the Fast Gradient Sign Method (FGSM) attack as a special case. The authors also provide tractable bounds for adversarial accuracy against distributional threat models, which offer insights even for pointwise attacks. To validate their approach, experiments are conducted on the CIFAR-10 dataset using state-of-the-art deep neural networks on RobustBench, demonstrating the effectiveness of the attack and the tightness of the bounds.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:

The strengths of the paper are:

-The introduction of a tractable attack for Wasserstein Distributionally Robust Optimization (WDRO) stands as a significant contribution, as does the adversarial accuracy boundary. Both offer key advancements to the current understanding and applications of deep learning models under adversarial conditions.

-The paper is underpinned by robust mathematical foundations, and its place in relation to the existing body of work in the field is clearly and precisely articulated, demonstrating a deep and accurate understanding of the state-of-the-art.


Weaknesses:
-The guarantees provided in the paper are asymptotic in nature and depend on the quality of the first-order approximation of the problem. This introduces some limitations and potential uncertainty in the practical applicability of the results.

Limitations:
The authors adequately addressed the limitations

Rating:
8

Confidence:
3

REVIEW 
Summary:
The paper proposes a new method combining Wasserstein distance, DRO, and adversarial attack. The novelty is that the method computes the adversarial attack bounded by the Wasserstein distance. The paper provides a bound on the performance of out-of-domain samples. It applies the new adversarial attack on CIFAR10 dataset. The numerical results show that the attack is effective on models that are robust to pointwise attacks, and computing attacks is much faster than other methods.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper proposes an interesting direction by combining DRO and adversarial attacks. It is clear in the writing and presenting methods.
2. The paper gives some theoretical results on the proposed methods, like lower bound on $R_{\delta}$. The paper also shows numerical results comparing the lower bound from different methods.
3. The paper provides numerical results to show that the adversarial attack is effective at attacking and fast to compute.

Weaknesses:
1. The paper misses some references and does not compare the method with the existing DRO adversarial attack methods, like [A].
[B, C] also have a similar approach by combining DRO with adversarial attacks. In the area of traditional attacks, the paper can compare with more methods, like CW attack in [D].

[A] Sinha, A., Namkoong, H., Volpi, R. and Duchi, J., 2017. Certifying some distributional robustness with principled adversarial training. arXiv preprint arXiv:1710.10571.
[B] Volpi, R., Namkoong, H., Sener, O., Duchi, J.C., Murino, V. and Savarese, S., 2018. Generalizing to unseen domains via adversarial data augmentation. Advances in neural information processing systems, 31.
[C] Hua, X., Xu, H., Blanchet, J. and Nguyen, V.A., 2022, December. Human imperceptible attacks and applications to improve fairness. In 2022 Winter Simulation Conference (WSC) (pp. 2641-2652). IEEE.
[D] Carlini, N. and Wagner, D., 2017, May. Towards evaluating the robustness of neural networks. In 2017 ieee symposium on security and privacy (sp) (pp. 39-57). Ieee.

2. The paper only does experiments on CIFAR10 dataset. It would be helpful to see results on other datasets.

Limitations:
The authors discuss some of the limitations of the work, including only valid on small-scale attacks. More limitations could include the stability of the attack and how it compares to more state-of-the-art attack methods. The method also should be compared with more attack methods, like DRO-based attacks. From the current experiments, it is not very convincing that the method outperforms many other adversarial attack methods.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper links more general attacks with question of out-of-sample performance and Knightian uncertainty. To evaluate the distributional robustness of neural networks, this paper proposes a first-order AA algorithm and its multistep version. The proposed attack algorithms include Fast Gradient Sign Method (FGSM) and Projected Gradient Descent (PGD) as special cases. Furthermore, the authors provide a new asymptotic estimate of the adversarial accuracy against distributional threat models. The bound is fast to compute and first-order accurate, offering new insights even for the pointwise AA. It also naturally yields out-of-sample performance guarantees. This paper also conducts numerical experiments on the CIFAR-10 dataset to illustrate the theoretical results.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper proposes a unified approach to adversarial attacks and training based on sensitivity analysis for Wasserstein DRO. This approach leveraging results from Bartl et al. (2021) is better suited for gradient-based optimization methods than duality approach adopted in most of the works to date. This paper further links the adversarial accuracy to the adversarial loss, and investigate the out-of-sample performance.
2. This paper derives a general adversarial attack method. As a special case, this recovers the classical FGSM attack lending it a further theoretical underpinning. However, the proposed method also allows to carry out attacks under a distributional threat model which has not been done before. This paper develops certified bounds on adversarial accuracy including the classical pointwise perturbations. The bounds are first-order accurate and much faster to compute than the AutoAttack benchmark.

Weaknesses:
1. The Wasserstein distance for the adversarial robustness was usually discussed in the theoretical analysis. This paper utilizes the first order approximation of the adversarial loss on the CIFAR-10 for the verification. This approach contributes on the efficiency of the Wasserstein distributional robustness for the practical use. Yet, it is still questionable on the robustness on the large-scale dataset. Due to the computational complexity of the data dimension for the Wasserstein distance, it would be interesting to see the performance and the efficiency of the proposed approach on the image data such as tiny-ImageNet with dimension 64 and ImageNet with dimension 224.
2. The Wasserstein distance is about the overall loss rather than the single norm such as L_2 and L_\infty. Thus, the Wasserstein robustness may contribute to other types of robustness such as common corruptions, natural adversarial examples and the perturbations beyond the adversarial robustness. It would be more sufficient to see the potential improvement of the proposed approach on more general perturbed images.

Limitations:
This paper uses Wasserstein distributionally robust optimization (DRO) and obtain novel contributions leveraging recent insights from DRO sensitivity analysis. Yet, it would be better to clarify with more sufficient evidences. The detailed comments can be seen in the weaknesses.

Rating:
6

Confidence:
4

";1
DAdfU1ASLb;"REVIEW 
Summary:
In this paper, authors have applied optimal transport to address selection-bias issue in binary treatment setting for individualized treatment effect estimation. They have proposed a relaxed mass-preserving regularizer to address the mis-alignment generated from outcome imbalance and outliers in non-ideal mini-batches. They also proposed a proximal factual outcome regularizer to relax the unconfoundedness assumption. They have done extensive evaluation of the proposed approach on IHDP and ACIC, two semi-synthetic datasets.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- A novel method is proposed for treatment effect estimation in binary setting based on optimal transport (OT) that uses a relaxed mass-preserving regularizer to address the mis-alignment generated from outcome imbalance and outliers in non-ideal mini-batches, as well as a proximal factual outcome regularizer to relax the unconfoundedness assumption.
- Extensive evaluations is performed to study the proposed model that provides necessary analysis and ablation studies.
- Theoretical results are provided to bound the error in terms of the proposed components to show the convergence (I have not verified the proof).
- Idea was clearly stated with sufficient details.


Weaknesses:
- Baselines used in the paper seem old. There are very large number of methods proposed for treatment effect (TE) estimation. It would have been nice to see comparison with recent techniques. Hoever, even if authors don't add new baselines, I think paper has sufficient novelty to be above the acceptance threshold.
- The paper solves (equation (1)) Conditional average treatment effect estimation also called as heterogeous treatment effects estimation, as called in machine learning literature, but paper did not even mention these names. Please update the paper so that readers don't have any confusion. There is already a lot of confusion in ML regarding the terminology.
- Causal inference methods are typically based on some assumptions -- but I did not see any. Please clearly state the assumptions required to apply these methods.

Limitations:
Authors did not discuss the assumptions and limitations of thier work.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors propose to minimize a (relaxed) Sinkhorn distance between representations under the two treatment groups in order to balance the latent space and better predict the treatment effects. They highlight two problems with existing TARNet-based approaches that do not regularize with optimal-transport cost: mini-batch sampling effects (MSE) and unobserved confounder effects (UCE). Theorem 3.1 & Corollary 3.1 formally characterize how the proposed method suffers less from MSE. The authors also introduce additional regularization terms that are meant to deal with UCE. Finally, they show improved performance with two classical semi-synthetic benchmarks.

Soundness:
2

Presentation:
4

Contribution:
3

Strengths:
 * The problem of estimating heterogeneous treatment effects is important, the paper is written well, and the solution is creative.
 * The motivation for the relaxed Sinkhorn distance (Sec. 3.2) is great. 
 * The benchmark results are solid.
 * The additional empirical explorations (Sec. 4.3 - 4.5) are interesting as well.

Weaknesses:
My biggest problem is with the UCE part of the paper.
 * Sec. 3.3: the explanation for the hidden-confounding regularizer makes intuitive sense. However, I do not see a formal connection. Once the authors substitute the actual potential outcomes with the model's predictions (Eq. 11 -> Eq. 12), one begins to wonder if this can actually help with hidden confounding.
 * A major confounder with the issue above is that the authors do not appear to include hidden confounders in their semi-synthetic benchmarks. Therefore, it is impossible to tell if this regularization approach really helps with UCE.
 * If the authors still wish to make the case for UCE mitigation, they need to reference the growing literature on partial identification / sensitivity analysis for hidden confounders and how their regularization relates to sensitivity models.

Besides that, I think the authors should spend more time addressing what the theoretical results mean for the MSE issue that they highlighted in the introduction.

Small nitpicks:
* Figure 1a, the orange annotation looks like it is referring to the X_1 part of the figure.
 * line 107-108: ""Optimal transport is a preferred method due to its advantages over competitors."" is overly vague.
 * line 108-109, ""It accounts for the distribution’s geometry, making it effective [where the KL-divergence] fails."" is a strange statement. Why does the geometry help here? No explanation is provided. 
 * The beginning of the paper is a bit verbose.

Limitations:
Limitations or potential societal impacts are not discussed.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The objective of this research paper is to tackle two primary concerns: (1) the effects of mini-batch sampling (MSE), which lead to misalignment in non-ideal mini-batches characterized by outcome imbalance and outliers, and (2) the impacts of unobserved confounders (UCE), resulting in inaccurate discrepancy calculation due to the neglect of these unobserved factors. To address these challenges, this paper presents a novel approach based on optimal transport in the context of causality.

More specifically, the proposed approach builds upon the stochastic optimal transport framework. It introduces a relaxed mass preserving regularizer to mitigate the MSE issue and devises a proximal factual outcome regularizer to address the UCE problem. By incorporating these techniques, the proposed method achieves a principled solution for treatment selection bias.

The effectiveness of the proposed method is extensively evaluated through a series of experiments. The results demonstrate that the approach successfully overcomes treatment selection bias and outperforms existing state-of-the-art methods by a significant margin.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is clearly written and well-presented. It delves into the important question of improving the estimation of individualized treatment effects, a crucial topic within the field of causal inference. Optimal transport, an emerging field in machine learning, is effectively employed to measure treatment selection bias. Furthermore, the two proposed regularizers are technically intriguing and efficient, as supported by both theoretical and empirical evidence.

Weaknesses:
I have two main concerns: 

1. The potential applicability of the proposed relaxed mass-preserving regularizer extends beyond causal effect estimation and encompasses a wide range of application settings. It appears to be independent of the estimation of causal effects. The authors acknowledge that the OT discrepancy can be easily affected by various sampling cases, highlighting that it is not solely limited to the distribution discrepancy across treatment groups. Consequently, the main contribution of this regularizer may be seen as an incremental addition to the field of causal inference, raising concerns about the originality and novelty of the paper.

2. The existing literature on the subject appears to be considerably understudied. For example, important references like ""Optimal Transport for Counterfactual Estimation: A Method for Causal Inference"" by Arthur Charpentier, Emmanuel Flachaire, and Ewen Gallic are missing and remain unaddressed in the related work section. It would be beneficial for the authors to conduct a more comprehensive investigation of the related literature and consider incorporating these references into their study.

Limitations:
See above

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper introduces a new approach called Entire Space CounterFactual Regression (ESCFR) to estimate individual treatment effects from observational data, addressing two critical problems that existing methods fail to solve. The first problem is mini-batch sampling effects (MSE), which lead to misalignment in non-ideal mini-batches with outcome imbalance and outliers. The second problem is unobserved confounder effects (UCE), which result in inaccurate discrepancy calculation due to the neglect of unobserved confounders. ESCFR combines the principles of optimal transport and causality to overcome these issues.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- The theoretical evidence is solid and convincing.
- The topic, mentioned in the paper as the mini-batch misalignment and UCE error, are interesting and practical issues of ITE estimation.
- Experimental results are solid.

Weaknesses:
Is there some theoretical guarantee for the UCE regularizer proposed in this paper? If so, I think the quality can be further improved.

Limitations:
See in Weaknesses

Rating:
8

Confidence:
5

";1
mA7nTGXjD3;"REVIEW 
Summary:
This work proposes an analysis of the independent natural policy gradient algorithm for Markov Potential games. Under technical assumptions on a sub-optimality gap problem dependent quantity and supposing access to exact (averaged) advantage functions, this paper provides a novel $O(1/\epsilon)$ iteration complexity to guarantee that the average Nash gap along iterations is smaller than the accuracy \epsilon, improving over the previously known $O(1/\epsilon^2)$ iteration complexity in the same independent learning setting. After discussing the potential game setting as a warm-up and generalizing to the MPG setting, the paper provides simulations in a synthetic potential game and a congestion game. 


Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The convergence analysis improves over the $O(1/\epsilon^2)$ iteration complexity in prior work under some technical assumptions.

- To the best of my knowledge, the analysis provided in this paper is new and the proofs seem to be correct (I went through the appendix, please see some additional comments below). However, please see some comments below regarding related works and quantities/notations to be precised for clarifications. In my opinion, the main technical novelty is Lemma 3.2 (and Lemma B.3 relying on the technical lemma C.1) which is key to obtain the result in Theorem 3.3. This result is interesting and connects the sum of the ‘gaps’ (over agents) for a fixed temperature parameter $\eta$ and for $\eta$ going to infinity. 

- The paper is well-organized. 


Weaknesses:
**1. About the suboptimality gap \delta_K and the $O(1/K)$ convergence rate**:  
Theorem 3.3 provides an upper-bound on the average NE-gap which depends on the K-dependent quantity $\delta_K$ without further assumption, this provides a $O(1/K\delta_K)$ convergence rate as mentioned in the paper. 
I think it should be clearly stated that the $O(1/K)$ convergence rate which is claimed in the abstract and in the contributions under ‘mild assumptions’ (as it is stated) is actually “asymptotic”.  Indeed, under assumption 3.2, the bound in Corollary 3.6.1 features an unknown constant (number of iterations) $K$’ that is not explicit in the problem parameters (such a constant is only guaranteed to exist under assumption 3.2). This is due to the fact that the NE gap is only controlled (independently of $\delta_k$) for a large enough number of iterations as it appears in l. 500 in the appendix (p. 17) under the chosen assumption.  The results that the paper compares to in Table 1 have stronger guarantees in the sense that they are not asymptotic. This is not clear in the presentation and the comparison to prior work. 

**2. About Assumption 3.2**: this technical assumption guarantees that $\delta_K$ is uniformly lower bounded away from zero but it seems hard to interpret or give a meaning to this assumption although the ‘sub-optimality gap’ $\delta_k$ is a standard quantity in the bandit literature for instance. I am also not sure if this assumption is ‘mild’ as it is formulated. See the ‘Questions’ section for clarification questions regarding this assumption. 

**3. Discussion of related works**: Some relevant related works are missing in the discussion. 

**(a)** While Song et al. 2021 [27] is cited, it is not mentioned in the discussion that a $O(1/\epsilon)$ iteration complexity has been achieved in that work for MPGs with a Nash-coordinate ascent algorithm (see Section 5 Algorithm 7, note that sample complexity is given and the $O(1/\epsilon)$ iteration complexity appears in the proof when discarding the $O(1/\epsilon^2)$ sample complexity needed for policy evaluation). However, this algorithm is ‘turn-based’ and requires coordination and hence is not independent as the present work. 

**(b)** Fox et al. 2022 derived an asymptotic convergence result for the independent natural policy gradient algorithm considered in this work that seemed to be later used by Zhang et al. 2022 [33] but this reference does not appear in related works. This same result seems also to be used in Proposition 3.1 of the paper. 

**(c)** The results shown in this paper seem to have some similarities with the asymptotic convergence analysis provided by Khodadadian et al. 2022 in the single agent setting. For instance, the analysis in that paper introduces the optimal advantage function gap (see $\Delta$ as defined in Definition 2), a quantity similar to the sub-optimality gap $\delta_k$ in the present paper (up to the multi-agent setting). However, I would like to point out that this is just for the purpose of comparison and the present work has to overcome many difficulties related to the game theoretic setting and the multi-agent nature of the problem that make this work very different from Khodadadian et al. 2022. The abstract precises that the result improves over “ $O(1/\epsilon)$, that is achievable for the single-agent case”. Actually, Khodadadian et al. 2022 provide an asymptotic geometric convergence rate. Other recent related works even prove a global linear rate with increasing step sizes for the natural policy gradient algorithm (see for e.g. Xiao 2022, Section 4.2).  

Fox et al. Independent natural policy gradient always converges in Markov potential games, AISTATS 2022.

Khodadadian et al. On linear and super-linear convergence of Natural Policy Gradient algorithm,  Systems and Control letters 2022.  

Xiao. On the convergence rates of policy gradient methods. JMLR 2022

**(d)** minor remark: you might want to give reference to [Monderer and Shapley 1996, Potential Games, Games and Economic Behaviour 14, 124-143] which introduced this class of games in section 3.1 when you mention [12] (l. 128) which is much more recent. 

**4. Regarding the definition of Markov potential games** (Definition 2.1), for a fair comparison in Table 1, it might be worth mentioning that this definition differs from the one considered in [8,16] although it matches the definition used in [33, 34]. Indeed, the potential function in Definition 2.1 is supposed to have a discounted cumulative structure whereas such a structure is not available in the more general definition considered in [8,16]. As a matter of fact, the analysis becomes more challenging in [8] for instance since showing the potential function improvement is then more involved in that case, another decomposition different from the decomposition in Lemma B.2 l. 440 is then used to guarantee policy improvement (see Lemma in [8]). Also the dependence with respect to some parameters such as $(1-\gamma)$ and the state action space sizes are usually improved with this additional structure. 

**5. Originality of the analysis:** While Lemma 3.2 and its use is indeed new, the potential function improvement lemmas (Lemma 3.1, Lemma 3.5) and their proofs in appendix follow prior techniques used in [4, 33]. I suggest that authors mention this somewhere in the main part or in the appendix and emphasize the novelty of the analysis (Lemma 3.2). For instance Lemma A.1 was proved in [4], the proof of Lemma A.2 is almost identical to the proof in [4] while the proof of Lemma B.2 is very similar to the proofs in [33].   

**6. Clarity**: Overall, writing can be substantially improved in my opinion. Few minor details below: 

— l. 108: what do you mean by ‘multiple stationary points for the problem’? Stationary points if the potential function?

— l. 134-135: not very clear, see the ‘Questions’ section below. 

— l. 136-137: ‘for any two sets of policies’. I guess you mean any two joint policies (in the product of the individual simplices) when you say sets.

— l. 158: ‘They are related by the following lemma’, the constants you just defined in l. 157 or the quantities defined few lines above in l. 153?

— notation $f(\infty)$ is used in Lemma 3.2 and does not seem to be defined before although it is quite obvious this corresponds to $\lim_{\alpha \to \infty} f(\alpha)$ as used in l. 153. 

— Lemma A.4 in the appendix: the statement and the proof are not very precise. What is $\mathcal{R}_i$ (it seems only defined in the end of the proof of this lemma, or we can guess it from the title of the lemma)? I guess this is the set of reward functions which is known to have a particular structure for MPGs as you write it. $\mathcal{R}_i$ is a linear space in which ambient space? Please precise the proof even if we can guess the idea. Also, where is this result used? 

—  Lemma C.1: if rewards are vectors, please precise somewhere in notations that inequalities hold for all the entries of the vectors.  

**Typos (main part and Appendix)**: 
 l. 91: $V_i(s)$; do you mean $V_i^{\pi}(s)$? 

l. 476-477 (proof of Lemma B.2 in appendix): $\bar{A}_{f_i}^{\pi^k}$, what is $f_i$? I guess you mean $h_i$.
 
l. 479 to l. 480: $\pi_i^{k+1}(\cdot|s)$, $s$ is missing in the first term of the last equation and in the KL divergences.

l. 503 (proof of corollary 3.6.1): is there a missing $\phi_{\max}$ in front of $K’$ in the first inequality of the page (given l. 501)?  

Limitations:
The paper mentions the MPG setting as a limitation in the conclusion. 

**Extension to the stochastic setting:** While the analysis in the deterministic setting is an important step towards understanding the more practical stochastic setting where exact advantage functions are not available and can only be estimated via sampled trajectories, this analysis does not seem to be easily extendable to the aforementioned stochastic setting. This seems to be related to the fact that showing that the constant c is positive in the stochastic setting seems to be much harder if not hopeless even in a single agent setting (see for e.g. Mei et al. 2021). However, this is a limitation that also applies to prior work in MPGs analyzing independent natural policy gradient such as Zhang et al. 2022 [33]. A comment on this or an additional remark in the paper would be welcome. For instance, [8] analyzes the sample complexity in the stochastic setting but their algorithm does not cover the case where the regularization in the policy mirror descent-like update rule is not euclidean, KL regularization (which leads to natural policy gradient) is not covered. 

   Mei et al. ‘Understanding the effect of stochasticity in policy optimization’ (Neurips 2021)

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper provides a new analysis for the (Markov) potential games with a $O(1/\epsilon)$ convergence rate. The new results are problem-dependent and may be a tighter guarantee for certain classes of potential games. Asymptotic guarantees are also provided to elaborate on the problem-dependent nature of the results.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The results it presents showcase an improvement over previous results on (markov) potential games. The new rates are now unaffected by the number of actions and are at the order of $O(1/\epsilon)$. The problem-dependent nature of the results may provide a tighter guarantee on a certain class of potential games and the new analysis methods may lead to future works. The results are extensively discussed and empirical results are provided to corroborates the theoretical results. 

Weaknesses:
The new results do not seem to be directly comparable to the previous ones due to the use of a suboptimality gap. I encourage the authors to explicitly state this early in the paper to avoid confusion (e.g. in Table 1). I also encourage the authors to complement the work with more empirical results. For example, it would be interesting to see how the algorithm performs against previous ones when the suboptimal gap is very small. It may also be helpful to state when the suboptimality gap is very small, the results can degenerate into previous results. 

Limitations:
Yes, it is discussed in the conclusion part.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The article considers what the authors describe to be “natural policy gradient” learning in normal form and Markov potential games. This is the discrete time algorithm in which individual players' mixed strategies are multiplied by a softmax response to the opponent mixed strategies then normalised. Convergence rates are derived in both normal form and the Markovian potential games.

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
The article is nicely written, and the claimed results are supported by the theory.

The convergence rate results are nice, given the general difficulty to provide convergence rates in anything multiagent, and especially in Markovian games - of course restricting to situations in which there is a potential function for the full Markovian game makes things much less impossible, but it is still a hard problem.

The paper is nicely self-contained, and a real pleasure to read.

Weaknesses:
I have some doubts over the claimed results which I would like to see clarified.

The authors claim that Thm3. Implies a 1/eps convergence rate. I don’t buy it I’m afraid. c and delta_K are not controlled. They could be arbitrarily small, at least without further work. I suspect c is okay, although a sudden switch in best response could easily result in a very small pi_i^k(br_i(pi_{-i}^k)) coming into c late in the process. And since delta is the optimality gap when playing a mixed strategy, I find it very difficult to see how to constrain it effectively. (I think line 158 tells us that c is the smallest ever pi_i^k(br(pi_{-i}^k)), and delta_K is the smallest optimality gap that occurs up to time K – if I have misinterpreted this then my objections may dissipate!)

The synchronous form of ""learning"", and the tight construction of Markovian potential games with average reward, means the step up to Markovian settings is much less then in a less restrictive setting.

A more philosophical point is that the article assumes players can receive oracle information about the long term payoff of any action. While it makes for a nice compact paper, I think for NeurIPS the authors need to at least posit some suggestions for where learners might be able to access the advantage functions that are required to implement the method.

Furthermore, the dynamic is well known as the multiplicative weights algorithm, and I would expect the authors to compare their results with those presented under the multiplicative weights description.

Limitations:
No discussion of limitations is presented. I don't feel such a discussion would add much to the paper, but the review form asks the question!

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper studies the convergence of Natural Policy Gradient method (NPG) in Markov Potential Game. Under stronger assumptions, the convergence result improves upon previous ones.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
Under the considered setting, NPG is shown to achieve a $1/K$ convergence rate for multi-agent Markov Potential Game, matching the convergence rate in single-agent setting. Discussion on the parameters that appear in the convergence rate is presented in detail.

Weaknesses:
The convergence result only makes sense under the assumption that $c>0, \delta>0$. However, such assumption could be too strong and unnecessary (as the table 1 indicates).

Limitations:
N/A

Rating:
6

Confidence:
4

";1
7w4RGjzd81;"REVIEW 
Summary:
This study explores watermarking large language models without reducing output quality. It introduces ""unbiased watermarking"" which avoids trade-offs in prior work. Two novel techniques - $\delta$-reweight and $\gamma$-reweight - are proposed along with an improved likelihood ratio test for detection. Risks of large language models and how unbiased watermarking enables responsible AI are discussed.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. Introduces unbiased watermarking that maintains output quality. Prior work showed a trade-off, but an unbiased watermark avoids this. 
 
2. Proposes novel $\delta$-reweight and $\gamma$-reweight techniques that preserve output quality in experiments.  

3. Develops an improved detection method with a proven upper error bound, improving detection reliability. 

4. Concrete demonstration of the efficacy of watermarking techniques in maintaining the utility of LLMs for downstream tasks.


Weaknesses:
1. The paper lacks a thorough examination of the efficacy of the watermark detection method. It would strengthen the findings to provide more details on factors like detection accuracy, robustness to interference, and computational efficiency of the detection approach.  

2. The experiments only test the watermarking techniques on BART language models, without evaluating other popular LLMs like GPT models.

3. Conducting additional experiments using LLMs for other natural language tasks, beyond text summarization and machine translation studied in the paper, would provide a wider test case set and bolster the claims regarding the output quality preservation of the watermarking techniques.  

4. The paper does not discuss the resilience of the proposed watermarking methods against potential adversarial attacks or interference attempts. 


Limitations:
See the weakness section.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper proposes a modification of the watermark of Kirchenbauer et al. that ensures each next token prediction is marginally indistinguishable from a regular sample from the language model (whereas Kirchenbauer et al. bias some tokens over others). The main idea is to use inverse transform sampling to sample the text token (the paper also proposes a ""soft"" version of inverse transform sampling, i.e., \gamma-reweight), where the inputs to the sampler are a function of the context.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
The paper proposes a neat, simple modification of the watermark of Kirchenbauer et al. that addresses a salient problem (i.e., the watermark of Kirchenbauer et al. does not preserve the original text distribution). The paper validates the proposed watermark with both theory and experiments.

Weaknesses:
The empirical validation of the watermarked proposed in the paper is somewhat lacking. For example, how robust is the watermark to paraphrasing compared to the watermark of Kirchenbauer et al.? Given Kirchenbauer et al. evaluate robustness to various kinds of paraphrasing attacks, it seems reasonable to expect the paper to do the same. It's also not clear what the purpose of Table 1 and Figure 3 is if the main claim is that the watermark *provably* does not bias the text distribution (i.e., shouldn't all the metrics stay the same?). Also, Section 4.1 seems needlessly abstract, since ultimately both watermarks are variations of inverse transform sampling (it feels unintuitive to think \delta-reweighting as a ""reweighting"" of a distribution, since it is essentially deterministic).

Limitations:
The paper does not discuss limitations (if any) of the proposed methods in meaningful detail. The Conclusion section would benefit from more detailed/concrete discussion and takeaways.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper discuss about the important problem of how to watermark the outputs from language models while keeping the model not impacted by watermarking. A perfect watermark scheme should be undetectable without prior information and should have no harm on the utilities of LLMs. This paper proposes some desired properties of watermark schemes such as **n-shot-undetectable** and **downstream-invariant**. The papers also gives the proof that there exists perfect watermark schemes. And guided by the proposed concepts, two reweighing watermark scheme are proposed, as well as corresponding methods for verification. The experiment results show the proposed methods have minor impact on the generated text quality. 


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. It's super important to have a theoretical framework to guide researchers to design better watermark schemes, and this paper is one of the pioneers in this direction. 
2. Considering those automatic metrics, the results shown in the paper, the proposed two methods look good.

Weaknesses:
Major concerns: 
1: The quality of the watermarked texts are only evaluated by automatic metrics. 
2: The results only comes from BART ( along with examples from OPT ).  

Minor concerns:
1: The word 'unbiased' is a little misleading.
2: It's a little hard to understand some paragraphs in this paper. 

Limitations:
Please read last section.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper introduces a novel framework for embedding watermarks into Large Language Models (LLMs) without compromising their output quality. The proposed watermark is designed to be undetectable by LLM users. 

A general framework is put forth for incorporating this unbiased watermark into LLMs. This is achieved using two innovative and practical watermarking techniques: $\delta$-rewrite and $\gamma$-reweight.

Experiments conducted on summarization and machine translation tasks demonstrate that these watermarking techniques do not degrade the LLM's output quality, thereby substantiating the effectiveness and practicality of the proposed framework.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
This paper tackles a critical issue in the Large Language Models (LLMs) field, which is the misuse of LLMs. The authors propose an unbiased watermark and a novel framework for its implementation. The experimental results clearly prove that this watermarking technique works effectively.

The paper's layout is good and easy to follow. It uses clear formulas and theorems that help explain both the problem and the proposed solution. Overall, this is a solid piece of work that contributes significantly to the field.

Weaknesses:
This paper does not provide a comparison with any existing watermark baselines. Such comparisons would be beneficial to demonstrate the relative performance of non-unbiased watermark techniques.

Only two types of downstream tasks have been evaluated in the study, which limits the generalizability of the findings. It would enhance the robustness of the results if a broader range of tasks, perhaps using a comprehensive benchmark, were tested.

Additionally, the evaluation of model output quality relies solely on automatic metrics. However, these metrics alone may not be sufficient to provide a comprehensive assessment of output quality. Including more diverse and possibly human-centric evaluation measures or LLM auto evaluator could strengthen the evaluation process.

Limitations:
N/A

Rating:
6

Confidence:
2

";0
ir6WWkFR80;"REVIEW 
Summary:
The paper proposes a novel mode of textual attack, punctuation-level attack, which aims to fool text models by conducting punctuation-level perturbations, including insertion, displacement, deletion, and replacement. This paper also introduces Text Position Punctuation Embedding (TPPE) as an embedding method and Text Position Punctuation Embedding and Paraphrase (TPPEP) as a search method to reduce the search cost and time complexity of determining optimal positions and punctuation types for the attack. The paper demonstrates the effectiveness of the punctuation-level attack and the proposed methods on various tasks such as summarization, semantic-similarity scoring, and text-to-image tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed punctuation-level attack expands the scope of adversarial textual attacks. By focusing on punctuation-level perturbations, the authors provide an approach to fooling text models while minimizing its impact on human perception.
2. The proposed methods, TPPE and TPPEP, not only enhance efficiency but also reduce computational costs. Additionally, the authors present a comprehensive mathematical analysis of these approaches.
3. The effectiveness and versatility of the proposed punctuation-level attack are demonstrated by the experimental results on various datasets and state-of-the-art (SOTA) models.


Weaknesses:
1. Whether LLMs can also be fooled, which should be discussed in the paper

I guess such robustness is due to the amount of training data is not large enough. It is curious that LLMs like ChatGPT will still fall into such a deficiency, since LLMs are trained on huge data.

2. Why PLMs fail on punctuations is not discussed

The punctuation-level attack does not surprise me a lot. The most interesting problem is to probe into the reason why PLMs can be fooled by punctuations.
Unfortunately, this is not in the paper, which large limits the contribution of the work.

3. The defense is not discussed

The authors do not provide the study on how to defense the punctuation-level attack. The contribution on how to enhance language models e.g. DeBERTa to be robust against punctuation attacks is much more significant than how to attack. To me, this is very important for the community to improve real-world systems against underlying attackers.

4. The attack success rate is only promising on CoLA, while limited on the other datasets.
However, CoLA is not suitable dataset for evaluation.

CoLA requires the model to decide whether the given sentence is linguistically correct. The manipulation of punctuations can spoil the label of the original sentence.

5. Punctuation modifications can change the original label on some task

Punctuation-level attack is safer than word-level modification. It still can change the original label on some task, e.g. CoLA. Even in extreme cases, punctuations can change the entire semantics. It is better for the authors to discuss this part in the paper, e.g. some bad cases. This is also important for future research.


Limitations:
see the weakness part

Rating:
4

Confidence:
5

REVIEW 
Summary:
By proposing a new type of adversarial attacks, i.e., the punctuation-level attacks, This paper can fool text models with less impact on the semantic information by human beings understanding. Its effectiveness is verified by experimental results on various datasets/tasks and victim models. What’s more, the attack method is accelerated by the proposed Text Position Punctuation Embedding and Paraphrase (TPPEP) approach, so that the attack can be accomplished with constant time cost. The efficiency has been demonstrated by the experimental studies. 


Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
+ This paper is well written and easy to follow.

+ This reviewer appreciates the novelty of the proposed punctuation-level attacks, and the motivation behind, i.e., attacking text models with minimal perceptual influence on human eyes. It indeed brings some insights.

+ Besides its effectiveness in fooling various textual models, the authors also propose a TPPEP method to accelerate the attacking procedure. I believe this can significantly improve the practical use of the punctuation-level attacks.

+ The attack results on the update-to-date Stable Diffusion model are quite interesting.


Weaknesses:
- Though the proposal of punctuation-level attacks is indeed well motivated, an obvious major concern is raised that only three tasks (thus three types of victim models) are selected for attack effectiveness evaluation in the experimental studies. It would be better if more methods are selected for evaluation. The proposed approach would be fully validated and the conclusion would be more convincing.

- The current experimental results fail to provide more insights on how the method work in different scenarios. In fact, as a new type of text-attack method, there should be more in-depth analyses and explanations to quantitatively or qualitatively show the effectiveness.

- The notations in the paper are not always consistent. And some parts are not well illustrated, especially in Sec. 3.3. It is suggested to be re-organized. 


Limitations:
The current discussion about the limitations is practical but not so comprehensive. For example, the resource required by the method is not discussed. 


Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper introduces a new approach to textual attacks called the punctuation-level attack. The method aims to fool text models while minimizing its impact on human perception and understanding. The paper discusses the effectiveness of this attack strategy, presents a search method to optimize its deployment, and provides experimental results showcasing its success. The authors also apply the single punctuation attack to summarization, semantic-similarity-scoring, and text-to-image tasks, achieving encouraging results. The paper concludes that the punctuation-level attack is more imperceptible to human beings and has less semantic impact compared to traditional character-/word-/sentence-level perturbations. The integrated Text Position Punctuation Embedding (TPPE) allows the punctuation attack to be applied at a constant cost of time. The experimental results on public datasets and state-of-the-art models demonstrate the effectiveness of the punctuation attack and the proposed TPPE. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper introduces a new approach to textual attacks called the punctuation-level attack, which is different from traditional character-/word-/sentence-level perturbations.
2. The punctuation-level attack is designed to be more imperceptible to human beings and has less semantic impact compared to traditional perturbations.
3. The paper presents a search method called Text Position Punctuation Embedding (TPPE) to optimize the deployment of the punctuation-level attack.
4. The paper provides experimental results showcasing the effectiveness of the punctuation-level attack and the proposed TPPE on public datasets and state-of-the-art models.



Weaknesses:
The adversarial attacks discussed in this paper can be categorized as non-pure white-box attacks, as the attack objective may differ from the model's evaluation metric. It is crucial to explicitly acknowledge this fact in the paper, as it is widely recognized that achieving white-box robustness represents an upper bound and is significantly more challenging than black-box robustness.

It appears that the number of attack iterations is restricted. To ensure robustness evaluation, it is advisable to ensure attack convergence by employing an adequate number of iterations.

In the experimental section, when comparing the proposed method with other approaches, a fixed number of updating steps is consistently utilized.



Limitations:
The main emphasis of the paper is placed on evaluating the punctuation-level attack's efficacy within specific tasks, including summarization, semantic-similarity-scoring, and text-to-image tasks. However, the evaluation of this attack is not comprehensive across a diverse set of NLP tasks, limiting the extent to which the findings can be generalized.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper introduces an adversarial attack against NLP models based on punctuation perturbations. The authors introduce an attack called Text Position Punctuation Embedding (TPPE) that comprises an insertion, displacement, deletion, and replacement attack based on textual punctuation (e.g., commas or periods).
Experiments are conducted on various datasets, ranging from text classification (CoLA) to paraphrasing (QQP) and natural language inference (WANLI). The attacks are applied to ELECTRA, XLMR, and BERT-based models (DistilBERT, RoBERTa, DeBERTa). Additionally, the attack is applied to semantic-similarity-scoring (STS12), summarization (gigaword), and text-to-image tasks (prompting Stable Diffusion V2). Experimental results are promising, showing that the attack can be used to successfully attack models for the above tasks.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
* The paper provides an extensive analysis of punctuation-level attacks against NLP models. These attacks are promising since they have the potential to be less perceptible as compared to existing character-, word-, and sentence-level attacks.
* The analysis is extensive in that multiple NLP tasks (classification, summarization, text-to-image, etc.) are analyzed.
* It is interesting to see that the investigated models are vulnerable to punctuation-level attacks across tasks and domains.


Weaknesses:
* The paper does not compare TPPE against existing works and baselines. In Section 2, the authors point out various existing works focusing on punctuation attacks. However, none of these works have been evaluated and compared against in their experimental settings. To identify and support the strengths and utility of TPPE, such experiments are essential.
* I additionally think that comparisons to character-, word, and sentence-level attacks on the selected datasets would have been insightful since these experiments would provide the reader with a better understanding of how punctuation-level attacks perform in comparison to attacks focusing on other parts of a textual sequence.
* The paper does not further analyze the semantics of perturbed adversarial examples. To support the claims of semantic imperceptibility, human experiments analyzing the change in semantics between an original sequence and its adversarial counterpart would be important. The examples in Figure 2 nicely illustrate that inserting single punctuation marks can substantially impact the meaning of a sequence. Since adversarial examples are desired to preserve the semantics of an attacked sequence, quantitative experiments would be needed to evaluate TPPE in that context.
* The paper does not discuss potential approaches to mitigate the models’ vulnerability against punctuation-level attacks, for instance by assessing whether adversarial training / data augmentation (i.e., training the model on adversarially perturbed sequences) can help increase the robustness of the attacked models. This would provide additional insights into how robust the attack is, and how it can potentially be defended against.
* The results for the text-to-image task consist only of two qualitative examples. These examples are highly interesting, but to better evaluate the vulnerability of Stable Diffusion models against such attacks, quantitative results over a larger dataset would be important. 
* Overall, the paper focuses too much on introducing the attack and discussing its details and time-complexity analysis, instead of extensively evaluating its performance (the Experiments Section spans under 2 pages in the manuscript).


Limitations:
The paper briefly discusses Limitations in Section 5. However, potential ethical considerations arising from this research remain unaddressed. Since discussing these is quite important in this context (the proposed attack can be misused for malicious purposes), I would encourage the authors to add a section for this.

Rating:
5

Confidence:
3

";1
K3BMejPSyQ;"REVIEW 
Summary:
The performative prediction problem targets real-world application using deployed models, and this method  studies the optimiazation method for such a scenario. Since directly optimiazing the non-convex objective is challenging, this paper utilizes weighted Markovian samples of states to estimate the gradient and update with designed timescale step size.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is in general well-written.
2. The motivation is clear for tackling the targeted problem.
3. The theoretical derivation and understanding are well-presented.
4. The authors conduct various experiments settings to examine the the proposed method.

Weaknesses:
1. The experiments are conducted on toy examples and illustrate the efficacy of the method. The question would arise for applying the method for more challenging task setting. Also, how does the method perform in comparison with other strategies that also fit with such tasks?

2. What the limitation of the proposed method? As it targets online deployed models and involves the Markov Chain sampling, what is the computational cost?

Limitations:
Please see the weaknesses and questions part.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper proposes a derivative-free two-time scale  for performative prediction problem. The two-timescale facilitates a faster accumulation of samples to compute a gradient with smaller bias. For smooth nonconvex objective, the work proves a iteration complexity of $O(\epsilon^{-3})$ for the $\epsilon$-staitonary point. They illustrate results via numerical experiments.

Soundness:
4

Presentation:
2

Contribution:
1

Strengths:
1. This paper deals with an important problem: derivative-free optimization of nonconvex function under performative prediction setup. 

2. The convergence rate seems reasonable. 

3. The experiments are well-motivated.

Weaknesses:
1. **The applications are shown for squared loss. But it is not a bounded loss and does not satisfy Assumption 3.2. This leads me to believe that Assumption 3.2 is made for the convenience of theoretical analysis.**

In fact, **one of the major challenges in the decision-dependent noise or state-dependent Markov noise setup is that the algorithm is often not stable** for unconstrained optimization (see [1,2] below). Assumption 3.2 helps to avoid this issue.

**So Assumption 3.2 is crucial for Lemma E.3 and Lemma B.i (i=1,2,3,4). Then the theoretical analysis of the algorithm does not apply to squared loss which is one of the most popular loss functions.**

2. The proof techniques required are pretty similar to Wu et al., 2020 ([3] below). In fact, **the main result is almost same as Corollary 4.9 of Wu et al., 2020. It's just that Corollary 4.9 of Wu et al., 2020 is wrapped in the cover of reinforcement learning but the algorithm, underlying proof techniques, and result are same.**  Wu et al., 2020 does achieve a faster rate of $\tilde{O}(\epsilon^{-2.5})$ although I guess here the poor rate is due to derivative-free algorithm. 


[1] Liang, Faming. ""Trajectory averaging for stochastic approximation MCMC algorithms."" (2010): 2823-2856.

[2] Andrieu, Christophe, Éric Moulines, and Pierre Priouret. ""Stability of stochastic approximation under verifiable conditions."" SIAM Journal on control and optimization 44, no. 1 (2005): 283-312.

[3] Wu, Yue Frank, Weitong Zhang, Pan Xu, and Quanquan Gu. ""A finite-time analysis of two time-scale actor-critic methods."" Advances in Neural Information Processing Systems 33 (2020): 17617-17628.

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper studies performative prediction when the data is stateful, in particular generated via a controlled Markov chain, and the learner's loss is possibly nonconvex. The paper develops a two-timescale derivative-free optimization algorithm for this setting and shows a O(1/eps^3) sample complexity for finding a point with squared gradient norm at most eps.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
There aren't many convergence results in performative prediction for nonconvex settings, so this is one strength. The stateful setting has been studied before in several papers, but this paper studies this setting under quite a bit of generality. The treatment of the stateful setting in this paper is probably my favorite treatment of the setting in the literature. The paper is very clearly written and easy to follow, also providing intuition for the ideas behind the analysis, which I appreciated.

Weaknesses:
There are some limitations to the conceptual novelty, in the sense that a similar algorithm has been studied outside of performative prediction, and the controlled Markov chain model for the distribution map has been studied. It would be good to be more precise about the differences to the recent works in performative prediction studying the stateful setting (bottom of page 2).

The problem is perhaps arguably a bit niche since it goes away if the performativity is not stateful, which is the most commonly studied observation model in performative prediction. In that case, a simple application of the Flaxman et al. ""gradient descent without a gradient"" algorithm suffices. The issue in this paper is that we can't sample from the stationary distribution directly so a naive application of the Flaxman et al. algorithm doesn't work. All this being said, the stateful setting is very well-motivated and deserves its own analyses, and this paper gives a clever solution.

Limitations:
NA

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper is on performative prediction i.e. a setting where the data distribution changes in response to the predictions of a learned model. The prototypical example of such a setting is where a learned model is used to make loan decisions, and then people or companies adjust their behavior based on knowledge of the learned model in order to secure more favorable loan terms. This paper extends prior work on performative prediction to the setting where the data distribution follows a controlled Markov process, where the control is given by the predictive model. While this setting has been studied before, this paper removes previous structural assumptions on (1) the loss function used to evaluate the model and (2) the dependence of the data distribution on model. Notably the loss function is not assumed to be convex, and so convergence is established to a stationary point (i.e. a point where the magnitude of the gradient is small).
The algorithm designed is based on standard methods for derivative-free optimization, with a two-timescale step-size modification that updates model parameters more slowly than the generation of gradient estimates in order to deal with high variance in the gradient estimator.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
The extension of algorithms for performative prediction to the setting without convexity of the loss expands the range of techniques for such problems. The paper clearly discusses the differences with prior work and gives a concise and intuitive overview of the modifications required to make derivative-free optimization work for this setting.

Weaknesses:
While I do appreciate the generality of extending algorithms for performative prediction beyond convex losses (as well as beyond the mixture dominance assumption on the data distribution), there are a couple of limitations that arise from assuming so little about the loss and data distribution.

1. The algorithm converges to a stationary point of the performative risk, rather than to a point that approximately minimizes the preformative risk. Of course this is necessary in the completely unstructured setting considered here, but it is clearly a weaker statement than e.g. the initial work of Perdomo et. al. that showed (for strongly convex losses) convergence of repeated risk minimization to a point that is close to an actual minimizer of the performative risk. This paper should state more clearly that such a strong result cannot be hoped for in the setting considered. In particular, the claim on line 43 seems somewhat misleading as written.

2. Removing the convexity assumption seems to come at the cost of introducing an additional assumption. In particular, Assumption 3.3 requires that the distribution map $\Pi_{\theta}$ is Lipschitz with respect to the total variation distance, rather than the Wasserstein 1 distance used in prior work. This is a **much stronger** assumption in many natural settings e.g. the mean of $n$-independent random $\pm 1$ valued variables has a distribution that converges in Wasserstein 1 distance to the Gaussian distribution at a rate of $\frac{1}{\sqrt{n}}$, but has the maximum possible total variation distance of 1 from the standard Gaussian distribution.

3. The algorithm has a slower convergence rate than those in prior work, as the authors show is necessary in this highly general setting. The original paper introducing performative prediction shows that repeated risk minimization converges at a linear rate, and thus allows us to leverage whatever fast optimization method fits the particular problem for each risk minimization step. In contrast, the algorithm in this paper requires us to essentially sample many, many random perturbations of the model and then deploy/evaluate each one in order to obtain gradient estimates, making it highly impractical for real-world problems.

To summarize, the generality of the setting has several drawbacks in terms of the solution quality, performance of the algorithm, and most notably the additional assumption described in (2) above. While some of these drawbacks are provably necessary (again with the notable exception of the issue in (2)), taken together they suggest that perhaps this complete lack of structure is not a particularly good model of the types of problems for which performative prediction is interesting.


Limitations:
Yes.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper considers the performative prediction problem, where the goal of learner is to optimize the expectation of the known loss function over a decision-dependent unknown data distribution that evolves according to an underlying controlled Markov chain. Authors presents a stochastic derivative-free optimization algorithm $DFO(\lambda)$ that achieves $O(d^2/\varepsilon^3)$ sample complexity using gradient accumulation mechanism and two-timescale diminishing step-sizes.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The presented algorithm achieves a $O(1/\varepsilon^3)$ sample complexity under a challenging setting of Markovian data.
- The other possible types of gradient estimator were considered that creates a more complete picture in the presented setup.
- The presentation of the paper is clear.

Weaknesses:
- No presented lower bound for this problem, thus it is not clear is the presented rates improbable or not.

Limitations:
This is a theoretical paper that does not need to address the potential societal impact.


Rating:
6

Confidence:
2

";0
9HJyRsgU13;"REVIEW 
Summary:
This proposes a novel surrogate for Bayesian optimization (BO), kernelized tensor factorization (BKTF). The authors claim that BKTF is able to model more complex functions (nonstationary, nonseparable) compared to additive and product kernel Gaussian processes. For inference, they leverage Gibbs sampling to do full Bayesian inference. They compare against BO with the regular Gaussian process surrogate and tree-structured Parzen estimators.

Soundness:
1

Presentation:
2

Contribution:
3

Strengths:
* The papers proposes a novel surrogate model, BKTF, for BO. I believe this is new, and as long as the authors can demonstrate the utility of BKTF, this will be a valuable contribution to the BO community.

Weaknesses:
* The proposed strategy uses Gibbs sampling, which is well known scale poorly with the number of parameters and correlations. Thus, I am concerned that this method's performance will fair poorly at even moderately higher dimensions and number of observations than those considered in this paper.
* On a similar note, unless I'm not mistaken, the method requires to infer the latent functions (or bases) $g_d^D$. This means inference needs to be performed at every BO steps. This contrast to GPs, where, even if one decides to do fully Bayesian inference, he does not to run MCMC at every step. Thus, the method comes with a reduction in flexibility. If the authors believe that their method can work with less expensive inference strategies, say, VI or MAP, then this should be demonstrated and evaluated.
* The paper claims that the experiments are ""extensive"" (line 73), but unfortunately, I find that the experiments conducted in this paper cannot be considered to be extensive in today's standards. See for example [1,2], which I would consider extensive. Furthermore, at this small scale / low budget applications, noise can very easily swamp the effects. Therefore, I would expect a lot more runs. Moreover, the hyperparameter tuning experiments in Section 5.2 are not reflective of real-world use cases. So these are gain inadequate to evaluate the real world performance of BKTF.
* Furthermore, the baselines are not enough. The research space for alternatives to BO surrogates has certainly been active, but here only the tree-structured Parzen estimator is considered. In fact, the paper mentions that BKTF here corresponds to a two-layer deep GP. Then, they should compare against deep GPs for an apple-to-apple comparison. The computational costs/scalability of DGPs would probably be comparable so this would be a more appropriate comparison.

Limitations:
Yes, in Section 6. However, I think the limitations I've discussed above could also be included.

Rating:
4

Confidence:
4

REVIEW 
Summary:
Bayesian optimisation most commonly uses Gaussian Processes with the Squared exponential or Matern kernel as the surrogate model. The authors propose a new type of surrogate model, ""Bayesian Kernelized Tensor Factorization"" which introduces some advantages and disadvantages over Gaussian Processes. There seems to be prior work investigating these models for surrogate modding in general, and this paper is a followup applying these models within Bayesian optimisation in particular.

The papers introduces the model which models the data $\{x_i, y_i}$ from a black box function $y=f(x)$ as a sum of functions where each function is a product of 1 dimensional GPs, e.g. in 2D, leteach $g()$ be a 1D GP then 
$$
\hat{f}(x_1, x_2) = \sum_i g^i_1(x_1)g^i_2(x_2)
$$
which is a continuous analogue of how a matrix can be represented by it's SVD or eigen decomposition. This concept generalizes for multiple input dimensions (e.g. $g^i_1(x_1)g^i_2(x_2)g^i_3(x_3)g^i_4(x_4).....$) and the authors discretize the search space into a grid hence the implementation uses tensors.
 and it positive properties,
- to be able to model function with separability (where variables do not interact like in additive kernels) and
- non-stationarity.

In my interpretation, the thesis of the paper is that these properties are significant disdvantes and using a model that has these properties enables a performance improvement.

The disadvantage of the proposed model is that inference is no longer in closed form (a product of Gaussian random variables is not another Gaussian) hence an MCMC method is proposed to sample function values at points across the input space. While one could use a random discretization, (e.g. a latin hypercube, or a cluster around the current best point)  given the product structure of the surrogate model, there appear to be implementation benefits using tensor and matrix Kronecker products if the discretization is a fixed grid, discretize each dimension and build a a Cartesian product of each dimension to have a full grid.

A range of synthetic and hyperparameter tuning benchmarks show the new model performing favourably with standard GP using SE-ARD kernel.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- in theory, I really like the idea of the model, in particular, that any matrix can be decomposed by SVD, hints that any function can be decomposed into a sum and product over functions of each dimension, i.e. the proposed model is, in theory, a universal approximator? Although intuitively, both BKTF and SE-ARD can model any smooth non-stationary surface, discontinuities and kinks are not modellable.

- the inclusion of grid based GP methods is nice to see, and shows how much the grid decays performance compared to using the full unrestricted continuous space

Weaknesses:
# Technical
- the proposed Cartesian discretization, $S_D$, scales exponentially with input dimension, and presumably contains _a lot_ of useless points in empty parts of the search space would a random discretization (LHC or Gaussian around current best $x$) be so much worse? Given a random set of points $X_D$, it is trivial to compute a the joint prior density $P[f(X_D)]$ density and the likelihood is just Gaussian $P[y_i|f(x_i)]$, sampling function values can be done with any off-the-shelf MCMC method.

- I believe at least an additive GP should be a baseline. If non-stationary and non-separability are the main advantages of the BKTF model, presumably an additive GP with 2 kernels per dimension (matching CP rank=2 for BKTF) is an obvious baseline that has separability, such a baseline is exactly equation (7) but with sum-sum instead of sum-product. From this perspective, BKTF is simply an additive GP (that can only model separable variables) with a product over dimensions instead of a sum and this one change introduces a lot of engineering overhead (MCMC inference vs closed form inference) but also introduces more modelling power (separability can be modelled), given a high enough CP rank (CP rank =1 is just a product of 1D funs and is not separable).


- the related work consists of two paragraphs, the first is discusses prior work on  BKTF (and feels a bit repetitive), the second focuses on stochastic process models. I feel the novelty of this paper is in using another surrogate model inside BO methods, and given the large body of BO work, there have been many works acknowledginbg the limitations of SE-ARD and proposing alternative models that are not cited or empirically compared to
  - [Bayesian Neural networks](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=bayesian+optimization+neural+networks&btnG=): 
    - Bayesian optimization with robust Bayesian neural networks, NeurIPS 2016
    - Scalable bayesian optimization using deep neural networks, ICML 2015
    - Multi-fidelity Bayesian optimization via deep neural networks: NeurIPS 2020
  - [Deep GPs](https://scholar.google.com/scholar?hl=en&as_sdt=0%2C5&q=bayesian+optimization+deep+gaussian+&btnG=)
    - Bayesian optimization using deep Gaussian processes, Arxiv

# Presentation

(in my personal subjective view) some changes to presentation would have made the paper far more accessible to me
- can ""CANDECOMP/PARAFAC"" be simply described as a tensor generalization of SVD to make it easier for readers?
- L119: ""we construct a D-dimensinoal Cartesian product Space"", can we just say ""grid"" like the authors do for the rest of the paper?
- L81: kronecker product is introduced and never used again in the main paper
- Section 3.1 would be much easier for me to understand if Eq (7) and (8) are introduced first, then next Section 3.3 (model inference) describes the grid and Equation (6) and MCMC details and the justification for the grid.
- Section 3.2 is nice to mention but for me distracts from the main paper hence would be much better suited to the appendix.
- L192: given a mean and uncertainty, this seems to be standard UCB, why is ""Bayesian-UCB"" defined?

Limitations:
- as above mentioned comment, discretizations in higher dimensions are generally considered bad practice, in particular, ungioded/naive grid discretizations that include many dead points.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper presents a new surrogate model called Bayesian Kernelized Tensor Factorization (BKTF) for Bayesian Optimization (BO).The BKTF model approximates the solid in the D-dimensional space using a fully Bayesian low-rank tensor CP decomposition. It uses Gaussian process (GP) priors on the latent basis functions for each dimension to capture local consistency and smoothness. This formulation allows sharing of information not only among neighboring samples but also across dimensions. The paper proposes using Markov chain Monte Carlo (MCMC) to efficiently approximate the posterior distribution. ). The paper demonstrates the effectiveness of BKTF through numerical experiments on standard BO test functions and machine learning hyperparameter tuning problems. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. One of the significant strengths of the paper is the novel and reasonable solution of incorporating the idea of tensor decomposition into Bayesian Optimization (BO). This approach allows for a more efficient and effective representation of the D-dimensional Cartesian product space, enhancing the performance of BO. The adoption of tensor decomposition represents a significant advancement in the field and demonstrates the authors' innovative thinking.

2. The usage of two-layer GPs is impressive. This approach is clever as it allows for the sharing of information among neighboring samples and across dimensions, enhancing the model's ability to capture local consistency and smoothness.  



Weaknesses:
1. The cost of several cascaded full GPs may be high, especially for cases with large nodes (refer to ""coordinate"" in paper) at some dimension. More discussions are encouraged on the scalability analysis or the possible solutions, such as sparse GP,  to reduce the cost. 
 
2. As the tensor rank R  is always a crucial hyperparameter for tensor decomposition. I'm curious about how the rank setting could influence the BO. It will be great if the authors could give some comments or results on why R=2 is sufficient for the model setting.   

Limitations:
See Weakness

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper presents a surrogate based on tensor decompositions for approximating complex functions, allowing for Bayesian-style maximization.
Numerical experiments show the (slight) superiority of this model over classical Bayesian approaches. However, a limitation of this approach is the small dimensionality of the target functions and the need to use a discrete grid.



Soundness:
3

Presentation:
3

Contribution:
3

Strengths:

- The proposed algorithm uses a very small budget to find the maximum of complex functions (gradient-free, multimodal).

- A good potential for expanding and improving the proposed algorithm.

- This article uses a tensor approach for machine learning problems

- The presented new algorithm, in my opinion, has quite a lot of possibilities for improvement, and the article itself is complete.

Weaknesses:
- A small number of numerical examples.

- Final accuracy in Fig. 3b better, but very close to the accuracy of the other methods with which the comparison is made.

- No comparison with non-Bayesian methods of finding the maximum.

Limitations:
Small dimension of functions for which the maximum is searched for. This is due to the fact that AF has to be found by unrolling the tensor from CP to the full format.
Thus, one of the main advantages of the CP tensor format, related to overcoming the curse of dimensionality, is not used.




Rating:
7

Confidence:
4

";0
CWdxHxVAGG;"REVIEW 
Summary:
The authors consider 3 rejection models in the OOD detection setting and establish theorems about the optimal rules for these models. Due to the commonly used metrics that may partially only focus on either OOD detection or misclassification, they proposed a double-score method to consider both aspects.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* The authors extend the rejection in the non-OOD setting to the OOD detection setting.
* The 3 different rejection models share similar optimal strategies, and those strategies outperform other baselines.
* Since previous works only focus on either OOD detection or misclassification, the authors proposed a novel double-score method to consider both aspects. 


Weaknesses:
* Now that we know all the distributions, including OOD and ID, why put effort into eq 3? Can’t we just treat augmented data of OOD + ID as the new collection with $K + 1$ ""inlier"" classes, then follow the regular rejection analysis under the closed-world assumption?

* The optimal rules heavily rely on the clear information (the distribution in the theorem or estimated from the sample) on OOD data. For example, the prior $\pi$ in the bounded precision-recall rejection model, the conditional probability (of OOD) in the bounded TPR-FPR rejection model, or even both in the cost-based model. However, it is challenging to access OOD data and estimate these probabilities in real-world applications within an open-world setting. This raises concerns about the practical guidance provided by these optimal rules.

* I have a reservation about the constraints in equation 8 or the setup of problem 1. In practice, it is more likely the threshold or the selective function is determined by only one constraint (even in your proof of theorem 2, there is nowhere about the constraint $\rho(c)$). For example, once the $\lambda$ is determined by one constraint, it does not necessarily satisfy another one, which is similar to the trade-off between the type I and type II errors or Neyman-Pearson classification. A similar argument applies to Problem 2.

* Optimal rules: the authors claim the optimal rules related to the Bayes classifier (lines 126, 170), but from theorems, it holds under the condition that the given $(h, c)$ is optimal. If we cannot find the optimal $h$, can we still say the Bayes classifier is optimal?


Limitations:
See my concerns and questions above.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper provides a unified viewpoint of reject model evaluation metrics under OOD-ness. Despite different performance metrics being used in practice, they can all be factorized into both the misclassification prediction on ID datapoints as well as the discrimination performance between ID and OOD. The experimental section provides some evidence that this proposed two score decomposition can provide competitive OOD detection and input rejection performance.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- OOD detection and classifying with a reject option are both timely and important issues in trustworthy machine learning. ML algorithms need to be aware of their limitations and signal if they are deployed on previously unseen data.
- The unified framework / viewpoint makes sense and yields an interesting decomposition result showing that the optimal uncertainty score is a function of both misclassification prediction on ID datapoints as well as distinction performance between ID and OOD. Although this appears like an intuitive desiderata for a prediction uncertainty score, explicitly tying it back to the different rejection models seems new.

Weaknesses:
- The paper assumes that the OOD distribution is known. This is a strong assumption and severely limits the applicability of the approach in practical settings where knowing or explicitly estimating this distribution is often prohibitive.
- Even though the paper does provide a nice unification of three performance measures for reject option models in an OOD setup (cost-based rejection, TPR-FPR rejection, precision-recall rejection), these metrics are not new and routinely used in existing applications. To me it seems like this kind of unified viewpoint would be better suited for a survey-style paper.
- Section 3 could have been motivated better. To me, it appears a bit sudden without much justification or transition from the previous section.
- Section 3.1 could have significantly profited from a figure showcasing the 1D Gaussian example for added intuition.
- While the optimal uncertainty score is unified across error models, the experimental section showing the efficacy of the double score method is very lackluster. Details about models, training methods, and hyper-parameters area all missing (and also not documented in the appendix). Results are also based on a single run which makes it impossible to judge the statistical significance of the presented results. Moreover, the considered datasets do not all share the same sample shapes (e.g. MNIST vs CIFAR-10) which makes me wonder how OOD scores were obtained here in the first place. Real life shifts, like from the WILDS dataset collection would have been a better place to assess OOD-ness of samples. I am not convinced by this evaluation.

### Post-rebuttal

I have read the author's rebuttal and found that although many of my concerns were adequately addressed, the experimental documentation issue remained unaddressed. I increased my score only slightly as a result.

Limitations:
Weaknesses of the approach are not adequately discussed in the paper. See weaknesses and questions above.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper proposes three reject option models and introduces double-score OOD methods that consistently outperform state-of-the-art methods. The authors also propose novel evaluation metrics for comprehensive and reliable assessment of OOD methods. The proposed metrics simultaneously evaluate the classification performance on the accepted ID samples and guarantee the performance of the OOD/ID discriminator, either via constraints in TPR-FPR or Precision-Recall pair. The authors argue that setting these extra parameters is better than using the existing metrics that provide incomplete if used separately, or inconsistent, if used in combination, view of the evaluated methods.  Overall, the paper's contributions provide a significant improvement in OOD detection and evaluation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper proposes three reject option models for OOD setups, which extend the standard reject option models. These models define the notion of an optimal OOD selective classifier and establish that all the proposed models, despite their different formulations, share a common class of optimal strategies. This is an original and creative approach to the problem of OOD detection, and the proposed models are well-motivated and clearly explained. The paper introduces double-score OOD methods that leverage uncertainty scores from two chosen OOD detectors: one focused on OOD/ID discrimination and the other on misclassification detection. The paper proposes novel evaluation metrics derived from the definition of the optimal strategy under the proposed OOD rejection models. These metrics provide a comprehensive and reliable assessment of OOD methods. 

Overall, the paper is well-written and easy to follow, with clear explanations of the proposed models, methods, and evaluation metrics. 

Weaknesses:
One weakness of the paper in the experimental results is that the dataset used in the experiments is relatively small, and the proposed methods do not show significant advantages over the baseline. This raises questions about the generalizability of the proposed methods to larger and more diverse datasets. Additionally, the paper could benefit from a more detailed analysis of the limitations of the proposed methods and how they can be improved in future work.

Limitations:
The authors did not explicitly discuss the limitation of the proposed method.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper addresses out-of-distribution detection (ID/OOD discrimination) and misclassification detection (selection classification). The authors introduce double-score OOD methods that leverage uncertainty scores from OOD detector and misclassification detector. For evaluation metric, this paper proposes to use PR curve.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
(1) Existing works mainly focus on OOD detection, which this paper simultaneously considers OOD and selection classification. This is more practical and useful for high risk applications.

(2) Theoretical analysis of OOD detection and selective classification is valuable.

Weaknesses:
(1) The analysis of Bayes-optimal OOD selective classifier as well as the Bayes-optimal misclassification selective classifier has also been investigated in [1]. The authors are suggested to demonstrate the difference.

(2) The experiments are insufficient, only evaluating on cifar-10 and mnist. Results on CIFAR-100 is valuable.

(3) What is the advantage of PR curve over AURC (risk-coverage) curve [2] for evaluating rejection model?

[1] Narasimhan, H., Menon, A. K., Jitkrittum, W., & Kumar, S. (2023). Learning to reject meets OOD detection: Are all abstentions created equal?. arXiv preprint arXiv:2301.12386.

[2] Geifman, Y., & El-Yaniv, R. (2017). Selective classification for deep neural networks. Advances in neural information processing systems, 30.

Limitations:
n/a

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper presents a formal analysis of three distinct models for classifiers with a reject option in the presence of out-of-distribution inputs at test time. All three models, viz. 1) cost-based rejection, 2) bounded TPR-FPR, and 3) bounded precision-recall, share the same form of optimal selective classifier (see Section 2.4). 

This selective classifier consists of the Bayes in-distribution classifier $h_B(x)$ (which is optimal for a given in-distribution), and a selection function score that is a linear combination of the conditional risk $r(x)$ and the likelihood-ratio of OOD to ID $g(x)$. Based on this analysis, the authors point out the limitations of current OOD detection methods which only have a single score, and instead propose double-score OOD detection methods which can focus on both mis-classification detection and ID/OOD separation.

Using a concrete synthetic example, they discuss the limitations of existing metrics such as AUROC and AUPR in evaluating selective classifiers in the OOD setting. They propose a novel metric (one each for the bounded TPR-FPR and bounded precision-recall models) which calculates the selective risk, subject to a given minimum TPR and maximum FPR (or a given minimum recall and minimum precision). The proposed metric is shown to be better at capturing the overall performance of selective classifiers in the OOD setting.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. The proposed reject option models and evaluation metrics are developed systematically and connected well with summary sections. Found the paper to be well written. 

2. The analysis builds upon the prior work [6] which deals with reject option models but not in the OOD setting. In this work, they consider rejecting inputs due to both mis-classification and being OOD.

3. Although the proposed work makes a strong assumption of a known OOD distribution, the analysis is useful to draw insights about the need for a double-score OOD detection method, and also to highlight the shortcomings of existing evaluation metrics.

[6] https://www.jmlr.org/papers/volume24/21-0048/21-0048.pdf

Weaknesses:
1. The analysis and proposed new metrics (parts of the new metrics like the FPR and precision) depend upon knowledge of the OOD distribution. This is a strong assumption for practical settings. 

2. For the proposed double-score OODD method, it seems to me that we need access to OOD data in order to set the hyper-parameter $\mu$ in the combined score $s_r(x) + \mu s_g(x)$. The authors should clarify if this is set based on validation data from a different OOD distribution than the test data.

3. Minor: it is a bit tedious to keep track of all the notations needed for the analysis.

**Update after author-reviewer discussions:** \
I have read the authors rebuttal. The paper lacks sufficient discussion on the experiments. Not enough details are provided in the appendix as well. Hence, I decrease my rating to 6.

Limitations:
Some limitations of the proposed work are mentioned in Section 3.5. Another limitation to include is the fact that the proposed analysis and novel metrics depend on the OOD distribution which is usually unknown. One could use a validation set (as in the paper) with a mix of in-distribution and auxiliary OOD data, but the distribution of auxiliary OOD data in the validation and test sets should be different. 

Negative societal impacts has not been discussed, but may not be applicable here.

Rating:
6

Confidence:
3

";0
E8vGACczsQ;"REVIEW 
Summary:
The paper shows the existence of a phenomenon that the authors refer to as out-of-contect meta learning in large language models. The authors design experiments that show that this phenomenon causes the internalization of text that is broadly useful, meaning that the LLM is more likely to treat this content as true. The paper shows two forms of internalization, namely weak and strong internalization, the later being a form of meta learning. Two reasons are suggested for this phenomenon, one based on the parameters of the model, and another one relying on the implicit gradient alignment bias of gradient-based optimization methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The paper shows an interesting phenomenon 
* The proposed explanations are sound and intersting
* The paper is well written and easy to follow

Weaknesses:
* There is no conclusive explanation of the reasons why internalization happens
* The phenomenon is hard to formalize and study, which limits the advantage of the insights in the paper

Limitations:
The authors describe limitations in the paper

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work introduces the phenomenon of out-of-context meta-learning in large language models (LLMs). It demonstrates, through carefully designed experiments, that LLMs have the ability to internalize the semantic content of the text that appears to be from a reliable source. 
Specifically, they focus on exploring the existence of weak internalization and strong internalization in the context of LLMs and other vision models.
Potential explanations for the emergence of internalization are explored, including the way models store knowledge in their parameters and the implicit gradient alignment bias of gradient-descent-based methods. Finally, the implications of these findings for future AI systems are discussed, including the potential risks associated with internalization.



Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
1. Innovative Methodology: The paper introduces the phenomenon of out-of-context meta-learning in large language models (LLMs) and presents a series of carefully designed synthetic experiments to establish its existence. 
The methodology employed in these experiments is unique and provides valuable insights into how LLMs internalize and apply semantic content in different contexts. 

2. Comprehensive Experimental Design: The paper describes a series of synthetic experiments that evaluate the phenomenon of out-of-context meta-learning in LLMs from multiple perspectives. The experiments consider different variables and define tags and questions.


3. Implications and Risks: The paper discusses the implications of the findings for future AI systems and highlights potential risks associated with internalization. This analysis adds an important dimension to the paper, emphasizing the importance of understanding and mitigating the challenges posed by out-of-context meta-learning in LLMs.

4. Reproducibility: The paper provides detailed information about the experimental setup, including hyperparameters and performance evaluation metrics. 


Weaknesses:
1. This work is hard to penetrate. For example, the definition of statements involving two different define tags is not well-defined. Do the statements indicate 'definitions'? Furthermore, the authors' intention behind the phrase 'in every example in which it appears' is unclear.  Additionally, the explanation of weak internalization and strong internalization is confusing. By stating that ""LLMs will be likely to respond to questions as if the true statements from the training set are in fact true,"" do the authors imply that LLMs tend to generate correct answers when variables are defined with consistent define tags?
 

2. Confusing annotations. 

    - In section 2.1, the named entity is represented by a randomly generated 5-character string, whereas Figure 1 shows a 3-character string as the named entity replacement.

    - It would be helpful to use the example presented in Figure 1 for illustration purposes, as it could alleviate comprehension difficulties.

    - The definition of $X_2$ is introduced after its usage, which makes it difficult to understand.

3. The interpretation of experimental results is lacking clarity.

    - The description of 'in the same (inconsistent) definition' in Line 120 is ambiguous.

    - While the authors suggest that usefulness for predicting other datapoints is not the sole reason, they do not elaborate on the meaning of 'usefulness' or identify other contributing factors.

    - What conclusions can be drawn from comparing EM_{test}(QA_4) and EM_{test}(QA_3)? What is the purpose of the authors' explanation in Line 123-129?

    - How should internalization be understood in the context of 'resemblance to useful data'? 

    - Is pretraining necessary? In section 3.1, the authors only provide the experiment setups but fail to give a conclusion.

4. The title does not accurately reflect the content, since this work only focuses on LLMs and also explores such a phenomenon in computer vision models. 



Limitations:
I do not foresee any potential for negative societal impact from this work.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper argues for the existence of ‘out-of-context meta-learning’ as a characteristic of LLMs. The authors support this claim with cleverly designed experiments on QA using a 2.3B parameter pretrained Pythia model. They argue that the presented experiments demonstrate out-of-context meta-learning. They perform additional LLM experiments and a pair of simple toy experiments a synthetic language task and modified MNIST task. The authors discuss hypotheses which may explain the mechanism of their proposed phenomenon, and discuss its implications for the research community at large.

*after rebuttal, updated score from 4 (borderline reject) to 6 (weak accept)*

Soundness:
2

Presentation:
4

Contribution:
2

Strengths:
The paper is well structured, flows nicely, and is clearly written. The experiment setup is motivated to test a specific hypothesis, and is highly creative, and the experiments are thoroughly analyzed. Many additional experiments were run. The results have tight error bars and seem likely to be correct and reproducible. The analysis of the implications of the central claim of the paper touches broadly on the capabilities of LLMs and is highly relevant to the general research community, especially as pertains to safety. 

Weaknesses:
Issue with the central claim
---------
The central claim of the paper is that the model is sensitive to the appearance of authoritativeness / usefulness of specific examples, and incorporates that assessment into some decision as to how thoroughly to 'internalize' those example. 

*    “is, or appears to be, *broadly useful* (such as true statements, or text from *authoritative sources*)” line 7
*    [the model] “pick[s] up on features that indicate whether a given data point is *likely to help reduce the loss on other data points*, and “internalize” data more or less based on these features” lines 16-17
*    “Thus *usefulness for predicting other datapoints* is not the only reason why a definition might be internalized."" 121-122
*    “So after finetuning on X1, the neural net ends up at a point in the parameter space where gradient updates on consistent-seeming definitions result in more internalization than updates on inconsistent-seeming definitions. We consider this out-of-context meta-learning; it is as if the neural network “expects” the definitions with [blue,dotted]Define to be more useful for reducing the training loss in the future, and thus internalizes them more.” lines 137-143
*    “Our work investigates whether LLM training biases models towards internalizing information that *appears broadly useful*, even when doing so does not improve training performance” line 342
*    “learning can lead LLMs to update their predictions more/less when they encounter an example *whose features indicate it is reliable/unreliable*“ line 377

This is an extraordinary claim, as it supposes capacities of the model that are not immediately obvious in model behavior or in potential underlying mechanism (sec 4). The authors seek to demonstrate this behavior with the QA experiments in section 2. The experiments presented are thorough and interesting, but it is not clear to me that the results they show need to be interpreted as grandly as the authors do. It seems plausible that a simpler explanation may sufficiently explain the observed data without relying on imbuing the model with surprising new capacities.

Potential alternative explanation:
In Section 2, in Figure 2, the authors present the main evidence for their claims. For this argument, Let us suppose the 5-char sequence for the “inconsistent” tag ‘redDEFINEbar’ is “*YUIOP*”, and that the 5-char sequence for the ‘consistent’ tag ‘blueDEFINEdotted’ is “*GHJKL*”. This helps to ground these strings in how the model sees them as opposed to how they may be interpreted.

Incorporating a *GHJKL* sequence, as in QA1, gives the model the opportunity to recover from the loss of information in the entity-string masking (shown in the gap between QA4 baseline and QA3), but only through the medium of updating the parameters of the model themselves (as opposed to via the activations as in in-context learning). QA2 obfuscates further from QA3 by incorporating a *YUIOP* sequence which connects each entity-string to a random incorrect entity. In essence, the *GHJKL* sequences tell the model that the entity-string and entity in the sequence are identical. However, it is not clear to me that the *YUIOP* sequences should be interpreted as “inconsistent seeming [definitions]” (line 138). There is nothing that forces the model to view *YUIOP* as an indicator of identity and then to figure out that its an unreliable identity indicator (which would involve the kind of self-reflection capacities supposed in the claim of the authors). Is it not more parsimonious to say the *YUIOP* sequences are consistent markers of non-identity - simply non-sequitur statements which are true but generally useless? If it is always true that the entity-strings and entities in *YUIOP* sequences are inconsistent with the QA examples holding those entity-strings (‘perfectly correlated’ line 87), a reasonable pattern that the model may learn is “'*YUIOP* X Y' means that X!=Y”. This bizarre anti-definition is almost useless as Y could be anything other than X, and potentially confusing, which can account for the drop from QA3 (brown) to QA2 (pink) following similar reasoning as in lines 120-121. So far this is basically the same, but from this perspective, the “surprise” result of Figure 2, that D5 outperforms D6, is no longer surprising. It is not necessary to rely upon the suggestion that the model “pick[s] up on features that indicate whether a given data point is likely to help reduce the loss on other data points, and “internalize[s]” data more or less based on these features” lines 16-17. Is it not simpler to suggest that the model has learned correctly that *GHJKL* indicates identity and *YUIOP* indicates non-identity? In this case, the fact that non-identity is ‘internalized’ to a lesser degree is no surprise at all: non-identity is only loosely incorporated (or incorporate-able!) because it is a non-sequitur. The model can fail to 'internalize' this non-sequitur information on account of it's general irrelevance, without relying on an surprising capacity to learn conditional on an example's ""*usefulness for predicting other datapoints*"" (line 122). A similar explanation can be given if *YUIOP* is not understood to be non-identity at all, but just random noise with no consistent interpretation. The fact that the model 'internalizes' the *GHJKL* information more than that of the *YUIOP* can rely solely upon the fact that an interpretation of *GHJKL* is readily apparent and there is no obvious interpretation of *YUIOP*. This line of reasoning begs the question as to the loss curves of the specific *GHJKL* and *YUIOP* examples in QA1/QA2 over the course of the training. You might expect to see higher loss for the *YUIOP* examples. Note that this argument extends to the non-QA experiments presented as well.

What is strange in this perspective is not that D5 outperforms D6 but that D6 outperforms QA7! But this surprising result does not carry the significant implications of the previously surprising result highlighted by the authors. Regardless of how you explain the superior performance of D6 over QA7, it bears explaining why the above reasoning (which explains away the 'surprise' of the gap between D5 and D6 and which does not stipulate any particularly surprising characteristics on the behalf of the model) is confused. It seems a plausible enough explanation that without a convincing rebuttal the central claim of the paper, which makes an extraordinary claim of model behavior, seems shaky. 

There is no reason to presuppose that a *YUIOP* example would ever be interpreted as a definition by the model, despite it being labeled as such in the analysis of the paper. Without this presupposition, the claim that *YUIOP* represents an ""inconsistent-seeming definition"" to the model is unfounded, as is the subsequent claim that that the model “pick[s] up on features that indicate whether a given data point is *likely to help reduce the loss on other data points*, and “internalize[s]” data more or less based on these features”. The gap supposed to demonstrate 'strong internalization' can be explained as nothing more than the difference between the model comprehending a useful control sequence marking identity, *GHJKL*, and a sequence marking random noise *YUIOP*.

(Note: the above arguments may indeed be plausible but subtly misguided and ultimately wrong, but the authors must address them convincingly in order to strengthen the paper.)

Other
-----
Lines 79-82 discuss ‘information leakage’ where replaced entities may be inferrable based on information present in the QA pairs and background information present in the pretrained model, and states that steps have been taken to reduce this possibility. Presumably the performance of QA3 in Fig2 would vary significantly with this information leakage, where highly ‘leaked’ entities would still have good performance? (Ie. training on “Q: xyz was the first president of which country. A: the USA” should yield better performance on xyz related test Qs than a more obfuscated relation). Is this interpretation correct? If so it seems that the function of this information leakage and the specific means of alleviating it are actually very important to the interpretation of the results, and should perhaps be given more attention than being left to the appendices / alluded to in lines 124-127.

Internalization is not formally defined in any way, yet it is a central aspect of the paper. It is 'measured' only via aggregated loss on each dataset. More time should be spent investigating and developing the idea of internalization (how does the 'internalization' of a specific example relate the loss on that example?).

Nits
-----

*    The title of the paper comes from a contrast to ‘in-context’ learning, which is referenced many times in the paper, but the meaning of the term is not made explicit until the Related work (line 314). It would improve clarity to define what is meant by in-context learning and to describe how the proposed ‘out-of-context’ learning differs when introducing the concept of out-of-context learning (line 42).

*    Figure 1 bottom right has a typo: “Q: What did qwe born? A:” is presumably a mish-mash of two different questions? And not actually in the test set? It would not be surprising to get a bad answer to this question as stated!
*    It would be helpful to label the data presented in Figure 1 with the dataset names (QA3, QA4..) used in Section 2.3.
*    Figure 1 depicts two stages of finetuning on two separate datasets X1 and X2, and their subsequent eval/analysis, but this is a little obfuscated by the presentation. Consider making it more organizationally clear. Perhaps draw a bubble around the box on the left and the box on the top right to show they are the same stage, and add a Train label to the dataset in the left box to be consistent with the others. Some explicatory text can be moved to the caption to make the figure itself easier to cartoonify.
*    Line 47 his -> this
*    Fig2: consider showing “the entities consistent with the QA pairs; the latter get accuracy 0 everywhere” (Line 153) in the plot as well
*    Line 291 vise-> vice
*    Consider adding the number of (entity, entity-string) pairs present in the datasets of Fig2. It might be helpful to add a table with each of the datasets presented in Fig2, showing their characteristics and size / number of entity - string pairs. This would help clarity / readability. 


Limitations:
The mentioned limitations are well selected (formalization of internalization, absence of obvious mechanism). The rebuttal of alternative explanations for the presented results is absent from the paper, a significant additional limitation. The potential social impacts are discussed.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The authors assert that ""out of context metalearning makes LLMs better at internalizing useful information for understanding."" They intuitively frame understanding as ""treating content as true in question answering."" They analyze its application to tasks such as mapping novel phrases or words to attributes and then performing question answers.

They introduce ""define tags"" which perform the mapping of novel info rather than using the word ""define"" and natural language, which I like a lot as an approach. This allows them to isolate the effect of the metalearning as a mechanism for improving performance rather than being clouded by the existing notions of the meaning of ""define"" that may be acquired by the LLM during pretraining.

Though I have some gripes that verge on the political for the limitations section I think this is an interesting and well-motivated work that deserves acceptance.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
Detailed analysis and clear statement of technique

Weaknesses:
Yudkowsky citation. I think that stuff is fundamentally unserious and hurts my willingness to recommend strong accept or award as an actual NLP expert.

Limitations:
In my opinion, perpetuating AI safety hype in academic papers is inappropriate, and citing Yudkowsky in particular is a negative signal. Opinions of others will vary and ultimately I don't think this is a reason to reject. Just wanted to register my discontent.

Otherwise discussion of limitations is strong.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors introduce the phenomenon of internalization in LLMs, specifically ""weak internalization"" and ""strong internalization"" (out-of-context meta-leanring). Weak internalization refers to LLMs' improved performance on questions with consistent definitions rather than with inconsistent ones. Strong internalization involves LLMs' ability to provide better answers for variables with a defined tag representing a consistent definition, demonstrating out-of-context meta-learning. The paper includes ablations to support their findings and discusses the limitation of the work.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper presents an interesting case of ""internalization"" in LLMs. 
- Authors addressed the limitations of their work and also mention the lack of conclusive explanations for internalization in general.

Weaknesses:
Major Concerns.
- Not enough models are analyzed in ablations. The paper presents only the evaluation of Pythia and T5 family of models and claims that the internalization phenomenon is quite general. I would suggest performing experiments with more recent models such as LLaMa, T5Flan, etc.
- The number of datasets presented for evaluation is also quite small. Although, I understand that creating datasets for this specific format could be expensive.
- it is unclear if the size of a model affects the internalization phenomenon. Ablations of a few models varying their size would help to solve this doubt.
- Not clear how the phenomenon of internalization can be taken by the community in order to improve or avoid pitfalls regarding the development of LLMs

Minor comments:
- The paper mentions several times to look at the appendix but doesn't indicate to which section the reader should pay attention. I would suggest indicating the specific section in order to improve the readability of the manuscript.
- The notations of the datasets are quite difficult to follow, I think authors could provide a general overview (in a table or any other format) rather than explaining each component in line with the text. This would improve the readability of the paper.

Limitations:
Yes, the authors addressed the limitations of their work and also mention the lack of conclusive explanations for internalization in general.

Rating:
6

Confidence:
3

";0
DEqiM9CmmZ;"REVIEW 
Summary:
This paper introduces the ANQ (Approximate Nearest Neighbor Q-Learning) framework, which aims to provide explainability in reinforcement learning models. ANQ combines neural networks for high performance and memory-based structures for explainability, offering a promising solution for domains like autonomous driving, quantitative trading, and healthcare. The paper discusses the challenges of explainability in reinforcement learning and how ANQ addresses them. It also presents the Sim-Encoder contrastive learning used in ANQ's state representation and provides insights into the evaluations of ANQ on MuJoCo continuous control tasks and its effectiveness in solving continuous tasks. Overall, the paper presents a novel approach to reinforcement learning that balances performance and transparency, making it suitable for real-world applications. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Some strengths of this paper include:  

1. Novelty: The ANQ framework is a novel approach to reinforcement learning that combines neural networks and memory-based structures to provide explainability in decision-making processes. This approach is different from traditional reinforcement learning methods that focus solely on performance.  

2. Explainability: The paper addresses the challenge of explainability in reinforcement learning models, which is crucial for real-world applications where transparency and trustworthiness are essential. ANQ's ""data is policy"" design principle ensures that the model's decisions are explainable and interpretable.  

3. Sim-Encoder contrastive learning: The paper introduces the Sim-Encoder contrastive learning approach for state representation, which demonstrates its effectiveness in memory retrieval learning tasks. This approach enhances ANQ's performance and explainability.  

4. Evaluations: The paper provides insights into the evaluations of ANQ on MuJoCo continuous control tasks and its effectiveness in solving continuous tasks. The results show that ANQ outperforms traditional reinforcement learning methods while maintaining explainability.  

5. Real-world applications: The paper highlights the potential of ANQ in domains like autonomous driving, quantitative trading, and healthcare, making it suitable for real-world applications. 

Weaknesses:
Some potential weaknesses of this paper include:  

1. Despite the advantage of interpretability, the performance of this framework is still far from that of SOTA RL algorithm. I think it would be better if this framework could have similar performance to SOTA RL method. 

2. Lack of real-world case studies: Although the paper highlights the potential of ANQ in various domains, it does not provide specific real-world case studies or examples to demonstrate the practical application and effectiveness of the framework. Including such case studies would strengthen the paper's claims and provide more concrete evidence of ANQ's utility in real-world scenarios. 

3. No comparison was made with other interpretable reinforcement learning algorithms. For example, some Neuro-Symbolic Search methods. 

3. The presentation of this paper could be better, for example, Figure 3 could be larger and clearer. 

Limitations:
Incorporation of latest techniques: The paper mentions that ANQ has not yet incorporated the latest techniques, such as maximum entropy learning from SAC and other contrastive learning methods for representation learning. While the paper acknowledges that these refinements will be addressed in future work, the absence of these techniques in the current implementation may limit the overall performance and effectiveness of ANQ.  

Rating:
5

Confidence:
2

REVIEW 
Summary:
The submission creates a framework called Approximate Nearest Neighbor Q-Learning (ANQ). ANQ uses a sim-encoder contrastive learning and approximate nearest neighbor search to find which states are similar. Utilizing this approach, they can use it to find similar states in aiding for the decision the framework has made. They showcase their performance by performing experiments on MuJoCo with continuous control tasks. The exact environments experimented were Walker2d, Ant, HalfCheetah, and Hopper. There is also an ablation study conducted to show the benefit of the sim-encoder and contrastive learning that aids this framework. They provide a component called Explainable Action to show why it executed a particular action based on the state.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Significance:
Good ablation experiments to showcase the benefit of the sim-encoder. The results make it convincing that for AQL, the sim-encoder is quite beneficial. Plus you did this ablation among 4 environments.

Originality:
The explainable action is an interesting piece that can provide a good impact. By searching for similar states and can provide the explanations as to why the decision was made.

Weaknesses:
Clarity:
Confusion, in Section 4.1, you mention that the algorithms included were SAC, PPO, and TRPO, then in the next paragraph you mention A2C. Consider in the first paragraph to mention the exact algorithms you compare because in the next paragraph, you mention an algorithm that was not discussed exactly in the previous paragraph. In Figure 3, you show TD3-1M, ARS-75M which were not discussed so please in Section 4.1 to denote exactly.

With the figures, please provide more with the caption like a summarization or a sentence to showcase why it is important. It can help the reader if they have not read the parts within the main text.

Significance:
The approach can be on par with one or two deep RL approaches. Consider to improve the performance to have bigger impact. Usually for methods that are explainable there is a drop in performance so others may be hesitant to use it due to the performance drop.



Limitations:
They do address the performance drop compared to the deep RL models. No negative societal impacts.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work proposes a memory-based Q-learning algorithm aimed at better explainability. The method extends the prior work ""Episodic Control"" and enables learning with continuous action space.

This work provides 2 main contributions:
1. a one-step-away contrastive-learning objective to learn embeddings from states.
2. modified policy evaluation and improvement rules to account for the continuous action space.

The authors show empirical results to support their design choices:
1. The proposed algorithm is able to achieve some meaningful learning in 4 continuous control tasks.
2. The question-answering example shows that the method is able to find the nearest-neighbor states and their Q values to explain a chosen action.
3. The ablation study on the embedding module shows that it is necessary to learn dynamics-aware embedding.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The proposed method can learn in environments with continuous state and action spaces, making it ubiquitously applicable to real-world applications.
2. The algorithm and experiment settings are clear.
3. The ablation study on the embedding module justifies the contrastive learning objective.

Weaknesses:
1. The main weakness of this work is in the experiment results. In the Mujoco tasks, the performance hovers around the weakest baseline among all compared methods. For Ant, HalfCheetah, and Hopper, the policies also appear to have converged to suboptimal ones. This could have been caused by insufficient exploration.

2. The algorithm is only demonstrated in state-based environments, whereas the prior work ""Episodic Control"" can work with image observations. Contrastive learning has been shown to be useful for learning good feature extractors for images. The results would have been much more convincing if they were from vision-based tasks.

3. The notations are not fully explained. For example, $k$ and $e_t$ both exist in the dataset but the authors say that they use embeddings as keys. Also, $R$ in Equation 1 is not introduced.

4. Finding out the nearest neighbors and printing out their Q values is not a convincing way to explain the chosen actions because the Q values are computed in expectation. One can arguably explain a neural policy in a similar way, by sampling a few different actions and printing out their Q values.

Limitations:
The authors are upfront about the limitation in task performance and provide viable options for improvement. I would also encourage the authors to try their method on image-based tasks.

Rating:
3

Confidence:
4

REVIEW 
Summary:
Instead of using deep neural networks to approximate Q functions as it's done in deep RL methods, the paper investigates the potential of using nearest neighbor methods to approximate Q functions. They used contrast learning with a Sim encoder. They argue that such a method is more explainable than the ones with deep neural nets.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The illustration is the proposed method is clear.

Weaknesses:
* The proposed nearest neighbor might be useful and efficient for low-dimensional domains like Mujoco. I doubt its effectiveness when it goes to high-dimensional domains like video games;

* I don't see why the nearest neighbor method is more explainable;

* The proposed method lacks proper baselines;

* The overall presentation needs improvements.

Limitations:
NA

Rating:
3

Confidence:
4

";0
vN9OBsZtYH;"REVIEW 
Summary:
The authors introduce an innovative framework that enhances the fairness guarantees of a classifier in the presence of both sensitive attribute noise and label noise, considering them independently as well as in combination. Their approach incorporates theoretical guarantees and involves training a fair encoder to learn a novel data representation that ensures both fairness and accuracy. They demonstrate that by imposing bounded divergence between the noisy and clean distributions, fairness can be effectively transferred from one distribution to another. Notably, their method tackles the problem from a distribution shift perspective, eliminating the need for noise rate estimation typically required by conventional noise tolerant models.


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The introduction effectively substantiates all the claims made, including the contributions put forth by the authors. These assertions find validation through a thorough description of the methodology employed and the experiments conducted. The method section elaborates on the techniques and approaches considered, demonstrating how they align with the stated objectives. Furthermore, the experimental results provide empirical evidence that supports the claims made in the introduction. 
- The problem addressed in the paper is well motivated. The authors provide a comprehensive and compelling rationale for the significance and relevance of the problem. They effectively highlight the real-world implications and potential consequences of the existing limitations in the field.  
- The authors introduce an innovative alternative approach that effectively addresses the limitations of state-of-the-art (SOTA) methods. By identifying and highlighting the drawbacks of existing techniques (noise rate requirements), they demonstrate a clear understanding of the challenges at hand. 
- The methodology is clearly explained and well-organized. The paper includes sub-sections that effectively delineate different aspects of the methodology, ensuring a coherent and structured presentation. 
- The paper demonstrates commendable attention to reproducibility by providing thorough and detailed information regarding the experimental setup.
- For the experimental evaluation, the authors take into account various types of data, including both tabular and image data.
- The selection of datasets and the procedure employed to generate synthetic datasets align well with similar approaches found in the existing literature. 
- The evaluation conducted in the paper is both sound and comprehensive. The authors meticulously consider various aspects to ensure a robust evaluation.


Weaknesses:
- (Section 4, Experiments) The authors put forth a proposition to tackle the challenge of ensuring fairness in the presence of noise from a distribution shift perspective. However, in the experimental section, they fail to compare their proposal with methods that specifically address distribution shift in a fairness-aware scenario. It is worth considering that these alternative methods may also yield promising results when handling noisy sensitive and label information. Including such comparisons would provide a more comprehensive understanding of the relative performance and effectiveness of the proposed approach within the context of fairness under distribution shift.
- (Section 4, Experiments) The paper presents theoretical bounds, but unfortunately, they are not evaluated empirically. While the theoretical analysis offers valuable insights and establishes the potential effectiveness of the proposed approach, the absence of empirical evaluations leaves room for uncertainty regarding its practical applicability. Empirical evaluations would have provided concrete evidence of the proposed method's performance and its ability to meet the expected bounds.
- (Section 2, Fairness metrics) The discussion of fairness metrics lacks a clear structure, and I would suggest that the authors differentiate between individual and group notions of fairness, providing distinct explanations for each. Additionally, it would be beneficial for the authors to acknowledge the emergence of mini-max fairness notions, which are gaining popularity in the field. 
- (Section 2, Fairness-enhancing interventions)  While describing the pre-, in-, and post-processing methods, the authors primarily focus on specific techniques instead of providing an overview of the general framework. Consequently, it is not accurate to claim that all preprocessing methods aim to rectify the distribution of input features, nor is it true that all in-processing methods incorporate fairness enhancement as relaxed constraints. In reality, regarding the latter, there are variations where fairness is achieved through techniques such as fairness penalizations. While these cases are commonly encountered, it would be preferable for the authors to first describe the overarching objectives of the general workflows before delving into specific specifications. This approach would provide a clearer understanding of the broader goals before examining the specific techniques used. Moreover, the authors fail to explicitly state that their method constitutes an in-processing intervention.
- (Section 3.2) The authors initially discuss general distribution shift, but in line 167, they assert that they address covariate shift. It is important to note that these two types of shifts have distinct mathematical implications. Covariate shift specifically involves changes in p(x) between the source and target domains, while assuming that the functional form of p(y|x) remains unchanged. It would be beneficial for the authors to clarify which shift they are specifically addressing and how the mathematical characteristics of covariate shift come into play within their approach. Providing further clarity on this matter would help readers understand the specific focus and contributions of the proposed method in addressing the relevant shift.
- (Section 2, Fairness under distribution shift) In this section, the authors overlook several pertinent works, and some of the works mentioned are not even published. However, there exists a substantial body of literature that specifically addresses the challenge of ensuring fairness guarantees under distribution shift (for a comprehensive survey, the authors can refer to [1]). It is important to differentiate between methods that solely tackle distribution shift and those specifically designed for ensuring fairness under distribution shift. Furthermore, it is worth noting that different methods consider varying levels of data availability in the target domain, and not all of them assume the availability of (X, A) pairs [1]. For instance, the work [2] cited in that section assumes the target data is not available. 
- (Section 2, Related works) The purpose of this section is to not only provide a description of the related works but also to establish the connection between them, elucidate the significance of these relationships, and highlight the novelty of the proposed work or its intended aim to address specific limitations. However, despite providing descriptions of various works, the authors do not explicitly specify the precise position of their work within the broader landscape.
- Notation issues: After defining the notation at the beginning of Section 3, and Section 3.1, the authors employ symbols that have not been defined, such as, A in line 137, or $\mathcal{L}_{cls}$  in Eq (6). Regarding the latter, there is no specification regarding its meaning nor its mathematical form. 
- The paper contains several typos: line 67 after more there is a full stop, line 129 after of there should be an 'a', line 217 let should be in uppercase. 

[1] Barrainkua, A., Gordaliza, P., Lozano, J. A., & Quadrianto, N. (2022). A Survey on Preserving Fairness Guarantees in Changing Environments. arXiv preprint arXiv:2211.07530.

[2] Rezaei, A., Fathony, R., Memarrast, O., & Ziebart, B. (2020, April). Fairness for robust log loss classification. In Proceedings of the AAAI Conference on Artificial Intelligence (Vol. 34, No. 04, pp. 5511-5518).


Limitations:
The authors do not thoroughly discuss the limitations of their method, which is an important aspect to consider. Taking inspiration from the questions raised concerning the shift type and potential implications beyond binary Y and binary S could be valuable in addressing the limitations and further refining their approach.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper aims to improve the performances of fair training when the group attributes or labels in the training data have noisy information. The paper views the noisy training data problem as a kind of distribution shift, where the training data is noisy and the test data is clean. To address this issue, the paper proposes a fair representation learning method to reduce the impact of distribution differences. The paper also provides some theoretical analyses to show the relationship between the group fairness results and noisy training data. In the experiment, the paper uses three datasets and compares with several baselines to show the performance gains of the proposed method.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
S1. The paper solves an important research problem, preserving the performances of fair training under the noisy training data. The paper views this problem as a distribution shift issue.

S2. The paper gives some theoretical analyses on the relationship between the group fairness and noisy data.

S3. The proposed algorithm empirically shows better fairness and accuracy performances compared to the baselines. 


Weaknesses:
W1. Many important details are missing in the proposed fair representation learning. 
- In Section 3.3, the final training objective in Eq. (6) has many unexplained important details. For example, what is L_cls, and how are the input arguments (e.g., g_00, h) used in L_cls? Also, it seems the lambda values are the tuning knobs, but there is no explanation on why the loss terms should be connected by two lambda values. Including these details, a clearer rationale for design choices is needed.

W2. In experiments, the proposed algorithm is not clearly analyzed. For example, it would be much better if the paper explains the following.
- How the lambda values in Equation 6 affect the training performances
- The computational complexity of the proposed algorithm

W3. Although this work is highly related to the studies on fairness under data distribution shifts, there are no clear comparisons with them. In experiments, all the baselines are from the noisy training literature. Since many algorithms for fair training under distribution shifts have been recently proposed, it would be better to compare with them empirically or at least to be clearly discussed.


Limitations:
The paper did not discuss the limitations and possible negative societal impacts. As the limitations, this work may discuss which types of data noises cannot be handled by the proposed algorithm.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper studies the fairness problem under noise perturbation on both label and sensitive attributes. In particular, it considers such a problem from the perspective of distribution shift and uses the normalizing flow framework to analyze the problem. Empirically, the proposed methods achieve the best utility and fairness trade-offs under different settings of noise perturbation.    

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The paper presents a method for learning fair representation when there is noise on both sensitive attributes and labels. The method is straightforward and empirically shown to be effective. 
2. The theoretical analysis is sound 
3. Compared to the previous work, this work considers both label and sensitive attribute noise without directly estimation the noise parameter, which is more practical in real-world applications.


Weaknesses:
I do not find any obvious weaknesses in the paper. But there are minor points that the author could further improve their paper.
1. The assumption of invertible function in the fair normalizing flow methods might be strong. For example, In ResNet, the default activation function is ReLU, which is not invertible. The authors might need to provide more justification for this.
2. Discussion of limitations. The paper could be improved if there is a discussion of the limitations. 


Limitations:
The authors do not discuss the limitations of the work, which is highly suggested.


Rating:
6

Confidence:
4

REVIEW 
Summary:
This work studies noise tolerance of fairness from the perspective of subpopulation/subgroup shift, by considering the perturbation of the sensitive attributes as well as the labels _without_ the need for noise-rate estimation - by considering the noisy distribution as the source and clean distribution as the target. This leads to a ""covariate"" shift between the source and the target distributions, with the shift being a consequence of the noise. The work then proposes a fair representation learning method for fairness under noisy attributes based on normalizing flows, and presents a theoretical result showing that this method minimizes the upper-bound of the clean equalized odds. Thorough empirical evaluation is presented for both static and varying noise rates, showing the efficacy of the proposed method.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
This work addresses an important setting of achieving fairness when both the label $y$ and sensitive attribute $a$ can be noisy - as traditional metrics can be biased under noisy data. The theoretical analysis presents a comprehensive study of fairness transfer between clean and noisy data and supports the choice of the minimizer in the proposed method. The empirical analysis is thorough - the section 4.2.2 is especially interesting as it considers both static and dynamic noise rates. 


Weaknesses:
The following points should be considered:
1. The loss function (in Eq. 6) focuses on a binary valued $a$ and $y$. How does this methodology extend to the more general case either/both can be multi-valued? Is it straightforward?
2. Is there any more intuition on leveraging normalized flows for this setting?




Limitations:
No limitations of this method are mentioned, and it would be nice if any potential drawbacks can be discussed. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper targets the problem of ensuring fairness with noises on either sensitive attributes or labels. Specifically, this paper models the noisy data training set and clean test set as a distribution shift and proposes a regularization term to improve the fairness of classifiers.
The theoretical analysis indicates that the classifier trained by the proposed framework on the noise data can be bounded when evaluating on the clean data.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The problem of ensuring fairness when training on noisy data is an important problem.

2. The whole framework makes sense.

3. The experimental results show the advantage of the proposed approach.

Weaknesses:
The writing is not very friendly to readers without a background in this specific fairness problem. Please check my questions below.

Limitations:
Please check my questions above.

Rating:
5

Confidence:
1

";0
5zipcfLC2Z;"REVIEW 
Summary:
This work proposes RapidBERT to train BERT in a faster way. Different from the previous accelerated method, this work attempts to employ some recent popular transformer architecture efficient designs as the basic modification of RapidBERT architecture, such revisions including the introduction of FlashAttention, AliBi, Bf16, GLU, Low Precision LayerNorm, etc. The training dataset is C4. It cost 1.13 hours on 8 A100-80G GPU.
Results on the GLUE NLU benchmark show that under fair comparison except for the training step, RapidBERT can get efficient training FLOPs compared to previous methods.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
- This work employs some recently released transformer-efficient architectures in BERT training, getting RapidBERT.
- The RapidBERT got similar NLU ability compared to the vanilla-BERT.

Weaknesses:
- For a fair comparison, The authors should also post the RapidBERT results pre-trained on the English Wikipedia and the Books Corpus. (The original BERT study trained on English Wikipedia and the Books Corpus).
- Similar studies have been proposed in the same faster BERT pertaining track, it would be better to have a comprehensive comparison with them in detail to help readers learn more about this field.
- In line 176, this work says that ""we modify the vocab size from 30522 to 30528"" inspired by MEGATRON so that the vocab size is a multiple of 64, and leads to a non-trivial throughput speedup. However, in MEGATRON's work, they pad the vocab because of the 8-way model parallelism. But this work does not employ any tensor parallelism or pipeline parallelism, could you explain why such an operation can lead to a speedup in the training of RapidBERT?
- Most of the new techs (eg. FlashAttention) tried in this work have already been employed and integrated into the BERT pre-trained stage in some frameworks, e.g., Megatron, DeepSpeed [1][2]. In this manner, this work is not so innovative and may not be the first work to do this, it would be better to discuss differences with these similar works. (Actually, it is inspiring to see more works on efficient LM pertaining field, so this term is not very influential, I just brought it up.)

[1] https://github.com/NVIDIA/Megatron-LM

[2] https://github.com/microsoft/DeepSpeed

Limitations:
Similar studies have been proposed in the same faster BERT pertaining track, this work does not have too much comparison with them in detail.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper benchmarks several architectural changes to BERT that allows for more efficient pretraining. More specifically, the paper adds flash attention, ALiBi position representations, and GLU activations to the original BERT architectures. For fair comparison, they re-implement the baseline BERT-base using the same pretraining hardwares. They show that with these architecture modifications they can achieve better performance on downstream tasks while using smaller amounts of pretraining time. Their base model’s performance is similar to BERT-base in (Devlin et al., 2018) using approximately 1 hour of pretraining on 8 A100 GPUs, costing $20 on a standard cloud provider. They also benchmark the training cost of each architecture modification, showing that reducing the sizes of GLU matrices help to improve throughput of training significantly

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper provides empirical evidence that combining flash attention, ALiBi position representations, and GLU activations reduces 50% of the training cost. Their ablation on GLU will benefit researchers working on the relevant areas

Weaknesses:
The contribution of this paper is limited. The proposed architecture modifications are all adopted from prior work. Their primary findings mostly come from tuning the hyperparameters of GLU. And their current findings depend on multiple architecture modifications, which is complicated and makes their scientific findings unclear. I’d hope that the authors can provide more experiments showing why all these modifications are necessary and whether it’s possible to simplify it further for a clearer takeaway message


Limitations:
The authors have discussed the limitations of their work.

Rating:
3

Confidence:
3

REVIEW 
Summary:
The paper introduces RapidBERT, an architecture and training paradigm for pretraining BERT-style language models that is cost-effective. The proposed approach incorporates several modifications into the conventional transformer encoder block, including FlashAttention, Attention with Linear Biases, Gated Linear Units, Unpadding Module, and a low precision LayerNorm. The pretraining process avoids the Next Sentence Prediction task and follows the RoBERTa practices, using the C4 dataset with a 30% masking ratio in MLM.

RapidBERT demonstrates faster convergence and achieves a better accuracy versus time Pareto curve during pretraining. Their base model consistently outperforms the original BERT model on average across the GLUE dev tasks. Remarkably, the training process only takes 1.13 hours using 8 A100 GPUs, resulting in a cost of approximately $20.

The authors show that RapidBERT is Pareto-optimal when compared to BERT for both base and large models. They also highlight the necessity of extensive training for larger models, as RapidBERT-Base outperforms both BERT-Large and RapidBERT-Large for a significant portion of their training.

In addition, the paper includes a comprehensive ablation analysis of the design choices made in RapidBERT and evaluates the throughput of each architecture. The results indicate that the GLU modification leads to a decrease in throughput, while ALiBi has minimal impact on throughput. On the other hand, incorporating low precision LayerNorm significantly improves throughput.

Overall, the paper presents RapidBERT as an efficient and effective approach for pretraining BERT-style language models, showcasing its superior performance, cost-effectiveness, and the benefits of the proposed modifications.

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
- The paper introduces a time and compute-efficient approach for pretraining BERT-like models, offering significant advantages over previous works. This approach holds promise for pretraining task-specific language models that do not require high parameter requirements, making it highly practical and valuable in various applications.
- The implementation details provided in the paper are clear and comprehensive, contributing to the reproducibility of the research. The authors effectively explain the concepts and methodologies, ensuring a thorough understanding of the proposed approach.
- The research conducted on related work demonstrates a comprehensive exploration of the existing literature. The design choices made in this study are grounded in well-established prior works, and the ablation study further strengthens the validity and effectiveness of the proposed modifications.
- The paper presents an intriguing finding regarding the impact of model size on performance in specific domains. The authors discover that larger models may not always yield superior results due to limited data availability and increased compute time. This insight, demonstrated by the Pareto optimality of RapidBERT-Base over BERT-Large and RapidBERT-Large for a significant portion of the training, adds a valuable contribution to the understanding of model performance and scalability.

Weaknesses:
~- The approach primarily combines existing techniques, such as FlashAttention, GLUs, and low precision LayerNorm. While the authors introduce the novel aspect of maintaining a 30% masking ratio, the overall novelty of the architectural choices is limited.~
~- It would have been beneficial to include a discussion comparing the proposed approach with the original RoBERTa model as a baseline. This comparison would provide a clearer understanding of the improvements achieved by RapidBERT and highlight its unique contributions.~
- The paper lacks a discussion on how the architectural choices made in RapidBERT could facilitate the development of larger language models, such as GPT 3.5. Exploring the potential scalability and benefits of these choices for larger models would enhance the practical relevance and implications of the research.
- The ablation studies do not investigate the effects of changing the masking ratio. Including an analysis of different masking ratios would offer insights into the choice behind keeping the ratio to be 30%.
- The paper would benefit from discussions on the comparison with RoBERTa, the scalability of the approach to larger models, and the effects of different masking ratios in the ablation studies.

Limitations:
The discussion on limitations is satisfactory. There are some limitations discussed in the appendix regarding potential training stability issues on the larger models. Potential negative societal impacts (ease of pre-training could lead to ease of more biased/inappropriate models) is also discussed in broader impacts section. General limitations with any pre-trained LMs will apply to RapidBERT.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper proposes a new efficient recipe for training BERT, matching the original performance of BERT on GLUE in ~1h on 8x A100. To do so, the authors leverage a number of architectural/implementation improvements: FlashAttention, ALiBi, GeGLU, `bf16` layer norm, unpadding, and tweaks to the masking ratio. The authors find that their recipe is optimal even when considering longer runs, and that it transfers well to BERT-Large. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
* **S1.** Reducing the costs associated with training language models can help practitioners iterate faster, improving downstream research outcomes. This makes this paper potentially valuable to the community, as it improves upon previously introduced similar recipe such as CrammingBERT.

* **S2.** The authors detail their contributions and open-source their code, making these results reproducible and enabling the community to build upon them.

* **S3.** The authors feature an updated baseline (BERT-Base) that helps for fair comparisons in their setup. 

Weaknesses:
* **W1. It is difficult to untangle individual contributions to the final result.**
    * **W1.1.** The proposed recipe visibly has some positive impact on GLUE score, as Figure 3 shows that it achieves significantly better performance than BERT-Base after the same amount of training. However, that impact is never quantified in the paper. Here, the ablations should not only focus on throughput, but also on how the proposed interventions might impact the GLUE score. 
   * **W1.2.** The proposed baseline is very strong (which is a positive point), and it would be good in Table 1 to also showcase the time it take for it to reach a 79.6 GLUE score. Furthermore it would be interesting to identify what makes the baseline such a strong one (with a final score much higher than BERT-Base). Is it the change in data to C4 (which authors acknowledge as an important factor l161)? Something else?
   * **W1.3.** l112 the authors discuss using ALiBi to pretrain with a shorter sequence length and extrapolate at test time. It's unclear if this end up being used, and if it is included in the ALiBi ablations. 
   * **W1.4.** l251 it is disappointing to not ablate adequately every component, especially since the value of this paper lies in having a potentially systematic approach to performance improvements of BERT models.

* **W2. The Pareto frontiers described may be slightly misleading, as they do not account for LR schedule.** The so-called Pareto optimality is obtained by taking points from the same run, instead of having one run per pretraining budget on the plot. This approximation penalises intermediate budgets, as they are evaluated with an incomplete LR schedule. While I don't think this has a significant influence in this work, since the authors discuss Pareto optimality so much, this should at the very least be clearly discussed as a limitation to avoid misleading other authors. This issue is particularly relevant, as it has lead to significant misunderstandings around scaling laws for instance (see Hoffmann et al., 2022).

* **W3.** The paper feels a bit repetitive, as if it had been stretched to fit the 9 pages of content. Section 3.2 and 3.3 are particularly egregious in this regard, and more time could instead be spent on ablations. 

* **W4.** (minor nits) l98 reference to Triton should cite ""Triton: An Intermediate Language and Compiler for Tiled Neural Network Computations"" (Tillet et al., 2019); l145 the sentence ""results from NVIDIA and others"" is confusing, as the final work cited is not from NVIDIA -- there should be a citation somewhere for the NVIDIA results.

Limitations:
While there is no explicit section, some limitations in terms of scope are discussed in the conclusion.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper presents a training recipe that can train a BERT-style encoder model efficiently (1.13 hours on 8 GPUs). The recipe combines several techniques including a higher masking rate for MLM, bf16, optimized vocabulary size. The model is trained on the C4 dataset for 1.13 hours and achieves 79.6 on GLUE. The paper conducts ablation studies to characterize properties of the architecture design choices.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The paper presents very impressive results of pre-training a BERT-like model very efficiently without loss in accuracy. In general, I believe that this is a solid outcome and it is definitely a good addition to the community, as it can enable more researchers to pretrain custom BERT models from scratch.
* The paper presents ablation experiments that quantify the impact of each proposed change.
* The paper is well written and easy to follow. Replicating the results should not be hard.


Weaknesses:
* The experiments are only conducted on GLUE tasks; it is not clear how well the trained model transfers to other datasets.
* There is no fair comparison (i.e., under the same hardware setup) of the proposed recipe and other efficient training methods such as CrammingBERT.


Limitations:
I didn’t see a clearly potential negative societal impact of this paper.


Rating:
7

Confidence:
4

";1
oAHjj0of5z;"REVIEW 
Summary:
This paper proposes a new probability theory named indeterminate probability theory and a new model name Indeterminate Probability Neural Network (IPNN). For the new probability theory, it is an extension of the classical probability theory, with which some intractable probability problems become tractable (analytical solution). For the new model IPNN, it can perform unsupervised clustering while doing classification and make very large classifications with very small neural networks.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. This paper is well organized. The intuition of the new concept of indeterminate probability is clearly demonstrated from a simple example. The theory of indeterminate probability theory is well formulated and all assumptions are clearly numbered.

2. To the best of my knowledge, the indeterminate probability theory is proposed for the first time.

Weaknesses:
1. The proposed new probability theory and the IPNN model are interesting contributions of this paper, but not the CIPNN. As listed in the second contribution in the introduction part, the authors claim CIPNN as the contribution of this paper. However, CIPNN is the main contribution of the authors' other papers. Thus, the authors should either delete this contribution and introduce CIPNN in other places (such as related work) or contain CIPNN in this paper (then the ICCV paper should be withdrawn).

2. There are many grammar mistakes in the use of articles. For example, 'as example' on line 46 should be 'as an example', and 'an unique category' on line 257 should be 'a unique category'.

3. On lines 110 to 112, the authors mention that the event in classical probability theory can only be happened or not happened, while for IPNN, we can have the probability of an event state. However, the authors do not consider (or demonstrate) the properties of the probability in IPNN. In classical probability theory, the happening of the event is an unbiased estimate of the probability, which means that we can approximate the true probability through plenty of experiments. But what about the probability in IPNN? What is the quality of the probability output by IPNN? Typically, the softmax output of a neural network can only be treated as a probability distribution (since the sum of all coordinates is 1) but does not indicate the true probability of one class.

4. On line 120, the authors mention that $\alpha_{ij}^j (k) \in \{ 0 , 1 \}$ in classical probability. But as defined in Eq. (2), $\alpha_{ij}^j (k)$ is a conditional probability. Then it can be any decimal between 0 and 1 if there is no more constraint. Can authors provide more detailed explanations for the claim on this line?

5. As mentioned in the abstract, IPNN 'is capable of making very large classification with very small neural network'. And the idea is using binary encoding ('the binary vector is the model inputs from 000000000000 to 111111111111, which are labeled from 0 to 4095') as mentioned in appendix D.1, which is a basic concept in information theory. But in practical implementations, one-hot encoding is preferred since it does not introduce a redundant distance between different labels. For example, the binary code 001 is closer to 000 than to 111. If binary encodings can successfully reduce the network size, this would be a great contribution to this paper. Can authors provide a more detailed introduction about output encodings (especially the comparison between binary encodings and one-hot encodings in history) and more discussions about the key tricks to making binary encodings work in IPNN?

6. As mentioned in the abstract, the indeterminate probability theory makes 'some intractable probability problems have now become tractable (analytical solution)'. It seems that the authors mix up the concept of tractable and analytical solutions. A tractable problem is a problem that can be solved with acceptable complexity (usually polynomial time and space complexity). Tractability has no relation with analytical solutions. An analytical solution can be intractable when there are exponential operations in the solution, and a tractable problem may have no analytical solution (there is no analytical expression of the error function but we can approximate the error function efficiently).

7. To my understanding, the contribution of this paper is IPNN as a new neural network (architecture, engineering trick, or training algorithm). The indeterminate probability theory is far from an extension of the classical probability theory. To extend the classical probability theory, the authors should at least formulate the new theory from measure-theoretic probability theory.

Limitations:
Yes.

Rating:
3

Confidence:
3

REVIEW 
Summary:
The main contribution is the proposal of the inference architecture which creates parallel softmax outputs (the authors call “splits”), which are combined into a joint soft-label space to make classification decisions under MAP rule; the joint label space can help with sub-classification tasks.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The architecture tries to create more Bayesian information before making final classification, which is a potential Bayesian neural network approach that can be developed in the future. On the other hand, this work may be inspiring to people who are interested in large-dimensional label representation.

Weaknesses:
1.	It is not clear to see in this paper what is new to the “classical theory of probability”;
2.	The novelty is limited.
3.	The assumptions may not be very reasonable:
The assumption 2 and 3 are ok at initialization, however when the weights are updated, A and Y are not generally independent;
The assumption 4 is very counterintuitive. Normally we have a joint distributions between X and Y, but assuming that X and Y are not independent mutually, then they are not given A; and if X and Y are independent (Y is random label for example), then they are independent given A. 
4.	The main contribution of this paper is the splitting part of the architecture, creating parallel softmax outputs and combine them to make MAP decision. I think the paper should emphasize on the reasoning of this mechanism, for example, the ensemble of different likelihoods that contributes to the performance, or the geometric interpretation of the proposed label embedding/representation that makes sense.
5.	Lacking of the verification of the advantage of the new method over original softmax. After reading this paper I still do not know if this method creates more or reduce a little uncertainty in the classification compared to original softmax approach.


Limitations:
yes

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper proposes what I consider to be a type of neuro-symbolic AI model involving neural networks for multi-class classification problems; the authors refer to the model as an indeterminate probability neural network (IPNN). Frankly I did not fully understand the model, but the main intent appears to be a form of latent variable modeling for multi-class classification. An important line to summarize the paper is mentioned in Section 2: “our proposed IPNN is one DPVM (deep latent variable model) with discrete latent variables and the intractable posterior calculation is now analytically solved with our proposed theory”. I have reviewed a version of this paper previously, and it looks like the paper has not changed much; this is unfortunate, because I did not understand the paper clearly at that time, and I feel that I still do not understand it well.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
The paper claims to blend neural networks and probability theory in a novel way; if this is true then it can be seen as an innovation in neural network modeling as well as neuro-symbolic AI. Another strength is that the paper is non-standard in that it tries to do something new around modeling multi-class classification problems.

Weaknesses:
In current form, the paper suffers from a number of weaknesses. A major weakness is that it remains unclear why a new theory of probability is needed in the first place. Note that the example in Section 3 can be studied using standard Bayesian modeling where X_i is the true coin toss result, A_i the adult’s reading and Y_i is the child’s reading, all for the i^{th} coin toss. Here X_i are i.i.d random variables, and A_i and Y_i are conditionally independent given X_i. Then the query of interest can be posed in terms of these random variables. I did not understand what the new theory is and why it is even needed. Also, I find it hard to follow the paper and feel it is not appropriately positioned in the literature. For instance, to me, it appears that the proposed approach is a form of neuro-symbolic AI, yet this is not even mentioned in the paper. I feel there is far too much lack of clarity in the paper in general.

Limitations:
The authors need to write more about the limitations of the work.

Rating:
3

Confidence:
2

";0
3tbTw2ga8K;"REVIEW 
Summary:
This paper proposes a possible mechanism that explains both the phenomenon where the cross-entropy loss of large language models (LLMs) decreases as a power law with respect to the training corpus size, and the phenomenon in which certain capabilities of LLMs emerge spontaneously as the loss becomes low enough. The authors empirically demonstrate that in a toy problem called ""multitask sparse parity"", in which their assumptions explicitly hold, an MLP network indeed obeys both the scaling laws and the emergence phenomena. Finally, the authors propose an empirical method to auto-discover skills in LLMs and apply this method to provide empirical evidence that their assumptions also hold in LLMs. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The paper proposes a novel and timely explanation for both the scaling laws phenomenon and the emergence phenomenon in large language models (LLMs). 

2. The authors support their explanation with a clear demonstration on a toy task of multitask sparse parity. Essentially, they trained a simple MLP network on multiple sparse parity tasks and demonstrate both the scaling laws phenomenon and the emergence phenomenon when the distribution of the different tasks follows a power law.

3. In addition, the authors demonstrate the relevance of their explanation for LLMs by proposing a novel method for auto-discovering skills in LLMs and show that the discovered skills obey power law frequencies. Using and scaling this method might be of independent interest for both the mechanistic interpretability community and for designing better pre-training datasets.

4. The paper is clearly written and accessible to readers with varying backgrounds.

Weaknesses:
There is insufficient evidence to conclusively prove that the Quantization Model accurately depicts the training of large language models.

Limitations:
The authors were very honest in acknowledging less plausible alternative explanations for their empirical findings.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes a hypothesis that there exists a universal and crucial discrete set of computations (quanta) for reducing loss in various prediction problems, and the model's performance is determined by successfully learning these computations. Through this hypothesis, it demonstrates the relationship between power law neural scaling and the effective learning of more quanta, which cannot be solved solely by the memorization ability of the existing model, particularly in complex problems. Additionally, the paper proposes ""Quanta Discovery from Gradients"" as a method to auto-discover quanta. 

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
1. Based on the assumption that there exists a discrete set of computations, known as ""quanta,"" which determines the performance of the model, the paper provides hints for understanding the emergent behavior observed in the scaling law of the model.
2. The objective of the study is to interpret power law scaling and emergent behavior phenomena by using a multitask sparse parity dataset that cannot be solved solely by the model's memorization ability.
3. Through experiments conducted on the multitask sparse parity dataset, the paper shows that when the frequency of using quanta follows a power law distribution, power law neural scaling can occur as the model learns more quanta.

Weaknesses:
1. The paper lacks a clear definition of quanta, which could benefit from further elaboration and clarification.
2. Insufficient explanation is provided regarding the criteria used to determine quanta through the Quanta Discovery from Gradients (QDG) method.
3. Extending the experiments on the Quantization Hypotheses, using the proposed toy dataset, to Language Models (LLM) is hindered by the lack of clarity regarding the relationship between LLM tokens and quanta.
4. Section 5 claims that ""Clustering based on inputs or outputs therefore seems unlikely to discover quanta in language modeling,"" but lacks substantial empirical evidence to support this statement.
5. The explanation concerning the relationship between quanta and gradients, which forms the basis for determining quanta in the QDG method, needs to be further developed and elaborated upon.

Limitations:
There is a need to demonstrate the effectiveness of the proposed Quanta Discovery from Gradients (QDG) method in identifying quanta in various Language Models (LLM) and datasets.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes a new way of understanding scaling laws. Namely, it shows that capabilities can be thought of as being broken into discrete units (quanta) which themselves follow a power law and have an inherent ordering--called the Q Sequence. This combined with the fact that 1) an increasing subset of quanta are learned over various scales and 2) model and data sizes can be related to the number of quanta gives rise to the scaling laws shown in previous works. The paper also proposes a method to cluster examples by unit-normalized gradient, and empirically confirms within margin of error that theory agrees with observed data. The quantization model explains a sudden emergence of new capabilities with scale: certain quanta must be learned before the model can carry out certain tasks.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
Simple, effective model to understand scaling laws

Weaknesses:
While the paper shows that the model scaling trend agrees with theory on real data, it does not empirically validate whether the same alpha of 0.083 matches the data scaling theory. Also, it would be interesting to show this holds on non LLM tasks like image classification, since the (titular) claim is ""neural scaling"".

Limitations:
Not that I am aware of

Rating:
8

Confidence:
4

REVIEW 
Summary:
The authors investigate neural scaling laws and propose an explanation for the power law scaling that is observed in addition to the emergence of new behaviours. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The ideas are interesting and I found some of the experiments such as per token losses on the language models to be quite interesting.



Weaknesses:
I found the presentation of Section 5 to be a bit confusing.

I also found the connection to emergent behaviours to be a bit speculative. Specifically I think the idea of looking at per token losses/etc to be quite interesting, but I wouldn't necessarily say that I am convinced that this is emergence in the same way I'd think of emergence on some BigBench tasks, for example. 

Limitations:
Yes, the limitations are mostly well addressed.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper proposes that the capabilities of a neural network are composed of discrete “quanta” that each help reduce loss on a subset of training examples, and that are learned in approximately decreasing order of frequency. If the quanta are present in a Zipfian (power-law) frequency distribution in the training data, and each quantum is approximately equally valuable in the examples where it’s present, such a theory would provide a mechanistic explanation for power law scaling of loss with respect to model and dataset size.

First the authors validate this hypothesis on a toy dataset constructed to have this quantized structure (by being composed of several independent tasks each with a different prefix key). Then they find similar patterns in real language models, and use an algorithm they call “quanta discovery with gradients” to identify potential quanta in these models and their data.

Their results are supportive of the quantization hypothesis, but with a high degree of uncertainty.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The hypothesis is a beautiful and important claim if true, and the paper provides a well presented and solid chunk of evidence that it is. The toy dataset experiment is simple and straightforward and demonstrates that the quantization hypothesis works under ideal conditions, and the LM experiment shows that similar phenomena are also present in real models, while proposing a reasonable initial approach to quanta discovery. The results on monogenic vs. polygenic samples also point pretty clearly towards the validity of quantization. 

Given QDG or any other model of what specifically the quanta might be, the hypothesis also enables testable predictions about scaling laws.


Weaknesses:
The multitask parity setting seems too obviously likely to lead to quanta, so it doesn’t seem to prove very much (although it’s useful to set up the framing, almost like a thought experiment).

The QDG methodology is also a bit disappointing in a few ways. By clustering the gradients, it assumes that quanta (conceptually defined without reference to model structure) will be localized in the model—which is probably true, but might otherwise have been a testable hypothesis. It’s also too slow for the authors to have applied it to models larger than the smallest Pythia LM, greatly limiting its applicability.


Limitations:
The authors acknowledge the limitations of their methods and experiments.


Rating:
8

Confidence:
4

";1
Ifq8GMdqJK;"REVIEW 
Summary:
The paper proposes a new Rao-Blackwellized Predictor Test which is a regression-based conditional independence test. It was shown to be robust in misspecification settings. 

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper is in general well-written and considers an important scenario which is not covered by standard conditional independence tests. The new approach guarantees Type-1 error control (asymptotically). 

Weaknesses:
The paper is quite technical (what is good), however, a number of important details are in the Appendix what is not always practical. 

Limitations:
N/A

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors study model-based tests of conditional independence under conditions where the models may be biased. They propose new approximations and bounds on testing errors resulting from bias in model-based CI tests. They also introduce RBPT, a new model-based CI test and evaluate it against several alternatives.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well-written and provides substantial theoretical derivation of the Type I error rate and power of the proposed test.

The authors address a very important problem that is under-appreciated. Many researchers would assume (mistakenly) that non-parametric conditional independence testing is a solved problem. In reality, there exists no generally accepted solution, despite substantial prior work.

Furthermore, the authors work on a very important aspect of the problem. Model misspecification, as they term it, appears to be both ubiquitous and difficult to detect and diagnose. Making model-based CI tests that are robust to this source of bias would represent a major step forward. 

The authors correctly point out that, despite a variety of deep models being *capable* of being universal function approximators, that does not mean that unbiased models are *actually learned in practice*.

Empirical evaluation of CI tests is difficult, at best. However, the authors provide both simulation and real-data experiments that provide some evidence that their approach delivers on its promises.

Weaknesses:
As the authors note, the term “model misspecification” has a long history in statistics (and some parts of machine learning), and it seems a stretch to expand that term to include non-optimal parameter settings due to the limitations of a given training algorithm. There is certainly an issue here, but calling it “model misspecification” is likely to cause some initial confusion for readers. The authors are quite clear that they are redefining the term, but seeking an alternative term may be more helpful to readers. Some options include “non-optimal training”, “underspecification”, and “non-identifiability”, though I suspect there is an even better term.

In the caption of Figure 3, the authors state that “RBPT and RBPT2 have better Type-I error control compared to other methods.” However, STFR fairly clearly outperforms RBPT2 in terms of absolute error with respect to the 0.10 gold standard and STFR usually outperforms RBPT.

One minor correction: The text describing Figure 1 says that the Type I error rate was set to 0.01, but the figure shows that it was set to 0.10. Furthermore, that text refers to Figure 4, which does not appear to exist in the paper.

Limitations:
There appear to be relatively few negative impacts to the authors' work. Indeed, it facilitates *avoiding* important classes of errors that could have substantial negative impacts.

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper presents new robustness results for three regression-based conditional independence (CI) tests, namely, Significance Test of Feature Relevance (STFR), Generalized Covariance Measure (GCM) test, and REgression with Subsequent Independence Test (RESIT). It introduces a new CI test, the Rao-Blackwellized Predictor Test (RBPT), designed to be more robust to model misspecification. Theoretical findings and experimental results show that the RBPT is able to control Type-I error effectively even when models are misspecified.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- Well written and structured. Thanks
- Great intro to related work.
- Addresses a significant problem
- Provides theoretical and empirical insights.

Weaknesses:
- The authors make a long and good argument, that misspecification needs to account for the training algorithm. It seems that this is a natural result of the model posing a non-convex or underspecified optimization problem for training. Therefore, I'm not sure this needs to be demonstrated in such depth. (Please correct me, if I'm misunderstanding this)
- Could have more empirical validation.

Limitations:
n.a.

Rating:
7

Confidence:
2

REVIEW 
Summary:
This paper discusses the challenges of conditional independence (CI) testing in modern statistics and machine learning. The authors note that many modern methods for CI testing rely on supervised learning methods to learn regression functions or Bayes predictors, but these methods can be unreliable when the models are misspecified. The paper proposes new approximations or upper bounds for the testing errors of three regression-based tests that depend on misspecification errors. The authors present new robustness results for three relevant regression-based CI tests and derive approximations or upper bounds for the testing errors that explicitly depend on the misspecification of the prediction models. The paper concludes that more attention should be given to theoretically understanding the effects of misspecification on CI hypothesis testing and that current regression-based methods are usually not designed to be robust against misspecification errors, making CI testing less reliable.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The writing is good.

The introduction on model misspecification is good.

Weaknesses:
Lack of introducing conditional independence testing.

RBPT2 is powerful but it cost much more time compared with other methods, such as STFR, GCM. Lack of analysis the time complexity of each designed method.

Only one real dataset. This method should be applied on more real application and dataset, like STFR which is applied on MNIST handwritten digits, Mechanisms of Action (MoA) prediction for new drugs and Chest X-rays for pneumonia diagnosis.

Limitations:
N/A

Rating:
6

Confidence:
1

REVIEW 
Summary:
The paper under review offers a thoughtful exploration of how model misspecification influences conditional independence testing. The significance and practical applications of such testing are aptly underscored in the introduction. The use of a toy example to demonstrate the impact of model misspecification on Type I error is a compelling approach that effectively propels their study. However, the paper's structure could be improved for better readability. While the work is technically exhaustive and appears sound, the layout requires refinement. For instance, the discussion and conclusion section seems truncated due to an overflow of technical details, presumably in an attempt to conform to the page limit.

Additionally, the paper presents an insightful study on Type I error bounds and illuminates how factors in model misspecification might influence the performance of our model. This is examined across three leading methods of regression-based conditional independence testing, STFR, GCM, and RESIT; a commendable breadth of investigation. On the other hand, there lies a lot of  ingenuity in the application of sufficient statistics and the Rao-Blackwell theorem to devise a conditional independence test - an approach that stands out as particularly clever.

Despite the paper's notable theoretical contribution and illumination of model misspecification implications, its current state warrants a rejection recommendation. The primary rationale lies in the sparse experimental validation, which leaves room for further expansion. While a more comprehensive set of experimental results could potentially sway my assessment towards endorsement, the manuscript, as it stands, along with the limited set of experiments, doesn't quite meet the high standards necessary for acceptance. Thus, my score, on balance, falls on the lower side.


Soundness:
4

Presentation:
1

Contribution:
3

Strengths:
1. The paper exhibits a robust theoretical foundation, ingeniously leveraging the principles of sufficient statistics and the Rao-Blackwell theorem to address conditional independence testing. This innovative application, to the best of my understanding, introduces a novel concept and forms a significant contribution.
2. The paper provides extensive misspecification bounds that comprehensively span across various regression-based methods. Moreover, the experiments also benchmark against simulation-based approaches.
3. The exploration of examples in Appendix A.2 is highly instructive, where model misspecification can influence and Type I error of our test significantly. In addition, the choice of synthetic experiments adapted from Berrett et al. serves as a fitting means of demonstrating the concept in this study.
4. In an innovative deviation from the traditional setting, this paper presents proofs across a series of models that converge to a pre-established class of models, thereby aligning well with contemporary machine learning paradigms


Weaknesses:
1. Acknowledging that the paper's paramount contribution resides in its theoretical development, I must express my reservations regarding the experimental element, which appears somewhat insufficient. To bolster the validation process, I've proposed additional experimental situations in the ""Questions"" section. As it stands, the validation primarily leans on a single toy experimental setup, and a comprehensive benchmarking across different methods in a real-world setup is lacking.
2. In terms of presentation, the paper could benefit from some refinement. Notably, a significant portion of the proof concepts across different Conditional Independence Testing methods appears to overlap. A majority of these proofs could have been concisely presented by adopting lemmas that generalize across multiple methods, thus increasing reader engagement. Also, the main body is currently dense with scattered technical terms that could be better consolidated.
3. Just a minor point of clarification, I'd like to delve into the details of the conducted toy experiments. Developing a unified framework that frames functions as a sequential path towards a distinct model group is truly commendable. Nonetheless, there's scope for refining its presentation. Specifically, the introduction gave the impression that this was a broad generalization of model misspecification, which it doesn't appear to be.
Upon incorporating regularization terms during training, it's important to note that while the overarching model class remains constant, the subset of models that surface must adhere to the regularization-imposed limits. Consequently, the model's prime focus may deviate from merely minimizing the Bayes predictor loss.
This deviation could result from either integrating regularization terms into the optimization formula, thus modifying the loss characteristics, or from imposing these terms as constraints, effectively limiting the model class.
Considering these factors, I'd suggest positioning your model misspecifications as a tool to tailor the concept to real-world machine learning environments, rather than representing it as a broad generalization of the concept of model misspecification itself.
4. One of the method's most significant practical implications, in my view, is the potential for semi-supervised learning on unlabeled data - a scenario where RBPT is likely to outshine RBPT2. Given its importance in practical real-world settings, this deserves an additional set of experiments. An easy experiment to consider would be to add a large set of unlabeled data to the toy experiment.


Limitations:
1. In the RBPT methodology, model misspecification is conceptualized as the difference between $Q^*(X|Z)$ and $P(X|Z)$, and between $g^*(x, y)$ and $f^*(x, y)$. This deviates from prior methods where the misspecification is solely defined between $g_1^*$ and $f_1^*$, and $g_2^*$ and $f_2^*$. While the latter defines misspecification between functions, the former is defined on an entire distribution.
Notably, even if we posit that the disparity between the predicted $\hat{Q}(X|Z)$ and $Q^*(X|Z)$ is infinitesimal, the divergence between $Q^*(X|Z)$ and $P(X|Z)$ remains far from negligible. Furthermore, the effectiveness of the entire method hinges on this misspecification gap ($\Omega_{P,1}^{\text{RBPT}}$) being smaller than the Jensen's gap, which places stringent restrictions on its applicability. While theoretically, this method relaxes the bound on the expected total variation between $Q^*$ and $P$ and sets it to $\frac{\Omega_{P,2}^{\text{RBPT}}}{M \cdot L}$, the $M$ and $L$ coefficients will be pretty large in practice. On the other hand, while RBPT2 appears to circumvent this issue, its theoretical grounding isn't as robust as that of RBPT, thereby reducing its perceived solidity.
I believe this issue will become more prominent in real-world settings and the toy experiments do not reveal this core flaw of the method. Therefore, the authors should have mentioned this limitation or run extra experiments to show the effect of this gap.
2. A prominent constraint associated with RBPT2 appears to be its performance when scaling $Z$. Unfortunately, this aspect is not addressed within the main paper text. It would be beneficial to incorporate some commentary on this issue, as it pertains to the most plausible plug-and-play application. Furthermore, I harbor concerns regarding the potential for error from the initial regression to percolate into the subsequent one, which could potentially escalate the overall margin of error. This is pointed out in Appendix A.6, and with the empirical results acting strangely with scaling (I also mentioned in the “Questions” section), I am concerned as to how much this can hinder the performance of RBPT2.


Rating:
7

Confidence:
3

";1
zKjSmbYFZe;"REVIEW 
Summary:
This paper considers fairness in multi-class classification under the notion of parity of true positive rates - an extension of binary class equalized odds - which ensures equal opportunity to qualified individuals regardless of their demographics. We focus on algorithm design and provide a post-processing method that derives fair classifiers from pretrained score functions. 

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
- paper deals with an interesting problem
- paper is technically sound

Weaknesses:
- paper does not properly compare w.r.t. the state of the art 
- novelty of the proposal is not clear
- paper is hard to read and follow

Limitations:
Paper is very hard to read, follow, and fully understand. 

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper proposes a novel post-processing approach to reduce the true positive rate parity for multi-class classification problems. It is shown on two real world data to outperform an existing baseline in terms of accuracy and true positive rate parity.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed approach is novel and technically sound.

2. The proposed approach guarantees fairness by a sample complexity bound.

3. The proposed approach outperforms an existing baseline post-processing approach in terms of reducing TPR parity.

4. The presentation is clear.

Weaknesses:
1. Some related work, such as [45], is mentioned in the paper but not compared in the experiments.

2. Fairness in multi-class problems seems to be a rather trivial/incremental problem when there are plenty of approaches solving the fairness problems in binary classification and regression problems. Approaches from fairness in regression (e.g. Narasimhan et al., ""Pairwise fairness for ranking and regression"", 2020) should be easily applied to solve the fairness problems in multi-class classification. Please justify more for why Fairness in multi-class classification is a problem worth studying. Some approaches from fairness in binary-classification (e.g. Kamiran and Calders, ""Data preprocessing techniques for classification without discrimination"", 2012; Yu et al., ""FairBalance: How to Achieve Equalized Odds With Data Pre-processing"", 2023) can also be easily adapted to multi-class problems.

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
The work proposes a post-processing algorithm to achieve the equal opportunity constraint in multi class classification. The proposed algorithm takes arbitrary Bayes rule estimate and only requires additional unlabelled data. The authors derive finite-sample guarantees and perform empirical evaluation to support their claims.

I did not check the math, but, having prior experience in this area it looks believable.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper is well written and the proposed methodology is sound. It extends a rather long line of works on post-processing with unlabelled data.

Weaknesses:
In the context of the paper, I do not see major weaknesses from the methodological side, but rather remarks that are presented in the next part. 

From the theoretical part, I could mention that the devision by p_{a, y} with large number of protected attribute and classes can make the bound non-informative. 

I think that the expectation in the fairness guarantee is not well placed, I expected to have  $E[\Delta(h)]$. 

Limitations:
---

Rating:
6

Confidence:
4

REVIEW 
Summary:

This paper studies algorithmic fairness in multiclass classification setting. The fairness notion considered is parity of true positive rates (TPR) which is the multi class analog to equalized odds. The paper gives a post-processing algorithm which, given a score function, outputs a fair classifier. The paper then gives sample and time complexity guarantees and experimental evaluation on benchmark datasets. 

This paper furthers work studied Alghmadi et al., which according to the authors, is the only other post-processing method available for multi-class TPR parity. 

The paper gives a general post-processing algorithm that takes as input a score function and outputs a classifier that satisfies approx TPR party. The authors show that if you begin with the Bayes score function, their post-processing returns an optimal, fair classifier. They also give results showing that if the initial score function is not Bayes optimal but instead satisfies a decision calibration condition, then the post-processing is optimal among all classifiers that can be derived from the initial score function. 

The post-processing method consists of two parts. The first part of the process estimates the feasible region of TPRs and then finds the utility-maximizing TPRs that satisfy fairness constraints (either exactly via search if we know the distribution, or via a linear program if we are estimating the TPRs from data). The second step involves finding the hypothesis that achieves these TPRs, which either corresponds to a tilting - essentially a threshold - of the score function without randomization or a mixture of two models that lie on the boundary. 

Finally, the paper gives some experimental evaluation on three benchmark datasets that serves as a proof of concept of the post-processing and showed that it is competitive with other standard techniques (notably reductions). 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Fairness via post-processing in multiclass classification settings is not a focus on most prior work on algorithmic fairness, which makes this an interesting and novel contribution. 

Although restricted to TPR parity, the paper does a complete analysis of the topic under this fairness definition. The paper addresses both not knowing the underlying distribution and so possibly starting from a non-Bayes score function and also discuss estimating the parameters from finite samples. 

Weaknesses:
The paper gets a little notationally and technically dense in Section 3. While the presentation is still fairly clear, I think additional higher level exposition to describe in particular Step 2 of the algorithm could be beneficial to readers - maybe including additional description of a tilting. 


Limitations:
The authors address one of the main limitations of this work that their method is confined to only reducing TPR disparity and not other fairness notions. 

Rating:
7

Confidence:
3

";0
IL7F4soYyg;"REVIEW 
Summary:
This paper introduces a new pre-training strategy that takes cells as tokens and tissues as sentences. It also encodes cell-cell relations by leveraging the spatial information of cells acquired from spatially-resolved transcriptomic data. The proposed method achieves state-of-the-art performance in various downstream tasks.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- It's a good insight and more biologically meaningful to take cells as tokens and tissues as sentences instead of using genes as tokens and cells as sentences, as there's no sequential relationship among the order of genes.
- It's also an interesting idea to utilize SRT data to encode spatial information as the input to the transformer.
- Good reproducibility: the authors have provided code. 
- The paper is overall well-written and conveys the idea clearly.

Weaknesses:
- Though the model is pretrained, full finetuning is required when deploying to the downstream tasks. This is somehow in contrast to the idea of training a pre-trained model that can be easily adapted to downstream tasks by only tuning the task-specific layer. The drawback is 1) it requires a lot of computing resources and data, and 2) we don't know if the performance (i.e. representation learning ability) is obtained from the pretraining or just fine-tuning. 
- Following the above problem, the author should compare the performance w/ and w/o pretraining, and also the performance of fine-tuning the task-specific layer, to ablate whether the pretraining helps.
- The authors evaluated tasks like denoising, imputation, and perturbation prediction, but why not the most common and important task like cell-type classification? I think it's the most straightforward way to benchmark the performance of the proposed method.


Limitations:
Overall I find the idea interesting, but the descriptions of the method are not very clear and the experimental evaluations are not strong enough to support the claim.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposed a pre-trained model based on single-cell data motivated by the special characteristics (i.e., bag of genes structure, cell-cell relation, noisy) in cell-data, which viewed cells as tokens and tissues as sentences. This is different from the most existing pre-trained models treat genes as tokens and cells as sentences. The paper designed a new training paradigm based on transformer while introducing Gaussian mixture prior to handle data limitation.

Experiments over some representative down-stream tasks are conducted by fine-tuning the pretrained models to demonstrate the effectiveness of the proposed training paradigm.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1.pre-trained model over cell-data which considers the cell-to-cell relation and bag-of-genes structure and introduces Gaussian mixture prior distributions to handle noisy and limited data.

2.Extensive experiments were conducted over different down-stream tasks (including both cell-level and gene-level tasks) to demonstrate the effectiveness of the proposed pre-trained models.

Weaknesses:
The training procedure is inspired from masked language model, however, as stated in this paper, cell/tissues are different from text sequence, the technical details of pre-trained data process are not clear enough. This is very important due to that it is helpful to a) understand the characteristics of cell-data and b) repeat the experiments and utilize more related data to further enhance the performance of pretrained models.

Please refer to questions for details.




Limitations:
The setting of pre-training should be clear stated.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper proposes a scRNA-seq model that takes into account correlations between gene expression values in nearby cells in a tissue. The proposed model represents cells as tokens with a very simple embedding model. The tissue representation is modelled using self-attention between cells. The model is trained with a probabilistic objective where the transformer outputs represent the latents with a mixture distribution and then an MLP decoder is used to decode the expression values for genes in the cell from the latent and a batch encoding.

The model is pretrained using 9M cells and 2M spatially resolved transcriptomics cells. The model is tested on 3 tasks scRNA denoising, spatial transcriptomic imputation and perturbation prediction. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* Understanding cell-cell communication is a very interesting open problem.
* Task 2 zero-shot performance is on par with finetuning which seems promising. 


Weaknesses:
* I have doubts about the scRNA denoising results and how meaningful the comparison to single cell imputation methods. (see question 1 below).

Limitations:
none

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper presents a study on the problem of enhancing cell representation learning through pretraining. Inspired by the recent success of pretrained language models, the researchers explore the use of Transformer architecture to achieve improved cell representations.

In contrast to previous approaches that only consider modeling a single cell, the authors propose a novel model called CellPLM that takes into account a sequence of cells, enabling the capturing of inter-cell relationships more effectively.

To incorporate spatial relationships into the model, 2D position embeddings are introduced to enhance the basic Transformer architecture. Additionally, a mixture of Gaussian distribution is employed to model the latent embedding space, enabling the capturing of cell group information and mitigating batch effects.

The proposed CellPLM model is first pretrained using a combination of scRNA-seq cells and SRT cells. Subsequently, three downstream tasks are conducted to evaluate the effectiveness of the pretrained model.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The representations and foundation models of genes and cells proposed in this study have the potential to be highly interesting and useful in advancing AI applications in the field of science (AI for Science).

The model architecture and adaptations made from the vanilla Transformer model are sensible and allow for efficient modeling of cells. In particular, the joint modeling of cell groups convincingly addresses the issue of data sparsity at the gene level.

The empirical design chosen to validate the pretrained CellPLM using three different use cases is reasonable and provides strong evidence of its effectiveness. Whether in a zero-shot or fine-tuned setting, the proposed CellPLM consistently performs well. Additionally, the results of ablation experiments further demonstrate the significance of each component designed in the model.

Weaknesses:
One weakness of this paper is that most of the datasets and tasks considered in the experiments are focused on single-cell analysis. Although the paper introduces a novel aspect by jointly modeling cell groups and sequences, it would be valuable to observe how the proposed model performs on tasks that involve multi-cell information.

Additionally, it is commendable that the author provides both the mean and standard deviation of the runs. However, to further support the significance of the improvements, it would be more informative to conduct statistical significance tests on the small downstream datasets, in order to validate whether the observed improvements are statistically significant.

Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
The CellPLM paper presents a novel pre-trained model that encodes cell-cell relationships in spatially-resolved transcriptomic data. The authors demonstrate that their model outperforms existing state-of-the-art methods in various downstream tasks, including cell type classification, cell-cell interaction prediction, and spatial gene expression imputation. 

The paper is well-written and provides a clear motivation for the need to develop a pre-trained model that can capture cell-cell relationships in spatially-resolved transcriptomic data. The authors also provide a detailed description of the architecture and training procedure of their model, which is helpful for readers who are interested in implementing the model in their own research.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. In terms of originality, the CellPLM paper presents a new approach for encoding cell-cell relationships in spatially-resolved transcriptomic data. The authors leverage a pre-trained transformer framework that encodes inter-cell relations and adopts a reasonable prior distribution. 
2. In terms of clarity, the CellPLM paper is well-organized and easy to follow. The authors provide a clear motivation for the need to develop a pre-trained model that can capture cell-cell relationships in spatially-resolved transcriptomic data and provide a detailed description of the model's architecture and training procedure. The authors also provide a clear evaluation of the model's performance on various downstream tasks, which is helpful for readers who are interested in implementing the model in their research.

Weaknesses:
1. It is unclear to me why to use a Gaussian Mixture Latent Space to capture the information of distinct functional groups of cells. First, I do not understand the distinct functional groups of cells, which have not been explained clearly. Moreover, it seems that you can directly decode the masked cells like BERT without complicated latent space estimation. From Figure 4, the ablation study show that removing this part almost does not hurt the performance. 
2. Not the weakness (just suggestion), please use a clearer figure or vectorgraph (e.g., Figure 2, 4).

Limitations:
As mentioned in the discussion, the work does not compare with other pre-trained models, which may be very important.

Rating:
6

Confidence:
3

";0
fUZUoSLXw3;"REVIEW 
Summary:
The article under review presents results to show that untuned SGD may be less adapted than normalized versions of it when solving smooth non-convex problems.
In order to prove this, the authors present several results, from upper to lower bounds on smooth non-convex problems to find a critical point, without the knowledge of the smoothness parameter. They show that while SGD may suffer from the non-adaptivity of the step-size, its normalized version get rid of this problem. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The paper's description of the problem it wants to tackle is good, and the questions addressed are well introduced.
- While I find it a weakness too (see below), it is remarkable that the authors present both lower bounds and upper bounds on a lot of different settings.  
- The figures summarize well the main idea of the negative result on untuned SGD and the principle of the lower bound 
- The table helps to navigate in this hairy paper


Weaknesses:
*Main comments*

While the problem asked in the introduction on the difference between SGD and Adam (and its many variants) is an important problem where almost nothing is known, I find the answer of this article non-really convincing:
- The main phenomenon pinpointed by the authors is the presence of the constant $e^{\eta^2 \ell^2}$ in the bound to find a critical point: this factor is due to the fact that, initially, the step size is too big compared to the local curvature of the function while after some time, the step-size being a decreasing function $\eta_t = \eta/\sqrt{t}$, the step-size becomes well-conditioned. While some similar phenomenon may take place for some learning problems, I am not sure that such an analysis is the crux of the problem for the comparison between Adam and SGD. Maybe some experiments on non-toyish problem might help convince the reader (or at least myself): do we really see this $e^{\eta^2 \ell^2}$ popping out and eventually really slows down the convergence?
- The article claims to address, for practical purposes, and improve the theory from *bounded stochastic gradients* to *bounded variance* of the stochastic gradient: surely, theoretically it is a nice contribution, but it does not really serve to give an answer to the claimed question. Furthermore, even in this case, I think that the set-up is still not valid for the simplest least square case… so it does not seem to me as an incredible update.
- Finally, as stated in the strengths paragraph, the article addresses a lot of different setups, with different algorithms, sometimes stochastic, sometimes deterministic and it is very difficult to understand the true contribution of the article if the authors do not pinpoint them. Sometimes the reader, or at least I, was completely lost in what was known, both it terms of technique and/or result.  

*Minor comments*

- Theorem 1: $\Delta$ is not defined. I am surprised that there is no problem when $\eta$ is too big (no upper bound on $\eta$, and it does not diverge!), but this may be an artifact on the bounded variance stochastic gradient assumption.
- Theorem 2: Good lower bound. I like it together with the illustration, as we understand well the phenomenon. However, why not proving it for SGD? This is only a technical limitation I guess.

*Final precaution.* 

Overall, I have to say that I do not come from the community that analyses the general convergence of SGD for non-convex problems and its many normalized variants. Hence, it is hard to see what is the reel contribution of the authors. 



Limitations:
Already said in the paragraph above

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper shows mainly two things:
(a) SGD suffers from an exponential dependence on the initial stepsize if it is not tuned to be smaller than the learning rate. This exponential dependence is unavoidable.
(b) Methods with gradient normalization and running gradient sum normalization, such as Normalized SGD, AMSGrad, and AdaGrad, suffer no such exponential dependence on the smoothness constant.

The paper also presents a novel analysis of AMSGrad that removes the bounded gradients assumption.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The result on AMSGrad is new and a welcome addition to the literature.
2. The paper's emphasis on the benefits of normalization even in the deterministic setting is good, since this is a point quite overlooked in the community.

Weaknesses:
1. The result on the exponential dependence on the smoothness constant is a known consequence of another result in the literature. Under the assumption on the stochastic gradients $\mathbb{E} \|g(x)\|^2 <= 2 A (f(x)-f_*) + B \|\nabla f(x\||^2 + C$. Note that bounded stochastic gradient variance corresponds to $A=L$, $B=0$ and $C=\sigma$ (since bounded variance implies $E||g(x)||^2 <= ||\nabla f(x)||^2 + C <= 2 L (f(x)-f_*) + C$). The result of Theorem 2 in [1] gives for this choice ($A=L$, $B=0$, $C=\sigma$) a rate of $\frac{(1+\gamma^2 L^2)^K}{\gamma K} \delta_0 + L \gamma C$ where $\delta_0$ is the initial suboptimality. The lower bound is also known, see [2, Theorem 5].

[1] Khaled and Richtárik. Better Theory for SGD in the Nonconvex World. arXiv:2002.03329
[2] Vaswani, Benjamin Dubois-Taine, and Babanezhad. Towards Noise-adaptive, Problem-adaptive (Accelerated) Stochastic Gradient Descent. arXiv:2110.11442.



Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors investigate the behavior of untuned SGD in the smooth nonconvex setting and show a new result on the convergence rate of SGD w.r.t. to gradient norm, where there is an exponential dependence on the smoothness constant. They further argue that the exponential dependence is unavoidable through a constructed class of 1-dimensional nonconvex functions. The paper then examines NSGD, AMSGrad and AdaGrad and shows that the exponential dependence can be avoided through adaptiveness, albeit without any information about the problem parameters.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper offers an interesting theoretical perspective on the explosive gradient problem and the nonconvergence properties of SGD. The authors complement their theoretical results and ideas with numerical illustrations, and the paper is easy to follow. Their results seem well-justified, but I did not check their proofs in the appendix. I believe this work is of interest to the NeurIPS community.

Weaknesses:
1) The current state of numerical experiments seem preliminarily and is only done on one dataset MNIST on a small-network. I would like to see a more comprehensive investigation into larger practical networks, perhaps from [6, 24, 54].

Limitations:
Yes, the authors have addressed their limitations through the checklist.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper analyzes the complexity of finding an $\epsilon$-stationary point for untuned SGD and compares that with three families of adaptive methods - NSGD, AMSGrad and AdaGrad. Compared to previous convergence analysis results for tuned SGD and Adaptive methods: this work gets rid of several assumptions that hides the true convergence behaviour of these algorithms. Specifically, the authors do not assume the step-size for SGD to be dependent on the smoothness parameter (hence untuned) and do not assume bounded gradients for the adaptive methods. These leads to an interesting comparison for convergence of these algorithms. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1) The authors show that untuned SGD converges to an $\epsilon$ stationary point in $O(e^{\eta^2 l^2}\epsilon^{-4})$ iterations. Although this algorithm does have optimal dependece on $\epsilon$ , it has a disastrous exponential term wrt the smoothness parameter $\eta^2 l^2$, Hence, the assumption on bounded gradients or chosing the $\eta$ to depend on $l$ is problematic, because we may not have prior knowledge of $l$. They show that even for a smooth 1D-function, the assumption of bounded gradient is problematic. This is indeed true and the experiment in figure-1 supports this claim. 

2) Adaptive gradient methods adjust their step-size based on observed gradients and hence can decrease the stepe-size when encountered with a large stepe-size preventing blow-up. This work does not assume bounded gradeint assumption for these methods and show that the convergence rate does not exponenetially depend on the smoothness parameter making it more stable than untuned SGD. 

The core strength of the paper lies in removing assumption on bounded gradients revealing true dependency of these algorithms on smoothness parameter $l$ which highlights the advantage of adaptive methods over untuned SGD. 

Weaknesses:
Disclaimer: I am unfamilair with recent developments in this specific diretion. But still i really enjoyed reading this work and I feel it improves over existence convergence results. I have a few questions that i encountered but I don't list them as major weaknesses:

1) the constructed function $f(x) $ in Figure-2 does not look so. Is there an extension of the function outside the segment-1. If so, the author should mention that. Although the equation for sregment-4 and segment-1 are still provided, segment-2 and 3 are still missing. 

2) From theorem-1, the main reason untuned SGD may blow up is that some initial $\eta \geq \frac{1}{l}$. But in practice SGD also converges with a small constant learning rate which does fall into this regime and in this regime it does not blow up. However, it is mostly observed that initial large learning rate SGD performs better in terms of generalization [1] . If practitioneers use a small enough (but contant) leanring rate $\eta \leq \frac{1}{l}$, then the whole issue of gradient blow-up can be avoided. Even in such practical cases, no prior knowledge of $l$ is required to select step-size $\eta$. So is it true that achieving generalization is the bottleneck in chosing step-size as large as possible ?



[1] Li, Yuanzhi, Colin Wei, and Tengyu Ma. ""Towards explaining the regularization effect of initial large learning rate in training neural networks."" Advances in Neural Information Processing Systems 32 (2019).


Limitations:
i believe the whole problem of gradient blow-up can be avoided by just using small enough practical $\eta$ according to Thoerem-1. But that would hurt generalization in overparameterized networks. Hence, I believe comparison of constant step SGD $\eta$ and other adaptive gradient methods should also be done in terms of generalization and not only convergence. This would give us the full picture. 

Rating:
7

Confidence:
2

REVIEW 
Summary:
This work analyses the rate of SGD and other adaptive SGD methods in reducing $\\mathbb{E}\\|\\nabla f(x_t)\\|$, where $f$ is non-convex $L$-smooth, and shows that SGD has an exponential dependence on $L$ when the stepsize is not properly tuned, while other adaptive methods do not incur this exponential dependence.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The analysis presented in this work is fresh and interesting. The authors make a compelling case that removing the bounded gradient assumption is perhaps essential in revealing a crucial advantage of adaptive SGD methods over standard SGD.

As I explain in the weakness section, I think the qualitative idea behind the described phenomenon is known. However, the quantitative results are, as far as I know, new and, in my opinion, very interesting. I think the qualitative understanding gained by the quantitative analysis is valuable, so I vote for the paper to be accepted.

Weaknesses:
At a high level, one can argue that the result was ""known"" in the following sense.

When a LR schedule of $\\eta_k=\\eta/\\sqrt{k}$ is used, $\\eta_k$ will be large for small $k$. It is known that SGD exhibits divergent behavior when the stepsize exceeds $2/L$, so SGD will initially diverge. The worst-case amount of divergence should be exponential in $L$. Once the $\\eta_k$ diminishes to a level below the divergent threshold, then the algorithm needs to recover from the initial exponential divergence, hence the rate.

On the other hand, adaptive methods should not exhibit such divergent behavior (at least not to an exponential extent). Therefore, there is no initial exponential divergence, so there is no catch-up to play at the later stage of the algorithm.

I think it would be worthwhile for the authors to discuss the view that the exponential constant corresponds to the amount of catch-up SGD needs to make.


Limitations:
.

Rating:
7

Confidence:
4

";1
B4xF1wfQnF;"REVIEW 
Summary:
Consider the classical statistical risk minimization framework with stochastic gradients, and propose new lower bounds on the optimal time complexity. Also propose a new method Rennala SGD that attains the lower bound. This work makes the case for asynchronous methods.

In particular, this work proposes a new oracle-based lower bound on the optimal convergence rate in time, rather than iterations, and develops an algorithm (essentially straggler-based SGD) that achieves this bound in the shared-system/parameter-server setting.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
**Significance**
This work takes a step towards developing more practical asynchronous optimization methods, and providing convergence rates and complexity bounds that more closely reflect practical considerations of convergence time, rather than iterations.

**Originality**
The proposed method is not entirely novel (e.g., see Dutta et al., 2018 or related works on straggler-based SGD methods that ignore computation from stragglers), however the new lower bounds, problem formulation, and analysis are of sufficient novelty, to the best of my knowledge.

**Clarity**
The work is well written and easy to follow.

Weaknesses:
Lacking empirical evaluations, the only such experiments are on quadratics, buried deep inside the appendix. While these plots are compelling, I would encourage the authors to consider small-scale numerical results on other machine learning workloads.

Another weakness of this work is on assuming a fixed computation time for each worker. Such models are highly restrictive and impractical. I would encourage the authors to consider ways for extending their framework to arbitrary, but bounded worker delays. I would appreciate a discussion on how such lower-bounds (or the optimal method), may change if these bounds were overly conservative and long-tailed.

Limitations:
Limitations of the work are not adequately addressed. There are for instance, many assumptions that are taken for granted in the analysis that may limit the applicability of the findings to practical settings... namely the fixed delay model per worker, and the relationship between the batch-size S and the number of workers n. I am curious how findings would change in settings where S >> n or S << n.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes a protocol that generalizes the classical oracle framework approach. Using this protocol, it establishes minimax complexities for parallel optimization methods that have access to an unbiased stochastic gradient oracle with bounded variance.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The motivation of this paper is clear. 
- The theoretical analysis seems sound.

Weaknesses:
I am not an expert on this topic of this paper.

1. The challenge of theoretical analysis should be made more clear, which highly relates to the contribution of this paper.
2. The empirical study lacks.
3. The organization of this paper is strange. E.g., related work is located in 6.1, and conclusion section lacks, etc.

Limitations:
none

Rating:
4

Confidence:
2

REVIEW 
Summary:
This paper studies time complexity of parallel stochastic optimization, establishes lower bounds, and proposes algorithms achieving matching complexity results.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Based on my reading of the main paper (I haven't read the proofs), this is a solid theoretical work. The theoretical results improve widely used asynchronous SGD, and are even a bit surprising, especially the fact that ignoring some computations actually helps.

Weaknesses:
General comments:
- The organization of the paper is a bit reader unfriendly. The assumptions are mentioned at multiple places, starting with page 1, but are formally introduced almost at the end (page 8). This makes the reading tedious.

Minor comments:
- line 16, shouldn't the codomain of $f$ be $\mathbb R$ rather than $\mathbb R^d$?
- line 39: $L, \Delta$ have not been introduced so far in the paper
- Async. SGD (after line 52): step 2 gives the impression that only homogeneous data distribution across devices is considered in the paper. This is true for the main paper, but since the authors do have heterogeneous case results as well, they might consider reflecting that here.
- Lines 98-99: ""this approach is not convenient"" Maybe giving a short intuition why (which becomes clear on the next page), would help the reader.
- Typo in line 212

Limitations:
n/a

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper studies the minimax complexities of distributed asynchronous stochastic optimization methods. By extending the oracle framework previously used in the literature, it establishes new (lower) lower bounds for parallel optimization methods. Based on the insights from their proof, they propose a new algorithm meeting these bounds. Notably, it is provably faster than previous synchronous and asynchronous methods in the homogeneous case, which is verified experimentally.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
* This paper naturally extends the oracle protocol of previous work to cater for analysing parallel methods instead of sequential ones, which allows for a finer analysis of distributed methods.
* Efforts have been made in the writing to explain the introduced novelties step by step, which greatly helps understand and position the contributions.
* The insight taken from their new complexity proof that “ignoring stale gradients might actually help to converge faster” is interesting and highly relevant to the community, as it led to the development of a new state-of-art optimal method, which has provably and experimentally faster convergence rates than previous asynchronous algorithms in the homogeneous case.


Weaknesses:
* Many notations are introduced, which makes the paper a bit cluttered and cumbersome to read at times.
* No experiments comparing Renata & Asynchronous SGD to minibatch (synchronized) SGD are made to confirm the “provably better rates” claimed Section 8. 


Limitations:
*

Rating:
7

Confidence:
4

";1
IQ6GI7fM2z;"REVIEW 
Summary:
This paper investigates the issue of noisy gradient-based explanations caused by high-frequency content. The authors empirically identify that several down-sampling operations, such as MaxPooling or stride convolution, could be the primary source of these high frequencies. To address this, they present FORGrad, which removes high-frequency content in gradients by leveraging low-pass filter on the Fourier spectrum. The results on three faithfulness metrics further show the effectiveness of FORGrad in improving existing gradient-based explanation methods.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well-written, and the motivation is sound. 
2. The analysis regarding the high-frequency noises in gradients is interesting and could benefit the further designs of explanation methods.
3. The proposed FORGrad achieves promising results on three faithfulness metrics, potentially improving existing gradient-based methods.


Weaknesses:
1. Throughout the paper, the term “High frequencies” is extensively employed to describe the phenomenon in gradients. However, how to clearly define “high” in actual deployment is not well explained. This could become especially problematic considering the significant variations in model architectures and datasets, thus degrading the significance of this work.
2. The argument, “High frequencies are just noise in the gradient,” is too strong. The analysis in Section 3.2 can only show that the high-frequency information has less effect on model decisions. This does not necessarily mean that such high-frequency information is always noise. Moreover, the perturbation-based analysis also has many limitations, e.g., the OOD problem [1]. It’d be better if the authors could provide another set of experiments to validate their findings.
3. It is nice to see that FORGrad could co-work with many gradient-based methods. However, in many cases, the improvement introduced by FORGrad is minor/marginal, e.g., Int.Grad* on ConvNeXT. It thus remains unclear whether FORGrad is genuinely effective or merely a result of randomness.
4. The source of high-frequency gradients is not systematically investigated. The analysis in Section 3.3 only covers two mechanisms, i.e., downsampling and training. This narrow focus fails to provide substantial evidence for the claim that these mechanisms are the main reasons, as other relevant factors are not adequately discussed.

[1] Peter Hase, Harry Xie, Mohit Bansal. The Out-of-Distribution Problem in Explainability and Search Methods for Feature Importance Explanations. NeurIPS 2021.


Limitations:
NA

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper analyzes the two types of attribution methods: gradient-based/ white-box methods and prediction-based/ black-box methods. The authors observe that the faithfulness of black-box methods surpasses the white-box methods despite white-box methods accessing the internals of the classifier being explained. The authors suspect the presence of noise in the gradients to be a contributing factor to its limited faithfulness. They further perform Fourier spectral analysis to examine the distribution of attribution signal frequencies. This analysis sheds light on the existence of high-frequency noise signals in the gradients, a by-product of pooling and stride operations, which are propagated as attribution signals. The obtained Fourier spectrum is filtered using a low-pass filter with an optimal cut-off threshold learned based on optimizing the Insertion and Deletion metrics, which are proxies for the faithfulness of the explanations. This yields a faithful explanation from the white-box methods surpassing the faithfulness that black-box methods achieved earlier.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
* The methodology is effectively presented, incorporating suitable figures and accompanying text that elucidate the significance of these visuals. The paper follows a systematic approach in introducing concepts, starting with the motivation, followed by the hypothesis, supporting evidence, and ultimately presenting a solution to the problem. This coherent structure enhances the elegance of the methodical exposition.

* The analysis of the gradient signals is sound, and the link between noise in the gradient signals and explainability is adequately established. 

* The solution’s strengths are its simplicity - performing a FFT and identifying a suitable frequency threshold; and modularity - can be integrated with many whitebox explainability approaches. It is a simple post-processing strategy to extract high fidelity explanations.

* The experiments are somewhat comprehensive - with comparison against some of the prevalent explainability approaches.


Weaknesses:
- *Possible misrepresentation of the related literature:* Reference [3] is described as both a black-box method and a white-box method in different sections. This inconsistency is also reiterated in Line 76. Similarly, in Table 1, Grad CAM and Grad CAM++ are categorized as prediction-based/black-box methods despite their reliance on gradients. These methods employ gradient backpropagation (typically) up to the last convolution layer to generate the CAM, which contradicts the authors’ characterization as purely black-box approaches.

- *Missing related literature and experiments:* The analysis of attribution-based explainability approaches overlooks significant contributions that introduce litmus tests for these methods, such as the work by Adebayo et al. NeurIPS 2018 and Sixt et al. ICML 2020. Sixt et al. explores the convergence of activations in saliency maps of gradient-based techniques when subjected to litmus tests proposed by Adebayo et al. It would be intriguing to investigate potential parallels between the convergence analysis presented by Sixt et al. and the spectral analysis conducted in the current paper. The application of Adebayo et al.'s litmus tests are necessary, and a comparative evaluation of the results before and after applying the proposed frequency cut-off indicating an increase in the faithfulness of the explanations could further strengthen the contribution. This is important considering the sweeping claim of the paper that gradient based attribution is reliable.

- *Missing experiments:* While the authors’ have conducted quantitative evaluation to highlight the effectiveness of the solution, the absence of qualitative evaluation is stark. It is evident from the Figures that the attribution map generated looks ‘better’ after the frequency cutoff. However, whether this attribution map measures up against other approaches (both white box and black box approaches) qualitatively needs to be checked. Human subject experiments to this effect are necessary.
 
- *Disconnected theoretical analysis:* The theoretical analysis seems disconnected from the empirical decisions in the current draft. The need for the theoretical analysis has to be well-motivated through a thorough rewrite. 

**Minor suggestions**
- References [1] and [2] are just the same, with just a difference in the year. The author may need to check their bib source to remove the irrelevant bib entry and modify the paper just to retain the relevant entry.
- Line 84 activations have been mistyped as activities. 
- Line 96 appears like a colloquial lingo. Please rewrite to make the statement formal to the research community. 
- Please check for possible typos in Line 244. 


Limitations:
An analysis of the limitations has been honestly presented

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper explores the differences between prediction-based and gradient-based attribution methods for explaining the decisions of deep neural networks. The authors observe that these two approaches produce attribution maps with distinct power spectra, with gradient-based methods exhibiting more high-frequency content. This discrepancy raises questions about the origin and relevance of high-frequency information and why its absence in prediction-based methods leads to better explainability scores. By analyzing the gradient of visual classification models, the authors identify downsampling operations in Convolutional Neural Networks (CNNs) as a significant source of high-frequency content, potentially due to aliasing. To address this issue, they propose applying an optimal low-pass filter to improve gradient-based attribution methods, leading to enhanced explainability scores.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. `Simple yet Effective Solution`: One of the key strengths of the paper is its proposal of a straightforward yet highly effective solution. By applying a frequency cut low-pass filter to the attribution maps, the authors successfully remove the high-frequency noise and improve the interpretability of the results. This simple intervention offers a practical ""plug-and-play"" approach that can provide meaningful and understandable attributions for human interpretation. The demonstrated enhancements in explainability scores serve as compelling evidence of the efficacy of this approach, showcasing its practical value in real-world applications.

Weaknesses:
1. `Flaw in Observation`: The paper's strong observation regarding the higher frequency content in gradient-based methods compared to prediction-based methods may be misleading. This difference is primarily attributed to the **distinct instances that each method focuses on, rather than being inherent properties of the attribution categories themselves**. Prediction-based methods typically concentrate on `image patches, super-pixels, or high-level features of specific receptive fields`, whereas gradient-based methods calculate derivatives with respect to each pixel individually. If gradients were calculated with respect to patches, super-pixels, or features, the resulting attributions would also exhibit dominance in low-frequency components.

2. `Naiveness in Theoretical Justification`: The paper's theoretical justification appears simplistic and lacks depth. It adopts a traditional signal processing approach, assuming the presence of noise with a specific form (such as uniform or Gaussian) and demonstrates the efficacy of a linear low-pass band cut filter. However, several critical questions remain unanswered. Firstly, the paper fails to address **why gradients with respect to the input are inherently noisy and exhibit high variance**. This noise can be attributed to the non-linearity and no-smoothness[A] of the network itself, and the downsampling operation might be just one contributing factor among others. Secondly, the derivation of the proposed solution seems to be a mere reproduction of standard textbook content on signal denoising, without providing any specific insights or considerations tailored to the problem at hand.

3. `Methodological Simplicity`: The application of a low-pass filter to gradients may be considered too simplistic for publication in a prestigious conference like NeurIPS. Similar techniques, such as those presented in reference [5] and other variance reduction methods, have already been established in the field. The paper does not introduce any novel or sophisticated methodologies, which might limit its overall contribution and impact.

[A] Wang Z, Wang H, Ramkumar S, et al. Smoothed geometry for robust attribution[J]. Advances in neural information processing systems, 2020, 33: 13623-13634.


Limitations:
N/A

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper investigates why gradient-based attributions contain more high-frequencies compared to prediction-based
attribution. The authors argue that these high-frequency components are not relevant for the model and can
therefore seen as noise. Different saliency methods are analyzed on three different image models (ConvNeXT, ResNET, ViT). They further investigate the source of the noise and find that the downsampling-block is the the main cause for this.  Finally, the authors find that removing the noise from saliency maps yields an improved performance on
various explanation metrics.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- In general, I think the paper is novel. The noise in attribution maps and its effect on interpretability metrics
are not completly understood yet.
- The paper analyzes the causes of the noise in detail
- FORGrad makes it easy to interpolate between detailed and smooth attribution maps.
- The focus of the paper is clear and generally well executed.

Weaknesses:
- **Missing relevant work**: The work does not discuss the findings of [(Balduzzi et al. 2017)](http://proceedings.mlr.press/v70/balduzzi17b/balduzzi17b.pdf). This work investigates the effect of Residual connections on the noise of neural networks. Although this work has a focus more on saliency maps, the findings
of (Balduzzi et al. 2017) should be considered in the discussion.


- **How relevant are high-frequencies?** : ""As anticipated, our observations reveal that the curves exhibiting reduced high-frequency content (from σ < 224 to σ = 10) closely align with the one of the non-filtered gradient (σ = 224)."" (line 148). However, the distance between the curves σ < 224 and σ = 10 are quite substantial in Figure 4.
For ConvNeXT, the distance is almost one magnitude (the log-scale makes it look closer together).
Could you please report the maximum error between the curves in the supplementary material?

- **The conclusion is a bit off**:

  > (333) Overall, our work leads to a surprising result – that the almost forgotten gradient-based methods turn out to contain all the information needed to provide a faithful explanation of a model’s decision and that they can be as interpretable as the newest methods.

  I disagree with multiple points in the conclusion:
  - Gradient-based attribution almost forgotten? The integrated gradients paper received over 1000 citations last year (500 citations this year already). (source: semantic scholar)
  - I would be cautious about the faithfulness claim. It could also be that the metrics are biased and prefer a smoother attribution map.

- **Why not change the model?**: I would consider the raw gradient to be just the most faithful local representation of the model. If you prefer a smoother version, would it not make more sense to make changes to the underlying network architecture? This proposed methods feels like treating the symptoms instead of resolving the core issues.


To summarize, I am cautious about the main-take away that apply smoothing to gradient-based attribution methods makes them faithful. It would be great to also analyze if the faithfulness metrics are biased toward smooth attribuiton maps. Furthermore, I want to see the maximum error is between the different sigma-curves. It might be more significant
than suggested by the authors. A large error would questions the assumption that a smoothed gradient is more faithful.


Limitations:

The limitation section reads very technical and misses a very important point: the Deletion, Insertion, and Fidelity metrics are only proxy metrics for interpretability. Performing well on them does not mean that the attribution map is actually interpretable. Only a user study could answer this question.

Rating:
4

Confidence:
3

";0
gSyjaunurQ;"REVIEW 
Summary:
- This study proposes a new framework to combine neural coding concepts of information transmission and probability density modeling.
- This framework is based on an even code principle where the output response density strives to be even, given some arbitrary input density.
- The authors show that this coding principle produces sensible bases for low-dimensional inputs, and orientation-tuned filters for natural image patches.
- While conceptually straightforward, it is unclear to me whether this study provides unique insight into sensory coding in neural populations.

UPDATE: Sep 1, 2023. I have read the rebuttal, and maintain my score (see details below).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The study is clearly presented.
- The concepts of max/min entropy on the output and input densities are conceptually simple to follow.
- The numerical experiments are reasonable.

Weaknesses:
- This paper begins with what seems like a false dichotomy of information transmission vs. sensory probability density modeling. Indeed, from a pragmatic point of view, how can one guarantee optimized information transmission without having a good density model of the signal to be transmitted? There exists literature in this area (see questions section), and the motivation/framing of this present study is concerning.
- There is a bit of a conceptual leap from 1 or 2 pixels to full image patches, with additional complexity and machinery introduced. The described rationale seems reasonable enough, but it is unclear whether the two-pixel orthogonal case can provide adequate intuition for the multi-dim case. Would a 2D non-orthogonal example be illuminating at all?
- Unclear to me whether these results, which rely on binary coding provides theoretical insight for real neural coding. Spikes are inherently binary, yes, but typically spike counts/rate are what is considered the informative variable in neural coding.

Limitations:
There was no discussion of limitations. Unclear to me what the drawbacks are of this approach compared to existing literature.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper presents a method for the representation of elementary natural images, based on the observation that classical studies in computational neuroscience focus mainly on methods to improve code efficiency, but that this could be complemented by a study of probability density modeling between neighboring pixels to improve image representation. This work consists in studying a coding principle based on a probabilistic representation and its formalization in a form of variational optimization. The paper presents the elementary method for a single pixel, then extends it to two pixels, and applies it to  small images extracted from natural images. This method is enhanced by a heuristic that allows  to formulate a cost function and thus derive an optimization algorithm. The results allow  to numerically validate this principle by deducing output statistics, as well as the emergence of local contour detectors.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
A major strength of this paper is that it derives the image representation algorithm from fundamental principles of machine learning, particularly probabilistic representations. In this way, it rigorously defines the problem of establishing dependencies between the luminance values of neighboring pixels.


Weaknesses:
The first limitation of this paper is that it applies to very elementary signals, i.e. a pixel, a pair of pixels, or small images of dots. As the initial aim of the paper is to understand the computational functioning of the biological networks that underlie the efficiency of vision, this approach is extremely caricatural, and dismisses many fundamental aspects, such as the largely parallel processing of large images, the use of large neural networks, or the ability to process multimodal images, in color or in motion, or more generally hierarchical processing that can be forward, but also modulated by feedback signals. Finally, the results that have been obtained, for example for the detection of local elementary contours, are difficult to interpret quantitatively and seem very preliminary.

Limitations:
Finally, these questions about the paper reveal the main limitations of this work.

In particular, the introduction to the paper presents at length principles that seem very general, such as Shannon entropy, and the rest of the paper does not sufficiently highlight the novelties that are brought forward. This brings to light a main limitation of the paper, which is the fact that the propositions that are put forward are very ambitious, but the results are applied to very limited situations.


Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper explores the relationship between the information theory approach and the probabilistic generative model approach in the context of understanding neural coding. The author suggests that maximizing the information-carrying capacity of output channels and modeling the input probability distribution can be pursued as independent dual objectives. To investigate this hypothesis, the author begins by examining a one-pixel system, followed by a two-pixel system, gradually progressing to 2D image patches. The resulting codes obtained for the images exhibit similarities to edge detectors and orientation-selective neurons in V1, akin to many efficient coding models developed over the past two decades.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The presentation is reasonably clear.  It is rather interesting that the author begins by examining a one-pixel system, followed by a two-pixel system, gradually progressing to 2D image patches. 

Weaknesses:
While the idea that both information transmission and probabilistic modeling of the images should be taken into consideration simultaneously might be new,  and is sufficient to learn edge detectors and orientation-selective neurons, the author has not established it is a necessary condition. In fact, literature in the last thirty years (from Law and Cooper's to Olshausen and Field and many others)  that such codes can be learned based on either one of the criteria. 

It is surprising that the V1 neural codes were assumed to be sparse binary codes. What is the evidence?  The distribution of output values as shown in Figure 2a has not been observed biologically.  This brings the Even Code hypothesis into serious question. 


Limitations:
Societal impact not discussed.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The authors studies simple of neural encoding. The question is whether two
distinct goals, accurate transmission of information and learning the
distribution of environmental stimuli can be achieved simultaneously. The
authors argue that yes, it can, using the key assumption of a uniformly
partitioned input space. The coding principle of the authors is finally applied
to image patches, where it yields edge-like features.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
- The author studies an important question, namely simple coding schemes that
  reproduce filters that resemble those of deep convolutional networks or parts
  of the visual system.
- The author develops intuitions in simple toy models before moving to
  applications on real images.
- The filters shown in Fig. 3 bear a striking resemblance to the filters of a
  trained VGG model (although I have some questions on the methodology, see
  below)



Weaknesses:
I found the article confusing to read in a few places. For example, early on,
the author states that ""maximising the rate of transmission"" is equivalent to
maximising the entropy of the output distribution $H_Q$. I would think that what
you transmission of information requires maximising the mutual information $I(X;
Q)$ between the distribution over inputs and outputs. (around eqs 1 + 2; note
that the notation is rather confusing here, using lower-case $p$ for the
distribution over input stimuli $x$, and capital $Q$ for the distribution over
output states $y$). Why are you maximising simply the entropy of the
distribution over outputs?

Similarly, in the section on the even code principle, I'm confused by the
question of how the IPU models the input distribution. The way I read Sec. 2,
the IPU is considered a function of the stimuli $y=f(x)$ -- in that sense, it
doesn't model the input distribution, we cannot sample from it. It can give a
more or less faithful representation of $x$, as measured for example by mutual
information if the mapping is probabilistic, 

As you then move on to learn two pixel distributions, I'm confused about your
use of MLPs. MLPs are powerful neural networks, but you seem to use them to
""learn"" to partition the input space into equal partitions - is this not
possible by just writing down a simpler model?

Given my trouble understanding the first few sections, I cannot competently
comment on the experimental results - while the filters obtained by the authors
do bear a striking resemblance to the filters of a VGG network, I don't really
understand how the author obtained them. Some additional clarifications would
therefore be more than welcome.


Limitations:
See above

Rating:
3

Confidence:
3

";0
NG4DaApavi;"REVIEW 
Summary:
This paper introduces two variants of MCTS designed to perform more exploration during search than the commonly used UCT search policy, while resolving an issue with a previously introduced MCTS variant, Maximum ENtropy Tree Search (MENTS), which was designed to explore more than UCT, but which does not converge to the optimal policy / value function with an increasing number of MCTS trials.

To address this issue, the authors modify MCTS in two different ways. In the first variant, Boltzmann Tree Search (BTS), the authors use a Boltzmann search policy during the selection/search phase of MCTS, instead of using UCT, but perform a regular Bellman backup to compute state and action values. In the second variant, Decaying ENtropy Tree Search (DENTS), they modify MENTs to separately keep track of the policy entropy and the (unmodified) value functions, allowing them to slowly decay the entropy contribution over time, while using the entropy-augmented value functions to drive exploration during search. Both variants of MCTS are consistent -- i.e. they value function they compute for the root node converges to the optimal value as the number of MCTS trials increases, and the simple regret decays exponentially with respect to the number of MCTS trials.

The authors evaluate BTS and DENTS against baseline methods on both toy MDP problems, and in a 9x9 Go round-robin tournament. They find that BTS and DENTS perform similarly or slightly better than existing baselines on the toy problems, and consistently outcompete other methods (modified to make use of neural-networks) in the Go tournament.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper clearly explains an existing problem with MENTS not converging to the optimal root-node policy with increasing trials  (e.g. using the D-chain problem to illustrate the point).
- The ""fix"" that this paper proposes to MENTS is intuitive and clearly explained.
- The theoretical results appear to be sound, with extensive proofs (though I did not check these carefully).
- The empirical results demonstrate that BTS & DENTS can improve upon baseline search policies like UCT and MENTS in non-trivial problems (i.e. Go), while generally performing no worse than the baselines on simpler problems.

Weaknesses:
- The motivation for increasing exploration during planning/search could be better explained.
  - In particular, I think the authors could better highlight that UCT was justified in the context of selecting actions in the real-world, and that per Tolpin & Shimony et al [1, 2], using UCT in *simulation* doesn't make as much sense, since computational actions are less costly than actual actions.
  - This better motivates the use of a Boltzmann search policy, since exploring more when ""thinking"" could ultimately lead to better exploitation when finally acting / executing.
  - Similarly, the authors could better explain their choice of using simple regret (instead of cumulative regret) as the metric for an MCTS algorithm, following the discussion / reasoning in [1, 2]. Cumulative regret doesn't matter if the cost of computation is effectively zero (and only matters slightly if computation cost is much smaller the actual cost of acting).

- I don't think this should count very much against the paper, given the extensive effort at formally proving the stated results (which I think is valuable, and should be recognized), but it's arguably the case that BTS are DENTS are fairly incremental and straightforward extensions of MENTS.

[1] David Tolpin and Solomon Shimony. Mcts based on simple regret. In Proceedings of the AAAI Conference on Artificial Intelligence, volume 26, pages 570–576, 2012.

[2] Nicholas Hay, Stuart Russell, David Tolpin, and Solomon Eyal Shimony. Selecting computations: Theory and applications. arXiv preprint arXiv:1408.2048, 2014.

Limitations:
One benefit of computing the max entropy policy, as MENTS does, is that it might help with exploration when *acting* (not just planning) --- this is perhaps helpful when the agent doesn't have an accurate model of the world, and needs to learn it over time. Adding some discussion on this point would be good, since it seems like a potential limitation of 

Apart from that, the authors have addressed the limitation of BTS and DENTS that the require $O(|\mathcal{A}|)$ backups relative to UCT, which may hinder performance when actions spaces are large.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes two new algorithms, Boltzmann Tree Search (BTS) and Decaying Entropy Tree Search (DENTS), based on the Maximum Entropy Tree Search (MENTS) method. The authors prove that in certain situations, MENTS fails to converge to the optimal policy. They analyze BTS and DENTS using simple regret and show that these algorithms will recommend the optimal actions. Finally, the authors evaluate various MCTS algorithms, where BTS and DENTS significantly outperform other MCTS algorithms.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is well-structured, with clear explanations of the concepts and methods used.
2. The paper identifies the limitations of MENTS and proposes two new algorithms, BTS and DENTS, to address the issues of MENTS. The authors conduct theoretical analysis and perform experiments to validate their proposed solutions.
3. The experiments show that the proposed algorithms outperform other MCTS algorithms.

Weaknesses:
It is not quite convincing for the baseline program used in the Go experiments. The Komi in 9x9 Go is usually set to 7 (a balanced setting for both players). Black will have an advantage when Komi is set to 6.5. In addition, in Table 1, does PUCT have the same performance (strength) as KataGo? Why not compare the AR-* methods to KataGo directly? Besides, it is quite confusing why the author uses 128 CPU threads. To the best of my knowledge, this is not the best machine configuration for running KataGo. Using 128 CPU threads may even hurt the performance of KataGo. Moreover, in line 266, the authors said they added Dirichlet noise at the root node. In AlphaGo Zero, there is no need to add Dirichlet noise during evaluation.

Limitations:
The authors addressed their limitations in section 5.3.

Rating:
5

Confidence:
3

REVIEW 
Summary:
<< I have read the authors' rebuttal and have raised my score based on the discussion >>

The paper addresses challenges in online tree-based search algorithms, such as UCT, which have shortcomings such as being prone to local optima and a slower convergence rate to the optimal action. An alternative approach, Maximum ENtropy Tree Search (MENTS), despite theoretically optimal performance, has practical limitations like sensitivity to the temperature parameter, a tendency to over explore, and increased computational expense. The authors present two new algorithms to address these issues: Boltzmann Tree Search (BTS) and Decaying ENtropy Tree Search (DENTS). BTS employs a Boltzmann search policy, focusing solely on reward maximisation, whereas DENTS introduces entropy backups that can be used with varying weighting in the search. Both algorithms preserve the exploratory benefits of MENTS, and importantly, they consistently converge to the optimal reward maximising policy. Key contributions of the paper include highlighting the misalignment of the entropy objective in MENTS with reward maximisation, introducing BTS and DENTS, providing an analysis of MENTS, BTS, and DENTS through the lens of simple regret, and demonstrating the improved performance of BTS and DENTS over other MCTS algorithms in specific environments and games.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Novelty: The presented Boltzmann Tree Search (BTS) algorithm is a variant of the Maximum ENtropy Tree Search (MENTS), substituting the soft update value in equation 13 with the max operator in equation 19. The Decaying ENtropy Tree Search (DENTS) proposes a novel method for decaying exploration to ultimately converge to the optimal solution, an aspect MENTS struggles with due to its focus on maintaining entropy in the search policy. The novelty of this paper's contributions, while discernible, seems to be limited.

Clarity: While the paper is generally well-written, comprehension is somewhat impeded by the multitude of mathematical symbols and technical jargon used throughout. An effort to streamline and elucidate the language could significantly enhance readability.

Significance: The paper's experimental results highlight DENTS' robustness to the adjustment of the temperature hyperparameter, a critical element in MENTS. Moreover, the simplicity of the proposed changes augments their potential for practical adoption in the field.

Weaknesses:
W1: The novelty of the work seems somewhat limited. It remains unclear whether the improvements result from the incorporation of entropy in the case of DENTS or from the use of hard updates instead of soft updates in BTS.

W2: The paper emphasizes the sensitivity of MENTS to the temperature hyperparameter. It is true that many online search algorithms rely on hyperparameter tuning for optimal performance. However, it is noteworthy that MENTS outperforms both BTS and DENTS on the 10-chain and gridworld tasks when the temperature hyperparameter is appropriately tuned. Therefore, it could be argued that the only advantage of using BTS or DENTS is their lesser dependency on temperature. Yet, with the correct tuning of this singular parameter, MENTS could potentially offer a superior solution.

Limitations:
The authors discussed the limitations of their work. No discussion needed regarding potential negative societal impact.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper presents two MCTS based algorithms called  Boltzmann Tree Search (BTS) and Decaying ENtropy Tree-Search (DENTS). These algorithms overcome the limitation of two state-of-the-art algorithms pUCT and MENTS of under and over exploration respectively. Further, the paper provides simple “simple regret” analysis and shows state-of-the-art performance on benchmark tasks.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
1. The paper is very well structured and easy to follow for the most part.
2. The empirical results are impressive, especially on Go.
3. The algorithms are novel to my knowledge and are inspiring.


Weaknesses:
1. The motivation for analyzing simple regret instead of cumulative regret is not clear. This limits the contribution of the proposed approach to some extent. (I will update the score post rebuttal, given the authors' response).
2. The paper puts all the theoretical proof in the appendix. Please provide at least a proof sketch in the main paper. 
3. Section 6.1 is not clearly written. What do you mean by “ To utilize ... a terminal/goal state.” (lines 232-234)

Minor
1. Graphs in Figure 2 are of poor quality. Please consider using transparent lines or a variety of dashed lines (different patterns of dashes) to represent different algorithms.
2. Please increase the font size of the text in Figure 3.

Note: I haven't carefully taken a look at the proofs in the appendix. I might update my score based on correctness.


Limitations:
Limitations are clearly mentioned in the paper.              

Rating:
6

Confidence:
3

";1
kXfrlWXLwH;"REVIEW 
Summary:
The authors study the general case of multi-vector retrieval, i.e., ColBERT and beyond. They propose and analyze a new algorithm for this ""vector set"" search task, with theoretical guarantees. When integrated into ColBERT, the proposed DESSERT method is 2-5x faster at some, relatively small loss in quality.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The authors discuss a general class of ""vector set"" retrieval scoring functions (Sec 1.1), generalizing ColBERT.

2. The authors formalize the ""vector set"" search problem, perhaps for the first time (although I'd be interested in a comparison with Luan et al. 2021 ""Sparse, Dense, and Attentional Representations for Text Retrieval"" for completeness). They propose a new algorithm, unlike ColBERT's. The algorithm provides theoretical guarantees.

3. The proposed method is shown to be ~2pt worse than a recent state-of-the-art ColBERT method (PLAID) on MRR quality, while being 3x faster, applying search in just 15.5 milliseconds per query on CPU. This is on the Pareto frontier with respect to the ColBERT retrieval quality/latency curve.

Weaknesses:
DESSERT is primarily tested on the MS MARCO query set (besides a small synthetic experiment). The authors justify much of this by citing PLAID [35], but that other paper like many IR papers in the last 1.5-2 years tests on several datasets, including especially out-of-domain and larger datasets. It's not inherently clear how much quality loss DESSERT would incur out-of-domain, say, on the LoTTE or BEIR (or even a subset of one of them, if computational resources do not permit more tests). This is the main weakness in my opinion.

Limitations:
See weaknesses.

Rating:
6

Confidence:
3

REVIEW 
Summary:
In this paper, the authors studied a new problem of vector set search with vector set queries. They have formalized this problem and proposed a novel, provable hashing scheme, DESSERT, to efficiently deal with this problem. Moreover, they have provided theoretical analysis for DESSERT and conducted experiments to validate its efficiency and accuracy.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The motivation to study this new problem is clear, which is a vital sub-problem in semantic search, and the authors formalize a general definition of this problem.

- They design an efficient hashing to solve this challenging problem and provide a theoretical bound and query time complexity.

- The presentation is clear and easy to follow.

Weaknesses:
***Novelty and Contributions***

- I appreciate the authors have provided the theoretical bound and query time complexity for their proposed hashing scheme. One of my concerns is that the improvement of the query time complexity is only marginal compared to the brute force method, which is still O(N) in general. Moreover, the impact of L is less discussed either in the theory part or in the experiments, making the efficiency improvement less promising.

***Experiments***

- Another primary concern is the experiments. The experimental results are too few. They only validate the performance of DESSERT on a single real-world dataset, making their conclusion of DESSERT compared to PLAID less convincing. Moreover, no ablation study and parameter study is performed, making users hard to know the effectiveness of implementation tricks and how to set the hyperparameters of DESSERT. Please see Q1-Q4.

***Presentation***

- The paper still needs some careful proofreading. There are some minor issues in Algorithm 2. Please refer to Q5 for more details.

Limitations:
This work does not appear to have any negative social impact.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper studies the vector set search problem, which is an extension of the canonical near-neighbor search problem and finds an application in the semantic search task. The paper claims that existing methods for vector set search are unacceptably slow, and thus, proposes an approximate search algorithm called DESSERT. The paper presents both theoretical analysis and empirical evaluation to demonstrate the effectiveness of the DESSERT algorithm.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
S1. The paper proposes a new algorithm, DESSERT, to solve the vector set search problem efficiently. 

S2. The paper presents both theoretical analysis and empirical evaluation to testify the effectiveness of the DESSERT algorithm. 

S3. The DESSERT algorithm can be applied to the semantic search task.

Weaknesses:
W1. [Motivations & Contributions] The paper is the first to formally formulate the vector set search problem (if not, then citations are required to be provided to the first work), yet only providing one concrete application of the vector set search problem (i.e., semnatic search). The reasons why we need to study the vector set search problem require further clarified. It seems to me that the DESSERT algorithm is actually an incremental work, with a specific focus for improving the running time of the ColBERT model. This severely limits the paper's contributions.

W2. [Experiments] According to the paper, the proposed algorithm, DESSERT, is implemented in C++. However, the baseline method, the brute force algorithm, is implemented in Python using the PyTorch library. The reasonability and fairness of implementing two methods in different programming languages need further explanations. 

W3. [Presentations] The paper's presentation needs improvement. There exist several typos and undefined notations in the paper. Some concrete examples are given as follows. 

- Line 139: I believe the notation H refers to the hash set, which requires further clarification. 

- Line 30: citations are required to the literature on traditional single-vector near-neighbor search. 

- Algorithm 2: the output given in the pseudo code doesn't concur with the desciptions given in the main text (though they may refer to the same variable).  



---
After the rebuttal process, most of my concerns mentioned above have been sufficiently addressed. Thus, I raise my rating from 4 to 5.

Limitations:
I do not see any potential negative societal impact in this paper.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper addresses the problem of set-to-set similarity calculation and retrieval, which is a problem with any downstream applications. While previous approaches inevitably perform a brute force similarity calculation over |Q| query vectors and |S| target vectors for each set comparison F(Q, S), this paper leverages random hashing functions to estimate similarity between query and target vectors to avoid this brute force search. Combined with highly space-optimized hash tables, and building on top of previous centroid-based candidate set filtering mechanisms, the proposed algorithm achieves notable speed improvements over existing methods at only a small cost to recall. Theoretical support is provided for the algorithm, along with experiments on synthetic, solvable data and the standard MS MARCO benchmark dataset. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- Very important: The proposed method cleverly utilizes randomness via an LSH based approach to estimate similarity and avoid brute force similarity calculation between sets of vectors, leading to notable improvements in speed over previous methods, at reasonable memory requirements for large scale settings, and with very limited negative impact on recall.
- Important: The experiments are simple and clear.
- Important: The choice of baselines seems appropriate. As the authors note, their method reaches points on the Pareto curve, w.r.t. latency, that are not reachable by baselines even though the baselines are nominally customizable for trading off latency vs. recall. 
- Important: The paper is clearly written.
- Of some importance: Theoretical analysis is provided for the proposed method, although, as the authors note, they “assume sufficiently high relevance scores and large gaps in our theoretical analysis to identify the correct results.”

Weaknesses:
- Important: As noted by the authors, this work draws heavily upon LSH-based NN vector search methods that already use LSH methods to approximate exact similarity calculation. In my opinion, the extension to the set-to-set comparison setting is fairly straightforward. This is a drawback of the methodological contribution of the paper, in terms of novelty, though it may be entirely outweighed by the positive empirical performance of the approach over strong baselines.
- Of some importance: The intro of the paper is written as if the paper proposes a generic framework for cross-query-vector aggregation (A2) and cross-target-vector aggregation steps (A1), but to my understanding, the only setting experimentally considered in the paper is that of A2 = weighted sum and A1 = max. This seems to be the main practical setting of interest in settings like passage retrieval, but I would not say this paper really broadens how people have been thinking about set-to-set comparison.

Limitations:
Yes, the limitation section was satisfactory. 

Rating:
7

Confidence:
2

REVIEW 
Summary:
The paper presents a study focused on the problem of vector set search with vector set queries, which is a crucial subroutine for various web applications. The authors highlight the insufficiency of existing solutions in terms of speed. To address this, they propose a new approximate search algorithm called DESSERT. DESSERT is a versatile tool that offers robust theoretical guarantees and demonstrates excellent empirical performance. The authors integrate DESSERT into ColBERT, a highly optimized state-of-the-art semantic search method, and report a notable 2-5x speed improvement on the MSMarco passage ranking task.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. The problem formulation is truly innovative. Vector set search is undeniably an essential and challenging task, with practical applicability in serving LLMs. 

2. The proposed methodologies are lucid and firmly grounded, ensuring the reproducibility of the research.  DESSERT uses hash tables for element-wise similarity search to build a data structure for vector set similarity search. Moreover, DESSERT taking advantage of hash tables in terms of query speed and performs an efficient query for similar sets.

3. DESSERT has a detailed theoretical analysis in terms of inner and outer aggregation, which justify its choices of aggregation functions in the algorithm.


Weaknesses:
It would be better to have a section that unifies the notation for algorithms and theory. Maybe a table would be better. 

Overall, I have a positive impression on the paper. If authors are willing to further polish the theory part of the paper, I'm willing to raise the score.

Limitations:
The authors adequately addressed the limitations.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper considers a nearest neighbor search problem where each point is a set of vectors, and the distance function is drawn from a general class of aggregation functions over the vectors. 

The approach presented here is based on the LSH algorithms, but since we're dealing with multiple vectors, the bounds behind LSH need to be re-derived. Assumptions used in order to allow or speed up the search include that the distance function obeys a certain Lipschitz smoothness property, and tools include inverted indices, sampling, and compressed tables. These are all fairly standard in the field.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The query model introduced is interesting, and the authors make a good argument as to its utility. The techniques utilized are non-trivial, and constitute a contribution to the field of nearest neighbor search. The fact that the distance function is very general is also a plus.

Weaknesses:
The presentation can be somewhat improved. The authors actually put in significant work into this direction, but the presentation can be improved by adding a high-level overview of the approach before introducing the pseudo-code. This is a more effective approach than referring to code to explain an approach.

The authors should also add background on LSH and inverted indices, so that the paper can be more accessible to non-experts.

The explanation of the motivation behind TinyTable is garbled, and a clearer exposition is necessary. 

---------------------------------

In response to the author rebuttal, and in particular the improvement in presentation, I raised my score.

Limitations:
N/A

Rating:
7

Confidence:
3

";1
NbkjMn7X8H;"REVIEW 
Summary:
The paper studies robust nonparametric regression under poisoning attacks. The input is samples from some fixed distribution and the goal is to approximate some unknown function on the input space. It is assumed that the values observed in q of the samples are adversarial. In this setting, classical approaches such as k-NN estimators can fail. This is because a single adversarial sample can affect the prediction on many points. In order to avoid this issue, the paper proposes robust variants of nonparametric regression.

The first result is a bound on the convergence rate of an M-estimator based on Huber loss minimization: the initial estimator has minimal optimal ell_infinity risk. When the number of adversarial samples is not too big, the ell_2 risk is optimal.

It is also shown that the estimator can suffer when many of the adversarial samples are concentrated within a small region. In order to resolve this issue, the paper proposes a correction step that projects the estimator into the space of Lipschitz functions. Upper bounds on the rate of convergence of the corrected estimator are established. Numerical results show that both estimators outperform standard methods in a simple low-dimensional synthetic scenario.



Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Nonparametric regression is a fundamental problem. The paper studies a natural adversarial setting where a small number of input samples are corrupted adversarially. This research direction can produce more robust useful data-analytic methods.

The upper and lower bounds match up to polylogarithmic factors when the number of adversarial samples is not too large.


Weaknesses:
The experimental evaluation is very limited. This is unclear whether the proposed methods can lead to significant robustness improvements in real-world scenarios.

No runtime analysis is given for the corrected estimator. This is a continuous optimization problem, so it is not clear how easy it is to solve in practice, especially in high or moderately-high dimensions.


Limitations:
The authors have adequately addressed the limitations of their work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
A robust version of the classic non-parametric problem is studied, when the training data is under an adversarial poisoning attack. The work is primarily theoretical, and makes assumptions on Lipschitzness and boundedness of the underlying function, boundedness of the data density, restrictions on sharpness of the data domain boundary, sub exponential noise. With some restrictions on the kernel, and appropriate parameters, upper bounds are shown on the $\ell_2$ and $\ell_\infty$ loss for the robust regression fit using Huber loss minimization. Lower bounds are obtained under the same assumptions, which match asymptotically with the upper bounds for $\ell_\infty$ and a correction is presented to match the upper bound for $\ell_2$ loss as well. Numerical simulations show usefulness of the algorithms on simulated one and two dimensional data.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The authors study robust nonparametric regression under poisoning attacks for smooth underlying functions under certain assumptions on the data and domain, under which asymptotically tight rates are obtained for $\ell_\infty$ loss by suitably parameterized Huber loss.
2. A novel correction method is proposed for which tight rates are obtained for $\ell_2$ loss as well. Together with above, the optimal algorithms under the given assumptions are obtained for both losses.
3. Numerical simulations are performed to verify the theoretical results.

Weaknesses:
1. The assumption needed for proving the theoretical result, specifically assumption 1(b) is too strong. Effectively, it reduces the possible test distributions to ""near-uniform"" distributions since there is both an upper and a lower bound on the probability density function. This assumption is too strong to make the results useful in most practical situations.
2. No insights is provided into the proofs of the theoretical results; in particular there are no proof sketches in the main body. This makes it challenging to understand the contributions, given the work is primarily theoretical.
3. Experiments only involve very specific numerical simulations. How does the approach work on real regression datasets? The effect of changing the hyperparameters is not studied and how they are selected is not described.
4. Huber loss is already known as a technique for robust nonparametric regression [1] and is not novel, the paper should mention that only the analysis under the present assumptions is new. The novel correction method is computationally intractable.

[1] Maronna, R. A., Martin, R. D., Yohai, V. J., & Salibián-Barrera, M. (2019). Robust statistics: theory and methods (with R). John Wiley & Sons.

Limitations:
The authors note several limitations of their work in a dedicated section.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors propose two new nonparametric regression methods that and study their resilience under poisoning attacks.  The baseline for comparing the performance of the proposed methods is kernel regression (aka Nadaraya-Watson estimator).  The proposed methods are two, since the initial idea that the authors have, turns out to be vulnerable to poisoning attacks that are concentrated in small regions and a large budget is spent there.  Hence, the authors arrive at a `corrected' estimator which behaves better and is closer to optimal behavior.


After rebuttal:

For the largest part I believe that the authors have provided sufficient clarifications to various issues that were raised and moreover are willing to integrate comments and clarifications that came up during the discussion period in the final version of the manuscript. Therefore, I am increasing my score from reject to borderline accept; I am also increasing the soundness from fair to good as well as the presentation from poor to fair.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
+ New methods for regression that are resilient to poisoning attacks.
+ Both theoretical and practical results.

Weaknesses:
+ Missing a section with preliminaries or background knowledge, where notions and notation that the authors use in the paper is well-defined. For example, in the section for preliminaries you can give information such as:
   - define loss functions, 
   - discuss kernel regression and separate the presentation from your initial estimator,
   - discuss notions that you use when it is unclear what these things are; e.g., ""bandwidth"".
   - define functions that you use without explanations at the moment; e.g., in (18) we see Clip and med; what are the arguments and what do these functions do?

+ References not in alphabetical order.

+ In line 52-54, you indicate that your approach is similar to a combination of an $\ell_1$ and $\ell_2$ loss functions. At that point in the text, I was expecting a comparison with ""elastic nets""; you may want to consider adding a small comment, or consider rephrasing and avoid such expectations from the readers.

+ The order by which some things are presented should probably change. For example, in lines 115-124, I think you need to give a slightly better explanation so that the reader can get better intuition, and moreover, this discussion should come up before you lay out the equations of the method that you propose.

+ You cannot start sentences with as in lines 167 or 168. You could add a word like ""Part"" or ""Parts"" in the beginning and make it read more naturally even if it takes a bit more space.

+ Larger font size in Figure 1 is expected.

+ It would be nice to see some commentary on the bounds and argue how they compare against each other and potentially compared to other methods that you cite in the literature. 

+ In general, you have no comparison with parametric methods, either in theory, or in the experiments. Alternatively, provide an explanation as to why you don't have such comparisons.

+ It is unfortunate and normally it should not be an issue, but the authors need help on writing a paper that has fewer spelling or expression mistakes. The additional problem here is that the authors also have a big appendix, which I suspect is written similarly along the main text. So, fixing the main text is not enough. The paper needs to be proofread by someone in its entirety, including the appendix of the authors.

I think that you have a very interesting story to tell, but the paper needs restructuring and better presentation of what you have accomplished.

Limitations:
N/A

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies nonparametric regression, where an adversary can corrupt $q$ samples from the training set. The paper proposes robust estimators for this problem. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:

The paper studied the Huber loss minimization approach under adversarial noise, giving theoretical upper bounds and lower bounds. 



Weaknesses:
The organization could be improved, e.g. the last two paragraphs of section 3 could conceivably be placed elsewhere. 

Some aspects of this paper are not really my area, so I cannot provide many helpful comments. 

Limitations:
The author addressed limitations. 

Rating:
6

Confidence:
2

";0
URI2aAQiQC;"REVIEW 
Summary:
This paper proposes SpikeBERT, a spiking-based BERT model for text classification. It employs LIF spiking neurons an surrogate gradients for backpropagation. The training method consists of a two-stage distillation process (pre-training + task-specific). The experiments conducted on different benchmarks for English and Chinese languages show that the proposed SpikeBERT achieves higher accuracy than prior spike-based language models and lower energy consumption than the (non-spiking) BERT.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	The proposed method is novel and relevant to the community.
2.	The technical sections are described clearly.
3.	The experiments provide good-quality results.


Weaknesses:
1.	The design decisions are not discussed in detail.
2.	Unlike non-spiking BERT, there are scalability issues when increasing the depth.


Limitations:
The limitations have not been discussed by the authors, but there are no major limitations in this work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presents SpikeBERT, an improved version of the Spikformer spiking transformer model for language tasks. SpikeBERT utilizes a two-stage knowledge distillation method that combines pre-training with BERT and fine-tuning with task-specific data. Experimental results show that SpikeBERT outperforms state-of-the-art spiking neural networks and achieves comparable results to BERT on text classification tasks for English and Chinese, while consuming significantly less energy. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1 The writing is commendable.
2 The proposed approach is captivating and innovative, and I believe it will generate significant interest within the machine learning community.
3 The results are promising, particularly in achieving comparable performance to BERT in text classification.

Weaknesses:
1 Although SpikeBERT significantly reduces energy consumption during inference, the two-stage knowledge distillation process may introduce additional costs. It would be helpful if the authors could provide insights on this matter.

2 Considering other model architectures such as the OPT model families, does SpikeBERT possess the potential to be extended to these models? It would be interesting to explore the applicability of SpikeBERT beyond the BERT architecture.

3 I am curious about the evaluations of SpikeBERT on additional tasks such as GLUE, RACE, and SQuAD. It would provide valuable insights into the model's performance across a broader range of language processing tasks.

Limitations:
No limitations

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper presents SpikeBERT, an implementation of BERT-based models on a Spiking Neural Network (SNN) architecture, motivated by theoretical energy efficiency benefits.

The paper presents the transformer architecture and a two-stage distillation approach which first distills a general purpose BERT model into a general purpose SpikeBERT, then finetunes the SpikeBERT by distilling a finetuned BERT model.

The approach is evaluated on several text classification tasks on English and Chinese datasets, resulting in accuracies lower but comparable than a standard finetuned BERT classifier.

A theoretical energy efficiency improvement is calculated, reporting improvements, which however I find dubious (see weaknesses)

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- Interesting work on an unusual architecture

Weaknesses:
I am especially concerned about the claims of improved energy efficiency, which serve as the main motivation of the paper.
Starting from the introduction, where the author claim: ""However, it requires too much computational power and energy to train and deploy state-of-the-art ANN models, leading to a consistent increase of energy consumption per model over the past decade. The energy consumption of large language models, such as ChatGPT[OpenAI, 2022] and GPT-4[OpenAI, 2023], is unfathomable even during inference.""
It is clearly not true that ""it requires too much computational power and energy to train and deploy state-of-the-art ANN models"" since these models are in fact trained and deployed.

More concerning is the theoretical energy comparison of SpikeBERT and BERT (Section 4.4 and Appendix C), where the authors compare FLOPs for BERT and SOPs (spiking operations) for SpikeBERT, multiply by theoretical energy costs and declare SpikeBERT the winner. The theoretical energy costs seem to be copied from other papers, and following the citation chain they seem to come from Yao et al. 2022 ""Attention Spiking Neural Networks"" where they are computed using data from Horowitz 2014 ""1.1 computing’s energy problem (and what we
can do about it)"", with the assumption of 32-bit floating point operations on 45nm hardware. Modern GPUs use 7nm hardware, and inference is often done with 8-bit floating point operation or less, therefore I wonder whether these number are obsolete.


Limitations:
Yes

Rating:
3

Confidence:
3

REVIEW 
Summary:
This work develops SpikeBERT, which extends Spikformer to perform language processing tasks, and proposes a two-stage knowledge distillation method for better training it. Experiments validate the improved accuracy of SpikeBERT over previous SNNs and the improved efficiency over vanilla BERT.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. This work is the first transformer-based SNNs for language processing tasks.

2. Experiments validate the achieved efficiency improvement over vanilla BERT.

Weaknesses:
I have the following concerns about this work:

1. The novelty and technical contributions of this work are limited: the modifications from Spikformer to SpikeBERT are minor, and similar distillation schemes have been widely studied and adopted in efficient BERT works, e.g., TinyBERT, TernaryBERT, FastBERT, DistilBERT, etc. It is hard to tell the key technical contribution of this work.

2. The experimental validation is insufficient: It only validates the improved efficiency over vanilla BERT while the aforementioned efficient BERT variants are not benchmarked, making it not clear whether SpikeBERT is a practical efficient BERT option. In addition, the theoretical power is not enough for indicating the real-device efficiency and on-device measurement is highly desirable for benchmarking the aforementioned efficient BERT variants.

3. This work may violate the formatting regulations, i.e., it adds an extra appendix in the main manuscript. In addition, the citation format seems to not follow the official template.

Limitations:
This work may violate the formatting regulations, i.e., it adds an extra appendix in the main manuscript.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The authors have proposed SpikeBERT which is an energy efficient Spiking Neural Networks(SNN) for natural language representation. The architectural design for SpikeBERT is inspired from Spikformer which is an SNN for computer vision, with the following major changes:
	- Spiking Patch Splitting for images is replaced by embedding layer to encode words/tokens into vectors.
	- BatchNorm replaced by LayerNorm
	- Shape of Spiking Self Attention is changed to adapt to language tasks by changing its size based on length on input instead of dimensionality of hidden layers.

The model is trained using a two-step knowledge distillation approach:
	- General purpose: The model is trained using embedding and intermediate hidden layer representations of BERT on unlabeled natural language texts.
Task specific: The model is tuned using task specific logits from a fine-tuned BERT model.

Soundness:
2

Presentation:
3

Contribution:
1

Strengths:
	- The paper is well written and easy to follow. The authors provide the necessary background on SNNs including the advantages and challenges in training them.
	- The motivation is clear and the energy consumption presented in the results section helps convey the same.
	- The metrics indicate that the proposed approach outperforms the previous spiking network based baseline developed for natural language understanding and standard SNN training mechanism using surrogate gradients.
	- The authors present a thorough ablation analysis for all the contributions presented in the paper.


Weaknesses:
	- The novelty of the paper is limited:
		○ The architecture is mostly derived from Spikformer: the usage of word embeddings instead of image patches, and using layer normalization in transformer  is a standard approach.
		○ The two-step knowledge distillation approach has been used widely in the past for distilling BERT and GPT style transformer models to smaller/specific architectures.
		○ The usage of hidden layer representations in the distillation process is also a standard practice for BERT style models.
		○ Most of the formulations and approaches related to spiking neurons, its derivatives and feature transformations are adapted from previous work.


Limitations:
	- The datasets used for evaluation seem limited. With models like BERT, it's a standard approach to present results on all GLUE tasks or at least the ones such as MNLI as they are considered to be good & reliable indicators of model quality.


Rating:
4

Confidence:
3

";0
IwnINorSZ5;"REVIEW 
Summary:
This paper develops a meta-learner-based approach for conformal inference of individual treatment effects (ITEs). Instead of predicting the missing outcome (the counterfactual), the proposed approach uses plug-in pseudo outcomes as the inference proxy. The validity of ITE inference is based on careful analysis of the statistical dominance of the ITE and the inference proxy. In this way, one no longer needs to adjust for the covariate shifts in applying conformal inference, and the validity holds in a model-free fashion (although the coverage guarantee is more limited). Overall, this paper finds a novel approach to predictive inference of ITEs, and the method works well in synthetic and real-world datasets. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Novel contribution to an important problem 

This paper makes novel contributions, with a new methodology, to an important problem: uncertainty quantification and predictive inference of individual treatment effects. The proposed methodology significantly differs from the PO-prediction-based approaches in the literature, and the analysis contains interesting findings/perspectives. I believe these are important contributions to the literature of conformal inference, and might inspire more developments in the future. 

2. Good writing quality and clarity 

This paper is well-written and enjoyable to read. The challenges are clearly stated and the contributions are easy to capture. I still have some questions/suggestions for improving the presentation; please see my questions. 

Weaknesses:
1. The analysis is somewhat limited to the known propensity case. 

Since previous ITE methods already achieve validity in the known propensity case, the contribution of this paper is somewhat marginal in the sense that it does not push the limit of how well/ how much we can do for this problem (although i still appreciate the novel perspective provided here). However, the authors only mentioned the challenge of unknown propensity score at the end of the paper, which is a bit unsatisfactory. 

2. The setting in synthetic datasets should be stated more clearly. 

I think the most challenging part of ITE inference is when both POs are missing, and this is where the proposed approach becomes most interesting. However, when reading the experiments part I am not sure which situation we are in. Are we inferring the ITE for both POs missing case, and what is the calibration data here? I think further clarification will be helpful. 

3. The benefits of this method in experiments is a bit unclear. 

The experiment part contains rich information. However, I am not sure I fully understand in which cases the proposed method performs the best - is it particularly beneficial for both-PO-missing case, or one-PO-missing part? I thought in the latter case the previous approach should suffice, or it still can be improved due to no covariate shift adjustment? I also do not understand why IPW and DR lead to so long prediction intervals for the NLSM dataset. It is mentioned that ""conformity scores have “very strong” dominance over oracle scores"", but it is difficult to understand in which cases this might happen (strong signal? large noise? high nonlinearity?).

Limitations:
Yes.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper introduces a framework for inferring treatment effects, merging concepts of conformal prediction and meta-learner. This framework is characterized by its distribution-free validity and coverage guarantees. The authors utilize stochastic ordering techniques to substantiate the framework's validity.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper is well written and sounded. The innovative approach of employing stochastic ordering to validate the use of pseudo outcomes in conformal prediction is particularly noteworthy.

Weaknesses:
The concept of employing stochastic ordering is innovative; however, its practical application is impeded by the fact that the assumptions are inherently uncheckable

In comparison to the method proposed in [1], the current method exhibits better performance only in terms of the RMSE of the CATE. However, when considering coverage and average length, it does not surpass the performance of the inexact method. 


[1] Lihua Lei and Emmanuel J Candès. Conformal inference of counterfactuals and individual treatment effects. Journal of the Royal Statistical Society Series B: Statistical Methodology, 83(5):911–938, 2021.

Limitations:
See weakness

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper adresses individual treatment effect (ITE) inference using conditional average treatment effect (CATE) meta learners. 

The authors propose to wrap the meta-learners with a conformal prediction step so as to obtain valid confidence intervals for the estimated treatment effect. Since the CATE estimator induces a distribution shift wrt conformity scores, the coverage guarantee does not immediately translate into the desired coverage of actual ITE. In a who can do more can do less spirit, the authors point out that ITE coverage is ensured if conformity scores obtained from pseudo-outcomes stochastically dominate the scores obtained from actual « oracle » outcomes. The authors succeed at providing stochastic dominance results for several stochastic order / meta-learner pairs. 



Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- the paper is very clear and pleasant to read
- the tackled problem is challenging and impactful
- the proposed solution is backup by both theoretical and experimental results

Weaknesses:
- the method is by-design over-conservative 


Limitations:
- knowledge of the propensity score is required
- validity range of level $\alpha$ not always easy to determine
- over-conservatism of the prediction intervals

The two first points are already discussed by the authors in the submission. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper deals with confidence intervals for the ITE prediction task, where the authors propose to use the framework of conformal prediction for meta learners. The learning of nuisance models for generating pseudo outcomes is done one separate data split, and the final regression models trained to predict pseudo outcomes are evaluated on a validation set to compute the empirical calibration distribution. The proposed approach for calibration removes the issues of covariate shift and inductive biases on nuisance models with the prior approaches of weighted conformal prediction. The authors also provide guarantees on when the confidence intervals for ITE with pseudo outcomes cover the confidence interval for ITE with true outcomes; and further validate their findings with synthetic and semi-synthetic datasets.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* ITE prediction is an extremely relevant problem in causal inference as population level effects (ATE, CATE) would not necessarily transfer to the individual level effects. Further, having an estimate of uncertainity along with ITE prediction is crucial for high-stakes application scenarios. Hence, the approach of conformal meta learners is highly relevant for the current challenges in causal inference. 

* The proposed combination of conformal prediction with meta learners is novel to the best of my knowledge. Further, the ITE coverage results with pseudo outcomes for standard meta learners (DR Learner) are quite significant as it is non-trivial to understand when the predicted confidence intervals with observed information can represent the confidence intervals with counterfactual information. 

* The proposed conformal meta learner approach is technically sound with theoretical proofs on their validity and empirical justification on a decently diverse set of benchmarks.

Weaknesses:
I do not think the paper has any serious weaknesses, but I have listed some of them in the questions section ahead. One major suggestion is that the paper could be written with better clarification for the section on conformal meta learner being robust to covariate shift and inductive biases. The arguments are stated informally and it is a bit hard to follow.

Limitations:
Yes, the authors have addressed any potential negative societal impact of their work.

Rating:
7

Confidence:
3

";1
D7LdL2SCCi;"REVIEW 
Summary:
This paper explores the relationship between OT-based distributional robustness and sharpness-aware minimization (SAM), and shows that SAM is a special case of the proposed framework when a Dirac delta distribution is used over a single model. To demonstrate the effectiveness of the framework, the authors perform several experiments and show improvements over the baselines.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. This paper is theoretically sound.
2. The experiment verifies the effectiveness of the proposed framework.

Weaknesses:
The practical methods stated in Section 4.2 are not easy to follow. It would be better if the authors can provide a pseudocode to help the readers understand the methods easily.

Limitations:
The practical methods stated in Section 4.2 are not easy to follow.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes an optimal-transport-based distribution robustness framework on model space. The proposed theory considers the worst-case loss w.r.t a Wasserstein distance from the model distribution, which can be considered a probabilistic extension of sharpness-aware minimization (SAM).

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The paper is well-written and easy to follow.
- The proposed approach shows empirical improvements compared to SGD, SAM, DGVB, etc.

Weaknesses:
- The theoretical and empirical results are not surprising to me. Moreover, they do not conduct generalization analysis, which is important in learning theory.
- I do not agree with the claim that this paper is the first work that considers distribution robustness within the model space. The sharpness-related learning problem has already been well addressed in PAC Bayes theory. The counterpart of the worst-case awareness studied in this paper can be found in [1]. And other Wasserstein or more general theories can be referred to [2] and [3].
- The single-model, ensemble, and BNN cases are also common algorithm realizations in the PAC Bayes community.

>[1] Jiang, Yiding, Behnam Neyshabur, Hossein Mobahi, Dilip Krishnan, and Samy Bengio. ""Fantastic Generalization Measures and Where to Find Them."" In *International Conference on Learning Representations*.

>[2] Amit, R., Epstein, B., Moran, S. and Meir, R., Integral Probability Metrics PAC-Bayes Bounds. In *Advances in Neural Information Processing Systems*.

>[3] Haddouche, Maxime, and Benjamin Guedj. ""Wasserstein PAC-Bayes Learning: Exploiting Optimisation Guarantees to Explain Generalisation."" (2023).

Limitations:
No, I suggest the authors add a section to discuss the limitation of the proposed algorithm, e.g., the increment of computation complexity of BNN and the multiple particle setting.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper explores the optimal transport distributional robustness framework in the model parameters spaces, and provides practical algorithms to solve the corresponding optimization problems. This flexible framework can be used to train a single model, ensemble models and Bayesian neural networks in a sharpness-aware fashion. The authors additionally establish that the sharpness-aware minimization serves as a special instance within their proposed framework. The experimental results demonstrate the usefulness of this framework.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
This paper considers an important question of neural networks training. It is clearly written and easy to follow. The idea of extending the form of SAM to the DRO framework is natural but very meaningful, since it provides more well-developed tools to understand and improve the generalization of training models.


Weaknesses:
1. In the theory, this paper shows that SAM is equivalent to the optimization problem in the OT-MDR framework when using a particular distance metric and picking the center distribution as a Dirac delta distribution. In the experiments, the results show (1) OT-MDR performs better than SAM for both single model and ensemble models cases; (2) The loss landscape found by OT-MDR is more flatten than the one found by SAM. This probably implies the training algorithm of OT-MDR is better than directly using SGD to solve SAM. But the paper doesn’t provide much intuitive understanding or analysis of these observations. 

2. Most of the experimental results don’t provide standard deviation, which makes the results less solid.


Limitations:
In general, I like the idea and the presentation of this paper. The authors could probably further improve this paper through more experiments and add some analysis of the observations from comparing SAM and OT-MDR.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a novel distributional robustness optimization problem on the space of model parameters (OT-MDR). By formulating the uncertainty set with Wasserstein distance and introducing an entopy regularization, the explicit solution to the primal problem is derived. A practical algorithm is proposed based on sampling techniques from SGLD. The algorithm is extended beyond point estimation to accomodate emsembling and bayesian learning. The dual of OT-MDR turns out as a stochastic relaxation of the objective of SAM, bridging sharpness aware optimization and distributional robustness. Empirical results on Cifar validates the advantage of proposed method over baseline optimizers.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. I might be unfamiliar with literature on sharpness aware optimization, but from my perspective this paper is the first to reveal an intrinsic connection between distributional robustness and local sharpness. Theorem 3.4 is intriguing since it might inspire further research to apply abundant literature on distributional robustness (and adversarial robustness which is highly relative) to solving sharpness aware optimization. 
1. The proposed method is a stochatstic extension of SAM but it introduces limited computational burden. On the other hand, the algorithm well extends to bayesian learning because of its stochastic formulation of model parameters. 

Weaknesses:
This paper proves OT-MDR to be more general than SAM but the advantage of OT-MDR in the case of single model and emsembling is theoretically vague. From Theorem 3.4, it seems that OT-MDR is equivalent to SAM for single model except that OT-MDR softly expands the uncertainty ball around the centering model, by allowing parameters to go beyond the radius $\rho$. The gap is insufficient to account for the empiricial success of OT-MDR for single model. Additionally, is it possible that enlarging the radius $\rho$ for SAM would make its loss landscape approach OT-MDR in Figure 2?

Limitations:
na

Rating:
7

Confidence:
3

";1
t877958UGZ;"REVIEW 
Summary:
This paper presents a novel method to connect vision-language instruction tuning with large language models.
To implement this, the authors introduce the adapters rather than heavy bottlenecks between vision-language input tokens and LLM.
A routing mechanism is designed to adaptively choose the right direction for different modalities of inputs.
After applying this method to the recent strong LLaMa model, the proposed method achieves efficient instruction tuning and close the performance gap with that of the original large language model.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is well-written and most parts of this paper are easy to follow.
- The proposed method achieves significant performance improvement pertaining to instruction tuning while using a very small magnitude of trainable parameters.
- The proposed method demonstrates favorable results on both single- and multi-modal instructions.

Weaknesses:
- My biggest concern lies in the routing mechanism.
  - Why do we need the routing between different kinds of inputs? Can we just use determined inputs to the modality-aware adapters?
  - What the role of ```s``` is is not well explained.
  - It seems the router output is simply a weighted sum of the two adapters. How should we explain this?

- It seems the authors also introduce adapters to the image encoder. This should also be explained.

- For the image input features, the authors use the ```[CLS]``` of every fourth layer of CLIP-ViT, are there any rationales behind this?

- I'm confused about Sec. 3.2. It seems that Sec. 3.2 can be cohesively organized with that of MMT. Are there more considerations about this section? If not, the writing should be better organized.

- Some typos:
  - Line 156, ```m``` and ```n``` are in reverse order;
  - Line 170, the dimension definition is not correct.

 
 

Limitations:
NA

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper presents a cost-efficient method to fine-tune LLMs thus enabling their multimodal reasoning capabilities. The main technical contribution includes using Mixture-of-Modality Adapation, which adopts lightweight adapters to bridge the gap between modality gaps. In the meanwhile, MMA also allows automatic routing such that the model can process both multimodal prompts and text-only prompts. Experiments show superior results on ScienceQA and training efficiency, in terms of both training time and number of trainable parameters.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The proposed method may help significantly reduce the fine-tuning cost of LLMs with multimodal inputs. 
- The proposed method is conceptually concise and generic. It could be potentially incorporated into different models/systems thus harvesting the development of both LLMs and vision encoders.
- Results on ScienceQA is promising, surpassing some strong competitors such as GPT-4.

Weaknesses:
- Evaluation is not sufficiently convincing. Only quantitative results on ScienceQA are presented. This leads to a narrowed view of the multimodal capabilities of the fine-tuned model. For example, does the model perform equally well on image captioning datasets such as NoCaps, COCO; how does the model perform on cross-modal retrieval / other VQA benchmarks? Missing these results makes it difficult to understand whether the proposed fine-tuning paradigm is actually bridging the modality gap as good as some previous works.
- In L50 and also Figure 1(b), authors claim ""this paradigm often requires to update most parameters of LLM, limiting the efficiency of VL instruction tuning"", ""these fine-tune schemes will inevitably undermine the NLP capabilities of LLMs due to the drastic changes in their parameter spaces"" and ""existing multimodal LLM do not support text-only instructions, greatly hindering their applications"". I wouldn't say these are correct. For example, BLIP-2/MiniGPT4 both keep LLMs frozen without updating their parameters, thus one can always prompt its LLMs with text-only prompt without degradation in language generation quality. I am therefore not fully convinced by the motivation of having automatic routing, instead of having just frozen LLMs speaking of to preserve the text generation capabilities.

Limitations:
Limitations are discussed.

Rating:
4

Confidence:
5

REVIEW 
Summary:
> Update: I bumped up my rating to 6 after rebuttal

This paper proposes, LaVIN, an efficient and effective vision-language instruction tuning scheme to adapt LLMs. Specifically, the authors utilize parameter-efficient modules to adapt the LLaMA LM – they insert several adapters to the image encoder and mixture of modality adapters to the LM, the LM is expected to automatically select and route through the adapters of different modalities. LaVIN can be trained in an end-to-end fashion. Because only adapters are learned during training, LaVIN training is much more efficient than LLaVA that uses full tuning. Instruction tuning in LaVIN includes both text-image data and text-only data in a multi-task fashion (but with better separation due to adapters) to enable text-image instructions and text-only instruction at inference time. Strong results are achieved on ScienceQA.


Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The proposed method is simple and sound. By the use of a bunch of adapters, the model can naturally learn different-modality instructions in an end-to-end fashion.  
2. LaVIN is much more efficient than LLaVA.  
3. Empirical results on ScienceQA are strong.  
4. The paper is well-written.


Weaknesses:
1. I think the proposed method resembles LLaMA-adapter a lot, maybe the authors should better note their difference early in the paper – currently LLaMA-adapter is not described until the experiment.  
2. The experimental comparison with LLaMA-Adapter is not an apple-to-apple comparison because  LLaMA-Adapter and LaVIN use different instruction tuning datasets – the performance gap may simply originate from the instruction tuning datasets. A better baseline should be using LLaMA-Adapter on the same instruction tuning datasets.


Limitations:
The authors have discussed the limitations of the proposed method in Section 5.


Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes Mixture-of-Modality Adaptation (MMA), which adopts lightweight adapters to bridge the gap between LLMs and VL tasks. The adapter utilizes a router to automatic shift between single-modal and multi-modal instructions. When applying MMA to LLaMA and training on both single-modal and multi-modal data, their proposed approach achieves competitive performance with supervised methods on ScienceQA dataset. Besides, the training of MMA is efficient and cheap.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The mixture-of-modality adapter could dynamically adjust the adaptations for single-modal inputs and uni-modal inputs, which help preserve the NLP capability of LLMs.
2. Achieve competitive performance given small number of training parameters.
3. Ablation studies support the effectiveness of mixture-of-modality training and mixture-of-modality adaptation. 

Weaknesses:
1. The adapter idea has been extensively explored in previous efficient VL training, and using adapter to efficiently bridge vision and LLM has been explored in LLaMA-Adapter.
2. Missing discussion/ablation of how the router in the MMA helps with the LLM learn visual information while preserving NLP capability.
3. This paper only evaluates on ScienceQA. Evaluation on other benchmarks like (COCO Caption) is needed for better comparison with LLaMA-Adapter. 

Limitations:
NA

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper presents a novel method to do efficient vision language fine-tuning. Through a mixture of modality adaptation mechanism, the model can close the gap between different modalities. Additionally, the paper proposes a routing algorithm to switch between multiple 
tasks. The training cost of the proposed system is low as the number of total trainable parameters is less than 4M. The model has been evaluated on public benchmark of ScienceQA. 

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- The proposed method is efficient. The total trainable parameters are less than 4M.
- The mixture-of-modality adaptation mechanism provides a way to adapt the LLM to vision modalities without expensive VL pretraining.
- The proposed method has been evaluated on public available benchmark, achieves comparable results, and for some of the evaluation metrics, it surpasses some existing models that have larger sizes

Weaknesses:
For weaknesses, please see my questions in below section.

Limitations:
N/A

Rating:
5

Confidence:
3

";1
qO9VagA7kF;"REVIEW 
Summary:
This paper conducts a comprehensive analysis of various quantization methods for large language models (LLMs). Some interesting takeaways were shared, for example, activation quantization is generally more susceptible to weight quantization; none of the current quantization methods can achieve the original model quality. Based on such insights, the paper also proposes an optimized method called Low-Rank Compensation (LoRC), which employs low-rank matrices to enhance model quality recovery with a minimal increase in model size.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The motivation of the paper is solid. With the rapid development of LLM, it is essential to study the methodology to deploy LLM on more accessible hardware, where quantization is an important category of the approach. Therefore, a comprehensive study of these methods is necessary.  

- The insights shared by this paper are helpful, e.g., it is interesting to know activation quantization is generally more susceptible to weight quantization.


- The proposed improvement based on the observation is reasonable. 



Weaknesses:
- The proposed method is based on low-rank approximation, which can be viewed as a sparsification-based method. It is kind of out of the scope of the proposed method. 


Limitations:
Not applicable. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper analyzes post-training quantization (PTQ) techniques in large language models, exploring various schemes, model families, and bit precision. The authors propose an optimized method called Low-Rank Compensation (LoRC) to enhance model quality recovery with minimal size increase.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. An evaluation and comparison of existing PTQ methods provide some insights for the community.
2. The paper is well-written.

Weaknesses:
Reading from paper provides a satisfying experience, but I must admit that my understanding of the LLM field is limited. Thus, my suggestions may be wrong, please directly point them out.

1. The points in Figure 1 are too dense and lack recognition.
2. Although the method is proposed for LLMs, it's also better to compare it with some traditional quantization methods on ResNet-series.

Limitations:
N/A

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper studied the post-training quantization method for 4-bit weight quantization and W4A4 quantization. The authors further proposed a Low-Rank Compensation (LoRC), to enhance model quality with low-rank matrices.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper is well-written and easy to follow.

Weaknesses:
The novelty is not very significant. The post-training quantization with finer-grained and zero-shift is not a new idea.

The sensitivity analysis is conducted with one particular quantization method, and the conclusion should be conditioned on that quantization method, but not general enough to conclude that PTQ exhibits the same behavior. I would suggest the authors to compare different quantization functions, such as minmax/percentile to deliver a more comprehensive conclusion.

The accuracy improvement is marginal. Figure 1 didn’t show much improvement compared to the previous naïve baseline of RTN. 


Limitations:
N/A

Rating:
4

Confidence:
3

REVIEW 
Summary:
This work focuses on systematic examination of various post training quantization techniques in large language models. Experimental analysis include comparison of different model sizes, different numerical precision, and quantization of only weights vs activations. In addition, the Low Rank Compensation method is proposed to enhance model quality recovery.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The research on LLMs is rapidly growing, but computational and memory capabilities are still limited when deploying huge models. Therefore, quantization is a must. The analysis presented in this work is definitely needed to advance LLMs deployment in multiple use cases. The paper flows logically and is well structured. The proposed compensation method is interesting.

Weaknesses:
1. Quantization may be input data specific. Same model topology when trained on different data may behave differently. It’s mentioned that you “use the zero-shot validation perplexity (PPL) differential on three datasets, namely, Wikitext-2 [23], PTB [22], and C4 [27], before and after the quantization”, however results presented in the following tables doesn’t indicate what error was achieved on which dataset. Could you clairfy?
2. Quantization may be operator specific. It would be interesting to list operators present in both model topologies and identify layers that were specifically sensitive to quantization.

Limitations:
Listed limitations and future work directions are clear and make sense.

Rating:
6

Confidence:
5

";0
rsrfEIdawr;"REVIEW 
Summary:
The paper proposes a sparse-view NeRF framework that jointly trains a NeRF with a monocular depth estimator. By adapting the MDE network to the target scene, the predicted depth will provide better geometry prior for the NeRF model.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The joint training of MDE and NeRF improves the model's ability compared to existing methods.
After training both models, we'll have a better NeRF that renders more accurate novel views and also a more accurate mono depth estimator.

Weaknesses:
- For the few-shot NeRF setting, since we hope to train the NeRF directly, we shall have access to camera poses for multi-views, right?
If that's the case, what's the benefit of using monocular depth estimators, compared to multiview stereo networks such as MVSNet?
- If we use COLMAP to obtain the camera poses, we should also be able to construct a point cloud. What's the benefit of using the proposed distillation mechanism, compared to overcoming the scale and shift ambiguity problem of mono depth with depths extracted from point clouds?
- How is Tab. 2 calculated? Are they evaluated on the trained views of ScanNet? 

Limitations:
Please refer to weaknesses.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper proposes a method to better use monocular depth in a few-shot NeRF setup. There are mainly two technical contributions to me:
1. Applying mono depth constraint to unseen view;
2. Un-distorting (scale and shift) monocular depth in a per-patch manner, rather than per-image.

---
**After rebuttal**: I have read authors' rebuttal and it addresses my concerns.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The motivation makes a lot of sense. Monocular depth estimation suffers from various distortions, and un-distorting it with a global scale and shift per image is only a rough approximation. Exploring better ways to un-distort monocular depth is definitely valid motivation.
2. It’s interesting to see that monocular depth networks actually perform decently on rendered images from half-trained NeRF models.

Weaknesses:
General
1. L118-L120: I disagree with this definition of few-shot, i.e. $| S < 20 |$. The definition should be based on view-angle and scene coverage, not on an absolute number of images. For example, in LLFF, a forward-facing scene, NeRF can be trained very well for $| S < 20 |$, because 20 images cover the simple forward-facing scenes very well. In contrast, 50 images might be challenging for 360-degree scenes.
2. Following point 1, how do the few-shot views cover the evaluated scenes? It would be good to have a top-view visualisation of selected cameras' positions and orientations.
3. How many patches are used in training? From the supp mat I can see the patch size is 64x64, i.e. 4096 rays. This means there is only one patch used during training? In this case, modelling patch-wise scale/shift is same as image-wise scale/shift?

Writing
1. Figure 1 caption: unclear, seems like should be … by _applying_ pretrained MDE … (missing a word _applying_)
2. Figure 2: 
    1. what is the input RGB and its monocular depth? I can see this is a top-view image of back-projected point clouds, but I cannot imagine what’s the input. 
    2. What is colour coding?
    3. I don’t see why Fig 2b is better than Fig 2a from this image. I understand the motivation and it is supposed to be better when un-distort with patch-wise scale/shift, but I cannot see that from this visualisation.
3. Sec 4.1 is kind of repeating intro. 
4. Symbol $M_l$ is undefined. I suppose it’s similar to $M_i$, but projected in $l \rightarrow i$ direction?
5. Symbol $s_i, t_i$ are undefined.

Limitations:
Yes.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper tackles the problem of sparse view NeRF reconstruction by leveraging on monocular depth estimation networks as a prior. The main difference between DaRF and existing work is it also computes for a depth loss on unseen views, in contrast to prior works that only constrain depth on the training views. Moreover, they also adapt, i.e. fine tune, the monocular depth estimation network to agree with the depth produced by the NeRF. They also introduce a confidence term that determines which pixels to use the depth loss on. They conduct experiments on Scannet and Tanks and Temples dataset.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The paper proposes to use monocular depth estimation as a prior to constrain NeRF optimization under the sparse view regime. Different from existing works, DaRF found that they can also use the monocular depth prior on unseen views, and even on NeRF rendered depth on early stage of the training, the monocular depth network is able to produce reasonable and cleaner depth maps that can supervise and constrain the NeRF loss (as shown in Figure 3). This is the main contribution and finding of this submission. The authors also showed an ablation study to justify the different components that were introduced.

Weaknesses:
The concerns I have for the paper is whether 1) the contribution introduced is enough for paper acceptance, and 2) how it positions itself and it claims in contrast to existing works. For the first point, the main contribution and distinction it has is the finding that the monocular depth prior can also be used to constrain unseen viewpoints. As shown in Figure 3, even on noisy NeRF renderings, the MDE prior can produce clean depth maps. An add-on to this is the slight improvement in results (as shown in the ablation study) by also fine-tuning the MDE network as NeRF is being optimized. However, the way the paper was written and motivated is the ambiguity in monocular depth estimation -- as written in the intro, Sec 4.1 and Sec 4.3, which is the main premise and story of the existing (though somewhat concurrent) work SCADE. The paper did cite SCADE, but however only mentions the difference that DaRF is able to train on unseen views. Differentiating with existing work and clarifying contributions can be improved for both paper presentation as well as the claims made for the submission. Now, the concern and question is whether 1) being the contribution of DaRF is enough as a contribution to meet the bar of Neurips. And for this reason I am giving an initial rating of borderline reject and would like to hear from the other reviewers.

Limitations:
The authors included the limitations in their supplementary material. However, according to the checklist, they claim to include the error bars. However, I don't think I saw error bars reported in the submission.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper addresses the problem of few-shot NeRF reconstruction. The authors propose using monocular depth estimation (MDE) networks to provide geometry prior to NeRF at both seen and unseen viewpoints. They propose overcoming the ambiguity problems associated with monocular depths by MDE adaption. Experimental results deomonstrate improvements in both rendered novel views and rendered depths.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
+ Unlike previous works which only exploit depth information at seen viewpoints, this work also exploits MDE to constraint NeRF at unseen viewpoints, leading to more robust and coherent NeRF optimization. The authors demonstrate through an example that the strong geometric prior within the MDE model enables it to generate reliable depth from noisy NeRF rendering. This makes MDE at unseen viewpoints feasible.
+ The proposed patch-wise scale-shift fitting helps reducing the impact of erroneous depth differences generated by MDE networks in distilling the monoclular depth prior to the NeRF.
+ Adapting MDE to absolute scene geometry embedded in NeRF further helps to resolve ambiguities in surface curvature and orientation, and improve multiview consistency.
+ The proposed method demonstrates sota results, both qualitatively and quantitatively, on two real-data sets, particularly showing superb results in rendered depths in few-shot NeRF.
+ Ablation study has been included to demonstrate the effectiveness of each major deisgn component.
+ Overall this paper is well-written and well-organized. It is easy to follow.


Weaknesses:
- In MDE adaption, it is not clear why the monocular depths predicted from the rendered input views instead of that predicted from the input views are adopted in (6). There is no explanations or disucssions on the effect of choosing between these two. There is also no explanations or discussions on why only input views are considered.
- In confidence modeling, why the predicted depth of a point in an input view (after scaling ana shifting) is directly compared with its rendered depth in a target view in (9)? Note both predicted depth and rendered depth are measured with respect to the viewpoints.

Limitations:
Limitations and impact are only included in the supplementary material, but not in the main paper.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper presents a new few-shot neural radience field approach based on joint monocular depth adaption. The main idea of the proposed approach is to utilize the monocular depth estimator to improve the geometry prior of NeRF representation. The motivation is reasonable. Also, it presents attractive performance on both indoor and outdoor scenes. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The idea of utilizing monocular depth estimator is interesing and the depth estimator can provide reasonable geometry information for the scene reference. 

2. The proposed approach has provided attractive performance on the real-world benchmarks. Sufficient ablations have been conducted to validate the design of the proposed algorithm. 

3. The presentation of the paper is good. 

Weaknesses:
1. The paper is based on SCADE[49]. According the results in Table 1, the proposed approach is lower than [49] on the LPIPS score. Is there any potential explanation of this result? Also, How about the results on the ""in-the-wild benchmark"" proposed in [49]?

2. How is case when the testing data is out of the distribution of the depth estimator? For example, in the case when the depth estimator failed to predict the accurate depth information.

3. How about the inference speed of the proposed algorithm?

Limitations:
The authors have not full discussed the limitations of the proposed approach. 

Rating:
5

Confidence:
3

";1
CFQBcz7k8n;"REVIEW 
Summary:
The paper studies adversarially robust learning with uncertain perturbation sets, where the set of points to which an adversary can perturb any test input is random, in contrast to previously studied settings of fixed known or unknown perturbation sets and unknown perturbation sets. The perturbation set is assumed to be coming from some fixed perturbation class.

Given a perfect attack oracle which certifies robustness or gives a successful attack, learnability is shown for concept classes with finite VC dimension with finite bounds on sample complexity and query complexity to the oracle, with some assumptions on U which are satisfied by commonly studied perturbation classes e.g. finite union of totally ordered perturbation sets which includes $L_p$ balls for a finite collection of $p$ values. 

When the classifier is equipped with abstention, learnability is possible under weaker assumptions e.g. finite disagreement coefficient of H wrt the data distribution. The authors further propose new notion of finite disagreement cover that depends on the perturbation class U, and reduce robust learning under abstentions to successful learning of a finite disagreement cover in the sense of their proposed notion.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- The work considers removing a common and unrealistic restriction in prior theoretical work that the adversary uses a fixed perturbation set.
- The authors give examples demonstrating power of abstention and the difficulty of agnostic learning under their definitions.
- Proofs or proof sketches of several major results are included and discussed.
- Comparison to prior works and techniques e.g. Montasser et al. 2021 helps clarify the novelty and technical contributions of the present work.

Weaknesses:
- Realizability: Realizability of perturbation set seems like a strong assumption on the adversary and should be discussed. No positive results are given in the main body in the agnostic case, which could be a limitation of the model under study.
- Some results are not adequately discussed, e.g. Theorem 5 and last paragraph on Pg 9.

Limitations:
I do not anticipate potential negative impacts as the work is primarily theoretical, but encourage the authors to summarize limitations and further questions in a conclusion section e.g. lack of positive results in the agnostic case, tightness of sample complexity bounds, etc.

Rating:
6

Confidence:
3

REVIEW 
Summary:
- This paper bridges the gap of PAC learning theory for adversarially robust learning between completely known and completely unknown perturbation across various settings.
- The authors introduce a notion of hypothesis class-induced partial ordering on the class of perturbation type which they use throughout the paper (setting 1).
- They show that in a realizable setting, hypothesis class with finite VC dimension and setting 1 are robustly learnable. They show that the same is not valid for agnostic setting without extra assumptions such as access to perfect attack oracle.
- They further show the existence of perturbation type and hypothesis class which cannot be robustly learned without abstention even with access to perfect attack oracle.
- They also investigate hypothesis classes with finite disagreement coefficients and present results for this setting.
- The define an $(\epsilon, \mathcal{H})$ cover for a class of perturbation types and use this definition to provide a generalization of their previous results.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper is well-written for the most part and provides a good explanation for the results. Furthermore, the claims seem mathematically sound and the authors have made good use of statistical learning theory to present their results. The results are interesting and close the gap between two known settings. They have also introduced some new notions such as the partial ordering of perturbation types with respect to hypothesis class and  $(\epsilon, \mathcal{H})$ cover for a class of perturbation types which are interesting concepts.

Weaknesses:
I liked the results and the general writing style of the paper. At times, the novelty seems to be a derivative of existing works with proofs being a clever extension of the proofs of existing settings. I believe this is bound to happen as the problem setting itself is an intermediary between two known settings. 

The following is not a criticism but a remark - It would have been better to include some more mathematical proofs in a theoretical paper like this. However, I understand that space limitations sometimes prohibit this.  

Limitations:
The authors have addressed the limitations adequately. In that, this paper doesn't propose any (efficient) optimization algorithms, it rather presents sample complexity guarantees with Oracle access to the solution of the optimization problems.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper studies robust classification when the perturbation region an adversary can access is unknown to the learner, but where it is guaranteed it comes from a certain class of perturbations, which is known to the learner. The authors show various results when the learner has access to a perfect attack oracle (PAO) (which either returns a robust loss of 0, or an admissible perturbation on which the hypothesis is not robust). 

For perturbation classes with total order, without access to the PAO, finite VC dimension is sufficient in the realizable setting, but not in the agnostic one. With access to the PAO, this becomes possible, by a reduction from agnostic to realizable robust learning. 

When the perturbation class is a finite union of totally ordered perturbations, robust learning is also possible with the PAO (but in general, not without it). Removing structure on the perturbation class renders robust learning impossible (even with PAO). 

The authors then consider robust learning with abstentions, where the learner cannot abstain on an unperturbed point, but can on a perturbed instance. They show that, in this set up,  robust learning is possible. They then relate robust learnability with the disagreement coefficient of a hypothesis class. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- Important question, nice set up that lies between unknown perturbation sets and models where the perturbation function is known a priori 
- The results are interesting and draw on multiples different concepts (learning with abstentions, disagreement coefficients, previous robust learning results, etc.)
- The paper is clear and well-written
- Meets the technical standards of NeurIPS

Weaknesses:
- No conclusion section - please add one for the final version!
- MRERM: it seems to be quite a strong assumption to have access to this oracle? Especially since it returns the perturbation $u^*$! The existence of $u^*$ is fine, but that an algorithm can find it for any/most $\mathcal{H}$ seems quite strong. Could you expand on this, and give examples? 

Limitations:
yes

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors present theoretical results on classification in a version of the PAC model where an adversary can perturb input instances.  Previous work assumed either a fixed perturbation type, known to the learner, or an unknown perturbation type.  This paper explores a middle ground where the perturbation type is a member of a fixed class of perturbation types.  The learner knows the class, but does not know which perturbation type within that class is being used.  For example, the perturbation class might correspond to balls with different radii.

The paper presents results on learning for a class U of perturbation types and a hypothesis class H.  It defines a partial order on perturbation types in U, defined with respect to H.  It considers variants of the learning model where the learner can abstain from predicting the label of a given example (which is counted as a misclassification error unless the example has been perturbed).  It also considers a variant of the learning model where the learner can has query access to a ""perfect attack oracle,"" a type of oracle studied in previous work on learning with unknown perturbation sets.  

The results in the paper are all with regard to sample size.  (Computational complexity is not considered.). The first two main results are as follows.  (1) When the class U is totally ordered with respect to H, and H has finite VC dimension, then learning in the realizable case is possible with a polynomial-sized sample (treating VC-dimension as a constant). (2) For the unrealizable case, when U is totally ordered wrt H, a polynomial-sized sample also suffices if the learner can also make polynomially many calls to a perfect attack oracle.  

The next result considers a similar situation to that in (1) above, except in a more general setting where where U is a union of totally ordered subclasses.  In this setting, even with finite VC dimension and realizability, learning from (finitely many) examples is not possible, but can be achieved with a combination of a polynomial-sized sample and the ability to make polynomially many calls to a perfect attack oracle.  

The remaining results are concerned with learning with abstentions.  The main results are (1) Learnability in the realizable case, with abstentions, when U is totally ordered and H has finite VC dimension (2) Learnability in the realizable case, with abstentions, when U is the class of all perturbation, when the class has finite VC dimension and finite disagreement coefficient.  A result is also given on learning with abstentions in the realizable case if there is a successful finite-disagreement-cover learner for (H,U).

The paper also contains simple negative results demonstrating the need for abstentions, total order, etc.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
A comprehensive exploration of uncertain perturbation sets, bridging work on unknown perturbation sets on the one hand, and a single known perturbation set on the other hand.  The paper is well-written.

Discussion phase:  I raised my score from 5 to 6.

Weaknesses:
From the presentation in the main paper, it does not seem that the paper introduces new techniques or that the presented results are surprising.  It seems as if the techniques have already been used in previous work on perturbations and that the results are similar.  Overall, the work seems like a solid contribution, but lacking in real novelty.

Limitations:
I have no concerns in this area.

Rating:
6

Confidence:
3

REVIEW 
Summary:
In adversarially robust PAC learning to test time attacks, usually we assume that the learner knows the perturbation function. 
Montasser et al. ['21] studied a setting were the learner doesn't know the perturbation function, but can interact with it through some oracles. In this paper, the setting is in between, assuming that there is a class of possible perturbations $U$ known to the learner, and the performance of the learner is measured on the worst-case perturbation function in this set. For example, the perturbation can lie in a ball centered in the original input, but the norm isn't known. 

The main contributions are as follows. 

- Considering an order on $U$ w.r.t. $H$, finite VC is sufficient for learning in the realizable case, but not in the agnostic case. 

- When the learner can interact with $U$ through a perfect attack oracle, and assuming some structure on $U$, finite VC is sufficient for learning. This is an improved result compared to the setting of an unknown perturbation function, where finite Littlestone dimension is sufficient, and $VC(H)\leq Lit(H)$ and the gap can be arbitrarily large.

- Introducing a setting where the learner can abstain, and showing that finite VC is sufficient for learning (assuming some structure on $U$).

- Assuming that $H$  has a finite disagreement coefficient (a parameter that is related to active learning), then $H$ can be learned with respect to every class of perturbations.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The in-between setting of robust learning suggested in this paper is natural and complements the picture of the other two settings (known $U$ or completely unknown $U$).
This paper studies thoroughly various scenarios, showing some limitations of this model, and when we can guarantee robust learning. 
The results look correct to me. 
I think that this paper could be of interest to the community of theoretical robust learning.

Weaknesses:
If I'm not missing anything, the technical contribution of the paper is moderate and mostly relies on standard ideas from PAC learning. 

Many references on theoretical robust learning are missing. For example, H-consistency bounds for surrogate loss minimizers (ICML 2022), Multi-class H-consistency bounds (NeurIPS 2022), Theoretically grounded loss functions and algorithms for adversarial robustness (AISTATS 2023), Cross-Entropy Loss Functions: Theoretical Analysis and Applications (ICML 2023), A Characterization of Semi-Supervised Adversarially Robust PAC Learnability (NeurIPS 2022), Adversarially Robust PAC Learnability of Real-Valued Functions (ICML 2023), Sample complexity of robust linear classification separated data (ICML 2021)...and many more!

Limitations:
I don't see any.

Rating:
6

Confidence:
5

";1
yQSb1n56lE;"REVIEW 
Summary:
In this paper, the authors propose a way to decouple the optimization process of RNA secondary structure prediction. Specifically, they decompose the constraint satisfaction problem into row-wise and column-wise optimization. Instead of hand-crafted features, attention maps are used to learn the pair-wise interactions of the nucleotide bases.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The authors show that it is more effective to use the attention maps as the input and then use U-Net to predict H, compared with using the hand-crafted features as input, which is interesting.
2. The proposed method reduces the inference time dramatically compared to various methods.
3. It achieves promising results on the RNAStralign dataset and large-scale benchmark evaluation.


Weaknesses:
1. The generalization ability is limited because the proposed method cannot achieve the best recall on ArchiveII and bpRNA-TS0 datasets.

Limitations:
I did not find the potential negative societal impact of this work.

Rating:
5

Confidence:
2

REVIEW 
Summary:
The paper introduces RFold for RNA secondary structure prediction (a prediction of LxL binary matrix). It proposes to add a row-column-wise softmax at output of the model, before computing the L2 loss with respect to the ground truth. The experimental results show higher precision and recall compared to prior works.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The paper is well-written with necessary backgrounds and basic introductions, problem formulations. Thus, this paper suited well for general audience of NeurIPS.
- RFold delivers strong performances in two commonly used datasets for evaluating RNA secondary structures.

Weaknesses:
The main concern is the limited novelty, RFold is incremental over Ufold. Both methods follow the paradigm of mapping a sequence RNA (using $\theta_{1}$) to $[L \times L \times n]$ features and further mapping the $[L\times L \times n]$ features (using $\theta_{2}$) to $[L \times L \times 1]$ output prediction. RFold differs from Ufold in two parts:
- RFold proposes $\theta_{1}$ to be represented by an attention-based layer. (RFold and Ufold use a similar, if not identical, $\theta_{2}$.)
- RFold adds a column-wise and a row-wised softmax after the $\theta_{2}$, before computing L2 loss.

Thus, RFold makes a few architectural modifications and improves the results.

E2Efold and Ufold also have a section on evaluation with pseudoknots on the RNAStralign test dataset, which this submission does not have.

Limitations:
The paper has no section for limitations.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This work presents an efficient and accurate approach for end-to-end RNA secondary structure prediction.
The optimization problem formulation and its solution are well defined.
The results are strong and supported by visualizations and ablation studies.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The key strengths of this work are:
1. Inference is an order of magnitude faster than previous methods.
2. Inference is between 4-20% more accurate than previous methods, with significant gains specifically in long-range interactions.
3. A well defined optimization problem formulation and solution.
4. Including visualizations and ablation studies underscores the gains achieved through the optimization formulation and attention architecture.
5. The results are validated using multiple datasets and baselines.

Weaknesses:
Weaknesses of this work are:
1. There is a discrepancy in the definition of G in equation 12, where it does not incorporate the softmax function. 
However, in equation 15, it is assumed as if it does. This can be fixed by introducing a new notation, such as G_{hat}, 
which includes the softmax function and will ensure consistency.
2. The definition of well-known metrics in section 5 is redundant.
3. The comparison between Rfold and Ufold could be more comprehensive, 
describing their similarities and differences.
4. There are a few minor typos, and the writing may be improved.

Limitations:
The limitations are adequately addressed.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes RFold, a simple and effective RNA secondary structure prediction algorithm. It adopts attention maps to learn informative representations for RNA rather than hand-crafted features. Then, based on a decoupled optimization process, RFold simplifies and guarantees satisfying the hard constraints on the formation of RNA secondary structure. Through the empirical experiments, the authors demonstrate that RFold achieves state-of-the-art performance with better computational efficiency compared to the previous works.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- The proposed decoupled optimization seems simple, but surprisingly effective for RNA secondary structure prediction. To the best of my knowledge, the proposed method is novel in the domain and might be promising for the broader machine-learning community.
- The proposed method shows great performance in three RNA benchmark datasets outperforming the previous state-of-the-art method by a significant margin. Some issues need to be addressed regarding the experiment setup (please refer to the weaknesses), but the improved performance seems truly impressive.

Weaknesses:
Major comments:
- [Data Split] To best approximate real-world applications that may require the prediction of novel structures, RNAs from the train/val/test set should bear minimal sequence and structural similarities. In contrast, it seems the authors have split datasets so that each RNA family has a similar fraction in each set. I think it may overestimate the true prediction performance of RFold. Likewise, “generalization to other datasets” experiments do not provide information about sequence/structure similarities between the datasets. If they are similar, it may not be a fair evaluation of generalization performance.
- Since the authors stated deep learning methods do not ignore the biologically essential structure such as pseudoknots, can you provide additional separate evaluation under the (non-) existence of pseudoknots?
- According to UFold, the bpRNA dataset contains mostly within family RNA species and does not adequately show the true generalization performance of the models. Can you provide additional evaluations with cross-family experiments?
- [Inference Time] It’s unclear whether the results are credible. The inference time can be quite different based on what type of machine (CPU, GPU, etc.) is used for the measurement. Since the other results seem to be excerpted from the UFold paper, the environments of UFold and RFold are likely to be different.
- [Reproduciblity] Architectural hyperparameters are missing. In addition, training codes do not seem to be included in the supplementary.

Minor comments:
- [Data Split] The authors stated that they split the RNAStralign dataset following the E2Efold paper. Can you confirm that all the methods including RFold used the same data splits? RNA sequences often have high sequence and structure similarities, so if you used different data splits it might affect the performance. 
- As the authors stated, other algorithms often post-process the outputs to satisfy the constraints. Can you also show how the results are improved for RFold-E/S with the post-processing?

Limitations:
The authors have not discussed the limitations of the work.

---Post-Rebuttal Comments---

I appreciate the authors' dedication evident in their comprehensive responses. They have effectively addressed many of the concerns I had about the paper. Overall, while some concerns persist, I am inclined to believe that by incorporating the authors' responses, the manuscript's quality would be improved. Hence, I've adjusted my rating to 5.

Rating:
5

Confidence:
3

";0
8S6ZeKB8tu;"REVIEW 
Summary:
The paper considers the problem of estimating the accuracy of noisy judges/classifiers in a streaming fashion, using only unlabeled data. Specifically, the goal is to compute the accuracy of each judge while processing items and the judge predictions for each item as part of a stream, without any associated labels for the items.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The problem of evaluating noisy judges that is being explored by the paper is an interesting and important one.

Weaknesses:
1. The main method presented in this paper (i.e., section 3) is not novel. For example, it can be seen as a special case of the approach presented in *Platanios, E. A., Blum A., and Mitchell T. ""Estimating accuracy from unlabeled data"" UAI (2014)*, which relaxes the independence assumption (though does not consider the streaming setting) and is also not cited in this paper. In fact, a lot of related work is missing including derivatives of the aforementioned paper (e.g., a direct follow-up in ICML 2016).

2. The premise of section 4 is weak. The idea that the method of section 3 is “self-alarming” because when you have dependent classifiers the accuracy estimates will be invalid is not completely correct. While this may capture some cases, there are still a lot of cases where you can have dependent classifiers, and where there exists a valid solution to the presented system of equations. Thus, I am not convinced by the main claim of this section.

3. The paper considers a streaming setting but it does not provide motivation for it. For example, it was not clear to me why we cannot store the predictions of the classifiers as items are processed in a database and then perform accuracy estimation periodically. If we have 8 classifiers and 2 possible labels, this would require 1MB per 1 million items, which does not seem expensive (and we can also perform random sampling if space becomes an issue).

4. The paper is presented in a manner that is hard to follow and could be significantly improved. I The whole paper would be presented in a simpler and more organized manner, but section 5 was particularly hard to follow without spending a significant amount of time to understand the argument that was being made.

5. The experimental evaluation is a bit lacking in that only toy datasets are being used, there is no explanation for what they are and why they are interesting, and there are very limited results being presented. Ideally, I’d like to see an “Experiments” section in the paper that describes the setup targeted at testing some hypotheses, the datasets, and the evaluation metrics, and then presents and discusses the evaluation results.

Limitations:
There is no discussion of limitations and potential negative social impact in this paper. One recommendation would be to try and think about what the implications could be for say voting systems, and also in situations where these methods are used to evaluate people whose income may depend on this evaluation (e.g., crowdworkers). In this case, the independence assumption being made by the paper may be too strong and yield in incorrect evaluations that could negatively and unfailrly affect the income of those people.

Rating:
2

Confidence:
5

REVIEW 
Summary:

This paper introduces a new inferential evaluator for evaluating noisy binary classifiers on unlabeled data in a streaming manner. Specifically, compared to the evaluator based on majority votes, the new evaluator gives a more complete and reasonable modeling of the true label prevalence and each classifier’s accuracy. In addition, the property of the new evaluator is also mathematically discussed, and the relationship between error dependence and the evaluator estimate is empirically discovered through experiments.

Soundness:
1

Presentation:
1

Contribution:
2

Strengths:

1. The paper addresses a significant problem in machine learning - evaluating the performance of binary classifiers on unlabeled data.
2. The proposed methods could have wide-ranging applications in various fields where machine learning is used, making the paper highly relevant.
3. The author provides mathematical proof to support the proposed methods and conducts empirical tests on several datasets.
4. The proposed generic framework that is based on algebraic geometry can cast a positive influence on evaluation methods on unlabeled data.
5.  The new algebraic evaluation method bypasses the representation and OOD problems in ML.

Weaknesses:
1. The paper is very hard to read and lacks the background to help the reviewer understand and improve the reading experience. Moreover, the supplement mentioned in lines 124, 171, 236, and 292 is missing from the paper. There are no detailed proofs for all the theorems.
2. The organization of the paper is confusing. The logic chain of the whole paper needs to be improved, better briefly introduce the outline and main content of each chapter at the beginning.
3. The aiming research problem needs to be explained formally in math language and to be explained clearly with intuitive explanations, better with a toy example or case study.
4. The current limitations of existing research, the proposed solutions (contributions of this paper), and the aiming experimental questions are not clearly listed, making it hard to catch the author's idea.
5. More experiments are needed. There are no experiments supporting that the performance of a majority vote-based evaluator is worse than that of the inferential one. And from the perspective of experiments, the advantage of the new evaluator is not made clear.
6. Only label prevalence is formalized in the paper, while there are no formulas for classifier accuracy.
7. The complexity of the concepts and the heavy use of mathematical proofs might affect their clarity. The author could consider providing more background information, intuitive explanations, or visual aids to improve the paper's accessibility.
8. The experiment part fails to show the superiority of the proposed method compared with other baselines.
9. The equations need to be carefully edited using formal math language. The space should be used for some meaningful and essential equations, not for simple ones such as summation or average operations.
10. Typos: In line 167, ""it could have been a βitem, not an αone"" should be ""it could have been a β item, not an α one"".

Limitations:
   The paper concludes with a brief discussion of how algebraic stream evaluation can and cannot help when done for safety or economic reasons.

Rating:
3

Confidence:
2

REVIEW 
Summary:
This paper considers the problem of evaluating noisy binary classifiers on unlabeled streaming data. It aims to estimate the prevalence of the labels and the accuracy of each classifier on them, given a data sketch of label predictions by the members of an ensemble of noisy binary classifiers. The authors propose two algebraic evaluators based on the assumption of error-independent classifiers: the first is based on an additional assumption of majority voting, and the second is fully inferential and is guaranteed to contain the true evaluation point.  

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The results of this paper seem to be well supported by the rigorous mathematical analysis as well as empirically demonstrated on three benchmark datasets.

Weaknesses:
- The independence assumption may not be satisfied in practice, especially when the ensemble of classifiers consists of different models trained on the same or overlapping datasets, or even the same model but trained for different durations.
- The significance of this paper is unclear. It would be better for the authors to provide some concrete real-world examples that fit the problem setting of this paper and explain the possible uses of the quantities desired to be estimated in these examples.
- The proposed evaluators may be sensitive to noise or corruption in the data sketch. The solutions of the algebraic equations will change if the data sketch is not perfectly recorded, leading to mistakes in distinguishing between independent and correlated evaluations. 

Limitations:
The authors has discussed the limitations of this paper.

Rating:
4

Confidence:
2

REVIEW 
Summary:

The paper addresses making decisions based on the outputs of three binary classifiers. More precisely, it focuses on evaluating the performances of noisy classifiers. It considers majority voting on one hand, and a proposed evaluation scheme based on the classifiers' accuracies. The paper establishes several theorems so as to demonstrate the superiority of the second (proposed) evaluation scheme. Then, experiments are conducted to test the ability of the proposal to avoid making decisions in problematic situations—e.g., correlated classifiers. 


Soundness:
2

Presentation:
1

Contribution:
1

Strengths:
I see no particular strength in the paper that would mitigate its flaws. 

Weaknesses:
The proposal suffers from several major flaws. 

First of all, it is badly written. The problem is not clearly stated. The mathematical objects (typically, the prevalence of the labels, and the label accuracies) are not properly introduced and defined. Some key notions (such as the ""evaluation variety"", the precise definition of correlated classifiers, among others) are also not defined. There are frequent references in the text to notions which have not been exposed yet (e.g., ""the evaluators for binary classifiers"" in the introduction, Theorems 1 and 2 in the introduction as well, Theorem 3 in Section 1.2). 

Besides, the paper ignores a large amount of literature. The problem addressed has connections with computational social choice (voting schemes), of which some works are mentioned. But it also relates to classifier combination (boosting, error-correcting output codes, weighted averaging, racing algorithms, etc): the problem of evaluating the performance of an ensemble has been addressed in a number of works which are ignored here. 

Last, but not least, the paper focuses on a very specific case—the ensemble has only three classifiers. This is very restrictive and such ensembles are hardly used in practice. Some claims are not supported—e.g., in Section 1.1, ""Seemingly correct estimates are estimated values that seem to be correct because they have this real, integer ratio form. Estimates that do not have this form are obviously incorrect."" This is not the case of the $F_\beta$ measure, for instance. 


Limitations:
The authors have addressed the limitations of their approach, but in my opinion only in a restricted way. They have not addressed the potential negative societal impact of their work, but I do not think that this is crucial here. 


Rating:
2

Confidence:
4

REVIEW 
Summary:
This paper considers the problem of evaluating an ensemble of binary classifiers on unlabeled data in a streaming setting. The authors first describe a baseline which treats the majority vote as the correct label and evaluate each classifier accordingly. Then they propose an evaluator based on an assumption that the classifiers are independent. The algebraic expression for this evaluator should return rational numbers if the assumption holds (as they should correspond to ratios of integer counts). Thus, this evaluator has failure modes that can be detected clearly unlike the majority-voting baseline that may return incorrect but seemingly sensible values.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
While I am not an expert in evaluation using unlabeled data and cannot speak definitively, the proposed algebraic evaluation and characterizing failure modes by algebraic failures seem creative and novel.  

Weaknesses:
I found the paper overall quite difficult to follow, and thus my assessment of its technical contributions may be limited. More thorough motivation and background on the problem setting (e.g. using real-world applications), as well as careful characterization of the proposed algebraic evaluator in contrast with existing approaches, would help make the paper much more approachable. I also had a hard time following most of Section 1 (especially 1.2) without the technical details in Section 3.

The paper is also missing some related work discussion, and its contributions with respect to prior work is not very clear. I struggled to see the connection to the works mentioned in the first paragraph of Section 1.3. Another work that appears very relevant is [1]; how does this paper relate to their approach?

Throughout the paper, only the setting with three binary classifiers is considered. A more general formulation may be helpful. As far as I can tell, this approach would scale exponentially in the number of classifiers which could limit its impact in practical settings.

Empirical evaluation was limited only to analyzing the failure rates, and there were no experiments on how well the proposed approach performs as an evaluator (i.e., how close are the error rate estimates to the true error rates?). 

[1] Platanios, Emmanouil Antonios, Avrim Blum, and Tom Mitchell. ""Estimating Accuracy from Unlabeled Data."" 2014.

Limitations:
Overall, yes. One limitation that was not discussed is that the approach seems to scale exponentially in the number of classifiers.

Rating:
2

Confidence:
2

";0
LUVqEs90mq;"REVIEW 
Summary:
The paper proposes an interesting method called PIPS for sampling transition paths between metastable states in molecular systems. These transitions are difficult to sample using standard Molecular Dynamics (MD) simulations due to high energy barriers. Traditional methods augment MD with a bias potential based on Collective Variables (CVs), but selecting appropriate CVs can be challenging and limiting for larger systems. PIPS instead does not rely on CVs and utilizes stochastic optimal control with neural network policies to efficiently learn the optimal bias potential sample molecular transitions. The method demonstrates success in generating low-energy transitions for several molecular systems.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. Presentation: The technical part of the paper is very well-organized and easy to follow. Every component, like MD, optimal control, and their connections, are well explained to me.

2. Originality: The paper introduces a new ML method, PIPS, for sampling molecular transition paths without the need for Collective Variables. This offers a fresh perspective on solving the problem of high energy barriers in molecular dynamics simulations via stochastic optimal control.

3. Technical soundness: The paper establishes a formal relationship between the problem of sampling molecular transition paths and the stochastic optimal control. This theoretical foundation well motivates the credibility of the proposed method.

4. Generalizability: PIPS does not depend on CVs, making it applicable to many molecular systems where traditional methods may not be suitable,w without intense requirements for domain knowledge. This generalizability is particularly valuable as molecular systems are under exploration.

Weaknesses:
1. Transferability across all molecular systems. I list this critical limitation in the limitation section below.

2. ML contribution: One minor concern for accepting the paper to NeurIPS is that the paper is its contribution as an ML paper. ML technical parts in the paper, such as optimal control and practical neural network implementations, are all existing things. Therefore the ML contribution is not significant for the ML community.

3. Domain-specific impact: Continue from point 1, apart from ML contribution, I fully understand the value of the application paper for addressing a critical real-world problem. Then my second minor concern is that even the application scenario is a little niche. Topics such as MD acceleration and protein folding are definitely highly impactful, but the specific problem ""Free Sampling of Molecular Transition Paths"" sounds niche to me. Maybe provide a little more real-world impact and successful application examples in the paper?

4. I have several questions listed below.

Limitations:
The main limitation in my mind is transferability to all systems: While PIPS shows promise in generating transition paths when trained on a single molecular system, is it possible to transfer the effectiveness to other or even all molecular systems? Or, for a weaker setting, transferable to arbitrary predefined initial and terminal states? If not, for even solving one structure, we need to train PIPS with extensive MD samplings, which might be too expensive.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper addresses the task of sampling transition paths between metastable states in molecular systems. This problem is particularly pronounced when dealing with complex systems, where traditional methods often fall short due to the need for chemical intuition and the limitations in scalability. To overcome these obstacles, the authors propose a novel approach called path Integral
stochastic optimal control (PIPS). Unlike previous methods, PIPS does not rely on the explicit selection of collective variables (CVs), making it applicable to larger systems without sacrificing accuracy.


Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
The paper provides a novel perspective and approach to sampling transition paths by leveraging stochastic optimal control theory and has already influenced a series of recent works. In particular, phrasing the problem as Schrödinger bridge (SB) problem is interesting and creative, as it presents a new application for optimal control as well as a clever parameterization of molecular dynamics frameworks.

Weaknesses:
Of course, the theoretical foundation utilized in this work is well-established in other fields. While this can be considered a weakness, I feel this work still provides a valuable contribution to the machine learning community.

The experiments are conducted on three molecular systems, i.e., alanine dipeptide, polyproline, and chignolin. While rather simple systems, I believe, these are sufficient illustrative examples to serve as an evaluation of this work. Could the authors still elaborate on how and why no larger molecules are considered?

For me, a core limitation is the lack of baselines, i.e., how do classical SBs or diffusion SBs such as [1] and [2] in this framework?

Is there any way to generalize to unseen molecules? Are there appropriate datasets available to tackle such tasks? What would be the necessary steps to achieve something like this?


[1] Tianrong Chen, Guan-Horng Liu, and Evangelos A. Theodorou. ""Likelihood training of Schrödinger bridge using forward-backward SDEs theory."" International Conference on Learning Representations (ICLR), 2022.

[2] Valentin De Bortoli, et al. ""Diffusion Schrödinger bridge with applications to score-based generative modeling."" Advances in Neural Information Processing Systems 34, 2021.

Limitations:
See weaknesses.


Rating:
7

Confidence:
4

REVIEW 
Summary:
This manuscript addresses the molecular transition pathway sampling problem, which is a fundamental and significant topic in computational chemistry/biology research. Specifically, the authors first made connections between transition path (TP) sampling, the Schrödinger Bridge Problem (SBP), and Stochastic Optimal Control (SOC), where the theoretical formulations are clearly described to facilitate understanding. Then, they proposed PIPS, a modified version of the PICE algorithm tailored to sample molecular transition paths without explicitly specifying the Collective Variables (CVs). It has been shown that one could learn a neural network bias potential (or force) policy to find the optimal transition paths in a few molecular systems, bypassing the need to use CVs or run long-time MD simulations.

I find this work interesting and enjoyed reading it. However, please see below for some of my concerns and questions.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The research topic (i.e., molecular transition path sampling, rare-event sampling) is quite significant and has garnered much attention over the years. This work provides a novel perspective towards more effectively and efficiently identifying molecular conformation changes between metastable states.
- The paper is clearly written and the main ideas are easy to follow.
- I would say this work is inspirational and has the potential to lead to more follow-up work in this direction.

Weaknesses:
- The theoretical part of this work has already been developed for a while (i.e., Kappen and Ruiz, 2016). This work is an application of existing theoretical tools (with some modifications) to solve a particularly important biological problem. It is not necessarily a *weakness* yet I just put this comment here.
- While making connections between TP, BSP, and SOC, the transition into adopting the PISOC (line 177) is a little bit abrupt.
- The policy network u has not been discussed much, i.e., is MLP+act_fn a good universal solution to any molecular system under the PIPS framework?

Limitations:
The authors discussed PIPS limitations in the final Discussion section.

Rating:
7

Confidence:
4

REVIEW 
Summary:
Molecular dynamics simulations yield trajectories which show how a system develops from a starting configuration to a final configuration over time.
Given a system with more than one (meta-)stable state, sampling trajectories which include transitions from one (meta-)stable state to another one might occur sparsely due to high energy barriers.
A solution is biasing the trajectory sampling procedure towards the ones which include transitions.
While classical approaches rely on Collective Variables (CV) to bias the sampling procedure, the authors present a CV-free method by learning an MLP, which takes the system configuration as inputs and biases the trajectory sampling procedure towards those who include transitions.
The authors recap that the problem of sampling these paths can be solved by stochastic optimal control theory and that related work suggested the PICE algorithm to find a way to control the path sampling.
They adapt the PICE algorithm and use the adaption - called PIPS - in their work.
PIPS allows to control the path sampling by introducing a learnable bias module, which is the MLP mentioned above.
The authors consider two versions of PIPS by modelling a) the bias potential and b) the bias force.
The authors apply their method to three different molecular systems.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
(S1) **significance**: Being able to sample trajectories of molecular systems which include transitions is a significant problem in physics. Also, exploration of CV-free methods is a significant research direction for molecular dynamics related fields.

(S2) **originality**: Applying the adaption of PICE to molecular systems is novel. Also the comparison of the two versions of PIPS (a) and b)) includes some novelty. The smoothing of the loss function is introduced newly and intuitively seems to be an important detail for succesfully ML model training.

(S3) **quality**:
- Code is given. The illustration notebooks - e.g. ExampleAlanine.ipynb - helps to understand what was done and give insights to model training and path sampling.
- The section in which connections between transition path sampling, the Schrödinger bridge problem, and stochastic control theory are summarized and described is very informative and well understandable.
 
(S4) **clarity**: The problem setting and the introduction to molecular dynamics and potentially occurring problems in sampling transition paths under the presence of high energy barriers are written clearly and very well. Section 3 is well structured and explains very well why SOC theory is useful for the given problem setting.

Weaknesses:
(W1) **significance**:
- Applying ML models to molecular systems to sample rare events is already done here [1]. Notably, the input to the model described in [1] is CV-free since Cartesian coordinates of the atoms are fed into the model.
- While significance is given from a physics point of view, there is hardly any core ML-related novelty. The ML-related part of the manuscript is learning the bias function with a standard MLP.

(W2) **originality**: While (S2) is given, the connection between biasing path sampling and SOC is already described by Kappen and Ruiz. The adaption of PICE to derive the new method PIPS just seems to include a smoothing of the loss function. So it seems like the major part of the method is re-using PICE.

(W3) **quality**: The experimental section is flawed because of missing baselines and missing error bars and statistical tests:
- All performance values in Table 1 are reported without error bars. Also statistical tests are missing.
- Variation across training reruns is missing
- [1] is neglected
- CV-based baselines are not included but would be desired since it is unclear how PIPS performs compared to methods which use CVs.

(W4) **clarity**:
- Sparse method description (PIPS): Having described very well where PICE comes from and that PICE is able to provide a solution to the transition path sampling problem, the manuscript lacks information about the presented method (PIPS). E.g., pseudo code for training and sampling would have been helpful.
    - Additional confusion comes from the fact that the authors describe first that the PICE algorithm helps to find  a suitable choice of the control $\boldsymbol{u}_\theta$ but later they describe that the learnable component of PIPS is $b$ (or $\boldsymbol b$). Even though the relation between the control $\boldsymbol u$ and the bias potential is mentioned it remains unclear what the MLP exaclty models and how the model outcome exactly influnences the path sampling.
    - Even though some of the information might already be spread in the manuscript, the authors should think about providing (again) relevant information for PIPS in section 4. This also includes information about the exact loss function, training setup, and hyperparameter selection.
    - Theorem 3.2 shows that SOC solves the BPTP problem with the ""standard"" choice of $\varphi$ but the authors choose a different one for PIPS. The authors missed to include mathematical implication of changing $\varphi$ in terms of whether it is still guaranteed find a BPTP problem solution with SOC.
- Mathematical notation and formulas: The clarity is diminished due to unclear notation, e.g., $\pi _0$ (l. 171) vs $\pi^0$ (l. 191) and $\pi _{\boldsymbol u^*}$ (l. 201) vs $\pi ^{\boldsymbol u^*}$.
- Section 3 - contribution vs related work: It remains unclear which parts are taken from related work and which parts are newly developed. E.g, [2] includes already a lot of the findings presented in equations (9) - (14).


Minor points:
- l. 174 - typo: $\boldsymbol u(\boldsymbol x_t)$
- l. 184 - typo: (eq. (5)) 

[1] Sun, Lixin, et al. ""Multitask machine learning of collective variables for enhanced sampling of rare events."" Journal of Chemical Theory and Computation 18.4 (2022): 2341-2353.
[2] Kappen, Hilbert Johan, and Hans Christian Ruiz. ""Adaptive importance sampling for control and inference."" Journal of Statistical Physics 162 (2016): 1244-1266.


Limitations:
-

Rating:
3

Confidence:
3

";1
3ZICE99e6n;"REVIEW 
Summary:
The paper presents a learning-based framework on the well-studied neural surface reconstruction problem. The key contribution of this paper is to take the complex photon-particle interaction into account and present a more generalized pipeline rather than relying on volume rendering. The proposed framework released the power of the transformer to achieve enhanced feature representation of sampled points along the ray. Experiments on several popular benchmarks have shown the effectiveness of the proposed approach. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
(1) The idea of modeling the complex light transport and releasing the flexibility from regular volume rendering is novel and interesting.
(2) Overall, the paper is well presented and easy to follow.
(3) The paper has achieved state-of-the-art generalizable neural surface reconstruction performance across different datasets.


Weaknesses:
(1) (Major) The idea of using 3D feature volumes with hybrid resolution is not new and has been proposed in NeuralRecon (https://arxiv.org/pdf/2104.00681.pdf). Besides, since there are two major differences (the elimination of FPN and the construction of multi-level projected feature maps) between the proposed hybrid extraction and the original one, it is better to make separate ablation studies to further verify the effect of the two variations.
(2) (Major) For the ablation study of the occlusion transformer, compared with directly removing this module, a better ablation way is to attend every point’s feature as the input of the key embedding in the self-attention computation. This way ensures a fair setting (roughly same architecture and complexity) and the only difference is whether the later points contribute to the former ones.
(3) (Minor) Visual comparison on view synthesis with other methods: since one of the main motivations of this work is to model the photon-particle interaction, I assume a major outcome is more robust rendering against the variations (blur, specular..) from input views. Thus, it is better to show some visual comparison with other baselines (SparseNeus, VolRecon, NeRF…) to verify this point.
(4) (Minor)The main diagram (Figure 2) can be displayed in a clearer and more elegant way. Basically the part of transformer details would belong to either occlusion transformer or render transformer, rather than ‘feature fusion’. Besides, there is ambiguity on what the patches with different colors stand for.


Limitations:
Authors have discussed their limitations on rendering speed. I think another limitation is that the SOTA neural surface reconstruction method is still only comparable with a classic MVS-based baseline (MVSNet) at this time. But MVSNet and its extensions must be much faster to get a depth map than the rendering-based ones. So there is a long way to go for this area to further release the power of implicit representation.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper proposes ReTR, a new architecture that leverages transformer to replace the traditional volume rendering process. The insight of the paper is that: the traditional volume rendering equation is oversimplified to model photon-particle interaction. Moreover, the color compositing function highly relies on the projected input view colors, and therefore overlooking intricate physical effects. To solve these two limitations, ReTR replaces the volume rendering equation with a render transformer. The attention map can be extracted from the render transformer to synthesize geometry details. An occlusion transformer is further introduced to obtain finer features. Instead of using FPN features and ResUNet, ReTR also utilizes features from different layers to construct multi-scale features. Experiments are conducted on the DTU dataset, Tanks & Temples, ETH3D, and BlendedMVS. The method is compared with state-of-the-art generalizable NeRF methods and generalizable surface reconstruction methods and achieves the best performance among them. Ablation studies also show the effectiveness of the network architecture. Moreover, ReTR surpasses SparseNeuS and VolRecon even without depth supervision.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
I like the insights proposed in the paper that the traditional volume rendering equation is oversimplified. The solution that utilizes transformers to replace volume rendering is straightforward yet sound and effective. Experiments are exhaustive and validated the network designation. I also like the discussion of interpreting the render transformer the hitting probability.

Weaknesses:
More related work should be discussed in Section 2 for generalizable NeRF methods (NeuRays, CVPR22; Generalizable Patch-Based Neural Rendering, ECCV 2022, ...) and neural surface reconstruction methods (NeuS, ...).
Notations in Equations (8) and (9) are not well explained, for example, what are $\mathbf{R}^f$ and $\mathbf{R}^{\text{occ}}$, and $\mathbf{f}_i^{\text{occ}}$ did not appear before Equation (10).
There is also a typo at Line 232: **tanks and temples** instead of **tanks and templates**.

Limitations:
The main concern of this paper is it requires a long training time, e.g. 3 days on a 3090. It comes with a cost when introducing transformers in the network architecture. Generally, it is not a big problem since the method is generalizable.

Rating:
7

Confidence:
5

REVIEW 
Summary:
The paper proposes a new framework for generalizable neural surface reconstruction, which utilizes the mechanism of transformers to model the rendering process. The authors first derive a general form of generalizable volume rendering based on existing methods and identify its limitations. They then suggest improving upon this framework by introducing a new rendering approach based on learned attention maps and performing over-ray accumulation in feature space rather than color space. Experiments are conducted on four different datasets and compared to recent baselines, demonstrating superior performance in generalizable reconstruction.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is well-written and exhibits a smooth flow. The authors effectively convey the motivation behind their methods, providing necessary background information and comparing against recent baselines. Additionally, the authors ensure that the reader can easily follow the logical progression of the paper.
- Section 3.1 presents a general framework that serves as a well grounded basis for existing methods. The authors' identification of limitations within this framework offers valuable observations, contributing to the overall storyline and motivation of the paper.
- The results presented in both the main paper and supplementary material demonstrate the superiority of the proposed method compared to the VolRecon baseline.
- The authors provided an ablation study on the various components of the method, effectively differentiating their individual contributions and providing a solid understanding of their impact.

Weaknesses:
- The section describing the reconstruction transformer appears to be incomplete and confusing due to several missing details and explanations:

  - It is unclear how equation 6 (and its improvement in equation 10) fit into the general framework proposed in equation 5. This confusion arises because the final MLP from feature to color does not align with the framework. Additionally, the weight function and color function are not explicitly provided.
  - The FeatureFusion operation is not defined, leaving ambiguity in understanding its purpose and implementation.
  - The definition of the ""meta-ray token"" $f^{tok}$ for a scene is unclear, specifically whether it pertains to per-image (per-ray/pixel) or per-scene information, and how it differs from the image features $f^{img}$.
  - The meaning of $F_i$ in line 161 is not provided or explained.

  It is crucial for the authors to address these misunderstandings and revisit the missing definitions in order to clarify the concepts.

- While it is acknowledged that the general form presented in section 3.1 oversimplifies the modeling of light transport in 3D scenes, it is hard to perceive how the mechanism of cross-attention over ray samples effectively models complex photon-particle interactions. Real interactions typically occur in spatial domains, whereas the suggested approach focuses on interactions over ray samples. It is suggested that the authors either temper these claims or provide further explanation on how their framework accounts for intricate global physical effects that encompass both global and local physical effects.

- The qualitative comparison is somewhat limited as it only includes a comparison to VolRecon. It is essential for the authors to provide visual comparisons with other methods as well, particularly SparseNeuS, to offer a more comprehensive evaluation.

- The authors have not presented timing evaluations of their method in both training and evaluation scenarios. Given that the limitation section highlights timing as a significant drawback of generalizable methods, it is important for the authors to address this by providing timing evaluations to enhance the paper's completeness.

Limitations:
The authors have discussed limitations in the supplementary material. However, it is necessary to present the main limitation of efficiency in the main paper, even if briefly in the conclusion section. Additionally, providing quantitative results that demonstrate the tradeoff between the number of parameters and training/rendering time would greatly benefit the presentation of the method.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This works focus on generalizable asset reconstruction: given a few posed images, predict the 3D representations using a network.
Instead of using volume rendering to compute the transmittance, the authors propose to use transformer on the sampled points to compute the weight of each point.
Besides, the author also  improve the CNN architecture for feature extraction.
Extensive experiments are conducted on multiple datasets, demonstrating better performance.



Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Better performance compared to previous compared with previous SOTA methods
2. Code is attached and will be release.
3. method is well explained


Weaknesses:
1. Compared to previous methods SparseNeus and Volrecon, this work seems somewhat incremental. Major difference is using a hybrid CNN extractor (Fig 3) and a new transformer architecture. The ""occlusion transformer"" and ""render transformer"" seems to be two transformers with fancy names, and I don't see significant difference from the transformer in volrecon. Though Volrecon is a CVPR2023 paper, but apparently the authors use its codebase for develop , as can be seen in the code.zip in the supplementary.

2. I don't agree with the ""rethink"" title, equation5 doesn't make too much sense to me. It's not explained why the equation can satisfy the occlusion-aware, and especially no guarantee of consistency across multi views.
For example, for a sampled point $x$, its weight may  be $1$ for the ray $r_1$, but may be $0$ for the ray $C(r_2)$ even when $x$ is the nearest point in ray $r_2$. Therefore, I'm not fully convinced with that Eq 5 is a better modelling of the  rendering, and the ""rethinking"" seems be some sort of exaggeration.
Furthermore, given the goal of this work is reconstruction, I don't see why loosing the physics constrain in rendering can benefit the learned geometry.

## Justification of rating.
1. Pros: results are solid (multiple datasets, compared with SOTA baselines), code available.
2. Cons: Somewhat incremental, ""rethinking"" seems a bit exaggeration.

Overall I lean to a borderline accept, as no big technical flaws, but I'm not very confident.

Limitations:
Discussion of limitation and broad impacts in the supplementary.
No license/asset description according to https://neurips.cc/public/guides/PaperChecklist . But I don't penalize it in the rating.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper introduces a interesting solution for volume renderings in generalizable neural surface reconstruction by leverage Transformers to predict depths and colors from feature volumes. The results on sparse view reconstruction prove its useness.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The authors identify the limitation and derive a general form of volume rendering and propose ReTR, a learning-based rendering framework utilizing transformer architecture to model light transport.
A hybrid feature extractor is also proposed for achieving better performance. 

Weaknesses:
Can we replace the volume rendering in the optimization-based methods (e.g. NeuS / VolSDF) with the learned Transformer ? Or is the solution only works for generalizable neural surface reconstruction?

What is the performance in scene-level sparse view reconstruction, i.e., Replica/ScanNet.

Will HybridExtractor also works for volume rendering based methods (e.g. SparseNeuS / VolRecon) ?

Why Transformer? Will CNN/MLP also works for this design?

Limitations:
See the weakness above.

Rating:
5

Confidence:
4

";1
CWnzWMLk3P;"REVIEW 
Summary:
The authors propose a new Asynchronous Federated Learning algorithm, FAVAS. They provide theoretical convergence analysis of the proposed algorithm as well as present experimental results. The main motivation behind their framework is to handle heterogeneities in the computation speed of clients in asynchronous settings. The work’s fundamental contributions are: 1) Compared to the algorithm proposed in QuAFL (Zakerinia et al., 2022), FAVAS weights updates of each contacted client according to their number of local iterates which is allowed to be different across clients. Its asymptotic bound on the total number of clients (n) is improved. 2) Authors present that the proposed algorithm is experimentally better than baselines on several datasets.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed algorithm seems efficient in terms of the utilization of clients. In conventional FL analysis, clients generally start computations when they are interrupted by the server. Here, clients always do computation (until $K$ local steps) and send the update when they are interrupted.

2. Although the algorithm is similar to QuAFL (Zakerinia et al., 2022), the authors redesigned local updates and local weighting so that aggregation becomes unbiased. I like how they highlighted the differences in Algorithm 1.

3. The mathematical analysis of the convergence seems strong and doesn’t use any unnatural assumptions. The intuition behind the proof abstract is well explained. In particular, I like the explanations in the main paper while the full proof is presented in Appendix.

4. Both in algorithmic and experimental analysis, the authors provide a good comparison with baselines in the literature. The authors successfully present their experimental setup details.

Weaknesses:
1. A discussion on the relation between the number of local iterates and training quality can enrich the study. In the proposed framework, the number of local iterates that slow & fast clients take are random and depend on the server interaction time and clients' computation time. On the contrary, in most baseline methods, the number of local iterates is a parameter that we set and directly affects the training time. For a fair comparison with baselines, some information (maybe the mean and standard deviation statistics) on the number of local iterates during training may be helpful. 

2. Lack of novelty: As the authors stated, the proof strategy and the algorithm are inspired by QuAFL (Zakerinia et al., 2022). I suspect the theoretical novelty in this work is above the threshold for the conference requirement.

Limitations:
Please see question 3 in Questions. The authors discussed both theoretical and experimental extends of the study adequately.
 
The authors also discussed the “Environmental footprint” of their study. The reviewer thanks them for their responsible attitude towards our environment.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes a new asynchronous federated learning update scheme to deal with the biasedness of conventional asynchronous updates which favor fast clients. The proposed method involves two main contributions, one is the direct local model update and the other is to reweight the contributions for the selected clients. The authors provide theoretical convergence analysis for the proposed method under stochastic non-convex settings. The numerical experimental results of the proposed method outperform other baselines in most settings. 


Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The paper is well structured with clear motivations. The theoretical analysis and numerical results are clear to support the intuition. 
 
2. The analysis of the proposed method can get rid of the constraint of bounded gradient and the uniformly bounded delay, which among the analysis of papers studying asynchronous FL, only very few of them do not require both conditions.


Weaknesses:
1. Some key information about the reweighted scheme need some further illustration. 

2. In my opinion, one difference between the proposed method and some previous baselines such as FedBuff and FedAsync is that the proposed FAVAS enables different clients to perform unfix steps of local training during two communication rounds, and the reweighted scheme is proposed to overcome the biasedness of unfix local steps. The independent number of local update steps to perform in each round based on the communication status shares a similar idea as [1]. Thus, the idea of FAVAS is similar to combining the QuAFL and [1] into a single framework.
 
3. One related paper [1] about federated learning could be discussed. 

[1] Yang, H., Zhang, X., Khanduri, P., & Liu, J. (2022, June). Anarchic federated learning. In International Conference on Machine Learning (pp. 25331-25363). PMLR.

Limitations:
see above

Rating:
4

Confidence:
4

REVIEW 
Summary:
Authors introduce a novel asynchronous centralized Federated Learning (FL) algorithm which corrects for clients with varying computational speeds (and delays). Theoretical convergence guarantees are provided as well as strong empirical results. Solves an interesting issue of model degradation when faster clients dominate asynchronous updates.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. Well-written paper with nice tables and clear algorithms.
2. Strong theoretical backing which align with peer algorithms (without favoring faster clients).
3. Impressive empirical results which showcase the algorithms importance in realistic (heterogeneous) settings for FL.

Weaknesses:
1. Novelty of paper seems to lie solely in the reweighting of fast/slow clients. This is important but seems to be the only main contribution. Algorithm is extremely close to QuAFL otherwise.
2. Convergence is theoretically degraded when a large subset of devices are sampled to participate (s/n ≪ 1). This is a cause for concern in realistic applications.

Limitations:
Limitations are not explicitly outlined in a section. I would recommend adding into the appendix at a minimum.

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this work, the authors propose an asynchronous communication protocol based on federated averaging, called FAVAS. The proposed method builds on top of a previously released method in asynchronous settings by addressing the bias and variance limitations of the previous approach through an unbiased estimator based on local reweighting. The authors make their claims clear with both theoretical and experimental results.  

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
(1) The paper is easy to follow, with good structure and motivation.
 
(2) The paper also provides a thorough related work discussion, convergence analysis, and empirical evaluation.

(3) Theoretical and empirical variance analysis reduction for asynchronous settings through unbiased local model reweighing. 


Weaknesses:
(1) Missing background makes it hard to understand essential concepts of the proposed framework.

(2) The work needs additional clarifications in the numerical evaluation and convergence bounds.

Limitations:
Yes.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes a novel algorithm called FAVAS for asynchronous federated learning with heterogeneous clients, that is the modification of the previous QuaFL algorithm. The paper provides theoretical convergence rate of the proposed algorithm, and experimental evaluations of the effectiveness of their method. 


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The studied question is important and well posed. 

- Experimental results show a large improvement of FAVAS over the previous QuaFL, and some improvement over FedBuff. 


Weaknesses:
Overall the paper is hard to read. The major concerns are unclear presentation of the theoretical results, and limited experimental evaluation.

1. In experiments the learning rate was set to some specific parameter 0.5, which is not a fair comparison. For a fair comparison, it is good to tune the learning rate separately for each of the methods. Same as for fair comparison it is good to tune the value of Z too. FedBuff proposed to set Z=10 when the delays are distributed close to the realistic FL system. However in your simulations the delays are distributed differently and thus the other values of Z might be optimal. 

2. The convergence rates in Table 1 are very hard to compare between each other. First, there are no estimates of how large $C_{FedAvg}, C_{FedBuff}, C_{AsyncSGD}$. Second, it is unclear how large $\sum_{i = 1}^n a_i / n $ might be, so it is unclear if the FAVAS leading term of convergence is better or worse than previous rates. 

3. The leading term  of convergence of FAVAS seems to be worse than QuAFL, as in QuaFL it is divided by E^2, while in FAVAS it is not divided by that. 

4. It is also hard to understand how tight the proposed analysis is. 

5. There are many other algorithms for asynchronous FL missing: such as R1, R2, R3. Experimental comparison to these baselines is also missing.

[R1] Gu et. al., Fast Federated Learning in the Presence of Arbitrary Device Unavailability, 
[R2] Yan et al, Distributed Non-Convex Optimization with Sublinear Speedup under Intermittent Client Availability. 
[R3] Glasgow et al, Asynchronous Distributed Optimization with Stochastic Delays.



Limitations:
-

Rating:
4

Confidence:
4

";0
vfDPA1Ft0c;"REVIEW 
Summary:
Stochastic Rising Bandits (SRBs) model sequential decision-making problems in which the expected reward of the available options increases after every time they are selected. 
While previous works addressed the regret minimization problem, this paper studied the fixed-budget Best Arm Identification (BAI) problem for SRBs. 
This work proposed the R-UCBE and R-SR algorithms and showed that these two algorithms with classical designs achieve a small failure probability when the time horizon is sufficiently large. 
With a lower bound, the author(s) also showed that the R-SR algorithm is near-optimal and a sufficiently large horizon is unavoidable for any algorithm to perform well.
Lastly, experiments are provided to validate the empirical performance of BAI algorithms.


===========

The score is increased after I read the response from authors.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. This work provides clearly formulates the rising bandit problem in Section 2.
2. Before presenting the R-UCBE and R-SR algorithms, the author(s) describe how to estimate the expected rewards of arms in Section 3, which actually imply the intuition of algorithm designs. This can inspire readers to propose efficient bandit algorithms even beyond this rising bandit setting.
3. In Section 7, experiments are provided to validate the empirical performance of BAI algorithms.

Weaknesses:
1. In Theorem 6.1, the lower bound on the time horizon $T$ depends on $\Delta_i(T)$. The quantity, $\Delta_i(T)$, depends on the instance, horizon $T$ and also the algorithm that we apply. Hence, $\Delta_i(T)$ seems to be a random variable, and I don't think a random variable should appear in the lower bound. Moreover, we usually expect a lower bound holds true for many algorithms, and the term $\Delta_i(T)$ seems to be different for different algorithms. or even different for the same algorithm in different trials.
2. I have a similar concern for the upper bounds on the failure probabilities of the R-UCBE and R-SR algorithms.

I think the contribution of this paper is much clearer, if my concerns above can be resolved.

Limitations:
NA

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper studies the fixed-budget best arm identification (BAI) under the stochastic rising bandit (SRB) problem. The stochastic rising bandit is to assume that the mean reward will increase as one plays the arm more. By assuming a concave increasing reward, the paper provides upper bounds for two algorithms: R-UCBE (which is UCB-type) and R-SR (which is elimination-based). It further provides lower bound proof, and such a bound is matched by R-SR up to logarithmic factors. Numerical experiments are listed.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is well-written and carefully organized

- Matching lower and upper bounds (up to logarithmic factors)

Weaknesses:
1) Deterministic growth function $\gamma$, meaning that the randomness of $\gamma$ does not accumulate; Not the practical case (consider SGD, former parameters will affect the consecutive)

2) What if $c \rightarrow 0$? The requirement for $T$ (10) seems to vanish. Can you make further explanations? 

Can you recover the bound of Audibert et al. (2010)?


Limitations:
Na.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper is about bandit best arm identification with fixed budget, in a non-stationary setting. This is a rested bandit problem: the mean reward of an arm changes each time it is pulled, but does not change when it is not pulled. The main assumption is that the mean reward is a non-decreasing, concave function of the number of pulls. Some results also use another assumption: an upper bound on the increments.
The authors introduce estimators of the mean rewards that are adapted to that setting and use them in two algorithms R-UCBE and R-SR, which are inspired by the UCBE and SR fixed budget algorithms. The paper contains upper bounds on the error probability of these algorithms as well as lower bounds on the error probability of any algorithm and a discussion of the minimal budget necessary to identify the best arm.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The rising bandit problem is important: it corresponds to allocating resources to different learning algorithms, in order to identify the one with best performance once fully trained.

The estimators and algorithms are well explained and motivated. The graphical representation of figure 1 is very helpful.

The lower and upper bounds show that the methods are close to optimal for the problem.

The discussion of the minimal budget necessary to identify the best arm is interesting and highlights a feature of rising bandits which is not present in standard BAI.

The experimental evaluation is convincing.

Weaknesses:
R-UCBE depends on a parameter that needs to be tuned using unavailable information, but that theoretical weakness is directly inherited from UCBE and the practical performance of the algorithm is very good. Hence this is a very mild weakness.

The lower bound of theorem 6.2 is of order $\exp(-T/H)$, while a famous feature of fixed budget BAI (in the stationary setting) is that this is not achievable. Indeed, it is shown in [Carpentier and Locatelli, Tight (lower) bounds for the fixed budget best arm identification bandit problem, Colt 2016] that there is a lower bound of order $\exp(-T/(H \log K))$, matching the upper bound of SR. In light of that lower bound, we would expect a similar result for rising bandits, stronger than Theorem 6.2.

Limitations:
The limitations are adequately discussed. No concern about a negative societal impact.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This study explores the stochastic rising bandits (SRB) in fixed-budget best arm identification (BAI). The authors initially formulate this novel problem setting and then introduce two types of estimators. For these estimators, they demonstrate upper bounds that match their lower bounds. Lastly, they validate the reliability of their approach through various experiments.






Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Firstly, this is my initial exposure to a paper on the SRB setting. Consequently, I'm not sure if the setting is truly novel. However, I am persuaded that the setting is both crucial and intriguing, particularly given the practical importance of the CASH problem. As I'm unable to gauge the novelty of the setting, my remarks are primarily oriented towards the technical aspects of BAI.

Firstly, the concepts of pessimistic and optimistic estimators are persuasive and well-founded. The authors deliver thorough and robust theoretical results for the estimators, consistent with established practices in this field. Even though the outcomes are not surprising, I see no strong grounds to reject this paper. On the whole, this study presents a typical analysis within an interesting and novel setting.

Please note, this is a preliminary review. I am currently delving into further details, including the proof. I may revise this review later.

Weaknesses:
It appears that a weakness resides in the need for a large budget. Moreover, verifying whether the condition is met could be challenging due to the somewhat complex form of the time budget constraint.

Limitations:
See weakness.

Rating:
6

Confidence:
2

";0
JMuKfZx2xU;"REVIEW 
Summary:
The paper estalishes a new quantity for measuring the dependence between random variables. The definition is similar to mutual information and sliced mutual information. As (for rather obvious reasons) it takes values between these two existing measures, it has some desireable properties. A variational representation further makes it better compatible with neural estimators.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
The paper is well written, provides good references and approaches the challenging gap between theory of information measures and their estimation in practice.

Weaknesses:
It is difficult to see the benefit of the specific definition (2). There is no convincing argument why (2) is better than (1). Also an optimization over a set of prabability measures appears to be much more difficult to approximate than the comparatively ""simple"" integration over the uniform measure.
Many details are in appendices that I unfortunately have no time to read in the limited time given for the review process of many papers.

Limitations:
Limitations were not addressed in the paper but discussed in an appendix.

Rating:
7

Confidence:
3

REVIEW 
Summary:
Recently, sliced mutual information ($\mathcal{SI}$) has been proposed as a statistical dependence measure that is scalable to high dimensions (Goldfeld and Greenewald, 2021). However, due to the use of uniform sampling for the slicing directions, $\mathcal{SI}$ generally requires many slices for accurate estimation. This is because it doesn’t prioritize informative features about the variables and introduces noisy slices by treating both relevant and irrelevant regions of the variable space equally. This paper aims to improve $\mathcal{SI}$ by finding an optimal distribution of slicing directions that satisfies two criteria: 1) slices are distributed over maximally informative regions, and 2) slices are scattered over the unit sphere. This new dependence measure is called optimal sliced mutual information, $\mathcal{SI}^*$. The two criteria are then formalized as a regularized optimization problem in the definition of $\mathcal{SI}^*$.

The contributions of this paper are as follows: (i) The authors introduce an improved version of $\mathcal{SI}$ known as $\mathcal{SI}^*$ and discuss its theoretical properties. In particular, they show that an optimal slicing policy exists, allowing them to treat the regularized optimization problem as a reward-maximizing optimal transport problem. (ii) The authors show that the convergence rate of the optimal estimator of  $\mathcal{SI}^*$ depends on the dimensions only up to a constant factor. In addition, they develop a neural estimator for $\mathcal{SI}^*$ which can be used in various machine learning problems. (iii) The authors perform several experiments to show the effectiveness and applications of $\mathcal{SI}^*$. In particular, they show that: (a) $\mathcal{SI}^*$ outperforms state-of-the-art measures in discerning structure from noise, (b) $\mathcal{SI}^*$ is more sample- and slice-efficient compared to $\mathcal{SI}$, and (c) $\mathcal{SI}^*$ performs well on representation and reinforcement learning problems.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper improves $\mathcal{SI}$ by challenging the uniform sampling assumption in the original definition. The paper is original as it proposes a novel, improved measure of $\mathcal{SI}$ that can capture statistical dependency better by retaining as much relevant information as possible. The theoretical result on the existence of the “optimal” slicing policy is interesting and non-trivial. The experiments clearly showcase the effectiveness of $\mathcal{SI}^*$ as a dependence measure and how it can be applied to various machine learning tasks. The paper shows how $\mathcal{SI}^*$ is an effective statistical dependence measure that works well in high dimensions. As many machine learning tasks involve high-dimensional variables, this measure will prove useful in practice. Overall, the paper is well-structured and was an interesting read.

Weaknesses:
While the paper has some interesting points, there are several weaknesses that should be addressed to make the paper more readable.

1. While the definition is $\mathcal{SI}^*$ is intriguing, the justification for the two criteria for “optimality” that motivated the definition is inadequate. (i) Why is it relevant or important to consider the maximally informative regions? (ii) If we accept that the maximally informative regions are important, why do we need criterion 2? The authors should explain more clearly why diversity of slices is important when measuring dependence between random variables.

1. From the definition of $\mathcal{SI}^*$, wouldn’t it be better to call it something like “maximum $\mathcal{SI}$”?

1. In Figure 2, the experiments are done for variables in $\mathbb{R}^3$. However, $\mathcal{SI}^*$, like $\mathcal{SI}$, should continue to be scalable to high dimensions. Thus, the experiments should be done in higher dimensions (> 100), so we can test if the three neural networks involved in the estimation of $\mathcal{SI}^*$ scale well with larger dimensions of inputs. 

1. Parts of the paper were hard to understand and could have been explained more clearly. (i) For example, the terms in the neural network estimator (Section 4) need more details and better explanation. (ii) In the motivating example (Section 3.1), it is not clear what the AUC of the ROC refers to or how it is computed in this case. The Appendix provides some details for another experiment but it is not clear if that applies here. Also, it seems that Fig. 1 is not an ROC AUC curve despite the caption stating as such. (iii) The setup and assumptions for the experiments require more details beyond that given in the paper and appendix. 

1. Since one claim in the paper is better sample and slice efficiency of $\mathcal{SI}^*$, is it possible to provide theoretical results along this direction? The results in the paper seem to indicate that $\mathcal{SI}^*$ has a similar convergence rate as $\mathcal{SI}$, thus reducing its significance. 

1. Another claim in the paper is the potential of $\mathcal{SI}^*$ to bring the approach to more complex machine learning scenarios. In this context, more experimental evaluation is necessary for both the representation learning and reinforcement learning scenarios. The current evaluation (e.g., Table 1) is not to the state of the art, in fact, a shallow vanilla CNN without data augmentation has 86% accuracy on CIFAR-10. This makes it difficult to interpret the significance of the results.

Limitations:
The limitations are discussed in Appendix E.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This work based on existing ideas of providing a tractable independence metric for high dimensional random variables, the proposed metric named ""Optimal sliced mutual information"" (SI*) improves upon existing method of uniform slicing to an adaptive optimal slicing with scalability design in mind. It has been shown that the proposed method has desirable properties to be a dependency metric. The estimator is shown to be faster converging than previous MI both theoretically and empirically. In addition, this method is applied on several machine learning tasks to show its effectiveness. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This work discovers weakness in previous work in terms of slicing efficiency. The idea is well motivated and explained well in introductory sections. The properties of the proposed metric is well explained with easy to understand examples and visualization. Examples in the empirical study showcases various in field applications which implies wide impact upon adoption.

Weaknesses:
The scalability of the NN estimator of SI* is only studied at slicing size level. A remark about the scalability with $d_x, d_y$ similar to convergence analysis can be beneficial.

Limitations:
Limitations are well addressed.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a new dependence measure based on the sliced mutual information with both the theoretical analysis and the modern machine learning experiments. This measure is not strictly the mutual information but still could capture the correlation between two distributions. 
The experiments are done in several different domains in eluding image classification, reinforcement learning. Their main idea is to replace the original uniformly sampled slices to the one whose directions could be diversified over the whole sphere as well as mainly concentrates on the interested area. 


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The article is well written, starting from an easy understanding toy example, followed by the motivation and explanation. The authors also clarify several important concepts that the reviewer will raise during the reading. The theoretical analysis is detailed and clearly written. 
- The observation of the the uniform distribution in sliced mutual information is insightful
- Extensive experiments are done in different domains 

Weaknesses:
Have two minor concern and hope to hear from the authors,
- In the theoretical part, the authors prove the existence of the optimal solution using the optimal transport's view. However, during the neural estimation, the problem is simplified to gradient descend with Lagrange constraints. The hyper parameters listed in equation (6) are not clearly described. 
 - The efficiency is not as good as the previous methods (ten times comparing with SI. Which factors cause the computation speed to be slow? 

Limitations:
No negative societal impact. 

Rating:
6

Confidence:
3

";1
UNOeQGHNaN;"REVIEW 
Summary:
This paper empirically shows previous supervised adversarial training methods have two shortcomings: (1) The features of the natural examples and those from other classes are not distinguishable and (2) the features of the natural and adversarial examples are not aligned. To mitigate these two issues, the authors propose a regularization to push away the features from other classes and freeze the natural examples and a reverse attention mechanism that increases the weight of the target class. The empirical results seem to validate the effectiveness of the proposed method.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The paper is well-written and can be easily followed. 

1. The proposed method is well-motivated. The empirical study on the feature spaces is interesting and inspirable. 

2. The authors conducted the experiments on comprehensive datasets to support their claim.


Weaknesses:
1. Regarding the experiments, the authors only provide the results on ResNet18 and PreActResNet18, which is limited. I suggest the authors provide the results on the WideResNet and make a comparison with the current state-of-the-art performance listed in the RobustBench (https://robustbench.github.io/) to validate the effectiveness of the proposed method.

2. The authors do not provide the error bar to validate the significance of the results.

3. It is weird the accuracies under PGD, FGSM, C&W are higher than the natural accuracy achieved by ANCRA shown in Table 1. It would be better to provide some explanations for these abnormal results. It seems the defence has the issue of the obfuscation gradients. I suggest that the authors report the results under the adaptive attacks that use the auxiliary probability $p$ and even different $p$ during the testing phase and AutoAttacks. 


Limitations:
Though the authors claim they have limitations, I did not find the discussion of the limitations.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper focuses on robust feature learning by combining two approaches: Adversarial Contrastive Learning and Robust Feature Selection. Specifically, it defines two characteristics for features: exclusion and alignment. The authors aim to enforce exclusion through Asymmetric Negative Contrast (ANC), which ideally should separate different classes. On the other hand, they aim to achieve alignment through Reverse Attention (RA), which should enhance the model's robustness.

Soundness:
1

Presentation:
1

Contribution:
2

Strengths:
The proposed method sounds interesting, especially the Reverse Attention approach. However, I have some concerns and feedback that will be provided in the later section of the review.

Weaknesses:
Writing Style:

Overall, the quality of the writing style could be significantly improved. Reading the paper is not smooth, and understanding it requires reading back and forth several times. For instance:

1. Generally, the flow of the paper is not engaging, which requires the reader to go back and forth in order to understand it. And, there are some sentences that are vague and difficult to understand.
3. Caption: Overall, the caption writing is not good; it contains long sentences that are difficult to follow. Labels for each plot have not been provided. Specifically, in Figure 1, regarding the distance between natural examples and OEs, it would be preferable to show distances between each pair instead of comparing class 0 with others, as class 0 may share common features with some other classes. It would be more informative to have the following plots: 0-1, 0-2, ..., 0-9.
4. Typos: ""perturbation -> perturbations"" in line 28 | ""which does harm to robust classification -> which harms robust classification"" in line 35 | ""... negative pair (PP) -> (NP)"" in line 64 | ""import feature -> important feature ..."" in line 118 | ""a example -> an example"" in line 147 | ... 

Content:
1. There are certain terms that are vague. For instance, what does ""well-trained DNN"" mean in line 20?
2. The definition of an Adversarial Example is not entirely correct. The perturbation itself may be visible, but when it is added to a natural image, both the adversarial example and the original image may not be easily distinguishable by human eyes. Furthermore, it is important to note that it is not the adversarial example itself to which perturbations are added. Rather, the perturbations are added to natural examples so that they cannot be correctly classified.
3. Description about Figure 1 in line 45-47: It is not a Gaussian distribution; it is a bell-shaped plot but not a Gaussian distribution. The main condition for a Gaussian distribution is that the area under it should be 1. And indeed, those numbers show a significant distance between classes. A cosine similarity below 0.5 should be large enough to indicate a difference between the representations of one class and another. Again, indeed, a cosine similarity between 0.9 and 0.99 indicates that representations of NEs and AEs are very close to each other! And comparing AEs plots with the OEs plots, we see that they have a very close representation to their original examples that OEs. Furthermore, if the representation of NEs and OEs is not significantly distant, models should confuse them, leading to lower clean accuracy. However, that is not the case.
4. The definition of OEs is not clear from the beginning of the paper. The reader should read the whole paper to figure out what they are, especially since different strategies are considered to select or generate them. 
6. Similarly, the term 'partial parameters' is vague, and the reader doesn't understand what it refers to until the later sections of the paper, which confuses the reader.


Approach and Methodology:
1. IT seems the main weakness is that ANC does not make significant contributions to the clean accuracy and robustness of the model, as indicated by the results in Table 3 of the ablation studies. Specifically, a model with only RA performs just as well as ANCRA. On the other hand, considering this, a large portion of the paper is dedicated to introducing and explaining ANC. However, the most important component, which is RA, is not studied well enough and lacks supporting experiments and theoretical insights.
2. The training process of a model with ANC is not clear enough. It would have been better to depict it through a diagram. Furthermore, the last paragraph in the introduction section is abstract and confusing, making it difficult for readers to understand the exact contributions.


Experiments:
1. The proposed method in the main table (Table 1) is evaluated without considering ""p"" in attack which creates a false sense of security. However, in Table 2, attacks with ""p"" demonstrate a significant decrease in the model's performance.
2. Even though ""Error Bars"" is marked in the checklist, I don't see standard deviation values in the reported tables.
3. Similarly, although ""Reproducibility"" is marked in the checklist, the code necessary to reproduce the results is not available.
4. I conducted an experiment with TRADES, and the accuracy I obtained differs from what is presented in Table 1. The clean accuracy is 80.92%, and the PGD accuracy is 50.10%.
4. The results of the Tiny-ImageNet experiments are inconsistent with those of other datasets, and the performance of the model does not seem promising. Additionally, they have not been compared with baselines, which is essential.

 

Limitations:
1. It appears that the approach does not work properly with large datasets like ImageNet.

Rating:
3

Confidence:
4

REVIEW 
Summary:
In this paper, the authors address a notions of exclusion and alignment in representaion learning for robust adversarial training (AT). They propose a generic framework for AT that includes asymmetric negative contrast and reverse attention in order to obtain robust representation. In addition, they propose to weight feature by parameters of the linear classifier as the reverse attention, to obtain class-aware feature and pull close the feature of the same class. The authors show empirical evaluations on three benchmark datasets, showing the improved robustness under AT and as well as giving increased performance in comparison to state-of-the-art methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Originality. The proposed  AT framework that concentrates on robust representation with the guidance of the two characteristics like exclusion and alignment appears to be novel. Generating and using negative samples by targeted attack to assist learning also appears novel. 

Quality. The quality is good. The paper is nicely motivated. The idea seems reasonable regarding the use two characteristics for robust representation: (1) exclusion: by pushing away the feature representation of natural examples from the feature representations for examples coming from other classes; and (2) alignment: by pulling close to each other the feature representation of the natural examples and the feature representation for the corresponding adversarial examples. The approach can be used in a plug-and-play manner for a number of defence methods, while the empirical validation shows advantages under the setting of white-box and adaptive attacks. While on table Table 2, we see consistent improvement for the used attacks with p, while we do not have results for comparison when we do not use p. 

Clarity. The paper organisation, the presentation and the writhing is good. I find the paper easy to follow on most parts. It might be beneficial some parts of the paper to be explained more easily so that the main idea behind the paper to be more obvious. Fig 2. might be improved to more concretely pin point the cases of class confusion issue. In Sections 3.2.1, the section within the lines 162-169, a bit difficult to rad on the first pass regarding the cases covered by the class confusion issue. I would suggest an additional small figure to illustrate intuitively and more obviously the cases. In Sections 3.2.1, the section within the lines 170-172 is not clear (not sure when the some sentences end and begin). Section 3.2.2 could benefit further polishing. Maybe a table or small figure describing what is negative samples or pairs (i.e., OEs), natural negative examples/pairs, adversarial negative examples/pairs, positive examples/pairs, adversarial negative examples/pairs, etc. could be helpful. The text in the lines 234-257 could also benefit sharpening, since some explanations are not that smoot and straightforward at least to my understanding. The results on Table 4 are interesting, but maybe the table could be reorganised and more clearly shown so that we can have a better grasp of what is the actual advantage. 

Significance. Adversarial training is important topic for privacy and security applications. This paper proposes an approach towards more reliable defence of adversarial attacks. The approach also seems to provide improvements on top of other approaches like the TRADES approach. Generating and using negative samples by targeted attack to assist learning seems valuable. Adversarial training with reverse attention on the feature level seems interesting.

Weaknesses:
Weaknesses
- In the empirical evaluation, some tables could be made more obvious (please see the comment on clarity for Table 4).
- The black box attacks not considered and comparison for black box attacks not done.

 

Limitations:
Limitations

- By generating negative samples by targeted attack, we use prior on about the possible attack, so possibly, the defence of the approach is biased towards the generated negative samples by the used targeted attack (or similar attacks).
- In the case of black box attacks when we do not have a prior about the attack, I'm not sure how much the proposed approach could help.  

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper presents two characteristics of robust features, exclusion and alignment, and proposes a novel adversarial training method with asymmetric negative contrast and reverse attention. For exclusion, it introduces asymmetric negative contrast loss and generates adversarial negative examples by targeted attacks to push out examples of other classes in the feature space, and for alignment, it introduces reverse attention that weights features based on the parameters of the linear classifier to obtain class-aware features. Experimental results on three datasets show that the proposed method significantly improves the robustness of the models.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
1. This paper introduces two characteristics that robust features should have: exclusion and alignment, and proposes a new AT framework that enables models to learn robust features effectively. It can be used in a plug-and-play manner and works well with existing AT methods.
2. Specifically, asymmetric negative contrast loss for exclusion of robust features, a technique to create hard negative samples through targeted attacks, and reverse attention using class information for alignment are proposed, which induce the characteristics of robust features. To my knowledge, these techniques are novel in AT.
3. The proposed method records the SoTA performance in experiments on CIFAR-10, -100, and Tiny-ImageNet. The performance improvement shown in Table 1 is impressive.
4. Overall, the paper is well written to help the readers understand the presented concepts. In detail, figures 1 and 3 show the statistical differences between NEs and OEs and NEs and AEs, making it easier for the reader to understand the difference in the distribution of features and the effectiveness of the proposed AT.  Figure 2 also helps to illustrate the problem of class confusion.


Weaknesses:
1. The practical implementation details for Reverse Attention in Section 3.3 are unclear. Looking at line 228 of the paper, it says that the proposed method uses p and p' together to train the model, but it is ambiguous in what way the proposed method uses them together (e.g., does it use p+p' or alternate between p and p'). Releasing the source code in the future will answer a lot of questions, but it would be nice if it could also be clarified in the paper.
2. The experimental results for ""adaptive attack"" in Section 4.3 raise the question of whether the experimental results in Section 4.2 are an unfair comparison. Therefore, it is necessary to clarify what the role of p is and whether this is the role of the key in the gray-box setting.
3. Table 1 in the supplementary material, the results on Tiny-ImageNet, does not have baselines, making it difficult to see performance gains.
4. Making hard negative examples via targeted attacks requires additional computation, but according to Table 4, the resulting performance gain is small. This paper also needs a clear and specific description of the negative sample generation (such as whether PGD-N was used).  In the supplementary material, it is described as if they used the AEs that are predicted as the classes of NEs in the current batch as OEs, which is difficult to apply in cases with a large number of classes like CIFAR-100 unless the batch size is large enough.


Limitations:
In this paper, possible limitations are not discussed separately.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This work aims to improve the adversarial training (AT) techniques from the perspective of learning robust representation representations. Specifically, the authors highlight two characteristics of having robust features. Exclusion: the similarity of features of samples of one class should be very less from the features of samples of other classes, so that model can differentiate between features of different classes for better classification.  The second attribute is Alignment: the gap between features of adversaries and clean samples of same class must be very small, which would increase model's robustness against perturbed samples.

To effectively satisfy these conditions, this work proposes two techniques, (1) to enforce exclusion, a asymmetric negative contrast loss is proposed which minimizes the clean sample similarity with negative samples of other classes, crafted by the adversarial attack. (2) to satisfy alignment: reverse attention strategy is proposed which align together the features of training examples which belong to the same class.

The proposed method is compatible with existing adversarial training techniques. Extensive experiments are conducted where the proposed method improves the natural accuracy as well as the robust accuracy when combined with existing AT algorithms.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
Strengths:
1) The idea of improving adversarial robustness of model by explicitly learning robust representations seems interesting. Although the traditional AT and contrastive learning based AT implicitly learns robust features, this method attempts to achieve the same in more explicit manner.

2) The proposed techniques of utilizing asymmetric negative contrast loss and reverse attention to achieve exclusion and alignment during adversarial training are intuitive and are properly justified in the manuscript. 

3) The method provides impressive results as compared to previous state-of-the-art approaches. 



Weaknesses:
Weaknesses:

1) There are concerns regarding the proposed reverse attention strategy. During the testing, the true labels are not known and reverse attention uses the predicted label h(x) to calculate z'. In case the model provides wrong predicted class label, the corresponding z' will be also then multiplied with wrong classifier vector. This will further lead to degraded performance. The authors have not tried to address this scenario. 
2) From the works of [1] and [34], how is the proposed reverse attention different? Unfortunately the authors have not provided any comparisons or contrast.
3) In the ablation studies provided in Table 3, combining ANC and RT marginally improves results as compared to individual ANC and RT results. It looks like there is some sort of competition between the both proposed techniques.
4) Similarly, the use of negative samples via proposed targeted attack in Table 4 shows marginal improvements overall.
5) The paper is very difficult to understand, especially for the readers who are new to the technique of adversarial training. The presentation can be significantly improved. 

Minor weaknesses:
typo at line 281 scenaios -> scenarios 

Limitations:
The authors have not discussed any limitations of their work. 

Rating:
5

Confidence:
3

";0
l6R4Go3noz;"REVIEW 
Summary:
This paper works on visual prompting. They proposed FGVP, together with Blur Reverse Mask, to improve the semantic localization ability of the vision-language model. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Solid experiments showing the effectiveness of their method.

Weaknesses:
In general, this is a good work. However, there’re several things concerning me:

- The novelty of this work is not well established. Seems like an engineering combination of previous works.
- The discussion of the upper bound is based on the assumption, while the legitimacy of the assumption is not well discussed.

Limitations:
No notable limitations.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper proposes Fine-Grained Visual Prompting (FGVP) that incorporates Blur Reverse Mask to improve the semantic localization capability of VLMs, like CLIP. It provides a comparison to other possible methods for highlighting the different parts/objects in the image, based on SAM and other techniques. The resulting method improves the performance on RefCOCO* datasets.  

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is written in a clear way and describes the algorithm well. The evaluation shows clear improvements and the ablation study is convincing. The idea to blur and mask the background of different object/part proposals is both innovative and significant.   

Weaknesses:
The related work section missing two relevant works in the visual prompting domain:

1. Bhang et al., ""Exploring Visual Prompts for Adapting Large-Scale Models"", 2022

2. Bar et al., ""Visual Prompting via Image Inpainting"", NeurIPS, 2022


Moreover, while SAM is a powerful method for segmentation, it requires running the model with a relatively dense grid of keypoints. This runtime can be significant and should be discussed in the limitations section.  

Limitations:
The limitation section is present and addresses some unexplored directions. I suggest adding runtime estimates.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes a new “visual prompting” method. Visual prompting refers to the idea of altering images to guide the “attention” of a vision-language model when the model is used to embed the image. For example, to obtain an embedding for an object in an image that contains many objects, the user could draw a red circle around the object of interest and then embed the image. Since the vision-language model might have seen images during training in which important objects are highlighted with red circles, the resulting embedding might focus on the circled object.

This approach can be used to partially solve tasks such as referring expression comprehension: Given a set of bounding boxes and text descriptions, visual prompting can be used to find the best-fitting description for each box. A separate object detector is needed to obtain the bounding boxes first.

The paper proposes a visual prompting method that consists of blurring everything in the image except for the object of interest. This is done by first using a pretrained segmentation model (Segment Anything) to obtain a mask for the object of interest, and then blur everything outside of the mask. The motivation for this approach is that it simulates the shallow depth of field seen in photographs taken with a large aperture, which are commonly found in VLM training data.

The blur method is compared to the “red circle” method and several variants in object detection and referring expression tasks. The blur method consistently performs best. In addition, hyperparameter sweeps for the blur radius and other hyperparameters are shown.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed visual prompting method consistently improves over other methods.

2. The blurring approach is well motivated by the abundance of photographs with shallow depth of field. This is a clever way to exploit “natural supervision” present in large-scale web image training data.


Weaknesses:
1. The proposed method is significantly more complex than the “red circle” method, since it relies on a large segmentation model. The inference cost of the proposed method is therefore much higher than the “red circle” method. This should be acknowledged in the discussion and/or limitation sections.

2. While the proposed method works well, it is an incremental improvement over the “red circle” idea (https://arxiv.org/pdf/2304.06712.pdf) and the evaluation is not as comprehensive as in the “red circle” paper. For example, no analysis of the relative biases of red circle vs blur methods are performed, and no failure cases are discussed. It is not clear if the contribution is substantial enough for NeurIPS.

Limitations:
The runtime cost of the proposed method needs to discussed further.

The biases of the method compared to the ""red circle"" method need to be evaluated and discussed.

A generic ""broader impact"" statement is given in the appendix.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes a visual prompting method that exploits the segmentation masks of interested objects in images to generate more fine-grained visual prompts. Experiments show that the proposed methods achieve competitive results on zero-shot referring expressions comprehension and part detection.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper is well-organized. The motivation and the framework is clearly presented.  
2. The experiments confirm the effectiveness of the visual prompt design.

Weaknesses:
1. The ablation studies of different VLMs are missing. Since visual prompting is a zero-shot framework, it is natural that the performance of the visual prompts differ on VLMs trained on different data. The paper adopts the CLIP as the VLM in all experiments. How do different visual prompts perform on other VLMs?  
2. Figures 2 & 3 are very similar and thus redundant. The authors should consider merging them into one figure.

Limitations:
The proposed visual prompts can be further verified on more object-based tasks.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes Fine-Grained Visual Prompting (FGVP), which uses precise semantic masks from SAM as visual prompts to improve spatial localization of vision-language models (VLMs) like CLIP for instance-level tasks. The key contributions are:

- Systematically study different visual prompting techniques like cropping, boxes, circles, masks, etc. Show that blurring background outside target mask (Blur Reverse Mask) works best.
- Achieve state-of-the-art results on referring expression comprehension benchmarks RefCOCO/RefCOCO+/RefCOCOg, outperforming prior works.
- Demonstrate FGVP can enable zero-shot part detection on PACO dataset without needing any box proposals, again outperforming other prompting methods.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Originality: The paper presents a novel idea that using SAM to generate semantic masks from detected bbox as better visual prompts of specific instance in the image, which is an original idea not explored in prior works.

Quality: The overall approach is technically sound. The experiments follow standard protocols and are extensive in studying different visual prompting design. The results demonstrate benefits over existing methods.

Clarity: The paper is well-written and easy to follow. The problem context, proposed method, experiments are clearly explained. Figures and tables aid understanding.

Significance: FGVP pushes state-of-the-art in two important vision-language tasks - referring expression and part detection. The analysis may inspires more research on the properties of VLMs regarding spatial understanding.

Weaknesses:
While the paper presents a novel fine-grained visual prompting technique and achieves state-of-the-art results, there are some aspects where the analysis could be strengthened:

More Insights from Study: The paper performs an extensive set of experiments on different visual prompt designs. However, it could provide a more detailed analysis of the inferences and insights derived from this study. For instance, comparing the gap between using ground truth and predicted bboxs would give insights into the impact of mask quality. Explaining the differences in various design choices (VP, PP, proposals etc.) in Table 3 would be informative. Attention visualizations could help reveal why fine-grained prompting is more beneficial.

Computational Overhead: The paper proposes generating semantic masks using SAM models. However, the computational overhead this introduces is not analyzed. Reporting inference times, scalability, etc. would provide a better understanding of its practical viability.

Joint Language and Visual Prompting: The study is currently limited to exploring only visual prompts. Evaluating prompts on both visual and textual modalities could offer a more comprehensive understanding of VLMs. It is good that authors discussed this point in the limitation though.

Granular and Failure Analysis: Providing per-class performance breakdowns and detailed failure analysis compared to other methods through examples would provide useful insights into where the improvements come from.

In summary, while the core ideas are promising, performing a more thorough empirical analysis along the above dimensions would strengthen the paper and provide a better understanding of the factors behind the efficacy of fine-grained visual prompting.

Limitations:
Limitations are discussed in the paper

Rating:
6

Confidence:
4

";1
lArwl3y9x6;"REVIEW 
Summary:
The paper proposes an adversarial perturbation method linked to SAM for the affine normalization parameters, in contrast to perturbing the full set of parameters. The results show this approach improves upon standard SAM and prior sparse SAM approaches.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The SAM-ON proposed method yields several benefits:
1. improves upon SAM-all and carious other SAM-variants
2. achieves this by perturbing only the normalization layers, which correspond to a small percentage of the total parameters


Weaknesses:
Weaknesses of this paper include:
1. gains in Table 1 are marginal
2. originality of approach is lacking given prior literature on SAM variants
3. justification of SAM-ON is poor; the experiments in 5.2 and Table 7 try to poke at this but fall short as the authors do not provide a solid explanation as to what this can be attributed to; they only show that SAM-ON increases sharpness in L-inf sense; but what about other metrics of flatness? is there a more universal metric that can explain the effectiveness of SAM-ON?
4. it is unclear whether SAM-ON is truly better than other SAM variants in out of distribution robustness, e.g. WILDS benchmark
5. results are only shown for CIFAR and ImageNet datasets; more datasets and benchmarks need to be considered

Limitations:
Somewhat discussed but limited. Would like to see a more thorough limitations discussion.

Rating:
3

Confidence:
4

REVIEW 
Summary:
As Sharpness-Aware Minimization (SAM) aims to regularize the flatness of the loss landscape for better generalization, this paper shows that only perturbing the normalization layers is sufficient to achieve this. To prove this, the authors first propose a method called SAM-ON (SAM-OnlyNorm). Then, they conduct experiments on CIFAR datasets across different models and variants of SAM, comparing the test accuracy with and without applying SAM-ON. The experiments (in most cases) show that SAM-ON outperforms vanilla SAM, as well as SAM variants with ON outperforming SAM variants. Furthermore, since SAM-ON only needs to perturb a few parameters, it can save computational resources. Finally, this paper conducts numerous experiments to understand SAM-ON and concludes that sharpness may not be the key to SAM.

## post-rebuttal
I've updated my score since my concerns are adequately addressed.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. This paper demonstrates a new understanding of the underlying mechanism of SAM, which is both novel and informative.
2. The proposed SAM-ON can improve efficiency and generalization in most cases.
3. The experiments are adequate to support the claims.
4. The code is provided, which guarantees reproducibility.

Weaknesses:
1. SAM-ON cannot outperform SAM (or its variants) in some cases, showing that SAM-ON may not always be effective and decreasing the soundness of the claims that support SAM-ON.
2. The improvement of SAM-ON on ImageNet seems to be consistently lower than on CIFAR datasets for different architectures. Therefore, this reviewer hypothesizes that SAM-ON only works for small datasets and is not very effective for large datasets.
3. As the author acknowledges, this paper only investigates SAM and SAM on vision data. Thus, the analysis cannot support the proposed understanding that generalizes to other tasks, such as language tasks.

Limitations:
N/A

Rating:
9

Confidence:
4

REVIEW 
Summary:
This paper relates the normalization layer with the Sharpness-Awareness Minimization (SAM). Surprisingly, this paper finds that in the perturbation stage, only perturbing the affine parameters of normalization layers in the networks leads to a better generalization performance. Later, the authors investigate the reason behind such surprising phenomenon, and find that SAM-ON(only perturbing the normalization layer) even increase the sharpness, which doubts the intuition that the effectiveness of SAM comes from sharpness minimization. Extensive experiments demonstrate the effectiveness of the methods this paper proposed.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper identifies an important yet interesting problem: Is Sharpness-Awareness minimization effective because of minimizing sharpness? This paper found that only perturbing the normalization layer in the perturbation stage of SAM leads to both superior generalization performance and higher sharpness. The interact relationship between sharpness and generalization are quite doubtful.
2. This paper investigates the SAM from the normalization layer and found the normalization might be the reason for the success of SAM, which is quite surprising.
3. SAM with only perturbing the normalization layer largely saves the computational cost than the original SAM which needs perturbing all the parameters.

Weaknesses:
1. These experiments should be conducted on more network architectures, including VGG-Net and etc.
2. The observation that SAM with only perturbing the normalization layer holds the higher sharpness than original SAM might be trivial. SAM are designed to reduce the sharpness, while a degraded version of SAM should hold a higher sharpness in principle. 
3. In this paper, it is clear that there is deep relationship between normalization layer and generalization, and there are indeed some papers on the relationship between normalization layer and generalization. Therefore, a related discussion should be included.

Limitations:
Yes. This paper discusses the limitation of this paper, where they only focus on the vision data.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper tries to analyze the effects of different layers for sharpness-aware minimization (SAM). The authors find that normalization layer plays an important role in the improvements of SAM and only perturbing the normalization layer can obtain a comparable result. Therefore, this paper proposes SAM-ON and the experimental results illustrate that SAM-ON can obtain a great performance on many different tasks with different models. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
Strengths:
1. This paper focus on an important problem, SAM is very important for improving generalization.
2. The experimental finding is very interesting and can efficiently reduce the computation.
3. The authors try to evaluate the performance of proposed method on various datasets and models.

Weaknesses:
Weakness:
1. Although the proposed method can improve the performance on CIFAR. But CIFAR is too simple for SAM and that is weak to illustrate the improvement of SAM-ON. However, the results about resnet on imagenet is too close for me and the improvement is minimal. I'm not sure whether you run the experiments multiple times with different seeds.
2. The experiments on ViT is also a little weak for me. For ViT, cifar is too simple and you should train ViT form scratch on imagenet. Since SAM can achieve significant improvement on ViT+ImageNet and maybe you need to analyze it to obtain more convincing results.
3. Although SAM-ON can reduce the computation, the training efficiency cannot be significantly improved. 

Limitations:
N/A

Rating:
4

Confidence:
4

";1
BMVcW1IL9l;"REVIEW 
Summary:
The submission proposes a method to improve the quality of pseudo labels for semi-supervised semantic segmentation. The proposed approach leverages the spatial correlation of labels in segmentation maps by grouping neighboring pixels and considering their pseudo-labels, rather than generating pseudo labels at each pixel independently. Experiments show improvements over state-of-the-art semi-supervised semantic segmentation methods.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The proposed approach is conceptually simple and experimental results indicate it is effective.

Weaknesses:
1. It is not clear to me what is the improvement from considering neighbors when generating pseudo labels. It would be good if Table 4 (a) has an extra column with experiment using neighbor size 1x1, meaning no neighbor is used. Right now, the only comparison is with SOTA models that do not consider context, but the comparison is not convincing as there might be implementation differences. Also, why using a larger neighbor size hurts performance in Table 4 (a)?
2. The experiment setting is still a bit unclear to me. It would be great to state what images are used as labeled images and what images are unlabeled. Also, I think there misses an upper bounding experiment with the same model trained with all annotated images. Right now the SSL model is outperforming fully-supervised model, which does not make much sense to me.
3. Since Pascal VOC is a relatively small dataset, can we use more unlabeled images (e.g., COCO) to further improve performance?

Limitations:
Please see weaknesses.

Rating:
4

Confidence:
3

REVIEW 
Summary:
In this paper, the authors propose a new confidence refinement scheme called S4MC for improving the pseudo-labels in semi-supervised semantic segmentation. Unlike existing methods, S4MC considers the spatial correlation of labels in segmentation maps by grouping neighboring pixels and collectively analyzing their pseudo-labels. This approach allows for the utilization of more unlabeled data during training while maintaining the quality of the pseudo-labels, all without significant computational overhead. Through extensive experiments on standard benchmarks, the authors demonstrate that S4MC surpasses current state-of-the-art methods in semi-supervised learning, offering a promising solution to reduce the cost of acquiring dense annotations.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
+ The paper is written in a clear and concise manner, effectively presenting the motivation and ideas behind the proposed approach. 
+ The proposed approach is rigorously evaluated through extensive experiments performed on multiple benchmarks.

Weaknesses:
- The contribution of this paper is somewhat limited, as the idea of leveraging contextual information from neighboring pixel predictions to enhance segmentation has been extensively explored in the literature.
- In Tables 1-3, it is evident that S4MC + FixMatch (Ours) demonstrates superior performance compared to S4MC + CutMix-Seg (Ours). However, the performance of FixMatch, U2PL+FixMatch, and PS-MT+FixMatch  is not explicitly mentioned in the given context.

Limitations:
The authors have adequately addressed the limitations

Rating:
5

Confidence:
4

REVIEW 
Summary:
This work focuses on semi-supervised semantic segmentation. To generate more accurate pseudo-labels, this work attempts to leverage the spatial correlation of labels in segmentation maps and proposes a confidence margin refinement module to model the contextual information. Extensive experiments have been conducted to validate the effectiveness of this method. The proposed method achieves promising performance on different datasets and settings.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Strengths:
The idea is interesting and the writing is clear.
The effectiveness of the proposed method has been validated on several standard benchmarks.


Weaknesses:
Weakness:
The time complexity of the proposed confidence margin refinement (CMR) module is necessary to be analyzed. According to L153, Eq9, and L168-170, the neighbor pixel selection will be performed for each class at every pixel location, that is the time complexity is proportional to the number of pixels, categories, and the number of unioned events. Since semantic segmentation is a dense prediction task, such a computation cost may not be neglected. 
The example presented in L155-158 is not convincing to support the argument in L154. In this example, the author only considers one specific neighboring pixel which has a higher probability for the first class, but for the second class, it should search again with the Eq9 and that is not necessary to select the previous neighboring pixel but to select a pixel whose probability for the second class can most contribute to the interested uncertain pixel in a predefined neighborhood. Especially at edge pixels where the category changes, this kind of argument is hardly guaranteed. 
L158-160 argues that the proposed CMR module can prevent the creation of over-confident predictions. However, the refined prediction \tilde{p}_c((x_{j,k})) = p_c(x_{j,k}) + p_c(x_{l,m}) (1 - p_c(x_{j,k})) based on Eq8 will definitely be greater than or at least equal to the original one p_c(x_{j,k}), that is the refined prediction will become more confident. It seems the argument is not reasonable.
The difference between S4MC+CutMix-seg and S4MC-FixMatch requires more explanation. Since the student-teacher modeling framework seems to be the default setting in this work and the CutMix operation is normally taken as a strong data augmentation in a semi-supervised semantic segmentation task[1][2], does that mean the latter method uses a stronger data augmentation to achieve a better performance? If it is, then what is the data augmentation? 
The latest comparing methods is published in CVPR2022. Due to the rapid development of the semi-supervised semantic segmentation area, it is necessary to compare the proposed approach with more up-to-date methods, such as [1][2]. It seems that the proposed method is comparable to [1] and inferior to [2] in the CutMix setting.
[1] Semi-supervised Semantic Segmentation with Prototype-based Consistency Regularization, NeurIPS2022
[2] Revisiting Weak-to-Strong Consistency in Semi-Supervised Semantic Segmentation, CVPR2023


Limitations:
NA

Rating:
6

Confidence:
5

REVIEW 
Summary:
To incorporate spatial contextual information in Semi-supervised segementation, the authors proposed S4MC, which refines confidence with the help of neighbors. This interesting refinement scheme aids in both adding falsely filtered pseudo-lables as well as removing erroeous ones. Additionally, a Dynamic Partition Adjustment module is employed to enable more pseduo-labeled pixels to be used. Results on several settings verified the effectiveness of S4MC.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* the confidence refinement scheme is novel and effective
* the evaluation is comprehensive and the results achieve new state-of-art
* the paper is well-written and the sturcture is clear
* easy to be followed

Weaknesses:
please refer to the questions listed below.

Limitations:
The authors should further discuss potential negative societal impact of this work and also should discuss the limitations of this work.

Rating:
6

Confidence:
4

";0
9STYRIVx6u;"REVIEW 
Summary:
This paper studies the convergence of various algorithms falling under the umbrella of *Mean-Field Langevin Dynamics* (MFLD). Those algorithms encompass diverse approximations of the usual Langevin dynamics
$$ dX_t = -\nabla\frac{\delta F(\mu_t)}{\delta \mu}(X_t) + \sqrt{2\lambda}dW_t $$
on three main points:
- when the measure $\mu_t$ is approximated by a finite-population version, i.e. the empirical measure $\mu_X$ of $(X_t^1, \dots X_t^N)$ all following the same dynamics,
- when the dynamics are discretized according to a given time-step $\eta$,
- and finally when we only have access to a noisy version $v_t^i$ of $\nabla\frac{\delta F(\mu_X)}{\delta \mu}(X^i)$.

The authors provide quantitative bounds for the convergence of $\mu_X$ to the minimum of an appropriately defined measure, which minimizes an entropy-regularized objective. They first provide a one-step bound in the general case, which is then turned into more precise convergence bounds depending on the version of $v_t^i$ considered.

Soundness:
4

Presentation:
2

Contribution:
3

Strengths:
This paper considers a very important and widely studied problem in the study of wide neural networks.The results obtained are impressive: even if the finite population approximation is borrowed from Chen et al., it is complemented by very precise and technical arguments about discretization and stochastic approximation, which makes this paper a major improvement on the state of the art.

The paper flows decently well, and even the appendix proof are nicely explained and made as easy to follow as possible (which is not a simple task, given their technicality). The significance of the paper is made clear by expanding on several well-studied instances of the general problem.

Weaknesses:
On the significance side, the main drawback of the paper is the necessity for two regularization terms (one strong convexity term in the objective function $U$, and the entropic regularization term), and the heavy dependency of the bounds on those two parameters. I am aware, unfortunately, that this is for now a common restriction for any time-independent bounds on Langevin dynamics.

The main drawback of this paper is that it is simply too technical for a Neurips submission. The presentation is good, but still suffers heavily from the 9-page (and even the 10-page, if accepted) limit, which forces a lot of inline math even for key equations (e.g. the log-Sobolev definition). This results in a very dense and sometimes hard to follow article, which could really benefit from a longer exposition. Notably, some examples (especially the first one) could be expanded upon, both to understand exactly the role of first-variation functional and the comparison with existing results on SGD/mean-field for 2LNNs.

All in all, this paper is more suited for a (very good) journal than for Neurips itself; however, I cannot recommend rejection based on the significance of the results.

Some parts of the appendix are also a bit rushed, especially Appendix A, which only lists the necessary conditions for the examples to fit Assumptions 1 and 2 without any proof.

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work considers the analysis of mean field langevin dynamics as implemented algorithmically. i.e, 
a) particle approximation b) time discretization and c) stochastic gradients. Under the assumption of certain logarithmic sobolev inequalities, prior works were mostly restrict

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Extensive and explicit non-asymptotic convergence rates are obtained for the practically implemented algorithm with finite particles, time discretization and stochastic gradients, which is a great addition to the literature.

Weaknesses:
1. What are the main technical insights in this work? It seems like an extension of Wibisono and Vempala's LSI analysis of LMC while accounting for the mean-field and stochastic gradients.

2. Assumption 4 seems non-standard and the existence of a.s. bounded large order derivatives of the stochastic gradient seems restrictive compared to the assumptions in SGD/ SGLD literature. Similarly Assumption 5 seems too restrictive. 

3. What is the point of SVRG type algorithm when straightforward stochastic approximation like SGLD itself outperforms this? This is true in Table 1 for MFLD. We also refer to the Theorem 6 in [A1] for results on regular Langevin dynamics. 

4. Can you compare your technique with the recent results which utilized the CLT structure in the batched noise to obtain sharp analysis of stochastic approximations such as SGLD? (see [A1]). In Theorem 3 in the current work, with a batchsize $B$, the error bounds will have a $\frac{\eta}{B}$ (similar to [A2]) Whereas [A1] gets a rate of $\frac{\eta}{B^2}$.  Since the analysis techniques are close to that of SGLD, and one of the main contributions is the addition of stochastic gradients, it would be good if the authors compare the results to that of [A0,A1,A2] on a technical level.

[A0] Non-convex learning via Stochastic Gradient Langevin Dynamics: a nonasymptotic analysis

[A1] Utilising the CLT Structure in Stochastic Gradient based Sampling: Improved Analysis and Faster Algorithms

[A2] Faster Convergence of Stochastic Gradient Langevin Dynamics for Non-Log-Concave Sampling


Minor: Theorem 2, definition of $\mathcal{Y}_k$ needs to have $\eta_k$
The notation is a bit cluttered at many points, which makes the paper hard to read sometimes. 


I will give a borderline accept for now. Happy to improve my score once the authors provide a satisfactory response.

Limitations:
Yes.

Rating:
6

Confidence:
4

REVIEW 
Summary:
In the work authors study mean field Langevin dynamics under stochastic gradient updates and prove uniform in time propagation of chaos that takes into account discretisation, stochastic errors which allows to establish convergence rates of MFLD.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. Strong theory with explicit bounds
2. Excellent outline of results
3. Thorough comparison of convergence rates with other approaches
4. Wide applicability of results to different sg estimators and neural networks in mean field regime.

Weaknesses:
Seeing some numerical evaluation could make results even stronger to support the theory.

Limitations:
Yes

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper studies the mean-field Langevin dynamics (MFLD) with stochastic gradient updates. In particular, the authors propose a general framework to prove a uniform-in-time propagation of chaos for MFLD. The authors establish the convergence rate guarantees to the regularized global optimal solution, simultaneously addressing the uses of particle approximation, time discretization and stochastic gradient. 


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This work provides a quite general framework for analyzing the mean-field Langevin dynamics (MFLD) with stochastic gradient updates. The authors establish the convergence rate guarantees to the regularized global optimal solution, simultaneously addressing the uses of particle approximation, time discretization and stochastic gradient. The results are interesting and appear to be novel. Various examples and practical implementations of MFLD are also provided to demonstrate the effectiveness of the proposed framework.

Weaknesses:
Purely theoretical paper. No numerical experiments are provided. 


Limitations:
Yes. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper provides a set of results to analyze convergence of mean-field Langevin dynamics with a set of related algorithms. These results are in discrete time and space. Using log-Sobolev inequality techniques from optimal transport, which creates the opportunity for extensions to other settings. The propagation of chaos results are also proved in discrete time. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
My feeling is that this paper provides a useful advance in its effort to systematically control discretization error in the propagation of chaos. I am not familiar with other results that have done this. The rather explicit results obtained for the mean-field dynamics settings illustrates favorable scaling for single loop algorithms, at least in sufficiently smooth problems. 

Weaknesses:
I think the differences from Nitanda and Chizat could be articulated more clearly. 

Limitations:
LSI conditions are obviously hard to satisfy for some problems, but this is well-known and well-discussed in the paper.

Rating:
8

Confidence:
4

";1
59D5vAGhHQ;"REVIEW 
Summary:
The paper presents new update rules for block-coordinate descent (BCD) methods to minimize a smooth function subject to one linear equality constraint (precisely, all variables must sum to 1) and possibly a box constraint. A popular method to solve large-scale problems of this type is BCD with blocks of size 2 (i.e. we always update 2 coordinates at a time). A prominent example is SVM training in LibSVM.

For problems without the box constraint, the key observation is lemma 2.1, which shows that greedy version of such updates is equivalent to 1-norm steepest descent. This allows the authors to employ known results from non-smooth optimization and propose better update rules and derive their convergence rate. This rate is better than known rates for non-greedy updates.

For problems with a box constraint, the situation is more complicated because Lemma 2.1 does not apply. The authors nevertheless propose a new update (GS-1) based on 1-norm steepest descent, which is not guaranteed to change at most 2 variables in every update, but more than 2 variables are changed only in a finite number of updates. This can be seen as a greedy version of BCD with small blocks. The update has a better convergence rate than known updates that can be computed in reasonable (less than $O(n^2)$) time.

The theory is supported by a synthetic toy experiment on the linear LS problem with one linear equality constraint and a box constraint.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Crisp and elegant theoretical result, non-trivial.
However, let me admit that convergence analysis of optimization algorithms is not precisely my expertise, so I am not reliably judge the novelty (these or similar results may be already known or obvious, ...).

The text is precise and clear.

Weaknesses:
A major weakness is experiments: they are limited only to a simple problem (linear least squares) on synthetic data, end even then the data variability tested is small (only square system). To my understanding, the GS-1 update has never been applied to SVM training - so such experiments would be very interesting.

Moreover, in the experiments the difference $f(x)-f^*$ (vertical axis in Figure 1) is never shown to converge to zero, it is shown only in the range $10^{4.8} - 10^{5.2}$. In my experience, it sometimes happens that greedy updates improve the objective better initially but later they slow down and cyclic/random updates catch up. Or, perhaps, the vertical axis in Figure 1 shows $f(x)$ rather than $f(x)-f^*$ (as in the Supplement)?

Minor issues in the text:

- The five-line derivation on line 79-80 is trivial - it can be shortened to save space.

- Lemma 2.1 is very much related to the well-known result that greedy coordinate descent is equivalent to 1-norm steepest descent (see, e.g., Section 9.4 in Boyd's book on convex optimization). This may deserve a citation.

- Lemma 2.1 would be clearer if $\nabla f(x)$ were replaced by a general vector, say c.

- Line 128: mentioning SVM is not OK here because SVM needs also a box constraint.

- Line 139: the notion of ""dimension independent convergence rate"" is not well-defined because the problem dimension can be hidden in constants (such as $L_1, \mu_1$). But I understand that this notion may be widely used.

Limitations:
The experiments are limited to a very simple problem with synthetic data.

Though the theoretical results are elegant, the optimization problem considered has a rather limited applicability.


Rating:
5

Confidence:
4

REVIEW 
Summary:
The first goal of the paper is to minimize a smooth function subject to a summation constraint. The authors demonstrate that the greedy 2-coordinate descent (CD) method, when applied to the problem with equality constraints, achieves a linear rate of convergence under the proximal PL inequality under the L1-norm formulation. Notably, this convergence rate remains unaffected by the problem dimension, which sets it apart from random selection methods. Furthermore, they establish that there exists at least one steepest descent direction with respect to L1-norm, which can be utilized as a 2-coordinate descent update. They leverage this relationship to derive the convergence rates of the CD algorithm.
Additionally, they explore the minimization involving a summation constraint and prescribed lower and upper bounds on the coordinates. They demonstrate that employing bound- and summation-constrained steepest descent in the L1-norm guarantees significant progress at each step, unlike the GS-s rule. Moreover, this method (called GS-1) is computationally more efficient than GS-q, requiring only $O(n \log(n))$ iterations instead of $O(n^2)$

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The paper gives linear convergence rates for greedy 2-coordinates CD, and the steepest descent in l1-norm under proximal PL for l1-norm. For problems with equality constraints, this work shows that greedy methods may be faster by a factor ranging from $O(n)$ up to $O(n^2)$ than methods picking coordinates at random,

2. For problems with bound constraints and equality constraints, the paper shows that GS-1 rule has the benefits of both GS-q and GS-s. Contrary to GS-q, GS-1 rule can be implemented in $O(n \log n)$ and contrary to GS-s, GS-1 guarantees non-trivial progress
at each iteration.

3. The authors also proved a linear convergence rate for GS-q rule under proximal PL for L2-norm. It is not straightforward to use prox-PL to obtain the linear convergence rate for this update rule. They consider the conformal realization used by Tseng and Yun [2009] to upper bound the extra terms in descent inequality by a notion of gradient mapping used in prox-PL’s definition.

4. The paper is clear and well written.

Weaknesses:
The experimental part is limited, which is fine for a theoretical optimization paper, but for a ML venue and given the claim made in the paper that many problems in ML require to satisfy an equality constraint, such as discrete probabilities or SVMs with an unregularized bias term, one could expect the experimental section to show the power of GS-1 for other such examples than only a synthetic equality-constrained least square problem.

Limitations:
N/A

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper studies new coordinate descent-type methods for equality-constrained problems, where 2 coordinates are updated on each iteration and proves new convergence guarantees under suitable proximal-PL conditions that allow to obtain linear convergence rates for the proposed methods. 
The first main result considers the case of a single constraint on the sum of coordinates with a simple greedy rule for selecting the two coordinates to update. A linear dimension-independent convergence rate is established using a fixed step-size and assuming PL inequality w.r.t. 1 norm.

Next the authors consider the more challenging setting in which on top to the constraints on the sum of coordinates, there are also box constraints on the individual coordinates. Here the authors propose to use a greedy-proixmal update named Gauss-Southwell-q which leads to a linear rate using a fixed step-size and assuming PL inequality w.r.t. 2 norm, however with linear dependence in exponent of the dimension.

Finally, in the latter setting (equality constraint on sum + box constraints), considering a PL inequality w.r.t. 1 norm and a diffrerent greedy rule (which corresponds to solving a proximal-style problem w.r.t to 1 norm, which given the gradient direction could be solved in O(nlogn) time), the authors obtain a linear dimension-independent rate.

The proofs hinge on relating the 2-coordinate descent updates to the steepest descent method w.r.t. the 1 norm, which is interesting.

The authors present experiments that support their findings on a random least squares problem with equality (and then also box) constraints.



Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is very well written and is easy to follow. The results and ideas are clearly presented. I enjoyed reading it. 

The problems considered are sound and make sense, and the type of methods discussed is of clear practical interest to the community.

The technical argument of relating 2-coordinate descent to steepest descent w.r.t. 1 norm is interesting and might be of further use.

The convergence rates obtained are novel and interesting to the best of my knowledge.

Weaknesses:
1. I think the authors could do a better job in rigorously comparing the complexity of their methods to previous ones. It is not clear at all times if the obtained complexities are state-of-the-art (when not considering only 2-CD greedy methods). In particular, from first paragraph of 2nd page, it is not clear how this compete with known random selection methods.

2. To continue the above point, it seems all selection rules suggested require the full gradient direction. However, in such cases why these methods are better than full-gradient methods? For instance, for unconstrained least squares indeed doing an update to a single coordinate is much faster than computing entire gradient, and makes perfect sense when using random selection (e.g., w.r.t. to coordinate-wise Lipchitz parameters). But in the greedy rules we already need the full gradient. 

3. The linear rates that are dimension-independent rely on PL w.r.t 1 norm. This might seem as ``hiding the dimension under the choice of norm''. Could the authors provide examples of interest that will satisfy this inequality with constant independent of the dimension? Or more precisely, that the ratio of the 1 norm constant and 2 norm constant is dimension independent?

4. At some point the authors claim that computing the coordinate-wise Lipchitz parameters can be difficult, however, in the most relevent setting I guess which allows also for the PL inequality: f(x) = g(Ax), with g strongly convex, in case g is well conditioned (e.g., squared Euclid norm in many cases), it is easy to estimate them I believe from the columns of A.

5. Throughout the paper the authors mention SVM as a major application, so why not conduct experiments with this application?

Limitations:
NA

Rating:
6

Confidence:
3

REVIEW 
Summary:
This work studies minimizing a smooth function with a summation equality constraint over its variables. The authors show a connection between the greedy 2-coordinate update and steepest descent w.r.t. 1-norm, and introduce a new proximal PL assumption w.r.t. 1-norm. They show improved convergence rates under such assumption over random selection. The authors also consider coordinate-wise Lipschitz smoothness and introduce an approximation for greedy 2-coordinate methods. They complement their theoretical results through numerical experiments on randomly generated problems.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The connection from 2-coordinate greedy update to steepest descent w.r.t. 1 norm seems non-trival and potentially interesting for equality-constrained problems.
2. The authors offer detailed comparisons on various Gauss-Southwell update rules (GS-1, GS-q and GS-s).

Weaknesses:
I believe this work has potential but lacks a few key ingredients.
1) This work lacks justification for their proximal-PL condition w.r.t. 1-norm. For example, Karimi et al (2016) provides five important classes of functions that statisfy the proximal-PL condition w.r.t. 2-norm. It is unclear to me what function classes can statisfy the proximal-PL condition w.r.t. 1-norm where the worst case dependence on $n$ for $\mu_1$ can be avoided. It would also be useful if there can be empirical comparisions for $\mu_1$ against $\mu_2$ as $n$ scales. At its current state, I do not think it is reasonable to claim that the convergence rate is independent of the problem dimension $n$.
2) I would like to see experiments performed on practical datasets (e.g., LIBSVM datasets) rather than randomly generated Gaussian data, since much of the theoretical results aim to improve upon the rules in LIBSVM. I would also like to see experiments on SVM problems to support their claimed improvements for SVM.
3) I suggest adding markers to the plots in Figure 1 and 2, as the overlapping nature of the results renders some plots unparsable.

Limitations:
N/A.

Rating:
5

Confidence:
3

";0
Bkrmr9LjeI;"REVIEW 
Summary:
This paper proposes an algorithm called DISCO-DANCE for unsupervised skill discovery in RL. The algorithm augments the mutual information reward of DIAYN with a gudiance reward. The guidance reward encourages indistinguishable or unconverged skills to follow skills that can potentially visit under-explored states, such that the over all skill collection can have broader state coverage. The authors conduct experiments on 2d navigation, Ant maze, and DMControl suites by evaluating the state coverage and the fine-tuned downstream task performance, showing that DISCO-DANCE outperforms baselines in most of these benchmarks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
+ Illustration of the main idea (i.e., find and follow a guide policy) is clear (in particular fig.1) and well-motivated.

+ The authors provide open-source code and extensive experiment details such as hyperparameters and resource requirements, which make the results presented in this paper reproducible.

+ The appendix provides extensive discussions about the algorithm's limitation and comparison with other baselines, which can help reader understand the proposed algorithm deeper.

+ Experiment results are promising.

Weaknesses:
+ The authors tend to address the problem unsupervised discovery in complex environments where existing methods are no longer effective, but experiments are mainly conducted on common benchmarks. I acknowledge that several maps in the navigation task are challenging, but the locomotion tasks (AntMaze and DMC) are not. Actually, baseline algorithms can outperform the proposed algorithm in terms of downstream task performance (see Appendix G).


Limitations:
+ The authors have addressed several main limitations in appendix I.
+ The selection of the guide skill depends on the final state visited existing skills. It does not seem to be a general solution to select guide skills even for state-based environments.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a new unsupervised skill discovery method based on the guidance of exploration. A policy is conditioned on a latent skill variable like in prior work. The method first starts by identifying a ""guide"" skill variable that is likely near unexplored states, the novelty of unexplored states is measured by the density of random walk arrival states started from the terminal states of each skill. It then trains other skills that explore the vicinity of terminal states region from guide skill.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper analyzes the limitations of state coverage in previous skill discovery methods and provides nice motivation for the proposed method. 

2. The empirical result in mazes and control tasks are promising.


Weaknesses:
1. Measuring the density of the state distribution via generating random walk arrival states from terminal states is not sample efficiency. 

2. The method needs to select a guide skill that is most adjacent to the unexplored states. In the bottleneck maze tasks in Figure 3, what if all skills including the guide skill cannot pass the first room? Will this method also encourages effective exploration? 


Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
In this submission, the authors propose an unsupervised skill discovery method called DISCO-DANCE. It samples *guide skills* with random walk processes that start from the terminal states of a set of skills and use them to guide less discriminable or new skills toward those guide skills so that they can reach unexplored areas more easily. They test their method in two navigation environments (2d maze and Ant maze) and Deepmind Control Suite and compare the state space coverages and performances with the baselines.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
- In terms of orignality, the main idea of this work to find a reachable state that is close to the unexplored region and to expand the skill set based on it is novel to some degree.
- The manuscript is mostly clear and easy to follow. Also, the concept figure effectively provides the intuition behind the method.
- The state space coverage problem is an important aspect of unsupervised skill discovery.

Weaknesses:
- The exploration issue with the mutual information (MI) objective could be more than what is described in this work. In theory, the MI objective is not supposed to contribute to the exploration meaningfully, especially in continuous control environments (Park et al. [21]), which can make this method mostly rely on the random walk processes for its exploration.
- I believe one important weakness of this submission is the random walk process. The manuscript mentions that the rise of the environmental complexity makes existing skill discovery methods less effective and motivates this work, but ironically, in complex environments (e.g., with high-dimensional state spaces), random walk would be one of the main bottlenecks in encouraging exploration. In such environments, this algorithm could require a large number of iterations.
- In terms of writing, I think it is not very fair to call the state spaces of the environments used for the benchmark *high-dimensional*. They are higher-dimensional compared to the 2D maze environment, but labeling them high-dimensional in general may not be a good standard for the field.

Limitations:
The authors state some limitations of the proposed method (difficulty in high-dimensional state spaces and stochastic environments), but I encourage the authors to consider taking the points I listed in the Weaknesses section into account.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper addresses the challenge of learning diverse skills in unsupervised reinforcement learning by introducing a method to selectively guide candidate skills to areas of the state space with low coverage from the current set of skills.  Rather than relying directly on mutual information maximization like many skill-learning approaches do, this paper proposes an algorithm for selecting guidance skills and optimizing apprentice policies through a combination of mutual information rewards and a guidance reward.  The guidance reward is high when the apprentice skill reaches similar areas of the state space as the guidance skill, which can help direct the apprentice skills to previously unexplored areas.  The paper shows that the proposed method, DISCO-DANCE, can cover more of the state space than alternate approaches by evaluating in maze navigation environments.  The paper also provides ablation studies to help understand what each part of the method contributes to the overall performance.  

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is clearly written, with each aspect of the DISCO-DANCE algorithm explained and compared with other methods from the literature.  The contribution, both in terms of empirical results and novelty of the proposed method, is well defined.  Diagrams were effectively used to make the approach intuitive.  

In addition to describing the method well, the paper evaluates the experimental performance thoroughly, comparing it to several similar methods on several domains.  The experiments use both state coverage and fine-tuning performance, and show that DISCO-DANCE performs well.

Weaknesses:
The main weakness of the paper mostly involves my uncertainty around the questions in the next section.  I hope that the rebuttal can help clear up confusion.

Limitations:
Limitations are explicitly described and claims are not overly grand.  

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper introduce a reward shaping approach called DISCO-DANCE to enhance exploration in unexplored states in the context of latent variable skill learning in self-supervised RL context. The approach consist in defining a guide skill with the highest potential for reaching unexplored states for a given environment, then select unconverged skills and incentivize them to follow the guide skill expressed at terminal state, helping them bypass state regions with low rewards. Finally, the approach disperse the skills to maximize their distinctiveness, resulting in a set of skills that cover a wide range of states. DISCO-DANCE fills the pathway to unexplored regions with positive rewards. The approach is compared in navigation and locomotion scenarios surpassing some previous methods in terms of exploring the state space and performing well in navigation tasks in 2D mazes and Ant mazes.

Soundness:
3

Presentation:
3

Contribution:
1

Strengths:
The paper is well written and explain rather well his approach.
The problem of skill learning with self-supervised learning is important and actual.
The justification is reasonably clear, maybe it would have been interesting to discuss more the difference between environment were most degrees of freedom are part of the action space, like in locomotion and the cases of manipulation that poses the most issues.
The experiment use classic but simple 2D navigation and simulated locomotion scenarios to illustrates the benefit.

Weaknesses:
The comparison is rather limited, we woud have like to see DADS, MUSIC and LSD for example.
We would also have like to see experiments in more known challenging environment like manipulation where MI is the most in trouble.

Limitations:
The experiments are rather limited to justify the benefit of the approach.
The random walk as last step of the definition of the guide skill definition isn't shown to be scalable to larger state environments.

Rating:
3

Confidence:
5

";1
VgQw8zXrH8;"REVIEW 
Summary:
This paper presents Uni-ControlNet, a model that aims to enhance Text-to-Image diffusion techniques by allowing the concurrent use of multiple local and global controls. It only requires two additional adapters, regardless of the number of controls used, and circumvents the necessity of training from scratch. The authors assert that Uni-ControlNet performs favorably in terms of controllability, generation quality, and composability.  

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The proposed method allows for the simultaneous utilization of different local and global controls within one model, making it flexible and composable.

- It eliminates the need for training from scratch, reducing costs and making it suitable for real-world deployment.

Weaknesses:
-	Limited Novelty and Contribution: The paper's primary contributions are its condition injection strategy and training approach. However, the condition injection strategy appears to be derived from SPADE, and the training strategy seems to be based primarily on empirical evidence, without providing much novel insight or theoretical explanation.

-	Insufficient Detail in Discussion: The description of the training strategy and the inference process, stated as major contributions, are not sufficiently clear. It remains unclear how the authors handle other conditions when only one condition is being utilized. It seems problematic to set the local conditions' values to zero with the intent of rendering them empty. Moreover, it seems that Uni-ControlNet cannot handle multiple conditions of the same type, as suggested in Figure 2 of the Supplementary Materials.

-	Incomplete Comparisons in Experiments: The experimental comparison does not seem comprehensive. For instance, it's known that Stable Diffusion 2.1 unclip can also accept CLIP image embeddings as inputs, like global condition in this paper. Additionally, ControlNet can accommodate multiple conditions. Furthermore, the quantitative results do not appear to be particularly impressive - with the method achieving the best result in only 4 out of 8 instances in Table 2, and only 2 out of 8 in Table 1.  Same for CLIP score in Supplementary Materials.


Limitations:
yes.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper proposes Uni-ControlNet that leverages lightweight local and global adapters to enable precise controls over pre-trained T2I diffusion models. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper is well written and organized.
2. The idea of local/global adapter to achieve all-in-one control is reasonable and interesting.
3. The results seem good.



Weaknesses:
Since this paper is easy to follow and self-consistent, I have only a few minor questions:

1. Please compare the training costs of Uni-ControlNet with those of other methods (T2IAdapter, ControlNet).
2. I notice that the different conditions are concatenated as inputs to the adapter. If we want to add other control conditions, does the adapter need to be retrained?

Overall, although this work has some limitations, I think it meets the bar of NeurIPS.

Limitations:
The authors have adequately addressed the limitations. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposed a method to do controlleble t2i generation from a pretrained diffusion model. The main contribution is that they only have two adapters one local (e.g., edge map, keypoint etc) and one global (e.g., image). For local, they use the controlnet, but concatenate conditions as input. For global, they extract image feature and treat it as text tokens. 



Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The idea is simple and writing is clear 

Weaknesses:
There are several weakness for this paper:

1, the technique novelty is incremental. Although, they tried something such as SPADE like injecting information for local branch and combine image and text tokens for global etc, but they are very straightforward. 

2, missing baseline GLIGEN [Li et al, CVPR, 2023] which also supports conditions studied in this paper. 

3, one more weakness for this paper is missing evaluation for condition correspondence. They only reported FID as a metric, which only reflects image quality. But they should also study how well the generated images are corresponded with input conditions. For example, in GLIGEN, they use mask-rcnn to detect keypoints from generated images and compare with input keypoint, so that we can know how well the model following the input. I understand that for certain conditions such as edge map, maybe it is hard to evaluate, but as least for keypoint, semantic map, depth map, it is easy to come up with some metrics.

 




Limitations:
See weakness 





====================================================================================

They addressed my main concern which is they only evaluated image quality, but not controllability.
I strongly encourage them to add results table in the rebuttal to their paper, thus will be served as a baseline for future controllable image generation work. 

Based on this point, I am willing to raise my score despite that I feel novelty is a bit weak   

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper proposes Uni-ControlNet for the simultaneous utilization of various local controls and global controls within a single model in a flexible and composable manner. This is achieved by fine-tuning of two additional adapters on top of pre-trained text-to-image diffusion models, eliminating the significant cost of training from scratch. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper propose a new framework that leverages lightweight adapters to enable precise controls in a single model. 

Weaknesses:
1.	The training sets for the different models in table 2 are not the same. It raises the question of fairness in comparisons between the models. 

2.	The author should compare with the simple baseline Multi-controlnet: https://huggingface.co/blog/controlnet.


Limitations:
yes

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors proposed Uni-ControlNet, a novel approach that allows for the simultaneous utilization of different local controls and global controls. It uses two additional adapters (local and global) and injects their outputs into the frozen pretrained diffusion models, and only the parameters in adapters need training. 

Through both quantitative and qualitative comparisons, Uni-ControlNet demonstrates its superiority over existing methods in terms of controllability, generation quality, and composability. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. By training with multiple conditions simultaneously, Uni-controlnet is able to perform various kinds of control with only one model. 
2. Uni-controlnet only adds 2 adapters, which is efficient in both training and inference. 
3. By concatenating clip image embedding with text embedding (condition), the method can control the style of generated image. 

Weaknesses:
1. The clarification of the dataset construction is unclear. For example, is the skeleton/sketches generated by model, or manually collected? If it's automatically generated by models, the performance will be bounded by the accuracy of those models, and may suffer from distribution gaps if the control map  are painted by human during inference. Otherwise, it's very hard to anotate such a complex dataset.  
2. Insufficient ablation study. As mentioned in L14, ""Uni-ControlNet only necessitates a constant number (i.e., 2) of adapters, regardless of the number of local or global controls used."" Is it because of the structure design? If so, a normal ControlNet with multiple controls trained together should be compared with. 

Limitations:
see weakness and questions. 

Rating:
7

Confidence:
4

";1
Eewh7sl0Xj;"REVIEW 
Summary:
The present paper offers a Toeplitz matrix architecture which can handle sequence modeling. The architecture comes in two flavors. The first flavor is a fast version that is most useful for bi-directional tasks. It speeds up previous Toeplitz networks by using an interpolation and low rank approximation scheme in its setup. The second is a Fourier based model that appears to offer advantages in causal tasks. Benefits are shown in terms of the speed of training and some marginal benefits in performance.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The main strength of this paper is a noticeable speed up in an alternative architecture to transformers. I am a fan of papers that look at ways to speed up sequence modeling. The Curren paper presents a nice idea. More specifically:
- The approximations to the TNN appear to be effective in experiments and result in a faster network
- Approximations do not seem to deteriorate performance and may offer some added performance boosts
- The changes to the architecture are grounded in some theory 


Weaknesses:

**Disclaimer**: I am not an NLP expert and am more focused on the theory side. I have significant concerns about the theory in this paper, but feel the experiments and techniques are solid enough to potentially overcome that issue. Furthermore, none of the theorems are crucial to the crux of the paper, and if any are wrong, they can be removed. For this reason, I placed a borderline accept rating for now, but I believe this will need to be confirmed by people who are closer to the experimental side of the literature and can assess the experiments in a more rigorous fashion. 

\
Broader comments:
- Most of my larger concerns revolve around the theory and proofs in this project listed below.
- Reading through many times, I could not understand what was gained in the “causal training” proposal in section 3.1. First, it seems the parameters are changed to live completely in the Fourier regime. This change was made to obtain “an alternate causal speedup"", but I don’t see where that speedup arises. Following the steps, the main change seems to take the algorithm to do an FFT on the $n$-dimensional space resulting in runtime of $O(n \log n)$ which is worse than before. Also, changing the algorithm to work in Fourier space introduces a different implicit bias that I’m not sure is desired. For example, the $\lambda$ parameter controlling the decay is not controlled here. I also have many questions about this approach which I’ve left below.
- Experiments in table 1 don’t appear to offer much improvement especially considering the added parameters. I would also ask the authors to include citations to the models or results compared to in this table so it is easier to see what is being compared to. 

\
Theory comments:
- Theorem about ReLU MLPs being $d$-piecewise linear has assumptions missing or is just wrong. If the authors are implying that any ReLU network from $\mathbb{R} \to \mathbb{R}^d$ has $d$ pieces or contiguous linear regions, this is clearly wrong. MLPs are universal approximators so this is clearly false. If the authors are saying that this only holds for an MLP with a single hidden layer of width at most $d$ then this may be correct. But I don’t see this assumption made anywhere.
- Theorem 2 has a few confusing elements from my end. First, it is hard to parse. There are many variables and factors like condition number that it is hard to know the scaling of. Second, the bound doesn’t appear to be all that good. The error grows at least linear in $n$ and depends on other factors like singular values or the nystrom error that may also be badly bounded. I suppose the authors would argue that it is exponentially small in the degree of interpolation $N$, but this degree would have to grow at least logarithmically in $n$ to counteract the $n$ factor. This would result in a runtime essentially equivalent to just doing FFTs on the whole space. Third, the error in interpolation is an unusual thing to even bound in my opinion. The weights are updated with this interpolation taken into account. In other words, the algorithm learns a weight matrix with parameters contained in this interpolation. 
- Definition 2 and 3 present the discrete time Fourier transform, but in practice only the DFT of the matrix form is ever used. The resulting statements, regardless of their correctness, do not seem to apply to the setting in practice. Unless I am missing something.
- Related to the above, I cannot see why Theorem 3 and 4 are correct. Similar to my previous statements, MLPs are universal approximations so they can output any possible function. How can that statement hold true? 

\
Small:
- Simply having a % label on the y axis of Fig 1B is confusing. Percentage relative to what? Also, what does 20% speed-up mean; i.e. that it ran in 20% less time? Simply having this number on the first page can be confusing without the context added.
- Fig 1A and 1B also appear to be different size fonts.
- Line 150: I think the dense-case runtime is only a factor of $r$ worse so $O(nr  + r \log r)$ and not $O(nr^2 + r \log r)$ unless I’m missing something.
- For someone outside of the NLP community, section 3.1 needed more motivation and formality. Some details about what “causal masking”, “causal kernel”, and the sequential nature of the data would be helpful.
- To follow easier, it would be good to define what the role of N is in section 4.1 (i.e., number of interpolating points)

\
Formatting:
- Hyperref links seem to be broken
- Line 189: sentence is a run-on and hard to follow

\
Finally, to add ideas not for criticism, but instead for completing the paper and offering new ideas, there is a wealth of literature on related techniques that could be useful here, or at the very least cited. For example, there are sparse Fourier transforms that can offer speed-ups beyond the $O(n \log n)$ that the paper aims to improve on, e.g., [1]. Since matrices are low-rank and/or sparse, this could be a more direct way to get the speed-ups desired. Second, there are a number of papers on optimizing structured matrices like Toeplitz matrices, e.g. [2]. In the sequence modeling specifically, there have been a lot of papers on unitary networks for example [3], one paper which actually uses low rank approximations in its implementation [4]. Third, low rank approximations have also been used to speed up other architectures like conv-nets, [5-6]


\
**References:** \
[1] Hassanieh, Haitham, et al. ""Simple and practical algorithm for sparse Fourier transform."" Proceedings of the twenty-third annual ACM-SIAM symposium on Discrete Algorithms. Society for Industrial and Applied Mathematics, 2012.\
[2] Kochurov, Max, Rasul Karimov, and Serge Kozlukov. ""Geoopt: Riemannian optimization in pytorch."" arXiv preprint arXiv:2005.02819 (2020).\
[3] Arjovsky, Martin, Amar Shah, and Yoshua Bengio. ""Unitary evolution recurrent neural networks."" International conference on machine learning. PMLR, 2016.\
[4] Kiani, Bobak, et al. ""projUNN: efficient method for training deep networks with unitary matrices."" arXiv preprint arXiv:2203.05483 (2022).\
[5] Max Jaderberg, Andrea Vedaldi, and Andrew Zisserman. Speeding up convolutional neural networks with low rank expansions. arXiv preprint arXiv:1405.3866, 2014.\
[6] Cheng Tai, Tong Xiao, Yi Zhang, Xiaogang Wang, et al. Convolutional neural networks with low-rank regularization. arXiv preprint arXiv:1511.06067, 2015.



Limitations:
There is a very brief discussion of limitations in the conclusion, though I feel this could be expanded. I would also appreciate some context for this work in relation to other works in NLP and how it fits into the broader landscape of NLP architectures. For someone like me not in the community, this would be useful to better understand its limitations from a practical perspective.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors of the paper propose two modifications of a recently published alternative to attention mechanism, Toeplitz Neural Operator (TNO), which constitutes the most important part of Toeplitz Neural Networks. The application of TNO is the multiplication of the input sequence by a Toeplitz matrix. Parameters of this Toeplitz matrix are given by a lightweight feed-forward network, called Relative Position Encoder (RPE). The first proposed modification, called SKI-TNN, represents a learned Toeplitz matrix as a sum of a sparse and low-rank matrix. Thus, its multiplication by a vector has the complexity of O(nr^2 + r log r) instead of O(n log n), where r is the rank of a second summand. However, this modification can speed up only the task of bidirectional modeling. In order to speed up the causal modeling (such as autoregressive language modeling), authors view the TNO as an application of kernel to a vector. Then, they train the RPE to model the real part of the Fourier transform of this kernel. The imaginary part is then computed via the Hilbert transform of the real part. This modification does not change the asymptotic complexity, but achieves empirical speed up.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The article explores an important topic of speeding up the token mixing part of a general sequence modeling pipeline. Nowadays, this topic is highly relevant because of its applications in the field of NLP. The article builds off of a very recent paper [1].
2. The article creatively combines together a large body of previous work. It uses the ideas of TNN, SKI, FFT, Hilbert transform, Nyström approximation, fast causal masking, etc. 
3. The experimental results show that the proposed modifications do indeed speed up the original TNN.
4. Overall, the presentation style is mathematically strict and to the point. The formulae in section 3.2.1 and in Appendix are sufficiently well-explained. Both modifications proposed in the paper are succinctly defined in Algorithm 1 and Algorithm 2. This helps the reader significantly to understand the main ideas.
5. In section 3.2.1, the authors specified not only the theoretical complexity of their modification, but also the practical limitations they meet when implementing it, and specified the practical complexity as well as theoretical.
6. The theory on the smoothness in Fourier Domain is supported with experimental visualizations
[1] Zhen Qin, Xiaodong Han, Weixuan Sun, Bowen He, Dong Li, Dongxu Li, Yuchao Dai, Lingpeng Kong, and Yiran Zhong. Toeplitz neural network for sequence modeling. In The Eleventh International Conference on Learning Representations, 2023.


Weaknesses:
1. The main claim of the article is the speedup achieved by the proposed modifications. The only results supporting this claim are Fig. 1 and some percentages in the text (in section 5.1). Fig. 1 shows the performance on specific tasks from the LRA benchmark. Firstly, it is not clear for which task the baseline (TNN)  is evaluated. Secondly, the choice of the tasks shown on the graph is questionable. The hardest task from the LRA benchmark, Pathfinder-X, is not shown. As for the speedups mentioned in the text: it would be better to put them all into a separate table. Moreover, it would be interesting to see a speed comparison in the form of a table similar to Table 5 from the TNN article.
2. In section 4.2, theoretical results on the choice of activations are presented. Several possible improvement ideas. The ablation study with experimental results for different activation types would be of interest. Moreover, the graphs showing the decay rate for randomly initialized networks might be improved. It would be better to leave only the lowest and highest lines and show the average line in between. Also, if you compare the rate of convergence to some baseline rate (e.g. exponential), plotting it would be appropriate. In addition, it seems to be not quite fair to compare the decay rates of trained and untrained networks.
3. While the overall presentation style is to the point, as mentioned earlier, it would help the reader if the abstract, introduction and related work were more general. Both abstract and introduction may be hard to read for an unprepared reader, as they contain too much mathematical details and not enough motivation. Moreover, some paragraphs of the introduction repeat the abstract almost word for word, while rephrasing would help the reader to understand the ideas more deeply. The related work section should describe either the ideas of mentioned papers or their connection to your paper more clearly.
4. The section 3.2.2 is a bit obscure. “Inverse time warp” is not a common term, and it is not described in the section.


Limitations:
None

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper presents several techniques to speed up Toeplitz neural networks (TNNs). In particular, TNNs use convolution of length n (sequence length of the input) and so scales as O(n log n), and TNNs have many calls to the MLP that generate relative positional encoding (RPE) and decay bias. To reduce the time of convolution, for bi-directional modeling the paper proposes to approximate the Toeplitz matrix as a sum of a short convolution and a low-rank matrix, which results in O(n + r log r) complexity where r is the rank of the approximation. For uni-directional model (e.g. auto-regressive modeling), the paper proposes to parameterize the convolution directly in frequency domain and uses the Hilbert transform to obtain the imaginary part from the real part of the filter to ensure causality. The approximation error is then analyzed. Validation on language model (Wikitext-103) and long-range benchmark (LRA) show that the approximation lead to some speedup (10-15%) and the quality stays around the same.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The idea of using asymmetric Nystrom to approximate the Toeplitz matrix is quite clever. This allows a decomposition into a sparse and a low-rank component, which leads to asymptotically faster algorithm in the case of bi-directional modeling.

2. While uni-directional modeling prevents the Nystrom technique due to causal masking, parameterizing the filters directly in the frequency domain is able to overcome this challenge. While this is not asymptotically faster, it avoids one inverse FFT per layer and leads to some speedup.

Weaknesses:
1. Unclear what problem the paper is trying to address, and how it is motivated.
The intro starts out with Toeplitz neural networks, and the paper aims to make it faster. However, it's not clear why we want to make these faster, and what we would enable if we make these faster. Are they being used in very large-scale tasks? Are they being scaled to very long sequences?
While the technical contributions are solid, it's not clear to me why the paper chose to tackle this problem.

2. Unclear what the technical challenges are. 
- The paper mention that they want to avoid O(n log n) computation. But in practice O(n log n) isn't very slow, especially on GPUs. FFTs are pretty much bounded by memory bandwidth, and they take only 2-3 times as long as any pointwise operation. So if the goal is to speed up TNNs, then it makes more sense to have an efficient implementation, rather that using algorithms that faster asymptotically (O(n + r log r)) but is slower than a hardware-friendly algorithm (line 150, where using matmul with O(n r^2 + r log r) is faster). 
- The paper mentioned ""many calls to the RPE"". Why is this a problem? Showing a profile of how much each operation is taking will be much more convincing. That would motivate the approaches in the paper much better.
Without knowing how long each operations in TNNs are taking, how do we know that we're solving the right problem?

3. Lack of detailed speed benchmark. Given the goal is to speed up TNNs, I would have expected one of the main results to be speed benchmarks, across different sequence lengths, on different devices, to show the tradeoff. In the main paper, speed is only reported in Figure 1b, which is end-to-end speed for a particular sequence length (512).
How do we know that we're close to the maximum speed on these devices (GPU)? Or are we still far from optimal? When we speed up convolution and RPE, what is the remaining bottlenecks.
Having these would make the paper stronger.

At sequence length 512, an optimized implementation of attention (e.g. FlashAttention) is likely faster than FFT and the method in this paper. This is my impression as the Hyena paper [1] reports that TNNs are not faster than FlashAttention until sequence length > 4k.

[1] Hyena Hierarchy: Towards Larger Convolutional Language Models. Poli et al. 2023.

Limitations:
Not necessary.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper proposes to reduce the computational complexity of Toeplitz neural networks. TNNs are a new form of network for sequence modeling that reduces space complexity of the attention matrix to allow for longer sequences.TNN model consists of a stack of Gated Toeplitz Units (GTU) that includes TNO (Toeplitz Neural Operator) that does token mixing with relative positioning. Then, GTU is a modified GLU layer injected with the proposed Toeplitz Neural Operator (TNO). 
The paper addresses the TNNs efficiency limitations: 1) super-linear computational complexity 2) many calls to the RPE: for each layer, one call per relative position. Thus, the paper proposes to reduce both the complexity of sequence modeling and of the relative positional encoder. The work proposes solutions by means of both the Structured Kernel Interpolation (SKI) [2] and working with frequency domains.
It does so through:
–	Approximating Toeplitz matrix using low-rank approximation and replacing the RPE MLP with linear interpolation and using Structured Kernel Interpolation.
(for O(n) complexity, that is use linear interpolation over a small set of inducing points to avoid the MLP entirely   -using an inverse time warp to handle extrapolation to time points not observed during training)
–	Causal training, SKI does not bring benefits, so instead they eliminate explicit decay bias by working in the frequency domain, using Hilbert transform (to force causality) and also use some smoothness
–	 For the bidirectional case, they eliminate the FFT applied to the kernels.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The work includes a number of solutions to improve TNN speed-up (addressing RPE MLP, the FFT, and the decay bias).

RPE is a neural network to obtain relative position embedding to obtain entries in Toeplitz matrices. These entries could be evaluated with stationary non-SPD kernel which is a good idea.

So first decomposing Toeplitz matrix and then using interpolation for MLP is a comprehensive pipeline.

The theory part makes the arguments more sound, and the explanations in supplementary materials are fairly abundant.

The experiments on LRA show good predictive performance on long range data and on wikitext some speed-ups.


Weaknesses:
The major paper of the paper talks about the SKI to accelerate the TNNs but in experiments SKI is only shown in the LRA experiment and does worse than both TNN and FD-TNN

As mentioned in the paper, doing sparse-dense multiplication in practice can be slower than dense-dense matrix multiplication (but that is only part of the potential speed-up)


Limitations:
Yes.

Rating:
5

Confidence:
3

";0
OAOt75zsdP;"REVIEW 
Summary:
The paper demonstrates a root cause of the over-pessimism issue of existing distributionally robust optimization (DRO) methods: excessive focus on noisy samples. To mitigate this issue, the authors proposed a novel DRO method called Geometrically-Calibrated DRO (GCDRO). They introduce the free energy implications of their method (Section 3.1) and approximate optimization method (Section 3.2). Finally, they empirically validate their method for both simulation data (Section 4.1) and real-world data (Section 4.2).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- While this paper is a theoretical work (for the ML community), I believe that the problems addressed in this paper can be applied in practical situations. Specifically, existing debiasing methods [1, 2] focus on minor group samples with loss/gradient information. Similar to existing DRO methods (as the paper shows), these debiasing methods will focus excessively on noisy samples. In this case, GCDRO (or a more computationally scalable approximation version) can be applied to mitigate this issue.
    - [1] Nam, Junhyun, et al. ""Learning from failure: De-biasing classifier from biased classifier."" *Advances in Neural Information Processing Systems* 33 (2020): 20673-20684.
    - [2] Ahn, Sumyeong, Seongyoon Kim, and Se-young Yun. ""Mitigating Dataset Bias by Using Per-sample Gradient."" *arXiv preprint arXiv:2205.15704* (2022).
- The free energy perspective of DRO using duality seems novel and interesting.

Weaknesses:
- The organization of this paper is somewhat puzzling and hard, although it deals with theoretical topics that are difficult for non-experts to understand. For example, it would be recommended to highlight the key difference between DRO and ERM in Section 2. As ERM does not assume graph structure G_N, readers unfamiliar with DRO would expect a detailed explanation/usage of the graph in L74.
- It would be recommended to add a confidence interval for the results in Figure 2. Also, a vector image format provides more clear results as it does not blur when zooming in.

Limitations:
The authors did not discuss the limitations of their method.

Rating:
8

Confidence:
3

REVIEW 
Summary:
In this work, the authors propose a novel approach called Geometry-Calibrated Distributionally Robust Optimization (GCDRO) to address the over-pessimism issue in traditional Distributionally Robust Optimization (DRO) methods. DRO aims to optimize worst-case risk within an uncertainty set to mitigate the effects of distributional shifts in machine learning algorithms. However, DRO often leads to low-confidence predictions, poor parameter estimations, and limited generalization.

The authors analyze a possible cause of over-pessimism in DRO, which is the excessive focus on noisy samples. To mitigate the impact of noise, they incorporate data geometry into calibration terms in DRO, resulting in GCDRO specifically designed for regression tasks. They demonstrate that their risk objective aligns with the concept of Helmholtz free energy in statistical physics, and this free-energy-based risk can be extended to standard DRO methods.

To optimize the GCDRO objective, the authors leverage gradient flow in the Wasserstein space and develop an approximate minimax optimization algorithm. This algorithm guarantees a bounded error ratio and standard convergence rate. The authors further explain how their approach alleviates the effects of noisy samples in the optimization process.

To validate the effectiveness of GCDRO, the authors conduct comprehensive experiments. These experiments demonstrate that GCDRO outperforms conventional DRO methods in terms of prediction accuracy, parameter estimation, and generalization performance.

Overall, the proposed GCDRO approach addresses the over-pessimism issue in DRO by incorporating data geometry into the calibration terms. The authors provide a theoretical analysis and empirical evidence to support the superiority of GCDRO over conventional DRO methods, showing its potential for improving the robustness and performance of machine learning algorithms in the face of distributional shifts and noisy samples.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Novel Approach: The paper introduces a novel approach called Geometry-Calibrated Distributionally Robust Optimization (GCDRO) to address the over-pessimism issue in traditional Distributionally Robust Optimization (DRO) methods. This new approach incorporates data geometry into calibration terms, offering a unique perspective on mitigating the impact of noisy samples and improving the performance of machine learning algorithms.

Theoretical Analysis: The paper provides a theoretical analysis of the over-pessimism issue in DRO and establishes a connection between the risk objective in GCDRO and the Helmholtz free energy in statistical physics. This theoretical analysis deepens the understanding of the problem and the proposed solution, providing a solid foundation for the proposed method.

Optimization Algorithm: The paper leverages gradient flow in Wasserstein space to develop an approximate minimax optimization algorithm for GCDRO. This algorithm guarantees a bounded error ratio and standard convergence rate, providing a reliable and efficient optimization framework for GCDRO.

Empirical Evaluation: The paper includes comprehensive experiments to evaluate the performance of GCDRO compared to conventional DRO methods. The experimental results demonstrate the superiority of GCDRO in terms of prediction accuracy, parameter estimation, and generalization performance. The empirical evaluation strengthens the claims made in the paper and highlights the practical benefits of the proposed approach.

Practical Significance: The paper addresses an important problem in machine learning—dealing with distributional shifts and noisy samples—and offers a practical solution that can improve the robustness and generalization of machine learning algorithms. The proposed GCDRO method has the potential to be applied in real-world scenarios where distributional shifts are prevalent, making it highly relevant and valuable.

Weaknesses:
Limited Comparison: The paper compares GCDRO only with conventional DRO methods, without exploring a broader range of state-of-the-art approaches or alternative methods for addressing the over-pessimism issue in DRO. Including a more diverse set of baselines would provide a more comprehensive evaluation and better contextualize the performance of GCDRO.

Lack of Real-world Applications: The paper focuses on theoretical analysis and empirical evaluations using synthetic or benchmark datasets. However, the absence of real-world applications or case studies limits the understanding of how GCDRO would perform in practical scenarios. Extending the evaluation to real-world datasets and applications would enhance the applicability and relevance of the proposed method.



Limitations:
See weaknesses

Rating:
7

Confidence:
1

REVIEW 
Summary:
This work investigates the impact of noisy samples on Distributionally Robust Optimization (DRO) algorithms. Using a simple model and empirically, they show how DRO variants tend to give to much importance to those samples, which make them overly pessimistic in finding the worst case distribution shift. To solve this problem they propose a new method called Geometrically Calibrated DRO (GCDRO), which adds two new calibration terms. One of this term is looking at the relationship between samples (provided by a graph $G_N$) and penalizes shifted distributions $\mathbf{q}$ which assign high probability mass to connected samples when those have very different prediction losses.The other entropy term favors shifted distributions $\mathbf{q}$ with larger entropy, penalizing $\mathbf{q}$ which overly focus on a subset of samples. The authors provide an algorithm to approximately solve the inner maximization loop which they use to apply their algorithms to several benchmarks in which GCDRO is outperforming prior methods. Moreover, the authors draw a parallel between their method and Helmholtz free energy.   

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Originality: While the two added calibration terms are not entirely novel on their own, I find some novelty in combining them and (i) demonstrating a parallel with Helmholtz free energy, as well as (ii) showing how those can tackle the specific problem of noisy samples in DRO.

Clarity: Albeit dense, the paper is clear enough. I would maybe recommend clarifying early on how to get the graph $G_N$.

Quality & significance: This work is well motivated and I can see it being impactful. Especially, the link between DRO methods and statistical physics could motivate interesting future directions.  

Weaknesses:
The method requires a graph $G_N$, which might not be easy to get. There should be a paragraph on how this graph was obtained for each dataset in the experiments section. In the supplementary, it is mentioned that $k$-NN graphs are used and the authors show that the results do not depends so much on $k$. While $k$-NN graphs might work on simple regression datasets, I doubt they would be efficient on more complex ones. For instance, DORO has been applied to CelebA, I'm not sure a $k$-NN graph would work there. Some more complex regression datasets can be found as part of WILDS [1]. I understand the goal of the paper is not manifold learning, yet better understanding the limitations associated with requiring $G_N$ seems important to grasp how useful the proposed method can be in practice.   

Would proposition 3.3 still hold considering the error obtained from the inner maximization? 

References:

[1] WILDS: A Benchmark of in-the-Wild Distribution Shifts (https://www-cs-faculty.stanford.edu/people/jure/pubs/wilds-icml21.pdf)

Limitations:
The limitations of the method are somewhat discussed in the supplementary. I like that the authors did experiments with several $k$-NN graphs, but I believe this analysis can be more convincing by testing the method on more complex regression datasets. 


Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper introduces a novel approach to tackle the issue of overconservativeness in conventional DRO models. The proposed method incorporates data geometry properties into the design of the objective function and ambiguity set. The authors leverage the discrete geometric Wasserstein distance, initially presented in Chow et al. (2017), as a probability metric to construct the ambiguity set. Additionally, they enhance the model's performance by introducing the graph total variation quantity to the objective function. This modification effectively diminishes the influence of detrimental data points, thus mitigating their impact on the overall optimization process. 
The effectiveness of the proposed approach is extensively validated through experiments conducted on synthetic and real datasets. The results demonstrate its superiority over conventional DRO models, exhibiting improved accuracy and robustness.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
I acknowledge the significance of addressing the challenge posed by noisy examples in machine learning, and I commend the motivation behind the design presented in this paper. The proposed method is intriguing, and the introduction of new calibration terms appears to be a reasonable approach. The experimental results showcase promise, particularly on synthetic datasets, and also demonstrate competitiveness on real datasets.

Weaknesses:
1.The Discrete Geometric Wasserstein Distance restricts the worst-case distribution to have the same support as the training dataset. If this is the case, then the method you propose is fundamentally distinct from Wasserstein DRO. I am interested in understanding the specific scenarios in which Wasserstein DRO outperforms divergence-based methods. As far as I comprehend, GCDRO aims to enhance phi-divergence DRO methods by incorporating graph information. 
2. I kindly request verification of the assumption made in Proposition 3.3, which states that F(\theta) is L-smooth. It appears that the convergence results were obtained based on this assumption alone.

Limitations:
NA

Rating:
5

Confidence:
5

";0
tLrkjK128n;"REVIEW 
Summary:
This paper addresses the problem of active exploration of a (Markovian) dynamical system with continuous states and actions. In the absence of any cost function, the proposed objective is the maximization of the one-step information gain on the dynamics model. The paper presents an algorithm, called OpAx, to maximize the introduced objective by alternating optimistic planning with the current estimate of the dynamics model and data collection with the resulting policy to improve the model estimate. Then, the paper provides a convergence analysis of OpAx, which provably converge to the true model asymptotically under various assumptions. Finally, the OpAx algorithm is evaluated against some relevant baselines in continuous control tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper tackles an active exploration problem for system identification that is of general interest to both the reinforcement learning and optimal control communities;
- The algorithm presented in the paper is simple and intuitively sound;
- The paper combines theoretical justification with empirical validation in challenging domains;
- The paper is well-presented and looks rigorous in the theoretical statements.

Weaknesses:
- The paper is mainly motivated as a tool for system identification that can be used in preparation to solve RL or control tasks, but this motivation looks somewhat weak for the RL side;
- The experimental results are not terrible, but I can hardly see improvements over simpler baselines, such as one using planning with the average model instead of optimism;
- The paper neglects several related works in RL, especially reward-free RL and active model estimation.

Limitations:
The paper does not explicitly discuss the limitations of this work in terms of empirical results or theoretical guarantees.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper presents an active exploration method (OPAX) for dynamics model learning. The method seeks to maximize information gain, while being optimistic about unknown dynamics with respect to the achievable information gain. Theoretical results establish a connection between information gain and model complexity, and a convergence guarantee for GP models. Experimental results compare OPAX to baselines in several control and manipulation domains.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
To my knowledge, the paper's principled approach to derive the planning objective in eqs. (6) and (7) from the optimal design perspective is novel. The paper is carefully written and easy to follow. The experiments are relevant and the selection of baselines seems fair. I believe that improving the data efficiency of learning dynamics models for zero-shot task generalization is a relevant research objective.

Weaknesses:
The approach does not really outperform the relatively the baseline methods used in the experiments. I don't think that's a huge problem, as I believe the main contribution of the paper is its theoretical part. Still, I think the paper could be improved by explaining better why the optimism does not lead to a significant difference in model performance (OPAX vs PETS-AE and Mean-AE).

Limitations:
There is no detailed discussion of limitations anywhere in the paper. I think adding this would improve the paper.


Rating:
5

Confidence:
2

REVIEW 
Summary:
The paper presents some insights into active exploration for model-based reinforcement learning. For certain kinds of environments, the authors show a convergence guarantee for model uncertainty. They augment their analysis with an empirical study of an agent using their approach OpAX.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The authors point out an interesting research direction. They motivate their approach well and built up almost all of their mathematical framework. The empirical study manages to give a broad picture of the approach's performance.

Weaknesses:
The empirical part of this paper is connected only weakly to the theoretical part. Where are the proven convergence properties in the study? Picking domains where edge cases of the convergence properties might be observed, would help the paper. The authors state that the approach they show in the empirical study is very similar to other approaches found in literature, but they show no direct comparison. Why is the empirical study part of this paper, then?

The authors' interpretation of the results is somewhat lavish. The performance of the main approach is very similar to other shown approaches and clear advantage is not shown. The comparison on downstream tasks is important to stress the overall benefit of intrinsic reward, but also to be expected. It is not discussed how the competitiveness or even advantage of the said approaches arises for the ""high-dimensional task"" (Fig. 4). Is there no price to pay for the more involved approach? Why are the baselines shown without training times? At some point, the authors argue that their approach is not limited to domains where simple assumption of physicality might help. But why are all tested domains strictly physical, then?

Errors:
- throughout the paper: ""c.f."" --> ""cf.""
- line 65: ""since"" --> ""when""
- line 133: ""since"" --> ""since,"" (add comma)
- lines 168ff switch to $f^\star$  from $f^*$
- line 203: ""Theorem 1;"" --> ""Theorem 1:"" (use colon)
- line 212: ""RKHS, however""--> ""RHKS; however,"" (use semicolon and comma)
- line 222f: ""deep mind"" --> ""DeepMind"" 

Limitations:
The authors discuss the approach's limitations quite well.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies provable exploration in model-based reinforcement learning and proposes an algorithm with optimistic active exploration based on information gain. Theoretical results and experimental results are provided to support their method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper presents a practical algorithmic implementation of active exploration with optimism and information gain, by incorporating the techniques from Curi et al. [1] to introduce a hallucinate policy.
2. The authors provide thorough theoretical and experimental analysis.
3. The paper is well written and easy to follow.

[1] Sebastian Curi et al. Efficient Model-Based Reinforcement Learning through Optimistic Policy Search and Planning.

Weaknesses:
1. My biggest concern is the novelty of this paper. Provable exploration based on optimism (mutual information or uncertainty) is not novel. Although the proposed algorithm provides an efficient way for practical implementation, which is good, the novelty is still limited compared to H-UCRL, which is also an optimism-based MBRL algorithm with the same hallucinate policy technique.
2. The authors claim that their method can achieve better zero-shot performance compared to baselines. But it is not clear why is this. Can the authors explain in more detail? The current theory does not indicate this result.
3. What makes the algorithm better in terms of generalization ability compared to H-UCRL? They seem to be developed from very similar optimism perspectives with the same hallucinate technique. 
4. From the experimental results, it seems that H-UCRL outperforms the proposed algorithm in most training tasks, despite that the proposed algorithm generalizes better (again, more explanation needed). Can the authors comment on this?
5. Can the authors provide the training curves of H-UCRL and CEE-US instead of asymptotic performance?

Limitations:
More discussions on related works and discussions on the zero-shot generalizability are needed to better characterize the contribution of this paper.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a task-agnostic active exploration algorithm for non-linear dynamic systems as long as it can be well calibrated. By combining the optimistic exploration principle and some standard baysian techinques, they give a general convergence bound as well as the more specfic bound in gaussian process case. Besides the theoretical proofs, they also provide several downstream experiments, showing their algorihthm can achieve better performance.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper gives the first theoretical gaurantees on general non-linear dynamic models, which is significant.
2. The bayesian-based framework is somewhat novel and can be extended in many cases. 
3. Their theoretical analysis are solid. Their results on guassian process model helps reader to further understand this problem.
4. I am not familiar with control experiments, but according to what stated in their paper, their approach give nontrivial improvements.

Weaknesses:
1. There is no discussion on the computational complexity. For eqn.(7), it is unclear to me how to efficiently solve $\pi$, $\eta$ when the policy space is large or the $T$ is large.
2. The techinque itself is not very surprising to me. Exploration by optimism is a widely used in all the RL related paper, and the estimation on confidence bound seems to me just a standard derivation from exsiting techiniques.
3. There is no dicussion on the lower-bound, therefore it is hard for me to understand how good this result is.  

But I am willing to raise my scores if my questions are addressed, or there are some techinical difficulty I am missing, or someone else want to advocate contribution of the experiment results.

Limitations:
NA

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes and studies a rather intuitive algorithm for active learning in nonlinear dynamical systems in the episodic setting. They establish consistency (in terms of mutual information) and provide supporting numerical experiments. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
* The proposed algorithm is intuitive and it is satisfying that it ""works"" (in terms of consistency).

* The paper is generally quite well-written and I did not have (m)any issues in terms of clarity and level of writing.

* The exact setting is relatively novel and well-motivated (some caveats below) and the question is interesting and definitely deserves further study.

* The experiments look relatively thorough (but I am not the best person to judge their significance).


Weaknesses:

* While I am sympathetic to the fact it can be hard to keep track of everything appearing in ML conferences,  unfortunately the manuscript does miss some of the most closely related recent references especially when it comes to learning in nonlinear dynamical systems or general mixing processes. My concern here is that missing these makes the contribution of the present paper appear larger than it actually is, since the below references [A,B,C,D] also treat learning in rather general dynamical systems/time-series.  In this light, the last part of the stated contributions (cf. line 52) are  somewhat misleading as it states that related work is not general enough to treat the present system dynamics. However, [A,B,C,D] are actually general enough (or at least very close to general enough) and the authors might want to complement their references  listed starting from line 50 (which certainly aren't the most general setups considered in the recent literature). In particular [B,C] also explicitly study RKHS-like dynamical systems.


[A] Roy, Abhishek, Krishnakumar Balasubramanian, and Murat A. Erdogdu. ""On empirical risk minimization with dependent and heavy-tailed data."" Advances in Neural Information Processing Systems 34 (2021): 8913-8926.

[B] Ziemann, Ingvar M., Henrik Sandberg, and Nikolai Matni. ""Single trajectory nonparametric learning of nonlinear dynamics."" conference on Learning Theory. PMLR, 2022.

[C] Ziemann, Ingvar, and Stephen Tu. ""Learning with little mixing."" Advances in Neural Information Processing Systems, 2022.

[D] Li, Yingcong, et al. ""Transformers as algorithms: Generalization and stability in in-context learning."" International Conference on Machine Learning. 2023.

* As the above references do not treat the active learning setting, this limitation can easily be overcome by including the above references and making the qualifying distinction that the contributions are not ""first in system identification/supervised learning of nonlinear systems"" but rather first in terms of experiment design/active learning.

* The exponential dependence on the horizon in the main results appears overly pessimistic to me and I question whether the the bound is informative at all in any meaningful setting. While convergence is not guaranteed in terms of MI the above references achieve convergence even from a single trajectory, at least indicating at a glance that such a dependence ought to be removable. This exponential dependence on the horizon in the provided bounds is a major caveat here---I have therefore estimated the theoretical results to be asymptotic consistency results and not bona fide finite sample guarantees.


Limitations:
N/A.

Rating:
6

Confidence:
4

";1
g8S53BmXE6;"REVIEW 
Summary:
The paper proposes a strategy of predicting heterodimers with ESMFold. 

Chains of heterodimers are linked by glycine linkers and inputed to esm. The output representations are then added with a learnable embedding layer, and then folded with the folding module in ESMFold. The finetuning is only done for the learnable embedding, with a weighted distogram loss.

Evaluations are done on 3 datasets, while one of them should be considered pointless. Some elevation is gained by the method, compared with directly using links.

Soundness:
1

Presentation:
3

Contribution:
2

Strengths:
1. The writing of the paper is clear.

2. The proposed method is reasonable and intuitive.

3. The method achieves comparable performances with other linker-based hacking strategies. Slight elevation of performances is gained (TMscore 0.62->0.65, ~0.03) from finetuning compared with directly using linkers.

Weaknesses:
Method:

3. The proposed idea, i.e. exploiting ESM models to predict protein complexes by architecture adjusting and finetuning has already been explored previously [1]. Seems that their implementations are more ""neat"": no external linkers are involved; permutation invariances are regarded; they solve complexes with arbitrary numbers of chains, both homomers and heteromers; and the performance elevation is more significant (TMscore 0.27->0.66, ~0.39 in their benchmark). However, no discussion let alone comparison is shown in this paper.

4. In fact, I don't know why linkers are needed to fold multimers at all: simply modifying the relative position indices suffices to tell the model that the residues are from separate chains. Involving linkers will implicitly pose in the model a geometrical constraint on the C-terminal of chain A and N-terminal of chain B (as they'll have relative position of +-L). Also the running time can be (slightly) added. Also, I don't personally like the saying that connects the linker idea to ""prompts"": they are totally different things. Doing so is more of attempting to ride the wave of LLM.

Evaluation:

5. The elevation of performances is in all sense too marginal. The results simply tell me that both ESMFold-linker and the proposed method are not reliable (avg DockQ of 0.11/0.17), instead of telling me that the proposed method is useful. In this case, maybe the percentage of success is a better metric to show.

6. The VH-VL docking benchmark is totally pointless and lacks commonsense. One who has basic senses of the domain knows that all protein folding efforts on antibodies should focus on Ab-Ag instead of VH-VL, because all interaction modes between VH-VL are the same, i.e. they fold almost identically (In table 2, as one can expect, all TM-scores are above 0.92). Therefore, they shouldn't be used as heterodimer folding or protein docking benchmarks. Even if one focuses on the structures of VH-VL, the metrics on the CDR loops should be independently reported, rather than global RMSD. 

[1] Zhu et al, Uni-Fold MuSSe: De Novo Protein Complex Prediction with Protein Language Models. https://www.biorxiv.org/content/10.1101/2023.02.14.528571v1.full.pdf

Limitations:
limitations are addressed in section 6.

The suggestion would be 

11. More solid benchmarks.

12. Discussions and comparisons with [1].



Rating:
3

Confidence:
5

REVIEW 
Summary:
In this paper, the author applies prompt tuning in the heterodimeric protein prediction task. Instead of using the poly-Glycine linker, this method automatically finds the best linker in the continuous space. The author compares this method with several existing methods including the current start-of-art algorithm AF-multimer, the best PLM-based algorithm ESMFold-Linker, and the rigid docking algorithm HDOCK. The performance shows this method which is PLM-based is better than ESMFold-Linker but worse than AF-multimer.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The novelty and contribution of the paper mentioned in the paper are clear and correct.
2. The motivation behind the method and its design are well-founded and logical.
3. Overall, the writing is great.

Weaknesses:
1. Based on my understanding, the methodology seems to be limited in its applicability to the pre-trained language model-based protein structure prediction method, which is not considered the most accurate algorithm for protein structure prediction. Furthermore, the performance of this methodology appears to be influenced by the linker's position and the location suggested in the paper is not the PLM part, raising concerns about its compatibility with other PLM-based methods like Omegafold. Consequently, the paper's value may diminish if there are more robust folding algorithms available.
2. Given that all other methods are unsupervised, there is a possibility of the method benefiting from overfitting. Regarding the antibody dataset, from my understanding, antibodies generally exhibit a rigid overall structure except for the six CDR loops. Consequently, I suspect that the flexible CDR loop will not lead to significant variations in the docking of the light chain and heavy chain. Unless supported by evidence, I prefer to believe that the results obtained from the antibody dataset are not reliable and it may not be an appropriate dataset for this task. As for the Heterodimer test, while a 40% threshold seems acceptable, I believe setting a lower threshold, preferably below 30%, would be better.
3. In comparison to the state-of-the-art method AF-multimer, this method exhibits a significant decrease in performance. When people have the opportunity to utilize a substantial number of CPUs for preparing MSA information, the speed advantage offered by this method does not compensate for its accuracy limitations. Moreover, as far as I understand, there aren't a lot of downstream tasks that necessitate high-throughput calculations.

Limitations:
The limitation is well addressed and there is no potential negative societal impact of their work.

Rating:
4

Confidence:
4

REVIEW 
Summary:
Inspired by the prompt tuning technique used in the field of NLP, the authors leverage the prompt tuning to adapt the single-chain pre-trained ESMFold for heterodimer protein structure prediction. To be specific, a learnable soft prompt is placed between protein chains. With such links, the pre-trained ESMFold treats complex structure prediction as the monomer structure prediction, and the prompt is tuned on the heterodimer dataset.

They show model with such a trick can outperform ESMFold-Linker baseline by large margins on both contact and structure prediction tasks on the heterodimer test set.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1 Leveraging the prompt tuning idea in multimer structure prediction using a single-chain pre-trained model is quite novel and interesting.

2. The proposed trick does improve the performance of the ESMFOLD-link baseline. The trick can potentially be applied to different single-chain pre-trained protein structure prediction models and can be considered a general trick.



Weaknesses:
1. The link-tuning idea is only validated on ESMFold. I'm curious about its effectiveness when applied to other protein structure prediction (PSP) models, e.g., Alphafold, and Omegafold. Prompt tuning is a general trick in NLP. Justifying the generalizability of the link-tuning trick on different PSP models can definitely make the manuscript stronger.

2. The idea of Linker-tuning is simple and effective, but I feel the current manuscript is not informative enough to be published on NeurIPS conference. If the authors can justify the generalizability of the link-tuning trick, I'm willing to adjust my score.

Limitations:
NA

Rating:
5

Confidence:
5

REVIEW 
Summary:
This work predicts the structure of heterodimeric protein chains by optimizing poly-G linkers that connect two chains of a heterodimer. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- This research, compared to other existing deep learning methods, finds an alternative to protein complex prediction methods, which is to make use of poly-G linkers. The idea connects closely to many biological applications and thus has more potential impacts on wider communities. 
- The evaluation metrics cover various aspects to comprehensively assess the model's performance. 

Weaknesses:
- ProteinMPNN as another famous multimeric structure prediction tool should have been compared.
- It seems very often the proposed method does not achieve the optimal performance (in Table 1 and Table 2).


Limitations:
No potential negative societal impacts were discussed in the main text. 

Rating:
4

Confidence:
4

";0
UZTpkfw0aC;"REVIEW 
Summary:
This paper presents a tissue specific transformer based splicing prediction model, TrASPr along with a Bayesian Optimization algorithm, BOS, capable of designing RNA with desired properties. The authors start by demonstrating the performance of TrASPr on RNA splicing data from both mouse and human tissues. Next, the authors assess the model’s ability to detect condition specific regulatory elements using ENCODE data involving three RBP Knockdown (KD) in two human cell lines, and data for tissue-specific regulatory elements from a mini-gene reporter assay. At last, TrASPr is used as an Oracle for BOS to generate AS event sequences with desired properties and an evaluation of BOS performance is presented.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
[-] Originality; This paper utilizes recent advances in large language models (LLMs) to define a splicing prediction model. While this is not the first attempt to use LLMs for nucleotide encoding, the authors improved existing models and incorporated existing prior knowledge through dedicated features. Furthermore, the authors use the model as an oracle for a generative model for RNA splicing design. 

[-] Quality; The paper is well-written and presented. The framework seems well thought through; combining both expert prior knowledge regarding the problems tackled and state-of-the-art computational models. 

[-] Clarity; The paper is self-inclusive, presenting the reader with all necessary information from the background regarding the biological problem, its complexity, and motivation regarding its importance. Following that, all framework parts are clearly presented.

[-] Significance; This work provides promising results towards utilizing LLMs for predicting splicing events and further using such these to train an RNA design model. While this work is only the initial step towards obtaining a robust, reliable framework that solves this task it is of great significance as it advances the field. 


Weaknesses:
[-] Reproducibility code; the authors claim for reproducibility however no code was provided. Providing the code could improve the understanding and evaluation of the presented framework.

[-] Structure; the paper is generally well structured in providing all necessary information however the organization could be improved. For example, the background section already contains model implementation details and prior attempts are split between the introduction and related work. In line with the latter, it would be beneficial if the authors could provide a more elaborate description of previous work, specifically for the prediction task. 

[-] Evaluation; the author’s explanation regarding the degradation in performance over the “Strict” test set is not convincing, and following the rationale behind the necessity of the “strict” test set raises some concerns. It would be insightful if the authors could look more into this point, potentially testing on alternative data to obtain a better understanding of this. 

[-] minor (these are provided to improve the manuscript’s readability);

[--] incomplete/unreadable sentences; a few sentences in the text are incomplete or unclear (e.g. line 78, lines 256-259)

[--] Space after TrASPr is omitted in many places in the manuscript (probably as a result of latex macro).

[--] Figure 1; Figure 1a is never referenced (and its components are hence not explained), the relationship between 1b-1c could be depicted better (in line with many transformer visualization models). 


Limitations:
Yes, limitations have been addressed.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors develop a new framework to predict alternative splicing of RNA. They then deploy it with adaptations and Bayesian Optimization to design new sequences.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
I find the validation using the RBP KD experiment interesting. It is great that the knowledge of the biological system can be used to inform your computational experiments.

“We repeated this process 5 times with different random mutations and the prediction results where then averaged and compared to the wild type (WT) sequence prediction.“ - Good to do lots of permutations!

Good to try to remove batch effects with ENCODE data, but going to be hard.

Levenshtein distance between designed sequences is good!

Figure 2 comparison to Pangolin is pretty good and convincing


Weaknesses:
Table 1 results of rAUPRC and AUROC are confusing. Can the “feature” and “Model” terms be a bit better defined?

Figure 4b is a bit confusing, and I feel like we need a bit more context. Should things be above or below the line? Can we have a legend for the figure as well?

BO part of paper barely discussed, required changes to core algorithm, and not validated–I would remove. Moreover, the baseline algorithm to compare BO (random mutation) is not a sufficient baseline. What about evolutionary strategies? What about Gibbs sampling?


Limitations:
It is unclear how much the BO section is needed. With a sufficient predictor, do evolutionary strategies work?

“Specifically, we only…” Background section is not complete…sort of just trails off.


Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose a new machine learning framework called TrASPr, which is a transformer-based architecture with pretrained RNA language models that is tailored for the prediction and design task of RNA alternative splicing. The authors demonstrate that TrASPr accurately predicts tissue-specific `percent spliced in’ (PSI) scores, and the trained model can facilitate RNA design for specific RNA splicing outcomes. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Overall, the authors introduce and explain the problem of alternative splicing, its significance, and the dataset they used to study this problem quite well for a general reader at a machine learning conference.
- The evaluation setting is generally rigorous, as the authors carefully curated the test set to avoid any information leakage.


Weaknesses:
- Some additional work and its relationship to this research should be discussed, such as ""RNA Alternative Splicing Prediction with Discrete Compositional Energy Network,"" which also focuses on the prediction of PSI scores in a tissue-specific setting.
- When evaluating the effect of RBP KD and mutations, the authors first identify a set of RBP motif matching sites that might affect alternative splicing and then check if their model can accurately predict those sites through in-silico mutations. However, this measurement only assesses the model's ability to recover positive samples. The authors should also evaluate and present examples of in-silico mutations on non-regulating motif matches and demonstrate their models' predictions. This is important to show that the model is genuinely learning context-dependent sequence features and not just memorizing motif matches.
- While the formulation of the RNA design problem and the utilization of language models for RNA sequences can be considered novel in the field of RNA splicing, similar techniques have been introduced and used in protein sequence design and protein language models before; it would be nice to discuss some of those (e.g., for a review, see https://doi.org/10.1016/j.cels.2021.05.017), perhaps in Related work.


Limitations:
The authors discussed the limitations of the noisy labels obtained from biological experiments and concluded that this work ""should be viewed more as a proof-of-concept outlining exciting directions for future developments rather than a finished product."" It would be helpful if the authors could comment on how many datasets exist and are expected to be generated, and whether these limitations would be addressed with more data or more careful model design. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper tackles two tasks in alternative splicing of pre-mRNA, where multiple unique mRNAs are produced by including different segments. First, the authors proposed a transformer-based splicing prediction model, TrASPr. A 6-layer transformer model is pre-trained with 1.5M pre-mRNA sequences centered in splice sites. TrASPr utilizes multiple pre-trained transformer encoders for the splicing prediction in a tissue-specific manner. Second, the authors used TrASPr as an Oracle to train and evaluate splicing sequence design based on the Bayesian Optimization algorithm. Through the experiment results, they argue that TrASPr significantly outperforms state-of-the-art models and BOS can generate sequences with desired characteristics.

Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
-	The authors tackle important problems in RNA biology. In particular, they argue that the sequence design for RNA splicing is a novel task and it can benefit therapeutics for correcting splicing defects and synthetic biology.
-	To tackle the problems, they adopt a couple of machine learning methods that have shown successes in other domains: pre-training and fine-tuning of language models for biological sequences, and latent space Bayesian optimization (LSBO) over structured and discrete inputs.

Weaknesses:
Major comments:
- [Originality] While the methods are novel for their first use for RNA alternative splicing, they still seem like mostly direct applications of widely known machine learning methods. For example, pre-training and fine-tuning of language models seem trivial even for the biological sequences. As referenced in the paper, DNABERT proposed a pre-trained language model for DNAs. There are also plenty of previous works that use pre-trained language models for various protein biology tasks.
- [Quality] While the paper includes a couple of experiment results, I do not think they are sufficient to verify the effectiveness of the proposed methods. It lacks strong baseline models for comparison and ablation studies for thoroughly understanding the proposed methods.
- [Significance] The paper does not bring significant and novel ideas that would be valuable to the broader NeurIPS community. 
- [Clarity] I don’t think this is the best version of the paper, considering the broad NeurIPS community is not familiar with computational biology. The explanations are not detailed enough to easily understand the problem and significance of the experiment results. 

Minor comments:
- Typo in L10: on Oracle -> an Oracle
- Sec 2.1: Incomplete. It suddenly ends with “Specifically, we only.”
- Sec 7: The authors mostly use the Discussion section for summarizing their contributions rather than discussing notable observations, limitations, and future directions. (except for the last paragraph where they discuss the inherent limitation of experiment data)

Limitations:
The authors adequately addressed that the experiment data are inherently noisy and limited in number, such that this work should be viewed more as a proof-of-concept rather than a finished product. 


---Post-Rebuttal Comments---
Overall, I am inclined to believe that incorporating the authors' responses would indeed enhance the manuscript's quality. Consequently, I have adjusted my rating from 3 to 4. Upon reviewing the comments from other reviewers and the authors' clarifications, it seems I'm not the only one who has struggled to understand the authors' contributions and has concerns about the presentation, especially regarding the BO for sequence design. This suggests that substantial revisions are needed beyond the inclusion of additional experimental results. Although the authors' responses have been comprehensive, I could not give a higher score as I respectfully believe resubmission after revision is more appropriate for this manuscript.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The authors propose two approaches to deal with the problems of alternative splicing (AS).  A transformer architecture-based tissue-specific splicing code model, TrASPr, and a Bayesian Optimization algorithm were performed on the latent variable space of VAE to address the design of RNA sequences with specific splicing characteristics.  The architecture is not so novel, but applying LLM to the AS is worth noting.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The proposed methods are exciting and computationally novel.  Using BERT with masking to pre-train the model was a nice touch.  Using VAE with Bayesian Optimization is interesting.

Weaknesses:
The proposed method is a nice modeling experiment exploring using LLM on a novel application. However, it lacks reliability as a tool to deal with biomedical problems that can be used for biological research.   The evaluation the author provides shows that the method fails.  The authors mention that TrASPr significantly outperforms AE-MLP in a particular situation but also point out that the performance degrades based on the filtering criteria.  There also seems to be a performance difference in BOS sequence generation based on the edit distance, a hyperparameter that lay users would not know how to tune on their particular problem.

Minor comment:
Line 78 is incomplete.


Limitations:
The proposed method has a potential to be a hypothesis generation method.  However, it lacks the biological soundness and it is unclear how to pinpoint the problem when they arise.  Authors are very casual about the evaluation and the problems that arise when the hyper parameters are chosen inappropriately.  

Rating:
5

Confidence:
3

";0
Oc1SIKxwdV;"REVIEW 
Summary:
The paper suggests a new method for continual model editing. To deploy a parameter-efficient model editing remedy, the authors introduce an adapter for a target layer, keeping the pre-trained model weights intact yet revising the model prediction correctly. The suggested adapter contains a codebook - key-value pairs and corresponding distance coefficient \epsilon. By training them via a fine-tuning mechanism, they can edit the original prediction accordingly.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The proposed method introduces the interesting idea of continual model editing. The basic idea of codebook updating and introducing new entries is simple yet reasonable. And suggested method outperforms baselines by a significant margin. And the ablation study done in the supplementary file is also good.

Weaknesses:
The most critical limitation is that the choice of editing layer is essential, but it is achieved via heuristic search. Recently released deep neural network architectures provide a depth (layerwise/blockwise) variety according to the scale, and heuristic search to find the best layer to edit is a nontrivial problem. Unfortunately, as shown in the analyses and the limitation section of the main paper, this limitation is the main obstacle to the practical usability of the suggested model. 

Also, most baselines are not targeted to the continual learning setting, so the benefit of the suggested method for continual model editing is a bit difficult to measure fairly. Simple combinations of existing (and recent) continual learning methods with model editing baselines can be better competitors to reveal the merits of GRACE more intuitively. And it will be also great to provide not only model performance like TRR, ERR, or ARR, and the change of additional memory overhead over long model edit sequences. 




Limitations:
Please see the weakness.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors propose GRACE, an approach to model editing that is tailored to many-edit scenarios. GRACE augments a hidden layer of a neural network with a key-value memory that overrides the output representations of the layer when the input representation falls near the stored key. The method is simple and computationally efficient, and compares favorably with existing editors. Closer analysis shows that GRACE ultimately provides a tradeoff between forward generalization of past edits and retention of pre-existing knowledge.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The authors study a clearly-defined and very relevant problem, sequential model editing, which extends the setting of applying one or a small number of model edits to hundreds or thousands, which is much more realistic. The method they describe is simple and novel, and does not include the requirements for training data that some existing methods have. GRACE also leverages the internal representations of the pre-trained model itself to (in theory) enable generalization of the edit information to related inputs (however empirical validation of this capability is mixed). The analysis of the method's memorization and generalization in 4.2.2 is welcome.

While the final results show that there is still a strong tradeoff between the ability to generalize edits forward (i.e., to future queries that ask about similar information to those queries that were already edited) and the retention of pre-existing information, I think the paper is a useful exploration of a straightforward and intuitive approach to sequential editing.

The authors also include a clear discussion of the limitations of their work, which is welcome.

Weaknesses:
The main weakness I see in the paper is the fact that GRACE might do little more than memorize the edits it is exposed to. The analysis in 4.2.2 (which is very welcome!) suggests that GRACE essentially has two regimes, one where we essentially memorize past edits and retain all past knowledge, but are not able to generalize edit information to future, related inputs (editing layers 4 or 6 in the $\epsilon=1.0$ plot) or where we are able to generalize edits to future, related edits, but suffer a (potentially dealbreaking) penalty in terms of the information we retain from the original task. In my view, the tradeoff as shown in the paper is not viable for real-world use cases. However, I still think the paper makes a useful and interesting contribution to model editing/sequential model editing.

One other point about related work:

I think L79 is wrong or misleading; MEND and ROME are only loosely based on PEFT. MEND only requires a single fine-tuning step, and neither ROME nor MEND is more prone to overfit than regular fine-tuning to my knowledge. Whether or not PEFT overfits more than regular fine-tuning isn’t relevant here, IMO.

Limitations:
The authors include a clear discussion of relevant limitations of their work.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper presents a new method for sequential model editing (GRACE). The approach works by modifying the output of a pretrained model's given layer $l$ through the use of an external key : value storage. Whenever an edit is required on input $x$, the model stores the input representation $h^{l-1}$ as key, and learns a value $v$ via gradient descent which yields the edited output $y_{edited}$. In order to enable generalization, the authors propose to map all representations within an adjustable (edit-specific) $\epsilon$ to the stored value $v$. Crucially, to enable sequential editing, the authors propose to update the codebook (Alg. 1) the following way : whenever a new edit overlaps with the $\epsilon$-ball of a previous edit, this $\epsilon$ is either shrunk to reduce the overlap between edits, or expanded if the two edits share the same desired output. 

The authors evaluate their method according to three metrics, which look at whether the sequentially performed edits are memorized, and the model's ability to preserve its original input on non edited points. From the results, it is shown that the proposed method performs across several benchmarks. Finally, the authors perform an model analysis, looking at the impact of $\epsilon$ and the choice of layer to add the GRACE mechanism. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
Originality : 
 - The proposed is, to the best of my knowledge, a novel approach of an external discrete memory for sequential model editing
 - Grace seems like a solution which could actually be deployed in realistic edit scenarios

Quality / Clarity : 
- The paper is well written and easy to follow. 
- The intuition behind how the method works is well-built, and the model analysis section is interesting
- The experimental section is overall well-designed


Weaknesses:
1. Given that the choice of layer and value for $\epsilon$ seems to highly impact the behavior of the deployed system, it's unclear how practitioners would choose this value a priori. Given that this deployed in an online setting, standard cross-validation cannot be used directly. 
2. The experimental section is somewhat limited, as the authors only look at NLP tasks. That being said, given that the authors evaluate different model scales, this is more a suggestion on how to make the paper more complete than a hard weakness. 

Limitations:
limitations properly addressed. 

Rating:
7

Confidence:
2

REVIEW 
Summary:
The authors propose a method to editing a model’s behavior, making corrections or specifying the desired output for a particular input. Such a method is broadly applicable, but is especially relevant in the context of large pre-trained models, which are costly to fine-tune or re-train, and whose data is often not publicly available. Specifically, the authors propose General Retrieval Adaptors for Continual Editing (GRACE), which store activations (keys) for past edits into a codebook; if a particular input is close enough to one of these stored keys, then the output of the layer is overridden by the key’s corresponding value. A deferral radius for each key allows grouping of similar edits and provides some generalization behavior. For experiments, the authors compare with a number of relevant model editing baselines, editing large language models (LLMs) hundreds to thousands of times, which is significantly more than prior evaluations. Empirical comparisons look promising across a variety of editing metrics.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
S1. Relevance: This paper tackles a topic that is especially important given recent developments. Large models trained on massive data (e.g. language, vision) are seeing increased interest and more exposure to the general public with the releases of chat bots/search, image generators, etc. This is largely due to impressive results, but there have also been a concerning number of wrong, offensive, or misleading examples as well. As these models continue to be used by the public, some way to apply targeted patches is essential.

S2. Assumptions: GRACE has relatively simple and realistic model assumptions. In particular, unlike some prior work, it does not require access to the pre-training data; this is important, as such data is often proprietary and massive in scale, as well as a potential data privacy risk.

S3. Experiments: \
a) Realistic settings: While past methods tend to evaluate success of a single or a handful of edits, this paper evaluates on the ability to make hundreds to thousands of edits, sequentially. This is a significantly more realistic and utilitarian setup. \
b) Three text-editing settings: The authors include experiments on multiple settings (context-free question-answering on a T5, BERT classification on SCOTUS dataset, and correction of hallucinations on GPT-3 wiki bios). \
c) GRACE outperforms a good selection of competing methods on a number of editing and retention metrics, demonstrating strong empirical potential.

S4. The paper is well-written and easy to understand. The figures and illustrative examples do an excellent job conveying the key concepts. 


Weaknesses:
W1. Editing examples vs concepts: From my understanding, the proposed method focuses on editing predictions of a model for a specific example. This contrasts with continual learning methods, which tend to focus on learning new concepts or classes. While the editing approach is good for correcting mispredictions or hallucinations, it doesn’t seem like it is capable of learning to make new predictions: for classification, this could represent new categories or new vocabulary.

W2. Scalability: The number of GRACE layers required in the codebook mostly scales linearly (technically sublinear because edits can be grouped with previous GRACE layers, but from Figure 4, the overall behavior is still close to linear—converging to 50% cache rate is still O(n)). In addition to storing an increasing number of GRACE layers, this increasingly large codebook also requires an increased number of comparisons at inference time, for *every* forward pass, even inputs that are correct and otherwise unaffected by the edits. Experiments show up to 5K edits, but real-world deployments may (e.g. chatbot search engines) may require significantly more.

W3. GRACE layers value storing vs override: My understanding is that the models being considered in this work are fully deterministic during inference. Thus, once a particular input triggers a particular GRACE layer anywhere in the model, then the retrieved activation $h^l$ should always result in the same output. This raises a couple questions: \
a) If the output from a GRACE layer deterministically maps to a particular output, then why wouldn’t we instead directly store the desired output and save the computation of the subsequent layers $l+1, …, L$? \
b) This also suggests a much simpler approach: maintain a dictionary of input-output corrections, and check if the input exists in the dictionary before running the model. The primary advantage of GRACE then would be the potential for better generalization, as matching to previous edits is done at the feature level, but this needs to be verified empirically.


Less critical concerns:
- Title: While the title is kind of cute, “Aging” is only partly relevant to the problem being addressed here. Edits are also necessary for examples for what the model gets wrong, even when it’s a “newborn”.
- I think it’s debatable whether the proposed method should be considered a “continual” or “lifelong” learning method, as it’s more or less sidestepping the catastrophic forgetting problem altogether with a roughly linearly increasing codebook of overrides (see W2). With the $\epsilon$ radius, there is some potential for forward or backward transfer, but I would classify it as minimal and relatively rudimentary. On the flip side though, I can see similar arguments being made about some expansion-based CL methods, though generally their scaling characteristics aim to be better.
- I was a little disappointed that there weren’t any experiments with large vision models, but I understand the desire for limiting scope.


Miscellaneous:
- Fig 2: “No near key” => “No nearby key”
- Equation between Lines 139-140: The top line doesn’t quite make sense. I believe the authors meant to write something along the lines of “if $d(h^{l-1}, K_{i_*}) < \epsilon_{i_*}$” instead. Also please include an equation number.
- Line 149: “one of two”?
- Line 191: missing period “.”
- Line 279: possibly a missing space after the “:”

========

Overall Assessment:

I found this paper interesting and well presented; the proposed GRACE layers appear to be a reasonable, straightforward solution to editing. However, I’m concerned about the method’s overall scalability, especially when the method in many ways is similar to keeping a dictionary of input-output overrides (e.g. prefacing the model inference call with simple if-then statements). By operating in feature space, perhaps GRACE has better generalizability than such a rudimentary approach, but there is little empirical evidence of this. Also, it still isn’t clear to me why values are activations for the next layer as opposed to just jumping straight to the output. I’m willing to raise my score if my concerns are sufficiently answered.


Limitations:
The limitations section is well-written and frankly acknowledges some valid limitations and potential ethical concerns. While important, I don’t necessarily see these as blocking acceptance of the paper.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes GRACE, a method towards lifelong editing of models through the use of a codebook, a mechanism to write to/make use of the codebook without perturbing model weights. In essence GRACE can be viewed as an inserted adapter layer, which caches activations as keys, fine tunes values that the keys map to, and employs a so called deferral radius per key to decide when to activate GRACE.   

---- 
Rating raised post-rebuttal from 6 to 7, as weaknesses were at least in parts clarified or addressed. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper has many strengths, starting from a well-motivated introduction and scope that tackles a highly important problem. 

In general, the exposition of the paper is clear and it is well written and structured. The mechanisms are mostly explained in sufficient detailed and GRACE itself becomes intuitive through the added illustrations. 

The empirical investigation further investigates a set of metrics and includes a set of meaningful benchmarks. Within the given set of explored alternatives, GRACE can be viewed to provide consistent advantages, making GRACE a good contender for this kind of problem setting.  

The addition of a limitations section with some statements on potential (harmful) social impact is also appreciated. 

Weaknesses:
The paper seems to have two primary weaknesses. 

One lies in the exposition with respect to related work and its empirical comparison, the other is a potential concern with GRACE’s practical hyper parameters. 

With regard to related work, the empirical comparison to continual learning works seems interesting and it is not fully clear why it is meaningful. If one were to take a look at one of the plethora of continual learning reviews (take Hadsell et al, Mundt et al, de Lange et al), they typically categorize techniques into rehearsal (memory buffers), adding components to the architecture or regularizing parameters. The comparison in this paper primarily seems to compare to fine tuning and EWC, which fall into the regularization category. Yet, to the best of my understanding, GRACE implies the use of extra parameters and memory, which would in that sense be much fairer to compare to continual learning methods to do something similar. There are many of those, but think of storing attention masks, adaptively learned gates, or memory buffers that similarly store activations. It is presently not clear how GRACE differs conceptually from these prior arts (mentioned in e.g. the above surveys) and following in this line, what the “cost” of GRACE is with respect to growth/memory of the codebook. 

As mentioned above, the second concern lies in the choice of parameters, specifically how to pick appropriate values for epsilon and how to decide which layer to edit. From figure 4 it seem the system is not very robust to this choice necessarily and it becomes a trade-off with respect to what value is chosen for which layer. Whereas this may be clear, it will be important to clarify how to pick suitable hyper-parameters without being able to look into the future in lifelong editing (i.e. plotting figure 4 and deciding on how the curve develops on the x-axis); also see below question.  

Limitations:
Limitations are mentioned appropriately and are appreciated. Divining deeper into the mentioned limitation of slowing down inference through similarity computation, along with potential other limitations on storage cost and codebook overhead, will be a further improvement to the paper and its limitations section. 

Rating:
7

Confidence:
3

";1
Dt71xKyabn;"REVIEW 
Summary:
In this manuscript, the authors proposed a special curvature-related graph descriptors for the evaluation of graph generative model. The key idea is to evaluate the discrete curvatures on graphs and construct a curvature-based filtration process. The corresponding persistent Landscape based metric is used for the evaluation of graphs. This metric is robust, stable, and expressive. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
A unique curvature-based TDA model is developed and used for the first time for the evaluation of graph generative model. The developed curvature-based metric is robust, stable, and expressive. 

Weaknesses:
The advantage to use curvature as filtration parameter is not clear. The advantage of the special filtration process and final vectorization approach is not clear. 

Limitations:
More discussions and explanations are needed to fully demonstrate the advantage of their model. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes a new set of graph features: first, 3 variants of graph curvature are calculated. Second, persistence landscapes are calculated over edge-based filtrations via curvature. Third, sets of graphs are compared via comparing their landscapes.
Theoretical properties of landscapes (stability, expressivity) are studied. Experiments for both synthetic and real graph sets are provided.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The research is original, afaik, since only few papers are dedicated to the developpement of new *measures* to evaluate graph generative models. However, the problem itself is an important one, having applications in molecular design.
The manuscript presents new theoretical results: stability w.r.t. changes in graph structure and expressivity of landscapes vs. raw filtrations (the results concern graph curvature). 
Experiments are quite diverse. 
The paper is well written and easy to follow.

Weaknesses:
1. While the paper provides versatile approaches to comparison of graph datasets, the single end-to-end algorithm to evaluation of graph generative models and corresponding score is not formulated. I expected to find something like FID [1] for graphs. But in the end, you have a set of persistence landscapes, not one number. These landscapes should be aggregated somehow? 
2. No experiments with real graph generative models.

[1] Heusel, M., Ramsauer, H., Unterthiner, T., Nessler, B., & Hochreiter, S. (2017). Gans trained by a two time-scale update rule converge to a local nash equilibrium. Advances in neural information processing systems, 30.

Limitations:
Authors adequately addressed the limitations and potential negative societal impact of their work.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper proposed a TDA-based graph generation model evaluation method using the discrete Ricci curvature of the graph as a filtration function. The proposed comparison between the two graphs is theoretically shown to have good properties for graph analysis, and the comparison is verified in experiments.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
A new way (distance) of comparing  two graphs from a new perspective is proposed, and it is theoretically shown that the properties required for graph comparison are satisfied.

Weaknesses:
- The main concern is the consistency between the issues in the paper and the proposed methodology and validation. Although the issue of the paper is supposed to be a method for evaluating graph generation models, the proposed method is about a measure of the distance between two graphs. The two are related, of course, but there is insufficient mention of comparison methods between graph-generating models and comparison with other comparison methods. If the issue is the importance of defining good distances, the paper should be constructed as such, and if the issue is the evaluation of graph generation models, there should be a full discussion of that.

- In any case, the key to the proposed method is the definition of the distance between two graphs, but most of the comparisons are in the Ricci curvature and TDA domains only, and the distance and similarity between graphs in other domains are not discussed. The following are representative of the distances and similarities between graphs. It is unnatural in light of the scope of NeurIPS that these are not mentioned.
	- Graph edit distance
	- Y. Bai et. al, SimGNN: A Neural Network Approach to Fast Graph Similarity Computation, WSDM '19
	
- This is a non-essential question (with little impact on the evaluation), but why do you use Landscape for comparisons between persisntent diagrams? Many have been proposed such as Wesserstein distance, but is there a reason why Landscape is the best? Or is this an issue for future study?

Limitations:
The authors do not explicitly address Limitation. On the other hand, the study is oriented toward reducing the social impact on the limitations of conventional methods and does not promote adverse effects.

Rating:
5

Confidence:
3

";1
6H8Md75kAw;"REVIEW 
Summary:
This paper proposes using hessian-based updates to solve the unlearning problem for strongly-convex-strongly-concave minimax learners. They also provide theoretical analyses of the error of such an unlearning algorithm as well as its sensitivity to deleted data. 

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. This paper is well-written and structured. The narrative is very easy to follow, and the analysis is strong. 
2. The unlearning update is simple.
3. While in a restricted setting, the analysis is coherent.
4. Motivation is obviously, well done, given the connections to privacy. 

This paper operates in a restricted setting with some small issues with the presentation. The issues with presentation are fixable, and I expect the authors will go through and better annotate their notation for the camera-ready version. Moreover, the setting is restricted and not practical for unlearning in modern settings. However, this paper achieves its goal and is a good step in the right direction. I, therefore, advocate for this paper's acceptance. 

Weaknesses:
1. There is a notational mistake for Equation (5). You are missing a closing parenthesis.
2. In equation 6, you should specify that $m$ is the size of $U$.
3. You miswrote Assumption 2 in Lemma 2.
4. I feel some work is needed on the motivation of the setting of this paper. I will discuss more of this in limitations. However, this paper seems only to work when the loss is strongly-convex-strongly-concave loss functions (or suffers bad constants for transforming in the c,c case). However, unlearning is meant to be a more practical tool for modern learning, where this condition rarely holds. 




Limitations:
Yes, they have sufficiently addressed the limitations of their works. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
Machine unlearning is a privacy-inspired area to remove certain training samples of users’ data from well-trained model making those samples uninfluential while the unlearning approach does not cause the model to be retrained from scratch (not cause comprehensive computational cost) to achieve the baseline relied on the rest of datasets. The conventional machine unlearning usually applies for standard statistical learning models with one variable, but there is limited work for minimax models. Borrowing from Gaussian mechanism of differential privacy, this work also focuses on epsilon-delta certification of machine unlearning. Moreover, generalization rates and deletion capacity (number of deleted samples) are also very crucial to study in this framework.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. Machine unlearning is a quite new field for privacy of machine learning. This work smoothly introduces the field with related work (a lot of citations) and preliminaries. Also it well states the reason minimax model is important for machine unlearning.
2. Certified minimax unlearning can be well extended to more general loss functions with reasonable generalization rates and deletion capacity results.
3. This work supports successive and online deletion requests, which is computationally efficient for unlearning phase with its minimax unlearning update.

Weaknesses:
1. Although this work focuses more on theoretical part of unlearning, it is still better to provide some preliminary results on some datasets. At least in one paragraph of intro or conclusion, authors can discuss scenarios or applications how to make the certified minimax unlearning practical.
2. This work does not discuss the relationships of certified unlearning between STL model and minimax model theoretically (How are they different).

Limitations:
1. As weakness 1, authors can consider some implications for the applications by adding a paragraph. Like in the beginning of the introduction, how does this work help GAN, ARL or RL?
2. As weakness 2, this work emphasizes how important the certified unlearning for minimax model, but this work does not talk too much about the connection to unlearning for STL models.
3. Instead of putting every proof in the appendix, it is better to add one or two sentences for each theorem or lemma about the clue to prove (what methods will you use? How will you prove?) to convince readers who are interested in the topic but not familiar.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The papers proposes a Newton-based differentially private algorithm for stochastic minimax problems and analyse the generalization rate and deletion capacity for the algorithms.
They analyse the generalization bound for weak primal-dual risk in SC-SC, C-SC and SC-C cases. Also the deletion capacity they derive is $O(n/d^{1/4})$, which is better than the baseline result $O(n/d^{1/2})$. The results of generalization bound and deletion capacity match the bound in the pure minimazation case.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The paper is the first one analysing certified machine unlearning for minimax problems and provide certain generalization and deletion capacity guarantee. 

Weaknesses:
There are several weaknesses and issues:
1. The reviewer is not convinced that weak primal-dual risk is enough to capture the behavior of generalization for minimax problems. In recent literature, people begin to study the strong primal-dual risk for minimax problems even without strong convexity.
For example, the authors in ``What is a Good Metric to Study Generalization of Minimax Learners'' derive the result for generalization of strong primal dual  risk in convex-concave setting.
Therefore, the reviewer thinks the paper can be improved if the authors can extend their result to strong PD risk. 
2. Suggestions for writing: The certified machine unlearning and deletion capacity might not be well-known to general audiums. However, in the abstract and begining of the paper, the authors use too many specific terminology for machine unlearning, deletion, etc. It is not friendly to the audiums. The reviewer suggests the authors can discuss more motivation for this paper and give more explanations for the intuition of these terminologies.
3. Missing reference: ''Uniform convergence and generalization for nonconvex stochastic minimax problems'',   ''What is a Good Metric to Study Generalization of Minimax Learners''.

Limitations:
None

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper studies the problem of machine learning for the minimax model. By using Newton's step update with the Hessian information of the leftover data and Gaussian Mechanism, the proposed method improves the deletion capacity from $O(n/d^{1/2})$ to $O(n/d^{1/4})$.                               

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Given the rise of GAN and the increasing concern for the privacy of participants in datasets used by GAN/ML models, minimax unlearning is an important problem to study and understand.

- The proposed algorithm shows improvement in both strongly convex-strongly convex and the more general convex-concave settings.

- The improvement of the deletion capacity from $O(n/d^{1/2})$ to $O(n/d^{1/4})$ could be quite significant for practical problems with high-dimensional features.

- The efficient algorithm that dispenses the need for recomputing some of the Hessian information is a nice touch since computing the Hessian matrix can be very computationally expensive.

- The paper is well-written overall.

Weaknesses:
- Some of the technical details are used before being defined properly. For example, in equation (7), $\kappa$ is defined as $\kappa = l/\mu$ even though $l$ and $\mu$ haven't been defined yet. These quantities are later mentioned in section 4 but it would be better for the readers if those quantities are defined right away.



Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper studies approximate unlearning for minimax problems. They design learning and unlearning procedures and provide bounds on deletion capacity in terms of generalization performance (weak gap). Akin to minimization (statistical learning), the deletion capacity for strongly convex-strongly concave setting, is shown to be, $n/d^{1/4}$, where $n$ is the number of samples and $d$, dimension. The authors also provide extensions to non strongly convex/concave settings and efficient updates.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The problem of machine unlearning has gathered a lot of interest recently, owing to various privacy regulations. Further, the minimax formulation, is widely applicable, especially in robust adversarial learning and reinforcement learning. The paper is the first to study unlearning for minimax settings. Therefore, the topic of the work is natural and timely.

Weaknesses:
1. The paper very closely follows the outline and techniques in prior work of Sekhari et. al. The extensions, to non-strongly convex settings (via regularization) and efficient updates also use techniques directly from prior work. If there are additional challenges due to the minimax (as opposed to min) structure, then I don't think they are communicated well in the write-up. The only relevant section is Section 4.1 ""Intuition for Minimax Unlearning Update"", however, I found it too raw to convey the intuition -- for instance, how does Eqn. 10 follow? In the current state of the write-up, it is difficult to evaluate if there are significant challenges overcome in relation to prior works.

2. Comparison between Section 4.3 and 5.2: It seems to me that both are in the same setting, and achieve the same guarantees, yet Section 5.2 provides a more efficient update. If indeed Section 5.2 is a strict improvement over 4.3, then what is the point of devoting considerable space to the weaker result in Section 4.3.  The authors should re-organize and present the strongest result in the main paper. The space should be used to explain the challenges compared to the minimization setting. If this is not the case, then please explain the differences.

3. Strong gap vs weak gap: The generalization performance considered in the paper is the weak primal-dual gap. In the non-private setting, ""strong"" primal-dual gap (as opposed to weak) is what is typically considered. Further, as explained in Bassily et al. 2023, the strong gap criterion has game-theoretic interpretation and motivation, and moreover, the weak and strong gaps may be arbitrarily apart. Seemingly, the consideration of the weak gap in the paper primarily stems from challenges of studying strong gap under privacy settings, until recently. However, given that the work Bassily et al. 2023 has established optimal rates for strong gap under privacy, can the authors, perhaps borrowing techniques from the aforementioned work, also provide guarantees in terms of the strong gap?

Limitations:
1 The work is limited to the approximate unlearning setting, as opposed to exact unlearning. 
2. The authors study generalization performance in terms of weak gap as opposed to strong gap.

Rating:
6

Confidence:
4

";1
wzg0BsV8rQ;"REVIEW 
Summary:
This paper considers the problem of estimating (heterogeneous) treatment effects via both interventional and observational data. The authors proposed a new estimation, namely the Fused and Accurate Shrinkage Tree (FAST), which optimally weights the interventional and observational estimator, and combines with a new spilt criteria for tree-based heterogeneous treatment effect estimation. The authors further conducted experiments to compare FAST against existing methods.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Apart from a few typos, the paper is well written and ideas are presented in a rigorous but clear also way. 
- The idea of applying shrinkage method to combine interventional and observational data for better estimator is novel and could be a nice addition to the literature.
- Note that I have not gone through all the proofs in the appendix, the mathematical correctness might need further input from other reviewers. 

Weaknesses:


Generally I like this paper, but there are still a few weaknesses.

- The main issue with shrinkage method is interpretability: we need to understand better how the variance-bias trade-off behaves in different regimes, especially the authors takes a more analytic way to first estimate the required quantities, then solve the optimal weights. More specifically for example, it would be beneficial to at least see different how trial mechanisms affects the estimation. For example, in a more realistic setting, one may consider non-randomized trials rather than RCT, in which treatments are assigned by a *known* true model. By adjusting the parameters of such true assignment model, the estimator variance for the trial population HTE estimator can be controlled (even with fixed N). Then the performance of FAST can be evaluated against different variance regimes of trial HTE estimator, which will help us understand the sweat spot of the method.

- Regarding experimental settings. It is indeed quite standard for these type of papers to have 1 or 2 synthetic experiments and 1 real data experiment. However, in the case of this paper I found the simulation setting is a bit weak. It would be great to perform experiments on multiple data generating mechanisms with randomly sampled parameters and coefficients, allowing us to evaluate the marginal performance of the method. Otherwise the authors at most demonstrated the capability of the method on only one single data generation mechanism (which arguably is much easier to hack/cherry-pick).


- The other potential room for improvement is the baseline. I understand that the paper mainly only compares to data fusion methods. However, due to the variance-bias trade-off of the shrinkage method, it would be natural to also expect some comparisons to variance reduction methods for trial estimators as well. 

Limitations:
The authors has somewhat discussed the limitations of the method.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a novel strategy for estimating the heterogeneous treatment effect called the Fused and Accurate Shrinkage Tree (FAST). The authors confirm the consistency of the proposed tree-based estimator and demonstrate the effectiveness of their criterion in reducing prediction error through theoretical analysis. The advantages of the proposed method over existing methods are demonstrated via simulations and real data analysis. As I am not very familiar with this field, it might be better to consider my opinion less.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. This paper is technically sound.
2. The proposed method has better performance than the existing methods.

Weaknesses:
Imcomplete references: The tree-based method seems to not be a new method in this field. There might be some other references as follows.

Agarwal, Abhineet, et al. ""Hierarchical Shrinkage: Improving the accuracy and interpretability of tree-based models."" International Conference on Machine Learning. PMLR, 2022.

Nasseri, Keyan, et al. ""Group Probability-Weighted Tree Sums for Interpretable Modeling of Heterogeneous Data."" arXiv preprint arXiv:2205.15135 (2022).

Limitations:
None

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper deals with the problem of estimating the heterogeneous treatment effects with multiple data sources. In particular, the paper aims to utilize the information from the observational data to better estimate the causal effects in the trial data. Inspired by the shrinkage estimation, a weighting scheme is developed to balance the unbiased estimator based on trial data and the potentially biased estimator based on observational data. Specifically, a tree-based algorithm with new split criterion is proposed based on above motivations. Some theoretical results about the causal effect estimation is derived. Finally, the author provides simulations and real data analysis to demonstrate the performance of the proposed method.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The papers deal with an interesting problem in practice, i.e., data fusion. In particular, we may have multiple data sources. However, some sources has limited observations with unbiased causal effects and other sources have sufficient observations with biased causal effects. The paper utilizes the tree-based algorithm with a new splitting criterion to tackle this issue. In addition, some theoretical analysis are provided to prove the advantages of the method.

Weaknesses:
1. Although the paper considers an important problem in practice, the reason why the method chooses tree-based algorithm is not convincing. In particular, other ML methods can also achieve data fusion. The advantages of using tree-based algorithm over other methods are confusing and are not clearly discussed. 
2. In the introduction and simulation studies, the author also mentions many other methods that deal with the data fusion problem. However, the reason why the proposed tree-based method can perform better than other methods are not interpreted. 
3. Although the paper provides the theoretical analysis for the causal effect estimation, the interpretation for the theorem are not convincing. For example, many other methods also have theoretical guarantee for the causal effect estimation. Which part in the theoretical analysis can illustrate the advantages of data fusion and the tree-based algorithm?
4. In the general picture, the idea of data fusion is very similar to that of transfer learning, i.e., we want to transfer the information from the observational studies to help the estimation of causal effects in trial data. However, the paper does not mention any related literature in transfer learning. In particular, what is the advantages and differences of the method compared with transfer learning? A more comprehensive literature review is encouraged. 

Limitations:
Please see in Weaknesses. 

Rating:
3

Confidence:
3

REVIEW 
Summary:
The authors propose a novel shrinkage method that fuses an unbiased estimator with a biased estimator. This method effectively reduces the MSE of the unbiased estimator. The approach offers a practical and straightforward implementation specifically tailored for estimating heterogeneous treatment effects. The authors extend the conventional node split criterion to align with the fused estimator and penalizes the use of observational data with substantial confounding bias. The authors also provide a theoretical analysis that explains the advantages of the modified splitting criterion.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- The application of the weighting strategy from shrinkage estimation to fusing unbiased and biased estimators in order to reduce the MSE of the unbiased estimator is a great idea.
- The modification of the node-splitting criterion that aligns with the fused estimator is an excellent enhancement to the methodology.
- The paper is well-organized and thanks to the authors's thoughtful and consistent notation, the methodology is easy to follow.

Weaknesses:
I'm concerned with the omission of $\sigma^2_b$ in practice. If we only look at the weight $w$, it make sense if $\sigma^2_b$ is small comparing to $\sigma^2_u$. However, in the tree building process, we need working estimates of the MSE of fused estimator, $\frac{(\sigma^2_b+b^2)\sigma^2_u}{\sigma^2_b+b^2+\sigma^2_u}$, and I don't think omitting $\sigma^2_b$ is justified by the same reason anymore.

Limitations:
No other limitation.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper introduces a Fused and Accurate Shrinkage Tree (FAST) algorithm for heterogenous treatment effect estimation given trial and observational data. The FAST algorithm introduces (i) a shrinkage based approach that combines trial and observational data for MSE reduction in treatment effect estimation, and (ii) a split criteria which down-weights observational data with high confounding bias. Further, the paper provides theoretical analysis demonstrating the benefits of the proposed split criteria. Experimental results on synthetic and real-world data demonstrate that the proposed approach outperforms baselines per metric MSE.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
- The paper is well written and easy to follow. The reviewer enjoyed reading this paper.
- The proposed FAST algorithm is well-justified and the theoretical analysis might be of interest to some readers. 
- The paper tackles an important problem (fusing small RCT with large readily available observational data ) with many applications. 
- The algorithm seems simple and easy to implement

Weaknesses:
-  Eqn. 5:  The paper seems to have glazed over the rationale for dropping $\sigma_b$ in the shrinkage estimator. It's unclear in what scenarios, e.g., how large the observational sample size must be for $\sigma_b$  to become negligible.

Limitations:
The limitation discussion is inadequate. I encourage the authors to add a paragraph discussing the limitations.

Rating:
7

Confidence:
3

";1
qoiOpVrIEa;"REVIEW 
Summary:
The paper is an empirical study of different dimensionality reduction (DR) methods for brain activity data. 
Authors compare various approaches to brain representation using the MRI data from Human Connectome Project. 
In the manuscript, ""brain representations"" are called ""dimensionality reduction"" (DR) since they present brain MRI data in a compact way.
By DR, the author mean ways of presenting brain activity: selecting brain segmentation (parcellations), measuring activity inside these zones, calculating cross-correlation etc. The definition is different from the one used in ML/AI community, where by DR we mean algorithms like t-SNE, UMAP, etc. 
Authors use topological data analysis (persistent homology, topological bootstrap, prevalence score) to evaluate the quality of DR.
Lots of computational resources were (80, 000 CPU hours over the course of a month) were spent to such an evaluation.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
Neuroimaging is an active field of research. The paper is technically correct in my opinion. Some recent tools of TDA (like prevalence, topological bootstrap) are applied.



Weaknesses:
First of all, I'm not an expert in neuroscience, so I can evaluate only the dimensionality reduction/topology part.

1. In the manuscript, ""brain representations"" are called ""dimensionality reduction"" (DR) since they present brain MRI data in a compact way. By DR, the authors mean ways of presenting brain activity: selecting brain segmentation (parcellations), measuring activity inside these zones, calculating cross-correlation etc. The definition is different from the one used in ML/AI community, where by DR we mean algorithms like t-SNE, UMAP, etc. 
2. No contribution for ML/AI/DL. Typical conclusion from the study: ""As expected, we also saw that feature number was a more important driver of persistence structure than the underlying rank of the decomposition"". The conclusion gives insights about types of brain features, not DR algorithms from ML.
3. I didn't understand some parts of the paper with neuroscience jargon (cortical parcels, grayordinates, subject space). Captions on Fig. 2 are not explained, what does mean ""Shaefer600 pNMs Psim-ztrans"", etc?
4. Some references are missing and some relevant methods are not evaluated.

Overall, the paper seems to fit more traditional neuroscience community than NeurIPS community.
But I can be mistaken.

Limitations:
Authors adequately addressed the limitations.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper investigates shared geometric structure across different (very broadly speaking) dimension reduction algorithms for functional brain connectivity. The authors examine different connectivity representations through persistent homology via topological statistical and bootstrap. 

Soundness:
3

Presentation:
1

Contribution:
2

Strengths:
- The paper is written clear language. 
- The paper proposes novel metrics to evaluate graph structures. 

Weaknesses:
- This paper is quite ambiguously written and not self-contained although the languages are clear. 
- I believe the presentation can be far improved by explaining background better, restructuring paragraphs and better description of mathematical notations. 
- For example, topological sampling is not well explained and readers would have to rely on other papers. Also, sections do not flow smoothly as mathematical notations are either not consistent or variables in equations do not connect. 
- There is no baseline experiments and the result from the analysis cannot be properly validated. 
 

Limitations:
There is a section describing the limitation. 

Rating:
3

Confidence:
4

REVIEW 
Summary:
The authors study shared geometric structure across different dimensionality reduction (DR) algorithms applied to neuroimaging data (fMRI data from the Human Connectome Project). In particular, they compare different DR algorithms (which they call ""brain representations"") by applying them to the same data sample and comparing the resulting Vietoris-Rips complex in each low rank data embedding using a modified topological bootstrap and cluster on the resulting estimated topologies.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
The authors do introduce framework for the comparison of DR methods that work with any data or dissimilarity measure amenable to Vietoris-Rips filtration and apply their method to real neuroscience data. 

Weaknesses:
The authors seem to have put in a good amount of work. However the paper currently lacks motivation or clear significance. Instead the paper reads like a number of different parcellations and DR methods were all applied to a public dataset after which a number of somewhat justified, somewhat arbitrary sequential analysis decisions were made to arrive at a clustering to determine similarity. Good background is given on each step, but it is not clear where the novelty lies here, or why these steps are the right ones. It is unclear exactly what deeply useful conclusions can be drawn from this analysis.

The writing and definitions could be much more clear throughout. Some are used before they are defined well, some terms with precise meaning seem misused (e.g., induced topology seem misused; ""induced on...data""), some terms are highly redundant and misleading (e.g., ""brain representations"").

The github link is not really anonymized (there is another project at the same github link clearly from the ""Personomics Lab""). 

Limitations:
The authors have a limitations section.

Rating:
3

Confidence:
3

REVIEW 
Summary:
The paper proposes an approach of comparison of different standard dimensionality reduction technics (DRT) of fMRI, using topological data analysis tools. In this case, these DRT computes lower dimensional representations of the Human Connectome Project dataset.
Then, for this specific dataset, the authors associate to each DRT (and for each feature type of each DRT) divergence matrices computed using correlations of these representations.
Finally, these matrices, seen here as distance matrices, are compared to each other, by 
 1. Computing persistence diagrams associated to the Vietoris-Rips complex of these matrices,
 2. Reweighting these diagrams by *prevalence scores*,
 3. Computing wasserstein distances between these diagrams,
 4. Cluster the DRTs using this distance matrix.

Soundness:
3

Presentation:
3

Contribution:
1

Strengths:
Looking at the fundamental differences between different dimension reduction techniques is a very interesting topic: on one hand, this can help increasing the performance of these methods by cleverly taking all of them into account; and on the other hand, this allows to have an insight on what these algorithms are retrieving from the original data. This also motivates to ignore the individual statistical performances of these algorithms. Furthermore, looking at the topology, and the *prevalence* of topological structures of the output of such method, for such a geometric dataset (brain representations), seems to be a very interesting and promising idea.

Weaknesses:
Some techniques used to tackle this problem are not natural or are not motivated enough; either intuitively or theoretically, especially the prevalence-weighted wasserstein distance. In particular, I have the following comments:

 - The bijections between persistence diagrams usually add the points of the diagonal, with an infinite mass. In particular, 
    - how is defined the prevalence on the diagonal?
    - If the diagonal has no mass here, there are also a few problems
        - A bijection may not exist (not the same cardinal)
        - This distance is not symmetric, as points of $d_2$ that are not matched to points of $d_1$ are not taken into account.
 - Multiplying points in the diagram by a real correspond to do an **homothety with center $(0,0)$**, which raises a few questions
    - To my knowledge, 0 has no particular role in a diagram, so what is the motivation for using it?
    - For a same prevalence $\alpha$, the rescaling depends at which scale the topological structure appears. And the same goes for the angle direction in which the points are moved. Is this behavior wanted, and why? 

I think that taking into account the prevalence is an interesting idea, but I'm not convinced that this is the best way to do it.

Moreover, there are small typos:
 - Section 2.1
   - Feature types details could be improved, i.e., mathematical definitions.
 - Section 3.2.1
   - Mention who is $k$
   - The Vietoris Rips filtration is not a graph, but a clique complex (it has simplices of arbitrary dimensions)
   - This section could be made bigger, for non-TDA practitioners. 
 - Section 2.3.2
   - Clarity could be improved here,
   - line 196 ""multiple data element to [represent] the same homology [class]""
   - line 205 : intervals are never defined

Limitations:
Limitations have been addressed.

Rating:
3

Confidence:
3

REVIEW 
Summary:
A very interesting and rigorous exercise of comparison of embeddings for the purpose of evaluating manifold learning is presented. The chosen framework is root in trendy topological concepts such as persistent homology for the purposes of analyzing the data topology mixed with geometry-based measures of similarity across representations, and coupled with stochastic (topological) bootstrap to study variations over co-embeddings.
Contributions are explicitly mentioned in lns 113-117 and certainly delivered.
Best of my lot for this year.


Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
+ The idea is exceptional IMHO. I’ve known of (and used myself) some other frameworks trying to establish and understand similarities or dissimilarities of projections, but they were all much more naïve. This one offers a clearly more sophisticated approach that yields a much richer picture without substantially sacrificing interpretability of results, with the “only” price to pay of a very large computational cost.
+ Extremely well explained despite the very complex concepts involved.
+ The observation on lns 290-1 that cycles with low persistence may carry meaningful structure is something that I have thought myself occasionally but couldn’t put my finger on it nor how to articulate it nor know how to reveal it. This is a very nice confirmation of that intuition.


Weaknesses:
Not many that I can see to be honest…
+ The study is only conducted in experimental data from the human connectome project but it is never validated in synthetic known manifolds with and without added noise. This means that some of the observations in the last part of the draft are (most likely correct but) difficult to verify. For instance, the implication that the proposed framework distinguishes more feature types and numbers than representation types.


Limitations:
+ Lns 92-106 literature review is perhaps missing some of the most primitive approaches; e.g. distance distortions plots, (graph/manifold) isomorphisms, … this is possibly intentional as the persistence element there is only implicit rather than explicit as in the case of the works reviewed. But if it wasn’t, well, it is perhaps convenient to at least mention some early efforts.
+ Perhaps the number of compared embeddings (brain representations) is not as large as one would like for this type of exercise but the authors clearly state why they stay on low numbers (computational cost) and promise larger comparisons in the future. Looking forward to those!


Rating:
8

Confidence:
4

";0
Z8q7GmS89a;"REVIEW 
Summary:
The paper proposes an algorithm for offline imitation learning on a mixture of “perfect” expert demonstrations and “imperfect” sub-optimal demonstrations. The authors provide a theoretical motivation for their approach and exhibit results on various Mujoco and Adroit tasks. The authors also conduct ablation studies to justify their design choices.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper proposes a novel and interesting approach for learning from suboptimal data. It also provides some theoretical motivation for why filtering data based on resultant states might be better than simply using the state-action distribution. 
- The paper is written clearly and concisely.
- In addition to the novelty in filtering the data, the paper proposes a novel constrained BC algorithm where a discount factor is used to reduce the impact of stochasticity in the MDP during the optimization process.
- The author’s compare their approach with a bunch of recently published offline imitation learning algorithms capable of learning of suboptimal data. The algorithms are evaluated across 4 MoJoCo tasks and 4 Adroit tasks with two variations (low and high expert data) for each task.
- The authors justify their design choices using ablation studies showing the impact of number of expert demonstrations, the number of rollback steps considered, the use of the discount factor and a comparison between runtimes of iLID and other algorithms.


Weaknesses:
- Though the paper has some good ablation studies, it would be interesting to have a study of the effect of quality of the suboptimal data on the performance of the method. The paper currently only considers random trajectories as suboptimal data which might not be very useful for harder tasks. Instead, imagine demonstrations that can complete a part of the task but not the entire task (for instance, can pick up the hammer but not hit the nail). These can probably be collected by rolling out an expert agent and taking random actions in between (varying the percentage of random actions can give different levels of suboptimality). Such a study can help (1) highlight the relevance of the work in the real world where collecting perfect demos might be hard but it is often possible to do parts of the task, and (2) highlight the importance of expert demonstrations in this problem setting (for instance, can we reduce the amount of expert data if the amount of suboptimality in the remaining data reduces).
- Fig. 4 plots the average return against the number of training steps. However, since the models are completely trained on offline data, a better metric might be a comparison of maximum performance attained by different algorithms. Also, training it for too long might result in the model overfitting on the data, thus, reducing the average return over time (as can be seen in quite a few tasks in Fig. 4).
- It would be great if the ablations in Fig. 3 could be shown on a few more tasks in the appendix.
- The paper is missing a limitations section.


Limitations:
The paper is missing a limitations section.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper addresses the challenge of learning good imitative policies from offline data, in which abundant imperfect demonstrations are mixed with few expert ones. Unlike previous work that measures the state-action similarity between imperfect and expert data, the present work proposes iLID, which leverages trajectories in imperfect data that lead to expert states in several steps. The sample complexity analysis indicates that this approach benefits the performance of imitation policy, and the empirical results suggest that iLID outperforms baselines including state-of-the-art offline imitation learning methods. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* As illustrated in Figure 1, the original idea of selecting imperfect demonstrations leading to expert states is novel and makes intuitive sense.
* The policy optimization problem is well-posed, and is straightforward to implement with alternating dual ascent.
* Empirical results suggest a large performance gain for iLID compared to state-of-the-art baselines, especially when the dataset contains very few expert demonstrations. In particular, the ablation study in Figure 3 does a good job of explaining why the constrained optimization problem leads to a better policy than the naive direct imitation approach.


Weaknesses:
The quality of presentation can be improved. 
1) $\tilde{\mathcal{D}}$ in equation (6) overloads the notation that was originally presented in Section 3.1 without time indices. 
2) Remarks in Section 3.1 state that the sample complexity of the proposed approach is better than the vanilla BC, but there’s no citation for the BC sample complexity. 
3) The explanation on the behavior interference for the complementary dataset $\tilde{\mathcal{D}}$ did not make full sense and requires further clarification. In particular, it is unclear why more recent actions are preferred when the same state appears multiple times in the trajectory, even though the underlying MDP does not have any discount factor in the definition of the value function. (What would happen if the discount factor $\gamma$ in equation (7) is set to 1 for all the experiments?)

Limitations:
The authors have not explicitly provided limitations in the paper. One conceivable limitation is that one requires the datasets of expert and imperfect demonstrations to be labeled as such, although it seems unavoidable for any methods of this kind. Another limitation could be that the resulting policy may still fail to discover diverse modes to accomplish the task, if the expert demonstration only has a single mode. For instance, in a goal-reaching navigation task similar to the one depicted in Figure 1, the expert policy still needs to exhibit the two different paths so that iLID learns to discover both modes to approach the goal.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The submission presents a novel method called Offline Imitation Learning with Imperfect Demonstrations (iLID) for Offline Imitation Learning, which aims to improve policy learning from both expert and imperfect demonstrations. Compared with previous IL methods, which only consider the state-action pairs during learning, this paper also considers the dynamics of the non-expert data. To this end, the submission proposed employs a data selection technique using a discriminator on the resultant state of behavior, meanwhile integrating lightweight-constrained behavior cloning. Some empirical studies showed the outperformance of the proposed method compared with other baselines.

Soundness:
1

Presentation:
3

Contribution:
2

Strengths:
1. Overall the paper is well-written.
2. The motivation for including dynamics in behavior cloning makes sense and easy-to-follow.
3. The experimental tasks and baselines are sufficient.

Weaknesses:
1. Some notations are so confusing that the descriptions of these notations can not lead to corresponding theoretical results.
2. The assumption of Theorem 3.1 is too strong but without any discussion.
3. The proposed algorithm and the motivation in the introduction (Figure 1) are isolated.

Limitations:
There is no discussion about the limitation or societal impact in the submission.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper addresses the problem of offline imitation learning (IL) from demonstrations that are noisy/suboptimal. To this end, the authors propose iLID which is a two-step process—a data selection step which only retains those $(s,a)$ transitions from suboptimal demonstrations which lead to states in the expert demonstrations, thereby maintaining a supplementary data buffer; then policy learning is performed by behavior cloning on samples from the supplementary buffer while also regularizing the policy to not stray too far from a BC expert policy (on samples from the expert data buffer.)  

The authors establish competitive upper bounds on the suboptimality and sample complexity of iLID. Extensive experimental on 8 complex robotic tasks, and accompanying sensitivity studies show that iLID outperforms 5 competing baselines, and limited sensitivity analysis is performed.


Soundness:
2

Presentation:
4

Contribution:
3

Strengths:
-	The paper tackles the important but challenging issue of offline imitation learning from suboptimal demonstrations. In this regard, the paper addresses a pertinent problem in the research area.
-	The rationale behind the formulation is simple yet powerful. Specifically, the data selection step trains a discriminator to select only those transitions in the suboptimal data $(s,a) \in \mathcal{D}_\mathcal{s}$ which lead to a state in the expert data $(s,a) \in \mathcal{D}_\mathcal{e}$ within some specified $K$ timesteps. This is a simple way of leveraging offline data to distil only useful knowledge from suboptimal data for policy learning and may aid the agent in correcting towards expert behavior from non-expert states. The formulation of the policy learning step as a regularized version of BC yields demonstrable improvements in training time.
-	Empirical results on the D4RL robotics benchmark dataset are impressive (Table 1) and hold across all but one environment. 
-	Overall, the paper is very well-written and provides helpful illustrations and examples to present ideas.

Weaknesses:
Experimentally, seeding imperfect dataset with expert data (~1-20%) seems like a strong assumption. Given that the data selection method explicitly selects $(s,a)$ pairs based on whether they lead to expert states $s \in \mathcal{D}_\mathcal{e}$. If the expert and suboptimal trajectories only share the seeded expert transitions i.e., $\mathcal{D}_\mathcal{e} \cap\mathcal{D}_\mathcal{s} = \mathcal{D}_\mathcal{\text{seeded}}$ (a realistic assumption in real-world cases), then the proposed selection criterion will select only the (seeded) “expert” transitions from $\mathcal{D}_\mathcal{s}$ to add to supplementary data $\tilde{\mathcal{D}}$ (since only states from the seeded expert trajectories in $\mathcal{D}_\mathcal{s}$ would lead to successor states in $\mathcal{D}_\mathcal{e}$). In this case does iLID reduce to just pure BC (Pomerlau, 1998) on just expert data with additional policy regularization? Further, Figure 3a also shows that iLID does rely heavily on expert demonstrations for it to perform well. Some ways to address this – 
- Experiment showing results when the imperfect data is not seeded with expert demonstrations would best clarify this issue.
- Experiment showing results for varying # of expert trajectories (as in Fig 3a) for different environments (including the unseeded case).
- As an alternative methodology, to bypass seeding, the imperfect demonstration set could be generated rolling out trajectories from the noise-injected expert BC policy $\tilde{\pi}_{\mathcal{e}}$.



Limitations:
While the iLID formulation is interesting, some weaknesses associated with data seeding could be discussed in more detail as per suggestions above.

Rating:
4

Confidence:
3

";0
wS3PPBUDX8;"REVIEW 
Summary:
In this work, a probabilistic view of adversarial examples based on the [projected stochastic gradient Langevin algorithm](https://proceedings.mlr.press/v134/lamperski21a.html) is introduced and used as an optimization algorithm instead of the SGD or Adam optimizer for adversarial examples. In addition, the geometric constraint (Lp norms) is replaced by a semantic distance criterion based on an instance-wise energy-based model (i.e., an EBM is trained for each instance, using transformed versions as the training dataset) to ensure semantic/visual proximity to the original input. They improved the adversarial examples using the [CW objective](https://www.computer.org/csdl/proceedings-article/sp/2017/07958570/12OmNviHK8t) and thin-plate splines transformation to create a more diverse training dataset for EBM training. Moreover, they generated a set of successful adversarial attacks (i.e., fooled the classifier) via rejection sampling and proposed a simple selection procedure to select the final adversarial examples based on the softmax probabilities of an auxiliary classifier and the energy of the examples. The experiments show that the proposed method is able to generate adversarial examples that fool the classifier while being visually/semantically indistinguishable to humans.

Soundness:
1

Presentation:
3

Contribution:
2

Strengths:
- The proposed method is very detailed and intricate.
- The Langevin Monte Carlo-based optimization procedure seems to improve the quality of adversarial examples overall.
- The paper is well-written and clearly structured.
- Code is provided.

Weaknesses:
- Previous work, e.g., by [Sharma & Chen](https://openreview.net/forum?id=Sy8WeUJPf), has also generated visually similar adversarial examples for the MadryNet while still using a geometric distance ([elastic-net regularization](https://arxiv.org/abs/1709.04114)). This raises questions about the generality of the work’s central claim that it “transcends the restriction imposed by geometric distance, instead opting for semantic constraints” (L4-5) beyond the limitations of the adversarial attack methods shown in the present work.
- The present work only shows experiments on digit-based datasets (MNIST & SVHN). Applications to datasets with natural images (e.g., CIFAR or ImageNet) are missing. Consequently, the necessity and applicability of the proposed adversarial attack are very unclear, since for natural images the adversarial examples typically remain visually very close to the original inputs; also after adversarial fine-tuning.
- The work is missing interesting experiments, e.g., what would happen if we use the proposed adversarial attack approach for adversarial training? Does it improve adversarial robustness? Does the adversarial attack also bypass certified defenses? Overall, the experimental section is very short (3 lines of results) and would greatly benefit from, e.g., the aforementioned experiments.
- The approach requires an instance-wise energy-based model for its semantic distance loss, which must be trained for every sample (on different augmented versions); cf. L122. This may limit its applicability.
- The proposed attack and problem setup are not quite original, i.e., it combines well-known techniques, or previous work (see first point above) has also already targeted the visual similarity challenge of adversarial examples for adversarially fine-tuned models.

Limitations:
The limitations are adequately addressed.

Rating:
5

Confidence:
4

REVIEW 
Summary:

This paper introduces a novel approach to adversarial attacks that goes beyond traditional norm bounded attacks. Instead, the proposed method focuses on unrestricted attacks that are both effective and capable of preserving the semantic meaning of the input data.

The method utilizes Langevin Monte Carlo techniques to sample from a distribution of potential attacks. To ensure semantic preservation, a learned energy function is employed, which guides the generation of adversarial samples. Rejection sampling and refinement techniques are then applied to select and further improve the quality of the generated samples.

The evaluation of the proposed method demonstrates a significant success rate when attacking defended models. By allowing for unrestricted attacks while maintaining semantic integrity, this approach presents a promising advancement in the field of adversarial attacks, showcasing its effectiveness and potential for practical application.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Interesting work on unrestricted adversarial attack, which is important given that most attacks now are bounded attack.

2. The method is effective in breaking already defended models. Fig 1,2 clearly shows the advantage over norm bounded attacks.

Weaknesses:
1. What is the computation cost of the attack? The paper only evaluates on two toy datasets, MNIST and SVHN, the reviewer is wondering if the method can generalize to larger dataset.

2. Ablation study on the component is missing. Like TPS as data augmentaion, the effect of the choice of the sampling method. Also the method requires specify several hyper parameters, like M. Ablation study is useful.

Limitations:
None

Rating:
6

Confidence:
4

REVIEW 
Summary:
The adversarial examples generated by classical methods such as PGD have different semantic meaning to the original label, which means that the adversarial examples are easy to be distinguished by human. In this paper, the authors focus on the generalization of adversarial example which preserves the original semantic information. They propose a semantically-aware distance measure to replace the geometrical distance measure. And they use Langevin Monte Carlo method to find the minimal point (adversarial sample) of their proposed loss function. Several techniques that further enhance the performance of the proposed method are presented. From the experimental results, it seems that their generated examples preserve the original semantical imformation.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* As far as I know, the proposed adversarial attack method is novel.
* They proposed a semantical distance measure to generate the semantic-aware examples. Although the idea of semantical measure already exists in many previous work, I think the usage here in adversarial example generalization scenario is interesting and reasonable.
* Their method is theoretically and experimentally reliable.

Weaknesses:
* One of the limitation of this paper is that, the loss of semantics of adversarial examples only exists in some simple tasks, such as MNIST and SVHN. As the experimental results in previous work shows, the adversarial examples of CIFAR and ImageNet have very little disturbations that cannot be distinguished by human and preserve the semantical information. Hence, I think the significance of this paper is somewhat limited.
* The motivation of using EBMs and LMC is not very clear to me. In my opinion, we can directly optimize the semantic-aware loss to generate the adversarial examples. The necessity of using the EBMs and LMC should be stated more clearly.
* In the experiment part, the success rate involves subjective factors. They use human annotators to determine whether the adversarial examples have the same meaning as the original label. Is there a more subjective metric? Otherwise, the experimental results may suffer a credibility crisis.
* More experiments on CIFAR-10 and CIFAR-100 are necessary.
* Can you give a more detailed explaination of the training of the energy-based model? I noticed that Section 2.5 includes some brief introduction, but what is the data distribution $p_d$ here? What is the specific training algorithm?

If the authors can address my concerns well, I will consider raise the score.

Limitations:
Yes.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes to generate semantics-preserving adversarial examples by framing the construction of adversarial examples as a box-constrained non-convex optimization problem. More specifically, the authors propose a Langevin Monte Carlo (LMC) technique to craft adversarial examples that preserve the meaning of the original inputs they are derived from. With this framing, they cast the generation of adversarial examples as a semantic-based probabilistic distribution. The authors showed that their semantic-aware adversarial attack is capable of fooling robust classifiers while preserving most of the semantics of their source images.  

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper is quite interesting paper and well-written. The problem is well-defined, and the solution quite intuitive. The math is also quite sound. Although the problem of generating semantics-preserving adversarial examples has been studied extensively in the past, it still remains relevant. This paper proposes another interesting perspective on how to approach this problem.     

Weaknesses:
Although the paper is interesting, the evaluation is quite limited. For instance, the approach is only evaluated on MNIST and SVHN. Evaluating the approach against ""more challenging"" datasets like ImageNet, CIFAR-10, CIFAR-100 would make their contributions more compelling. Also, studying the transferability property of their attacks would strengthen their paper, and give more confidence to the readers about the strength of their attacks. Moreover, I would have liked to see how the magnitude of the noise used in Thin-plate-spine affects the overall performance of their attacks. Finally, the related work section is rather limited. There is a plethora of interesting studies in crafting adversarial examples that are semantics-preserving. For instance, [1] and [2] are quite related to the approach the authors propose, and should be evaluated or discussed further in the related work section. 

[1]: Semantics Preserving Adversarial Examples. https://aisecure-workshop.github.io/amlcvpr2021/cr/27.pdf
[2]: Localized Uncertainty Attacks. https://ui.adsabs.harvard.edu/abs/2021arXiv210609222A/abstract


Limitations:
Yes.

Rating:
5

Confidence:
5

";0
pgmNZQdH7R;"REVIEW 
Summary:
The paper studies the dimension collapse problem in image self-supervised learning. Previously, people use image space data augmentation/momentum/stop gradient and so on to address this problem. This paper proposes to address the problem via spectrum transformation (ST) in the feature space instead (aka balancing the eigenvalue of covariance matrix while keeping eigenvectors the same in mini-batches). 



Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1.	Its ST generalized previous whitening approach (aka make the covariance matrix to be identity), and extends it to power functions (aka p \in R instead of p = 0.5).  
2.	For the calculation of whitening matrix, the paper proposes a less expensive approach to calculate (IterNorm). And the paper analyzes how the singular values will change using IterNorm. But to really make things work, the paper adds a trace loss (INTL). 
3.	The paper shows INTL reaches SOTA performance on image classification and object detection tasks. And INTL is less sensitive to batch size.

Weaknesses:
1. The **major concern** for this paper is that, it resembles quite a few similarities in the key ideas to a recent AAAI work: ""Spectral Feature Augmentation for Graph Contrastive Learning and Beyond"" (https://arxiv.org/abs/2212.01026), but the AAAI work has not been cited or discussed in this work, making the contribution of this work a bit incremental:

a. The motivations are similar: deal with feature collapse/dimension collapse via change feature space’s spectrum.

b. Similar pipeline, the AAAI paper apply SFA after encoder and before projection, this work applies ST in the end of the pipeline. the AAAI work injects noise for better augmentation while the paper does not. This paper finally needs to add an additional trace loss to make ST work, which may play a similar role to the injected noise. 

c. Iterative normalization to calculate \Sigma_{-0.5} is like appendix C8 algorithm 2. But with a slightly different formulation. The AAAI paper calculates \Sigma_{-0.5} and \Sigma_{0.5} at the same time. This work only calculates \Sigma_{0.5} with a slightly different iteration function.

d. This work’s power functions for ST are like AAAI’s MaxExp(F) and Matrix Square Root, but this work’s approach does not inject a Gaussian noise.

e.Both experiments on SSL on ImageNet. The AAAI work explored both graphs and images. This work focuses more on images, so it also explores CIFAR, and also object detection tasks. But AAAI work deals with contrastive SSL (InfoNCE) while this work uses a non-contrastive approach (normalized MSE loss).

2. It states to explore other possible spectrum transformation approaches but only extends from whitening (p=0.5) to power (p \in R) with a limited range of working p around 0.5. This does not validate its statement to generalize ST.

3. Directly using ST (IterNorm) does not work, and finally needs to add trace norm (INTL). It is not shown that if only using the implicit trace norm without explicit ST, whether the performance will still be good. This does not validate its need to do explicit ST.


Limitations:
n/a 

Rating:
3

Confidence:
3

REVIEW 
Summary:
Feature collapse is a major problem in contrastive-base self-supervised learning. To address this issue, the paper introduces the spectral transformation framework, which aims to mitigate the aforementioned problem. Generally speaking, the spectral transformation expands upon the extensively employed whitening transformation discussed in the literature and treats it as a special case. Furthermore, the paper proposes the IterNorm approach featuring trace loss to enhance the performance of self-supervised models. The experimental results validate the effectiveness of the IterNorm with trace loss (INTL) on various widely-used SSL-evaluation benchmarks, including but not limited to linear evaluation, classification, and object detection.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1)	The paper addresses a fundamental challenge in the context of contrastive-based self-supervised learning, and proposes an elegant method with theoretical guarantees. Specifically, the proposed framework extends the widely used whitening operation discussed in prior literature, treating it as a special instance, thereby enhancing the theoretical contribution of this work.

2)	 The experimental results are promising. The proposed method is evaluated on several SSL-evaluation benchmarks, including ImageNet linear classification (achieving 76.6% accuracy on ResNet-50), COCO object detection (achieving a score of 41.2 on ResNet50-C4), and significantly surpasses the previous state-of-the-art methods.

3)	The presentation of the paper is good, with a clear elaboration of the motivation and contribution of the proposed method.


Weaknesses:
1)	The comparison between the proposed method and baseline is unfair. Firstly, the proposed framework employs a 3-layer projection head with a substantially larger dimension (8192), which significantly increases both computation and memory costs. This discrepancy makes it hard to conduct a fair comparison between the proposed method and the baseline approach. 

2)	As mentioned earlier, the proposed method incurs nearly twice the computation time per epoch and peak memory per GPU compared to the naive contrastive baseline method MoCo/MoCo-v2.

3)	It is worth noting that the multi-crop strategy employed in the paper is highly similar to that used in [a], including the crop-nums and crop-sizes. It is unclear why this was not cited in the paper.

4)	It should be noted that the multi-crop strategy employed in [a] can also enhance the detection and classification results. Therefore, the absence of this information in the comparison makes it difficult to conduct a fair evaluation.

[a] Wang, X., & Qi, G. J. (2022). Contrastive learning with stronger augmentations. IEEE transactions on pattern analysis and machine intelligence, 45(5), 5549-5560.


Limitations:
yes


Rating:
5

Confidence:
5

REVIEW 
Summary:
For self-supervised learning, this paper proposes a spectral transformation (ST) framework to modulate embedding, seeking functions other than whitening that can avoid dimensional collapse. The authors propose IterNorm with trace loss and provide a lot of theoretical and experimental analysis. The results show its effectiveness and advancement.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. This paper provides intuitive experimental verification, extending the whitening transformation to a more general spectral transformation.
2. For the proposed IterNorm with trace loss, this paper conducts rigorous theoretical analysis and practice, showing that it is effective for avoiding collapse.
3. The proposed method can achieve good results without relying on large batches, which can be comparable to supervised performance under regular settings.


Weaknesses:
1.	The motivation for the proposed IterNorm with trace loss is unclear. The authors empirically observe that IterNorm suffers from severe dimensionality collapse, but do not state the limitations of existing SSL methods in this case.
2.	For the ablation study of batch size, Table 4 shows that the performance of the proposed method increases significantly as the batch size increases, indicating that the method is also sensitive to the batch size. In addition, SimCLR, SwAV and other methods require a batch size of 4096. Table 4 should provide the corresponding results with a batch size of 4096 and the sensitivity of other methods to the batch size.
3.	Among the existing SSL methods compared in the experimental part, few new methods since 2022 are covered. More state-of-the-art method comparisons need to be added.


Limitations:
The authors have explained some of the limitations in the paper, and others can be seen in weaknesses.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This submission proposes a spectral transformation for redundancy-reduction-based (a.k.a. whitening-based) self-supervised learning (SSL).
Spectrum-domain modulation is helpful in preventing collapsing caused by representations' rank deficiency.
Since the whitening operation can be seen as a square-root transformation on spectrum, it can be generalized to other
power-function transformation.
To avoid numerical instability of matrix decomposition in computing spectra, an approximation based on Newton’s iteration method IterNorm
is proposed. While IterNorm by itself suffers from dimensional collapse, its combination with trace loss, the term to encourage the spectrum to have equal-eigenvalue distribution, works well to avoid collapse.
Experiments on ImageNet, CIFAR-10/100, and COCO show that the proposed method outperforms existing state-of-the-art methods.

Soundness:
3

Presentation:
2

Contribution:
4

Strengths:
- The idea of spectrum-domain modulation seems intuitive to avoid dimensional collapse caused by rank deficiency of representations.
- State-of-the-art SSL performances using ResNet50 were achieved.
- Especially excellent performance avoiding collapse in small batch sizes is practical for low-resource training.

Weaknesses:
- The difference between existing whitening-based methods and power-function modulation with p = 0.5: Sec 3.2 empirically shows that p == 0.5 (== 1/2) is a good value. At the same time, p4 l 170 says, ""whitening is a special instance of spectral transformation, where g(·) is a power function g(λ) = λ^{− 1/2}"". This confused me with what difference this generalization made from usual whitening.

- Trace loss + BarlowTwins/VICReg?: It is said that IterNorm works well only when combined with trace loss.
This naturally implies that the source of success is trace loss itself rather than IterNorm. Is it possible to combine trace loss with existing whitening-based methods part from IterNorm?



Limitations:
Limitations or social impact are not discussed, but I do not have particular concerns.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper addresses the dimensional collapse problem in self-supervised learning and proposes a framework to moderate the spectrum of embedding. Besides, a new spectral transformation variant, called IterNorm with trace loss (INTL), is proposed that can avoid collapse and modulate the spectrum of embedding towards an equal-eigenvalue distribution. Experiments on various tasks verify the effectiveness of the proposed method. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The proposed method looks technically sound. 

- Extensive experiments have been conducted to verify the effectiveness by comparing with the existing methods in the field. 

- The presentation is great. 

Weaknesses:
This looks like a good paper. I do not have much to complain about (possibly due to I do not work in this direction). I just have two minor questions. 

- How do you balance the MSE loss and IterNorm trace loss? Are there coefficients in the two loss terms? If there are, how do you set the coefficient values? Do they keep fixed across all the datasets? 

- The proposed method INTL is based on the empirical finding that when p is in the neighborhood of 0.5, e.g. 0.45 ∼ 0.55, the model can avoid collapse. Is there any intuitive explanation or theoretic analysis for this phenomenon?   

Limitations:
See the weakness above. 

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper tackles the dimension collapse problem in self-supervised learning. The authors propose spectral transformation which can be served as an alternative for the whitening function in avoiding dimensional collapse. Further, they propose a new instance of ST, called IterNorm with trace loss (INTL), and prove that INTL can avoid collapse. Results on several SSL benchmarks show promising improvements over previous methods. 

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
- The paper is well-organized with a clear story and justification.
- The paper aims at analyzing the dimension collapse problem and prove the previous whitening function is a special case of spectrum transformation (ST).
- The authors claim that the numerical instability can be solved if a spectral transformation can modulate the spectrum without explicitly calculating λ or U. Then, they propose Iterative Normalization with Trace Loss as a solution.
-  The performance on Imagenet, CIFAR and COCo shows improvements over previous methods such as Btwins, and VICreg. 


Weaknesses:
1. Comparing methods in the Experiment section:
As the proposed method is claimed to be the general form of whitening function in SSL, I am wondering why the authors did not put the performance comparison against whitening loss [20, 40]. As [20, 40] have demonstrated the performance gains over Btwins, it is hard to tell how the INTL it better than [20, 40].

2. Concerns about novelty. 
In my view, section 3.2 Spectral Transformation proves the effectiveness of Whitening. The general form of ST is $g(\lambda) = \lambda^{-p}$ and the authors claim that 
> it seems to perform well to avoid collapse although the transformed output is not ideally whitened when p is in the neighborhood of 0.5, e.g. 0.45 ∼ 0.55. But when p gradually deviates from 0.5, collapse occurs.""
The whitening function is the case where $p=0.5$.

In section 3.3, the iterative normalization has also been explored by [40] in https://github.com/PatrickHua/FeatureDecorrelationSSL/blob/main/models/utils/iterative_normalization.py

The major contribution of this paper in my view is to propose a trace loss in iterative normalization to avoid dimension collapse. Yet, as the quantitative comparison to [20,40] is missing, it is hard to evaluate the contribution. 




Limitations:
I think the authors well address the limitations. 

Rating:
6

Confidence:
3

";0
BKAFLUcpBS;"REVIEW 
Summary:
The authors propose a new variant for unbalanced Gromov-Wasserstein with Kullback-Leibler (KL) divergence for marginal relaxation (Sejourne et al., 2021) by leveraging the outlier-robust approach (Mukherjee et al., 2021). The authors propose an algorithm approach by using Bregman proximal alternating linearization. The authors illustrate the advantages of the proposed method on several experiments.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
+ The authors propose a new variant for Unbalanced GW with KL divergence for marginal relaxation (Sejourne et al., 2021) by extending the outlier-robust approach (Mukherjee et al., 2021).
+ The authors propose to use the Bregman Proximal Alternating Linearized Method to optimize the proposed outlier-robust GW.
+ The authors empirically demonstrate the advantages of the proposed approach.

Weaknesses:
+ The authors leverage the outlier-robust approach (Mukherjee et al., 2021) for unbalanced GW. The proposed method is essentially a new variant of unbalanced GW, but somewhat expected results.
+ Some claims and experimental results are needed to elaborate with more details (see the Questions part)

Limitations:
The authors have discussed limitations of their work. However, there is no discussion about the potential negative societal impact of their work.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper proposes a Robust Gromov-Wasserstein (RGW) distance to tackle the high sensitivity to outliers of common GW distances. The key idea is to relax both the marginal distribution and the transport plan based on the KL divergence. An algorithm based on BPALM is further proposed with guaranteed convergence. Theoretical analysis demonstrates the robustness to Huber-$\epsilon$ contamination, and empirical results demonstrate the effectiveness in partial correspondence with the presence of outliers.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is well-written and easy to follow, with theoretically and empirically sounded analysis.
- The paper tackles the crucial robustness problem in GW distances, which could potentially benefit many OT-related research.
- Experiment results demonstrate the effectiveness of RGW on the partial correspondence task, a task that is hard to addressed by most OT methods.


Weaknesses:
- A section on related works is **strongly encouraged**. It would be beneficial to discuss the relation/superiority of the proposed RGW compared with other relaxed GW methods, e.g., UGW and srGW.
- Some modifications in the experiment part are encouraged for better presentation. See details in the “Questions” section.


Limitations:
The authors properly addressed the limitations in the paper.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors propose a robust version of the Gromov Wasserstein (RGW) distance and theoretically bound their RGW distance applied to noisy distances in terms of the GW distance between the clean distributions and an error term depending on the choice of parameters in RGW and the distance between the clean distribution and the outlier distribution. The authors develop an algorithm for computing the RGW distance using Sinkhorn’s algorithm and Newton’s method.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The theory provides guidance on how to choose parameters in order to get a good approximation (at least an upper bound) of the RGW distance in terms of the GW between clean distributions.

Weaknesses:
To get the upper bound (mentioned above) one needs to know the GW distance between the clean distributions.

Limitations:
The authors close the paper by discussing the heavy computational cost of the method.

Rating:
8

Confidence:
4

REVIEW 
Summary:
The authors propose a new formulation of unbalanced Gromov-Wasserstein (GW), called Robust Gromov-Wasserstein (RGW). Instead of controlling the deviations of the OT marginals from the prior marginal distributions $\mu$ and $\nu$ directly via quadratic phi-divergences like UGW, RGW proposes to use linear phi-divergences (KL) operating on the intermediate learned marginals $\alpha$ and $\beta$ which are constrained to be relatively close to $\mu$ and $\nu$ w.r.t the KL. It is shown that this new formulation is theoretically better at handling outliers than the UGW under the assumption of Huber's contamination model.

They then suggest using a Bregman proximal alternating linearisation minimisation (BPALM) algorithm to solve this problem, which is shown to converge asymptotically to a critical point. Finally, the authors show empirically on a synthetic dataset that their method is more robust to outliers, and that it leads to better performance than many competitors on subgraph alignments and partial shape correspondences.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
-	Overall I find the paper well-written and I didn’t find any mistake in the proofs.
-	RGW is novel in the Gromov-Wasserstein literature, even if inspired from prior work on unbalanced linear OT as mentioned by authors. The theoretical results on robustness are compeling. The solver proposed by authors seem clearly competitive compared to the one for UGW, while its use is also theoretically supported.
-	RGW shows better results than competitors on partial shape matching of both synthetic and real-world data.
-	RGW also shows better results than competitors on subgraph alignment problems on many real-world graph datasets. Authors empirical study considers subgraphs of various proportions sampled from given input (full) graphs. RGW shows consistent improvements compared to competitors along these various proportions. Moreover authors show that RGW performances are rather consistent within considered sets of validated hyperparameters.


Weaknesses:
 - 1. Positioning w.r.t the unbalanced OT literature: 
     - a) *[addressed by authors]* The extension to GW of the work on Wasserstein in [Balaji & al, 2020] should be more explicit. As the latter does not consider regularizing their loss function, you should justify why you do so in your GW setting.

     - b) *[addressed by authors]* The recent paper of [A] provides in a sense a primal point of view of robust GW, It would be good to discuss it and to benchmark it in your subgraph alignment benchmark.

- 2. Robustness guarantees :
     - a) *[addressed by authors]* Missing proof: Could you provide the proof for UGW in Remark 2.4 ?
 
     - b) *[addressed by authors]* Tightness: Could you empirically analyze the tightness of both bounds for instance while e.g validating algorithm initializations to get better estimates of various GW terms ? 

     - c) *[addressed by authors]* Positioning with recent analysis: Theorem 1 in [Tran & al, 2023] also provides an affine relation of the form: noisy UCOOT <= a * clean UCOOT + b. I believe that this theorem can be easily extended to UGW so could you discuss the relation of this theorem to your bounds ?
 
- 3. Benchmark: 
     - a) *[addressed by authors]* RGW depends on a lot of hyperparameters. Authors show that their performances were rather consistent for the hyperparameters tau and rho which are involved in the RGW program however it seems that the sensitivity analysis always includes an intensive validation of the BPALM step sizes hyperparameters.  i) Could you clarify if shown results in Section 4.2 always includes this validation of BPALM hyperparameters ? ii) Could you provide a sensitivity analysis to these hyperparameters ?

     - b) *[partially addressed by authors using normalized degrees]* Sensitivity to input graph distribution:  In the context of graph representation learning, an open question relates to a good choice of input graph distribution for tasks like graph partitioning [Xu & al, 2019a ; Chowdhury & al, 2021; Vincent-Cuaz & al , 2022]. Uniform distribution is the common simple choice which seems to be a good enough choice, but power-law distributions based on normalized degrees clearly allow better partitioning performances. Could you integrate these kind of considerations in your benchmark for subgraph alignment ? That would also provide insights on the robustness of RGW to these hyperparameters inherent to all transport problems.


[A] Liu, Weijie, et al. ""Robust Graph Dictionary Learning."" The Eleventh International Conference on Learning Representations. 2022.

NB: the citation model seems to be the wrong one for Neurips, currently names and dates instead of simple numbers.

*Update after rebuttal : The authors have addressed most of my concerns through their rebuttals and discussions. Considering that the authors have undertaken to amend the paper and supplementary material accordingly, I increase my grade from 5 (borderline accept) to 7 (accept).*

Limitations:
I think discussing the weaknesses I've mentioned and answering my questions detailed above would help identify and work around the other limitations of their current work. If the authors manage to address these, I will gladly increase my rating. 
This work has no negative societal impact.

Rating:
7

Confidence:
5

";1
lENeWLXn4W;"REVIEW 
Summary:
This paper presents a novel hyperparameter tuning method in the presence of a privacy budget: linearly extrapolating from observations with very low privacy loss.

Soundness:
1

Presentation:
3

Contribution:
3

Strengths:
The core technique presented here is certainly interesting and deserving of future study. The paper tackles an issue which is often unaddressed in the literature on training DP models: that of choosing hyperparameters subject to a privacy budget. This problem itself is also deserving of further study.

Weaknesses:
* A primarily empirical paper will live and die with the strength of its baselines (as well as its upper bounds in a case like this one where upper bounds on the efficacy of the technique can be computed). The baselines here are insufficiently strong, and do not seem to reflect the statements in the cited papers. The core technique _could_ be a component of a strong paper, but this paper is not it.

* Some baseline issues: the citation problems with [51], [52] (detailed below). Lack of comparison to the 'naive baseline' of directly applying gaussian mechanism to results of grid search, say given known training statistics / optimal hparam values for nonprivate datasets (to avoid infinite regress, and here not so much of a problem since the experiments are all focused on public feature extractor settings). Lack of clear comparison to the 'upper bound' of _forgetting_ about the privacy cost of hparam search, which _should_ be an upper bound in _all_ scenarios considered here (IE, performing a sufficiently large grid search directly targeted at the problem at hand).

* On [51]/[52], I see the reporeted CIFAR10 numbers from [51] as 98.8\% at $\epsilon=1$ and 98.9 at $\epsilon=\infty$ (table 1 of [the arxiv version](https://arxiv.org/pdf/2211.13403.pdf)). Is there a typo in figure 2 of the paper under submission? Similarly, [51] seems to claim 88.1\% and 90.6\% at the $\epsilon=1, \infty$ for CIFAR-100. I uncovered these discrepancies since the paper under submission seemed to present implausibly strong results to me--e.g. it should be _impossible_ to achieve at epsilon=1 what none of the cited papers achieved at epsilon=\infty just by tuning hyperparameters (see figure 2). 

* The statements of timing on Imagenet seem wrong? The cited paper [51] seems to be pointing to a version from Nov 2022, clicking through to [52] seems to show a version uploaded in May 2022--so where are Jan 2023 and 'within the last month' coming from?

* Some more consideration required in decomposition of $r$--do we know that random decomposition 'is enough'? Presumably it's not, since we _can_ generate an $\eta$ for which the problem will presumably diverge?

Limitations:
Societal impact not immediately applicable.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper proposes a linear scaling rule for finding the optimal value of the learning rate and number of training steps for differentially private SGD (DP-SGD). The idea is simple, small amount of privacy budgets are allocated for two initial DP learning rate optimization procedures, and then the values are extrapolated to bigger epsilon-values using linear scaling (as a function of epsilon). The work is mostly experimental, and the experimental results e.g. with CIFAR-10 show that for epsilon between 0 and 1, the scaling rule seems to nicely fit the optimal values found by the grid search. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The idea seems very interesting and novel.
- The paper is mostly written well and is easily readable.


Weaknesses:
- The technique is restricted to optimising the learning rate and the length of the training. I wonder if similar extrapolation (perhaps more generally polynomial extrapolation) could be used to find optimal the optimal hyperparameter values for other hyperparameters.

- The technical part could be written more carefully. It remains unclear whether you use RDP or GDP. The hyperparameter tuning cost of the method by Papernot and Steinke is in terms of RDP, but you list theoretical results in terms of GDP. In the end of Alg. 1 you write that the total cost is ""$\varepsilon_f + \varepsilon_0 + \varepsilon_1$"". Is that approximate DP? In case you use the classical composition result where you just add up the privacy parameters, what happens to the $\delta$-parameters?

- Some conclusions are a vaguely formulated/confusing. On p. 7 you have the subtitle ""Linear Scaling is robust to distribution shifts"", but then you seem to show and also claim in the subsequent text that DP itself is robust to distribution shifts. Somehow the message is vague here.

- The contribution remains too thin in my opinion. There is really no theoretical or even heuristic explanation for the proposed scaling rule. There two theoretical results given, a GDP composition result (which is well known and should be cited as such) and another result of which importance I find difficult to judge.


Limitations:
Some of the limitations are discussed in Section 5 but it could be expanded I think.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This study proposes a new algorithm for privately selecting hyperparameters subject to maximizing the model utility. The new algorithm draws inspiration from the linear scaling rule that suggests increasing learning rate as batch size increases. Given the number of hyperparameters in DP-SGD the proposed algorithm simply scales learning rate and number of iterations as the privacy budget increases. This introduces a new hyperparameter that is selected privately with a portion of the privacy budget while the rest is used to perform the normal hyperparameter search. The study provides brief theoretical intuition for why we can expect this linear scaling rule to more efficiently determine optimal hyperparamters compared to previous methods and extensive empirical evidence on 20 different benchmark datasets. 

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
- One of the first papers to demonstrate improved privacy-utility tradeoffs that takes hyperparameter tuning into account. This is substantial as the field has mainly focused on evaluating the privacy-uility tradeoff without considering the privacy cost of hyperparameter tuning. As we move towards more practical implementations, this will be necessary. 
- Clever use of the linear scaling rule to perform hyperparameter search and the resulting algorithm is simple to use. 
- Extensive empirical evaluation and insightful analysis. For example, very few analyses have been done on the intersection of DP and distriutional shift. Yet, this linear scaling rule that is proposed holds in the presence of distribution shift. 


Weaknesses:
- “We are 165 the first to show that DP-SGD is capable of learning to handle distribution shifts without using any 166 techniques from the distributionally robust optimization (DRO) literature” -> There are a couple of other papers that draw this connection. [1,2]
- Lack of comparison to other private hyperparameter selection algorithms or hyperparameter free private learning algorithms [3, 4]
- Unclear why the initial hyperparameter search can be done with such a small privacy budget even though this is a key factor driving the performance of the algorithm.

[1] Kulynych, Bogdan, et al. ""What you see is what you get: Distributional generalization for algorithm design in deep learning."" arXiv preprint arXiv:2204.03230 (2022): 13.
[2] Hulkund, Neha, et al. ""Limits of Algorithmic Stability for Distributional Generalization."" (2022).
[3] Mohapatra, Shubhankar, et al. ""The role of adaptive optimizers for honest private hyperparameter selection."" Proceedings of the aaai conference on artificial intelligence. Vol. 36. No. 7. 2022
[4] Koskela, Antti, and Tejas Kulkarni. ""Practical differentially private hyperparameter tuning with subsampling."" arXiv preprint arXiv:2301.11989 (2023). 


Limitations:
The paper does address the technical limitations of the paper (specifically the assumption of access to public and private data). The main improvement for the limitations is to address the comparison to other tuning algorithms or optimization algorithms that don’t require as much tuning. 



Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a new method to conduct hyper parameter tuning for DP stochastic gradient descent. The method is based on a linear scaling rule, with two pilot runs using small PLBs and a third run chosen based on a linear extrapolation from the first two. The pilot runs are used to establish an estimate of the interpret and the slope that the total step size r would have with the PLB. The author uses this linear scaling rules to demonstrate that it works as well as grid search in optimizing for the accuracy in a suite of benchmark tasks, and attempts to apply this rule to perform empirical analysis on the potential of making existing model architectures DP and the issue of robustness against domain shifts.

My assessment, consisting of strengths, weaknesses, and questions, can be found in the sections below.



Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
The best thing about this paper is that it develops a method based on an intuition that is potentially worthwhile. This intuition is captured in the small paragraph in Section 2, titled Linear Scaling is Intuitive. What the authors have proposed is essentially a dimensional reduction to the hyperparameter search, and the reason why that works, in the sense that what you end up finding may not be so far off from a greedier search, is due to the geometry where you force the updates to be more congruent with each other. The whole idea of a linear scaling would otherwise be rather unremarkable, but if the author can further develop this intuition, formalize it and expand on it, it would contribute some insight to the literature.

Weaknesses:
The most damning weakness of this paper is that it is written without due care. As a consequence, the main results and the accompanying algorithm are not correct as stated. I don’t suggest that the author is not capable of presenting the correct science -- to that question I do not know the answer. However, as things stand, the paper is not ready to be published.

The presentation in the introductory and main result sections wanders seemingly fluidly between epsilon-DP, (epsilon, delta)-DP and Gaussian DP:
1. Definition 1.1 is given in the language of (epsilon, delta)-DP;
2. The DP-SGD Definition is given without a quantification of its DP guarantee at all;
3.  Algorithm 1, which employs the DP-SGD given before, states that its output is epsilon-DP, where an alleged PLB accounting between epsilon and sigma is not supplied. (In reality, a delta would be needed, so the provided guarantee is incorrect to begin with.)
4. Then Proposition 2.1, which concerns Algorithm 1, gives a GDP guarantee in relation to sigma only, where sigma is not constructed as a function of epsilon (or the missing delta) in Algorithm 1;
5. Corollary 2.2 now qualifies Algorithm 1 as (epsilon, delta)-DP, with a one line proof given in the Appendix citing another work and has no substance on its own.

All of the above is confusing at best. For a standard reader, a student coming into the DP world for example, these are not pedagogically informative.

Back to Algorithm 1:
1. It contains four privacy loss budget expressions: epsilon, epsilon_0, epsilon_1, and epsilon_f. Based on the context, am I to infer that epsilon is the sum of the rest of the three? 
2. The quantity r on the 12th line (beginning with Decompose). Is this a generic r, as you use it on line 7, or is it in fact referring to r* on line 9?
3. When you speak of the “decomposition” or r, what is to be found exactly -- eta given r and T (my guess), T given r and eta (please explain), or both eta and T given r (please explain as well)? If my guess is correct, then do we know that the eta found here will automatically satisfy the condition given in Theorem 2.3? 

Line 143 begins with “We apply this theorem to logistic regression.” Then Line 151 continues, “While our theorem only holds for linear models…”. Nothing said between Line 143 and Line 151 constitutes a proof that Theorem 2.3 applies to linear models. This point should either be rectified with a formal analysis or deleted, so as to not be an exaggeration of contribution.

Section 3.1 is misleading and should be thoroughly rewritten to rid all expressions of “randomly”, “sample”, and “uniformly”. The author picked the experimental values. No sampling, particularly random sampling nor uniform random sampling of values took place. It is not clear to what is “r = 75” an approximation (Line 179).

In addition, based on my reading of Section 3.2 I believe it should not be presented as is.  My understanding of what Section 3.2 does is that it uses the linear scaling rule proposed in this work to construct ""accuracy hypotheticals” for the listed models and datasets as well as the domain shift situations, and compare those numbers with existing experimental results. If that is the case, this is a dangerous operation. The linear scaling rule, when used as a heuristic to make tuning faster, is fine as the worst that could happen is that one misses out on the most efficient model tuning. However, the way that the rule is employed in Section 3.2 it is taken as a scientific theory between epsilon and accuracy. The accuracy numbers you get from it is no different than a terribly extrapolated number based on a linear model fitted with two data points. If you really want to use the linear scaling rule to poke at the said questions, actual experiments should be conducted to confirm these extrapolations. Of course, I may have misunderstood what was actually done and in particular, whether actual experiments were performed — although if so, what would be the contribution from the linear scaling rule?


Limitations:
As stated before, I believe the paper is written hastily to the point that the central results presented are incorrect, significantly harming the quality of the contribution and its readability. I am also concerned with the scientific merit of Section 3.2. These points are elaborated in detail in my comment section on Weaknesses. 

Rating:
5

Confidence:
3

";0
nQ84YY9Iut;"REVIEW 
Summary:
Boosting algorithms is an important fundamental question in SLT starting with celebrated AdaBoost algorithm. While the original results consider binary setting and the condition on the weak learner is straightforward, it is not trivial to formulate the same result for classification with more than one classes.

I did not have the chance to go through the proofs carefully due to time constraint. I do not see where there could be a potential mistake that cannot be fixed. I did not read through section 4 and I am not familiar with list learning.

pros:
- interesting fundamental problem
- the paper is well written
- a new condition for multiclass boosting
- relatively simple proof
- connection to list learning

cons:
- the only disadvantage I can see is that this paper would better fit other venues


minor comments and questions:
- should Theorem 1 be stated only for realizable case, since you relying on the compression scheme generalization bound?
- it is better to refer to Theorem 30.2 (than to Corollary 30.3) in Shalev-Swartz and Ben-David since your ""outer"" risk is equal to zero only with probability close to one
- perhaps this is obvious or I have missed it, but how does your condition compare to conditions of existing results for the multi-class boosting?

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
-

Weaknesses:
-

Limitations:
-

Rating:
8

Confidence:
3

REVIEW 
Summary:
This paper tackles the problem of multi-class boosting by proposing a novel definition for weak-learning based on list boosting. The authors define a weak-learning condition for multi-class learning based on the simple principle of slightly better than random guessing. This definition is then used as a stepping stone in order to filter the potential classes for a sample recursively, thus reaching a stage where either the binary weak-learning condition can be applied, or the weak-learning condition BRG leads to boostability. The authors propose several theoretical justifications and definitions for the results presented in the paper.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
# Motivation
Multi-class boosting has received a lot of attention since the introduction of binary boosting, in particular since the extension of the weak-learning condition from binary to multi-class setting is tricky. As such, the novel definition of weak-learning introduced in this paper is quite interesting, especially since its simplicity mirrors the binary one.

# Theoretical results
The strongest point of this paper. The authors propose several theoretical justifications throughout the paper, and the generalization result in Theorem 2 is quite promising, same for the relation between weak PAC learning and list PAC learning in Theorem 3.

Weaknesses:
# Existing frameworks
There are several existing frameworks for multi-class boosting based on specific weak-learning conditions (some of which are cited in the paper). I strongly think that an actual comparison between the proposed framework and the existing ones should have been included. Particularly for methods such as Adaboost.MM and Adaboost.MR and their WL conditions, and the WL framework introduced in ""Multiclass boosting and the cost of weak learning."" [6].

# Experimental results
I'm not entirely sure why no experimental results are proposed in the main paper. There are several multi-class boosting that have been successfully used in practice, despite their WL conditions, as such it is important for new/novel methods to be compared to state-of-the-art approaches. In particular how the proposed method fares against state-of-the-art ones both in accuracy (or other multi-class performance measures) and in runtime. The recursive nature of Algorithm 1 and its dependance on 3 hyper-parameters might lead to prohibitive runtimes, even though several optimizations have been mentioned in the main paper.

# Conclusion and future works
I strongly think that a section on the future works and possibilities would've been more appropriate than the proof of Theorem 2. More globally, the paper could benefit from a better global organization.

Limitations:
The limitation of have not been discussed.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper proposes a novel treatment of traditional (non-gradient) boosting to multiclass prediction problems. In particular it shows that a novel relaxation of the weak learning criterion allows the definition of boosting algorithm with the usual success guarantee of reducing the empirical misclassification rate on the given data sample arbitrarily with high probability. This relaxation requires the base learner to also accept as “hint” a subset of labels and to, for each possible subset size k, to return a hypothesis that achieves an error rate better than 1-1/k with high probability. The proposed boosting algorithm makes use of this property by recursively eliminating candidate labels from the hints of each training example based on the hypotheses produced by previous iterations. Moreover, the paper shows a connection of the proposed theory to list-PAC learning and in particular gives a new characterisation of list-PAC learnability.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- The paper develops a beautifully simple extension of boosting to the multiclass case that, in contrast to previous approaches, does actually work to define a successful strong learner
- The connection to list learnability are very interesting
- The paper is well written and accessible

Weaknesses:
- Papers on classical boosting, in contrast to the additive ensembles produced by gradient boosting, feel rather narrow at this point. In particular, generalisations from misclassification rate to proper statistical loss functions are unclear.
- There is no empirical performance investigation of the proposed recursive boosting algorithm

Limitations:
One potential limitation might be the definition of weak learners that satisfy the weak learning assumption. This is because it seems to require the restriction to different subsets of classes for different inputs. It is not immediately obvious to me how one would incorporate this, e.g., with a decision tree (if it depends on the concrete model at all). 


Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper proposes a generalization of boosting to the multiclass setting. To this end, the authors introduce a new weak learning assumption for multiclass based on a ‘hint’, which takes the form of a list of $k$ labels, named ‘Better-than-Random Guess’ (BRG). the authors present a main boosting method and provide theoretical analysis for PAC guarantees. Finally, the authors demonstrate applications based on the framework of List PAC learning.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- This paper proposes a new weak learning assumption, which encompasses both the binary case (i.e., $k=2$) as well as the cases with $k>2$.
- The main method is well-written.
- This paper provides abundant theoretical analyses.

Weaknesses:
- Although this paper is based on theories, it lacks experiments, including synthetic ones, and there is no conclusion.
- Due to the lack of experiments, there is insufficient comparison with previous works, with only half a page dedicated to it in section 1.2.

Limitations:
.

Rating:
4

Confidence:
1

";1
3FJaFElIVN;"REVIEW 
Summary:
This work is embedded in the research on model-agnostic explanations, i.e., to provide the user an understanding on the outputs of otherwise black-box prediction methods without knowing about the model's internals.  While LIME is a popular approach to solve this problem, prior work has demonstrated LIME to suffer from a strong dependence on the random seed, leading to instability and inconsistency. Since reliable, model-agnostic explanations will be a crucial tool for research and application alike to afford the use of otherwise black-box machine learning models, this paper is tackling an important issue considering LIME's popularity yet evident short-comings. GLIME is presented as a step towards more general, stable and consistent model-explanations. Due to the free choice of its sampling distribution and weights, it is shown how GLIME not only improves on LIME but generalizes over other methods.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1.The method GLIME is presented very clearly. Not only are text and equations supporting the reader in understanding the method well, but its motivation as successor of LIME in terms of stability and local fidelity are easy to follow and well justified by both the presented related work as well as this paper's own evaluation.

2. The unification of various model-explanation methods not only gives the reader an overview of how these methods relate to each other but shows well how GLIME is not only succeeding at stability and local fidelity but also a more general framework then LIME.

Weaknesses:
1. This work is strongly focused on comparing GLIME and its variants to LIME. While the relation of LIME and GLIME are made clear and well supported by the experiments, a more comprehensive overview on the field of explanation methods other than LIME could help the reader to better understand how GLIME fits into current research. Similarly, a discussion of GLIME's short-comings and an outline of Future Work would reinforce the contribution. Along the same line, a discussion on GLIME's quality as model-explainer and human-interpretability of the achieved results would greatly support the claims.

2. While the figures present the concepts and results of this paper quite well, they could benefit from some additional attention and polishing. For example, Fig 1a misses an explanation of the employed colormap. Fig. 1b shows GLIME-Gauss as blue dot in the legend but not the graphic itself. In Fig. 4a, the legend occupies important parts of the plot such that GLIME-Binomial and GLIME-Gauss curves are hard to see.

3. The use of inline-math can at times be overwhelming, e.g., in Theorem 4.2. While it is important to state all the relevant equations and relations, reverting to display- rather than inline-math for the key concepts might help the reader to better digest the underlying theory and assumptions.

Limitations:
Overall I would say it's a technically sound paper, but for a model-explanation paper I am missing the human-side a bit. My understanding is that the improvements the work shows are only meaningful if the consistent/stable explanations are also still good explanations, which I think is not really discussed here.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes a new explanation method, GLIME, which provides more general, stable and local LIME explanations over ML models. Specifically, the authors demonstrate that small sample weights cause the instability of LIME, which results in dominance of regularization slow convergence and worse local fidelity.To address those issues, the authors proposed GLIME framework, which takes a slightly different form of LIME. Through rigorous theoretical analysis and some experiments, the authors can demonstrate that GLIME addressed the above issues and outperformed LIME.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
+ The authors addressed a very important problem, i.e., the well-known instability issue of LIME, and proposed an effective solution to address it.
+ The authors conducted a rigorous theoretical analysis to support their claims, which is very convincing.
+ The overall presentation is very clear and easy to follow.

Weaknesses:
+ The experiments are only conducted on one dataset, i.e., ImageNet dataset. It would be better if the authors could show more results on more benchmark datasets
+ Some properties that are studied in theory for GLIME are not empirically verified. For example, in Section 4.1, the authors showed that their method can converge faster than LIME. Although they provide clear proof for it, the authors did not demonstrate it in experiments. So it would be better if some empirical experiments can cover this.

Limitations:
Not applicable.

Rating:
7

Confidence:
3

REVIEW 
Summary:
In this paper, the authors present GLIME an approach for explainable ai that generalizes LIME. Here, the authors present a framework that encompasses different explainability methods as instantiations of different aspects such as loss function, sampling function, etc. 

The authors also present an analysis of problematic cases for LIME. More precisely, they show how the interaction of the weighting and regularization can cause instability in the explanations and how the samples generated in LIME might not be close to the original sampling space.

The paper then presents different sampling procedures and show empirically how they converge and how stable the explanations are given different parameterizations.


Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
I find the paper insightful, in particular the aspect of the weights becoming all zeros in the standard case for low values of sigma.

The paper is easy to read, technically sound and guides the reader through the concepts in a solid yet understandable way.
The technical contributions are solid and overall provides a good foundation for further research. 

Overall the paper is original and I would say significant as it has the potential to become the standard replacement for LIME.

Weaknesses:
The main concern I have is regarding the empirical section. In particular, you mention two main issues with LIME being the interaction of the weighting with regularization and sub-par sampling. However, it would seem like ALIME does not suffer from those two issues. It would be good to see a comparison of GLIME and ALIME in Fig. 4.

There are some minor improvements I would suggest on the presentation. 
I would suggest you unify the color scheme in Fig. 4 and if possible present as many of the methods in both graphs.
Is there a typo in the norm of the weighting function in line 171? shouldn't it be 2 and 2?
The language on the sub-section in Feature attributions could be improved. 


Limitations:
I don't consider the paper to have potential negative societal impact. 



Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper introduces GLIME as a solution to tackle the issues of instability and diminished local fidelity encountered in the original LIME method. To address the problem of instability, GLIME employs a novel sampling scheme that guaranteed to have a faster sampling rate. The diminished local fidelity problem is resolved by modifying sampling distribution so that nearby samples have higher probability to be sampled.

Disclaimer: I only read the main text and do not check correctness of the proposed sample complexity argument.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. The problem they tackle with is specific and well-formulated. The proposed solution is simple and effective.
2. Their methods are supported by sample complexity analysis. This analysis not only provides mathematical evidence that the original LIME approach necessitates an exponentially large number of samples to achieve convergence, but also demonstrates that their proposed method requires only a polynomial number of samples, offering a significant improvement in efficiency.

Weaknesses:
One weakness would be limited applicability of the proposed GLIME. The paper only demonstrates it can only be applied to the image domain. As other features from different domains, such as texts or categorical features, are not necessarily to be continuous, GLIME equipped with continuous distributions may not resolve the local fidelity issue.

Limitations:
The potential societal impact is not stated in the main paper.

Rating:
7

Confidence:
4

";1
BqTv1Mtuhu;"REVIEW 
Summary:
In this work the authors present novel adaptation of a nonstandard self attention module, like those used in transformers. Previous works have demonstrated that the weighting matrix in self attention can be evaluated using a positive kernel rather than the inner product, which ends up looking like a kernel regressor, which itself can be understood through kernel density estimation. In this work the authors propose using robust versions KDEs in place of KDEs for this method. The authors introduce and describe 3 existing versions of robust KDEs for this and apply their method to several experimental settings on a few data types. 

On image datasets the authors find that, while their method does not improve the standard performance on clean datasets, the methods improve performance when robustness is advantageous, eg. adversarial samples, dataset shift, etc.. On time series and NLP datasets the authors observe some improvement on the clean data as well as unclean data.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The core contribution of this paper is providing a new deep learning methodology that yields substantial performance improvements. The method seems to work on several datatypes and robustness scenarios indicating a fundamental improvement to the general transformer methodology, rather than something that is very application/task/architecture specific. The results also give some insight as to origins of non-robustness of transformer architecture. Its not obvious that key attention values being ""heavy-tailed"" may make a transformer sensitive to word swapping.

An additional strength of this paper is its novelty. This work is brings methods far outside of what one usually sees in deep learning research, which is also a plus. 

Overall the paper is fairly clearly written with a nice overview of a few robust KDEs and introduction to using KDEs for transformers.

Weaknesses:
The primary weakness, in my opinion, is that the proposed method is simply swapping out a component of a transformer with a existing robust version of that component, and is somewhat trivial.

Limitations:
None

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper re-interpret the self-attention mechanism in transformer architectures as a non-parametric kernel density estimator (KDE). Leveraging existing robustness results on classical KDE, the authors develop new variants of transformers; empirical results on a variety of datasets demonstrate how these variants are resistant to adversarial attacks and data contamination.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
This paper provides a simple yet powerful re-interpretation of the self-attention mechanism through the lens of KDE. This re-interpretation allows to leverage results from robust KDE to design better transformer architectures. Kernel methods are notably amenable to theoretical analysis; therefore, links between transformers and kernels as the one proposed in this paper can help sheding lights on justify the empirical successess of transformer architectures. The paper is in general well-written and its structure is easy to follow. 

Weaknesses:
Links between Transformers and kernel methods have been proposed in prior literature, see for example [1]. Although an extensive literature review is proposed by the authors, some important references such as [1] are omitted. 

**References**
[1] Tsai, Yao-Hung Hubert, et al. ""Transformer dissection: a unified understanding of transformer's attention via the lens of kernel."" arXiv preprint arXiv:1908.11775 (2019).


Limitations:
The authors have discussed limitations of their work.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper addresses robustness of transformer architecture in both natural language and vision contexts.
Motivated by classical nonparametric estimation problem, the authors propose a procedure to robustly estimate the self-attention values. While not loosing too much in performance on the original data, the method obtains improvements on corrupted data, with several benchmarks considered. The paper proposes a number of methods to address an important problem of robustness to adversarial contaminations and outliers.

Pros:
- sound experimental results. the proposed method beats sota on a number of benchmarks.
- ablation study is included in the appendix
- comparison of time

Cons:
- I did not understand from reading the introduction whether this method is a post-hoc modification or requires in-processing training
- the theoretical contribution is questionable, I have some questions for Theorem 1 in the appendix, see below
- lack of originality, the strongest part of the paper is experimental results.

minor comments:
- line 84: it is best to refer to Nguyen et al to motivate using this form of regression
- line 84: usually conditional mean is equal to zero E[eps | k] = 0
- line 86: I suggest to make a precise statement that the derivations only hold for Gaussian kernel earlier.
- line 256 ""We follow..."" correct the grammar in this sentence
- theorem 1 in appendix: the sentence ""by adapting Lemma 1 to uniform case"". How exactly do you adapt this to uniform case without any loss in the upper bound? I don't think it's that easy to turn a pointwise bound into a uniform one, so I reckon you do not have a proof of the claimed theorem.


Questions:

- do the methods require fine-tunning the parameters?

- how do you choose hyper-parameters, such as number of bins in MoM, cut-off parameter in Huber loss, etc?

- it is completely unclear from the paper whether robust modification only concerns with inference time, or it changes the training as well?

- robust estimators such as MoM are typically robust towards a corruptions of finite amount of data points. Does it mean that you can be robust to one pixel modifications? Or even one 16x16 patch modifications?

- in the experiments, does the hyperparameter choice procedure require the access to contaminated data?

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
-

Weaknesses:
-

Limitations:
-

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper introduces fortified transformers that employ robust KDE as an alternative to dot-product attention, self-attention, mitigating the impacts of attacked images. Especially, this paper proposed two variants of robust self-attention mechanisms based on kernel density estimators (KDE). Extensive experimental results demonstrate the effectiveness of the proposed approach on diverse tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- This paper is well-written.
- This paper analyzes the characteristics of the proposed methods, especially their limitation.
- Extensive experiments are conducted on diverse tasks with multiple modalities.

Weaknesses:
- The paper offers a comprehensive study of several methods, but a dominant strategy does not emerge as the results seem to depend largely on the task at hand, as evidenced in the data tables provided.
- Although the stated limitations are acknowledged, a more in-depth analysis of these shortcomings would enhance the persuasive power. 
   - It is acknowledged that RKDE and SPKDE possess computational inefficiencies. Then, how computationally expensive they are than the original ones which are not robust? It would be better to provide the experimental results.
   - Suboptimality is a possible limitation of MoM, then how much the extent of MoM's accessibility of MoM is restricted compared to the originals? How those restricted accessibility affects the performances?


Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper describes and leverages the connection between the self-attention mechanism and KDE estimation to motivate replacing dot-product attention with a robust KDE approach to enable more robust transformer models. The experiments illustrate that the proposed approach can maintain performance on clean data while improving performance on contaminated data.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* Given the wide use of transformer models, the paper tackles an important topic, as robustness to training data contamination can improve their performance.
* The proposed approach is well motivated by the connection to non-parametric regression.
* The experiments show very promising results on both clean data and contaminated data, across 
* The proposed approach does not introduce additional parameters.

Weaknesses:
Practicality of the approach for large datasets: the proposed approach incurs additional computational complexity even with the proposed approximations. The experiments are performed on small versions of the models. It's unclear how well this can be applied to large scale training of transformer models.

Limitations:
The authors accurately describe limitations and potential solutions throughout the paper and in the Conclusion and Future Work section.

Rating:
6

Confidence:
2

";1
3Lx8vMNJZ1;"REVIEW 
Summary:
The authors consider the linear dueling bandit problem, where the regret is measured in terms of Borda regret. The Borda regret function was adopted as that used in Saha et al. 2021a. Different from conventional bandit settings, there is a mismatch between the reward function and the regret function definition in the sense that the expected reward of pulling a pair of arms is p_i,j, while the regret is defined based on the function B(i)+B(j), where B(i) is the Borda score of i-th arm. The authors provide a lower bound on the regret by the construction of hard cases and a matching algorithm based on explore-then-commit. The adversarial setting is also considered, where the EXP3 style algorithm is proposed and analyzed, and shown to match the upper bound.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
+ For both settings, the proposed algorithms are shown to match the upper bounds.
+ The construction of hard cases is interesting. 
 

Weaknesses:
- The experiment results are not very convincing.
- The algorithms are relatively straightforward extensions of the existing approaches
- The constructed hard case is also a relatively straightforward extension of that in Saha et al. 


Limitations:
not applicable

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies the problem of minimising Borda regret for dueling bandits in the generalised linear setting where each pair of arms has a context vector associated with it.

The paper considers both the stochastic setting where the parameter $\theta^*$ used for generating rewards is fixed and the adversarial setting where it is time dependent.

**[Lower bound]** The authors show a hard problem instance for which any algorithm will incur a regret of $\Omega(d^{2/3}T^{2/3})$ in the stochastic setting, $d$ being the dimension of the context vectors.

**[BETC-GLM for stochastic setting]** The authors propose an explore-then-commit style algorithm that first pulls arm pairs based on a G-optimal design for some rounds and then exploits by selecting the arm $\hat{i}$ with the highest MLE Borda score and pulling $(\hat{i}, \hat{i})$. This algorithm incurs $\tilde{O}(\kappa^{-1} d^{2/3} T^{2/3})$ regret, where $\kappa$ is a problem dependent parameter (which is constant for linear bandits).

**[BEXP3]** The authors then propose an EXP style algorithm for the adversarial case. The algorithm is restricted to the linear setting. At each step, it estimates $\hat{\theta}_t$ using the reward obtained in that step, and uses it to estimate the Borda scores of all arms at that step. These scores are used to compute a distribution over arm pairs in the same spirit at EXP. This algorithm incurs a regret of $O((d \log K)^{1/3} T^{2/3})$, which matches the lower bound when $K = O(2^d)$.

Experiments on real and synthetic data corroborate the theoretical findings.



Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
Originality - The Borda regret minimisation problem for generalised linear dueling bandits is new. The authors seem to have adequately cited the related work.

Quality - I have not checked the details in the appendix but the arguments presented in the main paper are inspired from well-known techniques, and appear sound to me. Barring a few issues listed in the weakness section, I believe that the claims are well supported, both theoretically and empirically via experiments on real and synthetic data.

Clarity - The paper is very well written and easy to understand. In particular, I appreciate the explanation for ``Borda reduction'' and why it is not sufficient in Section 3.1.

Significance - The results fill an important research gap.

Weaknesses:
Originality - The algorithms and their analysis have limited novelty. While I understand why Borda reduction does not trivially work, it would be helpful to have a summary of the technical challenges encountered in analysing the algorithms, while highlighting the new ideas that were employed.

Quality - BEXP3 assumes a linear model for $p_{i,j}^t$ and not a generalised linear model. I think this should be clarified in the abstract. It would also be useful to have a discussion on what makes having a link function here hard.

Clarity - Just two minor comments, 
1. Use \citep instead of \citet wherever appropriate (e.g., L19-20)
2. Explaining the utility of using a G-optimal design in Algorithm 1 (to have a uniformly good estimate of $\theta$) will improve the readability of the paper.

Significance - The authors note in their conclusion that their exploration scheme guarantees accurate estimation in all directions, thereby paving way for extensions like top-k recovery and ranking problems. However, the exploration scheme is the G-optimal design, which is not a new contribution. Outside of this, the work seems to have limited impact.

Limitations:
I suggest adding some discussion addressing my concern under ""Quality"" in the weakness section as a limitation of the analysis for BEXP3 (if applicable, I may be wrong in which case please correct me).

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper discusses Borda regret minimization in a contextual dueling bandits scenario, where the context is given in a generalized linear form.
It provides a worst-case lower bound for the stochastic and adversarial learning scenario. The authors develop for both scenarios algorithmic solutions whose asymptotic Borda regret matches these lower bounds (up to logarithmic terms). These solutions are shown to empirically outperform current state-of-the art solutions in experiments.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper tackles an interesting problem. It contributes a novel lower bound for the particular learning scenario and develops algorithmic solutions, which (a) come with log-optimal regret upper bounds and (b) outperform current methods on both synthetic and real-world data.
In my opinion, this paper is well-written, the notation is convenient and the algorithms and proofs seem to be presented in a reader-friendly way.

Weaknesses:
I did not find any weaknesses while skimming the paper and the proofs.

While reading, I've collected some the following typos/minor suggestions:
- 170f.: $\lambda_{min}$ has not formally been introduced.
- 216: Refer here to the appendix for the proof of Thm. 4. Or did you mention before that proofs are to be found in the appendix?
- 240f.: Here, you call the G-optimal design $\pi^\ast$. For consistency, denote it also like this in Alg. 1 and the further discussion?
- 297: Refer to Thm.4 for convenience?
- 309: ""we study""
- 331: [the] EventTime dataset
- 338: for [the/a] linear setting
- 457: be satisfied
- 522f.: \P_{\theta,\A} has not been defined
- 522f.: C=10?
- 529f.: Should averaging be done over $\theta \in \{-\Delta,\Delta\}$? Also, in 532ff. etc.
- 532f.: Why is ""sign"" in bold?
- 552f.: Regarding your identity of $V_\tau$ here, you seem to have switched from column- to rows for $\theta_{i,j}$ in comparison to l. 171f.
- 568f.: For readability, formally introduce $\succeq$
- 582: $|\mathrm{supp}(\pi)|$

Limitations:
The authors adequately addressed the limitations of their work.

Rating:
7

Confidence:
3

REVIEW 
Summary:

In dueling bandits, each pair of arms corresponds to some unknown probability $p_{i,j}$ where $p_{i,j}$ is the probability arm i is ranked higher than $j$. The learner sequentially chooses pairs of arms and receives a noisy result as to the ordering of the pair, i.e. Bernoulli$(p_{i,j})$. This paper considers dueling bandits under a generalised linear model where each pair of arms $i,j$ corresponds to a known context vector. The probability that $i$ is ranked higher $j$ is then the product of this context vector with some unknown vector $\theta^*$, passed through a link function. The goal of the learner is to pull arms with high ranking as much as possible, the regret on a single pair of arms is given as the average Borda score between the arms, and the learner aims to minimise cumulative regret. This setting has been considered previously, however, under the assumption that a hidden coherent ordering is forced on the arms. The key contribution of this paper is to relax this assumption and have essentially no constraint on the $p_{i,j}$s. In addition to the above setting, the authors consider a adversarial setting where the unknown vector $\theta^*$ is allowed to change round by round. Adversarial dueling bandits has been studied in the literature, the main contribution of this section is to extend this analysis to the contextual setting.  

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper is well written and easy to read. Sufficient treatment is given to previous works and care is taken to illustrate how the results of this paper are novel in comparison. In section 4 the class of hard instances is clearly constructed and the following discussion gives good intuition as to why explore then commit algorithms can work in this setting, which is in itself an interesting phenomenon. The results give a complete treatment of the setting, with matching upper and lower bounds, up to log terms. In their experiments the authors consider a variety of benchmarks, with application to synthetic and real world data. 

Weaknesses:
I do not see why $\delta$ is given as input to the algorithm, it is not taken as a parameter but rather passed to the exploration phase $\tau$. 

The discussion in section A.1 of the appendix is vital to understanding the novelty of this work in comparison to Saha 2021, it is a shame that this section is not present in the main text. 



Limitations:
For the BETC-GLM algorithm, the authors acknowledge the potential limitation of not having access to the exact G optimal design. They suggest a well known sub routine to estimate the G optimal design and describe the additional error term that would incurred by this. The authors discuss and provide compelling arguments as to whether the assumptions are reasonable. 

Rating:
7

Confidence:
3

";0
R4ivHjNi8V;"REVIEW 
Summary:
This paper addresses the challenges associated with the tensor nuclear norm, which is a proxy for the low-rank property of tensors. Existing transformations for the tensor nuclear norm lack adaptability and theoretical guarantees due to fixed or non-invertible nonlinear approaches. This paper introduces a practical and data-adaptive framework and provides an exact recoverable theoretical guarantee. Experiments are included to support the theoretical guarantees.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper contains solid theoretical results for a very challenging problem. It provides efficient algorithms for recovery that are explained well and clearly. The experiments include both real and synthetic data.

Weaknesses:
The paper is dense to read in some places, like in the slurry of definitions at the end of Section 2. This is likely due to space limitations, but reduces the readability of the paper. The main results include some pretty strict assumptions like incoherence and uniformly distributed support sets. Although some of these are also used in other results, it could be discussed further what kinds of tensors have these properties in practice and how sensitive the methods are to these.

Limitations:
The authors discuss interesting future work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper presents a new approach for learning a data-adaptive and learnable column-orthogonal matrix (COM) transform for tensor nuclear norm (TNN) minimization. The authors show that the proposed transform can capture the low-rank structure of tensors more effectively than existing fixed or nonlinear transforms. They also provide theoretical guarantees for the exact recovery of tensors under the COM transform. The proposed method is applied to tensor completion and tensor robust principal component analysis tasks and achieves better performance and efficiency than several state-of-the-art methods.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The authors build up an adaptive tensor nuclear norm, and develop tensor completion robust tenor PCA model. 
2. To guarantee the theoretical correctness of the proposed models, the authors analyze two theorems to ensure the exact recovery capability.

Weaknesses:
1. The proof architecture is regular and simple and has been used in [1,2], thus the theoretical novelty of this paper is rather limited.
2. The adaptive TNN model has been already proposed in many related works, but the authors missed some important related references, see the linear invertible TNN-based RPCA [1,2], the nonlinear transform TNN [3], and the subspace denoising strategy NGmeet [4]. Thus, a related works section should be considered to give detailed differences between these methods and the proposed ATNN.
3. In the experimental section, the authors only compare the computation-cost methods and neglect the related adaptive TNN methods.

[1] Lu C. Transforms based tensor robust PCA: Corrupted low-rank tensors recovery via convex optimization[C]//Proceedings of the IEEE/CVF International Conference on Computer Vision. 2021: 1145-1152.

[2] Lu C, Peng X, Wei Y. Low-rank tensor completion with a new tensor nuclear norm induced by invertible linear transforms[C]//Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2019: 5996-6004.

[3] Yisi Luo, Xile Zhao, Deyu Meng, and Taixiang Jiang, ‘‘HLRTF: Hierarchical Low-Rank Tensor Factorization for Inverse Problems in Multi-Dimensional Imaging,’’ IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR), 2022.

[4] He W, Yao Q, Li C, et al. Non-local meets global: An integrated paradigm for hyperspectral denoising[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2019: 6868-6877.

Limitations:
Please see the above Weakness section.

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper introduces a new method of learning a transformation that can make the tensor nuclear norm better capture the low-rank structure of tensor data. The method is fast, data-adaptive, and has a theoretical guarantee of being reversible. The paper shows that the proposed method works well in experiments.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. This paper presents a clear and reasonable idea that is well written and easy to understand. 

2. The paper proposes an adaptive transformation matrix, which has a better expressiveness than methods with fixed transformations. 

3. The paper proves that ATNN has exact recovery capability under certain conditions. 

4. The paper conducts extensive experiments to validate the theoretical results and demonstrate the effectiveness of ATNN.


Weaknesses:
1. The performance of ATNN is limited by the linear transformation. As shown in Table 3, S2NTNN achieves higher accuracy and faster speed than ATNN. Since the theoretical analysis of deep neural networks is challenging and remains an open problem, the theory of S2NTNN is harder to establish.

2. ATNN has an advantage over TCTV in terms of computational efficiency for color video completion task, but it cannot surpass TCTV in terms of overall model performance.

3. Typos: Eq.(2) $A^{(k)}\rightarrow A^{i}$


Limitations:
See Weaknesses.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose a novel Adaptive TNN (ATNN) for tubal rank (t-SVD)-based low-rank tensor recovery and apply it to Tensor Robust PCA and Tensor Complete problems. Theoretical recovery guarantees have been established which is technically sound. However, tubal rank is known as an analog to matrix rank. Compared to CP rank and Tucker rank, the theory of tubal rank is relatively easy to work with. That said, the reported theoretical results are solid but no surprise. In my option, the proposed ATNN is an analog to matrix sketch, although fast, it is not equivalent to the standard t-SVD.I also think 'learnable' is a misleading term in this paper.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
Solid analysis reslut.

Weaknesses:
1. In my option, the proposed ATNN is an analog to matrix sketch, although fast, it is not equivalent to the standard t-SVD.Just like matrix sketch is not equivalent to the stardand SVD. I don't see this discussed in the paper.

2. When a matrix algorithm is learnable, something like 
      Deep convolutional robust PCA with application to ultrasound imaging, ICASSP 2019.
      Learned Robust PCA: A Scalable Deep Unfolding Approach for High-Dimensional Outlier Detection, NIPS 2021.
come to my mind. To me, 'learnable' is a bit misleading in the paper. Anyhow, the learnable feature of the ATNN is neither discussed enough nor numerically demonstrated through the paper.

3. The reported numerical results in Tables 3-5, the proposed approach doesn't show sufficient emperical advantage.

4. The notation is really messed up in some part of the paper. For example, in (12), \mathcal{E] should be \mathcal{S} (or the other way around). In (13), \mathcal{S} should be \mathcal{E}.

Limitations:
No negative societal impact was found.

Rating:
4

Confidence:
3

";0
aS2Yl8s5OG;"REVIEW 
Summary:
The authors propose a new approach called Subset Adversarial Training (SAT), which differs from traditional adversarial training methods that generate adversarial examples on the whole training set. Instead, SAT applies adversarial training on a subset of the training data. They studied two variants of subset adversarial training (CSAT and ESAT). They found that robust training in one class could generalize to other classes that were not adversarially trained, which is surprising. The also found that ESAT, where they adversarially trained on harder examples, gives surprising boost to downstream robust performance with much less data.

The paper also discusses the concept of loss balancing, which is used to counteract an imbalance between the adversarial subset and non-adversarial subset when the training split is not even. The authors found that loss balancing is important for the adversarial robustness transfer observed.

In conclusion, the paper presents a novel approach to adversarial training that could help us better understand the underlying mechanism of robust learning as well as having potential implication to more efficient adversarial training.

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
- The setting of experiment is interesting. It is surprising that adversarially training on a single class yields adversarial robustness to other classes. The originality of the experiment is strong.
- The experiments has demonstrated possibility to decrease the cost of adversarial training.
- The paper is also very clear with thorough experiments and analysis

Weaknesses:
- Even though the finding is interesting, I think the paper could do more in terms of understanding its implication to  robust generalization. What does robust generalization to other classes imply or show about the process of adversarial learning?
- While the experiment demonstrate possibility of decreasing cost of adversarial training, it doesn’t demonstrate this in more challenging scenarios. I understand that the paper’s intention is to understand how adversarial generalization happen as opposed to achieving the best performance, but if it is the paper’s intention to gain further understanding of the adversarial training process, I am hoping for more analysis and comments about its implication to robust generalization.

Limitations:
The authors have adequately addressed the limitation of the potential negative societal impact of their work.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper demonstrates an interesting observation: when we conduct adversarial training, we can only choose to generate adversarial examples on a subset of the training data, if this subset contains the hardest examples, then adversarial training on a subset can achieve competitive performance in robustness over the whole dataset. In addition, models trained in such a manner demonstrate good transferable feature.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The observation is surprising and interesting, which indicates the transferability of adversarial examples across different classes.

2. The authors conduct comprehensive experiments on various datasets to validate the findings.

Weaknesses:
1. One major concern is the contribution, the proposed method neither improve the robust accuracy (or clean accuracy) nor improve the training efficiency (because SAT still uses PGD-7 to generate adversarial examples, which is inefficient).

2. All the experiments are conducted on the $l_2$ bounded adversarial perturbations, more types of adversarial perturbations should be included, especially the $l_\infty$ bounded ones which is popular for benchmarking. In addition, for CIFAR10, the adversarial budget $\epsilon = 0.5$ is very small when considering the dimensionality of the input image. Experiments based on larger adversarial budgets should be included, e.g. $\epsilon = 2$ for CIFAR10.

3. Similar to the first point, the experiments does not demonstrate the advantages of the proposed method. In addition to adversarial training, is the method general and compatible to other popular robust learning method, such as TRADES? Is the observation the same in this context?

4. It would be better if the authors can provide some intuition or explanations for the observations in this paper.

Limitations:
The limitations and the broader societal impacts are not adequately discussed in the current manuscripts, although ethnicity should not be an issue for general research like this work.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors proposed the use of Subset Adversarial Training (SAT), a technique that splits the training data into A and B and constructs AEs only for data in A. Using SAT, they demonstrate how adversarial robustness transfers between classes, examples, and tasks. The authors report several insights: 1) that they've observed robustness transfers by difficulty and to classes in B 2) hard examples to provide better robustness transfer, and 3) Generating AEs on part of the data (e.g, 50%) is enough to get the standard AT accuracy.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The paper is relatively easy to follow
2. Existing empirical results seem sound

Weaknesses:
1. If I understand correctly, the experiments were done only on L2, even though the most common AT is done using L_inf. Can the authors present results using L_inf? 
2. I'm missing many details about the AT process, you need to be much more specific for reproduction purposes. which AT is the baseline? did you try other methods? which method did you use? Madry's/TRADES/Other? the paper needs to be much clearer. Many important implementation details are missing.
3. It's hard to validate the results without supplying code/models.
4. The novelty is marginal, due to the fact that much prior art exists on the transferability of AEs, revisiting hard examples, or pruning a part of the training examples throughout the training. The paper will benefit from a comparison of these methods to SAT, so we can see the differences in performance/resources requirements/etc.

Limitations:
No discussion on limitations, I suggest the authors to add one.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This work considers the transferability of adversarial robustness for partially adversarially trained models. The authors examine 3 variants of subset adversarial training (SAT): Class SAT, where only samples from selected, difficult classes are adversarially perturbed in training; Example SAT, where only examples with the highest predictive entropy are perturbed; Source-task SAT, where SAT-trained, robust models are fine-tuned on downstream training sets and evaluated for downstream adversarial robustness. They further draw connections between SAT and loss balancing, thus proposing a method for sample-efficient, low-cost adversarial robustness transfer between datasets in foundational settings.  
  
The authors report interesting insights from various experiments. From CSAT, it is noted that difficult classes transfer best; class-wise transfer gains are asymmetric; and robustness transfers between seemingly unrelated classes. From ESAT, the authors concur with previous findings that harder examples contribute more to training robust models; the gain in robust accuracy is more rapid than CSAT with respect to the size of subset A; hardness rankings suffer from a possible lack of sample diversity and its performance is matched by random rankings. From S-SAT, they find that SAT on the source dataset with only 30% of AEs can match the robustness transfer gains using normal adversarial training, on the downstream dataset; both clean and robust downstream accuracies are transferred and they are positively correlated under appropriate loss balancing.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. **Data efficiency.** The proposed SAT greatly reduces the amount of data required for adversarial training, which is promising for resource limited or real-world settings. ESAT with only 50% of AEs matches normal AT performance; S-SAT with 30% of AEs matches AT (on the source dataset) as well.   
2. **Loss balancing.** I appreciate the discussion on connections between the SAT formulation and loss balancing. I also recognise that both clean and robust accuracy transfer positively  from source to downstream tasks, under S-SAT with appropriate loss balancing, which is rare and difficult for adversarial training.  
3. **Experimentation.** The experiments are relatively thorough (except that only ResNet-18 and ResNet-50 are SAT-trained) and many details (such as the inter-class robustness transfer statistics for CSAT, or the difficulty rankings of classes) are provided, which give rise to valuable insights.  
4. **Presentation.** The presentation of this work is exemplary. It is well-organised, logically-coherent and persuasive.

Weaknesses:
### 1 Cost and efficiency  
1.1 $\hspace{5pt}$ SAT relies on meticulous pre-processing to discover hard classes and requires access to the per-epoch model weight snapshots of a normally trained classifier, to compute the difficulty metric of Equation 3. This shifts the partial cost of adversarial training to the pre-processing stage and can be costly for large models / datasets in foundational settings.   
1.2 $\hspace{5pt}$ More importantly, SAT relies on this non-robust classifier, presumably with identical architecture and training data as the target model for subset adversarial training. This means that all the pre-training and loss balancing procedures have to be repeated for every single new model-dataset combination, which might end up being more costly than normal adversarial training.    
1.3 $\hspace{5pt}$ One also notes that hardness ranking is important for the guaranteed performance of SAT, especially for CSAT and to a lesser extent for ESAT (where the authors acknowledge that it is ""possible to accidentally select poor performing subsets"", as per the easy rankings experiment).    
### 2 Experimentation
2.1 $\hspace{5pt}$ SAT is only verified for ResNet-18 and ResNet-50, which is a non-negligible shortcoming, for the reasons described above.   
2.2 $\hspace{5pt}$ Does SAT hold for other convolutional and non-convolutional architectures of variable capacity?   
2.3 $\hspace{5pt}$ Is it not cost and time prohibitive to run SAT for more than 2 baseline models? Would this also be a barrier that impedes the practical adoption of SAT?  
### 3 Scaling up
3.1 $\hspace{5pt}$ SAT experiments are notably performed on smaller datasets with fewer (or a subset of) classes. The computational complexity of SAT seems to scale non-negligibly with the number of classes (CSAT) and the size of the dataset (ESAT), which is not ideal on real-world datasets with fine-grained labels in foundational settings.   
3.2 $\hspace{5pt}$ As aforementioned, even considering the S-SAT setup (where one does not need to do SAT on every downstream dataset), it is very costly to add a new model to the SAT experiments because of the method's dependence on snapshots of a normally-trained, non-robust version of the same model, for hardness rankings and loss balancing.

Limitations:
The authors have adequately addressed the societal and ethical limitations of their work. This work strives to improve the foundational adversarial robustness of AI systems in practice; experimental and implementation details have been documented.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper investigates the transferability of adversarial robustness among different classes and different examples. Different from previous studies, authors split the training dataset into two groups and only apply adversarial training on one group while another one using clean training. Based on experiment results, authors obtain several interesting observations, including classes without adversarial training can still have some capacity to defense against adversarial attacks, hard classes and examples can provide better robustness transferability than easier ones, and only 50% of training data is sufficient to recover the performance with vanilla adversarial training method in terms of robustness, etc.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. This paper explores the transferability of adversarial robustness from a new perspective and hence proposes a novel training mechanism to study.
2. This paper conducts a series of experiments to investigate the robustness transferability. Based on experiments, some interesting observations are obtained, which may give some new insights for future works.

Weaknesses:
1. The motivation of this work is not quite clear. Although authors find that some classes still can obtain capacity to defense against adversarial attacks without adversarial training, data samples of these classes are available in the training dataset. Hence, applying adversarial training on all data samples of all classes directly can achieve much better robustness, comparing with the transferred robustness obtained in this paper. Hence, it's not clear why authors study this kind of transferability when all training data are available and which scenarios are suitable for the problem studied in this paper.
2. Based on experimental results, authors claim that utilizing only half training data can achieve comparable robustness performance with vanilla adversarial training methods. However, related experiments only report robustness of different methods. Considering the trade-off between clean accuracy and robustness in adversarially trained models, it would be better if corresponding clean accuracy of each method can also be provided.

Limitations:
Questions about how to apply the proposed training mechanism and observations obtained from experiments in real applications to boost the robustness of models need to be discussed in detail.

Rating:
5

Confidence:
3

";0
yloVae273c;"REVIEW 
Summary:
This paper studies offline reinforcement learning (RL) with linear function approximation and partial data coverage. The authors propose a primal-dual optimization method based on the linear programming (LP) formulation of RL. They prove a $O(\epsilon^{-4})$ sample complexity in both discounted setting and average-reward setting.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	The algorithm proposed in this paper only requires near-minimal dataset coverage assumption, which is important in offline RL.
2.	The paper also considers average-reward offline RL, which is often neglected by literature.
3.	I like the table for comparison to previous work, which makes the presentation more clear (although I think there is some missing important literature, which I will mention in the weakness section).
4.	The proposed algorithm is both computationally and sample efficient.

Weaknesses:
1.	The first concern is the ‘linear function approximation’ setting, which is restricted. Actually, the main motivation that this paper studies function approximation beyond tabular settings is large state (or action) spaces in practice. However, in real settings, the linear function approximation assumption hardly ever holds. Even in Table 1, many algorithms in previous work apply to general function approximation, which further makes the setting studied in this paper restricted.
2.	Algorithm 1 in this paper achieves a $O(\epsilon^{-4})$ sample complexity. This is in terms of expectation (as shown in Theorem 3.2) instead of high probability. The previous results that the authors are comparing to are high probability bound (e.g., [1,2]), so it would be more comparable if the authors could also show a $O(\epsilon^{-4})$ sample complexity bound under high probability. Also, since the previous work studies general function approximation while this paper studies only linear function approximation, it is hard to say that a $O(\epsilon^{-4})$ sample complexity bound in linear function approximation setting is better than a $O(\epsilon^{-5})$ bound in general function approximation setting. Moreover, [3] achieves the near-optimal sample complexity $O(\epsilon^{-2})$ with near-identical settings of [1,2]. (So I disagree with the statement that ‘It is very important to notice that no practical algorithm for this setting so far, including ours, can match the minimax optimal sample complexity rate of $O(\epsilon^{-2})$’. Therefore, a $O(\epsilon^{-4})$ in linear function approximation is not that attractive compared to previous work.
3.	The authors use an LP formulation of offline RL. I think it would be better to compare to other work using LP formulation, e.g. [4,5], where [4] is computational and sample efficient under partial data coverage and general function approximation, and [5] achieves near-optimal sample complexity under similar settings.
4.	The authors compare the computational complexity. However, it is not that direct to compare an $O(n)$ complexity in linear settings to a $O(n^{7/5})$ complexity in general settings. If the authors really want to demonstrate that their algorithm has better computational complexity, it would be better to do some simulations in the same environment (even in some toy examples).
5.	Another advantage that the authors claim is that their algorithm could be adapted to average-reward setting. However, neither did the authors emphasize and explain the importance and challenges of average-reward settings, nor discuss why (or whether) previous work could not be adapted to average-reward settings. I suggest the authors discuss this a bit more.

**References**

[1] Xie, T., Cheng, C. A., Jiang, N., Mineiro, P., & Agarwal, A. (2021). Bellman-consistent pessimism for offline reinforcement learning. Advances in neural information processing systems, 34, 6683-6694.

[2] Cheng, C. A., Xie, T., Jiang, N., & Agarwal, A. (2022, June). Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning (pp. 3852-3878). PMLR.

[3] Zhu, H., Rashidinejad, P., & Jiao, J. (2023). Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning. arXiv preprint arXiv:2301.12714.

[4] Zhan, W., Huang, B., Huang, A., Jiang, N., & Lee, J. (2022, June). Offline reinforcement learning with realizability and single-policy concentrability. In Conference on Learning Theory (pp. 2730-2775). PMLR.

[5] Rashidinejad, P., Zhu, H., Yang, K., Russell, S., & Jiao, J. (2022). Optimal conservative offline rl with general function approximation via augmented lagrangian. arXiv preprint arXiv:2211.00716.

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper studied offline RL in linear MDP setting, where the transition and reward have low-rank structures and the feature $\phi$ is known. The authors formulated the problem in a primal-dual way and proposed a gradient-based algorithm. They provided convergence guarantees, which only requires coverage over optimal policy. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper writing is clear and easy to follow.

The discussion and comparison with previous works is very detailed.

The algorithm is computationally efficient. The algorithm design has some interesting points, especially the reparameterization design to avoid knowlegde of $\Lambda^{-1}$ and updates for variables $v$ and $u$.

The coverage assumption seems weaker than previous literatures.

Weaknesses:
I didn't see too much technical novelty in the method and proof.

The setting is linear MDP, which is kind of restrictive.

Convergance rate is kind of far away from optimal.

Limitations:
N.A.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposed an primal-dual framework for offline reinforcement learning in linear MDP Contrary to the more common case of finite horizon, they considered the case of infinite horizon with discounted reward. They reduced the problem of offline reinforcement learning to a problem about solving the saddle-point of a Lagrange form. They designed an algorithm which uses stochastic gradient-based optimization to solve the saddle point. They provide a sample complexity of O(\eps^-4) for both cases of discounted MDP and averaged-reward MDP, and their algorithm is also computational efficient. 

To summarize, the formulation of offline RL into a linear programming problem is very interesting. The proof seems very solid, and I like the comparison for the concentrability constant in the last discussion section. The comparison for the constant C is thorough and very good. 

However, I still have some questions about some details in the main text.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. The formulation of offline reinforcement learning to a linear programming problem is very good.
2. The algorithm is clearly motivated by solving the saddle points of a Lagrange form. The algorithm itself is simple and computationally efficient, with a guaranteed sample complexity for both discounted MDP and averaged-reward MDP.
3. They proposed a new concentrability constant C and compare it to other constants appearing in other literatures about offline RL. I think the understanding of the relationship of these concentrability constant is basically correct and very clearly expressed.
4. The proof seems very solid and the result in averaged-reward case is new.

Weaknesses:
1. I have some question about your comparison to previous results. Your main references are Cheng et al and Xie et al. 

1.1 For Xie et al, the Theorem 3.2 in https://proceedings.neurips.cc/paper_files/paper/2021/file/34f98c7c5d7063181da890ea8d25265a-Paper.pdf implies that their sample complexity is O(1/\eps^2) when applied in linear function approximation. This result is based on assumption3 in their paper. This assumption naturally holds in your paper since you consider linear MDP and they consider the case of 'linear function approximation' (for their difference, see point 2). So it is natural for you to compare your sample complexity to this result, not the O(1/\eps^5) one. [notice that, their algorithm in section 3 is computationally inefficient]

1.2 In Theorem 4.1 in Xie's paper, their sample complexity is O(1/\eps^5) when applied on general function approximation, and O(1/\eps^3) when reduced to linear function approximation case (see paragraph 'Dependence oon T'). Again, their assumption for linear function approximation holds in your case. **This algorithm, however, is computationally efficient.** So you should also compare with this alg with  O(1/\eps^3) sample complexity.

1.3 In Cheng's paper, in theorem 5, their sample complexity seems to be O(1/\eps^3), not O(1/\eps^5). I wonder how you derive their sample complexity in Table one.

1.4 I am not sure how you get the O(n^{7/5}) computational complexity for Xie's paper. Could you derive it in more detail?

2. I think in many places in your paper, you confuse the two terms: linear MDP and linear function approximation. Your case is called linear MDP instead of linear function approximation, so I suggest you changing the wrong terms. For reference, both Xie's paper and Cheng's paper consider the 'linear function approximation' case.

Limitations:
/

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies offline reinforcement learning with linear function approximation. They propose a primal-dual algorithm, formulating linear RL into a minimax problem and solving it with gradient descent-ascent. Sample complexity analysis is provided for infinite-horizon discounted and average-reward MDPs, where the rate is $O(\frac{1}{\epsilon^4})$ for both settings.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The algorithm is primal-dual and thus easy to implement in practice.
2. The paper provides rigid theoretical analysis.

Weaknesses:
1.  The newly defined coverage ratio $C_{\phi,c}$ is a little strange when $c\neq \frac{1}{2}$. For example, when we choose $c=1$ and thus we don't need the knowledge of $\Lambda$, the coverage ratio $C_{\phi,1}=\sum_{x,a}(\frac{\mu^*(x,a)}{\mu_B(x,a)})^2$. Then when $\mu^*=\mu_B$, $C_{\phi,1}$ will become $|X||A|$. However, in the literature, when the behavior policy is the same as the optimal policy, the coverage is typically 1. The authors claim that we can estimate the $\Lambda$ via the offline dataset so that we can choose $c=\frac{1}{2}$, but do not provide any theoretical analysis about this point. I will be more convinced if the authors can give more rigid proofs for this method.
2. The sample complexity is worse than the typical rate $\frac{1}{\epsilon^2}$.

Limitations:
None.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper considers the problem of offline reinforcement learning (RL) for linear Markov Decision Processes (MDPs) under the infinite-horizon discounted and average-reward settings. The authors propose a primal-dual optimization method based on the linear programming formulation of RL, which allows for efficient learning of near-optimal policies from a fixed dataset of transitions under partial coverage. The proposed algorithms improve the sample complexity compared to previous methods from $O(\epsilon^{-5})$ to $O(\epsilon^{-4})$ under the discounted setting and provide the first line of result in the average-reward setting with realizable linear function approximation and partial coverage.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1 The proposed algorithm improves existing algorithms in both statistical efficiency and computational efficiency under the discounted reward setting with the linear function approximation (we note the baseline may handle problems beyond the linear MDPs).

2 The algorithms presented in this paper do not explicitly leverage the principle of pessimism, but focus on the linear programming formulation of MDP, and rely on a new reparametrization trick extended from the tabular case. The technique itself seems to be novel to me.

3 The algorithms present the first line of work for the offline average-reward MDP.  

4 The paper is easy to follow, with a thorough comparison with existing work that clearly positions the results in the literature.

Weaknesses:
1 I am confused about the requirement of $\Lambda$ to be invertible (line 140) as this seems to be very closely related to the uniform coverage condition where we assume that the smallest eigenvalue of $\Lambda$ is lower bounded from zero. I am wondering what is the key difference between them. Can you elaborate on this with some intuitions or examples?

2 The authors discuss the relationship between the coverage condition considered in this paper and that of [1] and show that the coverage condition is a low-variance version of the standard feature coverage ratio if $c=1/2$. However, in this case, the algorithm explicitly uses $\Lambda$, while the PEVI proposed in [1] does not. In contrast, $c=1$ leads to a worse bound but we do not need the knowledge of $\Lambda$. Could you provide a more detailed characterization or example to illustrate the difference between these two cases?


typo: line 328, $\epsilon^2 \to \epsilon^{-2}$

[1] is pessimism provably efficient for offline rl

Limitations:
yes

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors investigate offline RL in linear MDPs and introduce a novel LP-based method. They assert that their proposed approach achieves the lowest sample complexity of $O(1/\epsilon^4)$ among computationally efficient algorithms. In comparison, existing computationally efficient algorithms can achieve $O(1/\epsilon^5)$. Additionally, the author's theory can be extended to the average reward setting.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* To the best of the author’s knowledge, in offline linear MDPs, the result in the average-reward setting is novel. 
* The LP formulation in linear MDPs is worthwhile to investigate 


Weaknesses:
* I am uncertain about whether it is appropriate to claim that existing offline RL algorithms in linear MDPs achieve $O(1/\epsilon^5)$. It appears that [38] may have better sample complexity. In Table 1 of the manuscript, the author mentions that [38] cannot handle the discounted setting. However, extending from the finite-horizon to the discounted infinite-horizon setting is relatively straightforward. Hence, this comparison may not be entirely fair. If [38] indeed has better sample complexity, it significantly impacts the author's contribution. Thus, I currently rate the paper with a score of 4.

* I am not entirely certain about the significance of the extension to the average reward case.

* Presently, I cannot determine whether the reason [9] and [36] cannot handle the average reward case is due to the algorithms or their analysis. If this limitation arises from their analysis, their algorithm has the potential to be superior as it can handle more general MDPs.

Limitations:
They discussed. 

Rating:
4

Confidence:
2

";0
taQ64d2KBX;"REVIEW 
Summary:
This paper introduces a new integration method (mean inverse integrator) for learning dynamics from noisy data. Experiments on Hamiltonian systems show the effectiveness of the proposed method.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
- The problem of learning physical dynamics from noisy data is an interesting one.
- It combines techniques from the field of numerical computation with machine learning.


Weaknesses:
- The usefulness and significance of the proposed method is not clear.
- I feel that comparative experiments are not sufficient.
- It is not clear what advantages the proposed method has over the naive noise handling method.

Limitations:
It would be good to add a careful discussion of the advantages and disadvantages of the proposed method.

Rating:
5

Confidence:
2

REVIEW 
Summary:
The paper investigates mono-implicit Runge--Kutta (MIRK) methods for learning dynamical systems from data. In particular, MIRK methods can be made explicit by introducing the external data into the solver step itself, leading to a more efficient integrator while keeping favorable stability, symmetry, and symplecticity properties. To handle noisy data, the paper proposes the ""mean inverse integrator"" as an efficient way to average multiple trajectories and learn meaningful vector fields from these. The methods are demonstrated in multiple numerical experiments.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper is well-written, the proposed method is presented very clearly, and the main claims made are well-supported.

Weaknesses:
I find the presentation of the explicit Runge--Kutta (ERK) baseline a bit confusing, and in particular counter-intuitive of my current understanding of the usage of ERK methods for inverse problems in the context of ODEs. Concrete questions are below, in ""Questions"".
This criticism also extends to insufficiently detailed baselines, and potentially to insufficient baselines overall. 

Limitations:
The authors address limitations in a dedicated section, which is much appreciated.

One additional limitation that is not explicitly mentioned in the section that comes to mind is the influence of the noise on the data: Since the data is explicitly included into the numerical solver, as opposed to being just part of some L2 loss to guide some estimated trajectory, I would expect that for very noisy data other methods might be preferrable (in particular a least-squares approach with RK from a learned initial value).

Another potential limitation: I assume that this method is not able to be a plug-in replacement in some latent neural ODE setting, where the ODE trajectory is not observed directly but only in some transformed space, e.g. when having video data and modeling an ODE in latent space? This is of course far from a trivial setting and I do not expect that such specific scenarios need to be mentioned explicitly; but the necessity of actual trajectory observations, as opposed to partial or non-linear observations, could indeed be another limitation.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors present a novel method, Mean Inverse Integrator (MII), used to aggregate data generated through numerical integration of the vector field characterizing Hamiltonian systems. In particular, the objective is to improve the training of Hamiltonian Neural Networks (HNNs) when the data used is noisy.
The authors train HNNs on two tasks, using data generated by different methods of numerical integration.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is well written and complete. My background on the topic is limited, and so I was happy that Sections 3 and 4 were included.
- The theoretical analysis is thorough and convincing.


Weaknesses:
- I am not sure how much the paper fits with the themes of NeurIPS, but it should still be of interest to some of the audience.


Limitations:
N/A

Rating:
6

Confidence:
2

REVIEW 
Summary:
The presented work considers a novel class of integrators that are used to train Hamiltonian Neural Networks (HNNs). This class is called mean inverse integrator and it averages the trajectories from mono implicit RK methods (MIRK) to obtain higher accuracy. The authors provide theoretical results on how MIRK convergence suffers from noisy data. Also, experimental results on test dynamical systems illustrate the performance of the proposed approach.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The paper presents a comprehensive introduction to the different types of integrators and smoothly introduces the new one. The novel integrator is investigated w.r.t. the relations with classical RK methods (Theorem 4.4 and Proposition 4.3). Also, theoretical results on the robustness w.r.t. noise in the data are presented. 

Weaknesses:
1) the experimental results show that the proposed approach is not always better than the alternative methods in terms of accuracy, but always slower in terms of runtime. So the use cases for the MII-related approaches should be stated more strictly.
2) the manuscript is mostly about integrators theory rather than learning dynamical systems from data. Revision of the structure and shift of the focus from integrators to its application in learning HNN can be very helpful
3) the connection of the proposed integrator and its usage in forward or in backward passes is ignored. Thus, it is unclear from the text how this new integrator should be incorporated into the existing pipeline of training HNN
4) experiments are performed only for systems, where the states are small dimensional. The scalability and robustness w.r.t the large dimension of states are unclear
5) since no batching is used for training, it indicates that a large amount of data are not tested yet and all available data are fitted in the GPU memory. It will be interesting to study how the stochasticity of the data and corresponding gradient estimates affect the convergence of the training curves.

Limitations:
The authors explicitly state the limitations of the proposed method.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel method aimed at learning the vector field of a dynamical system. The proposed approach is called the mean inverse integrator, which utilizes a neural network (e.g., SRNN) to accurately estimate the integrator in the presence of noisy data. The authors provide theoretical insights into the sensitivity of both the one-step target function and the mean inverse integrator to data noise. Additionally, the paper presents empirical evaluations by comparing the method to five different types of integrators.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper effectively conveys its ideas and arguments with clarity.

- The research addresses an important question and presents a new approach to handle noise when learning data dynamics.

- Theoretical analysis shows how the proposed method and the baseline approach respond to noise, contributing to a better understanding of their performance.

- Empirical results demonstrate the effectiveness of the proposed method, showing significant advantages over the baseline approaches.

Weaknesses:
- The paper lacks clarity on why the mean inverse integrator yields more accurate estimates compared to the one-step baseline.

- Scalability to high-dimensional systems is not explored, as current experiments primarily focus on trivial test cases. It would be beneficial to investigate potential challenges arising from increased computational complexity, instability of estimation in the presence of chaotic dynamics, and provide examples demonstrating the method's efficacy.

Limitations:
N/A

Rating:
5

Confidence:
2

";0
pCucay08Co;"REVIEW 
Summary:
In this work, the authors characterize the problem of the Barren Plateau from different perspectives: (1) local unitary within a QNN on the cost function, particularly the randomness for the generic cost function; (2) quantum information theory; (3) the optimization methods during training. This work discusses those factors impacting the Barren Plateau landscape.  

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
(1) The work provides a theoretical understanding of the Barren Plateau problem and determines what factors actually impact the training of VQC, which is very interesting. 

(2) Solid mathematical formulation is given and the experiments can corroborate the theoretical analysis. 


Weaknesses:
(1) Some latest work on the Barren Plateau problem in the training of VQC should be included, such as Refs. [1], [2], and [3]. Ref. [1] aims at the QNN architecture for dealing with the Barren Plateau problem, Ref. [2] focuses on the initialization strategy, and Ref. [3] puts forth the pre-training method for mitigating the VQC training problem of Barren Plateau. 

[1] Jun Qi, Chao-Han Huck Yang, Pin-Yu Chen, Min-Hsiu Hsieh, ""Theoretical Error Performance Analysis for Variational Quantum Circuit Based Functional Regression,"" npj Quantum Information, Vol. 9, no. 4, 2023

[2] Zhang, Kaining, Hsieh, Min-Hsiu, Liu, Liu, and Tao, Dacheng. Gaussian Initializations Help Deep Variational Quantum Circuits Escape From the Barren Plateau. In Neural Information Processing Systems, 2022.

[3] Jun Qi, Chao-Han Huck Yang, Pin-Yu Chen, Min-Hsiu Hsieh, ""Pre-Training Tensor-Train Networks Facilitate Machine Learning with Variational Quantum Circuits,"" arXiv:2306.03741v1

Limitations:
It is expected to have experimental simulations on real data like the MNIST dataset to demonstrate the effectiveness of the proposed analysis approach. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper examines the critical issue of trainability in quantum neural networks (QNNs) by adopting a perspective centered around the locality. Through extensive analysis, the authors convincingly demonstrate that the adjustment of local quantum gates within a diverse range of QNNs results in an exponential decay of the loss function range as the number of qubits scales up. The authors bolster their claims with carefully conducted numerical simulations, providing compelling evidence that locality plays a fundamental role in shaping the behavior of QNNs. Building upon prior research on barren plateaus, the paper makes a technically sound contribution, albeit with an incremental advancement in the field.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The analysis of Theorems and Propositions, which shows the exponential decay of the loss function range by adjusting local quantum gates, is technically sound. Additionally, the ideas, concepts, and results are well presented. The authors effectively communicate their methodology, theoretical framework, and experimental simulations, making it easier for readers to comprehend and follow their arguments.  

 







Weaknesses:
The main weakness of this paper lies in its limited impact. While the authors conduct a clear and thorough analysis of how the concentration results of random circuits depend on the locality unitary, the technical tools employed bear a striking resemblance to prior literature concerning barren plateaus. The achieved results can be derived from existing works, with the only notable distinction being the introduction of a parameter, m, related to the locality in the derived bound. Additionally, the authors' claim that few rigorous scaling results exist for generic QNNs is contradicted by the abundance of relevant research, as evidenced by references [1], [2], [3], and [4], which address similar theoretical aspects the authors aim to explore. Previous studies have already established that deep ansatz can lead to the concentration of the cost function, rendering the observation regarding the exponential vanishing of the loss function range via the adjustment of local quantum gates less novel.


[1] Leone, Lorenzo, et al. ""On the practical usefulness of the Hardware Efficient Ansatz."" arXiv preprint arXiv:2211.01477 (2022).
[2] Thanasilp, Supanut, et al. ""Subtleties in the trainability of quantum machine learning models."" Quantum Machine Intelligence 5.1 (2023): 21.
[3] Garcia, Roy J., et al. ""Barren plateaus from learning scramblers with local cost functions."" Journal of High Energy Physics 2023.1 (2023): 1-79.
[4] Larocca, Martin, et al. ""Diagnosing barren plateaus with tools from quantum optimal control."" Quantum 6 (2022): 824.

Limitations:
No, the authors did not address the limitations of their work

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper investigates the trainability of random quantum circuits from the perspective of their locality and demonstrates the variation range of the cost function via adjusting any local quantum gate vanishes exponentially in the number of qubits. This theorem unifies the restrictions on gradient-based and gradient-free optimizations. The paper also verifies their theorem on three applications with numerical simulations and deepens the understanding of the role of locality in QNNs.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well-written and provides a rigorous analysis of QNN trainability and scalability from the perspective of their locality. 
2. The paper applies the proposed theorem to three representative QNN models, including the VQE, quantum autoencoder, and quantum state learning, and provides the numerical simulation results.

Weaknesses:
1.  Although Line 66-73 provides the advances of the proposed method, the comparison with previous works is not clear enough. It is important to review the previous methods and compare their specific differences. 
2.  The contribution of the paper seems weak, the novelty, comparisons with related works, and guidances for future QNN training or design need to be highlighted and enhanced.

Limitations:
The paper can be improved by considering the above weaknesses and questions.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper proof a result on the range of possible values that the cost function of a variational quantum algorithm can take when one optimises over a given unitary that is before or after random gates that form unitary 2 designs. This quantity vanishes exponentially with the number of qubits. This generalises previous results on Barren plateaus, concerned with the vanishing of the gradient of the cost function.
The material is presented clearly and the paper also has numerical verification of the scaling in the case of VQE, quantum autoencoder and quantum state learning. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is clearly written, with figures explaining concepts. It presents both theory and numerical checks
- The problem studied is relevant in scaling up quantum neural networks 
- The main theorem allows the authors to recover and unify previous results on exponentially vanishing gradients and cost function differences

Weaknesses:
- The paper does not comment on recommendations to avoid the exponentially vanishing variation range
- The comparison to previous works is limited. The authors mention that their work opens a new venue for analysing trainability of QNNs but it is not clear to me what new insights are gained. It would be useful to comment on what is gained wrt previous literature. Also, on whether the methods used to prove their main theorem are similar to those used in the literature or not.
- The VQE experiments are taken with circuits of depth 10 x n. That depth was chosen so that the hardware aware ansatz approximates a 2-design. However no comment on the required depth to compute the ground state of the Hamiltonian is presented and it is not clear whether the choice of ansatz and depth is something that practitioner would actually do.

Minor

- Sentence Line 71 - 73 does not read very well, you could rephrase it
- Line 153 - 155: it would be helpful for the reader to have an explanation of the connection between parameter shift rule and $e^{-i\theta \Omega}$ with $\Omega^2=1$. Also why does this imply the existence of $W$ as claimed? 

Limitations:
- The method applies when the $V_1$ or $V_2$ are 2-designs. This limits applicability.
- Strategies to overcome the exponentially vanishing range are not discussed. 

Rating:
6

Confidence:
3

";0
EcReRm7q9p;"REVIEW 
Summary:
The paper designs methods for taking actions in multiple sequential settings (Markovian, time-varying Markovian, non-Markovian) to generate high-quality data for the estimation of average treatment effect.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Focuses on improving ATE estimation by improving the data collection process, as opposed to improving estimators etc, which is a nice take on this problem.

Weaknesses:
None identified.

Limitations:
1. Hard to appreciate the optimal treatment strategy introduced in Sec 3, likely because the treatment is a bit terse and skips some useful discussion.
2. The data collection strategy is agnostic to the estimator used later. Is there any further improvement to be had? It would be good to see comments on this, as the estimators are already known in the literature.

Rating:
6

Confidence:
1

REVIEW 
Summary:
This paper studies optimal designs for allocating treatments so that treatment effects can be accurately estimated in online experiments. The focus of the paper is designing behavior policies for data generation to minimize the MSE of the ATE estimator when we know that dynamics may evolve through an NMDP, TMDP, or MDP.

Soundness:
4

Presentation:
4

Contribution:
2

Strengths:
1. The paper studies the challenging and important problem of SUTVA violations in online experiments. Without accounting for these violations, the temporal carryover effects that occur due to sequential allocation of treatment in online experiments can yield biased treatment effect estimates.
2. The authors consider an ambitious problem of designing optimal behavior policies when data generation evolves via NMDP, TMDP, or MDP.
3. Although there are many works that focus on off-policy evaluation and estimation of treatment effects, not many of these focus on how to design the behavior policy to improve the efficiency of the policy value estimator (it is more common to assume that a fixed dataset, generated according to some prespecified or unknown behavior policy, already exists). 
4. The paper has a thorough and comprehensive related work section.
5. The paper is clear and well-written.


Weaknesses:
1. In Section 4, when analyzing the optimal designs for the TMDP and MDP models, the choice to restrict the treatment assignment policies that are considered to the class $\Pi^{b}$, the class of policies that randomly assign the first action and sticks with the same action for the rest of the trajectory, seems quite restrictive. In some sense, this removes some of the “dynamic” aspects of the original problem and greatly reduces the complexity of the problem.

2. It seems like the burn-in period is quite essential to estimating the ATE with the proposed designs because proposed behavior policies randomly selects an action $A_{1}$ in the first time step and then sticks with it for the remaining time steps. As a result, if we didn’t have the burn-in period, we wouldn’t be able to estimate both $V_{1}^{0}$ and $V_{1}^{1}$. The importance of the burn-in period isn’t emphasized in the text and is worth commenting on (or at least citing references that burn-in periods are necessary for consistent estimation).


Limitations:
Yes.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper studies optimal treatment allocation aiming to maximize the obtained information from online experiments to estimate treatment effects accurately. The authors propose optimal allocation strategies in a dynamic setting. These strategies are designed to minimize the variance of the treatment effect estimator when data follow a NMDP or TMDP. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The problem that the paper investigates is important, in my opinion. The paper provides an insightful paradigm that how reinforcement learning techniques can be effectively applied in experimental design/causal inference. The authors also did a great job on giving credits to RL literature.

Weaknesses:
1.	There are several very relevant papers in experimental design literature that needs to be discussed carefully, for example, Bojinov et al. (2022) and Farias et al. (2022). I think some insights and the designs in this paper are closely related to these two papers.
2.	From experimental design/causal inference literature, we may not only want to know an estimator of ATE, but also the corresponding confidence intervals for the estimators. I am wondering that whether the authors could have some comments on how to construct confidence intervals.
3.	The definitions and relationships between $n$ and $T$ are a little bit confusing, especially when reading Section 2. I think some more descriptions on the relationship them will be helpful. From my understanding, the data structure looks very similar to “panel data” in the literature.

Reference:

Bojinov, I., Simchi-Levi, D., & Zhao, J. (2022). Design and analysis of switchback experiments. Management Science.

Farias, V., Li, A., Peng, T., & Zheng, A. (2022). Markovian interference in experiments. Advances in Neural Information Processing Systems, 35, 535-549.


Limitations:
See previous comments.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper considers A/B tests which are often employed for evaluating new products/treatments/policies against existing baselines. The goal is to study the optimal design of such A/B tests to maximize the information obtained and estimate treatment effect more accurately. The paper considers three possible settings governing how the data is generated: whether it follows an MDP or if it follows a time-varying MDP or a non-Markov process and presents an optimal allocation strategy for each. The paper tests these policies on a real-world sourced dataset pertaining to a ride-sharing company and shows that these approaches achieve better accuracy. Finally, the paper also establishes nice theoretical properties of these methods and derive an upper boudn for the MSE of the proposed estimator. 


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
– The paper studies an interesting problem that can be very useful in improving the efficiency of A/B tests that are commonly employed everywhere.

– Paper is well-written. It also sufficiently talks about existing related work and distinguishes itself. 

– The key results in the paper are strongly grounded in theory and are supported by good results in the empirical section.  


Weaknesses:
– Could there be some imbalance in allocation of treatment leading to fairness issues? For example, to optimize sample efficiency, could the algorithm withhold treatment unnecessarily if the placebo group has high variance? That way it could put most of the people in placebo and almost none in treatment? Or similarly, if there is a negative treatment but with high variance, it could opt to allocate that treatment more often?  It is unclear if the algorithm guards against these issues. 

Limitations:
No negative social impact (other than pointed out in questions section)

Rating:
6

Confidence:
2

";1
tpIUgkq0xa;"REVIEW 
Summary:
Large language models lack expertise skills and this is reflected in their limited capability for arithmetic etc.

The paper proposes a method to integrate a CoNN (compliled neural networks) into an LLM via gating. Such integrations allow better performance for rule intensive tasks such as symbolic reasoning, arithmetic reasoning etc. To perform this, the paper proposes a gating mechanism although the implementation seems rule based triggering (line 167).

With the proposed mechanisms, the authors evaluate on arithmetic tasks (5.1) where the model achieves consistently 100% accuracy. On symbolic reasoning (5.2) and arithmetic reasoning, the method also performs better than finetuning appraoches in terms of performance or data efficiency. 







Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The paper correctly identifies the limitations of LLMs and propose a novel approach to tackle the problem. The solution consists of a neural machine that serves as expert for symbolic/arithmetic operations.

Empirically, the paper demonstrates strong results over competitive baselines on arithmetic/symbolic reasoning approaches.

Weaknesses:
The novelty introduced in the paper doesn't quality a Neurips paper. The novelty of the paper concretely is using the rule based trigger to combine a CoNN with an LLM; neither components consist of the paper novelty. 

There are various presentation issues that make the paper quite hard to follow:
- The second contribution item is put in Appendix, I read it but please note that even reviewers are not obliged to read those materials in Appendix to judge the paper. Related to this remark, CoNN is only introduced and referenced, for people not familiar with the technology, there is nowhere in the paper to know how it works.
- Section 3.2 introduces the gating between LLM and CoNN, I think equation (1) has a flaw since it involves choose argmax from a matrix which seems ill defined and I think the authors mean to concatenate HL and HC instead. The gradient flow (3.3) doesn't give further insight rather than it is a gating mechanism. 
- I am confused by the illustrative figures in the paper. Figure 2 has the input text on top which is easily confused to a paper title and I feel Figure 1 is not relevant to the paper.



Limitations:
Unless Neural Comprehension machines are widely used, I don't see why this approach is particularly useful: I don't see the advantage compared to API calling approach adopted by today's industry. Particularly, as shown in this paper, many operations have to be implemented individually (subtraction, addition, etc.)

Rating:
3

Confidence:
3

REVIEW 
Summary:
Authors have proposed a novel way to augment large language model called Neural Comprehension to improve symbolic reasoning in tasks where rule-based execution is required by design such as numbers summation. The core idea behind their method is to augment the LM with compiled NN (CoNN) for a specific task is a manner of mixture of experts where they design a policy which determines LM or CoNN will be executing the next token prediction at each time step. In addition, they described how their method could be used with in-context learning (ICL). Authors perform set of experiments in symbolic operations (parity, reverse. addition, subtraction), symbolic reasoning (concat, coin flip) and arithmetic reasoning. They show empirically how their method outperforms stand-alone LMs finetuned on corresponding tasks data. Finally they show a potential of combining multiple CoNN with the given LM to increase the task capability of the final Neural Comprehension model.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
This work proposes an original way to combine LMs with CoNN networks and analyze the performance using multiple correlated or uncorrelated CoNNs together. 
The wide range of symbolic experimental tasks show that authors performed high quality experimental investigation.

Weaknesses:
I think major weaknesses of this work is (1) a hardcoded structure of CoNNs under consideration and (2) hard coded policy of choosing the LM vs CoNN component by connecting that to task-based properties. I believe the most interesting part would be to learn the beta factor which also seems to be very challenging. 

Authors claimed that their work suggests potential improvements from using Neural Comprehension in other tasks, but they did not mention how to get CoNNs for these tasks? In general, the discussion about CoNN design and implementation is somewhat skipped in the paper while it seem to be a crucial factor in this paper's impact.

Limitations:
Authors discuss some statistical limitation aspects of LMs in the appendix and how their proposed method alleviates that. However, I did not find explicit discussion of limitations of their own approach except for describing future work. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
While Large Language Models show promise for a wide swath of tasks, they are lacking when applied to symbolic reasoning tasks. To overcome this limitation, the authors propose to employ Compiled Neural Networks (CoNNs). They create networks specialised to arithmetic and symbolic tasks and propose a mechanism by which an LLM can propagate the gradient through CoNNs to better learn to solve symbolic reasoning tasks. They demonstrate improvement in pure symbolic manipulation (parity and reverse), arithmetic (addition and subtraction), and more complex symbolic reasoning (coin flip and last letter concatenation). They demonstrate better generalisation to out-of-distribution examples (proxied by digit length or sequence length for the first four tasks), a considerable improvement on LLC, and parity with a larger LLM when augmenting a smaller one (T5 small + NC v vanilla T5 large). The improvements are comparable to external tool-based approaches, however, hold a promise of better integration (as gradients can propagate without surrogates), and the mixture-of-experts style combination can be learnt rather than rule-based.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- By construction, CoNNs are interpretable from their basic building blocks, ensuring that paths that do go through them can be interpreted according to the rules they encode.
- The reduced number of parameters holds promise for reducing the cost of language models (relative to GPT-3)
- Even with simple gating, there is a non-trivial improvement on tasks when multiple CoNNs are employed (Section 5.4)

Weaknesses:
- There is an implicit assumption on practitioners to know what rules to expect and generate appropriate networks that then get used MoE style.
- Expanding on the previous, practitioners should be able to translate their rules, from, for example, regular expressions, to RASP to enable compilation to a NN.

Limitations:
The authors have address most limitations that arose as questions during the reading of the paper spare the one I listed under Questions with regards to the cost of producing CoNNs.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper shows a strategy to improvise ICL by including CoNNs in the learning pipeline, which enable the LM to learn symbolic operations in addition to standard autoregressive LM generation. The resulting model is trained by a hand designed gradient accumulation technique and results are compared on symbolic tasks.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
The paper shows how symbolic tasks can be included with general autoregressive training and therefore provides a way to train models following a few fixed symbolic tasks in mind. The paper provides a solid training recipe with proper mathematical justification. The results correlate with the claims and justify using the method.

Weaknesses:
It is unclear from the paper how different gating mechanisms are being derived in this network and how they are being included in the training framework. The authors say that \beta is not learned in the algorithm and essentially rule calculations are assigned to CoNNs. If that is true, the applicability seems a bit ad-hoc as different rules will then need to be hand written and not derived, and the benefits of the network will only be applicable to scenarios that are symbolically encoded. In other words, this seems to be a scalability challenge in terms of letting LMs learn rules. The experimental evaluation to justify the benefits seem very limited.

The paper also suffers from poor presentability with multiple grammatical errors, spelling mistakes etc. Also more context on training the overall CoNN+DNN system is missing from the paper or appendix. 

Limitations:
None

Rating:
4

Confidence:
4

";0
Og2HCj3V1I;"REVIEW 
Summary:
Paper proposes an evaluation metric for generative models which compare the distributions of real and generated images using a predefined set of attributes, or pairwise occurrences of attributes. The advantage of these metrics over the previous work is that they provide explicit visibility of which aspects contribute to the final metric value.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The idea for the proposed metric is novel – it uses language-image models to measure alignment of distributions of real and generated images given a set of text attributes. Additionally, the metric is highly customizable for downstream tasks because users may define their own set of attributes that are of interest to the specific task and drop irrelevant attributes.

Weaknesses:
Failure modes of the metric are not discussed (or limitations of language-image models and how they affect the metric). 

Consider a scenario where there are two models, A and B, with the same SaKLD. Model A produces essentially perfect alignment on all attributes other than one which fails dramatically, causing a large spike in SaKLD histogram, and this attribute is the only contributing to the final score. Model B on the other hand performs poorly across all attributes but averaging over attributes yields the same SaKLD score as the model A. In this scenario SaKLD would potentially not agree with human judgment, since failing in a single attribute might not be visible when inspecting large image grids. Can this kind of scenario occur in practice, and if it can, what would be your recommendation for the user of the metric in that case?

Fig. 4 shows that SaKLD and PaKLD are dominated by few attributes of attribute pairs. Is this usually the case in practice? Fig. 5 (b) also indicates that adding new attributes contribute to the metric with diminishing strength. This might be misleading for the user of the metric. Intuitively, adding a large set of attributes should correspond to more thorough evaluation of the model, however, this might not be the case if few attributes are dominating the final value of the metric.

The empirical effectiveness of the attribute based metric is not fully demonstrated. The authors advocated for an interpretable metric but unfortunately end up comparing modern generative models using single scalar numbers (as the existing metrics do), instead of taking advantage of the interpretability of the metric and showing a more fine-grained analysis of the models.


Limitations:
The authors adequately addressed the limitations of their work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a new metric to evaluate the quality and diversity of generated images based on
interpretable embeddings. To obtain the interpretable embeddings for selected attributes, the cluster
centers of the encodings of two separate encoders, one for images and the other for the text attributes
are calculated and the interpretable embeddings are the difference between the encodings of each
image and attribute and their respective cluster centers. The interpretable embeddings represent the
direction in which a particular embedding lies with respect to it’s cluster center. The CLIPScore
between the interpretable embeddings of an image and an attribute is calculated and is named as
’Directional CLIPScore’, since the interpretable embeddings represent the ’direction’.
The authors have proposed two metrics : 1. SaKLD - to quatify how closely the attribute distri-
bution in generated images matches with that of training images. 2. PaKLD - to quantify correlations
between different attributes. The KL divergence between the probability density functions of the
Directional CLIPScores for all the images in the train data and generated data.
Using SaKLD, the KL divergence between the attribute distribution in training and generated
images is calculated. PaKLD calculates the KL divergence similar to SaKLD, except that the presence
of a pair of attributes is required in the training and generated images.


Soundness:
1

Presentation:
3

Contribution:
1

Strengths:
The motivation for the idea is good and has a huge potential impact for improving evaluation of generative models.

Weaknesses:
- Although, the motivation for developing this metric is valid, the overall methodology and the
experimental results are not convincing for the use of this metric. Also, the experiments performed
are inadequate and do not sufficiently justify how well the metric performs compared to the previously
proposed metrics. Additionally, previous metrics can directly evaluate quality of generation based
on the generated images alone, but this metric heavily relies on the attributes in the form of text
descriptions. Thus, it limits the applicability and generalizability of this metric.
- The proposed metrics use CLIPScore (which already exists) for interpretable embeddings and then
applies KL Divergence for the PDFs of the ClipScores of images and attributes, thus, showing limited
novelty.
- Most of the paper is easy to follow but some important parts like, how is the center of text attributes
calculated, results from table 1 (what does accuracy stand for) etc. are a bit ambiguous. There is a
lot of scope to improve the technical soundness of the paper. Although some of the popular metrics
1are mentioned, there has been a lot of work in the generative modelling domain which the literature
survey must cover. The proposed methodology is not very sound and is not well supported with the
experimental setup. The results are also not sufficiently explained. Diversity is the main motivation
for the paper as it is mentioned in the abstract but any theoretical or empirical work to support it is
completely missing.
- Motivation and methodology including the steps to evaluate generated samples is understandable but
some parts are ambiguous (please refer above comments).
- Based on the current state of the experiments, the contributions don’t seem to be significant as there
is not enough validation to support the claims.

 typos:
- Line 43 : Instead of ”Figure 1 (b)”, it should be Figure 1 (a)
- Line 168 : Section 3.3 First letters of all the words in the heading must be capital
Other remarks :
- Line 56 : Instead of ”If the model lacks essential attributes” the following sounds technically
correct ”if model lacks ”representation” of essential attributes”.
- Line 56-58 : This claim does not seem to be correct.
- Line 147 : Link to the specified figure is missing
- Figure 2 is never refered to in the text, is it an unnecessary figure?
- Line 253 : Generated set has non-smiling men and non-smiling women. But Figure 4 caption
says otherwise

Limitations:
limitations have been addressed

Rating:
4

Confidence:
2

REVIEW 
Summary:
The embedding space used for calculating FID is computed with Inception V3 which is trained for image classification as the target task. This means it is more likely to capture discriminative features, raising doubts about its ability to effectively evaluate generative models. There is also a need to devise a new evaluation metric that can interpret underlying factors. This paper introduces a method called Directional CLIP Score (DCS) to properly evaluate this. The pre-trained CLIP is used as the embedding space. In particular, the paper proposes ""Single attribute KL div (SaKLD)"" to measure single attribute alignment and ""Paired attribute KL div (PaKLD)"" to measure multiple attribute alignment as new metrics. This paper provides some insightful measurement result using the proposed metrics.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The effort to evaluate generative models from various perspectives through the introduction of such metrics can be considered novel and a contribution. Especially considering the current issues such as the bias in stable diffusion [1], the proposal of such metrics can bring benefits to the field from the perspective of trustworthy AI.

Weaknesses:
It appears to be an intrinsic limitation that a significant number of samples (50k) are still required to obtain stable results. Nonetheless, thanks to the reported findings, we can gain more insights, and I appreciate that. 
Remaining concerns are written in [Questions] section.

Limitations:
It seems that there are no particular specific limitations.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This is a very interesting research work. The main contribution of this paper is to consider the attribute information in the original training data when evaluating the quality of images generated by the model. There are two benefits to this approach: 1) determining whether the model can correctly imitate the distribution of the training data; 2) explaining which attributes the model does not perform well on. In implementing this idea, the authors found that directly calculating the CLIPScore between the image representation and the text representation of the attribute does not yield distinctive results. Therefore, they proposed Directional CLIPScore (DSC). The main idea of this approach is to move the reference point for calculating vector similarity to a more reasonable point. At the same time, they proposed two methods to apply DSC, one is Single attribute KL Divergence, and the other is Paired attribute KL Divergence (PaKLD) considering the combination of attributes.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. I think this research question is very interesting. It is very meaningful to use the attribute information of the dataset to help evaluate the quality of the model's generation.
2. The Directional CLIPScore (DSC) proposed by the authors is very concise and appears to be quite effective in the case study.

Weaknesses:
1. In Section 3.3, I agree with the extraction of attributes using the BLIP and manual annotation methods. However, one method of extracting attributes is to generate an attribute list with GPT and then filter it with training data. I think the attribute list generated by GPT in advance may bring biases. The core of this paper is mainly to study the correlation between the model and the training data. However, if the list generated by GPT is not the most representative attribute, the results may be biased.
2. The experimental part in Sections 5.1-5.3 does not seem very convincing. The authors mainly verify that the proposed method can indeed be consistent with some expected experimental designs, but there is a lack of more convincing quantitative indicators to show that their proposed evaluation metrics are better than those proposed by others previous research works. I would prefer to see the authors analyze the correlation coefficient between their proposed evaluation metrics and human evaluation, as well as whether their evaluation metrics have improved in terms of correlation coefficient compared to previous evaluation indicators. This is my biggest concern for this work.


Limitations:
see weakness

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes two new metrics allowing to measure and explain the diversity of a generated set of images w.r.t a training set. Instead of the usual distributional distances relying upon an embedding space from a pre-trained model, these metrics rely on a set of textual attributes. The similarity between an image and an attribute is computed using their representation in a common semantic space, via the CLIP model - vectors are shifted using a centre of training images/attributes to make similarity scores more meaningful. Several ways to obtain attributes (Captioning, User-based or GPT-based) are investigated. The usefulness of the metrics is tested with an experiment injecting images correlated with target attributes in data, an experiment aiming to detect a specific attribute relationship in a curated dataset, and in a comparison of several generative models. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- This paper proposes an interesting approach, aiming at using the common semantic space between images and text proposed by CLIP to measure attribute and image relatedness, in order to provide interpretable representations of generated images and measure attribute-based metrics between a reference and generated dataset; such an application seems relatively original to me. 
- The two first experiments demonstrate well the usefulness of the approach, able to detect a shift when the data has been curated by human, based on which attributes. 

Weaknesses:
- The presentation of the paper could be improved upon. This include the writing, and the readability of the figures, as well as the quantity of information provided. This last points concerns mainly the presentation and framing of the problem of interpretability of representations, as well as the presentation and motivation of the experimental settings. 
- In particular, the paper lacks related work on concept-based representation for interpretability of images. While building metrics dedicated to generative model seems new to me, there are based on an idea which has been explored extensively before. See for example ""Concept Whitening for Interpretable Image Recognition, Chen et al, 2020"". 
- The paper focuses on a narrow choice of methods to generate attributes, which, to me, should be one of the key experimental investigation of the paper. Notably, the previous literature explores using different kind of attributes, coming from existing data (for example, ""Interpretable Basis Decomposition for Visual Explanation, Zhou et al, 2018"") or to be learnt (""A Framework to Learn with Interpretation"", Parekh et al, 2021). The authors only (very shortly) argue about the number of needed attributes.
- The toy experiments seem relevant but are very fastly presented and should be expanded upon. The remaining experiments are too short to be convincing and only focus on a handful of models. 

Limitations:
- Previous distributional metrics are several times referred as relying on external models in your paper. However, the attributes that you use also rely on external models (except the USER one, of course - but in this case, the computing of DCS still relies on a captioning model). How would you address this issue ? 

Rating:
6

Confidence:
3

";0
nRfClnMhVX;"REVIEW 
Summary:
Finding alignments between variables in a user-hypothesized causal model for how a task could (or should) be performed and representations in neural networks, termed ""causal abstraction"", is a mechanistic interpretability technique for understanding how neural networks develop predictions at a granular level by using causal interventions.  Prior work has recently proposed the Distributed Alignment Search algorithm for efficiently and robustly finding this alignment, but it still relies on brute-force search over the dimensionality of neural representations. This paper proposes to circumvent the brute force search by *learning* parameters of an orthogonal rotation matrix to maximize the alignment between a neural network representation's linear subspace and one of the variables in the high-level causal model.

They test their method on one rather narrowly scoped task, which is determining if a price is between two numbers, and use the Alpaca (instruction-tuned LLaMA) 7B decoder-only language model. The authors also test whether the found alignments generalize across settings (both input prompts and output token choices for labels), and find that they do.

Edit: I read the authors' rebuttal and the other reviews & discussions. I did not change my score as only one of the weaknesses (writing/presentation) was alleviated. I still think the paper should be accepted though.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
Originality:
- The application of causal abstraction (or even other variants, like causal mediation analysis/causal tracing) to large instruction-tuned LMs has not been done; this is highly original to the best of my knowledge.

Quality & Clarity:
- The experiments seem sound. The analysis in sections 4.3 - 4.6 is insightful and interesting. The overall findings of the paper are useful to a larger community for understanding math processing in LMs.
- The writing is clear overall.
- Related works section is comprehensive.

Significance:
- Causal analysis of language models at the representation-level is a very important goal with potentially far-reaching impacts for the broader NLP community. Using instruction-tuned models that are in widespread use today, such as Alpaca 7B, bridges the gap between theory and practice and makes this work more applicable to a wider audience.

Weaknesses:
- The paper does not read independently (particularly section 3.1) to a reader unfamiliar with prior work. I had to refer extensively to [32] to grasp what the various terms and variables used meant, and to understand what components of the proposed algorithm are novel w.r.t prior work. If there is not space in the main paper, the appendix should be used to thoroughly explain these concepts in a camera-ready version. The math notation could be improved; see ""questions"" section.
- The method involves pre-registering some possible symbolic models for performing a task; it cannot discover novel symbolic models that are not specified by the user.
- The method is tested on one narrowly-scoped task (determining if a price is between two numbers) and one model. There is no way to ""validate"" a mechanistic interpretability method beyond principle or axioms as we don't have the ground truth, so some of the findings are more suggestive rather than extremely concrete conclusions. However, this is a common problem in the subfield and I think this paper is convincing overall.

Limitations:
Yes- sufficiently detailed & comprehensive limitations section.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper introduces an advancement of Distributed Alignment Search (DAS), termed Boundless DAS, where the brute-force search steps are replaced with learned parameters. This enhancement enables efficient exploration of interpretable causal structures in large language models and therefore results in a scalable interpretable model.

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
1) The increasing significance of LLMs necessitates an evaluation of their interpretability. Developing a scalable method to achieve this goal is undeniably crucial and intriguing.
2) Current interpretability methods are inadequate for LLMs due to scalability issues, rendering them impractical. In contrast, the proposed method offers a solution that enables the interpretation of real-world LLMs, filling this crucial gap.

Weaknesses:
1) My primary concern lies with the novelty of the paper, which appears to be exceedingly limited. The paper seems to be primarily an extension of DAS and a direct amalgamation of two existing works, namely DAS and neural PDE [58], without addressing any specific challenges or introducing significant innovative elements.
2) The paper heavily relies on the DAS method as its foundation, and it is imperative to provide a comprehensive explanation of DAS, including its limitations, which are currently absent in the paper. A thorough understanding of DAS and its shortcomings is crucial to fully comprehend the context and motivation of the proposed research.



Limitations:
The authors have discussed the limitations of their work.

Rating:
4

Confidence:
5

REVIEW 
Summary:
This work proposes an extension to Distributed Alignment Search (DAS) called ""Boundless DAS"" that replace the brute-force search with learned parameters in order to scale the method to large language models (LLMs).
The method evaluated on the Alpaca 7B model tasked to output ""yes"" or ""no"" to a simple numerical problem such as ""is X between Y and Z?"".
Experimental analysis showed that the method is able to scale and find an alignment with two potential causal mechanisms.
Further experiments also show that the method is robust to slight modifications in the prompt of the model (additional sentence in the input, change of output format). 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This work proposes a new explainability method that is scalable to LLMs, which is an important research question as the field quickly progresses towards larger models that become part of real-life products. Hence, research to understand the inner mechanisms of LLMs is very important.

The proposed method is well-tested on various input/output perturbations of the target task, showing the robustness of Boundless DAS.

Weaknesses:
- the technical aspect of the paper is challenging to understand for someone that is not familiar with the field of causality

- the scope of the task used to evaluate the method is very narrow. Some discussion on how this can be applied to more critical scenarios where interpretability is required (such as the medical, financial, or legal domains) could improve the paper.

- It is mentioned that a good causal model could also explain the errors of the system. However, the current method does not explain why the model fails in certain examples.

Limitations:
Some limitations are mentioned.
However, one limitation of this work that should be mentioned is the relatively narrow scope of the target task being evaluated. This may be ok if the authors discuss how to transfer their method to more ""realistic"" scenarios in which interpretability is required (such as the medical, legal, or financial fields). Another possibility is that it is challenging or impractical to apply the same type of analysis in more complex scenarios. Either way, the paper could benefit from this type of discussion.

Rating:
6

Confidence:
2

REVIEW 
Summary:
This paper introduces a method called Boundless DAS, an algorithm that can automatically test the ""alignment"" between human-specified symbolic algorithms and the computational structure found in the weights of a neural network. The authors apply Boundless DAS to a real-world 7B parameter language model called Alpaca and find evidence of the implementation of a particular symbolic algorithm that computes whether some number $q$ is in the range $[a, b]$.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
Really cool paper!
* I like the idea of using gradient descent to create a correspondence between a symbolic algorithm and a set of neural network features. Continuous optimization feels a little easier to tackle (initially) than discrete combinatorial stuff.
* It's impressive that this works on a real-world, billion-parameter model.
* While the tricks to go from DAS to Boundless DAS seem reasonable and slick, my favorite part is the empirical section — inspecting the alignments that the algorithm discovers, looking for generalization, etc. Well done.

Weaknesses:
Not much here. If I had to say anything, the paper is quite dense and hard to follow. For newcomers, it requires somewhat extensive background reading on causal abstraction and the previous DAS work(s). Otherwise, the writing is fairly clear, ideas are well-motivated, claims are well-supported and analyzed, etc.

Limitations:
n/a

Rating:
8

Confidence:
3

REVIEW 
Summary:
This paper proposes a method for interpreting neural models by learning an alignment of their representations to a specified interpretable causal model. The method extends prior work, namely Distributed Alignment Search (DAS), by addressing a key limitation that prevents the scaling of DAS. The limitation is overcome by learning a set of masks that selects subsets of neural representations to align to different parts of the interpretable causal model. The method is evaluated experimentally by aligning Alpaca representations to a set of correct and incorrect hand-written causal models.

Soundness:
3

Presentation:
1

Contribution:
3

Strengths:
* The problem of interpreting large models is important.
* The approach is reasonable and scales a previous approach.
* The experimental results were convincing.

Weaknesses:
* Section 3 was hard to read. See the questions below.
* Section 3 did not feel self-contained. The source of intractability in DAS did not jump out to me.
* Giving the computational complexity for DAS and boundless DAS would make the efficiency argument stronger, unless they are the same.



Limitations:
The limitations were adequately addressed.

Rating:
6

Confidence:
2

";1
IcIQbCWoFj;"REVIEW 
Summary:
This paper provides near-optimal optimization algorithm to group distributionally robust optimization problem (GDRO). Since it is a special case of the stochastic convex-concave saddle point algorithm, the standard SMD could indeed achieve near-optimal sample complexity. Besides, the authors also apply online learning techniques to reduce the number of samples required in each iteration. Finally, the authors consider a complicated scenario where the number of samples that can be drawn from each target distribution could be different. Then the authors consider a weighted GDRO formulation and the convergence analysis of optimization algorithm.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
- Results are presented in a clear way. The proof is also fluent. It should be accepted.
- SMD and the online learning fashion of SMD are used in a cleaver way such that the authors achieved near-optimal sample complexity, and the per-iteration sample complexity cost is reduced (the per-iteraiton complexity is important because it relates to the storage cost of an optimization algorithm). 
- The notion of weighted GDRO is also of research interest. 

Weaknesses:
- No numerical study is presented to demonstrate the convergence rate of proposed algorithm.

Limitations:
The authors properly discussed relate work regarding optimization algorithms for DRO. I think one reference could be added: Wang et. al, Sinkhorn Distributionally Robust Optimization, arXiv preprint arXiv:2109.11926, June 2023. 

Reason: The authors listed some progress on solving Wasserstein DRO. The reference (Wang et. al, 2023) provides a SMD algorithm with biased gradient oracles that solves entropic-regularized DRO problem up to $\delta$-optimality gap with complexity $\tilde{O}(\delta^{-2})$. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper considers group DRO and shows an SMD algorithm to optimize. This algorithm requires a sample from every distribution in each iteration and matches the sample complexity lower bound up to a log factor. The second algorithm is designed for the case when the online functions depend on past decisions. For this, the authors use SMD and Exp3-IX. Finally, for the case with sample budgets from each distribution, the paper proposes two algorithms, one satisfies the constraints on the number of samples in expectation and the other using mini-batches satisfies the exact constraints. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Despite some weaknesses, I think all algorithms proposed in this paper are simple and natural, which at the same time achieve optimal sample complexity. 
- The presentation is clear to follow with proper explanations and intuitions.

Weaknesses:
1. I think the weakness of the first algorithm is that in many cases, it might be impossible to obtain samples from all distributions.
2. I’m not sure I understand the novelty of the second algorithm compared with Soma, Gatmiry & Jegelka (2022). In the comparison after remark 3, the authors point out an issue with this prior work. The authors use an alternative objective ($\epsilon_\phi$) to control the optimality gap, while Soma, Gatmiry & Jegelka (2022) directly bound the optimality gap instead of $\epsilon_\phi$, I don’t see any issue with not being able to bound $\epsilon_\phi$. Note that in Soma, Gatmiry & Jegelka (2022), the authors also propose a few algorithms in the same spirit as Algorithm 2, which I think work for the non-oblivious case as well.
3.  Mini-batch sizes in Algorithm 4 can be very large.
4. No experiments

Limitations:
The authors discuss some limitations of the proposed algorithms.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper presents a comprehensive study on group distributionally robust optimization (GDRO) by formulating it as a convex-concave saddle point problem. The authors propose utilizing mirror descent algorithms and online convex optimization techniques to effectively address this problem. Furthermore, the authors investigate the case of unequal sample sizes drawn from each distribution, leading to the development of a weighted GDRO algorithm. This work provides valuable insights into GDRO and offers practical solutions to address various scenarios.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The GDRO problem presented in the paper is interesting and well-defined. It has broad applications and holds significant interest. The authors demonstrate a deep understanding of the literature on mirror descent algorithms and their variants, showcasing their expertise in the field.

Weaknesses:
Unfortunately, the paper has several weaknesses, which are listed below:

1) Contribution is unclear: Upon careful review of the paper, I have concerns regarding the lack of clarity regarding the contributions. Section 2 appears to primarily present known results, where the formulation of GDRO as a convex-concave saddle point problem and its solution using stochastic mirror descent are well-established techniques. As a result, Theorem 1 seems to be directly derived from the work by Nemirovski et al. (2009). Similarly, Algorithm 2 appears to be a widely-known algorithm in the online convex optimization (OCO) literature, with known convergence guarantees (Theorem 2).
In this context, Section 3, which focuses on the ""weighted GDRO and SA approaches,"" seems to be the section where the potential contributions lie. However, it appears to be a straightforward extension of Algorithm 1 and Theorem 1, which have already been outlined in Juditsky et al. (2011).
To provide a clearer understanding of the novel and non-trivial aspects of Theorem 3 and Algorithm 3, it would be beneficial for the authors to explicitly highlight the specific advancements or modifications they have introduced. Clarifying the unique features, surprising results, or novel insights that differentiate Theorem 3 and Algorithm 3 from the existing literature will help in addressing the concerns raised regarding the originality and significance of the contributions.

2) Numerical experiments are missing: I am perfectly fine if theoretical NeurIPS papers do not contain numerical experiments. This paper, however, presents an algorithm (or actually  algorithms) for solving a practically relevant GDRO problem. In my opinion, it is absolutely necessary that when presenting algorithms to solve a known problem - these algorithms are numerically verified. Also if something is new about Algorithm 3 (which I am not fully sure - see point above) - I would be curious to see if this algorithm behaves better then existing approaches.

3) Technical issues: See questions in the next paragraph.

Limitations:
The novelty and contribution of this paper, in my understanding, is questionable. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
In this paper, the authors present a systematic investigation of group distributionally robust optimization (GDRO). First, they consider the standard standing where the algorithm can draw samples from all distributions freely. In this case, they demonstrate that stochastic mirror descent (SMD) attains an optimal $O(m \log m/\epsilon^2)$ sample complexity by drawing $m$ samples per iteration. Then, they make use of non-oblivious MAB to reduce the number of samples from m to 1, and obtain the same complexity.

The authors also consider a practical setting where different distributions may have different sample budgets. To this end, they introduce a weighted formulation of GDRO, and develop two stochastic approaches based on non-uniform sampling and mini-batches. By setting the weights appropriately, they are able to establish distribution-dependent convergence rates.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. DRO/GDRO is an important topic in machine learning, and has wide applications. The authors identify technical flaws in existing works [Haghtalab et al., 2022, Soma et al., 2022], and present simple and principled stochastic approaches. i.e., Algorithms 1 and 2. The sample complexities of their methods are nearly optimal, and the idea of applying non-oblivious MAB to GDRO in Algorithms 2 is smart and novel. 

2. The new setting in Section 3 is important in practice, since it is common to encounter imbalanced data sets. The formulation of weighted GDRO is interesting, and provides one possible solution. The theoretical guarantee of Algorithm 4 is very strong in the sense that it demonstrates nearly optimal rates for multiple distributions simultaneously.

3. The paper is well-written and acknowledges previous work properly. The authors present their techniques very clearly, and the results are sound to me.

Weaknesses:
There are no empirical studies. While theoretical contributions stand on its own right, I would like to suggest the authors to conduct experiments to verify the main theorems.

Limitations:
The authors have discussed the limitations of their approaches in their remarks. I do not find any potential negative societal impact of this work.

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper provides algorithms for group distributionally robust optimization.
For the case in which the sample budget for each distribution is identical,
the proposed algorithm achieves nearly optimal sample complexity.
This paper also considers the sample budgets are different for each distribution and provide an algorithm with non-trivial sample-complexity bounds.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- This paper is the first to consider a practical setting in which the sample budget varies by distribution.
- The paper is well structured and easy to follow.

Weaknesses:
- Given the results of [Soma et al., 2022], the contribution for the uniform-sample-budget setting appears somewhat limited.
- The obtained bounds in this work includes extra $\log m$ factors, unlike ones by [Soma et al., 2022].

Limitations:
The limitations and potential negative societal impact are adequately addressed.

Rating:
6

Confidence:
2

";1
1cY5WLTN0k;"REVIEW 
Summary:
Designing neural PDE sovler using deep neural networks is a challenging task for which several solutions have been proposed in the literature using for instance networks that encode the initial conditions or physics informed neural networks.

The authors propose to use Monte Carlo methods to train neural PDE solver for the solution of a general convection-diffusion equation.
Using Feynman-Kacformula, the authors derive a loss function that can be used to learn a mapping that can simulate the target fields using the input parameters and the initial condition. 

They propose a theoretical guarantee on the solution provided by the Monte Carlo solver and the paper illustrates the performance of the proposed method with a  one dimensional differential equation and a 2-dimensional Navier-Stokes equation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper proposes a Monte Carlo based PDE solver strained via Monte Carlo approximation which can  handle  coarse time steps better than existing alternatives. 

The proposed method does not require many particles in the numerical experiment and is computationally efficient in the settings explored.

Under some assumptions, the authors propose an upper bound on the error explicit in some hyperparameters of the approach in the case of a  convection diffusion equation.


Weaknesses:
Theorem 1 is obtained under several assumptions. 
These assumptions should be discussed more, are they restrictive or common assumptions in the PDE literature ?

The claim that the approach is efficient even with few samples seems correct in the proposed experiments. However, these experiments are in dimension 1 and 2 and Monte Carlo methods can be cumbersome in high dimensional settings without tunning. 
The authors could detail the explicit advice or theoretical guarantees we have for the error with respect to M.

The authors only explore one discretization scheme (Euler), the scheme used could have an impact on the performance of the method, can this be discussed ?

Limitations:
The authors provide several research perspectives for this work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors propose an unsupervised neural technique for solving PDEs based on the classical correspondence between (parabolic) partial differential equations (PDE) and stochastic differential equations (SDE) as given by the Feynman-Kac formula. Specifically, they propose minimizing the error between the neural approximation's deterministic prediction at timestep t+1 and the expected prediction of the neural approximation over particles that have evolved up to time t stochastically according to the Feynman-Kac SDE representation of the given PDE.

Soundness:
1

Presentation:
1

Contribution:
2

Strengths:
Incorporating Feynman-Kac into the PINN framework is an interesting idea.

Weaknesses:
# Poor numerics
The experiments do not justify the claims, e.g. that long rollouts are more stable using this method. To prove this, at the very least the authors need to present results where the trajectories actually exhibit turbulence. Then, an ablation with the multi-scale framework is required.

# Poor presentation
The authors cannot expect the reader to be familiar with Feynman-Kac and need to give explanations in plain English of the significance of this result. Then, the equations in the main paper should aim to clarify this further, not give a comprehensive mathematical presentation. For example, the inclusion of the forcing function  distracts from the main result which is using time reversal to obtain an SDE that moves in the right time direction for equations that are most often solved using neural networks, e.g. ones with an initial condition, not a final condition.

Furthermore, only the most experienced reads will walk away from this paper with a clear idea of how to implement the proposed algorithm. The emphasis in the paper should be to give the reader something to implement and try, not a theoretical proof.

Finally, there are a number of tricks that are not included in the initial idea and are not sufficiently explained. For example, the Fourier interpolation. The multi-scale framework makes the model non-parametric which is a significant departure from previous work and from the presentation of this paper as a Monte-Carlo approximation to PDEs.

Limitations:
N/A

Rating:
3

Confidence:
4

REVIEW 
Summary:
The authors present MCNP, a new unsupervised training loss for surrogate simulation networks. This loss is based on the link between stochastic processes and PDEs, sampling one-step Brownian motion to estimate the PDE solution. The learned network takes an initial state and the target simulation time to compute the state at that time in one pass. For longer periods, multiple NNs are trained, one for each sub-interval.

Soundness:
2

Presentation:
4

Contribution:
4

Strengths:
The paper is generally well-written and relatively easy to understand. It includes a good overview over related work.

The paper includes both theoretical and numerical results. The method is derived using the Feynman-Kac formula and the authors show how the errors of PSM and MCM scale when given an incorrect input state, such as predicted by a neural network.

A total of five numerical experiments are performed, covering a large range of simulation configurations. The paper contains an ablation study, giving some insight into the impacts of various parts of the MCNP method.

Weaknesses:
While the experiments are varied in the tested configurations, all but on experiment consider simple diffusion equations, some of which can be solved analytically.

The paper does not show any simulation trajectories and gives no insight into how the various tested methods behave in their experiments. Instead, only the final losses are reported. This makes it hard to determine the cause of improvement from the numerical results. I strongly recommend documenting your observations in the appendix.

The Navier-Stokes experiment seems to be mostly forcing-driven with all initial states ending in a similar configuration. A different forcing, such as Kolmogorov flow, would result in a much wider range of trajectories.

The paper does not give details as to how the numerical simulation (PSM) of the Navier-Stokes experiment was performed. Please describe the simulator in more detail if you implemented it yourself.

The source code is not part of the submission and the authors have not declared their intent to make it public. I strongly recommend doing so, especially if the employed CFD solver was implemented from scratch.

Minor:

* Eq. 3: Please indicate the x-dependence of xi
* Fig 1 caption: The formulas are missing a factor of 1/M
* L184: You mention that cutting the gradients prevents numerical instabilities but ignore the positive effects that gradient backpropagation can have.
* You refer to Eq.13 as a convection-diffusion equation but it does not contain a convection term.
* L199: The notation Δ ₜ u is confusing since Δ is already in use.
* L216: The argument that label noise helps MCNP perform coarser time steps seems unfounded to me. This requires an explanation.
* L237: By lattices, you probably mean frames or time steps?
* L268 and Eq. 17: You can simplify the forcing to a single sine term.
* Figure 2 is never referenced.
* Figure 2 shows the vorticity, correct? Please specify in the caption.

Limitations:
The limitations are adequately discussed.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The authors propose Monte Carlo Neural PDE Solver (MCNP Solver) which leverages the Feynman-Kac formula to train neural PDE solvers in an unsupervised manner.

I'm willing to revise my score based on the rebuttal from the authors to the questions that I raised below.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* That paper addresses and interesting problem: learning the neural operator in an unsupervised way. 
* The authors propose practical enhancements to their method such as one-step rollout, Fourier Interpolation and the use of a multi-scale framework.

Weaknesses:
* Limited set of experiments, only two cases: 1d diffusion and 2d Navier-Stokes.

Limitations:
* The limitations that I see are encapsulated on the questions that I raised above.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper proposes a new physics informed neural network based solver that utilizes the connection between PDEs and SPDEs. This is achieved through the Feynman-Kac formula and applies to a large class of PDEs. It comes with a bound on the error at each step in the rollout. The results are compared with multiple supervised and unsupervised PDE solvers on a number of 1D and 2D equations.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Originality: To the best of my knowledge, the paper is original in combining the Feynman-Kac-based approach with a neural operator architecture. The connections to existing work that relies on the Feynman-Kac formula as well as other PINN/NO approaches are discussed in detail in the main paper and the appendix.

Quality: The work is thorough in discussing existing literature and presenting the methodology. The theoretical result gives some intuition relating to how the proposed method scales as compared to the classical solver. 

Significance: The methodology combines a number of existing techniques (Feynman-Kac formulation, neural operators, Fourier interpolation) to achieve some improvement in specific setups, e.g. where the solution of the PDE is rapidly varying in space and/or time.





Weaknesses:
Clarity: The quality of the writing could be improved, particularly in the abstract/introduction. The rest of the paper is detailed enough in describing the experiments and discussing the results. 

Significance: The proposed approach seems to give an advantage only in specific situations. While this is acknowledged in the paper, it could be beneficial to discuss specific applications where such oscillatory conditions occur.



Limitations:
The limitations and extent to which this method gives an advantage over existing approaches are discussed in detail in the final section of the paper. 

Rating:
5

Confidence:
4

";0
F5FVsfCxt8;"REVIEW 
Summary:
In this paper, the authors propose a scheme for training decision trees on a combination of public and private data. By using local differential privacy the algorithm guarantees that adversaries learn little about the private data from the outcomes of the algorithm. Under assumptions and a specific splitting rule for the algorithm's public part, the authors prove properties about the convergence and generalization of the algorithm. The algorithm is empirically evaluated on 15 real and 1 synthetic dataset and compared against private histograms and deconvolution.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is written in clear English.
- Substantial theoretical analysis.
- Empirical evaluation on many (15) datasets shows that this method outperforms private histograms and deconvolution.
- The effect of different hyperparameter settings was clearly explored.

Weaknesses:
Major:
- Assumption 3.2 is key to deriving error bounds but I am unsure if this holds. Decision trees are discontinuous models that are often used to model discontinuous data.
- Sections 2 and 3 are very dense in mathematical notation and use complicated phrasing to say simple things. E.g. section 2.2 spends a full page that is hard to read on explaining that: an estimated regression tree predicts the mean of samples in a leaf and that the private tree predicts it based on private estimates of reaching the leaf (U) and the sample value (Y), privatized using randomized response and Laplace mechanism.
- Only relatively high values of $\epsilon$ were explored. While it is debatable what is a good value, to the best of my knowledge $\epsilon < 1$ is generally considered 'good privacy'.

Minor:
- Although I understand the 'max-edge partition with variance reduction' was introduced to prove theoretical properties it suffers from data that is non-uniformly distributed or contains useless features.
    - Useless features: the algorithm will never choose the same feature to split on twice unless depth > d (as a result of splitting on the largest distance). This means that if there are few informative features these will be split on only once.
    - Non-uniform data: the splits are determined on the midpoint between minimum and maximum values of a feature instead of looking at how data is distributed in the feature. This means data with e.g. long tails will cause a split to create one leaf with almost all data and one leaf with almost none.
- The algorithm and analyses are based on continuous features but the datasets used for evaluation also contain categorical features. E.g. 'sex' in Abalone.
- Limitations and broader impact have been moved to the appendix while these should be part of the main text.
- The visualizations only show mean estimates, no standard deviations / standard errors.
- It would be nice to compare to (global) differentially private decision trees to get an idea of the cost of decentralization in this task.

Limitations:
The limitations are moved to the appendix with the only limitation discussed being assumption 3.3.

Depending on the answer to question 1 I believe assumption 3.2 needs to be added to the limitations and a discussion on the choices of $\epsilon$ should be added. The limitations should be given in the main text.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper studies nonparametric regression under local differential privacy (LDP) constraints with the aid of public data. The paper proposes an algorithm, locally differentially private decision tree (LPDT), which uses public data to construct the splitting criteria for the decision tree and private data to compute the regressed values for each leaf node. The paper shows that under certain assumptions, with a small amount of public data, the algorithm can achieve a near-optimal convergence rate for decision tree regression under LDP constraints. The paper also demonstrates the effectiveness of the algorithm through experiments.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper studies an interesting problem, which has not been studied before to the best of my knowledge. The paper also proposes an algorithm that achieves an optimal convergence rate under reasonable assumptions. 

2. The paper shows that without public data, decision tree learning will have a nontrivial risk with any amount of private data. This suggests the importance of public data for decision tree regression under LDP constraints.

Weaknesses:
1. The importance of public data has been established for different learning tasks recently, especially under the central notion of differential privacy. Extending a similar observation to decision tree learning under local DP constraints is interesting, but less surprising.

2. The similarity assumption between P and Q, Assumption 3.3, looks weird. The assumption doesn't pose any requirement on how y is correlated with x. Imagine the case when P and Q have the same marginal on the feature space while the correlation between X and Y are completely different under P and Q. It would be hard to learn useful information about how x can be used to predict y from public data, and hence the structure of a good decision tree. I am not sure whether I have missed other important assumptions.

3. Another baseline to compare (theoretically, and empirically) would be to only use public data to train the decision tree. How would the proposed method compare against this baseline? E.g., in the experiment in Section 4.2.

Limitations:
Yes.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This work studies non-parametric estimation under local differential privacy with public data. The authors propose a locally private decision tree algorithm and show that it is min-max optimal under some regimes. The authors also test the algorithm on real and synthetic datasets.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The idea to leverage public data in LDP estimation is practically relevant. 
2. The experiment results are extensive. It includes both synthetic and real-world data and discussion on the effect of various parameters. Empirically the algorithm outperforms existing algorithms in most experiment settings.
3. The authors provide a thorough theoretical analysis of the proposed algorithm in terms of sample complexity and computation cost.  The proposed algorithm has improved time complexity over prior works.

Weaknesses:
1. The proposed algorithm does not seem to improve too much in terms of sample complexity with public data. We normally hope that using public data should improve the performance somehow (e.g. in reference [8], with public data the prior bound on the mean can be removed), but it seems that the algorithm merely attains the optimal bound without public data only when $\varepsilon\lesssim 1$. Existing algorithms that do not use public data can also achieve the same sample complexity.
2. Given the first point, the fact that the proposed algorithm does not work without public data appears to be a significant disadvantage. While LPDT has advantages in time complexity and empirically outperforms existing methods, it comes at the expense of additional resource of public data. It is thus in some sense unfair to compare with existing methods that do not use public data.

#### Some minor problems

1. The lower bound only matches the upper bound in Theorem 3.4 when $\varepsilon\lesssim 1$ (since $e^{\varepsilon}-1\simeq \varepsilon$ only with small $\varepsilon$), but all experiment results are for large $\varepsilon\ge 2$.
2. The notation $p$ is used first for the density function and then again used for decision tree depth. Please consider changing one of them to avoid confusion.

#### Update
- Increased my score to 5 after the authors added new results for small $\varepsilon$ and introduced a new algorithm that matches the lower bound for all ranges of $\varepsilon$.
- Increased to 6 after the authors resolved my question for removing range parameters.

Limitations:
The authors adequately addressed the limitations and potential negative societal impact.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a locally differentially private (LDP) decision-tree (DT) regressor algorithm that takes advantage of public data for utility improvement.
The proposed algorithm, LPDT, uses Randomized Response (RR) and the Laplace mechanism to protect the tree partition step.
This paper also introduces a new splitting rule named ""max-edge partition rule"" by using the variance as a reduction criterium.
Theoretical results were presented for convergence rates, training/testing time and space complexity, with important gains in comparison with the state-of-the-art.
Experimental results were provided with synthetic and many real-world datasets to validate the proposed approach, with significative gains over the state-of-the-art.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The motivation behind using Locally Differentially Private (LDP) methods for decision trees is well-founded. The idea of using public data is new and has proven to bring advantages in many fields (e.g., vision, languages).
Theoretical and experimental validation were presented showing advancement over the state-of-the-art.

Weaknesses:
The experiments only considered medium privacy regimes \epsilon >= 2. The paper would greatly benefit from including high privacy regimes \epsilon <=1 and the utility gain of using public data.

Limitations:
Although using public data can lead to utility improvements, the authors did not discuss the limitation of having such data in real-world applications. Indeed, following my last question on ""data heterogeneity"", the learning process could be really damaged if public and private data have different distributions. I recommend the authors to further discuss this paper's limitations.

Rating:
6

Confidence:
3

";1
zOCIKYVaF5;"REVIEW 
Summary:
The paper tries to analyze the remarkable performance of ResNet architecture and they find the residual alignment phenomenon. The phenomenon is general and they also proposed a mathematical model call the Unconstrained Jacobian Models to theoretically analyze it.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The authors find an interesting phenomenon for residual networks called residual alignments. I think it can guide future exploration for module designing, model compression, and regularization techniques.

Weaknesses:
I think the weaknesses of this paper have they only discussed such a phenomenon. It will be great if they can utilize such phenomenon to design some useful techniques.

Limitations:
None

Rating:
6

Confidence:
2

REVIEW 
Summary:
This work discovers the phenomenon of Residual Alignment (RA) in ResNets, whereby the top left and right singular vectors of residual Jacobians align with each other and in between different residual blocks. Through extensive experimental verification as well as novel theoretical frameworks and derivations, the paper shows that RA naturally emerges in ResNets but does not emerge in non-residual networks. By directly linking RA to ResNets, this work sheds new light on the broad success of these architectures.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
I believe this paper can have a significant impact.
- *Significance*: This work provides novel insights that have wide ramifications in a broad range of topics (deep learning theory, regularization, architecture design, model compression). Indeed, the precise mechanisms underpinning the success of ResNets remains a hot topic of research with high stakes. I believe this work makes a rare breakthrough in that direction.
- *Novelty*: To my knowledge, the discovery of RA and its precise mechanisms is novel. There is also novelty in both the experimental protocols and the theoretical frameworks. These protocols and frameworks might prove useful for the community.
- *Quality*: The analysis is highly convincing. I feel the experimental and theoretical evidences supporting the analysis are undebatable.
   - The experimental protocols are innovative and strongly convey the paper's claim, demonstrating both the presence of RA in residual networks and its absence in non-residual networks. 
   - The theoretical analysis is equally innovative and excellent.
- *Contextualization*: The context of the work is appropriately provided (aside from minor points detailed below), with connections made to previous works, including to the latest developments on layer-wise Neural Collapse. The paper also details potential ramifications of the discovery of RA.
- *Clarity*: The paper is well-structured, concise, clear. The writing is good. The plots and figures are striking, providing the required evidence to support the claims.
- *Soundness*: The theoretical proofs are sound (aside from very minor points detailed below).
- *Reproducibility*: The authors released their code for reproducibility. The code is neat and clean, with high quality standards.

Weaknesses:
I see *no real weaknesses*, only minor points that can be easily addressed.

Minor points:
- Restriction to the context of classification. RA1 and RA3 are specific to classification, and all the tasks considered fall within such a context. If the findings are restricted to classification, I think this restriction should be clearly stated.
- I understand that the class-wise equi-spacing on a line of intermediate representations is a consequence of RA (in particular the scaling of top singular values inversely with depth), combined with the increased magnitude of $h_i$ due to the aggregated summation in ResNets. If this reasoning is correct, perhaps it would be worthwhile detailing it?
- The authors missed some previous works that studied ResNets vs non-residual networks at initialization, notably [1], [2] and [3].
- There seem to be typos on lines 160, 239, 243. Also shouldn't ""these values"" be replaced by ""the top-1 singular values"" in the caption of Figure 5?
- It seems Theorem 3.1 is only proved in the context of binary classification. Perhaps this should be stated. Also, I believe there is a term missing in the Equation following line 11 in the Appendix. That term relates to $h_1 - U_{k,1} U_{k,1}^T h_1$ (as can be seen e.g. in the case where all $S_i$ would be equal to zero).
- Perhaps Theorem 2 in the Appendix should state the convention of positive singular values, meaning that inequality on the $\sup$ of the trace would always be guaranteed, with equality obtained if the determinant's sign equals 1. Also shouldn't $L+1$ be replaced by $L$ on line 68?

References:

[1] The Shattered Gradients Problem: If ResNets are the answer, then what is the question?, D. Balduzzi et al., ICML 2017 

[2] Gradients Explode - Deep Networks Are Shallow - ResNet Explained , G. Philipp et al., ICLR Workshop 2018

[3] Characterizing Well-Behaved vs. Pathological Deep Neural Networks, A. Labatie, ICML 2019

Limitations:
If the authors' findings are restricted to the context of classification, perhaps this restriction should be more clearly stressed.

Rating:
8

Confidence:
4

REVIEW 
Summary:
The paper ""Residual Alignment: Uncovering the Mechanisms of Residual Networks"" explores the underlying mechanisms and success factors of the ResNet architecture, which has gained significant popularity in deep learning. The authors conduct an empirical study by linearizing the residual blocks of ResNet using Residual Jacobians and measuring their singular value decompositions. 

They introduce the concept of Residual Alignment (RA) characterized by four properties: equispaced intermediate representations, alignment of singular vectors, low rank of Residual Jacobians, and inverse scaling of singular values. The phenomenon of RA is observed in well-generalizing models and is absent when skip connections are removed. The authors also propose a mathematical model that demonstrates the occurrence of RA. 

*Contribution & significance:* The paper identifies an interesting and seemingly novel phenomenon which they term residual alignment. They show that this observation is on solid theoretical grounds by studying the binary classification problem with cross entropy loss.  This sheds new light on an the role of residual connections in neural nets and will contribute to our overall understanding of this importance architectural component. 

*Major writing issues* The main drawback of this paper is its writing quality which is very low at the moment. While I grasped the main ideas by going through the appendix, several of critical pieces of information, such as definition of residual alignment Jacobians ($J_i$'s), are missing from the main text. This will make it very difficult to understand this paper just by reading the main text (which should be the default assumption for a reader). This is quite unfortunate given that the contributions are technically strong & very interesting. Thus, a significant overhaul of the writing seems to be necessary to make this work publishable.

*Clarity about novelty* It would be helpful if authors clarify the new insights and novelty of this work, in contrast to the paper that they numerously cite ""A Mathematical Principle of Deep Learning: Learn the Geodesic Curve in the Wasserstein Space"". Also adding more related literature will also be helpful to the readers.

Soundness:
4

Presentation:
1

Contribution:
4

Strengths:
- Identification of Residual Alignment: The paper identifies and characterizes an interesting and seemingly novel phenomenon of Residual Alignment (RA), and highlights its four key properties. The properties of RA are logically interrelated and consistently observed across various architectures. The authors present evidence to support the existence of RA.

- The authors conducted a comprehensive empirical study of the ResNet architecture, linearizing residual blocks using Residual Jacobians and analyzing their singular value decompositions. This approach provides valuable insights into the underlying mechanisms of ResNet models.

- Theoretical Proofs and Abstraction: One of the strongest points of this paper seems to be the theoretical proofs for the emergence of RA in the setting of binary classification with cross-entropy loss. The introduction of the Unconstrained Jacobians Model as a mathematical abstraction adds further depth to the analysis and strengthens the paper's theoretical foundations.

- The proof the main theorem in the appendix is interesting in its own right. They authors first linearize the loss function and then approximate it as a product of $(I+J_i)$ terms where $J_i$'s are the Jacobians. They go on to argue that the loss of a general configurtion (non-aligned singular vectors) is surprisingly bounded by a term that is equivalent to the aligned Jacobians. This is mainly done by invoking an interesting mathematical theorem. While I did not go into full detail of the proof and there might be details/flaws that I missed, the overall proof strategy makes sense and is very interesting, as it breaks down a very intricate problem to a tractable one. (As a side point, I recommend authors to conduct a few more detailed experimentation on the details of these steps, namely by directly. testing the inequalities they arrive at). 


Weaknesses:

# Major issues with writing & presentation of results

One of the main drawbacks of the current manuscript is its presentation. There are major problems with the flow and writing which hinder my understanding of some details. Another major issues is the lack of enough related works and connection to the existing machine learning & statistics literature. Overall, I would recommend authors to do a major revision of this manuscript to make it publishable and readable. 

 I tried to compile a list of minor issues with the writing to help with this.

- Fig 1 caption: “s true label and connected to form a trajectory” every connected line goes from input to output? Shouldn’t it have 34 layers (it looks like fewer dots than 34) 

- Equation line 34-35: what is $I$? 
- Equation between 57 & 58, what’s the dimensionality of $ \sigma’(…) $ in this equation? 
- Non capitalised sentences (examples 68, 70, 74, 92, 93, several more ) 
- Jacobians are input-dependent, how are the Figures 2 & 3 made? Do the figures correspond to a single sample input? or averaged across multiple inputs? 
- What does index $i$ in $J_i$ stand for?  This seems to be defined in the appendix but in the main text?
- line 59 “excluding the contribution from the skip connection.” What does this mean? 



Limitations:
yes

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper investigates the ResNet architecture, a popular deep-learning model known for its improved performance through skip connections. The authors aim to uncover the underlying mechanisms behind its success. They conduct an empirical study by linearizing the residual blocks of ResNet using Residual Jacobians and analyzing their singular value decompositions. The measurements and analysis conducted in the study reveal a phenomenon called Residual Alignment (RA), which is characterized by four fundamental properties. First, intermediate representations of a given input are evenly distributed on an embedded line in the space. Second, the top left and right singular vectors of Residual Jacobians align with each other and across different depths. Third, Residual Jacobians are at most rank C for fully-connected ResNets, where C represents the number of classes. Lastly, the top singular values of Residual Jacobians decrease inversely with depth. The study consistently demonstrates the occurrence of RA in ResNet models that generalize well. This phenomenon holds for both fully-connected and convolutional architectures across various depths and widths and different numbers of classes on benchmark datasets. However, RA ceases to occur when skip connections are removed. The authors also propose a novel mathematical model where RA is present. The findings suggest a strong alignment between residual branches in ResNet, imparting a rigid geometric structure to the intermediate representations as they progress linearly through the network until they reach the final layer, where Neural Collapse occurs.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Originality: Using the idea of linearization of the Residual Jacobian is commonly known for exploring the behavior of Residual Networks during the training and even pre-training. The authors use the idea of the Unconstrained Jacobian Model, which is interesting but also limited to a binary classification task. Still, it is novel to the best of my knowledge. 

Quality: The motivation and idea are clearly defined, and experiments support the RA phenomena.

Clarity: The paper would benefit from improved organization, such as relocating the related work section from section 5 to section 3 or 2. Additionally, the appendix appears disorganized, with figures located randomly. Other than these, it is written very well and easy to follow.

Significance: The study's results demonstrate the presence of the Residual Alignment (RA) phenomenon in the singular value decomposition (SVD) of the linearized Residual Jacobian. The mathematical analysis further confirms the occurrence of RA in binary classification. However, the paper lacks a deeper exploration of RA (2-4) and other potential insights.


Weaknesses:
Linearization of the Residual Jacobian and analysis of its SVD decomposition: Despite being a valid idea, is not novel. The authors could also potentially investigate the distribution of singular values with the help of random matrix theory. These singular values depend on the input feature vectors (or hidden representations), but this dependency is not explored.

Limited applicability to binary classification: The use of Unconstrained Jacobian Models, while interesting, is deemed limited in the context of binary classification tasks. The paper does not explore its potential beyond this specific task.

Organization and clarity: The paper would benefit from improved organization, particularly in the placement of the related work section, which could be better situated in an earlier section. The appendix also lacks clear structure and organization, with figures placed randomly throughout.


Limitations:
The authors do not provide potential limitations of their work.

Rating:
7

Confidence:
4

";1
cB0BImqSS9;"REVIEW 
Summary:
The paper introduces a new neural network layer which runs efficiently on modern GPUs, and exhibits strong performance against state-of-the art on several benchmarks. The layer is based on Monarch matrices, introduced in [7]. Monarch matrices use permutation matrices, and block diagonal matrices to represent dependancies across features and temporal dimensions. Inspired by butterfly matrices in FFT, this matrix parameterization can represent anything from convolutions to fully connected matrices, depending on their order. The Monarch Mixer layer introduced in this paper uses two monarch matrices as well as a matrix (K) that is used in point-wise multiplication. Performance is compared on language and image classification, replacing transformer and fully connected layers with the M2 layer. Finally, the paper presents a theoretical derivation of how this layer can be modified for use in causal language tasks, while maintaining its computational efficiency. 

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
Technically, this paper makes a strong contribution by proposing a computationally efficient layer. The computational performance is benchmarked on modern GPUs. I really liked the discussion on factors affecting runtime performance on modern GPUs -- this is a really valuable introduction to this topic. The proposed layer achieves superior performance, with fewer trainable parameters, in less time. Like MLPMixer, it also does away with the attention mechanism of transformers -- which suggests M2 is also a good architectural inductive bias -- perhaps competitive with attention? Comparing against [7], I would say the largest technical contribution of this paper is on how to modify the M2 layer perform in the causal setting.  

Weaknesses:
The main weakness of the paper is that it is somewhat hard to read/understand without having to read [7]. Section 4 in particular is quite dense. It seems that the authors struggled to fit the paper within the NeurIPS page limit. 

Limitations:
Limitations were addressed in second paragraph of Section 6. 

Rating:
8

Confidence:
3

REVIEW 
Summary:
This paper takes a fresh approach by addressing the issue of high complexity in current neural networks. It points out that the computational complexity of Transformers is quadratic with respect to both the sequence length and the feature dimension. Previous papers primarily focused on reducing the complexity related to sequence length, but this paper is the first to propose a method that reduces complexity for both sequence length and feature dimension. The specific approach used is the utilization of second-order Monarch Matrices, with the model structure referred to as the Monarch Mixer. Additionally, the authors introduce a novel initialization method for the Monarch Matrices, enabling them to handle causal l language modeling. The effectiveness of this approach is validated in the areas of non-causal language modeling, causal language modeling, and image classification.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
Indeed, it is novel to consider the optimization of complexity from both the sequence dimension and the feature dimension. Furthermore, initializing the model for the causal scenario poses a definite challenge, and the authors have successfully accomplished this task.

Weaknesses:
There aren't many weaknesses, for specific questions, please refer to the **Questions** section.

Limitations:
Yes.

Rating:
8

Confidence:
5

REVIEW 
Summary:
The Monarch Mixer (M2) combines MLP mixer and Conv mixer and yields a new family of mixers that is formalized in terms of Monarch matrices. the approach is novel and reminds me of an extension of [15] in their reference. 

The main advantage of M2 is in its sub-quadratic computation capability. The authors evaluate their method on a set of large language models and prove their framework functions properly on meaningful and challenging tasks. The paper is well-written and easy to follow. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper stands out in its clear and proper presentation. The idea is well motivated from a hardware perspective, explained intuitively, and studied theoretically. Evaluation of the method on causal and non-causal large language models is an asset and demonstrates a clear potential of the method.

Weaknesses:
- The new approach is only benchmarked on Transformer tasks, while speech applications look relevant but are ignored.
- It would be fair to see a comparison of inference latency and metric performance of the M2-based BERT with a configuration of BERT that has a smaller number of parameters, but the same number of FLOPs.

Limitations:
While the method clearly shows that the method works well on Transformer models, convolutional models, and speech applications are ignored. 


Rating:
8

Confidence:
3

";1
zJMutieTgh;"REVIEW 
Summary:
This paper introduces a novel inference attack algorithm for face recognition models that do not have classification layers. The proposed attack consists of two stages: membership inference and model inversion. The membership inference attack analyzes the distances between intermediate features and batch normalization parameters to determine if a face image belongs to the training dataset. The model inversion attack reconstructs sensitive private data using a pre-trained generative adversarial network (GAN) guided by the attack model.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1) The paper introduces a novel two-stage attack algorithm for face recognition models without classification layers. 
2) The proposed method outperforms state-of-the-art similar works and can recover the identities of some training members. 
3) This research has implications for the development of more robust and privacy-preserving face recognition models.

Weaknesses:
1) This paper would be beneficial to compare the proposed method with more state-of-the-art techniques to demonstrate its superiority.
2) This paper does not provide any code implementation.
3) The model performance used in the experiment is relatively low, and we hope to conduct experiments with models at a higher level of accuracy. (For example, ResNet200/VIT-Large on WebFace260M).

Limitations:
Please refer to the weaknesses section.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose a membership inference attack against face recognition (FR) models in the white-box scenario where membership information is known for some records and white-box model access is available, but without access to a classification layer. The attack utilizes information stored in batch-norm statistics and using a meta-classifier, the authors demonstrate the effectiveness of the proposed attack. They also extend the attack to improve model inversion attacks by utilizing their membership-classifier to ""reject"" generated samples that fall below a certain threshold.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- The utilization of an attack meant for one kind of privacy leakage (MI) in another scenario (model inversion) is interesting.

- The paper is well written, and proposed techniques/motivation are explained properly.

Weaknesses:
- L16: ""..very first....without a classification layer."" This is not the first work to explore FR models that do not use a classification layers. Please refer to [1, 2]. Similar claims appear on L39 about necessarily requiring logit access for good model performance. The authors in [1] report near-perfect detection for 3 different kinds of learning algorithms/models, none of which require classification logits.

- L25: ""attribute attacks (also known as property inference attacks)"" - these two are not the same at all. Similarly, L31-32 claim that all inference attacks on ML can be categorized into membership and model inversion attacks. Please refer to [3] for a detailed explanation and to better understand these differences.

- L188: ""If it is from the training data set, then the attack model should output 1, otherwise we expect it to output 0"" -how is this membership information obtained? As also outlined in Algorithm 1, the attack very clearly assumes access to not only batch-normalization parameters (which can only realistically come from full-model white-box access), but also knowledge of $m$ members and $n$ non-members. While the latter is reasonable, assuming knowledge of members is a very strong assumption (on top of an already-strong access model).

- L292 says ""...theoretically analyze.."" but nowhere in the paper did I see any theoretical analysis?

# Minor comments
- Figure 2: Why is Stage 2 on the left? It seems counter-intuitive.

## References

[1] Chen, Min, et al. ""FACE-AUDITOR: Data Auditing in Facial Recognition Systems."" USENIX, 2023

[2] Li, G., S. Rezaei, and X. Liu. ""User-Level Membership Inference Attack against Metric Embedding Learning."" ICLR 2022 Workshop on PAIR2Struct 2022.

[3] Salem, Ahmed, et al. ""SoK: Let the Privacy Games Begin! A Unified Treatment of Data Inference Privacy in Machine Learning."" 2023 IEEE S&P, 2023.

Limitations:
- Section 3.1 - most FR models are not trained in the way that the authors describe (and assume in their experiments) here. The norm is to focus on training good embedding models so that models can be scaled easily to enroll future participants. As expected, training the model repeatedly whenever a new user (""class"") is added is not optimal. 

- Figure 3: Model inversion is supposed to recover actual records from the training data, but looking at the images in the figure that doesn't seem to be the case. Many faces (like second from left, second from right in first 2 rows) are not the same people, but rather people that ""look like"" each other.

Rating:
8

Confidence:
5

REVIEW 
Summary:
The paper presents a novel method for inference attacks against face recognition method. In particular, it advocates two-stage inference attack, where the first stage identify the membership and the second stage involves model inversion attack that recover the input from embedding. Experimental evaluation shows that the proposed method can largely identify the correct membership and the model inversion sees good as well.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. It proposes a novel two-stage method for inference attack of face recognition systems.

2. Experimental evaluation shows the proposed method has promising results.

Weaknesses:
1. The evaluation is not thorough. It only conduct some ablation study with comparing to the existing attack methods.

2. The paper does not cover some import topic for inference attack of face recognition, such as the black box attacks for face recognition.

Limitations:
unavailable.

Rating:
4

Confidence:
3

REVIEW 
Summary:
In this submission, the authors advocate an inference attack composed of two stages for practical FR models. The first stage analyzes the distances between the intermediate features and batch normalization parameters. The second stage reconstructs data using a pre-trained generative adversarial network (GAN) guided by the attack model in the first stage.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1.The writing and presentation are good and easy to follow.

2.The experimental results also demonstrate some effectiveness of the proposed method.

Weaknesses:
1.The overall technical contributions are somewhat limited, firstly, the way of using BN to perform membership inference attack has been explored for a long time. And secondly, the inversed training data are from a pretrain GAN, which is heavily depending on the strength of the pretrain GAN. And optimizing the synthesized face data is too weak only by the single supervision from the first stage. 

2.And from the Figure 3, I don’t think the recovered face data is visually close to the original data for some of the pairs. Therefore, I doubt that whether the authors really achieve the initial goal, recovering the similar enough or effective enough face training data, by their proposed method or not.

3.The experimental comparisons are too simple and rough, lacking some important state-of-art related competitors. 

Limitations:
This submission has adequately addressed the limitations.

Rating:
3

Confidence:
4

";0
HUuEMMM8Ik;"REVIEW 
Summary:
This paper presents a method for detecting hidden confounding in observational data.  Without further assumptions, detecting hidden confounding is known to be impossible.  This paper takes advantage of a common scenario where one has access to observational data collected from multiple environments.  Given particular assumptions that some of the underlying mechanisms and/or exogenous factors are varying across environments, this paper presents a method to detect hidden confounding between a pair of target variables <T,Y>.  The paper presents the theory, a discussion of its assumptions and which can be weakened, synthetic experiments and one experiment in a real-world dataset.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- detecting hidden confounding under reasonable assumptions is a useful contribution.

- the paper is well written and easy to understand.  I appreciate the additional detail on assumptions in sec 4.1.

- experiments demonstrate implications in synthetic settings and 1 real-world dataset

Weaknesses:
- I don't think this approach will handle detection of hidden confounding under selection bias (i.e., where a data collection mechanism means not all data samples are visible in an environment).  This is a common challenge in practice, and it is worth mentioning this as a limitation and/or noting it in the initial problem statement.

- the paper addresses a scenario where causal mechanisms are parameterized by environment variables $\theta_{T,X,U,V,..}$.  If the causal mechanisms themselves are varying, I'm not sure I understand what the motivating causal inference scenario is.  That is, if the causal inference question is to identify $P(Y|T)$, how is that a well-defined question if the causal mechanism defining $Y|Pa(Y)$ is changing across environments?

- the paper would be improved with additional real world datasets.

Limitations:

The authors have largely addressed limitations and broader impacts.  I appreciate the note of caution regarding use of this work in high-stakes settings.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors develop a test for hidden confounding using only observational data from multiple environments with the same DAG under the independent causal mechanisms assumption.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is well-written and covers the related work well.
Their setting, assumptions, and contributions are clear and they meaningfully discuss the profound limitations of their work with potential solutions.
Under new (but strong) assumptions, they propose a test for no unmeasured confounding and provide an algorithm using only observational data. The proofs in the construction of the test and the test itself is novel and can be counted as original contributions.
They support their claims through adequate (semi)-synthetic experiments.

Weaknesses:
1. My main concerns about the paper is about its practical utility.
    - It is not often a practitioner has access to multiple observational datasets
    - The observational datasets may be highly heterogeneous, e.g., the type of data recorded, the way they are recorded, etc., making the preprocessing task for those datasets to be used in a way that is proposed in this manuscript very hard, if not impossible.
    - Analyzing observational studies is hard for a lot of reasons. As we increase the number of studies, the chances of introducing bias through other and often overlooked ways increase such as non-adherence to treatment assignments, or selection bias introduced while defining the treatment assignments and where the follow-up begins etc. Those can all be reasons behind rejection beyond confounding. Please correct me if I am wrong in that regard.

2. I find it hard to believe that different observational datasets will have the same DAG.
    - Different environments (e.g. hospitals) may have very different mechanisms (e.g. institutional practices) that result in different DAGs.

3. In summary, although the paper formalizes a framework where we can test the no unmeasured confounding assumption, I think the approach would only work in very controlled settings.


Limitations:
The authors acknowledge some important limitations page 6.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors focus on a setting where we observe treatments, outcomes, and a set of confounders across multiple environments.  While it's generally not possible to identify unobserved confounders in a single data set, when we have multiple data sets and assume the mechanisms operate in the same way across those environments (even if the parameters may differ), we can test for the presence of unobserved confounders.  The authors propose an independence test between a treatment in one environment and an outcome in another, and show that, if they are not independent (conditioned on the observed confounders and the other treatment), then there must be at least one unobserved confounder that's producing that dependence.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The presentation of this paper is excellent, and I found the explanation throughout very clear.  The idea comes across as fairly simple, which speaks to the clarity of the explanation.   The graphics in particular (Figures 1 and 2) go a long way towards making the approach understandable

I appreciate the attention paid to assumptions, including the discussion in 4.1 about assumption relaxation.

Weaknesses:
The main weakness I see of this paper is the lack of motivation for the setting the authors consider.  This work is focused on a situation where we have observational data from multiple environments where we have collected the same variables from all environments and, while parameters are allowed to differ between them, the actual mechanisms at play need to have the same structure.  The authors bring up the example in the intro of different hospitals administering the same treatment but serving different populations, but otherwise, I feel like this work would be served by a discussion of how realistic this sort of setting is, or how common it is in the real world.

Similarly, while I do like Section 4.1 and the authors' attention to the assumptions, the lack of clear real-world ties extends here.  What would it look like in real data if we had pair-wise dependent mechanisms?  Is this something we would expect to see frequently?  This lack of discussion makes it hard to assess how important these assumptions actually are, and how useful the relaxations are in turn.

Considering that Section 4.2 is the actual core of the approach, I wish more attention were paid to it.  I understand that space is limited, but including the actual algorithm you're proposing seems worth moving something else to the appendix.

Limitations:
The authors' approach is confined to a fairly specific setting, where the same variables, operating in the same manner (with different parameters), are measured across different environments.  The approach also only outputs a binary yes or no for whether hidden confounding exists.  If the test comes back positive for hidden confounding, it's unclear if this means causal analysis is impossible, or what the next steps should be.

In terms of societal impact, I don't see any real potential for negative impact.  This work can be used to test for one potential hurdle for accurate causal inference (hidden confounders), but there are many other assumptions that need to be met as well, and this paper does a good job at making those clear.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors proposed a detection method for hidden confounders, with only observational data in multiple environments. They, in particular, designed a testable independence condition for it, under the assumptions of (i) Faithfulness & Causal Markov Property, (ii)Shared Causal Graph, (iii) Independent Causal Mechanism Principle, and (iv) Non-degenerate Probabilistic Independent Causal Mechanisms. They also performed synthetic and semi-synthetic experiments to verify the effectiveness of their proposed method.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
This paper is well written with clear motivation and a description of their method.

What the authors focused on is indeed an interesting yet challenging research topic in causal inference. And the idea of using conditional independence conditions, especially between observations, is interesting to me.

Weaknesses:
The problem setting has some confusing notations.

The experiments cannot verify better performances than other methods for detecting latent confounders.


Limitations:
The needed number of environments is so high.

Rating:
4

Confidence:
4

";1
T47mUw8pW4;"REVIEW 
Summary:
This paper is an extension of a result from Chazal and Soufflet, which states that the Hausdorff distance of a set to its medial axis is lipschitz-bound under ambient deformations. The authors extend the result from C2 sets and C2 deformations to arbitrary closed sets and C1,1 diffeomorphisms which preserve a bounded sphere in the set. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Medial axis play a central role in vision, and 3D geometry and investigating may lead to novel approaches and algorithms. 

Weaknesses:
Unfortunately, I do not believe it is within my capacity to evaluate the full correctness of the theorems presented in this paper, as such I feel uncomfortable to recommend its acceptance. I am a first-time reviewer hence will reassess my thoughts given other reviewers' input, but in a sense, placing the proofs in the supplementary is a somewhat odd choice to me, as the paper is a completely theoretical paper with its crux being the proofs themselves. Additionally, I find the theorem somewhat esoteric, being both a slight extension of Chazal and Soufflet, and requiring the ambient deformation preserve a bounded sphere to only yield a lipschitz bound. Given the above, I find this paper more suiting to a computational geometry/mathematical journal than NeurIPS. 

Limitations:
Yes

Rating:
3

Confidence:
2

REVIEW 
Summary:
The medial axis of a closed set $\mathcal{S} \subset \mathbb{R}^d$ is defined to be the set of points in $\mathbb{R}^d$ which do not have a unique closest point on $\mathcal{S}.$ The authors develop a notion of stability for such sets with respect to ambient diffeomorphisms of $\mathbb{R}^d.$ The main result proves stability with respect to $C^{1,1}$ diffeomorphisms under additional assumptions about the set $\mathcal{S}$ (for instance, that $\mathcal{S}$ is bounded, See Assumption 3.8 for a full list.) This result is considered a generalization of an earlier result, which makes stronger smoothness assumptions (namely $C^2$) on both the set $\mathcal{S}$ and the class of ambient diffeomorphisms. The authors argue in the early parts of the paper that this extra generality is needed for (unspecified) applications in astrophysics.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The expository style in the early parts of the paper is inviting, where it does a good job of illustrating some basic notions, including familiar ones such as the medial axis and less-familiar ones like the generalized tangent space. 

Weaknesses:
There are a few different criticisms one can make of this paper:

  1. The topic is niche for a NeuRIPS audience.
  2. The main result is technical and difficult for non-experts to verify.
  3. The main result, as described in the introduction, is a marginal improvement over the current state of the art in reference [13], in the sense that one gains only less restrictive assumptions about the regularity of the functions and shape that appear.
  4. The connection to applications is tenuous at best, and no experiments are provided.

With regards to 1 and 2 above, let me draw attention to the statement of the main result in Theorem 3.9 and the preceding assumptions it requires, which take nearly a page to write down even with many prerequisite definitions that appear before it. One would at least hope based on the promises of the introduction that a simple definition of ""stability"" would be available for use in the statement of the theorem. With regards to 3 and 4, there is very little given to convince the reader that the $C^2$ results are insufficient for applications. 

I would not necessarily suggest that a paper with one or more of these deficiencies be excluded from NeuRIPS. However, given that all four issues are present, it seems better to focus on resolving some of them, or to send the paper to another venue (eg. a pure mathematics journal) where some of these criteria are judged to be less important.

Limitations:
n/a

Rating:
4

Confidence:
1

REVIEW 
Summary:
The authors prove that the medial axis of closed set is Hausdorff stable without any further assumption on it. In this proof, the authors achieve stability without pruning the medial axis which is a significant advantage. Meanwhile, the results hold for sets in arbitrary dimensions.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
In originality, this work holds for sets in arbitrary dimentions and removes the limitation of manifold assumption when in proof, and it does not need to prune the medial axis which is a significant advantage.

The quality and clarity is good enough, it is easy to understand the motivation, outline and contribution.

The proof in this paper implies that the medial axis of an imprecise shape is stable. The medial axis plays an important role in the field of computational geometry, computer vision and graphics. 

Weaknesses:
The result of this work shows the numerical stability of medial axis, but there is little analysis about the impact from noise size and quantity in real world data.


Limitations:
From line 36 to 39, the authors give the limitation about the work, but it could be more clear if there is more explanation. I also asked the questions about it in section Question.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This work proves that the medial axis of closed sets is Hausdorff stable, this extends existing stability result on the stability of the medial axis of C^2 manifolds under C2 ambient diffeomorphisms. The contributions are

1. This work makes no assumptions of the set except the closedness. The stability of the medial axis of smooth manifolds has been intensively studied in the literature, this work omits the manifold assumption.

2. The stability is achieved without pruning the medial axis. Large body of works have to prune the medial axis.

3. The stability results hold for sets in arbitrary dimensions and are insensitive to the dimension of the set itself.

This theoretical result plays a fundamental role in many fields, the generalization is important  to many practical applications.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The theoretic results are much general, it doesn't require the manifold assumption, it doesn't need to prune the medial axis, the results hold for any dimensions. 

The work is clearly represented. All the key concepts are explained thoroughly, the lemmas, theorems, corollaries are explained in detail, and rigorously formulated. The proofs are step by step, clean and easy to follow. 

Weaknesses:
The theoretical results are elegant and convincing. It will be helpful to give some numerical experimental results. 

Limitations:
The current stability result assumes the diffeomorphisms is a small perturbation of the identity, and it preserves the bounding sphere. This constraint seems to be artificial and inconvenient for practical applications. Maybe this requirement can be weakened or the bounding sphere is pushed to infinity.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proves the Hausdorff stability of the medial axis of closed bounded sets. This is a mathematics paper. The authors set up a foundation of their problem, then applied Theorem 2.6 (from [19]) to complete their proof. The end result is quite beautiful in fact. The authors also show that the results in [13] is a special case of their result.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is written well. Despite not having a mathematics background, I am able to read and understand the majority part of the proof. (Nitpick: there are small typos, for example, some \pi_{S}(p_4) are annotated incorrectly in Figure 1.)
- The authors proved a difficult result (as an indication, [13] is a special case of the result). The proof seems to be correct to me.

Weaknesses:
- I have a hard time understanding how this result can be used in machine learning / computer vision / computational geometry applications. Yet the motivation is explained in ln45 - ln73. However, I still do not see how this result can be applied. For the benefit of the readers, I think applications need to be demonstrated, otherwise Neurips might not be the right audience.

Limitations:
N/A

Rating:
4

Confidence:
1

REVIEW 
Summary:
In this paper, the authors analyze the stability of the medial axis of a set S, when S is perturbed by a map that is lipschitz with lipschitz derivatives.

This stability result is of interest in numerous applications in machine learning, such as astrophysics.

The author's results improve upon an existing result by Chazal and Soufflet in a few ways:
1. The authors remove an assumption that the set S must be a piecewise smooth manifold; here they only require S to be closed and bounded.
2. They do not require pruning the medial axis.
3. Their result holds in high dimensions.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
I think the result is significant, and of interest to the neurips community. Compared to Chazal and Soufflet, I think another significant aspect of this result is that this result is quantitative whereas Chazal and Soufflet's result is only qualitative.

Weaknesses:
I have some questions about how this paper's results compare to existing results, as well as about several aspects of the result (see below). These may not be considered weaknesses if the authors can address them.

Limitations:
n/a

Rating:
7

Confidence:
3

";0
SfXjt1FtMQ;"REVIEW 
Summary:
This paper introduces the Gaussian multi-Graphical Model (GmGM) as a novel method to construct sparse graph representations of matrix- and tensor-variate data. It simultaneously learns the representation across several tensors that share axes. The authors demonstrate that GmGM outperforms previous methods in terms of speed when applied to matrix data. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1.	GmGM extends the application of Gaussian Graphical Models to multi-tensor datasets, presenting a novel approach in the field.

2.	GmGM exhibits significantly improved speed compared to previous methods when dealing with matrix data.

3.	The results of GmGM on five real datasets are well explained.

4.	Especially, I appreciate the comprehensive discussion provided in this paper. The authors present cases where the results are excellent, as well as cases where the results are not as impressive, such as the performance on higher-order tensor data (fig 4b) and the E-MTAB-2805 dataset (fig 6a). This in-depth analysis helps readers gain a better understanding of the method and be aware of the situations in which it should be employed.

Weaknesses:
One major concern I have relates to the evaluation. Although the authors present many intriguing findings on the datasets, it would be beneficial to include some more quantitative analysis.

Limitations:
Yes. It is well discussed in the study.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The authors propose Gaussian multi-Graphical Model (GmGM), a novel approach to constructing sparse graph representations of matrix- and tensor-variate data. It stands out from previous models by learning representations across multiple tensors that share axes simultaneously, a feature crucial for analyzing multimodal datasets, particularly in multi-omics scenarios. The GmGM algorithm utilizes a single eigendecomposition per axis, which results in a significant speedup over previous models. This efficiency enables the application of the methodology on large multi-modal datasets, such as single-cell multi-omics data, a task that was challenging with previous approaches.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. Fair and Interesting Motivation: 
The paper's motivation on model multi-tensor decomposition with shared axis is rooted in the real-world need for handling multi-omics scenarios, which often involve multi-tensor data with shared axes. The GmGM is introduced as a solution, addressing a significant gap in existing data analysis methodologies and providing a fair and interesting motivation for the study.

2. Reasonable solution and impressive improvements in Efficiency
The GmGM model stands out for its impressive efficiency improvements, achieved through the use of the KS decomposition of the precision matrix and transiting it to the eigen-decomposition over each dim. This approach results in a substantial speedup over previous models, enabling the handling of large multi-modal datasets, This efficiency, coupled with the model's ability to maintain state-of-the-art performance, underscores the strength of the paper.

Weaknesses:
1. **Limited Technical Contribution**

While the problem setting proposed in the paper is reasonable, the algorithm's strict assumptions about data integrity (no missing data) and quality (no noise) somewhat limit its potential for broader application. The authors are encouraged to consider relaxing these assumptions or proposing strategies to handle missing data and noise, which are common issues in real-world datasets. Addressing these issues could significantly enhance the model's practical utility and broaden its applicability.

2. **Improvements Needed in Representation and Flow**

The paper could benefit from substantial improvements in its representation and flow. The omission of important concepts and content significantly hinders reader comprehension. Some sentences appear casual and can lead to confusion. The overall logical flow of the paper is not clear, making it difficult to follow. This is particularly evident in the following areas:

   - Concepts such as the Kronecker product and Gram matrix are not clearly introduced.
   - Many notations and their subscripts and superscripts in the algorithm table are not clearly defined.
   - The task setting and metric definition in the experimental section are vague, reducing the persuasiveness of the validation part.

Overall, the authors are encouraged to make a concerted effort to reorganize and polish the paper's presentation, improve the flow, and highlight the key points of the work and problem. This could significantly enhance the readability and impact of the paper.

Limitations:
See weakness parts

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes the Gaussian multi-Graphical Model, a novel method to extend the use of Gaussian Graphical Models to multi-tensor datasets. It generalizes Gaussian graphical models to the common scenario of multi-tensor datasets. For the single-tensor case,  the proposed algorithm is faster than prior work while still preserving state-of-the-art performance.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
The paper considers an interesting and still challenging topic, extending conventional Gaussian graphical models (GGM) for complex systems like multi-modal data models. The paper has been generally well-written and the problem has been clearly defined. Indeed, the theoretical parts that extend the GGM to multi-tensor datasets have proper quality.  This algorithm is significantly faster on lower-order tensor data (reported for the synthetic data sets) and its efficacy is slightly better in the real-world data sets. 

Weaknesses:
- Some parts of the paper should be checked again. For instance, line 52 starts to explain the computational costs of the state-of-the-art methods. The parameters n and p have not been defined before. It seems it uses the defined parameter in the main reference paper (Kalaitzis et. al. 2013), where n and p are the numbers of observations and features, respectively. Indeed, the computational costs of the other baselines need a piece of clarification. For instance, O(n^2 * p^2) in BIGLasso represents the number of non-zeros in the Kronecker-sum (KS) structure. It would be better if the authors consider the full cost of the algorithm for the proposed method and available baselines.

- The paper models each tensor as being drawn independently
 from a Kronecker-sum normal distribution. It makes sense to see this assumption reduces the computational cost at least in small-order data sets. However, it does not describe how this strong assumption still preserves state-of-the-art performance.

- As has been reported in the paper, the proposed solution can not improve the complexity of higher-order tensor data sets (fig. 4b). Indeed, its performance can not significantly outperform the other baseline (Fig. 5a). By decreasing the sparsity, the performance of the model suffers and it seems it works properly only on high sparse graphs (Fig 7).

Limitations:
The authors addressed the limitation of the work in the paper. 

Rating:
6

Confidence:
3

";0
IklhryC2up;"REVIEW 
Summary:
The paper studies the idealized phenomenon of lossless compressibility, whereby an identical
function can be implemented with a smaller network, in the setting of single-hidden-layer hyperbolic tangent networks.  It introduces the notion of rank as the minimal number of hidden units (used to measure the ""size"" of a network) required to implement a given NN function and a constructive algorithm for computing it, as well as the notion of *proximate rank* as the rank of the most compressible parameter vector within a small L_infty norm ball and an algorithm for upper-bounding it. The paper further shows that bounding the proximate rank of a given parameter vector is an NP-complete problem. 


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
While I'm not an expert on algorithms and cannot comment on correctness or the novelty of the approach, 
I followed the high-level argument and believe the contribution is significant. The paper studies a fundamental aspect of deep learning, i.e., the compressibility and description length of neural networks, which is mostly dominated by empirical research and demands better theoretical understanding. The paper goes beyond the classical setting of lossless compressibility (c.f. Sussmann 1992) by introducing the notion of proximate rank and proving a basic hardness results for it, which will hopefully lay a foundation for future studies of this topic. 

Weaknesses:
Section 6 (Computational complexity of proximate rank) is a bit hard to follow for non-experts. One possibility could be to include a discussion at the beginning of Section 6 that summarizes the proof at a high-level. 

Limitations:
The authors adequately addressed the limitations.

Rating:
7

Confidence:
2

REVIEW 
Summary:
This paper studies the replication of the same function using a smaller network. It provides a procedure for achieving optimal lossless compression in the context of single-hidden-layer hyperbolic tangent networks.

This paper introduces the idea of 'rank' of a parameter, defined as the least number of hidden units needed to replicate the same function. The paper further defines the 'proximate rank' of a parameter as the rank of the most compressible parameter within a limited L-inf neighborhood.

The paper also demonstrates that estimating the proximate rank is an NP-complete problem through a reduction from Boolean satisfiability using a geometric problem that involves covering points in a plane with small squares.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. This is a theoretical treatment of a very important problem and the results are interesting.

2. The paper shows that the problem of bounding the approximate rank below a given value is an NP-complete decision problem.

3. Paper is well presented and clear.

Weaknesses:
1. The results do not seem to be important. Specifically, does anyone care if instead of totally lossless compression, we have an infinitesimal error in representation?

2. The algorithm given in this paper is only applicable to single-hidden layer hyperbolic tangent networks, without obvious ways of extending it to more general cases.

3. The paper aims to provide a theoretical ground for network compression, but did not argue extensively on its connection on the empirical success of existing network compression literature.

Limitations:
1. In practice it is allowed that the compressed networks have slight performance drop compared to the original network, but this paper does not consider this problem.

2. The proposed algorithm only considers the single-hidden layer hyperbolic tangent networks, which is not often employed in practice.

Rating:
4

Confidence:
2

REVIEW 
Summary:
The authors propose two notions for studying neural network complexity with, on single hidden layer neural networks. The first is _rank_, which corresponds to the smallest number of parameters that can produce a network that is functionally equivalent to the original, and the second is _proximate rank_, which is the rank of the network with smallest rank in an $L^\inf$ neighborhood of the original model('s parameters). The authors propose an algorithm for computing rank, and propose a heuristic algorithm for upper bounding proximate rank. They also show that exactly computing proximate rank is NP-complete, so any future study will have to rely on approximations / bounds on this quantity.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
- The notions the authors introduce are interesting and future work can indeed utilize them in interesting ways. The metrics they propose importantly diverge from metrics such as $\ell_p$-compressibility in that they are defined based on functional equivalence, which might imply them being more useful for future learning theoretic research.
- The paper is very well structured, and the exposition is clear and easy to follow. Connections to previous literature are made clear (but see below).

Weaknesses:
The paper is beset by two important problems:
- As the authors widely acknowledge, their notions are defined on single layer neural networks with tanh activation, with no clear map for generalization to more realistic setups. It would be valuable to see a roadmap for this, as it would help evaluate whether proximate rank is a potentially useful theoretical notion for future research or in the worst case a ""dead end"" in terms of applicability to realistic ML models.
- The authors do not discuss how or why their notions of rank and proximate rank could be useful for future research on generalization in deep learning (or robustness etc.). Given the venue the paper is submitted, I am assuming that the authors believe that these results might have learning theoretical implications down the line, but they are mostly silent on this issue. They correctly observe that approximate compressibility research has been utilized for such aims, but do not opine on how their alternative notions should be superior or even have qualitatively different contributions to such research. While this is not a problem per se in general, it is more so for a machine learning conference.

Limitations:
The authors are transparent about the limitations of their work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper deep-dives into the concept of lossless network compressibility. It presents an algorithm for optimal lossless compression in single-hidden-layer hyperbolic tangent networks, which can in part generalize to other more relevant feedforward architectures. The authors introduce the novel concept of ""proximate rank"", which measures network complexity, and demonstrate that bounding the proximate rank is an NP-complete problem, thereby suggesting that the problem of finding highly compressible networks is very hard.         

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- originality: The paper introduces novel approaches to obtain lossless compression and introduces the novel concept of proximate rank as a measure of neural network complexity. Furthermore the paper also introduces novel techniques to prove that bounding the proximate rank measure is an NP-complete problem. In summary, many novel contributions which, as the authors argue, lay the foundations for future work identifying losslessly compressible parameters in deep learning structures.   

- quality: The quality of the paper seems sound. The topic is a bit away from my area of expertise and I did not check the math in detail though.

- clarity: the paper is very well written. 

- significance: Lossless compressibility is a fundamental topic in deep learning, particularly since most recent state-of-the-art research involve very large foundation models. Having a theoretical framework and some robust approaches to measure network complexity and compressibility are key to advance the field of deep learning and the applicability of modern architectures.   

Weaknesses:
Single-hidden-layer hyperbolic tangent network can be a good subject to start developing a theoretical framework and a set of tools to measure and optimize network complexity and compressibility, but as the authors point out, this architecture is of little relevance otherwise (for current research). This issue, which is raised by the authors, is a minor weakness but it does limit the potential impact of the present work.

Limitations:
Authors have addressed all the limitations clearly. 

Rating:
6

Confidence:
1

";0
7FitZnnnu8;"REVIEW 
Summary:
This paper uses optimal transport for learning the parameters of a DAG structure that represents a Bayesian network given samples drawn from it (data). In the proposed algorithm the data can be incomplete (i.e. some random variables may be latent). 
The algorithm can be seen as a generalization of the existing ""Optimal transport-based divergence minimization"" on target distributions that are factorized as a product of conditional univariate densities. 
  

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The generalization of the Optimal Transport to Bayesian networks is interesting and as far as I can say novel (though, more or less straightforward).
2. The paper is well-written (In particular the first two sections).
3. Several experimental models are presented.  
  

Weaknesses:
1. This method is only suitable for learning the parameters of Bayesian network DAGs. It can not be used for learning the DAG structure. 
2. The description of the proposed algorithm (section 3 of the draft) is quite compact and many details are missing.
More details are presented in the supplementary material (Section B). Still, even there, the cost function and push-forward divergence measure that are used in the paper's experiments are not spelled out.  

 
 

Limitations:
yes

Rating:
5

Confidence:
3

REVIEW 
Summary:
In this submission, the authors propose an optimal transport-based method to infer the parameters of probabilistic directed graphical models from partial observations. 
In particular, given a DAG associated with the target model, the proposed method reparameterizes the probability of a node conditioned on its parents by an encoder with external perturbation. 
Accordingly, a stochastic decoder is applied to map each node to the conditional density of its parents. 
The above two modules lead to a model with an auto-encoding architecture, which can be learned like a Wasserstein autoencoder (WAE), as shown in Eqs. (2, 3).
Experiments on the inference of LDA, HMM, and discrete representation models demonstrate the potentials of the proposed method.

--- After rebuttal ---

Thanks for the authors' efforts in the rebuttal phase. After reading other reviewers' comments, my main concern about this work is still its similarity to WAE, especially the theoretical part. Although the authors claimed that WAE can be viewed as a special case of this work, in my opinion, it is more likely that this work is a special case of WAE. I am satisfied with the other part of this work, so my final score is kept as ""borderline accept"".

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well-written and easy to follow.

2. The proposed method is reasonable --- the objective function is based on the theoretical part of WAE, whose rationality has been guaranteed. In addition, the implementation of the proposed method is simple.

3. The authors consider various applications, demonstrating the universality of the proposed method.

4. The limitations of the proposed method are discussed, and the potential solutions are provided at the same time.

Weaknesses:
1. If my understanding is correct, Theorem 1 in this submission is a special case of Theorem 1 in the WAE work [a]. The final objective (Eq.(3)) approximates the Wasserstein distance by relaxing the constraint of phi_i to a regularizer, which is also similar to the strategy of WAE. The authors should discuss the connections and the differences between the proposed method (and theory) and that in [a]. 

[a] Tolstikhin, Ilya, Olivier Bousquet, Sylvain Gelly, and Bernhard Schoelkopf. ""Wasserstein Auto-Encoders."" In International Conference on Learning Representations. 2018.


2. As the authors mentioned, the proposed method leverages the amortization strategy to reparameterize the conditional distributions. Therefore, in the experimental part, the authors should consider some amortization methods as baselines, e.g., those in [b, c, d].

[b] Kim, Yoon, Sam Wiseman, Andrew Miller, David Sontag, and Alexander Rush. ""Semi-amortized variational autoencoders."" In International Conference on Machine Learning, pp. 2678-2687. PMLR, 2018.

[c] Agrawal, Abhinav, and Justin Domke. ""Amortized variational inference for simple hierarchical models."" Advances in Neural Information Processing Systems 34 (2021): 21388-21399.

[d] Huang, Chin-Wei, Shawn Tan, Alexandre Lacoste, and Aaron C. Courville. ""Improving explorability in variational inference with annealed variational objectives."" Advances in neural information processing systems 31 (2018).


3. The datasets used in the experimental part are relatively simple and small. Especially in the experiments of discrete representation learning, I wonder 1) whether the proposed method can deal with images with larger sizes, e.g., face images in CelebA, and 2) besides reconstruction, whether the proposed method can generate images with tolerable qualities.

Limitations:
The authors discussed the limitations of using the amortization strategy at the end of the submission. 
Some potential solutions are proposed at the same time.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose a method for learning parameters $\theta$ of a DAG within an optimal transport (OT) framework, minimizing the Wasserstein distance between the data distribution $P_d$ and the model distribution $P_\theta$ in $\theta$. The Kantorovich formulation of this problem is a minimization of an expected cost $c(X,Y)$ over all joint distributions on $X,Y$ such that marginally $X \sim P_d, Y\sim P_\theta$; this is implemented in practice by empirically drawing $X_i$ from the data, and then $PA_{X_i}$ conditionally on $X_i$ to satisfy $PA_{X_i} \mid X_i \sim P_\theta(\textrm{PA}_{X_i})$ through the use of stochastic “backward” mappings. This makes optimization tractable over the space of backward mappings $\phi$ and model parameters $\theta$, so long as the constraint above is relaxed to a regularization term.   


Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
* DAGs represent an extremely rich family of models, and the work also generalizes to settings with unobserved variables, making this method easy to apply in a variety of settings.
* Unlike variational methods, evidence bounds need not be computed; the proposed method is more “direct” in this sense. 
* The final optimization objective is easy to compute and computationally cheap.
* The framing of the problem from the lens of OT is novel to my knowledge, and provides an interesting formulation for optimization.
* The most significant strength of the paper is the evaluation of the method on a rich test suite of interesting problems such as LDA and Poisson time series segmentation. Comparisons show that the proposed method outperforms existing methods such as Batch EM and SVI in a variety of scenarios and metrics.


Weaknesses:
* The method reuqires that the random variables be reparameterizable in the sense of the equation at line 122 (this equation should be numbered); this may limit the family of joint distributions that can be considered. Though the authors say on line 167 that discrete variables can be used, reparameterization is tricky in these cases (as the authors acknowledge in the limitations section). 
* The backward maps $\phi_i$ are a confusing quantity (and potentially hard to fit); see the Questions section. This could be due to a lack of understanding of my part. As I understand it, however, it raises questions about the efficacy of the proposed method.
* The formality of the OT framing is appealing, yet this formality is ultimately dropped for a regularized analogue that does not solve exactly the same problem that is posed.
* Some notation is confusing; for example, does $PA_{X_i}$ include exogenous variable $U_i$? The discussion around line 120 and line 147 suggest so, but then notationally the equations at line 122 and 136 suggest otherwise.


Limitations:
The settings where the work can be applied are discussed by the authors. A limitations section is included that proactively assesses some limitations of the proposed method. 

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors propose a framework for learning the parameters of directed graphical models based on the idea of fitting by selecting the parameter values that minimize the Wasserstein distance (WD) between the data and model distributions. They prove (Thm. 1) that these distances can be characterized as the result minimizing a cost functional over a family of constrained stochastic 'backwards mappings', which yields a solvable objective when the constraint is relaxed and a regularization term is added to the objective.  

The approach is illustrated via a series of experiments, including 3 real-world datasets, in which the proposed OPT-DAG method outperforms various baselines across disparate tasks. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
I'm am not particularly familiar with the literature on DAG learning, nor on learning with 'optimal transport' objectives, so the extent of the originality of this paper is impossible to gauge. Operating under the assumption that this is the first work to introduce an OPT objective in this context, I would feel confident in stating that this is a fairly significant innovation. 

The presentation of the paper is, in my opinion, of a high standard (modulo one or two concerns that I will voice in the next section). The appendices provide a considerable depth of exposition of their procedure, and I am satisfied that the most pressing immediate concerns are addressed, either there or in the main body. 

Weaknesses:
I believe that the experimental section as well as the discussion could be improved. Again, I am unaware of what constitutes the benchmarking standard for DAG learning methods, but it seems to me as though the following are not adequately addressed:

a) The authors mention that their goal is not to achieve state-of-the-art performance, but rather to demonstrate the inherent versatility of their method. Situations in which their method might not be feasible are alluded to in Section 5 (Limitations), but the discussion here is extremely terse. Do these situations pose problems for competing alternatives as well? The chosen examples strike me as being somewhat simple. Do the authors claim that these examples are roughly representative of DAG learning problems generally? 

b) For the topic evaluation example (239-250) OPT-DAG is outperformed by the baselines in the 'diversity' metric in all three datasets, and outperformed on 'coherence' on the DBLP data. These results are reported with precisely no discussion or explanation. 

c) Section 4.2, first part (251- 266) could use some clarification. For example, the authors say ""we generate a synthetic dataset D with 200 observations at rates {12, 87, 60, 33} with change points occurring at times (40, 60, 55)."" If the model is as they say, (with a change of state happening with probability $1-p$, of which the lowest value is when $p = 0.95$, then a) why are there so few change points? At $p = 0.95$ would we not expect 10? Perhaps I am confused about what the authors are doing. Have they fixed the dataset, and computed the estimates of the rate parameters assuming that $p$ is fixed at the values indicated in the table (i.e. they fit the model 6 times, each time with a different assumed p)? Also, averaged over just the values $p = 0.75, 0.95$, OPT-DAG is actually inferior to MAP. Again, no explanation or discussion is forthcoming. 

d) Generally, it is not clear to me if the baselines being compared against are the most appropriate. For instance, in the final example, VQ-VAE is used as a baseline, and its poor performance is linked to a phenomenon called 'codebook collapse'. The paper provided as a reference in fact proposes an extension to the vanilla version (of VQ-VAE) that they seem to be comparing against, which seems to indicate that not only is their baseline not state-of-the-art, it is in fact very well known to not be so... 


Limitations:
Limitations are discussed, but as per my comment above, it does not seem to me as though this discussion is adequate. The authors seem to have a variety of situations in mind in which their method will either not work or not be competitive, and a more explicit discussion here would be welcome. 

Rating:
6

Confidence:
3

";0
Bto5a6w06l;"REVIEW 
Summary:
This paper proposes a comprehensive stable diffusion compression pipeline, including architecture compression and knowledge distillation. The authors apply this technique to general text-to-image generation and subject-driven image generation tasks, showing competitive performance as compared with the original stable diffusion while reducing parameter numbers and training dataset size.

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
1. The approach proposed in this paper is straightforward.
2. The writing of this paper is easy to understand.

Weaknesses:
1. The most important issue is that this paper is just a marginal improvement based on stable diffusion, so it cannot be generalized to other text-to-image synthesis frameworks, which severally narrows its application scope.
2. Reducing the SD parameter number from 1B to 0.76B or 0.66B is not so significant that this approach may not bring leaping progress in industrial applications.
3. The output and feature distillation are just classical distillation strategies that are widely adopted in other domains, e.g., image classification or GAN-based image generation. So here is a lack of innovation.

Limitations:
N/A

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper studies model compression for stable diffusion models (1.4 specifically). Specifically, they study 1. removing entire layers from the diffusion U-Net, 2. fine-tuning from a small subset of LAION to recover the loss, and 3. knowledge distillation from intermediate feature layers. They show that they can get a significant (~30-40%) reduction in both parameter count and inference latency at a modest output quality degradation, at a modest training cost (60-300 hours on a single GPU) that individuals can have access to.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper works on a timely topic, especially given that Stable Diffusion has widespread use in the open source community, and reducing the inference cost will not only have a broad impact on usability for these models, but also the environment. The paper is easy to understand, and seems to be first to evaluate structured pruning (broadly) on Stable Diffusion to the best of my knowledge. The analysis includes several different metrics and ablation studies that are useful to the community.

Weaknesses:
I found it hard to intuitively understand why the layers were specifically chosen to remove. One of the interesting results the authors show in this paper is that removing the mid-layer without any additional fine tuning already results in decent quality (competitive with some of the smaller models that are fine-tuned). Then, a natural question that might come up then is if we were to remove every layer and run an analysis of how much cost (in quality loss) the removal of each layer ends up with, would we then be able to confirm that the layers that were removed are in fact the best layers to remove in terms of their quality cost? This would be much cheaper of an experiment to run than fine-tuning the entire removed architecture, and would be very helpful to know if the ""unsupervised"" structured pruning is a good indicator for the end performance after fine-tuning. 

It would at least be interesting if there could be an analysis of how much the fine-tuning (and KD) adds (in terms of quality) to an 'unsupervised' baseline, whether that means running an ablation specifically with the mid results removed, or adding an 'unsupervised' baseline with all layers removed (the orange and red in Figure 3). 

This is very much concurrent work (and does not affect my score), but it could be worth adding https://arxiv.org/abs/2305.10924 to the citations. 

Limitations:
The limitations are adequately addressed. It might also be interesting if the authors can comment whether model compression itself could have risks in society, for example if people prefer worse but slightly faster models over better models, then that could potentially exacerbate the bias that these models have (and of the models people use in practice). 

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper presents an technique to compress the network structure of a diffusion model. Specifically, the method combines a static block pruning strategy and a knowledge distillation retraining to learn compact diffusion model. Experiments on text to image generation and customization shows the effectiveness of the approach. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The paper shows that conventional compression techniques leveraged in CNNs/GANs can also be used for diffusion model.
2. Some experiments are conducted to show the effectiveness of the method.

Weaknesses:
1. The paper is not novel at all. Pruning and distillation has long been used for vision network compression since the era of CNNs and later GANs. Nothing surprising comes out from this paper, where the author just simply combine two techniques and makes it work. Block pruning and output + feature distillation is developed far long ago and naively combines them show little academic value.
2. The paper lacks insight significantly. There's no theoretical proof nor ablation study to judge the design choice. For example, why shall we remove the second pairs of R-A instead of the first pair of A-R in down blocks? Why shall we retain the third R-A pair in the up blocks? Why can't we do selective removal where some blocks only remove R and some blocks only remove A? The pruning design choice is totally ad-hoc where nothing systematic and insightful comes out. Due to this, the method seems to only be applicable on SD-v1.4. I see no way that how this method can be applied to SD-v2.1 / SDXL or any other new SDs where we simply add one/two more modules of R/A into a block.   
3. The use of distillation-based pre-training sounds weird. Training/fine-tuning with distillation to recover the performance is a more appropriate wording.
4. In Sec 4.1, the paper says that the BK-SDM is only fine-tuned on 0.22M image-text pairs. Why only fine-tuned on such smaller scale of dataset? How about fine-tuning on 2.2M or 22M? Would the quality get improved? How about 22k or even fewer? Will the quality stay the same? No judgement is presented here as well.
5. In Figure 5, the paper should compare more diffusion method instead of showing the first 3 column of GAN-based approach. Since the speed of the network falls into the scale of diffusion model, it does not make much sense to compare GANs.

Limitations:
Yes, the author addressed both. 

Rating:
3

Confidence:
5

REVIEW 
Summary:
This submission proposes a light-weighted network for stable-diffusion-based text-to-image generation.
The proposed network is named BK-SDM, which became light-weighted thanks to removing unnecessary layers
and knowledge distillation.
An interesting finding is that removal of the entire mid-stage (the most low-resolution part of the u-net) does not seriously harm
generation quality.
As a result, 30% computational cost reduction was achieved while retaining 97% scores of generated image quality.


Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
- Light-weight diffusion models are, despite their importance,  not explored. I appreciate pioneering this direction.
- Training using a small subset (0.22M image-text pairs from LAION) is a nice point that is easier to reproduce, while keeping the generation quality.

Weaknesses:
-  Methodological novelty is limited because layer removing and knowledge distillation are well-used technics, while they were not done in diffusion. I did not find special challenges in applying them to diffusion models.

- Trades-off between speed and quality are not aggressively investigated; the only presented model is the  30% faster / 97% worse version. For example, what happens when the model is 90% faster? Is it possible to achieve any speed-up with 100% quality? Perhaps too aggressive speed-up results in the uselessly worse generation quality, but trying and clarifying it would be a useful contribution for a paper.

Limitations:
Limitations and potential social impacts are properly discussed.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work explores traditional model compression for diffusion models, aiming to mitigate their considerable computational overhead. By employing block removal and knowledge distillation, the authors are able to reduce parameters by over 30% with only minimal performance loss with the 0.22M LAION dataset. The paper further showcases its applicability for personalized generation tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed hand-crafted compression strategy, that leverages block-removing knowledge distillation, 
simple, but effective.  
2.  This work demonstrates the effectiveness of their proposed method on different model sizes, different tasks, including text-image generation and personalized text-image generation. 
The paper provides robust evidence of the approach's versatility across varying model sizes and tasks, which include both text-image and personalized text-image generation.


Weaknesses:
1. The main concern lies in the hand-crafted block removal process from the original DDPM. It may be more reasonable to calculate an importance score for each block and remove them accordingly. The simplicity of the current approach, while appreciated, seems to compromise on the degree of novelty. Please further clarify this.

2. A comparison of different sampling steps is missing. Will the model compression influence the trade-off between fidelity and sampling efficiency? 


3. The proposed methods seem to be general for the diffusion model.  Could the authors elaborate on the reason not to include experiments on general image generation?

Limitations:
Yes

Rating:
5

Confidence:
4

";0
c5Inzw6giM;"REVIEW 
Summary:
The paper employs a few heuristic methods to accelerate logistic regression training over encrypted data. The heuristics considered include: a new loss function called squared likelihood error (SLE) along with a polynomial approximation of sigmoid function, a faster gradient-descent method based on quadratic gradient, and a matrix encoding method called volley revolver.  

Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
The only strength of the paper is that it attempts to solve a really challenging problem of learning over encrypted data and it provides the code apriori through an anonymous GitHub link.

Weaknesses:
1) First and foremost, the title of the paper is misleading. The paper never deals with CNN training even in the limited context of transfer learning. It is true that most practical ML applications start with a pre-trained model and finetunes the parameters of this model. However, transfer learning implies that the whole model is finetuned apart from learning the application-specific last fully connected (FC) layer. What this paper attempts to do is just learn the last FC layer, which is nothing but multiclass logistic regression (MLR) training. Therefore, the title of the paper should not claim anything about CNN training.

2) Numerous attempts have been made over the last five years attempting to achieve MLR training on encrypted data, which have not been acknowledged in this paper and compared against. For example, see the works starting from:

[A] Crawford et al., ""Doing Real Work with FHE: The Case of Logistic Regression"", 2018
[B] Han et al., ""Logistic regression on homomorphic encrypted data at scale"", AAAI 2019
[C] Bergamaschi et al., ""Homomorphic Training of 30,000 Logistic Regression Models"", 2019

3) This current paper appears to be very similar to the rejected NeurIPS 2022 submission entitled ""Privacy-Preserving Logistic Regression Training with A Faster Gradient Variant"". While the NeurIPS 2022 submission focused on only the quadratic gradient component, the current paper also introduces the SLE loss. However, it is not clear how this SLE loss function is better. Moreover, what is the expression for the gradient of $ln L_2$ and where is it used in Algorithm 1?

4) The so-called volley revolver does not constitute any novel ""matrix-encoding"" method. Such packing tricks are regularly used in the context of efficient SIMD operations in FHE.

5) Overall, none of the three claimed contributions (namely, quadratic gradient, SLE loss, and volley revolver) appear to be original or significant enough to make an overall impact.

6) Finally, though the paper claims that the goal is to make logistic regression training practical, not a single experimental result has been shown to prove this point. Running 2 iterations with 128 MNIST images takes approximately 21 minutes and the last line claims that real experiments would take ""weeks, if not months"". There are other reported works in the literature, which showed more realistic results.

[D] Nandakumar et al., ""Towards Deep Neural Network Training on Encrypted Data"", CVPR-W 2019
[E] Lou et al., ""Glyph: Fast and Accurately Training Deep Neural Networks on Encrypted Data"", NeurIPS 2020

Limitations:
All the limitations have not been presented and addressed. There appears to be no potential negative societal impact.

Rating:
2

Confidence:
5

REVIEW 
Summary:
The paper presents a method for CNN transfer learning implemented in homomorphic encryption to protect privacy.


Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
I'm not aware of the method being implemented in HE before.


Weaknesses:
I cannot judge the machine learning aspects, but I don't see a strong novelty on the cryptographic side. The paper claims that some prior work is overly complex without going into details.

I find it concerning that the work relies relatively heavily on non-peer reviewed references by a single author (5 out of 19).

Line 150 says ""well-studied by several works"" without giving any reference.


Minor issues:
- l6: ::
- l15: .;
- l50: pervacy-persevering (privacy-preserving?)
- l51: diffuclt
- l71: seveal
- l154: After many attempts (unscholarly language)


Limitations:
n/a

Rating:
3

Confidence:
2

REVIEW 
Summary:
In this paper, the authors proposed a CNN training technique on the homomorphic encryption domain based on transfer learning. A gradient variant called Quadratic Gradient on homomorphic encryption was proposed. And a sigmoid function-based Softmax approximation was proposed. In addition, a new loss function for squared likelihood error was proposed, and a matrix-encoding method called Volly Revolver was also proposed. Finally, they released the code they implemented.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
Properly implementing functions for training on a homomorphic encryption domain is challenging. It is worth evaluating for implementing this and also disclosing their source code.

Weaknesses:
This paper performed a simulation on the MNIST dataset for performance evaluation. This seems too simple a dataset, even considering the homomorphic encryption environment. Although they claim that it is to be the first implementation of transfer learning-based CNN training on the homomorphic encryption domain, a similar study was recently published first. Of course, this paper takes a different approach.

[*] https://openreview.net/forum?id=jJXuL3hQvt

This paper is considered incomplete in several respects. The main reason is that the proposed scheme's threat model needs to be clarified. At first, ""the proposed architecture"" is not clear. There needs to be a description of the proposed architecture. They should explain the exact part where homomorphic encryption was actually carried out in the transfer learning process and what benefits can be gained from doing so.


Limitations:
Not exactly.

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper combines several existing techniques to achieve privacy-preserving CNN training. These techniques include transfer learning,  Quadratic Gradient,  mathematical transformation, and matrix-encoding method Volley Revolver. 

This writing is more of a technical document rather than a research paper with insights.

Soundness:
4

Presentation:
1

Contribution:
3

Strengths:
1)For the first time, they apply homomorphic encryption to neural network training.
2)They demonstrate the feasibility of homomorphic CNN training.
3)They propose pervacy-perserving friendly Squared Likelihood Error (SLE) for CNN training.
4)Experimentally, their algorithm has a state-of-the-art performance in convergence speed.

Weaknesses:
1)The introduction of related works is pretty simple, which makes it difficult to evaluate the contributes of the paper.
2)The quality of writing/presentation is very weak and unreadable.

Limitations:
See Questions and weakness.

Rating:
5

Confidence:
3

";0
XOotfgPiUF;"REVIEW 
Summary:
The authors propose to use paired synthetic data to train a semantic segmentation model. Specifically, a filtering strategy and a resampling strategy are proposed to control the quality of synthetic data. In this way, the paired synthetic data could further promote the standard segmentation model.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The overall idea makes sense, and the implementations are reasonable.
- The paper is well written and easy to follow.

Weaknesses:
- There may be conflict between filtering hard pixels and re-sampling hard masks, because the re-sampled hard samples may be filtered out. More in-depth analysis could be appended to solve this concern.
- The performance gains  (e.g., 48.5 → 50.6) seems not significant enough, considering that the proposed method introduce too many extra heavy processings. Specifically, 1) fine-tune the generative model on the specific dataset; 2) pre-train a segmentation model, i.e., Line 197; 3) finally train the desired segmentation model.
- The effectiveness of the proposed method depends on the gap between the generative model and specific dataset, and thus the in-depth analysis about the adaption (fine-tuning) of generative model is necessary. On the one hand, the generative model is hard to adapt when the specific dataset is small. On the other hand, the generative model is harder to provide rich cases when the specific dataset is large. Some in-depth analyses could be appended.


Limitations:
See *Weaknesses.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper introduces an automatic dataset generation with mask-to-image translator. The proposed dataset generator enables to generate controllable and consistent semantic labels in generated images. The labels can be treated as fully supervised teachers from a generator and create pre-trained segmentation models under the (synthetic) supervised learning. Moreover, the dataset generation framework serves difficulty levels inside of the contents in an image. In the framework, the authors utilize FreestyleNet pre-trained with StableDiffusion in order to translate from mask to image (mask-to-image) for the pre-training pairs.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- This paper is clearly written and easy to understand. The presentation to describe the proposed method (e.g., Figures 2, 3, 4) is high quality and convincing in visualization. 

- The paper could serve as a good example for data-driven approach in semantic segmentation tasks.

- Experiments and their results cover multiple aspects of the performance of the synthetic pre-training. The paper will serve a good inspiration to others in synthetic pre-training and related topics. 

- The two different aspects, 'noise filtering (Section 3.2)' and 'image re-sampling (Section 3.3)' in synthetic pre-training are very reasonable approaches. Indeed, these two have been shown to be effective as shown in Tables 4 and 5. These two approaches are complementarily improve the segmentation performance in terms of mIoU.

Weaknesses:
The reviewer does not find a critical weakness in the paper. However, it could be shown a little weaknesses.

- The paper includes the descriptions as 'controllable' in l.38 and l.179, however, could the proposed approach make more controllable dense annotations? For example, a semantic mask can be manually edited in addition to the image dataset as is. Could the authors consider the kind of flexibly edited approach by humans? As an example of image editing and generation, the reviewer can raise GauGAN from [Park+, CVPR19]. The reviewer doesn't think any image editing should be acceptable, but it would be effective in terms of making the synthetic segmentation dataset more flexible.

[Park+, CVPR19] Taesung Park et al. ""Semantic Image Synthesis with Spatially-Adaptive Normalization"" in CVPR 2019.

- This is not a critical weakness, however, can alone with the synthetic pre-training surpass the real-image dataset? In fact, the single approach with synthetic images can reach comparative scores as shown in Tables 1 and 2. In some cases, the gap is quite close each other (e.g., 48.5 vs. 48.3 in SegFormer-B4 on ADE20k dataset). If it is possible on paper limitation, adding the discussion would make the paper more valuable.

Limitations:
There are no negative limitations and societal impacts. On the contrary, the paper alleviate the privacy issues by means of synthetic image (pre-)training. The direction can effectively accelerate the kind of ethical problems in the future.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The Paper talks about using synthetic images generated using generative models as the training set to achieve stronger semantic segmentation models. The efficacy of the model is evaluated on ADE2K and COCO dataset, using SegFormer model. Authors propose pretraining with synthetic and joint images to evaluate which one works the best and in which scenario. Uses sampling and filtering training mechanism to further help the results.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper demonstrates a clear structure, effectively explaining the fundamental modules and training procedure involved. It successfully addresses the task of scene understanding by leveraging annotations obtained from generative models, thereby enhancing the results of the semantic segmentation task.

2. The inclusion of a self-adaptive module within the training pipeline is a valuable contribution. This module effectively refines erroneous or spurious training examples, leading to improved model performance.

3. The paper incorporates a sampling strategy that focuses on hard-to-learn cases. By prioritizing these challenging instances, the overall performance of the model is enhanced, resulting in better final results.

Weaknesses:
Methods used to improve the performance.
1. Mining Hard Examples: This is not something novel and (different flavors of it) has been used in many previous work to help in training better examples in case semantic segmentation and object detection.

2. Remove extraneous and harmful samples: Again the filtering mechanism proposed here is not that novel as previous works in the domain of active learning have again used similar techniques to improve training.

3. Use off-the-self models both for generation and training the model, nothing novel in that regard.

Limitations:
None

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper proposes to generate densely annotated synthetic images with generative models to help supervise the learning of fully supervised semantic segmentation frameworks. To improve the effectiveness of synthetic images, the authors further design a robust filtering criterion to suppress noisy synthetic samples at the pixel and class levels and propose an effective metric to indicate the hardness of semantic masks where they sample more synthetic images for harder masks. Ablation studies validate the effectiveness of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The logic of the article is generally clear, and the method is easy to understand.
2. The ablation experiments of this paper indicate the effectiveness of the proposed method to a certain extent.

Weaknesses:
1. The proposed strategy termed re-sampling synthetic images based on mask-level hardness is somehow like “Online Hard Example Mining” (OHEM) which is widely used in computer vision area including semantic segmentation, and the adopted metric to measure the sample hardness is also the widely used average losses of all pixels in the input image. Where is the novelty of this part? I don't think generating more hard samples should be the contribution of this section since it is more likely to belong to the contribution of Section 3.1.
2. The analysis in Section 4.4 for Table 6 is not convincing in my eyes. First, after adopting “Filtering & Re-sampling”, N_max no longer denotes for the number of synthetic images used for training, thus the comparison in Table 6 is unreasonable. If I understand correctly, the number of used synthetic images should be n_p in Eq. (2), which may result in a wrong conclusion in Line 339-348. Second, where is the performance ceiling when setting N_max after adopting “Filtering & Re-sampling”? It looks like setting larger N_max (> 20) will yield better segmentation results. Finally, how to set p in Eq. (2). The reviewer does not find this detail in both paper and the supplement materials.
3. Why there are no quantitative comparisons between the proposed method and previous methods like DatasetGAN and BigDatasetGAN? The authors argue that “the main drawback of such methods is the involvement of expensive human efforts.” So, how about applying a simple pseudo labeling strategy on these generated images and comparing the results between image-pseudo labeling strategy and your proposed mask-to-image synthesis strategy? If the results are comparable, where are the advantages of using your proposed method? The reviewer believe it is important to compare your method and previous similar methods quantitatively to show the novelty of this paper.
4. It seems that the performance improvements in Table 3 for Mask2Former is limited. To my knowledge, just simply run Mask2Former for two times may also bring such improvements. Could the author give some analysis about the limited gains for Mask2Former?
5. The filtering strategy in Section3.2 is naïve and not reliable in my eyes. Specifically, could the authors provide any quantitative results to show that the proposed filter strategy could filter noisy synthetic region rather than some hard samples? 
6. Previous studies like “Focal Loss for Dense Object Detection” indicate that OHEM is unreasonable. Whether the authors compare your re-sampling strategy with some objective-function-based strategy like Focal Loss, weighted cross entropy loss to show the effectiveness of your method? 
7. Why higher mIoU must be a good point is Table 4? For example, if your method could generate some densely annotated synthetic images with other domains which is different from the training and testing image? Whether it would lead to the decrease of mIoU but make the segmentor adapt to broader application scenarios? The reviewer thinks the latter is more important.
8. Could the authors give the results of performing the methods in Section 3.2 and 3.3 on the real training images? I mean filter the real images with the proposed strategies and re-train the model to see the importance of the generated images.

Limitations:
Limitations have been discussed in the paper.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper proposes a method of synthesizing training images and corresponding semantic masks for training a semantic segmentation network. The off-the-shelf semantic image synthesis model, FreestyleNet, is used to generate images from existing semantic masks. Following the proposed re-sampling technique based on mask-level hardness, harder samples are more frequently generated. During training, to avoid the noisy pixel hampering the model training, pixel-level ignoring technique is used. The generated synthetic images are shown to be effective for model training when they are used together with the existing fully supervised labels.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is overall well-written and easy to understand.

- It is interesting that the synthesized images can improve the performance together with fully supervised dataset. This can be practically utilized for many researchers.

Weaknesses:
- The proposed method heavily depends on trained mask-to-image generative models. The authors showed that naive generation of synthetic images is not sufficient for training a segmentation network, but the filtering and re-sampling techniques are quite naive. Specifically, ignoring uncertain pixels during training is popularly used for label-efficient learning (e.g., weakly and semi-supervised semanic segmentation).

- Closely related references, copy-paste methods (e.g., [ref1]), are missing. They also augment training data by synthesizing images, but unlike the proposed method, they do not require any additional heavy models. The copy-paste method should be discussed and compared.

[ref1] Simple Copy-Paste is a Strong Data Augmentation Method for Instance Segmentation

- In Abstract, the authors mentioned that ""We surprisingly observe that, merely with synthetic images, we already achieve comparable performance with real ones"", but I think it is overstated. To synthesize these images, the trained FreestyleNet is required, but FreestyleNet is already trained with real image-mask pairs. In addition, all the technical design (filtering, re-sampling) and values of hyper-parameters are determined with fully supervised validation data. I recommend the authors to tone down the sentence in Abstract.

- I guess the global hardness in Line 229 do not consider the difficulty of segmenting small objects. Intuitively thinking, small objects of hard class should increase the global hardness, but they actually slightly contribute to the global hardness.

- The authors used only the limited number of synthesized images due to the disk issue. I recommend the two additional experiments: 1) the performance change by varying the number of synthesized images. With this trend, we can infer if more synthesized images can further improve the performance. 2) Increase the total number of synthesized images by saving low-resolution images or high-tolerence polygon of masks. 


Limitations:
No limitation is discussed.

Rating:
4

Confidence:
4

";1
R2rJq5OHdr;"REVIEW 
Summary:
This paper unifies DEQs and Neural ODEs via homotopy continuation. The authors propose a novel implicit model, HomoODE, which inherits the property of high accuracy from DEQs and the property of stability from Neural ODEs. Moreover, they develop an acceleration method by borrowing the idea from the DEQ-solver.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1 This paper reveals the inherent connection between two typical implicit models, i.e., DEQs and ODEs.  This connection is really insightful. This paper, in my opinion, contributes significantly to our knowledge of implicit models.

2 The proposed HomoODE combines the advantages of DEQs and ODEs. The authors leverage homotopy continuation, and thus HomoODEs do not suffer from the issue of unique equilibrium like DEQs. The proposed acceleration method is also nice. The authors provide a new design paradigm for implicit models.



Weaknesses:
1 This paper argures that ""DEQs and Neural ODEs are two sides of the same coin"", but compared with the analysis on  ""two sides"" is less than that on ""the same coin.""   

2 More experiments on large scale datasets, e.g., ImageNet, would be nice.

Limitations:
The authors have adequately addressed the limitations, and there is no potential negative societal impact of their work.

Rating:
8

Confidence:
4

REVIEW 
Summary:
The paper shows that the fixed point equation solved by a Deep Equilibrium Model (DEQ) can be written as a Neural ODE.This is achieved by first introducing the equation of the solution path for the Homotopy equation, which is referred to as the Fixed Point Homotopy equation, and then deriving the appropriate homotopy mapping for the fixed point equation solved by a DEQ. Hence, they introduce an ODE that solves for a fixed point of a system (using homotopy continuation) and therefore claim that DEQs and neural ODEs are “essentially two-sides of the same coin”.

The authors then introduced HomoODE, that uses the input injection from a DEQ and the derived fixed-point-ODE to solve for a task, however instead of solving for the fixed point using newton-method in a DEQ homoODE uses a normal ode-solver similar to neural ODEs. The input injection helps in getting better performance than neural ODE and the implicit solving using ODE solver results in a more memory efficient methodology

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well written, and effectively establishes the connection between neural ODEs and DEQs. Furthermore, they introduce HomoODE, which inherits the better performance capabilities of a DEQ, and has a lesser memory footprint (thanks to the ODEsolver instead of newton method based solver, used in Neural ODEs). 

The experiments show on various classification tasks that HomoODE matches or beats DEQs in various benchmarks with fewer model parameters and with faster inference time.

Weaknesses:
There are some details missing in some parts of the paper, I have pointed them out in the Questions section.

Limitations:
Authors have discussed the limitations of the work.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper uses homotopy continuation to show that Deep Equilibrium Models and Neural ODEs are effectively equivalent. They use the best properties of both worlds: DEQs for their higher accuracy and Neural ODEs for their stability to propose a new implicit method HomoODE.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* Theoretical explorations of the relationship between Neural ODEs and DEQs are limited, and the paper does a good job of linking the two ideas.

Weaknesses:
* Reformulating DEQs as Neural ODEs have been done in a prior work [1], which has not been mentioned or compared against
* MNIST and SVHN are too small-scale experiments and can lead to incorrect extrapolations on the method's benefits. I would recommend including larger problems like ImageNet.
* Some information on the baseline would be useful: Typically, solving discrete dynamical systems (DEQs) is faster than a continuous dynamical system. So the 8x inference time might be an artifact of poor solver choice for the DEQ.

[1] Pal, Avik, Alan Edelman, and Christopher Rackauckas. ""Continuous Deep Equilibrium Models: Training Neural ODEs faster by integrating them to Infinity."" arXiv preprint arXiv:2201.12240 (2022).

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
The work shows the connection between Deep Equilibrium Moldes (DEQs) and Neural Ordinary Differential Equations (NODEs) using homotopy continuation, a method to solve nonlinear equations. Based on the analysis, the authors suggest a new implicit model, HomoODE, which has high expressible power from DEQs and stability from NODEs. The method is demonstrated to obtain the best performance for image classification tasks between the models tested.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* The connection between DEQ and NODE is presented clearly and seems mathematically solid, which delivers a lot of insights to the community (in particular, in the field of implicit models).
* In addition to mathematical notions, the paper covers practical ideas, e.g., the learnable initial condition. The effect of the adjoint method is also investigated.

Weaknesses:
* The reason why the authors choose image classification is not very clear. With the current manuscript, the practical contribution of the paper looks quite limited because DEQ is designed for sequential data, and NODE is for ODEs. Thus, the reviewer cannot assess the effectiveness of HomoODE because image classification is not a main field of the baseline methods. The reviewer strongly recommends the authors run experiments on ODEs to show real improvement compared to DEQ and NODE. 
* Related to the first point, the reason why the HomoODE works better in the image classification task is not clear. The reviewer did not find out what the authors would like to state with these experiments. I believe there are a lot of excellent models to compare in the field of image classification if the authors would like to claim the model's effectiveness in the image classification tasks. It could be understandable if the image classification task is to show the adaptability of the model, and there is another main experiment (e.g., on an ODE dataset).

Limitations:
* The method to optimize the shared initial information seems tricky and could be stated as a part of the limitation.

Rating:
5

Confidence:
3

";1
mWMJN0vbDF;"REVIEW 
Summary:
The paper proposes a novel framework for sign language translation (SLT), which can integrate multiple SLT subtasks. The work is motivated by a series of experimental analysis, e.g., converging speed of different subtasks and relationship between SLT and sign language recognition (SLR) performance. Besides, two constraints are proposed to improve the faithfulness of the model and ease the model training. The method achieves SOTA performance on two widely adopted SLT benchmarks using only keypoint inputs.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper figures out two important problems: the lack of faithfulness in current SLT models and the inconsistent trend between SLR and SLT, which can inspire future works in this field.
2. The method is well motivated by a series of in-depth analysis in Figure 3.
3. The code-switching operation is interesting and novel, which can inspire future works on cross-modality modeling for SLT.
4. SOTA performance are achieved with a lightweight model using only keypoint inputs.


Weaknesses:
My major concerns come from method details and experiments.

Method:

1. The MLP and classifier in Figure 2 should be shared or not? In VAC [9], two different classifiers are appended to the visual and contextual module, while SMKD [40] uses a shared classifier, and the paper follows the design of SMKD. Intuitively, different classifiers should be used to project two features from different spaces into a common space. More discussion is needed for the discrepancy.
2. Gloss embeddings and mixup also appeared in a recent paper in the field of SLR [R1]. In [R1], the gloss embeddings are extracted by FastText, and the mixup is also achieved between visual and gloss embeddings. Some discussion or comparison should be added.
3. What is the motivation to fulfill code switch between the visual and gloss embeddings? Is it possible to use it between the contextual and gloss embeddings?

Experiments:

4. In Table 5, the sentence-wise code-switching does not consistently outperform the token-wise counterpart. The authors may explain why the sentence-wise one performs better when not using annealing and consistency.
5. As stated in line 299, logits are a closer representation of glosses. Also, [12] uses logits as the input for the translation module for CSL-Daily. Thus, it is not rigorous to conclude that adopting logits will degrade the performance since the ablation study is conducted on Phoenix14T.
6. The paper focuses on improving the faithfulness of SLT. But there are not objective metrics mentioned to measure the faithfulness. 

[R1] Natural Language-Assisted Sign Language Recognition, CVPR 2023.

Limitations:
The authors have discussed the limitations and societal impact adequately.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper mainly discusses the challenges of improving faithfulness in sign language translation and proposes solutions. The researchers leverage rich monolingual data and adopt back-translation to generate synthetic parallel data, explore the potential of denoising auto-encoder, and propose the MonoSLT framework to improve the accuracy of sign language translation. They also emphasize the importance of alignment and consistency constraints to align visual and linguistic embeddings and improve faithfulness. This paper has important reference value for improving faithfulness in sign language translation.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Proposing a new unified framework, MonoSLT, which integrates subtasks of sign language translation into a single framework, allowing these subtasks to share acquired knowledge.

2. Proposing two constraints: alignment constraint and consistency constraint, which help improve the faithfulness of translation.

3. Experimental results show that the MonoSLT framework is competitive in improving the faithfulness of sign language translation and can increase the utilization of visual signals, especially when sign language vocabulary is imprecise.

Weaknesses:
The paper does not explicitly address the handling of non-manual signals and sign language morphological changes, which are crucial factors influencing the faithfulness of sign language translation. 

The experimental settings in the paper do not provide detailed explanations for many hyperparameters.

Limitations:
The proposed method lacks proper metrics to quantitatively evaluate the faithfulness of Sign Language Translation models and continues to use BLEU and ROUGE for evaluation. 

The paper's analysis and experimental results regarding faithfulness are not clearly defined.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper discusses the issue of faithfulness in sign language translation (SLT), which refers to whether the SLT model captures the correct visual signals. It is found that imprecise glosses and limited corpora can hinder faithfulness in SLT. To address this, the paper proposes MonoSLT, which integrates SLT subtasks into a single framework that can share knowledge among subtasks. Two kinds of constraints are proposed to improve faithfulness: the alignment constraint and the consistency constraint. Experimental results show that MonoSLT is competitive against previous SLT methods and can increase the utilization of visual signals, especially when glosses are imprecise.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The method proposed in this paper outperforms multiple baseline methods, which is a promising contribution to sign language translation. 

Weaknesses:
1. There is no comparison with [12,15] on the bug-free dataset, which is a concern. Although I understand that reproducing [15] would require additional effort, since your code is based on MMTLB, it would be reasonable to verify the effectiveness of MMTLAB on the bug-free data.

2. The analysis of faithfulness and hallucination in the paper is not in-depth enough. There is no metric (either manual or automatic) to quantify faithfulness and hallucinations, and the improvement in BLEU is not sufficient to indicate that the faithfulness issue has been effectively addressed. The few cases presented in the paper are not enough to support the conclusions.

3. The analysis in section 3.2 is not thorough enough, and the conclusions are somewhat forced. For example, the statement that overfitting is caused by faithfulness is not well-supported, and the conclusion that there is no obvious negative correlation between SLT and SLR in Figure 3(c) is due to hallucination lacks data support and quantitative analysis. The few examples presented in section 4 are not sufficient to demonstrate the issue of hallucination.

Limitations:
see weakness

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work is dedicated to enabling the SLT model to capture correct visual signals (faithfulness in SLT). In order to improve faithfulness in SLT, the author integrates SLT subtasks into a single framework named MonoSLT, and based on this, proposes alignment constraints and consistency constraints. The former is used for aligning the visual and linguistic embeddings. The latter is used for integrating the advantages of subtasks. To demonstrate the effectiveness of the proposed method, the authors conduct experiments on two public datasets.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
[1 - complete layout and detailed description]. The article has a relatively complete overall layout and a detailed work description.

[2 – method novelty]. The author Introduced the code-switching phenomenon in the Alignment Constraint, mimicking the phenomenon of language alternation in conversations between multilinguals, and mixed visual embedding and gloss embedding as an input to the Translation Module.

[3 – the rationality of alignment]. Implicitly align visual and linguistic embeddings through shared translation modules and synthetic code-switching corpora. Better utilization of the characteristics of different subtasks.

[4 – method performance]. On the Phoenix14T dataset, the author's method only uses skeleton sequences as input, which improves performance (+2.2 BLUE-4) compared to the best method using RGB video as input.


Weaknesses:
[1 – Writing quality]. In section 3.2, some analysis is confusing, and the conclusion seems to be the author's subjective thoughts. And in the title of table 2, ‘the inconsistent punctation bug’ is confusing.

[2 - method performance on CLS-Daily]. On the CLS-Daily dataset, MonoSLT performs poorly, lagging behind several sota models.

[3 - Model evaluation issues]. The paper also mentions that although it alleviates the problem of faithfulness in SLT, there are no suitable metrics to measure it. The author still uses BLEU and ROUGE for evaluation


Limitations:
1.Find or create proper metrics to quantitatively evaluate the faithfulness of SLT models.

2.You said that the CLS-Daily dataset provides more precise gloss annotations, which leads to other models with lower SLR performance being able to achieve better SLT results This also leads to your model not performing as well as some models on the CLS-Daily dataset. As the sign language dataset becomes larger and more accurate, your model may not be as good as other models. I think this is worth considering.


Rating:
5

Confidence:
5

REVIEW 
Summary:
This work mainly studies the faithfulness issue in SLT (i.e., whether the SLT model captures correct visual signals). The study identifies imprecise glosses and limited corpora as the main factors contributing to limited faithfulness. In order to mitigate this issue, this work proposes a framework called MonoSLT, which leverages the shared monotonically aligned nature among SLT subtasks. This framework incorporates alignment and consistency constraints. Experiments demonstrates the effectiveness of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This paper is well-written and well-organized.

This work performs in-depth analysis on the previous works and the association among SLT-relevant tasks.

The overall performance is promising and shows notable performance gain over the baseline.

Weaknesses:
The main focus of this work is on the concept of faithfulness in spoken language translation (SLT). However, a notable limitation of the study is the absence of quantitative metrics to evaluate faithfulness. While the authors acknowledge this limitation in the paper's discussion of limitations, it remains a drawback. It would be beneficial for the authors to provide further clarification on this issue, perhaps by suggesting potential quantitative metrics that could be used to assess faithfulness in future research.

It is suggested that the proposed framework be compared with VAC, as they share similar components such as consistency loss and visual module constraints.

Regarding the discrepancy in length between the embeddings produced by the visual GCN module and the gloss module, it is essential to understand how the code-switching module handles this challenge. The authors should provide clarification or explanation on how the code-switching module addresses this issue.

Limitations:
It is better to design a suitable metric to evaluate faithfulness in SLT.

Rating:
5

Confidence:
5

";0
QwQ5HhhSNo;"REVIEW 
Summary:
This paper proposes k-DisGNNs, that can use distances alone to represent higher-order interactions, instead of the direct use of angles, torsional angles, and dihedral angles. The authors try to build a bridge between geometric deep learning and the traditional expressiveness of graph representation learning without geometric information. They conduct experiments mainly on MD17 and show results on QM9 in the appendix. For MD17, 2F-Dis. and 3E-Dis. are comparable and SOTA, but for QM9 (still small geometric graphs), 2F-Dis. is worse than several previous works and 3E-Dis. can not be implemented due to high complexity, which means their method with higher k cannot extend to larger geometric graphs with more nodes.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The first work to use distances alone to represent higher order geometric information including angles, torsional angles, and dihedral angles.
2. They establish a connection between geometric deep learning (GDL) and traditional graph representation learning (GRL).
3. Good performances of k=2 model on MD17.
4. Figures are good and informative. 

Weaknesses:
1. Prohibitive Computational Costs for k > 2: The computational demands associated with the proposed method escalate dramatically when k > 2, as demonstrated in the appendix. Specifically, when k is set to 3, the model becomes computationally infeasible for the QM9 dataset, which comprises small molecules.

2. Lack of Scalability and Performance for k = 2: When applied to the modestly-sized MD17 dataset, the method exhibits satisfactory performance with k = 2. However, this performance deteriorates markedly when transitioning to the more extensive QM9 dataset. The method's performance, as evaluated with 2F-Dis, fails to measure up to that of prior approaches such as TorchMD, GNN-LF, and PaiNN. 

3. The writing about the message passing mechanism lacks clarity and is hard to follow.

Major Concern: The major limitation of the proposed method is its inability to maintain the good performances when scaling to larger datasets, particularly QM9 concerning small molecules with not so many atoms. This significantly constrains the method's applicability and undermines its contribution.

Limitations:
They mentioned the limitations in the main paper. More potential limitations are listed above. 

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper proposes $k$-DistGNNs, a sequence of k-(F)WL-like models that operate on the distance matrix of a geometric graphs. These models are able to compute higher-order geometric invariants, generalise existent models, and for certain values of $k$ are shown to be able to approximate any geometrically invariant or equivariant functions. The model is evaluated against other SOTA models on standard molecular benchmarks. Additionally, the paper proposes a procedure to generate new counter-examples of geometric graphs that cannot be distinguished by MPNNs operating on the distance matrix. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
-	The paper is very well written and easy to follow. 
-	The expressive power of Geometric GNNs is a relatively new and unstudied topic, and the paper makes an important contribution in this regard. 
-	The proposed approach is a simple, but powerful, extension of k-WL to distance matrices and implicitly geometric graphs.
-	The evaluation shows that the proposed models perform very well on standard benchmarks against the latest geometric models (outperforming them in some tasks)
-	The universal approximation properties of the model are very attractive and back up the claim that the distance matrix is “enough”. Although, the question in the title is somewhat rhetorical since the distance matrix fully determines geometric graphs. So just from that, it follows the information contained inside is enough. 
-	The new counterexamples that the paper provides are very welcome as well as the general procedure used to generate them.


Weaknesses:
- Note that WL generalisations for Geometric GNNs were also studied in _On the Expressive Power of Geometric Graph Neural Networks_ (ICML 2023), which is not mentioned in the text as far as I could see. 

Limitations:
Yes.

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this paper, the authors argue that the distance matrix associated with a geometric graph is sufficient to build powerful geometric neural networks.
In particular, the paper proposes a new family of graph architectures, named k-DisGNN, operating on a complete graph with node distances on the edges.
These architectures, inspired by the k-WL test (and its variations like the k-FWL or k-EWL tests), are able to model higher-order geometric information, which makes them more expressive than vanilla message-passing networks and ensures their universal approximator property.


Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
The paper is well written and well motivated.
The proposed ideas lead to a relatively simple yet expressive design to process geometric graphs.
The experiments support the theoretical results in the main paper.


In addition, the supplementary materials include a number of examples where message-passing networks fail to distinguish geometric graphs.
These interpretable examples can be useful to understand the limitations of existing and future architectures.


Weaknesses:

Some details about the proposed models were not completely clear to me, although this might be due to my little familiarity with Weisfeiler-Leman algorithms.
Still, I think the presentation could be made a bit clearer, see Questions section.

I don't see major flaws in this paper.


Limitations:
The authors explicitly addressed the limitations of the work in the main paper.


Rating:
7

Confidence:
3

REVIEW 
Summary:
This work proposes neural networks for operating on distance matrices obtained from 3D/Euclidean point clouds. New architectures which generalise the k-WL/k-tuple higher order GNN framework (Morris et al.) to distance matrices are proposed, along with proofs of their universality/completeness for k=2 (scalar) and k=3 (vector predictions). The approach is evaluated to work as well or better than current geometric GNNs for small molecular tasks like MD17 and QM9.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- This work presents a **novel** k-WL framework for complete distance graphs from Euclidean point clouds. New architectures for processing k-tuple distance graphs are proposed, in a spirit similar to influential work by Morris et al. (https://arxiv.org/abs/1810.02244) on higher order GNNs for standard graphs.

- I believe the theoretical results showing that architectures based on 2-WL and 3-WL variants of the proposed test are complete for scalar/vector predictions on distance graphs is **significant**.

- The proposed architectures are shown to **work as well or better** than existing geometric GNNs for small molecular simulation tasks (QM9, MD17, rMD17).

- The procedures to construct families of counterexamples for 1-EWL and vanilla disGNNs are described in detail and seem interesting.

Weaknesses:
- I don't have major concerns.

- Ambiguous claim re. Geometric Deep Learning: 
    - Geometric Deep Learning is a pretty broad field beyond point clouds embedded in Euclidean space. It includes Groups, Gauges, Meshes, Grids, and Graphs. I personally felt the title and claims within the paper could be perceived ambiguously because the findings of this paper are not directly applicable across **the entire** spectrum covered by GDL; it only covers geometric *graph* learning.
    - Is this work relevant to other areas of GDL? If so, how?

- Scalability: One major downside of the proposed approach of adapting higher order tuple-based GNNs to operate on distance graphs will be the computational cost of working with anything other than small molecules.
    - On MD17/rMD17, the models use a batch size of 2. I suppose this is due to high memory usage.
    - Regarding the experiments on timings in the appendix: this seems like an unfair comparison b/c one would naturally expect using dense tensor-based implementations like yours to be much faster than PyG implementations of DimeNet/GemNet (especially for small molecule graphs with tens of nodes). 
    - **Is it possible to compare apples-to-apples the timing/memory usage of dense implementations of DimeNet/GemNet, too?**
    - I see this paper's main contribution to be theoretical, so I don't think this is a major concern.

- I had difficulty following Proof B.3.1. Perhaps proving a proof sketch or some warmup text outlining the proof techniques may be helpful, especially as it spans several pages. 
    - (It may be that I need more time to study the proof in further detail.)

Limitations:
The authors have adequately addressed technical limitations but have not stated potential negative social impact. 

Beyond what the authors mention regarding scalability of their models, one major theoretical limitation of the proposed framework is that it is restricted to full geometric graphs, and the construction of complete/universal models for the general sparse graph setting remains an open question. This my be worth reiterating.

Rating:
7

Confidence:
4

";1
vpMBqdt9Hl;"REVIEW 
Summary:
This paper presents an interesting CO agent model that conditions policy by latent vectors and finds latent vectors through CMA-ES.
In addition, considering latent vector in the learning process, this paper presents a training method that induces the agent to be specialized for various instances.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
1. The idea of ​​using latent vectors to condition the policy and updating latent vectors via CMA-ES during inference is novel.   

2. The training method with this inference method in mind is also interesting.

Weaknesses:
1. Inference time is not included in Table1. Since COMPASS is a method of continuously finding a better solution during inference, it is crucial to include the inference time in the experimental results.   

2. It is difficult to view the experimental results of COMPASS as state-of-the-art results considering EAS (Hottung et al., 2022), SGBS+EAS (Choo et al., 2022) and DPDP [1] in CVRP experiments.

3. Overall, the description of the model structure, training method, and inference method lacks details.  


***

[1] Kool, Wouter, et al. ""Deep policy dynamic programming for vehicle routing problems."" Integration of Constraint Programming, Artificial Intelligence, and Operations Research, 2022.

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
Building upon a pre-trained neural constructive model (such as POMO), this paper proposes COMPASS, which introduces the idea of learning a continuous latent search space to fine-tune the pre-trained POMO model parameter. The latent space allows for the sampling of a vector, which the pre-trained POMO model uses as a conditional vector to generate its own parameters. After training such latent space, continuous optimization algorithms (such as CAM-ES) can be utilized to search this space, in order to yield the most performant POMO model parameters for each test instance during inference. This allows for per-instance search during inference, while avoiding the need to retrain the deep model for each new test data (as is the case with active search in EAS). Experiments on benchmarks verify that COMPASS outperforms the state-of-the-art baselines.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The concept of learning a latent search space for fine-tuning parameters of an NCO model is novel and could potentially impact the NCO community positively. However, it is important to acknowledge that the idea of learning a latent search space followed by using continuous optimizer to search within the space is not new, as evidenced by the CVAE-Opt method (ICLR’21).
- The authors conducted comprehensive experiments, supported by detailed tables, figures, and useful visualizations.
- COMPASS achieves state-of-the-art performance on benchmark TSP-100 and CVRP-100 instances.


Weaknesses:
- Although the authors provided reasons, I still think it would be useful to benchmark COMPASS against POMO and EAS equipped with data augmentation, to gain a complete understanding of COMPASS's advantages. That is to say, all baseline should be at their best settings. Furthermore, the recent method SGBS (simulation guided beam search, NeurIPS’22) is also overlooked.
- The method by which POMO is conditioned on the vector sampled from the latent space is unclear. Specifically, given a pre-trained NCO model, how should the user determine which parameters to condition on in practice?
- The literature review lacks comprehensiveness, missing important works like CVAE-Opt, SGBS, and other recent works.
- COMPASS's generalization to larger sizes (CVRP-200) appears less efficient than EAS.

Limitations:
The revised paper should mention more limitations and future works.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper proposes COMPASS, an RL-based training framework to learn a diversified neural solver for combinatorial optimization problems. This framework trains a conditioned neural network conditioned on a prior vector sampled from a fixed distribution. During the training phase, multiple priors are sampled, and only the parameters corresponding to the best prior are updated. During the inference phase, the evolution algorithm is employed to find the best prior for the current instance.

The authors test its method on TSP, CVRP, and JSSP. The experiment design is solid, and the results are promising. 



Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. The resources, e.g., memory and time, used in the inference time are shorter than the current SOTA from its low-dimensional prior.

2. The learned conditional neural solver has the potential to generalize to the out-of-training distribution instances. That is also somehow verified from the section 4.2 experiments.



Weaknesses:
1. One key challenge in the RL for CO area is how to train/generalize a model to the real big cases, e.g., TSP1000/10000. This framework is designed to improve upon another neural solver. However, it cannot solve the real big cases. 

2. In the inference time, the priors are sampled and selected using the evolution algorithm. This is somehow like a search-related method. It is not clearly verified ""the improvement comes from a good conditional neural solver or the strong evolution algorithm"". 

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes a neural combinatorial optimization approach that allows for an extensive search for high-quality solutions. The approach uses reinforcement learning to train a network to construct solutions for the traveling salesman problem (TSP), capacitated vehicle routing problem (CVRP), and job shop scheduling problem (JSSP). During training, the network is trained to parameterize a diverse set of policies that are conditioned on a continuous latent space. At test time, a guided search is performed using Covariance Matrix Adaptation (CMA-ES) to find regions in the continuous latent space that are associated with policies leading to high-quality solutions. The experiments indicate that the proposed method provides a competitive performance and is able to generalize well to instances that are different from those seen during training.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The results show that the method offers a very good performance on all considered problems.
- The paper addresses an important problem (designing a neural combinatorial optimization approach that is able to perform an extensive search).
- The authors perform some interesting generalization experiments that go beyond changing only the instance size and also considers other shifts in the distribution. 
- The authors will make the code of their method publicly available.
- Overall, the paper is well written and clearly organized.

Weaknesses:
- The proposed method is very similar to the method from [Hottung et al.] which also trains a neural network conditioned on a continuous latent space to construct diverse solutions to routing problems. At test time, both methods search the continuous latent space using continuous optimization methods (in this work CMA-ES, in Hottung et al. differential evolution.) The authors should make this clear in the paper and discuss the differences between the methods.
- In general, the paper omits discussing most of the existing works on learning extensive search methods (or improvement methods) for combinatorial optimization problems. Instead the authors conclude that “[..] the field has reached a point where methods [...] can hardly make significant improvements [over quickly generated solutions when] given a budget for additional computation.” (page 2) In fact, many approaches have been proposed that aim to exploit bigger computation budgets and that perform an extensive, guided search:
   - Xinyun Chen and Yuandong Tian. Learning to perform local rewriting for combinatorial optimization. Advances in Neural Information Processing Systems 32, 2019.
   - André Hottung and Kevin Tierney. Neural large neighborhood search for the capacitated vehicle routing problem. European Conference on Artificial Intelligence, pages 443–450, 2020.
   - Kim, Minsu, and Jinkyoo Park. ""Learning collaborative policies to solve NP-hard routing problems."" Advances in Neural Information Processing Systems 34 (2021): 10418-10430.
   - Ma, Yining, et al. ""Learning to iteratively solve routing problems with dual-aspect collaborative transformer."" Advances in Neural Information Processing Systems 34 (2021): 11096-11107.

- The reported performance of the baseline approaches is significantly worse than in their original works.
   - The validity of the experimental results is limited by the fact that the authors remove a core component from the considered baseline approaches. More precisely, the authors do not use instance augmentation (which considers 8 different augmentations of each test instance during the search). Using augmentations of an instance is an established way of increasing the exploration during the search, because neural network based construction methods tend to generate different solutions for each augmented version of an instance). While it can be argued that an augmentation mechanism is a “domain-specific trick” it also can be considered unfair to remove a component from a baseline approach that encourages exploration without replacing it with a different component that increases exploration. Most methods will perform worse if  a component that the developers considered as given when designing the method (even if it can be easily replaced by something else) is removed. Hence, I suggest that the authors also report results for the baselines with augmentation.
   - The authors reimplement the baseline approaches even though their code is publicly available. This has some pros and cons. On the plus side, implementing all approaches within the same code base allows a fair comparison of the runtime unaffected by implementation tricks or differences in the speed of the used framework. However, (to my surprise) the authors do not report any runtimes in the main paper. On the negative side, there is a risk that the implementation does not work as well as the original approach due small mistakes/misunderstandings. Currently, it is not possible to evaluate if the implementation of the authors matches the performance of the original code base because the authors do not report results with the augmentation mechanism being enabled. Hence, I again suggest that the authors also report results for the baselines with augmentation to demonstrate that their reimplementation is correct (in that case having a unified code base for all approaches could even be considered a strength of the paper).
- The authors should make it more clear if they used identical test instances to earlier work or if they generated new test instances (I hope the former, because generating new test instances makes a comparison to earlier works unnecessarily difficult).

[Hottung et al.] Hottung, André, Bhanu Bhandari, and Kevin Tierney. ""Learning a latent search space for routing problems using variational autoencoders."" International Conference on Learning Representations. 2020.

Limitations:
- The paper does not discuss limitations of the proposed method.

Rating:
6

Confidence:
4

";1
xz8j3r3oUA;"REVIEW 
Summary:
This paper questions the importance of color variations for neural classifiers and proposes to use color-equivariant architectures in the case of unbalanced datasets.
To demonstrate the validity of the presented approach, the authors conduct experiments both on synthetic controlled datasets and on common object recognition benchmarks.
As the experiments show, the injection of color-equivariant layers leads to a slight improvement on almost all common benchmarks when the performance is measured on the original test set but the advantage of the presented method becomes more evident when the test data is corrupted with hue shifts.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This paper studies an interesting and underinvestigated question of the importance of color representation for neural networks.
The submission is easy to read, and the motivation is well explained in the example of the Flowers dataset. 
The authors have conducted a significant number of experiments to support their claims.
Additional strength is that the demonstrated performance improvement is achieved without increasing the number of trainable parameters (line 238).

Weaknesses:
1. While the idea of extending equivariance from geometric to photometric transformations is definitely interesting, the submitted manuscript, unfortunately, focuses on the only type of such transformations, i.e. hue shifts. Despite the case of the Flowers dataset is a perfect fit for this transformation, the authors do not discuss other use cases when this type of equivariance may be interesting in practice and just mention ""accidental recording conditions"" (line 3). For other datasets, hue shifts seem less meaningful, and the better robustness of the proposed CE-ResNets to such shifts at test time is explained by the fact the architecture was just intentionally designed for this scenario. Taking this into account, I find the scope of the paper a bit limited.

1. In addition to being limited in the number of considered photometric transformations, the paper also considers a single task of object recognition. I would encourage the authors to consider other tasks as well, e.g. unsupervised domain adaptation.

1. While the authors claim their approach makes networks more robust to test time corruptions (Tab. 1), they do not demonstrate other baselines aiming to provide robust outputs, e.g. adversarially robust models.

Limitations:
Limitations are addressed adequately.

Rating:
5

Confidence:
2

REVIEW 
Summary:
The authors introduce a color equivariant convolutional neural network. To achieve this the authors represent the image in HSV format, and achieve hue equivariance using methods for rotational equivariance. This is possible since hue can be represented by an angle. The authors show that the proposed approach out performs standard CNNs and color invariant CNNs when there is a hue shift between the train and test set.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* Originality: The presented method of building a color equivariant CNN appears to be original. 
* Quality: The work appears to be of fairly good quality. 
* Clarity: The paper is well written. 
* Significance: The observation that color equivariance can be achieved by identifying hue with the rotation group is interesting. The results show the proposed approach leads to improved performance when there is a color based domain shift.

Weaknesses:
* Quality: I have some questions about the mathematical presentation, and experiment design (see questions).
* Clarity: Some aspects were unclear to me, due to presentation or motivation (see questions)

Limitations:
* Limitations -- in particular the issue of computational cost -- are communicated

Rating:
6

Confidence:
5

REVIEW 
Summary:
Paper proposes color-equivariant CNN layers by imposing equivariance to H_n (a discrete subgroup of SO(3)) in the RGB space which is imposes hue equivariance. Implementation follows the framework of Group-equivariant CNNs. Experiments show marginal improvements over standard CNNs for in-distribution test data but significant improvements when test data is hue-shifted.

Soundness:
2

Presentation:
4

Contribution:
3

Strengths:
1. Color equivariance in CNNs is a relatively less-studied but an important topic for robustness. The proposed idea of incorporating equivariance to hue transformations via rotations in the RGB space is novel. 
2. Experiments are setup well clearly showing when color equivariance is helpful vs color invariance vs no symmetry. Proposed approach shows improves over CNN even when in in-distribution test data, but major improvements come when test data is hue-shifted.

Weaknesses:
1. Definition of color equivariance considered in the paper seems to be restricted as it only considers the hue dimension. One of the motivations for incorporating color equivariance is for robustness to illumination changes which I do not think is guaranteed here. A general definition of color-equivariance should consider other dimensions. Maybe the claims are better justified if Hue-equivariance is emphasized in the title/introduction/method name, etc.
2. The definition of hue-equivariance is not precise in the paper. Ideally, it should include all rotations in the RGB space (i.e., SO(3)), but also consider the fact that many of these rotations take the color values out of the RGB space (unit cube). In general, this issue occurs for the discrete subgroup $H_n$ as well. Simply projecting the color values back into the RGB space does not work as it breaks the invertibility property of these transformations. 
3. Experiments compare with a standard CNN (+grayscale) as baseline. Other baselines can be included, for example [1], that considers invariance to illumination color/intensity. 
4. Experiments in the main paper only consider the group $H_3$ (i.e., 3 rotations in the RGB space), which seems limited in robustness, as shown in Figure 1 without jitter augmentations. 


References:

[1] Lengyel, Attila, et al. ""Zero-shot day-night domain adaptation with a physics prior."" _Proceedings of the IEEE/CVF International Conference on Computer Vision_. 2021.

Limitations:
Limitations should discuss lack of robustness to other color dimensions (e.g., illumination).

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes color equivariant convolutional networks (CE-CNNs), a novel convolutional neural network architecture that achieves equivariance to hue changes.&#x20;

They introduce color equivariant convolutions that apply a discrete set of hue rotations to the convolution filters during the forward pass. This makes the network output invariant to corresponding hue changes in the input image.
They propose a group coset pooling layer that pools feature maps over the group of hue transformations to achieve invariance.
They evaluate CE-CNNs on several image classification datasets, showing improved robustness to hue changes at test time compared to regular CNNs. The method also improves absolute classification performance in some cases.
Overall, the paper presents a novel and intuitive technique to build invariance to hue changes into CNNs. The evaluations demonstrate its advantages over standard networks, especially under shifted hue distributions between training and test.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
This paper introduces a clever yet intuitive technique to make convolutional neural networks invariant to hue changes in the input image. The core idea is to apply discrete hue rotations to the convolution filters during the forward pass, essentially ""baking in"" robustness to color changes.

The paper is clearly written and easy to follow. The authors motivate the problem well, explain their proposed method succinctly, and provide thorough experimentation across image datasets. The visualizations offer useful insights, confirming that the networks learn consistent features across hues.

Overall, I found this to be an original and significant contribution. Invariance to hue shifts is a practical problem, and this paper tackles it through an elegant approach that outperforms regular CNNs. The concept of encoding transformations into convolutions seems powerful. While not the flashiest technique, the method is thoughtful, principled, and achieves strong results. The paper is presented clearly and comprehensively, making the ideas accessible. In summary, this is a high quality paper with both theoretical and practical value.

Weaknesses:
-   The method is demonstrated on image classification, but it's unclear how well it would generalize to other tasks like detection or segmentation. Additional experiments on other applications could strengthen the claims.
-   The ablation study on number of hue rotations suggests performance varies across different shifts. It would be useful to dig deeper into why - is it an artifact of how shifts are applied? Better understanding this could improve results further.
-   The approach encodes discrete hue rotations. An interesting extension could be supporting continuous rotations for finer-grained equivariance.
-   The comparisons to ""grayscale"" networks should be interpreted carefully, as removing color information entirely handicaps models. Comparisons to networks pre-trained onImagenet may be more meaningful.
-   The Flowers-102 experiments indicate the method doesn't help much on datasets without color bias. Analyzing when color equivariance helps or hurts could guide adoption.

Limitations:
1.  The comparison to only a ResNet baseline limits the conclusions on the benefits of the proposed method. Comparisons to other approaches could provide useful context.
2.  The long-term impacts of building color equivariance into models is unclear. Discussion of downstream effects on fairness and interpretability could be beneficial.

Rating:
5

Confidence:
3

";1
m9uHv1Pxq7;"REVIEW 
Summary:
This paper contributes to the field of unsupervised face animation by introducing a novel motion refinement method to overcome the limitations of existing prior-based motion models, especially in estimating detailed facial movements.

The paper's approach introduces a new method which uses a structure correlation volume built from keypoint features. This provides motion information that does not rely on prior data. This information is used to iteratively refine the coarse motion flow estimated by a previous motion model.

The authors have conducted numerous experiments on challenging benchmarks to test the effectiveness of their approach. The results indicate that this new method enhances the capability of prior-based motion representation by learning how to refine motion. This suggests that their approach can effectively increase the accuracy of unsupervised face animation tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
In this research, the authors present a new unsupervised face animation approach that concurrently learns both coarse (global) and finer (local) facial motions. Their method integrates a local affine motion model to learn the global, coarse facial motion and a novel motion refinement module to compensate for the local affine motion model's ability to model more detailed facial motions in local areas.

The motion refinement process is based on the dense correlation between the source and driving images. To achieve this, a structure correlation volume is first constructed using the keypoint features of the source and driving images. The authors then train a model to iteratively generate minor facial motions from low to high resolution.

The learned motion refinements are combined with the coarse motion to generate the new image. After performing extensive experiments on widely used benchmarks, the method was found to deliver the best results among existing state-of-the-art methods. The authors have also committed to making the source code for their method publicly available in the future.

Weaknesses:
1. The goal of this article is to learn fine facial motion, but using the VoxCeleb dataset may not be sufficient for this purpose. In my view, finer facial motion refers to the ability to reproduce wrinkles, eyeballs, and micro-expressions at high resolution, which may require higher resolution datasets.

2. As noted by the author, the reliability of keypoints estimation heavily influences the quality of finer facial motion. It is not clear whether the improvement of existing models comes from more accurate keypoints estimation or the proposed module.

3. This paper is very similar to RAFT from ECCV2020 in terms of insight and specifically for the module. A clearer comparison between the two works and their essential differences would be helpful.

4. It is important to note that fine facial motion may not equal to better image quality. Therefore, a more thorough evaluation of the estimated motion would be beneficial. Additionally, conducting this task on higher resolution facial images would provide more convincing results.

5. The author sets the iteration number to 8, which is an important hyperparameter. It would be useful to know why this setting was chosen and whether it significantly affects the model's runtime.


Limitations:
Both limitations and social impact are discussed in this paper.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper anaylizes the limitations of existing face animation methods in capturing the finer facial motions, and hence design a non-prior-based motion refinement approach to achieve finer face motion transfer in local areas. A correlation volume between the source and driving images is constructed as non-prior motion evidence, and a refinement module is introduced to generate the fine facial motions iteratively. Extensive experiments show the effectiveness of this paper.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The idea of involving a non-prior based motion refinement module is effective to capture the fine motions in local areas.
2. The manuscript is well written and clearly states the main idea and contribution in face animation.
3. This paper performs extensive experiments, and the results of the same-identity video reconstruction task outperform many previous methods. 

Weaknesses:
1. This paper concentrates on the motion issue, but the video results of this paper are not impressive enough. No obvious improvements can be seen given the video results on the self reconstruction.
2. This paper does not show the quantitative results in terms of some usual metrics, i.e. CSIM and FID, on the cross-identity reenactment task. I wonder the performance of this work in preserving identity.
3. The introduction should be improved, as it only concludes limited contributions. 

Limitations:
This paper is effective to refine the motions to obtain better results, but is still limited to the quality of the learned keypoints, as the limitaions in the paper stated, which I think is more urgent to solve for unsupervised face animation. And this paper did not give comprehensive evaluations on the performance in identity preserving, which is important for face animation in practical applications. 
I'm not sure whether it is qualified for the conference.  If the authors could give reasonable response for the problems stated in **Weakness** and **Questions**, I would like to consider improving my rating. 

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper proposes a method for face animation via learning to refine a coarse motion field. The refinement is performed in a recurrent manner using the previous iteration's motion, occlusion map and structure correlation volume.  Both the qualitative and quantitative results show improvement over prior art.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1) Qualitative results show that the proposed method is clearly model finer facial movement much better than prior art, especially around the eye. FNeVR is very close but the proposed method has fewer uncanny artifacts.

2) Quantitatively as well, the proposed method outperforms prior art.

3) The paper is well written.

Weaknesses:
1) Across all the results, I notice a strong identity shift during animation. While this is common (and seemingly worse) in prior art as well, it is important to quantify. One way to measure this is to report the FaceIDLoss or L2-loss as the head is rotated 15, 30, 60 degrees from the original head-pose. I believe this is an important evaluation to include for the sake of completeness. 

Limitations:
1) An important limitation of this method is that it is sensitive to the scale of the face of the driving video. For example, around the 1:47 of the supplementary video, the face structure of the animated face deviates significantly from the source and is actually closer to that of the target. This is unavoidable due to the use of 2D key points in the method, in fact it is expected. The authors must include this as a limitation of their work. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper presents an unsupervised method for learning to create continuous video animations of faces given a single input image and a driving video of the same or a different subject. The method relies on the optical flow between the driving and source images to warp the features of the source image, which are then passed to a generator to create the final output frames. The main contribution of the proposed method lies in the two-step formulation of the flow estimation, in which starting from a coarse prediction using a known prior-based method (Affine Transforms, Thin Plate Splines) it iteratively produces updates to reach a more fine-grained flow (and occlusion) estimation with more local details. The flow updates are learnt in an unsupervised way without using prior models, but by building a correlation volume between the source and driving images. The paper includes experiments, generated images and supplementary videos that validate the authors' claims for more detailed animations in comparison to similar methods. 


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is well written overall and it is easy to follow along with the presented concepts and results.
- The paper includes experimental results that validate the authors' claims, as well as ablations that offer a better understanding about model design choices.
- The method produces higher-quality generated images/videos than the compared methods for unsupervised learning of face animation.
- The idea of using motivation from RAFT to built a similar correlation structure for estimating motion flow for face animation is interesting.

Weaknesses:
- Some design choices are not empirically or experimentally justified in the paper. For example, why are specifically 6 iterations used to update the initial coarse estimations? Why does it make sense to sample from the same correlation structure for all iterations, while each iteration operates at a different image scale? 
- Even though the method performs well compared to previous methods, there is still some evident identity shift in cross-subject experiments, meaning that the shape of the face slightly changes from the original to the one of the driving frame. This is evident in Figure 5 (for example 2nd row) as well as in the supplemental video. This effect makes sense because the estimated flow between different subjects possibly includes more deformation information, rather than only deformation because of expressions or rigid motion. Disentangling identity from other motions is a common weakness of cross-identity animation methods, which exists in this method, too. 

Limitations:
The authors have included a section discussing the method's limitations and possible negative societal impact. A mentioned limitation (the correlation matrix relying on learned keypoints) might also be the reason for changing the facial shape in cross-subject animations.

Rating:
5

Confidence:
2

REVIEW 
Summary:
1. This work proposed a  new unsupervised face animation approach to learn simultaneously the coarse and finer motions.
2. The results outperformed the state-of-the-art methods on two representative datasets.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The target task is important.
2. The method is intuitive and reasonable.
3. The results are good.
4. The paper is well-written.

Weaknesses:
1. Compared to prior model. The proposed non-prior models seems like a non-linear version of prior model based on affine transformation. When the number of keypoints increasing and the grid of affine transformation being much finer, will it approach the proposed non-prior model? Another related question is that why the proposed model works better than a local thin-plate-spline motion model (such as [38])?

2. Visualization. 1) In the visualization of comparison in video, the results of the proposed model looks close to FNeVR. Authors could also provide user study in visual quality for comparison. 2) Also, the results on CelebV-HQ is really important, since it is a much challenging dataset. Visualization results on this dataset is necessary, in comparison, ablation study and video. 3) Why didn't report the results of FNeVR on CelebV-HQ on Table 1?

I am inclined to accept this paper, if the concerns can be solved.

Limitations:
Yes, limitations are discussed in the main paper.

Rating:
5

Confidence:
5

REVIEW 
Summary:
The paper presents a new approach to generate human face videos based on a source image and a driving video, which simultaneously learning both coarse and finer facial motions. The proposed approach utilizes a structure correlation volume constructed from keypoint features to provide non-prior-based motion information, which is used to iteratively refine the coarse motion flow that is estimated by a prior motion model. 

The main contributions of the paper are: 
1. A non-prior-based motion refinement approach to compensate for the inadequacy of existing prior-based motion models.
2. Utilize the keypoint features to build a structure correlation volume that represents the structure correspondence between the source and driving images across all spatial locations.
3. Extensive experiments on challenging benchmarks that demonstrate the effectiveness of the proposed approach in enhancing the capability of prior-based motion representation through learning the motion refinement.


Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper presents a novel approach to generate human face videos that simultaneously learns both coarse and finer facial motions. The proposed approach utilizes a structure correlation volume constructed from keypoint features to provide non-prior-based motion information, which is used to iteratively refine the coarse motion flow estimated by a prior motion model. The approach addresses a significant problem in the field of unsupervised face animation.

2. The paper is of good quality, and the method proposed is clear. The authors provide a detailed description of the approach, including the structure correlation volume and the motion refinement module. The experimental results demonstrate the effectiveness of the proposed approach in enhancing the capability of prior-based motion representation through learning the motion refinement.

3. The paper is well-written and easy to follow. The authors provide a clear and concise description of the proposed approach, including the key components and the experimental setup. The paper is well-organized, and the authors provide a clear summary of the contributions and limitations of the proposed approach.

4. The proposed approach has significant implications for the field of unsupervised face animation which addresses an important problem in this field: the inadequacy of existing prior-based motion models to capture detailed facial motions. The proposed approach is effective in enhancing the capability of prior-based motion representation through learning the motion refinement. The approach has potential applications in creating imaginative image animations for entertainment purposes, but it also has the potential to be used in creating deepfakes, which could have negative impacts. The authors acknowledge this limitation and provide recommendations for future work. 


Weaknesses:
1.	The pictures in Figure 1 are too small, especially the optical flow map is not clear enough, no obvious changes can be seen before and after refinement. Also, is the schematic diagram of affine transformation in Figure 1 drawn based on a real example? Why does the affine transformation change so much after refinement?

2.	Straightforward combination of existing techniques. The innovation of this paper is not enough, the main innovation point lies in the motion refinement module. However, the method of correlation volume and iteratively refine optical flow used in it is very similar to the correlation matrix and iteratively update in some flow estimation[1,2,3] and correspondence estimation[4,5] methods, and this set of process of first constructing a 4D correlation volume, and then iteratively updating and optimizing optical flow has also been used in some neural style transfer(NST) methods[6,7], but the author did not clarify the differences between the module they used in this paper and related modules in these NST methods.

3.	There is an error in line 180 of the article: the author claims that the look up operation on the correlation volume is shown on the left side of Figure 2, but there is no corresponding content in Figure 2. Should Figure 2 be changed to Figure 3? 

4.	In the comparison video provided in the supplemental material, the video of each method is too small, and it is difficult to see the tiny facial deformation details. It is recommended to arrange the videos of each method in the form of a nine-square grid, and enlarge the size of each video window.

5.	The experiments of verifying the effectiveness of the proposed method are insufficient. The framework used by your method is the framework of unsupervised image animation [8,9,10,11], which should be able to generate animation videos on any object category. In addition to human faces, this framework can also be applied to human bodies and animals. Previous unsupervised image animation methods [8,9,10,11] have also been tested on related datasets of different subjects, including TaiChiHD[9], TED-talks[10], and MGif[8]. Your method has only been tested on face-related datasets, but from your overall framework, I don't see any modules that restrict the method to only work on human face. Therefore, I think that the non-prior-based motion refinement module should be applied to datasets of different objects to further test its effectiveness.

6.	Lack of quantitative experiments on the Cross-identity task, such as user study that have been used in previous methods [8,9,10,11].

References

[1] Dosovitskiy, Alexey, Philipp Fischer, Eddy Ilg, Philip Hausser, Caner Hazirbas, Vladimir Golkov, Patrick van der Smagt, Daniel Cremers, and Thomas Brox. 2015. “FlowNet: Learning Optical Flow with Convolutional Networks.” In 2015 IEEE International Conference on Computer Vision (ICCV). doi:10.1109/iccv.2015.316.

[2] Teed, Zachary, and Jia Deng. 2020. “RAFT: Recurrent All-Pairs Field Transforms for Optical Flow.” In Computer Vision – ECCV 2020, Lecture Notes in Computer Science, 402–19. doi:10.1007/978-3-030-58536-5_24.

[3] Xu, Haofei, Jing Zhang, Jianfei Cai, Hamid Rezatofighi, and Dacheng Tao. 2022. “GMFlow: Learning Optical Flow via Global Matching.” In 2022 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). doi:10.1109/cvpr52688.2022.00795.

[4] Kim, Seungryong, Stephen Lin, SangRyul Jeon, Dongbo Min, and Kwanghoon Sohn. 2018. “Recurrent Transformer Networks for Semantic Correspondence.” arXiv: Computer Vision and Pattern Recognition,arXiv: Computer Vision and Pattern Recognition, October.

[5] Zhang, Pan, Bo Zhang, Dong Chen, Lu Yuan, and Fang Wen. 2020. “Cross-Domain Correspondence Learning for Exemplar-Based Image Translation.” In 2020 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). doi:10.1109/cvpr42600.2020.00519.

[6] Liu, Xiaochang, Xuanyi Li, Ming-Ming Cheng, and Peter Hall. 2020. “Geometric Style Transfer.” Cornell University - arXiv,Cornell University - arXiv, July.

[7] Liu, Xiao-Chang, Yong-Liang Yang, and Peter Hall. 2021. “Learning to Warp for Style Transfer.” In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). doi:10.1109/cvpr46437.2021.00370.

[8] Siarohin, Aliaksandr, Stephane Lathuiliere, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. 2019. “Animating Arbitrary Objects via Deep Motion Transfer.” In 2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). doi:10.1109/cvpr.2019.00248.

[9] Siarohin, Aliaksandr, Stéphane Lathuilière, Sergey Tulyakov, Elisa Ricci, and Nicu Sebe. 2019. “First Order Motion Model for Image Animation.” Neural Information Processing Systems,Neural Information Processing Systems, January.

[10] Siarohin, Aliaksandr, Oliver J. Woodford, Jian Ren, Menglei Chai, and Sergey Tulyakov. 2021. “Motion Representations for Articulated Animation.” In 2021 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR). doi:10.1109/cvpr46437.2021.01344.

[11] Zhao, Jian, and Hui Zhang. n.d. “Thin-Plate Spline Motion Model for Image Animation.”


Limitations:
Yes, they have adequately addressed the limitations.

Rating:
4

Confidence:
4

";1
K4FK7I8Jnl;"REVIEW 
Summary:
The authors present MAG-GNN, a subgraph-based Graph Neural Network (GNN) method that utilizes reinforcement learning to effectively identify discriminative subgraphs. They empirically assess the performance of MAG-GNN using both synthetic and real-world datasets.

Soundness:
3

Presentation:
1

Contribution:
3

Strengths:
- The main elements of the RL framework are properly explained.
- The empirical section includes clear and valuable research questions.

Weaknesses:
1. The writing is significantly lacking in clarity and context.
2. The presentation of results lacks clarity.

Limitations:
The authors have acknowledged the limitations of their proposed approach.

Rating:
4

Confidence:
2

REVIEW 
Summary:
The paper considers the problem of increased complexity of subgraph GNNs with respect to standard MPNNs, due to the enumeration of the entire bag of subgraphs for every given graph.

The authors first study the need of enumerating all subgraphs, and show that there exist graphs where using a small subset of subgraphs have the same discriminative power as the entire bag of subgraphs. Motivated by these cases, the authors propose an RL-based approach, named MAG-GNN, to find the best subset of subgraphs to be employed in the task at hand.

The method starts with a subset of subgraphs that are randomly selected. Then, it employs a DQN framework, where the Q-network is parametrized by a subgraph GNN, to replace each subgraph in the subset with a another subgraph. The new subgraph is obtained by replacing only one index of the node tuple identifying the subgraph. Upon reaching a maximum number of steps, the obtained subgraph set is passed through a downstream subgraph GNN for the final predictions.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The problem of reducing the complexity of subgraph GNNs without trading off performances is significant. The authors identify graphs where a small subset of subgraphs has the same discriminative power as the full bag of subgraphs, which motivates the search for such subsets.
2. The combination of RL with subgraph GNNs is original, and represent a natural way of learning to select subgraphs.

Weaknesses:
1. **The paper misses a thorough comparison with a random policy**, where a random subset of subgraphs is randomly selected at every forward pass. The authors consider tangentially the question in the experimental evaluation (Q1) but fail to answer in details. For every dataset you consider, there should be a table entry reporting the performance of the random baseline. **Such baseline should have the same $k$ and the same $m$ that you consider**. Furthermore, it should be based on the same architecture that MAG-GNN uses as downstream prediction model.  Only with such comparison we can conclude that MAG-GNN performs better than random. Note that:
    - The answer of Q1 is insufficient, both because you consider only one dataset, and also because you focus only on one node tuple ($m=1$). Please include a table entry for every dataset. 
   - The experiment in Appendix E reports a comparison with a random baseline that uses ensemble predictions. However, it would make more sense to aggregate the representations of the subgraphs and _then_ perform a prediction, an approach that is indeed followed by subgraph GNNs (as reported in Equation 3). The ensemble method instead predicts using each subgraph separately and then aggregates the predictions, which is clearly less powerful. Please include a comparison with random that does not use the ensemble approach, but that aggregates the representations of the $m$ subgraphs to obtain a graph representation, and then performs the prediction. Note that you should evaluate the same $m$ and $k$ values that you sweep on for MAG-GNN (up to 4 and 6 respectively).

2. **The paper misses comparisons with the I-MLE approach proposed in $k$-OSAN** to select subgraphs, which has exactly the same goal.

3. **The paper misses comparison with many subgraph GNNs**, such as Bevilacqua et al 2022, Zhang et al 2023. These methods obtain better performances than MAG-GNN for example on ZINC, using node-based subgraphs (where the bag of subgraph is of size $\vert V \vert$, and therefore are tractable). It is unclear what is the advantage of training an RL agent, which has much higher training time, and using $k$-order policies with $k>1$, if the performances are worse than node-based subgraph GNNs and the inference time is not better.

4. The comparison on the ZINC dataset is unfair because the model does not respect the parameter budget imposed by the dataset (500k). Currently he budget is twice the one that is allowed (three times if we consider the target q-network). Please fix and ensure you remain within the budget.

5. **The paper misses a thorough comparison with the model using the full bag and $k>1$**. Again, it should be based on the same architecture that MAG-GNN uses as downstream prediction model. Only in this way we can know what is the performance achievable with the full bag, and understand how distant MAG-GNN is from it. 



Bevilacqua et al 2022. Equivariant Subgraph Aggregation Networks. ICLR 2022

Zhang et al 2023. A Complete Expressiveness Hierarchy for Subgraph GNNs via Subgraph Weisfeiler-Lehman Tests. ICLR 2023

Limitations:
n/a

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper aims to locate expressive subgraphs for graph representation learning. To do so, the author proposed to use deep Q Learning for training an efficient agent which optimizes the combinatorial problem of choosing the optimal subgraph. The proposed agent-aided GNN shows good performance and is able to transfer to real-world datasets.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Motivation is interesting and the methodology is well explained. The experiment result seems sound. The proposed model is on par with state-of-the-art while using less computational resources.

Weaknesses:
* How to train an effective agent seems to be a critical point. Despite that, the author has a brief discussion in sec 4.1, more quantitative analysis and illustration on this part can better support the author's choice of ORD.
* The proposed method seems to be on par with the state-of-the-art. While I don't think absolute performance is an important concern given the method is more efficient. I think it's important to investigate in L316 Q3 how much of the RL agent expressiveness benefit the real-world datasets compared to random ones or human-designed ones.

-------------------------------------------------------------------------------------
I've read the author's response to me and other reviewers which resolves my concern. I would like to remain my rating as weak accept.

Limitations:
N/A

Rating:
6

Confidence:
2

REVIEW 
Summary:
this paper proposes a strategy to select the optimal subset instead of extensive enumeration for the subgraph GNN. The proposed method can achieve comparative results and reduce the running time.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. the paper is well written and easy to follow.
2. the experiment part is thoroughly detailed。
3. the experiment shows the model can achieve the comparative result with subgraph GNN while significantly reducing the running time.

Weaknesses:
N/A

Limitations:
N/A

Rating:
7

Confidence:
2

REVIEW 
Summary:
This paper proposes a reinforcement learning boosted approach to subgraph graph neural networks (GNNs) that achieves high expressivity with constant complexity, instead of the exponential complexity of existing methods. They demonstrate the effectiveness and efficiency of their approach through experiments that compare it with the state-of-the-art methods.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper is well-written.
2. The proposed MAG-GNN can achieve good expressivity while reducing the time complexity of subgraph enumeration.


Weaknesses:
1. The authors report that most methods in Table 1 achieve 100% accuracy on the dataset. This suggests that the dataset may be too simple to adequately evaluate the expressivity of a model. The authors should consider applying more challenging datasets to better assess the performance of their models.
2. The table captions are vague and do not provide sufficient information for readers to interpret the experimental results properly.
3. In Section 3, the authors use the same data for training and test, which may lead to overfitting and unrealistic performance.
4. The notation |V^2| should be |V|^2.
5. The proof of the theorem is unclear and lacks details. 


Limitations:
The paper does not discuss limitations or potential social impacts.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper introduces MAG-GNN, an RL approach for identifying important subgraphs. This is valuable since models using fewer subgraphs incur lower inference costs. Experimentally, the authors show that MAG-GNN retains the strong performance of sub-graph approaches while reducing the run time.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The guiding question of the paper ""Are a small number of subgraphs sufficient for graph problems?"" is well-motivated. The proposed RL-based approach for identifying subgraphs is well-explained. The approach's performance and efficiency are validated on various benchmark datasets. The negative results of the node-level task are helpful, revealing both the current limit and future potential of the paper.


Weaknesses:
**Training efficiency**. While MAG-GNN accelerates inference, the training process becomes more time-consuming. On smaller graphs, this training pipeline may dominate the overall computational costs.

**Task scope**. MAG-GNN excels in graph-level tasks but underperforms in node-level tasks, as the authors gracefully acknowledge.

**Baseline comparison**. The authors primarily compare MAG-GNN with methods using full subgraphs in the experiment section. Considering its efficiency goals, comparing it with other efficient alternatives, such as applying node/edge pruning approaches to the full graph and then using standard subgraph methods, would be useful.

Limitations:
See the ""Weaknesses"" section above.

Rating:
7

Confidence:
3

";1
S8DFqgmEbe;"REVIEW 
Summary:
The paper discusses the identification and reconstruction of latent causal graphs from unknown interventions in the latent space. The main focus is on uncovering the latent structure in a measurement model, where the dependence between observed variables is less significant than the dependence between latent representations without parametric assumptions.
The paper presents a characterization of the limits of edge orientations within the class of Directed Acyclic Graphs (DAGs) induced by unknown interventions. The paper concludes with an experimental evaluation that shows the recovery of the causal graph using structural Hamming distance as the error metric of the true vs. learned causal structure.

********** POST REBUTTAL *******

Thank you to the authors for their responses. I’m satisfied with the clarifications and increased my score

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
Creating models that identify the causal structure in scenarios where parametric assumptions are not applicable is an important problem in causal inference.

The proposed approach uses interesting and clever concepts to provide a novel perspective of latent causal graphs. 

The proposed approach provides an intriguing perspective about the DAGs’ equivalence class associated to unknown interventions.

Weaknesses:
Some details of the paper could be further clarified such as why not used stablished concepts of Markov equivalence classes, etc. Or why not consider non-parametric learning of causal structure such as the one in Gao, et al. (2020) or Azadkia et al. (2021)

Another aspect to consider is the fact that the evaluation is done with two settings and 100 runs for only 4 combinations of M,N. 

In general the paper main contributions seem interesting from a theoretical perspective and because of that a more thorough discussion could have improved the paper to compensate for the limited evaluation.

Limitations:
The paper does not describe potential societal impacts.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper studies the problem of recovering causal relationships under the measurement model where there are latents but no direct causal edges between observed covariates. The authors introduced two graphical concepts -- imaginary subsets and isolated edges -- and show how they relate to sufficient conditions for recovery (under some additional assumptions). The assumptions are discussed at length in the appendix and a two phased recovery algorithm is proposed.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper is well-written and easy to follow in general. I also appreciate the in-depth discussion of the assumptions.

Weaknesses:
Assumption 1(d) seems redundant and I think it is implied by assumption 1(c); rewriting it in terms of a lemma or consequence of assumption 1(c) would strengthen the paper and reduce the number of assumptions required. Consider the following argument: Fix any two latents $H_i$ and $H_j$. By assumption 1(c), $H_i$ has a child $X_i$ that is not a child of $H_j$, and $H_j$ has a child $X_j$ which is not a child of $H_i$. We now consider the contrapositive of 1(d). Suppose $X_i$ and $X_j$ are d-connected. In the measurement model, this means that there is a path $X_i \gets H_i - \ldots - H_j \to X_j$ which has no colliders. Then, this same path is a witness to $H_i$ and $H_j$ being d-connected.

I am unsure how interesting Definition 5.2 on Isolated equivalence class (IEC) is. I do not think it is fair to compare its significance to Chickering's ""covered edge reversal"" characterization, which I believe is significantly more subtle and interesting. For instance, while there exists a sequence of covered edge reversals (say edge $e_1$, then $e_2$, then $e_3$, ..., then $e_r$) between any two DAGs in the same Markov equivalence class (MEC), the edges may actually NOT be covered edges midway through the transformation and one cannot arbitrarily reverse the set of edges $\{e_1, \ldots, e_r\}$ in any ordering whilst ensuring that we always get a DAG from the same MEC. Chickering further gives a constructive algorithm which tells us how to find this sequence $(e_1, \ldots, e_r)$. In contrast, IEC seems trivial since it involves a union of disjoint edges, where the size of IEC is always 2^(number of isolated edges) and every edge can be reversed at any point in time.

Experimental details are lacking: Section 6 is short and there is nothing in the appendix about the experiments. It is hard to judge or appreciate any empirical contribution. I feel that the authors should have just focused on presenting this work as a theoretical contribution (which I think is already sufficient on its own, modulo the questions below).

Limitations:
Nil.

Rating:
6

Confidence:
4

REVIEW 
Summary:
- The paper studies causal representation learning, or more precisely the identification of the causal graph between observed and latent variables, from interventional data with unknown intervention targets.
- Its main contribution is an identifiability result for the causal graph. This theorem makes no assumptions on the functional form of the causal model or the mixing function, but is based on several graphical requirements. In various ways, the paper requires that different latents affect different sets of observed variables.
- The authors spend a large part of the paper discussing these assumptions and providing sufficient conditions for them.
- In the end, they also briefly demonstrate their algorithm on toy data.
- Unlike most of the CRL literature, the paper does not study the identification of the latent *variables*. The authors delegate this task to ""existing work, since one can [...] use deep latent-variable-models to infer the latent distributions from the latent structure"".

I have read the author's rebuttal. They have addressed my questions clearly.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
- It is great that the authors can prove identifiability from observational, unlabelled data, and without functional assumptions. This makes the results potentially quite practical, barring limitations from the graphical assumptions (see below).
- While I have not been able to check the proof in detail and I do have some questions (see below), I overall believe that the key results are correct.
- The paper is very thorough, with precise statements, extensive discussion, useful examples, and thorough appendices. There is a lot in here that may be useful beyond the concrete identifiability result.
- It is also extremely well-written, really a joy to read. I appreciate the frequent signposting. Great job!

Weaknesses:
- Different from virtually all other CRL works, the authors choose to focus *only* on the identifiability of the causal structure and entirely disregard the identification of the causal variables.
	- Arguably, in most applications of CRL, the latent variables are at least as important as a result.
	- The strategy of solving the structure learning problem first and delegating the variable identification to a latent-variable model makes sense, but deserves a discussion that goes beyond the two lines that the authors have reserved for it.
	- Could you perhaps quote or sketch what kind of guarantees on the identification of the latent variables one can expect when following such a two-step procedure?
- As with most of the CRL literature, a key question is whether the assumptions are too unrealistic or too difficult to verify to make the results useful beyond pure academic curiosity. I am particularly worried that assumptions 1(c), 1(d), and the lack of imaginary subsets do not apply to typical CRL settings.
	- For instance, I find it difficult to imagine these assumptions applying to any of the systems sketched in lines 18-20 in the introduction. Could the authors discuss this and provide perhaps some semi-realistic examples of systems that satisfy them?
	- Negative results are equally valuable though, and I appreciate the counter-examples that show the lack of identifiability when these assumptions are violated.
- I am also concerned about the restriction to maximal measurement models.
	- This seems to be a bit at odds with Okham's razor: if multiple models explain the data, why should we focus on the most complex model that explains the data? Should we not identify the family of all models, or the simplest model?
	- It would be great if the authors could comment on how strong this assumption is and in what kind of systems they expect it to be satisfied.
- The experiments are very limited and really just a minimal proof of concept.
	- It would make the paper stronger if the authors would design experiment that test whether the approach scales to interesting problems and that test how robust the approach is to violations of the assumptions.
	- In addition, a comparison to other methods (for instance on problems that satisfy  functional assumptions made by other papers) would be interesting.

Limitations:
- The authors are very clear about the assumptions of their theoretical results.
- Nevertheless, I believe it deserves more discussion whether these assumptions fit realistic problems (see above).

Rating:
5

Confidence:
3

REVIEW 
Summary:
this paper aims the learn the causal structure in the latent space, by using interventional data, where the intervention targets are unknown, but with certain restrictions. This paper's focus is to provide some theoretical analysis, to show that, under what assumptions, we can recover (up to what level) the causal structure (include the bipartite DAG between observable variables and latent variables, and the DAG within the latent variables).

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1 - the paper deals with a very challenging, or even ill-posed problem, about recovering the causal structure in latent space.
2 - the theoretical part seems to be sound.
I have checked the illustration and the proofs in appendix C, the idea of the necessity of the assumptions in (1) is well presented
3 - recovering latent causal structure has great potential for AI or AGI area.

Weaknesses:
The most concern from my side is that, what this paper is telling lacks real-world relevance. this makes me doubt if this work can be helpful for practical usage. It seems to be more like a analytical deduction in that, given what assumptions, what I can achieve. but whether these assumptions are relevant in real-world is unclear. Some details
1) Can this work be evaluated from ""causal representation learning"" perspective?
for example, image pixels are generated from some latent concepts/entities, which seems to be quite aligned with the motivation of this work, can this work be experimented on any such case to justify its real usefulness?
2) justification of the required assumptions
many assumptions used in this work are untestable, although the authors discussed some of them, and pointed out that they are not be ignored, which is fine. But without real-world relevance, how can I know in what situation should I apply the proposed algorithm? for example, a complete family of targets seems to be too strong, from the perspective of ""causal discovery from interventional data"".
3) Better re-structure the paper writing
Overall, the paper is very dense and lacks intuitions. Regarding identifying the latent variables (or at least, detecting the number of latent variables), one sentence in line 245 is very interesting: ""Proposition 4.1 suggests that we assign a latent variable to each maximal valid subset"", I think this has a potentially nice intuition about identifying latent variables by using dependencies among the observational variables. I suggest the authors to provide an overview, with intuitions about the key idea, rather than defer then to Section 4 and 5.
4) claim of Example 1 is improper.
even with unknown interventional target, and by intervening once, we can still recover the orientation between X1 and X2. you can further checking if there is marginal independence between X1 and ""whether the intervention is performed"", and X1 and ""whether the intervention is performed"", this information can further help you to identify the causal direction. you can check more details from [1].

[1] Mooij, J. M., Magliacane, S., & Claassen, T. (2020). Joint causal inference from multiple contexts. The Journal of Machine Learning Research, 21(1), 3919-4026.

The second concern is the weakness of experiment.
as the author pointed, ""Compared to these existing works, our focus is on nonparametric models with unknown, hard, single-node interventions on the latent variables"", I think this is a clear configuration, and we can certainly conducts comparison with other SOTAs (such as using parametric approach)


Limitations:
N/A

Rating:
4

Confidence:
2

REVIEW 
Summary:
This paper studies the problem of nonparametrically constructing latent causal graphs given unknown _interventions_ in a measurement model. In particular the paper given a constructive proof of establishing sufficient conditions under which one can find the latent causal structure between observed and hidden variables. 

Authors achieve this by first defining a notion of *imaginary subsets,* which is a subset of observed variable that are not children of a single latent variable. Authors show that if a graph does not have any imaginary subsets then up to some assumptions on the graph, one can identify a causal structure. Author further give sufficient conditions that guarantee there don't exist imaginary subsets in the graph. 

**Update**: After reading other reviews and rebuttals. I have updated the score from 5 to 6. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:

- Authors identify sufficient conditions for when causal structure is possible and construct a causal graph under this setting.
- Paper is thorough and identify limitations with respect to  limitations of the proposed construction.

Weaknesses:
- Overall though the paper is difficult to follow for non-experts. It heavily relies on jargon that is used commonly in causal inference and thus proves to be a difficult read. For instance from the abstract authors use unknown *intervention* starting in the abstract which is not defined much later in the paper.
- The paper can really benefit from a running example that could guide the reader through all the definitions because in the current state one it is difficult to follow.

Limitations:
Authors have addressed limitations of the work. 

Rating:
6

Confidence:
1

";1
pNtG6NAmx0;"REVIEW 
Summary:
This paper proposes a statistical approach called KaRR to assess the factual knowledge contained in Generative Language Models (GLMs). The authors use a large-scale assessment suite with 994,123 entities and 600 relations to evaluate 14 GLMs of various sizes. The results show that KaRR exhibits a strong correlation with human assessment and achieves a lower variance to varying prompts. The experiments also reveal interesting insights into the scaling law of GLMs and the impact of tuning on instruction-following data. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed statistical approach is novel and effective in assessing the factual knowledge contained in GLMs. The proposed method effectively considers different surface forms of subject, object, and relation.
2. The large-scale assessment suite used in the experiments is comprehensive and orders of magnitude larger than prior studies.1. Some of the paper's findings, such as scaling laws of knowledge and fine-tuning instructions to improve consistency, have been found in previous work.
2. Previous work has systematically assessed the consistency of the same facts under multiple prompts, but the facts considered in this paper are larger in scale and prompts are more diverse.

Weaknesses:
1. Some of the paper's findings, such as scaling laws of knowledge and fine-tuning instructions to improve consistency, have been found in previous work.
2. Previous work has systematically assessed the consistency of the same facts under multiple prompts, but the facts considered in this paper are larger in scale and prompts are more diverse.

Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper introduces an automatic evaluation metric to assess the amount of factual knowledge kept by large language models (LLMs). This proposed metric considers various surface forms of factual knowledge presentation, allowing for an evaluation that not only measures the accuracy of the models in terms of factual knowledge but also considers the prediction robustness of the knowledge. Later, the authors demonstrated that this metric has a higher correlation with human annotations than previously proposed metrics. Additionally, the paper includes several robustness analyses for the metric, such as examining the impact of the prompting format and its relationship to co-occurrence statistics. And it shows the new metric's effectiveness compared to previous metrics.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The proposed metric is commendable as it goes beyond capturing mere accuracy and considers model performance consistency. This aspect is crucial in evaluating the factual knowledge of large language models, as it reflects their ability to provide correct information consistently.
2. The strong invariance results achieved by the automatic evaluation metric compared to other baselines are a notable strength.
3. The paper's comprehensive analysis of the proposed metric is great. By covering multiple aspects, the authors thoroughly evaluate the metric's performance. This level of analysis contributes to a better understanding of the metric's strengths, limitations, and overall effectiveness in assessing the factual knowledge of large language models.

Weaknesses:
1. The human correlation score of 0.43 for the automatic evaluation metric may be considered relatively low. 
2. The proposed metric is not interpretable. To improve the interpretability of the score, calculating an oracle score and then comparing the current score to the oracle score ratio could be a useful approach. This ratio can aid in better understanding the effectiveness of the metric and provide a clearer picture of its performance relative to the best achievable outcome.

Limitations:
No, the limitation of the work is not discussed.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes KaRR, a statistical approach to assess factual knowledge for generative language models based on graphical models. An assessment suite is also proposed for future research. Experiments are conducted with 14 popular large language models and comprehensive analyses are also conducted to reveal related properties.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- This paper released a dataset for assessing factual knowledge in generative language models, which is large-scale (millions of entities and text aliases) and could be used in future works.
- This paper proposed a statistical score KaRR based on graphical models and KaRR aligns well with human preferences as shown with human evaluation.
- Lots of popular large language models are experimented with and the analyses are interesting, e.g., Table 2(b).

Weaknesses:
- The knowledge assessment focuses on entity-aware knowledge, which could be a relatively limited knowledge form. The current LLMs are good at identifying entity-aware knowledge. The major hallucinations actually come from numbers, dates, etc. Not a strict weakness, just wondering if KaRR could be extended to these aspects.
- There are in total 600 relations in the assessment suite. Does that mean the suite can only be employed for the specific 600 relations? What if there are new relations?

Limitations:
Refer to previous weaknesses and questions.

Rating:
5

Confidence:
3

REVIEW 
Summary:
this paper proposes a statistical method to probe the knowledge in generative language models, which aims at connecting symbolic knowledge and GLM's text format generation. More specifically, the KaRR comprises two components with regard to specifying relation and subject entity. The authors also present the graphical model for model implementation on the text. The results show that the knowledge in GLMs follows the scaling law, but when the model is finetuned on instruction-following data, it may compromise the model's ability to consistently generate factually correct text.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. very important and interesting problem, assessing the knowledge stored in generative language models is challenging and worth studying
2. the proposed KaRR method shows strong robustness to prompt variance
3. Interesting findings: instruction-following data compromises the model’s capability to generate factually correct answer.

Weaknesses:
1. This work only focuses on generating the object, I believe the author can assess the model's ability to generate the subject (by reversing the triples/facts) to gain a more comprehensive assessment of knowledge stored in GLMs.
2. The average alias count for each subject is approx 1.39, and for each object it is about 1.78, which I believe could not support the ""diverse"" claim.
3. The proposed KaRR’s design to measure reliability is not clear enough.
4. In line 38, the authors claim ""Prior methods are designed for masked language models (MLMs) and are incapable of measuring GLMs."". However, to the best of my knowledge, there are several works about assessing the knowledge in GLMs, such as [1, 2].
 
[1] Hendrycks, D., Burns, C., Basart, S., Zou, A., Mazeika, M., Song, D., & Steinhardt, J. (2020). Measuring massive multitask language understanding. arXiv preprint arXiv:2009.03300.
[2] Dhingra, B., Cole, J. R., Eisenschlos, J. M., Gillick, D., Eisenstein, J., & Cohen, W. W. (2022). Time-aware language models as temporal knowledge bases. Transactions of the Association for Computational Linguistics, 10, 257-273.


Limitations:
N/A

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper presents a framework for the quantitative assessment of the knowledge captured by large language models, which includes a proposed metric and a large set of relations. Given subject-relation-object triplets, a basic approach to quantify the model's knowledge would be to use the probability of the object entity being generated, given the subject and the relation. This naive approach has shortcomings, which the authors address in the following way:
1. Entities can have multiple synonyms, which are important to consider to accurately assess consistency. The authors therefore augment their proposed evaluation suite with a set of aliases for each entity, extracted from Wikidata, and adapt the metric to take these into account.
2. As this kind of probing often suffers from spurious correlations, the authors propose the ""knowledge assessment risk ratio"" metric, which  also takes into account the expected generation probability when either the subject or the relation entity are not specified.

The authors thoroughly evaluate the proposed approach on 14 generative model. They additionally measure the effectiveness of their approach compared to other pre-existing metrics, showing strong correlation with human judgement and robustness of the metric towards prompt variation.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
* This is a sound and extensively evaluated approach towards the automatic quantitative assessment of the knowledge learned by LLMs. This method displays strong correlation with human judgements. While it cannot replace human annotation entirely, it should be extremely useful to the community as a cheaper, faster automatic way of performing knowledge assessment, much like how BLEU can be used during model development to validate the performance of translation models.
* The authors should be commended for calling out robustness (consistency of generation given similar prompts). This aspect is crucial for real-world applications of such models, and is often ignored in similar works.

Weaknesses:
* The proposed framework is limited to assessing knowledge in a fairly simplistic way, via the prediction of entities in subject-relation-object triplets. It would have been interesting to hear more about the limitations of such an approach, taking into account e.g. slightly more complex type of queries (see e.g. arXiv:2305.01157).

Very minor:
* lines 141-144, 148, 202, 291, 313-315: straight quotes should be turned into curly quotes

Limitations:
The authors did a decent job of addressing limitations, and I don't expect any potential negative societal impact from this work.

Rating:
7

Confidence:
3

";1
vcNjibzV3P;"REVIEW 
Summary:
The authors provided theoretical analyses and proof that the 3-WL algorithm and the Euclidean version of the 2-WL algorithm can distinguish any complete Euclidean graph pairs. The authors then demonstrated that the algorithm can be approximated with GNNs and ran the proposed model on synthetic data to show that it was indeed able to ""separate"" the hard graph pairs.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. The authors provided rigid mathematical formulation and proof. The problem was well-defined and formulated in the paper.
2. The organization of the paper is clear. The authors first discussed the problem, followed by theoretical analyses and proof. Model architectures and experiments are then provided to support the theoretical claims.
3. The theoretical results are general and can be potentially applied to a wide range of models.

Weaknesses:
1. The experiments are inadequate. The authors only tested their model and the baselines on small-scale synthetic datasets. The dataset used in the paper only contains small molecular graph pairs whereas, in practical 3D point cloud scenarios, there can be easily thousands of points. Furthermore, at such a scale, many traditional GNN-based networks including MACE and TFN are also ""separating"" according to the results. The authors may test their model and the baselines on larger practical datasets.

2. The potential benefit of a model being ""separating"" was also not experimented. Eventually, we want models to output some meaningful values in the classification or regression task. The author may experiment with such tasks to demonstrate the capability of graph isomorphism tests indeed help with representation learning.

3. The potential application of the algorithm is greatly limited by the assumption that the graph is complete. The authors also mentioned in the future work section that the proposed model scales as $O(n^4)$ with respect to the number of nodes, which is prohibitively large even for small point clouds.

4. The writing of the manuscript needs to be improved in some places. All references were not in parenthesis, making it hard to read the manuscript (e.g. line 143 when quoting the WL test). The quotation marks are not paired. The sections are also a bit strange. I would suggest making the related work and future work a separate section and the current Sec 3 into a subsection (as it is also a theoretical analysis).

Limitations:
The authors have mentioned the limitation of this work in the manuscript and no further negative societal impact is expected.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper analyzes neural networks for point clouds toward modeling of geometric phenomena. It considers the application of message passing networks/GNNs to Euclidean graphs, whereby a variation of the well-studied k-WL test is adapted to point clouds by using a complete graph on the point cloud and making use of Euclidean pairwise distances. To this effect, the authors propose the k-EWL test and show that: (1) For k=1, two iterations of message passing are sufficient to separate most point clouds in any dimension, (2) A single iteration is sufficient for all point 3D point clouds when k=3. Furthermore, additional differential architectures are proposed and demonstrated to have similar separation power as k-EWL tests.

I think the paper has some promise and is generally well-written. But it can be strengthened by better motivating the problem and providing more detailed experimentation, ideally on chemistry/molecular datasets (given that this was cited as a motivation/application in the intro). I think the paper needs some more work, but addressing some of these points would make me open to raising my score.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
(1.) The paper is generally easy to follow and well-written. I understood the definitions and theorem statements without any problem.
(2.) GNNs + point clouds seems like an underexplored area, and the authors make progress in this area (motivation nonwithstanding).

Weaknesses:
(1.) I think there is some motivation lacking for why one wishes to separate point clouds via GNNs, or why it is desirable to construct variants of k-WL. The paper hints at the importance of this for chemical applications, but there isn't much discussion about this beyond the intro.
(2.) The formulation of EWL doesn't seem novel; it is simply a standard MPNN on a complete graph with use of distance as an edge feature in the update rule (in the framework of Gilmer et. al 2017). SEWL seems more interesting, but the motivation is a bit lacking.
(3.) The experiments could be stronger. While the authors provide experiments on synthetic datasets of point clouds demonstrating effectiveness of the proposed architectures, the paper could benefit from some experiments on real-world chemistry tasks, as chemical applications, biological molecular datasets, etc. were cited as a motivation in the intro.

Limitations:
Some limitations are highlighted in the Future Work section at the end of the paper.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper studies the theoretical completeness of neural networks for Euclidean/3D point clouds, from the perspective of whether they can distinguish all non-isomorphic point clouds. 

Key theoretical contributions include showing that variations of the k-WL graph isomorphism test are complete for 3D point clouds, and that distance-based 1-WL tests are complete for *almost all* point clouds (measure theoretic perspective). 

The work also demonstrates that a GNN can be designed with the proposed completeness guarantees, and sanity checks the theoretical results on synthetic counterexamples from previous studies.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- This work shows that adaptations of the k-WL hierarchy of graph isomorphism tests can be 'complete' on 3D point clouds. I believe this is a **novel** theoretical contributions for neural networks on point clouds in Euclidean space.

- I believe the findings are **significant**, as neural networks on Euclidean graphs and point clouds are an emerging area of interest from both theoretical and applied perspectives.

- The paper is **well written** and **clear** in terms of presentation:
    - The Introduction does a good job highlighting the research gap.
    - The coverage of related work in Section 1.1 is useful.
    - Section 2 makes a good bridge from WL to the Euclidean setting.

- I went through the proofs, which are correct to the best of my understanding.

Weaknesses:
- It seems challenging to translate this paper's ideas into practice as the model's parameters depend on the number of points $n$ taken as input. This probably makes it very difficult to build a trainable model **while retaining** theoretical guarantees.
    - The authors are upfront about this when discussing limitations.

- Beyond sanity-checking the theoretical ideas on the counterexample from Pozdnyakov-Ceriotti, 2022, the synthetic experiment does not seem to provide any further insights into practical instantiations of the ideas in this paper, or about this class of models more broadly.



Limitations:
The authors have adequately addressed the limitations but not discussed any potential negative social impact.

Beyond what the authors mention regarding practical instantiation of their models, one major theoretical limitation is that the framework is restricted to complete geometric graphs, and the construction of complete/universal models for the general sparse graph setting remains an open question. This my be worth reiterating. 

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper seeks to theoretically demonstrate the complete determination of point clouds, up to permutation and rigid motion. The authors formulate a Euclidean variant of the 2-WL test, effectively illustrating the separation capacity of the Euclidean Graph Neural Network on highly symmetrical point clouds.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The paper delivers a theoretical exploration of point cloud completeness.
2. It discusses the separation capability of the Euclidean Graph Neural Network in high-dimensional representations.

Weaknesses:
1. In appendix Line 564, what does $(\star)$ stand for? 
2. Does the proposed method risk confounding reflection equivariance?

Limitations:
NA

Rating:
5

Confidence:
2

";0
1TJaITmK2Q;"REVIEW 
Summary:
In this manuscript, the authors develop a special module that can adaptively learn a suitable filtration function for persistent homology (PH) and its downstream tasks. In particular, their learning module is specially designed, so that resulting persistent homology is isometry invariant. Their model has been tested on two datasets for protein classification and CAD data classification. The model is novel and very interesting! 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
It is a novel approach to use machine learning to learn a suitable filtration function! It has also demonstrated the advantage over traditional approaches. 

Weaknesses:
Missing important references for related works. The test examples have limited data points. The model may suffer from over-fitting issues. 

Limitations:
It will be great if the authors can use more realistic examples and compare with state-of-the-art models. 


Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper develops a neural network to learn weights of given points in addition to other internal parameters to classify 3D clouds of unlabeled points on several public datasets. 


Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The authors should be highly praised for a rigorous approach to an important problem of point cloud classification by using isometry invariants from persistent homology. 

The paper is generally well-written.

Weaknesses:
Questions arise already when reading the abstract in line 11: ""to make the resulting persistent homology isometry-invariant, we develop a neural network"". For standard filtrations such as Vietoris-Rips, Cech, or Delaunay complexes on a point cloud, the persistent homology is already an isometry invariant of a given cloud of unlabeled points because constructions of all complexes above depend only on inter-point distances.

The main drawback of persistent homology is its weakness as an isometry invariant, which should have been clear to all experts in computational geometry many years ago but was demonstrated only recently. The paper by Smith et al (arxiv:2202.00577) explicitly constructs generic families of point clouds in Euclidean and metric spaces that are indistinguishable by persistence and even have empty persistence in dimension 1. 

Though Topological Data Analysis was largely developed by mathematicians, the huge effort over many years was invested into speeding up computations, rather surprisingly, instead of trying to understand the strengths and weaknesses of persistent homology, especially in comparison with the much simpler, faster, and stronger invariants of clouds under isometry. 

Persistence in dimension 0 was actually extended to a strictly stronger invariant mergegram by Elkin et al in MFCS 2020 and Mathematics 2021, which has the same asymptotic time as the classical 0D persistence and is also stable under perturbations of points.

A SoCG 2022 workshop included a frank discussion concluding that there was no high-level problem that persistent homology solves. In fact, persistence as an isometry invariant essentially tries to distinguish clouds up to isometry, not up to continuous deformations since even non-uniform scaling changes persistence in a non-controllable way. 
  
On the other hand, the isometry classification problem for point clouds was nearly solved by Boutin and Kemper (2004), who proved that the total distribution of pairwise distances is a generically complete invariant of point clouds in any Euclidean space. The remaining singular cases were recently covered by Widdowson et al in NeurIPS 2022 and CVPR 2023. 

Limitations:
The very last paragraph on limitations only discusses talks about future work, for example about ""generalizing our framework to include a wider class of weighted filtrations"" (line 342). Yes, there are numerous papers that go even further and include a given point cloud as a raw input without computing any justified invariants. 

Since the authors have written detailed and accurate mathematical proofs in the appendix, they could probably agree that *examples prove nothing* because counter-examples can still exist, especially when all possible data (as for point clouds) fill a continuous space.

For continuous data, a continuous parametrization or metric can be more suitable than a discrete classification, which practically cuts a continuous space into disjoint pieces.

Since all tables of experimental results include accuracies of at most 84% in table 1 (maximum 75% in table 2), the best and certainly publishable contribution seems to be Theorem 4.1. Can a mathematical venue be more suitable for this important result? 

To help the authors with future submissions, the key insight from ""AlphaFold2 one year on"" https://www.nature.com/articles/s41592-021-01365-3 exposes the major limitation of all brute-force predictions, not only for AlphaFold. What resources (money, people, even electricity and water) are needed not only to get the first predictions but to annually update predictions with new training data? Is it really sustainable in the long term? 


Rating:
4

Confidence:
4

REVIEW 
Summary:
A neural network that learns the filtration for persistent homology on given point cloud data is introduced, theoretically justified, and evaluated experimentally on 2 data sets.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
(S1) If this is indeed the first work that considers learning filtrations on point clouds, I find the idea very relevant.

(S2) The filtration learning approach is very nicely motivated and described (Lines 185 – 196).

Weaknesses:
(W1) The need for learned filtrations should be better motivated. Think of an example point cloud where some other learnable filtration is more meaningful than Rips or DTM, visualize all three filtrations and their PDs. For example, we know that DTM is more suitable than Rips in the presence of outliers, but when is another filtration better than Rips and DTM? This would provide guidance to readers when it would make sense to use your approach, instead of relying simply on the Rips and DTM filtration. From Table 1 results, it seems that some answers might lie in the protein data set you consider. 

(W2) [1] and [2] seem to be related work, but are not referenced?

(W3) The improvement with a learned filtration is good for protein classification, but it is for sure not convincing for the 3D CAD data (whereas it is much more complicated and computationally difficult). It would therefore be useful to provide more information about the data (including visualizations), and more detailed insights. I took a look at Appendix C, but this does not provide answers to these questions. Negative insights are also meaningful, i.e., that learning a filtration might not be useful for a lot of problems, and that relying on Rips or DTM filtration is good enough.

(W4) You write: “… the classification accuracy is better when our method was concatenated with DeepSets/PointNet compared to using DeepSets/PointNet alone. The accuracy when we combine our method and PointMLP is not higher than that of PointMLP. This would mean that concatenating the topological feature is effective when the number of parameters of a DNN-based method is relatively small.” I find this argument extremely flawed, since the number of parameters for PointNet and PointMLP is very comparable?


[1] Zhang, Simon, Soham Mukherjee, and Tamal K. Dey. ""GEFL: Extended Filtration Learning for Graph Classification."" Learning on Graphs Conference. PMLR, 2022.  
[2] Horn, Max, et al. ""Topological graph neural networks."" arXiv preprint arXiv:2102.07835 (2021).


Limitations:
Some limitations and corresponding future research directions are mentioned, but they do not address the lack of insights on when a learned filtration can be expected to be beneficial (compared to e.g. Rips and DTM filtrations).

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper investigates the extraction of global topological features using the framework of persistent homology. The authors have proposed a neural network architecture to learn the filtration weights for each point in an end-to-end and data-driven manner, which is later supported by an approximability theorem. Additionally, a two-phase training procedure is introduced to further improve the performance of the proposed architecture. The proposed framework is then applied to different tasks such as protein 3D CAD classifications. 


Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. [Originality] The filtration weight is usually constructed without considering the label information, e.g., a constant for VR complex and k-NN info for DTM. The authors proposed a framework to learn the weights for each point from the label using a neural network.
1. [Quality] Being able to provide an approximability theorem to support/motivate the choice of the neural network architecture is nice. 
1. [Clarity] Great overview on the persistent homology as well as the limitations of the current framework. The author laid out the contribution in the very beginning of the introduction which can help present the novelty of this work. Overall, a well-written paper.
1. [Significance] TDA framework has long been suffered from the topological noise issue. Being able to propose a way to automatically learn a weight function to suppress the noises is an important contribution to the field. 


Weaknesses:
1. It looks like there are two contributions in this paper: 1) propose a way to learn the weighted filtration as per Section 3.1, with theoretical guarantee in Section 4; and 2) Section 3.2 in the ability to approximation with composite function. The contribution in 1) is more related to filtration learning as it learns a weighting function, but 2) is a more general use case. However, by looking at the experimental results, it is not clear which contribution is more significant, i.e., the performance gain in Table 1 comes from 1), but the gain in Table 2 comes from 2). It would be nice to consolidate the comparisons between different experiments and provide some discussions. That way, the readers can get a better understanding of the interplay between different contributions. 
1. It is not clear to me from reading the main text on how you choose the hyper-parameters for DTM. From reading the appendix, it looks like the parameters are not chosen by cross-validation (Section C.2). Is there any specific reason to not choose this parameter end-to-end using cross-validation? How will a different $q$ and $k$ affect the prediction performance in the protein datasets?
1. It looks like Theorem 4.1 suggests the function can be approximated, but it does not mention whether it can be recovered. Will it be possible to get some results regarding the convergence (i.e., with $n \to \infty$, will the $\epsilon$ shrink?) and/or whether $\psi_1$ and $\psi_2$ can be estimated by the proposed architecture (i.e., how close $\psi_1$ and $\psi_2$ to $h$ and $\phi^{(6)}$, respectively).
1. Related to #4, can we support Theorem 4.1 by running a synthetic example? Specifically, can we show that the true weighting function $f$ can be recovered by the $f_\theta$ in Figure 2?


Limitations:
Authors have addressed the limitations of their work. Negative social impact statement is not necessary in this work, as the primary focus of this manuscript lies in its theoretical contribution.


Rating:
5

Confidence:
3

REVIEW 
Summary:
This work proposes a novel framework to obtain adaptive topological features for point clouds based on persistent homology by introducing an isometry-invariant network architecture for a weight function and proposes a way to learn a weighted filtration. The work theoretically proves that any continuous weight function can be approximated by the composite of two continuous functions which factor through a finite-dimensional space. The experiments on public datasets shows the proposed method improves the accuracy in classification tasks.

All of my questions are carefully addressed in the rebuttal. The work is theoretically solid and has potential.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The idea of learning filtration by the weight function is novel. The approximation ability theorem is important. The architecture design is based on this theorem and the isometry-invariance, which is rigorous and interpretable. The experimental results are convincing.  The paper is well written, all the concepts, math symbols, theorems are thoroughly explained. The deductions are clear and easy to follow. The experimental results are convincing.

Weaknesses:
It is not clear why different weight functions affect the qualities of the topological features, and what are the criteria for the persistent diagrams. Some theoretical explanations and numerical demonstration will be helpful.

Limitations:
The work can be further improved by use more realistic examples and compare with state-of-the-art models.

Rating:
5

Confidence:
3

";1
9B57dEeP3O;"REVIEW 
Summary:
The authors aim to generate a series of coherent images given a series of text prompts resembling a visual storybook. To do so, the authors focus on two fronts: (1) leveraging the Stable Diffusion model to generate the series of images and (2) generating a diverse dataset used to train the model on a range of styles. For generating a set of coherent images, the authors condition the Stable Diffusion on both the text prompt and a set of previously generated frames, both encoded using a frozen CLIP encoder. The text conditioning is passed to the U-Net layers via the standard cross-attention module and LoRA. To insert the image conditioning, the authors introduce a Visual Context Model resembling the standard text conditioning module. The diffusion model is then partially trained to generate images that are both consistent with the text prompt and previously generated frames. To attain images ranging in style, the authors construct a new dataset, named StorySalon, consisting of Youtube videos and E-Books. These raw storybooks are filtered and re-captioned to better align with the visual content of the storybook. Qualitative results demonstrate the ability to generate new storybooks on prompts generated by ChatGPT while quantitative results demonstrate improvements over simple baselines. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The authors focus on an important task of generating a series of coherent images that follow a given text prompt. This could potentially be useful beyond a simple storybook creation, e.g., for video generation. 
- The Visual Context Module as a means for injecting image-level details to the denoising network is simple and intuitive and could be useful in other tasks. For example, in image editing where the desired edit cannot be easily described using language.
- The visual results generated by StoryGen are impressive in comparison to the evaluated baselines.  


Weaknesses:
**Dataset Creation:**   
- I have some reservations regarding the use of the term Human Feedback in Section 3.2.3 and Section 4.2. While ChatGPT was fine-tuned to align with human preference, I believe that using ChatGPT for generating additional prompts should not be considered Human Feedback. While this is an intriguing approach, I believe replacing Human with LLM is more reflective of what is actually done here.
- Regarding the ablation study performed by the authors, it seems that “Human Feedback” leads to a quite negligible decrease in FID and I am therefore uncertain if this really contributes to the curriculum learning scheme. The authors mention that more stories can be added, but I would have expected to see a bigger improvement if this stage is truly important.

**Evaluation:**   
- There are numerous essential evaluations that are missing from the current submission. Among these, the most important is a thorough evaluation and comparison of StoryGAN, Story-DALL-E, and AR-LDM. All three have publicly available code so an evaluation is needed to understand the improvement realized by StoryGen. 
- I am not sure that FID is a particularly interesting metric here since all evaluated methods in Table 1 use Stable Diffusion to generate the images. Moreover, I do not believe that FID is a good metric when trying to measure how much the image captures a given style, as is the goal here. Maybe a CLIP-based metric using a prompt depicting a style would be more appropriate here? 
- There are numerous ablation studies that I believe are required to understand the contribution of both the proposed architecture and dataset. 
    - Architecture: 
        - An ablation study on the Visual Context Module and whether a simpler conditioning is possible (see my detailed question below). 
        - Was an ablation study performed on the BERT-like masking during the multi-frame fine-tuning? 
    - Dataset
        - The impact of the visual-language alignment stage in preparing the dataset. The authors state that directly fine-tuning on the story narrative may be detrimental, but do not validate this claim. 
- Some additional evaluations could help validate the effectiveness of the method. 
    - First, the authors claim that the method can be used to generate stories of arbitrary lengths (Line 106). It would be great to quantify this by generating stories of varying lengths and validating whether there is a loss in quality after a certain length. 
- One particularly interesting component of the method is the Visual Context Module, so I would have liked to see far more evaluations performed on it. For example, the authors mention that multiple frames can be used for conditioning by concatenating their CLIP feature. Some interesting questions that could help strengthen the importance of the component include: 
    - How much was this evaluated? 
    - How much does conditioning on more frames assist in temporal consistency? 
    - How many previous frames can be concatenated without hindering performance?
- Stating that the model achieves a significant improvement in the alternative models seems like a strong over-claiming when approximately 30 participants were used for the user study. A substantially larger pool of participants would be needed to truly quantify this improvement, especially since this is the only relevant metric used to evaluate the methods. Why are the reported FID metrics between Table 1 and Table 2 different? Is a different dataset used? I would expect only “without HF” to be different if both use the same dataset. 


Limitations:
The authors include a discussion on current limitations and potential societal impacts in the supplementary. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work proposes the model StoryGen for the task of visual storytelling. Visual storytelling is a task to generate a sequence of consistent images given a story (several sentences). StoryGen is a diffusion model taking in both image and text as conditions, and outputs an image consistent with the conditions. The training process includes pre-training for single image, finetuning for multiple image and finetuning with human feedback. On top of the StoryGen model, this work also provides a dataset, called StorySalon, which consists of 2k story books (30k well aligned text-image pairs). 

The overall structure of StoryGen model is simple. It is built upon existing well trained diffusion models and image/text encoders. To generate cartoon-like image, LoRA is adopted into the text conditioning module in a diffusion model. The author calls it the style-transfoer module. The parameters in LoRA are updated at this pre-training stage to give single cartoon image. Next is the multipe image fune-tuning. StoryGen conditions on both text and image, which is implemented by using two cross attention layers: One is noise input + text and the other is noise input + encoded previous generated image. After the second step, StoryGen if further finetuned on 100 high-quality stories. 

The author also spends some efforts to collect the StorySalon dataset. To begin with, the author downloads a huge number of videos and subtitles from online web resources with potential stories. Then give story-level description and visual level description for each story. The story level description is obtained by using dynamic time warping algorithm using subtitles. The visual level description is derived from ChatCaptioner. Finally, OCR method is applied to get potential videos captions.

The experiment section shows that StoryGen model can give consistent and story-like output images, while other methods fail.


Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
The paper is clearly written. The collected StorySalon dataset could benefit the research community. 

Weaknesses:
There is not much technical novelty, and the experimental results are limited.

Limitations:
- Since there is no human labeler involved in collecting StorySalon dataset and the total number of stories in StorySalon is only 2k, I’m concerned about its quality.

- It is also unclear from the examples provided if the results is derived by just overfitting the training set.


Rating:
6

Confidence:
5

REVIEW 
Summary:
The work focuses on the application of image generation based on a given story. Specifically, the proposed model is conditioned on the current sentence and prior generated images to ensure the story is engaging and coherent. A progressive training strategy is proposed to achieve a good model. To improve the proposed method, a new dataset is collected, while a set of human-verified generative samples are also utilized to improve the generated images. 

--------------------------
I acknowledge the author's effort in the rebuttal and have made changes to the review accordingly.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
+ This work demonstrates the possibility of generating visual storytelling images conditioned on the given stories. 
+ A three-stage curriculum training strategy is proposed to train the proposed model. However, it would be great to demonstrate the limitation of training the model with multiple-frame (i.e., without single-frame pre-training)
+ The authors collected a large-scale dataset to enable model training for storytelling purposes.

Weaknesses:
- the technical contribution of this work is limited. Most of the components are not novel and the key contributions are the way it is combined to generate a plausible output. It is unclear what the insights generated from this work that is not previously obvious to the community.
- The description of the new StorySalon dataset is limited. Specifically, it is unclear if the collected dataset has obtained legal consensus and properly handled copyright issues. 
- The work lacks a comparison with existing work, such as those introduced in line 44 and line 93-102. The two baselines in Table 1 are too naive as both are inherently limited to generate a fair comparison with the proposed method. 


Minor:
- Fig 2 should clearly state that the Image encoder only considers a single previous frame to generate the next frame.

Limitations:
The paper (in supplementary) discusses that data bias is an issue that needs to address in this domain. Collecting a larger dataset for training is the solution discussed. This may be valid considering this work is still in the early stage of the research. 

I want to point out that the discussed approach is limited as (1) it is resource-consuming, and (2) it will face the problem of copyright in order to obtain a good dataset for training. The data ownership issue may be a major hindrance.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper presents StoryGen, an auto-regressive image generator that leverages text and image conditioning. StoryGen incorporates a style transfer module integrated into the text-conditioning module, along with a visual context module. The authors also constructed a substantial dataset called StorySalon, comprising 2K storybooks and 30K text-image pairs.


Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. This paper constructs a new dataset StorySalon contains 2K storybooks and more than 30K well-aligned text-image pairs. The authors have invested significant effort into filtering the data, making it a valuable resource for advancing the field of story visualization.
2. The paper is well-written and easy to follow.

Weaknesses:
1. The illustration does not align with the description provided. In line 135, the authors state that ""StoryGen generates the current frame $\mathcal{I}_k$ by conditioning on both the current text description $\mathcal{T}_k$ and the previous frame $\mathcal{I}_{k-1}$, as illustrated in Figure 2."" However, the left figure of Figure 2 shows the image conditioned on more than one previous image, which contradicts the mentioned conditioning approach.
2. The improvement of Human feedback appears to be trivial, as indicated in Table 2. The 0.19 FID score gap could potentially be attributed to different training seeds, which raises doubts about the significance of the reported improvement. (I do not agree with the statement that 200 stories are too small since the model is trained using 2k stories overall. It appears to be sufficient for human alignment and does not require an extensive amount of data.)
3. The FID score lacks precision in the test set, particularly with only 100 storylines. It is recommended that the authors expand the test set by including more stories to provide a more accurate evaluation.
4. The baselines SDM and Prompt-SDM are too weak.  It is suggested that the authors compare StoryGen with finetuned or LoRA-finetuned SDM models using the same training settings to establish a more robust baseline for comparison.
5. The auto-regressive generation approach employed by StoryGen has already been proposed by AR-LDM. Consequently, the architecture design itself lacks novelty.
6. StoryGen is only conditioned on one previous image and does not utilize the corresponding caption of the previous image. In the depicted cases of Figure 1 and Figure 4, there is only one main recurring character. If multiple characters were present, StoryGen may struggle to ground the characters in the previous images. Furthermore, if a character does not exist in the previous image, StoryGen may face difficulties in maintaining consistency between frames.
7. The language understanding capacity of StoryGen appears to be weak. For instance, in the second case of Figure 4, the rabbit appears small in the fourth frame, whereas it should be as big as it is in the fifth frame. Additionally, in the sixth frame, the boy's hair does not become brighter as described in the caption. Moreover, in the seventh frame, multiple other boys are depicted with the same yellow hair, which contradicts the previous story setting. This limitation may stem from StoryGen solely relying on the only one previous frame $\mathcal{I}_{k-1}$ and not incorporating previous captions into its generation process.

Limitations:
1. The author should provide examples of the constructed dataset showcasing different visual styles and character appearances.
2. The StorySalon dataset consists of 2K storybooks and over 30K well-aligned text-image pairs, which is smaller compared to datasets such as FlintstonesSV (24K stories and 123K image-caption pairs), PororoSV (14K stories and 74K image-caption pairs), and VIST (27K stories and 136K image-caption pairs). Despite the authors' claim that StoryGen can perform open-ended story generation, it remains unclear whether StoryGen can generate stories involving more complex scenarios with unusual entities.
3. Apart from human feedback, the authors have not conducted any other ablation studies to evaluate the effectiveness of their proposed techniques, such as word dropout and curriculum learning.
4. There are concerns regarding the legality of using web-crawled e-books. The authors should provide additional information about the sources of the e-books and clarify whether proper copyright guidelines were followed.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper propose an approach for fine-tuning diffusion models for the task of story generation, where a model must generate frames for sentences in a story. To do so, they propose adding adaptors conditioned on both images and text into a pre-trained stable diffusion UNet. The authors also introduce a scraped dataset of 2k stories with 30k image-text pairs, which serves as the data foundation for their fine-tuning.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
S1. The dataset of story text and images is a significant contribution, which can be useful for future work on visual storytelling.

S2. The paper is generally well framed and motivated. Writing and presentation is in general strong and polished.

S3. Simplicity of the method. The method uses off-the-shelf components and algorithms (LoRA, cross-attention, etc.) to enable new capabilities. I see this simplicity as a strength not a weakness. 

S4. Inclusion of human evaluation is a strength.

Weaknesses:
W1. Presentation. Figure 2 suggests that StoryGen is conditioned on all past frames. However, in reality StoryGen is conditioned on the most recent frame.

W2. Results. It appears that the model is not able to preserve style and content as well as say DreamBooth. For example the stripes on the shirt in Figure 4 are not preserved. 

W3. Evaluation. It seems that the StoryGen model without human feedback model was not evaluated in the human evaluation (Table 1). Given that the ablation in Table 2 shows similar FID scores for StoryGen with and without human feedback, it is not clear if this step is really necessary. 

W4. Lack of baselines. StoryGen is specifically fine-tuned for the desired task, while stable diffusion is not. Given this, it is perhaps not surprising hat StoryGen greatly outperforms stable diffusion. Can some other baselines, perhaps based on the DreamBooth (with the subject in the first image representing the special [V] token) be used for a stronger baseline? This is just one idea; however, comparing against some prior work may help to elucidate the strength of the method.

W5. Evaluation. Fine-tuning can sometimes hurt the generality of a model. How well are individual frames generated relative to the base SD model? Is it possible to do a human evaluation here or compute CLIP scores? Ideally, the story would be cohesive (frame-to-frame consistency) without degradation in quality of each frame.

Limitations:
The paper does not directly address limitations in the main paper, which is an additional weakness. I suggest discussing failure cases or otherwise conducting a failure analysis.

Rating:
5

Confidence:
4

";0
dw6xO1Nbk5;"REVIEW 
Summary:
This paper studies the generalization error of neural operators that contain kernel operations. Under the basic setting of neural operators (such as FNO), this paper establishes upper bounds of the excess risk of neural operators. How to apply NO to irregular domains, and the error analysis of super resolution are also discussed. The techniques of this paper is rather standard. Some parts of the upper bounds are not clear. I think the contribution of this work is incremental.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. This paper provides an upper bound for the excess risk of neural operators.

2. The upper bound in this paper improves the one in [14]

3. Extension of NOs to irregular domains is discussed.

4. The super resolution error is studied.

Weaknesses:
1. The technique used in this paper is pretty standard. The author should emphasis their novelties. 

2. Theorems in this paper is not impressive and is unclear. For example, the error bound in Theorem 3.1 depends on the network structure, parameters, and the covering number of the space $B$, which is infinite dimensional. If the norms of network parameters are all larger than 1, the upper bound can be very large. Since the space $B$ is infinite dimensional, the covering number can also be very large, which makes the result less attractive. On the other hand, the magnitude of the training loss is also unclear. I believe the training loss depends on the network's width and depth, which relates closely to the upper bound in this paper. It will make the paper much stronger if these relations are analyzed clearly and a more clear upper bound is derived.

3. For applying NOs to problems with irregular domains, the authors only give an example on unbounded domain. The case for arbitrary irregular domains is only briefly discussed. However, the authors claim the construction of NOs on arbitrary domains as the second contribution. More details on this part should be given.

Limitations:
Limitations are not disucssed in this paper.

Rating:
4

Confidence:
3

REVIEW 
Summary:
A theoretical analysis of neural operators (NOs) is presented, which provides further insight into the construction and performance of NOs.  The theoretical insights are validated by numerical experiments.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
A thorough theoretical analysis of NOs is presented, with significant improvements in the tightness of generalization bounds compared to prior work.  The implications of the theory and insights that may be drawn are discussed.  While some of these may seem obvious, it is important to base such insight on robust underlying theory, as presented.  The insights gained from the theory are validated by numerical experiments.

Weaknesses:
The summary of the overall model is not the most clear (Equation 1), which suggests the non-linear activation is applied first, rather than following the kernel and linear transforms.

Typo: ""project"" -> ""projection""

Typo: sometimes $\mathcal{L}$ used to represent loss, sometimes $L$.

Typo: ""Conbining"" -> ""Combining""

Limitations:
No special societal concerns.

Rating:
7

Confidence:
2

REVIEW 
Summary:
The image super-resolution (SR) is a recurrently used task, nowadays, since the SR images can improve the accuracy of downstream tasks like object detection. Many proposals rely on heavy Deep Learning models or lightweight models based on efficiently designed architectures. This work studies neural operators based on examining the orthogonal base. The operations proposed in the manuscript, according to their theorems and demonstrations with the appropriate orthogonal bases and the grip points, reduce the time of convergence and make the network adapt faster to the  irregular domains.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* The proposal section in the manuscript  is easy to read and follow.
* Theorem 3.2 and 3.3 are properly defined. For theorem 3.3, this will assist in re-planning new proposals for improving image SR accuracy.

Weaknesses:
Despite a short evaluation performed that shows a better performance of the proposal, I feel it is not sufficient to validate the generalization ability in the super-resolution task. To this end is necessary to validate with the standard evaluation metrics used in the SR domain. Personally, I feel that more explanation and motivation is needed for equation 3.

Minor error:
The machine learning articles, mostly, are organized Introduction, related works, the proposal, material, and experiments. It should be convenient to use this structure in the manuscript.

Limitations:
There are no limitations addressed in the manuscript

Rating:
5

Confidence:
3

REVIEW 
Summary:
In this paper, the authors propose to analyze neural operators from the orthogonal bases in the kernel operators, which helps to guide designing kernel operators and choosing grid points, analyzing generalization and super-resolution capabilities, and adapting neural operators to irregular domains.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This is an interesting paper providing not specific models but new analysis perspectives and design principles on Neural Operators.


Weaknesses:
- I feel there are still gaps between the overall claims, the theoretical results in Section 3, and the implication and experiments in Sections 4&5. For example, the ""NOs on Unbounded Domain"" section is not what I would expect for ""adapting neural operators to irregular domains""; the ""Combining Multiple Bases"" section seems more like analyzing kernel operators, instead of providing insight and principles in ""designing kernel operators"".
- The writing is not always good. For example, in the abstract, similar contents reappear three times.


Limitations:
This work is a bit beyond my capability. I think it is an interesting and important work, but I don't feel like it has reached its perfect state.


Rating:
5

Confidence:
2

REVIEW 
Summary:
The authors consider a new perspective to neural operators, by examining the role of orthogonal bases. The kernel operators are constructed such that their eigenfunctions are predefined orthogonal bases, with the eingenvalues as trainable parameters. That is, a neural operator can be seen as a mapping from input coefficients to output coefficients of the orthogonal basis functions. The authors show several theoretical results using this new perspective, backed by empirical results:
- Improved generalization bounds.
- Super-resolution error bounds.
-  Irregular domains - they show that neural operators can be extended to irregular domains using random Fourier features and polynomials on irregular domains.
- Choosing other orthogonal bases. The authors show that Fourier bases can be combined with wavelet or polynomial bases.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
This paper can be impactful both because of the four concrete results they show (generalization bounds, super resolution bounds, irregular domains, and choice of orthogonal bases), but also because of the novel perspective of neural operators. In particular, the novel eigenvalue / orthogonal basis-based perspective of neural operators can be a useful view for studying neural operators in the future, both theoretically and empirically.

While several other works have studied neural operators for irregular domains / topologies, the other results by the authors are much more novel. Not much prior research has studied generalization bounds, super resolution bounds, or combining bases before this work.

Weaknesses:
- Some of the results are fairly obvious or have only limited empirical value. For example, the super-resolution theorem/experiments, although it is novel to study super resolution, the main punchline is that super-resolution depends on the accuracy of the integration method, and the density of the low-resolution grid (i.e., the results in Figure 2), which is not so surprising.
- Section 4 shows implications of the theory, motivating the experiments in the next section. However, some of these experiments have only tangential ties to the theory. For example, “Guiding the choice of Orthogonal Basis” is justified because Theorem 3.1 impacts their expressiveness. I would argue that we would decide to research the choice of orthogonal basis even if we didn’t see Theorem 3.1 first.
- It would be nice if there was a bit more intuition for each of the proofs, in the main text.
- Did not discuss limitations.
- The authors could have released the code (anonymously during submission).

Limitations:
The authors did not discuss the limitations of their work, and they answered “no” without explanation in the OpenReview paper checklist to that question. The NeurIPS call for papers states that authors can answer no to a checklist question, provided they give a good explanation. But I cannot think of any explanation why an author would be justified in not describing the limitations of their work; I think it is strictly better to do so.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper attempts to provide theoretical analyses of Neural Operators (NOs) mainly considering the following aspects: 1. The generalization bound of NOs; 2. The discretization error of NOs; 3. The ""super-resolution"" (trained on low-resolution grid then evaluate on the high-resolution grid) error of NOs on the uniform grids. Several numerical experiments are carried out to validate the theory. The theory results also motivate the authors to come up with improved designs on existing NOs, including bases/integration schemes that suit specific PDE better, and generalization to unbounded domains

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* A tighter and more general generalization bound compared to prior works;
* Advance in the understanding of discretization errors and super-resolution errors in NOs;
* The improved design of NOs is validated by extensive numerical experiments.


The paper in general has analyzed several crucial properties of NOs, which is timely, and its practical implication I believe will benefit the scientific ML community. Specifically given that the concept of doing super-resolution with NOs is often vaguely studied in many relevant works.

Weaknesses:
The overall presentation of the paper is fairly clear and easy to follow. However, some of the discussion in the experiment section is relatively vague, especially in section 6.3. As revealed by Theorem 3.3, the integration scheme along with basis selection has a crucial effect on the super-resolution error. The authors only briefly covered what basis and quadrature rule are used for the harmonic oscillator example, but then skim through the 3D DFT experiment, which makes it difficult to interpret the improvement in the results shown in Table 1. These details might be trivial for someone who is an expert in DFT, but they can help other audiences better understand the practical implication of the theorems.

Limitations:
The paper did not discuss any limitation. While this is somewhat understandable as a theory paper, the authors can discuss the limitation of their theory results in terms of the application scope and its assumption.

Rating:
5

Confidence:
4

";0
HKueO74ZTB;"REVIEW 
Summary:
The paper introduces a novel data augmentation method called MultiMix, which aims to go beyond the limitations of existing mixup methods. The authors propose interpolating an arbitrarily large number of examples beyond the mini-batch size in the embedding space, rather than along linear segments between pairs of examples. They also extend the method to sequence data with Dense MultiMix, which densely interpolates features and target labels at each spatial location. The paper demonstrates that their proposed solutions outperform state-of-the-art mixup methods on various benchmarks.

*Contribution and novelty* The scientific contribution of the paper lies in the introduction of MultiMix and Dense MultiMix, which address the limitations of existing mixup methods. The authors propose novel approaches for generating an arbitrary number of interpolated examples beyond the mini-batch size, both in the embedding space and for sequence data. The empirical results, as described in the additional explanations, validate the effectiveness of these methods. Overall, this paper seems to improve the research frontier on data augmentation, but empirically and gives some new intuitive understanding of their results as their method leads to a more uniform distribution of embeddings within each class.  I do not see any clear weaknesses in the methods that may challenge the main claim of this work, but I have raised some issues in the questions section. Finally, since I have not read some of the papers that are cited in this work or other related papers in the literature, I defer the judgment about novelty of the ideas presented here to other reviewers.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- Novelty: The paper introduces a new data augmentation method, MultiMix, that extends the concept of mixup beyond the limitations of existing methods. The idea of interpolating in the embedding space and densely interpolating features and labels in sequence data is original and contributes to the field of data augmentation in machine learning.

- Empirical Results: The authors provide empirical evidence to support the effectiveness of their proposed solutions. They demonstrate significant improvements over state-of-the-art mixup methods on multiple benchmarks. The results validate the scientific contribution of the work and highlight its practical relevance.

- High quality of presentation of results: the presentation of the results follows a simple and logical flow, with main contributions and their difference to the previsous works clearly stated. The Methods, notations,  & results are all clearly stated and easy to understand for the reader. 

- Intuitions about results: The paper provides some justifications & intuitions about  why their approach works. They do so by analyzing the embedding space. The authors show that the classes become more tightly clustered and uniformly spread over the embedding space, explaining the improved behavior of their solutions. This analysis adds depth to the scientific contribution of the work.

Weaknesses:
Since I have not read some of the papers that are cited in this work or other related papers in the literature, I defer the judgment about novelty of the ideas presented here to other reviewers. Besides that, I have raised a few issues in the questions section as they are not direct criticism of this work.

Limitations:
yes

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper considers a generalized form of Manifold mixup by expanding the concept of a pair-wise interpolation method to the multi-sample-based interpolation on the entire convex hull. The proposed methods called MultiMix and Dense MultiMix show the best performance over prior mixup-based methods for the various classification benchmarks. Also, they are empirically shown to have better robustness than prior methods. 


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
$\textbf{Strength 1:}$ The main strength of this paper is the suggestion of the general form of the prior mixup-based methods to cover the entire convex hull which can be formed by a given set of training samples. Because the power of the mixup methods originated from the interpolation of the space, i.e., either input or embedding, in-between different samples, expanding the concept of mixup into the entire convex hull of the given samples are clear and convincing direction.

$\textbf{Strength 2:}$ The paper is well-written and easy to understand. To be specific, Fig. 1 is simple yet effective to understand the main strength of the proposed method. Also, the formulations in the paper are quite neat and well-organized.

$\textbf{Strength 3:}$ The experiments are done for various kinds of architectures ranging from the ResNet family to ViT, and benchmarks including CIFAR and ImageNet cases.


Weaknesses:
$\textbf{Weakness 1:}$ The most crucial weakness of this paper is the novelty of the proposed method. Specifically, I admit that the expansion of the sample interpolation method to the generalized form, i.e., MultiMix, is convincing and the authors show a well-organized formulation of it, but I also feel that the expansion of the concept is quite easy to be anticipated from the prior Mixup methods. For example, in the original Mixup paper, the authors already tried to utilize multiple samples by weighting them with a simplex coefficient in the input space. Although it is reported not to show a further gain, it is hard to find a noticeable difference between the `trial' of the original Mixup paper and the proposed method except for the target space, i.e., input for Mixup vs. embedding space for MultiMix. When focusing on the difference, i.e., mixing on the embedding space, it is natural to ask for the reason why the multi-sample-based mixup becomes effective in the embedding space rather than the input space. However, I cannot find a deep insight into the phenomenon.

$\textbf{Weakness 2:}$ A more precise comparison in the Related Work part would be beneficial to highlight the significance of the proposed method. To be specific, in Related Work, a bunch of works are grouped into their attributes and briefly mentioned. The authors drop the detailed explanation of the prior works with a suggested survey paper but I am sure that the authors must provide a compact but clear explanation of the related works. As a simple suggestion, mentioning the names of prior mixup methods along with short descriptions would make it more clear. When a set of important baselines including AlignMix, Co-Mixup, PuzzleMix, and $\zeta$-Mixup are called with names in the Related Work part, readers can easily match the name of the priors appeared in Experiments and the descriptions in Related Work.

$\textbf{Weakness 3:}$ The metrics in 4.4 Quantitative analysis, i.e., 'Alignment' and 'Uniformity' do not fully represent the quality of the embedding space. These measurements can evaluate the similarity of embedded features of intra-class samples but do not capture the separation between different classes, i.e., the separability of inter-class distributions. 

$textbf{Weakness 4:}$ Manifold intrusion is the main challenge of mixup-based methods. However, I cannot find any analysis for the effect of manifold intrusion of MultiMix. The experimental gains empirically show that the effect of manifold intrusion is minimal in practice. However, further discussion is needed on the effect of manifold intrusion of the multi-sample-based mixup approaches. I wonder if the chance of the intrusion might be higher than the pair-wise mixup of prior works. 

Limitations:
Weakness 1 and Question 1 are for pointing out the limitation of the proposed work in view of novelty.

Also, I provide a suggestion for improving Related Work in Weakness 2.

As better metrics for quantitative analysis in section 4.4, I provide the modified measurements in Weakness 3 and Question 2.

For emphasizing the computational efficiency of MultiMix and Dense MultiMix, providing the computational costs would be beneficial (as aforementioned in Question 3).

For the manifold intrusion issue, I described the limitation and the corresponding question in Weakness 4 and Question 4.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper aims to increase the performance and robustness of deep learning model by increasing sample diversity. The authors propose data augmentation techniques which interpolate samples from a convex hull. The experimental evaluations show that the algorithm outperforms existing techniques.

**I have read the rebuttals. Please see the comments below for the discussion.**

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. Intuitive idea: the use of convex hull is natural and simple.
2. Applicability: the algorithm can be generally applied in many settings.

Weaknesses:
1. Presentation: the presentation of the paper needs significant improvement. For example, the abstract should succinctly summarizes the key ideas of the paper.
2. Novelty: sampling from a convex hull is not novel. The use of Dirichlet distribution has also been done in many other works. Could you please compare your work with the work by Shu et al. ?





[1] Shu, Y., Cao, Z., Wang, C., Wang, J., & Long, M. (2021). Open domain generalization with domain-augmented meta-learning. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 9624-9633).

Limitations:
The authors discuss the limitation of the algorithm in the concluding paragraph.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The authors introduce MultiMix, an approach aimed at enhancing the Mixup class of techniques, which involve interpolating between input examples and their corresponding targets. This enhancement is achieved through the interpolation of a substantially more training examples, increased number of loss terms for each minibatch, and a more flexible interpolation factors. Further, the authors expand their proposed method with DenseMultiMix, a variation capable of interpolating examples in sequential data. Empirical evidence indicates that the proposed MultiMix and DenseMultiMix not only boost accuracy but also bolster the robustness metric across a diverse set of tasks, including but not limited to classification and object detection.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
There's a noticeable diminishing return when interpolating additional input examples in Mixup. This presents a limitation to the advancement of Mixup-related methodologies, which focus on interpolating more training examples. However, the approach introduced in this paper appears to overcome this limitation by implementing interpolation in the embedding space, as opposed to the input space, evidenced by Fig 4c. I regard this as a significant contribution of this paper.

Furthermore, the adaptation of Mixup to sequential data like text has always been frustrated. There are three primary obstacles: 1. The blending of text tokens doesn't carry the same significance as mixed images. 2. How to interpolate along the temporal dimension or the spatial dimension. 3. Discrepancies in length among examples within a mini-batch. The introduced MultiMixup provides solutions to at least the first two challenges. Interpolating within the embedding space prevents the creation of nonsensical input tokens, while MultiMixup along with attention delivers an effective means of interpolating representations in the spatial dimension.

Weaknesses:
There are several weaknesses in this manualscript, as explaint in the following.

1. The motivation for the suggested method is insufficiently compelling within the manuscript. The only statement regarding to the motivation is ```In this work, we argue that a data augmentation process should increase the data seen by the model, or at least by its last few layers, as much as possible. ```

    This reasoning seems to be superficial as it fails to distinguish itself from the preceding data augmentation methods. Moreover, it doesn't elaborate on how the proposed approach enhances accuracy and robustness. More explicitly, while it's true that any data augmentation method would increase the volume of data processed by the model, not all such methods that do so effectively improve the model performance. Thus, the motivation fails to pinpoint the core of the research problem.

2. Regarding the empirical evaluation, it would be appropriate for the authors to include comparison with methodologies such as AugMix [1]. Similar to the proposed method, AugMix enhances the volume of data points processed by the model through interpolating examples derived from varying combinations of image pre-processors using the Dirichlet distribution. Moreover, it offers a substantial gain in both accuracy and robustness metrics. Also, the divergence loss could potentially counter the scarcity of dense labels.

     [1]: Hendrycks, Dan et al. “AugMix: A Simple Data Processing Method to Improve Robustness and Uncertainty.” ICLR 2020.

3. The performance improvement over the established baseline is rather modest. In such a scenario, it would be beneficial if the manuscript offered more profound insights to the community to better understand Mixup-oriented techniques. Such insights could include how the proposed method is connected to existing methods like AugMix, as well as an exploration of why the proposed methodology shows greater promise. For instance, while Section 4.4 and Figure 6 in the appendix provide useful information, they lack a comparison between the proposed method and existing methods, as well as a comparison among different variants of MultiMix as done in Section 4.5. Empirical insights of this would offer more compelling evidence that the proposed methodology is a valuable contribution to the community.

4. A less significant shortcoming pertains to the scope of the empirical evaluation. The batch size is held to be 128 in the domain of image classification. Understanding the robustness of the proposed method to batch size selection is crucial, especially as we move towards models with more # parameters where batch size tends to decrease. Specifically, when the batch size is small, MultiMix should offer more advantages over standard Mixup due to its ability to draw from a larger pool of interpolated examples. Another dimension worth exploring is the application of DenseMultiMix in the realm of text data. The manuscript would significantly benefit from showing evidence that the proposed method can address some of the challenges of extending Mixup-style techniques to the domain of text. I understand that interpolating dense text labels is a non-trivial task, preliminary studies on tasks such as text classification could suffice.

Limitations:
One limitation as mentioned above is the application of DenseMultiMix in the realm of text data.

Rating:
6

Confidence:
3

";1
IfRHdBy3wb;"REVIEW 
Summary:
This paper proposes a novel local-level post-hoc GNN explainer (GOAt) that takes the issue of discriminability and consistency into consideration.  The discriminability and consistency of graph explanation are significant and meaningful for the community’s understanding of the GNN decision-process. The GOAt explainer attributes the prediction of the trained GNN to the input graph features, i.e., the node feature matrix and the adjacency matrix, based on the Equal Contribution technique. The authors implement the specific GOAt with regard to GCN, GraphSAGE, and GIN. Sufficient experiments, including both qualitative and quantitative evaluation, verify that GOAt outperforms the SOTA GNN explainers in terms of both fidelity, discriminability, and stability.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
S1: The issue of discriminability and consistency (i.e., stability) in GNN explanation methods is thought-provoking. However, it is neglected by previous methods. The GOAt explainer bridges the research gap.
S2: The mathematical derivation of the equal contribution technique is solid and ingenious, which well support the design of GOAt explainer.
S3: The experiments to verify the superiority of GOAt are sufficient and the results are discussed in detail.

Weaknesses:
W1: My main concern is about the effectiveness of the attribution strategy based on equal contribution. Though the experimental results have shown it outperformance, the intuitive reason why equal contribution can improve the discriminability and stability of GOAt explanation is still obscure for me. In other words, the motivation behind GOAt is not well-illustrated.
W2: The organization and writing of this manuscript need to be improved. Some contents are a little confusing. For example, in line 155, the authors claim that the number of occurrences is not necessarily equal to the number of unique variables. However, the example following does not explain this claim well.

Limitations:
The authors have discussed some limitation about GOAt, including the requirement of expert knowledge, the failure in explaining deep models (e.g., Transformer). In my regard, as a post-hoc explainer, GOAt has to access the detailed architecture of the GNN model to be explained, which may serve as the main limitation in real-world application.

Rating:
6

Confidence:
5

REVIEW 
Summary:
In this paper, the local explanation algorithm for Graph Neural Networks (GNN), Graph Output Attribution (GOAt), is proposed. The key idea of the algorithm is decomposing the GNN's forward propagation as sum of product form. After decomposing, authors compute the contributions of nodes and edges by proposed measure. Next, (sum of the contribution including the interest node or edge) - (contribution from global bias) becomes the contribution (explanation) of the node or edge. The experiments show (1) the explanation can capture the importance of edges (fidelity, discriminability) (2) the explanation is consistence (Stability).

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The algorithm is general for graph neural networks.
2. The examples help the reader understand the explanations.
3. The experiments show the performance of proposed model with regard to baselines for various criteria.

Weaknesses:
1. The paper used zero-vector as base manifold. It makes that the contributions for all variables are always equal.
2. The contribution does not consider the value of variables.
3. It would be better to explain the difference between the baseline algorithms and proposed algorithm.

Limitations:
The paper used zero-vector as base manifold. It makes that the contributions for all variables are always equal. Also, the proposed contribution consider absence or presence of variable only without the value of variable.

Rating:
5

Confidence:
3

REVIEW 
Summary:
It proposes a local GNN explanation method called GOAt by analysing the output attribution of graphs. Specifically, it proposes a novel explanation framework by the notion of equation contribution, which is transparent compared to the existing black-box type explanation models. Experiments on real and simulation data are tested on several metrics, where the results are convincing.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The point of equation contribution is quite novel and interesting. Especially its transparency compared to current baselines.
2. The experiments are extensive and convincing.

Weaknesses:
1. In line 28, it seems that ""their effectiveness relies heavily on the quality of local-level explanations"" is the main point of abandoning the global style in this paper, could you explain more about this in the rebuttal phase or say more about it in section 2 - see second con below.
2. In section 2, the differences between global and local explanatory methods should be introduced in a proper way to show the superiority of global explanations. Also, please consider including the matrices in lines 66-69 as another subsection to introduce the matrices together.

Limitations:
The authors presented the limitations in the Appendix about wider range of model architectures. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
Unlike existing methods that rely on training auxiliary models, the paper introduces a computationally efficient instance-level GNN explanation technique (called GOAt), which enables attribution of GNN output to input features by direct algebraically computation. More specifically, a GNN is represented in an expansion form as a sum of scalar product term, involving input graph features, model parameters, and activation levels. Based on the assumption that all scalar variables in a scalar product term contribute equally to the term, each product term can be attributed to its corresponding factors and thus to input features, which gives the importance of each node or edge feature in the input graph to GNN outputs. Besides that, the paper also introduce two new metrics called discriminability and stability, which assess the ability of explanations to distinguish between classes and the ability to generate consistent explanations across similar data instances, respectively. Through extensive experiments on synthetic and real data, the paper show that GOAt has consistently outperformed various state-of-the-art GNN explainers in terms of fidelity, discriminability, and stability.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1.	The assumption that all scalar variables in a scalar product term contribute equally to the term is acceptable to me. And the derived lemmas and theorems are well proved and practically applicable.
2.	The proposed explanation method is computationally efficient, and does not rely on back-propagation with gradients, hyper-parameters or training complex black-box models, which is novel to me.
3.	The work contributes a new attribution method to the field of explaining GNNs (might extend to other types of neural networks, e.g., CNN) .


Weaknesses:
1.	The specific calculation expression of the attribution process needs to be carefully derived with expert knowledge for different neural network architectures and becomes very complex as the network goes deeper, which greatly limits its widespread use, though it shows superior explanation ability.
2.	The main argument that existing methods struggle with issues such as overfitting to noise, insufficient discriminability, and inconsistent explanations across data samples of the same class is not well elaborated. Though the experiments seem to have verified this argument, it will be more motivated if some detailed illustrations (e.g., by presenting a toy example) could be provided in the introduction part.


Limitations:
a)	Build an algorithm to automatically derive the attribution expressions based on the network architecture.
b)	Although the experiments seem to corroborate the main argument, bolstering the introduction with some detailed illustrations (for instance, by incorporating a simple, illustrative example) could make the paper more compelling.


Rating:
5

Confidence:
4

REVIEW 
Summary:
This work studies the problem of GNN local explanation.  It points out that existing GNN explainers are suffering from a few limitations including insufficient discriminability, inconsistency on same-class data samples, and overfitting to noise, and the aim to address these limitations by proposing the GOAt method. Given a pre-trained GNN model, GOAt defines a contribution score on each node feature and each edge feature (i.e. values on each edge) by expanding the GNN as a sum of scalar products involving these features, and thus would be able to find the edges that contribute more to the GNN's output. The authors evaluate GOAt with the frequently used fidelity metric, as well as the newly proposed discriminability and stability metric in this paper.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. Results are impressive.
2. Code is provided in the supplementary. 
3. The newly introduced discriminability and stability metric for explanation evaluation makes sense to me.

Weaknesses:
1. The writing for the method section is not easy to follow. 
2. The motivation of why the contribution should be designed in this way is not clearly explained. Why the current design can address the insufficient discriminability, inconsistency on same-class data samples, and overfitting to noise issues also remains unclear to me.

Some minor weaknesses:
1. There seems to be one typo in the appendix proof lemma 2: line 11 should be X_j not E_j?
2. The term ``feature`` is used to describe all X_{ij}s and A_{ij}s, which looks a little bit confusing for me. I think usually people use the term ``feature`` to describe the attributes on nodes and edges, but here A_{ij} is more like a weight scaler on edge instead of the edge attributes? I feel it would be clearer to put a note to clarify that the term ``feature`` is used to describe all node attributes and edge weights.
3. It would be clearer if the authors can explain what does each lemma and theorem imply.

Limitations:
Please refer to ``weaknesses``.

Rating:
5

Confidence:
3

";0
Pe9WxkN8Ff;"REVIEW 
Summary:
_Background_: RASP is a tensor processing language which provides a language to hand-write transformers. Tracr presents a framework to compile RASP programs to a transformer architecture automatically. 

The authors of this project are interested in distilling a transformer architecture to an equivalent program in RASP. This is challenging because RASP is not a sound language: the behavior of many trained transformers is equivalent to that of a single RASP program. The authors go around this problem by constraining the transformer architecture in strategic places so that there is a deterministic mapping from a transformer to an equivalent program in RASP. The authors present a model distillation procedure to construct such reduced transformers (called Transformer Programs) automatically. The authors test the Transformer Programs on three tasks: 1) in-context learning, 2) recovering handwritten RASP programs, and 3) simple real-world NLP tasks. They observe mixed results: transformer programs can fully solve (1), they cannot generalize to larger sequences for (2), and they are better than non-sequential baselines on (3) but worse than generic transformers. The authors also show the utility and interpretability of the distilled programs by analyzing the programs using off-the-shelf debuggers.

Soundness:
2

Presentation:
4

Contribution:
3

Strengths:
Originality
* RASP programs are inherently unsound: one RASP program is equivalent to many trained transformers. Solving this problem by constraining the transformer to elicit a bijective mapping is a simple yet elegant idea. 

Significance
* This paper shows a simple method to distill transformers into interpretable programs. This work opens the door for work in program synthesis, static analysis, and model checking to be used for sequential processing tasks.
* Many downstream tasks rely on transformers to processing temporal sequences of data into labels. While the transformer architecture presented is constrained, an interpretable transformer can be very useful for tasks where high degree of interpretability and accuracy are required. 

Clarity and Quality
* The framework is well motivated, and the writing is clear and precise.

Weaknesses:
__Weaknesses__

I have three main concerns (paraphrased from the questions section below):
* I’m concerned that the constraints mentioned here violate properties[1] that the mechanistic interpretability community have identified as being intrinsic to the utility of transformers.
* Poor robustness: Model distillation has been observed to produce models that are more robust to outliers while being slightly less accurate[2]. However, in experiments it seems that the distilled models are neither robust to outliers nor as accurate as baselines, which makes it harder to use this as a drop-in replacement.
* Lack of Reusability: An interesting property of the RASP programs (and RASP distilled transformers) is that many tasks can be efficiently solved by precomputing useful primitives. The algorithm presented here does not seem to model this property. For instance, in the reverse proposed in RASP, the authors' solution precomputes length of the sequence and then uses length to flip the indices of an identity matrix. The derived transformer also captures this behavior. Does the Transformer Program also capture this behavior? If not, why not? 

Clarity:
* 135. subpace -> subspace
* Related work: I defer to the authors, however, I feel adding a section on other model distillation approaches would be relevant. In the context of model distillation, [3] seems similar to this work in learning a transformer (for multi-agent communication) and then by discretizing it (using MCMC) to obtain an interpretable policy.

Overall, I've given the paper a __borderline rejection__. The problem statement presented has promise to have significant impact, however, the experimental results do not instill confidence in the downstream utility of this method and I believe this paper will benefit from another round of review. I'm more than willing to change my recommendation after the rebuttal period.

====
__EDIT__: I misinterpreted the paper as a _model distillation_ paper instead of a continuous relaxation one. In light of this, and other clarifications presented by the authors, I'm raising my score to a __weak accept__.  

1. https://transformer-circuits.pub/2022/toy_model/index.html
2. https://arxiv.org/abs/2006.11287
3. https://arxiv.org/abs/2101.03238



Limitations:
The authors have addressed the limitations of the method adequately to the best of my knowledge.

Rating:
6

Confidence:
2

REVIEW 
Summary:
There has been prior work on compiling programs written in a domain-specific language (RASP) into a transformer that emulates the function. In this paper, the authors target the opposite direction of training transformers and generating RASP programs that are faithful to the model's computation. The authors do this by proposing an architecture that serves as a discretized version of a transformer, allowing translation to a discrete, interpretable python program after training. They find that for a variety of tasks, it is possible to 1) learn a solution in this architecture and 2) compile it into RASP.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
This work demonstrates a novel architecture that allows for direct translation to code. The paper does a great job of demonstrating their method for synthetic tasks. Though prior work has noticed sparsity improved interpretability, this work leverages it for full programmatic translation. I hope this can inspire future work in interpretable neural networks.

Weaknesses:
1. Learning solutions to the RASP tasks makes it easier for the model to learn interpretable solutions. The paper would be stronger if it targeted other tasks where the compilation is more surprising.
2. How well can a discrete transformer solve problems relative to the original transformer? I imagine the discretized model has a lower representation capacity or worse optimization. I think it's important to experimentally/theoretically analyze what is lost as a result of this architectural choice, considering it's not a standard transformer in many key aspects.


Limitations:
Are there tasks where you tested this method and either the code was uninterpretable or the discretized transformer couldn't achieve high accuracy, unlike the standard transformer? If not, are there any tasks where you wouldn't expect this to work?

Other than this, the authors have adequately addressed limitations.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper presents an approach to constraint the parameter space of Transformers so that when trained, the resulting weights can be directly translated to a human-readable program, e.g. in Python. To do so, attention is modified so that the internal model representation is, in practice, composed of a set of distinct variables that the attention processes read/write to. The base approach focuses in attention-only models (no MLP layers), but the authors propose extensions to support MLP layers.

- page 5: line 165-166: ""For row in the predicate matrix"" -> there seems to be some word missing around ""row"".


Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
I found the idea of constraining the parameter space extremely interesting. While the types of programs that can currently be learned seem a bit restricted (e.g., as the number of variables grows, the model/program could potentially grow very large), I think this is a very promising direction, and a greag foundation to build on.


Weaknesses:
While it seems clear that programs for simple tasks can be interpreted, it is not clear that many tasks (specially vision/natural language tasks) actually even afford interpretable algorithms. So, perhaps future work needs to focus on more complex primitives for the programming language supported by the models that would simplify the programs, or enable higher-level constructs that are interpretable.


Limitations:
See weaknesses above.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper proposes a new technique for training Transformer models while ensuring that the learned weights obey certain constraints, designed to facilitate mechanistic interpretability. In particular, the technique works by (1) freezing certain weight matrices throughout training (e.g., to always write the output of an attention layer to a previously unused linear subspace of the residual stream) and (2) introducing Gumbel-softmax sampling at various steps of the computation, and annealing the temperature down during training to ensure that at convergence, those computations perform discrete operations. The resulting Transformers can be automatically converted into symbolic programs that read, write, and perform lookup-table-style computation with a fixed number of categorical and integer variables. The authors train (quite small) Transformers in this way, to solve several tasks, including named entity recognition and sentiment classification on short sequences. The authors also provide some analysis of the programs extracted from these Transformers.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This is a neat paper that advances two exciting lines of research:

- RASP and Tracr been helpful as intuition pumps for how trained Transformers *could* work, but it has not been clear whether the (correct) RASP algorithms researchers manually developed could be discovered automatically by gradient descent, and if not, what different properties these learned Transformer algorithms would have. This paper proposes a method for learning Transformer programs that could be used to start to answer these questions.
- A key challenge in mechanistic interpretability research (e.g. on Transformer circuits) is that Transformers trained by (unconstrained) gradient descent appear to collapse multiple conceptually distinct concepts via superposition. This paper proposes training methods that may help to mitigate superposition by forcing the network to read, write, and manipulate individual ‘variables’ that live in orthogonal subspaces of the residual stream.

Although terse in some places (perhaps due to length constraints), the methodology is for the most part clearly described, and the limitations of the approach seem sensible for a first attempt at this kind of constrained training. Another strength is that the framework seems to be extensible—the examples in the paper paint a clear enough picture of how to modularly extend the target symbolic programming language with new features (and then how to extend the neural net architecture and training procedure accordingly).

Weaknesses:
- Although the motivation is interpretability, I was not convinced that the learned programs in the experiments were particularly interpretable. It would have been nice to see more detailed analysis of the programs. Outside the toy in-context learning example, are there any interpretable algorithms that are learned? Can examples on which the learned programs fail be traced back to particular ‘bugs’ in the symbolic programs? Can the symbolic programs be used to engineer adversarial examples on which the Transformer will fail? Do ordinary transformers trained without constraints also fail on these adversarial examples (indicating that they might be using similar flawed algorithms)? How stable are the learned programs across different training runs?

- It is unclear how scalable the presented technique is, to more complex problems, longer sequences, or bigger networks. I also would have appreciated more discussion / investigation of how easy or tricky the optimization was, and why it appears to work more poorly for larger Transformers (e.g., that handle larger sequence lengths). What are the key challenges in scaling up? Do the Gumbel gradient estimates become too noisy, e.g.? Or are there other main challenges?

Limitations:
Limitations are adequately discussed, but I do think it could be useful to say a bit more about the challenge of scaling this approach -- what are the key obstacles & prospects for addressing them?

Rating:
7

Confidence:
3

";1
yBoVwpGa5E;"REVIEW 
Summary:
This paper empirically shows that robust overfitting is caused by the over-memorization of the non-robust features after learning rate decay. To mitigate the issue of robust overfitting, the authors propose to use a stronger training attack, a smaller learning rate decay rate, and a bootstrapped adversarial training loss. The comprehensive empirical results validate the effectiveness of the proposed method in mitigating robust overfitting and even improving robustness.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This paper provides comprehensive verifications for their proposed reason for robust overfitting. The authors clearly show the effect of the false memorization of the adversarial non-robust features after learning decay in robust overfitting.

2. The empirical experiments on various datasets and networks are comprehensive. The results support the authors’ claim.


Weaknesses:
1. Besides extensive empirical results, it would be better for the authors to provide some analyses from a theoretical perspective (possibly using game theory which could be related to the empirical results in this paper). 

2. The proposed method could hurt the natural test accuracy to some extent.


Limitations:
The proposed method can effectively mitigate the issue of robust overfitting.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper explains the phenomenon of robust overfitting in adversarial training from a minimax game perspective. The author considers AT as a minimax game between the model trainer and the attacker, pointing out the imbalance between them leads to the network memorizing non-robust features, causing robust overfitting. Based on these explanations, the author proposes several measures to rebalance the minimax game, thereby mitigating robust overfitting and improving adversarial robustness.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. It is interesting to study AT from a minimax game perspective.  
2. The paper proposes multiple measures to alleviate robust overfitting and enhance adversarial robustness.

Weaknesses:
1. The motivation is unclear. The author's explanation of the robust overfitting process is based on some observation-driven analysis, which are difficult to be convincing. For example, the attacker injects non-robust features for misclassification, and the cause of robust overfitting is the network's memorization of non-robust features. What exactly are the false non-robust mapping and the falsely memorized non-robust features? Can the authors use the intuitive and precise statement to explain the mechanism of robust overfitting?

2. The method's novelty is limited. The author claims that previous attempts to change attacker strength have not focused much on robust overfitting. However, there is existing research in this area:   
Yu C, Zhou D, Shen L, et al. Strength-Adaptive Adversarial Training, arXiv preprint arXiv:2210.01288, 2022.

3. The experimental results are incomplete and not significant. 1) Did the author confirm that the robustness of MLCAT in Table 1 is lower than AWP? 2) The author introduces multiple measures to mitigate robust overfitting and reports their combined performance. However, what are the individual performances of each technique? Considering that even combining multiple existing techniques for robust overfitting mitigation can further improve robustness, it is necessary to report the experimental results of each individual technique applied to Standard AT.

Limitations:
N/A

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper studies the robust overfitting phenomenon in adversarial training. Meanwhile, this paper focuses on a specific problem “the robust overfitting occurs when we use learning rate decay techniques.” This paper proposes a game perspective to explain the robust overfitting. It claims that the robust overfitting happens because of the imbalance between the attacker and the model after the LR decay. Moreover, this paper proposes ReBAT method to improve the robustness and mitigates the overfitting phenomenon.

Soundness:
3

Presentation:
2

Contribution:
4

Strengths:
This paper studies a specific but interesting problem, the robust overfitting phenomenon. This paper explains that the overfitting happens because of the learned mapping of the non-robust features after LR decay.

This paper provides an intersting perspective to help us to understand the cause of the robust ovetfitting. In particular, the robust problem is a min-max problem, which could be regarded as a game. The robust overfitting is because of the breaks of the original equilibrium.

This paper provides extensive verification to support its explanation, which addresses most of my questions when I first review this paper.

Weaknesses:
1.For the Figure 2 (b), I guess the red line denotes  w/ LR decay and the blue line denotes w/o LR decay;

2.For section 3.2.1, the authors share an interesting and important insight. I’m afraid it is a little trifling. Could the author provide a figure to demonstrate it?

3.In section 4.1, the authors provide some techniques to address the robust overfitting, e.g., bootstrapping, small decay factor. From the theoretical analysis in this paper,  I guess there may exist other methods to achieve the balanced learning of robust and non-robust feature, and more discussions is helpful.  Could the author provide some theoretical analysis about the proposed method for the re-balance?

4.I find the proposed method ReBAT may cause more computation overhead to get a convergent result. Could the author discusses the limitations of this work?

5.Could the author share more discussions about the game idea?  For example, in multi-task learning, could it be regarded as a game, where there are multiple players?

Limitations:
N/A

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper investigates the phenomenon of robust overfitting in adversarial training and explains it from a minimax game perspective. The authors analyze how the decay of the learning rate disrupts the balance between the model trainer and the attacker, leading to robust overfitting. They propose a method called ReBalanced Adversarial Training (ReBAT) to mitigate robust overfitting and achieve good robustness even after long training.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1 The paper is well organized and easy to follow.

2  The paper provides a holistic understanding of robust overfitting in adversarial training by analyzing the imbalance between the model trainer and the attacker from a minimax game perspective. This perspective helps explain why robust overfitting occurs in adversarial training and why it does not occur in other training methods.

3 The experiments cover different network architectures and benchmark datasets, providing strong empirical evidence.

Weaknesses:
1 it is better to clearly state the diffference between the defined robust/non-robust features and the previous one in [15]

2 ReBAT[strong] seems like a strange notation, please consider to change one. 

3 The caption of Table 1 seems not correct?

4 Does the minimax game view provide any insights on the accuracy and robustness tradeoff?

Limitations:
Missing...

Please stately explicitly the limitations and potential negative impact. 

Rating:
6

Confidence:
4

";1
z37ki6nqAY;"REVIEW 
Summary:
This paper is about the classic Online List Labeling problem in the recent model of algorithms with predictions (a.k.a. learning-augmented algorithms).

In this problem n orderable items (say, integers) arrive over time. We need to keep them in the sorted order in an array of size c*n, for some constant c. When the i-th item arrives, we know its relative ordering only with respect to the i-1 items that arrived before, so we do not know where to put it, and we might need to reinsert it later at a different location (in order to have space for future items while maintaining the sorted order). Our goal is to minimize the number of such reinsertions.

There is already known a deterministic data structure with amortized cost O(log^2 n) per insert, and a randomized one with cost O(log^3/2 n).

In this paper the authors propose a data structure that together with each new item receives a prediction on what would be the final rank of that item among all n items.

The data structure uses internally as a black-box a classic (prediction-less) data structure for the problem. Denoting by f(n) the cost of this internal black-box data structure – so, f(n) = O(log^2 n) if we want a deterministic solution, and f(n) = O(log^3/2 n) if we are fine with a randomized one – and assuming all predicted ranks are within eta from the true ranks, the amortized cost of the new algorithm is O(f(eta)). Moreover, if prediction errors are distributed according to a Gaussian distribution with mean mu and variance var, the cost is O(f((mu + var)^2)).

The main idea of the data structure is to maintain a tree-like structure over the inserted items, and delegate maintaining items in non-overlapping subtrees of different sizes to instances of a classic data structure for the problem. Locations of items in the tree are guided by their predicted ranks. If a data structure corresponding to a subtree becomes overfull because of prediction errors, it gets merged with neighboring subtrees. This general idea was already present in the literature around the problem, but the details of implementation and analysis are very novel.

The authors also provide results of some simple experiments. What is nice (and always present in learning-augmented literature) is that the predictions are actually learned from training data, so the experiments can be considered end-to-end. The cost improvements observed over classic data structures are roughly 50%.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1) The main algorithm is nontrivial, yet simple and with a clearly visible main idea.

2) I like the additional analysis of the algorithm's performance under prediction errors coming from a Gaussian distribution (on top of the more standard analysis in terms of the maximum prediction error).

3) The algorithm uses existing classic data structures in a black-box manner, so it should be easy to implement and can leverage already optimized implementations of these data structures.

4) The authors provide a matching lower bound (albeit only for deterministic algorithms).

5) The paper is well written and easy to follow.

Simply speaking, it is one of the most interesting recent papers that I have read in this area.

Weaknesses:
1) The studied problem seems relevant in practice, but I am not sure how important it really is – however, even if it is not, I find the paper sufficiently appealing on purely theoretical grounds to merit acceptance.

2) The experiments are of a proof-of-concept style – e.g., datasets are adapted from benchmarks for other problems that have not much to do with this one – but it is a common thing for papers in this area.

Limitations:
There are only some minor limitations to this work:

1) The lower bound holds only against deterministic algorithms.

2) The performance guarantees are using the L_infty error measure, so it is not clear how the algorithm behaves when there is one outlier item with huge error but all other items are predicted almost correctly. The natural distribution analysis gives some hints that the performance of the algorithm should be good in such a case, but there is no formal proof.

3) There are only pretty basic proof-of-concept experiments.

Rating:
8

Confidence:
4

REVIEW 
Summary:
The author presented a novel learning-augmented algorithm for the online list labeling problem, providing solid theoretical guarantees in terms of the error in the predictions. They also investigated the stochastic error model and bound the performance in terms of the expectation and variance of the error. Finally, they carried out an experimental evaluation of the proposed methods.


Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
Although the problem studied, online list labeling, might be considered quite peculiar for this conference, I believe it can be a seminal work for learning augmented data structures. I like this work very much because they provide solid performance guarantees, showing that our algorithm utilizes predictions optimally because their cost upper bound matches a lower bound shown in Section 3.3 for any prediction error. They also show that their algorithm is optimal in the case of entirely erroneous predictions and that for deterministic LLAs, the learned LLA is optimal for all prediction errors.
I also appreciated the study of the case in which the error for each element x is sampled independently from an unknown probability distribution with a given average and variance error.

Weaknesses:
The main weakness of this work is that the problem studied, online list labeling, is quite specific. It would be interesting to apply this methodology to other data structures to solve problems that are more common and known. 

Limitations:
I cannot identify any restrictions or potential negative effects this work may have on society.

Rating:
8

Confidence:
3

REVIEW 
Summary:
Author study a fundamental online problem which is also important
in practice: online list labeling.
The introduce rank predictions for this problem and define prediction error
in a natural way. They achieve O(1) consistency and a robustness comparable
with the best possible classical online algorithm.
They show that the dependence of their competitive ratio on prediction error
is the best possible. They also analyze the case where the prediction
error comes from some unknown distribution.
Given the importance of the problem and soundness of the results, I recommend
this paper to be accepted.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* important problem
* tight bounds, complete analysis

Weaknesses:
* consistency is O(1) but not really close to 1.

Limitations:
* authors state their theoretical results formally, describing all assumptions.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper considers the online list labelling problem: A set of $n$ elements arrive online and have to be inserted into an $c\cdot n, c>1$ constant, large array while the array must at all times be sorted. Every time an already inserted element is ""shifted"" in order to maintain the order, it incurs a cost of $1$ and the goal is to minimize the total cost. The variant of the problem studied here, is the one where alongside the arrival of each element the algorithm obtains a possibly erroneous prediction on the rank of that element.

The paper presents an algorithm, that if the total error $\eta$ is defined as the maximum error in the predictions, has an amortised cost of $O(\log^2 \eta)$ and thus only constant cost when $\eta$ is almost zero. This bound is tight for any deterministic algorithm. These results are complemented with empirical evaluation.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
It is an interesting problem and data structures with predictions are indeed not as developed as online problems with predictions.

Weaknesses:
- The writing is at places not very precise and can be misleading. To give an example saying that the data structure is ""optimal for any prediction error"" is not true. For one I think the authors use ""optimal"" to mean ""best possible"" which differs from the convention that optimal refers to the offline optimal algorithm which knows the whole input in advance. Furthermore even ""best possible"" would not be completely accurate since it  can be up to a constant factor worse than the best previously known result with predictions. Perhaps ""tight up to a constant factor"" would be accurate? See also L80 ""improvement at no cost"", or L79 where ""proportionally"" I think is meant to be ""linearly"" etc.

- The prediction error is not very natural: Consider the scenario where one prediction is off by a lot and all others are perfect. Considering the maximum error in the predictions over all elements is heavily unfair against such a scenario where the predictions are arguably quite good and one could easily obtain an optimal insertion of all elements without (or with barely) any shifting.

Limitations:
No limitations or potential negative societal impact anticipated. 

Rating:
6

Confidence:
4

";1
7sjLvCkEwq;"REVIEW 
Summary:
The paper presents a very interesting analysis of discriminative entropy clustering in the literature and their use for self-labeling highlighting clear interpretation of the conditional and marginal entropy terms as decisiveness (push to have confident predictions) and fairness (to encourage desired proportions in clusters). The paper analyzes the variants of this kind of losses and their connections. They also discusses the relationship of this loss to K-means and refines the previously mistaken connection in a previous paper to point out the distinct difference. They further point out the effectiveness of reverse cross-entropy in case of uncertainty error and forward KL term to make them more effective  in the endpoints of softmax  interval. They also propose to use the regularization of classifier weights for margin maximization similar to SVM. A closed form update is derived to compute the pseudolabels from the combined weighted loss and shown its efficacy in clustering experiments.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
- I think the paper discusses beautifully the intricacies and usage of Mutual information based entropy clustering loss, which is widely used in self labeling/self supervised/ weak supervised learning. The flow of the discussion is to the point and tries to give interpretation of each terms in the loss in a concise manner by drawing connection among the variants. 

- The paper discusses the the previous link of entropy clustering to K-means and identify the distinct difference to k-means which was in fault in previous literature. On top of that they find the usefulness of the classifier weight regularization used in previous proof to link to the loss explicitly for margin maximization similar to SVM based clustering. 

- The use of reverse cross-entropy and forward KL term and the motivation behind it is explained  very well with the aid of Figure 2, so that they are more robust in case of uncertainty around the corner of the softmax simplex.

- The formulation of the EM algorithm is nice to make it work faster along with batchwise operation, showing its global optimal solution guarantee at convergence due to drawing convexity with formulated tight upper bound.
- The experimental results shows clear improvement according to their claim.

Weaknesses:

1. I would say the results of joint clustering and feature learning in Table 3 is not encouraging even when showing a very shallow network of VGG4, the improvement is not significant apart from  MNIST. But in Resnet-18,  the inductive bias learned from pretraining is helping, then the improvement from the proposed loss might not improve very significantly with the proposed loss.  Also in Table 4, the regularization on the feature extractor $\textbf{w}$ done or not in the loss or by weight decay? 

2. The experiments on semi supervised learning could also be shown to understand more when some supervision is available. How the idea of using reverse cross-entropy could be used in case of labeled one-hot y in equation 13? or it will be the regular cross-entropy for labeled set and reverse for pseudolabels with updated y_i?

3. What if the loss in 13 is directly optimized with gradient descent instead of using the EM? Although, it seems if $y_i= \sigma_i$ the reverse cross-entropy does not change anything if not updating y_i with closed-form update, is it?


Limitations:
The limitations are not discussed.

Rating:
7

Confidence:
4

REVIEW 
Summary:
In this paper, the authors first presented an analysis of the relationship between the regularized information maximization (RIM) clustering framework to K-means and SVM-based clustering methods, showing stronger similarities to the SVM-based clustering than K-means. Then they proposed a new loss function and associated EM optimization algorithm for clustering leveraging the reverse cross-entropy/KL divergence to obtain more robust and fair clustering, which has been demonstrated to improve the performance on several balanced image classification benchmarks.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
1. The authors identified an error about the missing normalization term in the proof of the equivalence theorem presented in Jabi et al. (2021).

2. The replacement of the forward cross-entropy with the reverse counterpart in the RIM loss appears novel in the clustering scenarios and has the potential to effectively mitigate the impact of uncertain/noisy pseudo-labels.

3. The proposed method showed improved performance on different image classification benchmarks over several baselines.

Weaknesses:
1. The manuscript was poorly written. The authors dedicated more than two pages to discuss the general background of the information maximization clustering framework and related methods. However, these discussions were confusing and largely limited the space for presenting the actual contributions of this work. Furthermore, many terms, including concepts like H and KL divergence, are not explicitly defined or explained, which may cause difficulties to understand the differences between the forward and reverse formulations. Additionally, the claimed conceptual and algorithmic contributions seem to be independent of each other. It is unclear if any of these conceptual clarifications contribute the discovery of the new loss function.

2. The disproof of the equivalence theorem in Jabi et al. (2021) is not convincing. While the authors pointed out an error in the original proof, this does not necessarily eliminate the possibility that the equivalence itself remains valid. Furthermore, this work focused on the standard K-means objective (including Figure 1), whereas Jabi et al. (2021) considered a soft and regularized K-means loss.

3. The authors' claim that the L2 regularization is linked to margin maximization seems questionable. [1] demonstrated that margin maximization is a property of the loss function itself rather than the regularization, which serves to control the model complexity. Indeed, certain combinations of loss function and regularization are not margin-maximizing.

4. The experimental validation is limited. A more comprehensive evaluation of the proposed modifications to the loss function would involve investigating the individual and combined effects of these changes on selected benchmarks and then comparing the results with multiple established baselines. It is still unclear how each modification contributes to the final improvement. Although the authors presented the impact of the reverse cross-entropy modification, they did so within the fully supervised setting rather than unsupervised scenarios. Furthermore, the authors only considered balanced datasets and tested the clustering with the ground truth number of clusters. A more diverse set of experimental conditions, including unbalanced datasets and varying numbers of clusters, would provide a more robust evaluation of the proposed method. Both NMI and ARI metrics used in Table 4 are capable of handling different number of clusters. Additionally, the architecture used in Section 4.2 is different from that used in the baseline methods. It would be preferable to standardize the experimental settings, including model architecture, to be able to directly compare with the results in the literature. Lastly, the inability of the proposed method to properly train a deep network-based clustering model is a concern as well. Most of the tricks should be independent of the loss function modifications, especially the reverse cross-entropy term, thus can be naturally integrated together.

[1] Rosset et al., 2003. Margin maximizing loss functions.

Limitations:
There is no discussion of limitations or potential societal impact.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper first discusses a number of general properties of  entropy clustering methods, including their relation to K-means and unsupervised SVM-based techniques.
Then the aurthors find that cross-entropy is not robust to pseudo-label errors in clustering.
Finally, this paper proposes a new loss function based on reverse KL divergence for clustering  to obtain more robust and fair clustering.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
(1) The proposed  loss function is interesting and and seems to be effective to obtain more robust and fair clustering.

(2) The authors try to establish connections between entropy clustering methods and classical methods.


Weaknesses:
(1) This paper is not well organized. There are too many details for the proposed method. Some of them can be moved to Appendix.

(2) There can be more descriptions and examples about the proposed method.

(3) The proposed method is merely tested on three image datasets.

Limitations:
I mentioned them in the Weakness section.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The authors consider discriminative entropy clustering and produce a discussions linking previous works. They have a version of the algorithm based on EM and a modified KL-divergence term. Experiments show the modified algorithm works better than competing methods with small networks. 


Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The authors provide a good overview of discriminative entropy clustering and its development, from the work of Bridle et al to the regularized version by Krause et al, to the more recent work using deep learning and representation learning (Asano et al and Jabi et al).  


Weaknesses:
- The contribution of this paper is somewhat limited: 
  1. The pointing out of a proof error in Jabi et al is helpful but is not significant on its own
  2. The discussion on SVM is based on previous works and simply replaces the logistic loss with margin loss, and is not particularly insightful 
  3. Section 3 is the authors' contributed new algorithm, but the main difference with previous works is changing the order of the KL term in the objective. 

- The improvements in empirical evaluations, compared to other methods, are somewhat limited. Many of the improvements are within standard error of competing methods. 

- The authors use a lot of space to discuss previous work (first 5.5 pages), and not enough space to explain what is new about their method and specifically what problems it addresses. 


Limitations:
Limitations not mentioned; potential negative societal impact not applicable. 

Rating:
3

Confidence:
4

";0
w7TyuWhGZP;"REVIEW 
Summary:
This paper proposes a novel algorithm for return decomposition with causal treatment. To do reward redistribution, GRD uses factored representations to model the Markovian reward function and dynamics function. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The writing is clear and easy to follow. It is interesting to see the visualization in section 6.4, especially Fig. 4.

Weaknesses:
The technical contribution is somehow limited and the stronger experiments are expected.

Limitations:
N/A

Rating:
5

Confidence:
3

REVIEW 
Summary:
This study introduces a novel approach, termed Generative Return Decomposition (GRD), to address a key challenge in reinforcement learning: identifying the state-action pairs that contribute to future, delayed rewards. While many methods redistribute rewards in a non-transparent manner, GRD offers a clear return decomposition by explicitly modeling the contributions of states and actions from a causal perspective.

GRD works by first recognizing unobservable Markovian rewards and causal relations in the data generation process. Then, it leverages these to create a compact representation for policy training over the agent's most favorable state-space subset. The researchers provide theoretical proof of the identifiability of the Markovian reward function and underlying causal structure and models. Experimental data also reveal GRD's superior performance and interpretability compared to other methods. However, some limitations exist due to the assumptions made, such as the stationary nature of the reward function, which may not be applicable in dynamic or online RL scenarios.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper excels in presenting Generative Return Decomposition (GRD), an innovative method that improves interpretability in reinforcement learning. GRD successfully addresses the identification of impactful state-action pairs for future rewards. Its effectiveness is supported by both theoretical evidence and practical experiments, demonstrating its superior performance over other existing methods. Moreover, the paper demonstrates a high degree of clarity and coherence, enabling smooth comprehension. Additionally, the explicit description of assumptions contributes to a more profound comprehension of the inherent strengths and weaknesses of the study.

Weaknesses:
- The quality of the text in Figure 1 could be improved by removing the shadow around the text. The same applies to Figure 2.
- Minor typos and errors that need to be edited for the next version of the paper. E.g. in line 172: ""provide"" -> ""provides""
- In Section 5, only one policy is considered, which is SAC. How about having the experiments run based on another policy optimization algorithm? What would be the differences in performance and results?
- Regarding the last paragraph of Section 5, there are two possible scenarios to train the agent.
    - First, the generative model is learned while the policy is being updated, in an end-to-end paradigm.
    - Second, the generative model is first trained (and stays fixed thereafter), then the policy begins to be optimized.

    In either case, how would that affect the policy training and performance? And how the insights from the GRD interpretations would be changed?

- In the experiment section, the visualizations for the learned causal structure are only provided for Ant. Please provide the same type of analysis for other environments.

- Considering Figure 4, having GRD, how the agent's robustness and generalizability would be affected? For example, consider the case where there are some anomalies injected into the agent and environment interaction, more specifically changing some values from the state-space. If such anomalies target the features that are less important to the agent, then its performance should not be affected that much, right? If so, could you provide some results in this regard?

Limitations:
See Weaknesses.

Rating:
5

Confidence:
4

REVIEW 
Summary:
Delayed reward in reinforcement learning is the major challenge in reinforcement learning. The return distribution technique is the direct way to resolve this issue while preserving policy. The existing works redistribute the returns in an uninterpretable manner. In this regard, this paper proposes a GRD which generates the Markovian rewards in delayed reward scenarios. GRD first checks the casual relations of state and actions and from a compact representation using causal generative model. The experiment results show that GRD outperforms the baselines and helps visualization. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* Experiment results seem promising.


Weaknesses:
* Explanation on line 345-352 is not sufficient and hard to understand. This experiment section is very important since authors insist that GRD give the interpretable structure of reward. 
* The methods have to construct casual inference which is only possible when all the states are exactly defined. If the number of states explodes, the parameters to learn casual structure would explode. If states and actions are given, we can construct the casual structure without learning with parameters.

Minor
* Equations are too messy and hard to understand. Use under bracket in equation 6. 
* Notations are not familiar. It is hard to understand C^{\cdot -> \cdot}, d^a , d^s (?). The authors have to redefine all the variables step-by-step to improve the presentation of this paper.
* The arrow size is not consistent in Figure 2. The arrow to \hat{r}_3 is narrower. Also, arrow directions which come from R are weird. 

Limitations:
Yes

Rating:
4

Confidence:
2

REVIEW 
Summary:
The paper introduces a new algorithm called Generative Return Decomposition (GRD) for return decomposition with causal treatment. GRD addresses the problem by modeling causal relationships among variables, providing advantages over flat representations. It specifies each state and action as a combination of constituent variables and considers causal relationships within the system. The algorithm utilizes a factored representation similar to Factored MDP, enabling the formation and identification of the Markovian reward function based on causality. Unlike previous approaches, GRD uses a graphical representation to determine the contribution of each dimension of state and action to the Markovian reward. It also explains and models the observed delayed return as a causal effect of the unobserved Markovian reward sequence. The framework of GRD visualizes the causal relationships among environmental variables. The paper proves the identifiability of the underlying generative process and introduces a component-wise learning approach for recovering the causal generative process and redistributing rewards. The learned parameters provide a minimal sufficient representation for policy training, aiding in the effectiveness and stability of policy learning. The main contributions of the paper include the reformulation of return decomposition with a graphical representation, the introduction of GRD for learning the causal generative process, and empirical experiments demonstrating the method's superiority over state-of-the-art approaches in robot tasks with sparse rewards.

Soundness:
3

Presentation:
2

Contribution:
4

Strengths:
1 - Interpretability: Having interpretable reward redistribution is an advantage over non-interpretable methods. This can be used to diagnose the reason for failures policy optimization. 

2 - Reduces the state dimensionality: A very nice side effect of learning causal masks using a dynamics models is that a policy can be learned using very few features of the state. This leads to simpler policies, which could be more robust. 


Weaknesses:
1 - Writing: The paper needs a lot of work in explaining the method. Especially section 4 and section 5.1. A figure showing how the causal masks are applied would be a good idea. I am willing to improve my score, if the method explanation is improved. 

2 - Experiments: The experiments include only Mujoco tasks. It would be interesting to see how the method behaves on delayed reward Atari environments like Bowling. 

Missing Related work: 
[1] Modern hopfield networks for return decomposition for delayed rewards

Limitations:
Yes, the limitations have been addressed. 

Rating:
7

Confidence:
5

REVIEW 
Summary:
The paper addresses a major challenge in reinforcement learning: identifying which state-action pairs contribute to delayed future rewards. They propose a solution called ""Return Decomposition"" that redistributes rewards from observed sequences while maintaining policy invariance. Unlike other methods, their approach explicitly models state and action contributions from a causal perspective, making it interpretable.

The authors introduce a framework called ""Generative Return Decomposition (GRD)"" for optimizing policies in scenarios with delayed rewards. GRD identifies unobservable Markovian rewards and causal relationships in the generative process. Using this causal generative model, GRD creates a compact representation to train policies efficiently.

The paper proves the identifiability of the unobservable Markovian reward function and the underlying causal structure and causal models. Experimental results show that their method outperforms existing techniques, and visualizations demonstrate its interpretability. The source code for their approach is publicly available.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
  - The authors provide theoretical proof for the identifiability of the unobservable Markovian reward function and the underlying causal structure. This solidifies the theoretical foundation and robustness of the model.
  - The GRD method outperforms state-of-the-art methods in experimental results across a range of tasks. This demonstrates its practical effectiveness and application potential.
  - Visualization of the learned causal structure and decomposed rewards contributes to the interpretability aspect, a valued characteristic in contemporary machine learning.

Weaknesses:
I have no knowledge about reinforcement learning, and I don't understand why the system assigned me to review papers on this topic. Please disregard my review comments.

Limitations:
I have no knowledge about reinforcement learning, and I don't understand why the system assigned me to review papers on this topic. Please disregard my review comments.

Rating:
5

Confidence:
1

";1
VUlYp3jiEI;"REVIEW 
Summary:
The paper presents an analysis of the latent structure of diffusion models using differential geometry. The authors propose a method to define a geometry in the latent space by pulling back the Euclidean metric from the U-Net bottleneck space *H* via the network encoder. This approach enables the identification of directions of maximum variation in the latent space. The paper also explores the application of the proposed latent structure guidance for image editing. Finally, the evolution of the geometric structure over time steps and its dependence on text conditioning are investigated.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- I found the analysis presented in the paper interesting. It both uncovers unknown details about diffusion models (effect of text prompt and complexity of the dataset on the latent space) and confirm some previous observations (e.g., coarse-to-fine behaviour). This exploration can potentially reveal new capabilities of diffusion models, contributing to the advancement of the field.
- The paper is technically sound, and the claims made by the authors in sections 4 are supported by experiments. This experimental validation enhances the credibility of the proposed approach.

Weaknesses:
- The paper lacks comparisons with other diffusion-based image editing techniques, like [7,18]. Including such comparisons would have provided a more comprehensive evaluation and demonstrated the advantages of the proposed method.
- The presentation and clarity of the paper could be improved. For example, the abstract contains too much detail, making it challenging to understand upon initial reading. Also the explanation of the image editing technique could be improved: what is DDIM (section 4)? what is epsilon in Equation 4? Finally, Figure 1 is not sufficiently clear to me, it may hampers the reader's comprehension.

Limitations:
The authors have addressed the limitations of their approach.

Rating:
6

Confidence:
4

REVIEW 
Summary:
In this submission, the authors probe the latent space, xt ∈ X, of diffusion models (DMs) from a geometric perspective, utilizing the pullback metric to identify local latent basis in X and corresponding local tangent basis in H. To confirm their findings, they edit images via latent space traversal. The authors provide a two-pronged analysis, investigating the evolution of geometric structure over time and its variation based on text conditioning in Stable Diffusion. Notably, they discovered that in the generative process, the model prioritizes low-frequency components initially, moving to high-frequency details later, and that the model's dependence on text conditions reduces over time. The paper introduces image editing through x-space traversal and to offer comprehensive analyses of the latent structure of DMs, with a specific emphasis on the use of the pullback metric and the SVD of the Jacobian in computing a basis.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:

The paper presents a distinctive idea that provides an alternative method for editing in diffusion models, as well as enhancing comprehension of the latent space dynamics. By utilizing a geometric perspective, the authors make use of the pullback metric to investigate the latent space, offering insights into its structure and operation.  The exploration of the evolution of geometric structure over time and its response to various text conditions offers additional insights into the dynamics of the latent space is interesting. 

Weaknesses:

The first area where the paper could see improvement is in terms of the clarity of its analysis. Given its nature as an analysis paper, it's crucial that the analysis presented is as comprehensible as possible. However, the method and notation used in this work can lead to some confusion. For instance, Section 3, in its current form, may not be as accessible to all readers as it could be, and it could benefit from being revised for clearer communication of the ideas contained therein. Additionally, Fig. 1, which is presumably intended to illustrate key concepts, is perhaps too dense with information. Dividing Fig. 1 into two separate figures could make it easier to digest, enabling a clearer explanation of the approach.

A second aspect that could be improved upon is the overall presentation and proofreading of the paper. While the approach is relatively simple, its translation into the written form has not been as clear as one would hope. The text could benefit from a thorough proofreading to ensure that it is not just grammatically correct, but also that it conveys the authors' ideas in a way that is accessible to the wider machine learning community. As it stands, the paper's usefulness to this community may be hindered by its presentation.

Lastly, the paper could do more to address the computational implications of its approach. The authors use the power method to approximate the Jacobian, which, while effective, can be computationally costly. It would be beneficial if the authors were more transparent about this fact, allowing readers to fully understand the computational demands of the approach and evaluate whether or not it would be feasible in their own applications. Being upfront about such limitations can help to build a more honest and comprehensive understanding of the paper's methodologies and implications.

Limitations:
Societal impact has been properly address but I would like to see a deeper analysis of the computational complexity and runtime of the approach. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper studies the geometry of latent spaces of diffusion models (Dms) using the pullback metric. In the analyses, they mainly examine change of the frequency representations in latent spaces over time and the change of the structure based on text conditioning.

After the rebuttal:

I checked all reviewer comments and responses.

I agree with the other reviewers regarding limited algorithmic novelty of the work. Therefore, I keep my original score.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
- The paper is well written in general (there are several typos but they can be fixed in proof-reading).

- The proposed methods and analyses are interesting. Some of the theoretical results highlight several important properties of diffusion models.

Weaknesses:
1. Some of the statements and claims are not clear as pointed in the Questions.

2. The results are given considering the Riemannian geometry of the latent spaces and utilizing the related transformations (e.g. PT) among tangent spaces on the manifolds. However, vanilla DMs do not employ these transformations. Therefore, it is not clear whether these results are for vanilla DMs or the  DMs utilizing the proposed transformations.

3. A major claim is that the proposed methods improve effectiveness of the DMs. However, the employed transformations can increase the footprints of DMs.

Limitations:
Some of the limitations were addressed but potential impacts were not addressed.

Rating:
5

Confidence:
5

REVIEW 
Summary:
The paper proposes a study on the latent space of diffusion models and on how to manipulate it. It takes advantage of an observation made by previous work [22] on the flatness and semantic structure of the U-Net model used in  DDIM and uses pullback metric from the latent space of the U-Net to the space of diffusion to measure some properties of the latter under different conditions. 
The paper also proposes a method to manipulate the diffusion space through the different time steps, so as to carry out semantically meaningful edits.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
This paper is one of the first to study the behavior of the space diffusion models. It presents some interesting studies on the behavior of the process during time, showing that the early stages convey higher frequency while the last steps are more concerned with higher frequencies. Another interesting observation is that tangent spaces of different samples tend to be more aligned at T=1, while they diverge toward T=0 (end of the generative process). 

The paper also shows (even if it had already been observed in 22) that the pullback metric is effective in transferring the shift along the semantically meaningful principal components of the U-Net latent space into the diffusion process, thus resulting in meaningful edits of the generated image, which frequency depends on the time the edit was performed.

Weaknesses:
I may have misunderstood or missed some important information, but the method described is not really clear. Specifically, it is not really clear to me how the editing process works (sections 3.3 and 3.4):
- In 3.3, the letter v is used to indicate elements of both T_x and T_h, so it is not always clear to which space they are referring.
- In general, it is not clear why the idea expressed in 3.3 is useful and where it was adopted.
- In eq 4 what is the epsilon function? In general, isn’t the vector toward which to shift selected from T_H (so it should be u) and then transferred to T_x?


Another concern is about the generalization of the proposed method to other diffusion techniques, or with other score models (i.e. not UNet). I think that this point needs more discussion.

Limitations:
I don’t foresee any particular negative societal impact. A discussion on how the proposed study may generalize to other domains and architectures would be of value.

Rating:
7

Confidence:
3

";1
iT9MOAZqsb;"REVIEW 
Summary:
The authors proposed a new theoretical framework based on the mean field theory to analyse adversarial training from several perspectives, such as the upper bounds of adversarial loss, the time evolution of weight variance and adversarially trainable conditions. Besides the theoretical analysis, the authors conducted several experiments verifying the proposed theoretical framework. 
Generally speaking, the proposed theoretical framework provides a new perspective to analyse adversarial training and is highly versatile and even can extend to other training methods.


Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper is organised well and easy to follow. 
The proposed theoretical framework seems inspiring and intriguing and gives a new perspective to analyse adversarial training from several aspects, which may serve as a good guidance for future work. Besides the proposed theory, verification experiments were also conducted to prove its effectiveness further.



Weaknesses:
1.	The verification experiments were only conducted on the easy dataset (MNIST); it may strengthen the findings if additional experiments are conducted on more challenging datasets.
2.	 Including a more systematic evaluation of the results may be beneficial, such as adversarial loss in residual networks vs. training steps for normally or adversarially training. 
3.	Several conclusions match the previous works; it could be more convincing if the reference could be given in the main content, such as ‘’the square sum of the weights in Ineq. (9) suggests that adversarial training exhibits a weight regularisation effect,’’ is consistent with [1], ‘’to achieve high capacity in adversarial training, it is necessary to increase not only the number of layers L but also the width N to keep L^2/N constant’’ is somehow consistent with [2].
4.	The authors mentioned in Line 260 that ‘’This result suggests that residual networks are better suited for adversarial training. However, one of the previous studies indicated that residual networks are more vulnerable to transfer attacks [54] than vanilla networks. ’’ However, the proposed theoretical framework did not explain such a transfer attack phenomenon. Could the authors explain or give more comments about this?
5. in Equation 3 and 4, it seems that the minimize part is not shown, perhaps it would be more reasonable to change the form of min-max in equation 3 and 4.

[1] A unified gradient regularization family for adversarial examples, in: IEEE International Conference on Data Mining (ICDM), 2015.
[2] Do wider neural networks really help adversarial robustness? Advances in Neural Information Processing Systems, 34.


Limitations:
The authors have clearly discussed the limitations of the proposed framework, such as, ‘’some theorems begin to diverge from the actual behaviour‘’ and ’’ the mean field theory assumes infinite network width, which is practically infeasible’’.
From my point of view, this article does not involve any potential negative social impact. 


Rating:
8

Confidence:
4

REVIEW 
Summary:
The authors propose a mean field based framework for theoretically analyzing the training dynamics of adversarial training for MLP and residual networks with ReLU non-linearities. Based on this framework, the authors provide tight bounds on the adversarial loss (squared loss between clean and adversarial example output) and investigate trainability of MLP and residual networks.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- While I am not very familiar with related work on mean field theory for (adversarially) training networks, the proposed framework seems to address limitations of prior work quite elegantly.
- The paper includes quite a number of interesting results using the proposed framework (loss bounds, training dynamics, trainability, impact of width).
- Formulation includes MLP and residual network with a class of “ReLU-like” non-linearities; is also includes adversarial training with clean loss (as e.g. done by TRADES).
- I feel the framework can be quite insightful and helpful for future work in understanding or improving adversarial training.
- Many claims are empirically supported on MNIST.

Weaknesses:
My main concern about the paper is that it is incredibly dense (due to the number of included claims) and its structure does not make checking the proofs and claims easy. Even though I invested significantly more time in this review compared to other reviews, I was unable to fully follow all derivations and proofs. I believe this is mainly due to the extremely convoluted way of presenting the theoretical results across main paper and appendix. Unfortunately, I feel that this also makes me less enthusiastic about the results. Here are some more detailed comments and suggestions:
- Starting with 4.2, the reader is somewhat forced to jump to the appendix at least once just to see that the authors simply reformulate the ReLU using its derivative and D for J and a. This trend continues throughout the paper and appendix. Especially in the appendix, one is forced to jump around a lot just to follow 2-3 sentences.
- The proofs are structured starting with simple lemmata and building up to the actual theorems. This is generally fine, but again, the referencing is overdone. For Thm 4.1 in Appendix E, there are 9 pages of derivation with >20 individual lemmata. So if I want to follow the proof of Thm 4.1, I am forced to go through many but not all of these lemmeta. Many have proofs of only 1-2 lines, but reference 2+ other lemmata or remarks and I need to remember the numbers or jump back and forth 2+ times just to read a single sentence. I feel the root cause for this is that many of the results are over-generalized and compartmentalized too much. I appreciate the thorough job of the authors in establishing many of the independence results, but as a reviewer and reader my #1 interest is following Thm 4.1, nothing more and nothing less. Everything that complicates this job is – in my opinion – bad for the paper. For me, the ideal solution would be a separate, easier to follow section for the proof of Thm 4.1 - even if it restates many of the lemmata and remarks, and moving the other results to a separate section for the (very very) interested reader.
- The main paper includes so many results that there is basically no discussion of each individual result. Often it feels that every sentence refers to some additional result in the appendix. I think for me, and many readers, actually discussing the results informally, in words, and taking more time and space to introduce the required notation would be more beneficial than including the current amount of results. I would prefer to have fewer results well-described and the remaining ones being in the appendix.
- Empirical results are discussed twice – after the corresponding theorems as well as in Section 6 – the space of the latter could be used to address one of the points above.

Comments and questions unrelated to structure and writing:
- In the introduction, contribution (d) is unclear to me – what theoretical result does it refer to, the trainability?
- The use of “probabilistic properties” is a bit unclear until the discussion of training dynamics. It would be helpful if the meaning would be detailed earlier in the paper.
- In 3.1, why is having $P^{in}$ and $P^{out}$ important, i.e., why do we need these fixed layers?
- Usually, the adversarial loss is also cross-entropy or something similar. While I saw papers arguing for a squared loss, TRADES does not use it AFAIK. Instead, the common setting is adversarial cross-entropy loss only, combined with clean cross-entropy loss or clean cross-entropy + KL as in TRADES. This makes me ask how assuming an adversarial cross-entropy loss would impact the results? Can similar results be derived?
- In 4, the assumption of independence is also unclear. I feel making this more explicit, e.g., by informally providing a short result on independence from the appendix, could be useful.
- The authors also highlight broader applicability; I am wondering if the authors derived similar results for standard training as reference? Or has this been done in previous work with other frameworks? This also related to the statement in l280.
- In 5.3 l261, I can’t follow how transfer attacks are relevant here? Transfer attacks should be weaker than the general attack modeled in the paper …

Conclusion:
I think that the paper has many interesting contributions and will be valued by the NeurIPS community. However, as I was not able to follow all derivations in a reasonable time, I will closely follow what the other reviewers have to say about the theoretical results. Also, I believe that the paper in its current form would have better fitted a long-format journal. For NeurIPS, I hope the authors invest some time in simplifying the main paper and restructuring the appendix. I think the current format will limit the audience of the paper to those very familiar with related work or willing to invest hours jumping back and forth between main paper and appendix. Some restructuring could really make this paper more accessible to the broad audience at NeurIPS.

Limitations:
See weaknesses.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper provides a mean field analysis on relu networks for adversarial training. The main insight is that networks without residual connections are not most likely inevitably suffer from gradient explosion or vanishing and thus are not adversarially trainable, unlike vanilla network training.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Analyzing adversarial training performance is an important problem both theoretically.

The insight from the analysis that adversarial training vanilla relu networks is more likely to suffer from gradient explosion/vanishing is an interesting insight.

Weaknesses:
The verification of the theorems might need more effort. E.g., Figure 4 is showing accuracy of vanilla network, it would be helpful to also show the curves for residual networks.



Limitations:
Limitations are well discussed.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The theoretical understanding of adversarial training is an important and valuable topic. This work proposes a new theoretical framework for this based on mean field theory. With the proposed framework, the authors analyze the properties of adversarial training from multiple aspects, including the upper bounds of adversarial loss, the time evolution of weight variance, the adversarial trainable conditions, and the degradation of network capacity. These results could be helpful for the understanding of adversarial training and inspire more efforts on this topic. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
1. It proposes a new framework to analyze adversarial training theoretically based on mean field theory. 
2. Based on the proposed framework, it presents several theoretical results for adversarial training.
3. The proposed framework and the presented theoretical results are non-trivial and helpful for the understanding of adversarial training.

Weaknesses:
It studies several different adversarial training characteristics in the main paper. Is there any correlation between these different characteristics？ Why do we choose these aspects for analysis? Further, is it possible to provide a global diagram to better see which properties can be analyzed and which cannot be analyzed at present based on the proposed framework?

Limitations:
yes

Rating:
7

Confidence:
3

";1
XsZ5YebcCz;"REVIEW 
Summary:
The paper investigates the problem of policy constraints in offline reinforcement learning (RL) settings, and finds the phenomenon that milder constraints on policies during training can lead to better performances at inference tests. The proposed component MCEP can be added on existing algorithms including TD3BC and AWAC. Experiments on D4RL dataset show improved performances over vanilla TD3BC and AWAC.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
The strength of policy constraints in offline RL is an important problem. It is a novel perspective to separate the policies for value estimation and inference with different constraint levels. I would say the proposition that milder constraints can improve policy inference performances is an interesting problem. The experiments are thorough with necessary ablation studies, and the results indeed show some improvement by using the proposed constraining method.

Weaknesses:
I think one major critique of the paper is: the most essential discovery that milder constraints may be required for test-time inference is mostly from experimental evaluations. The observations are not even consistent for that only 6 out of 9 shows this pattern. This is not strong evidence showing that milder constraints are necessarily always better. Some theoretical analysis or at least insights about this observation can be provided to make it more convincing.

Another critique is that although the experiments show some improvement by using MCEP on TD3BC and AWAC and over some baselines like CQL and IQL. These are not the SOTA results on these offline datasets, there exists better algorithms proposed by the time of NeurIPS submission that should be aware of:

[1] Hansen-Estruch, Philippe, et al. ""Idql: Implicit q-learning as an actor-critic method with diffusion policies."" arXiv preprint arXiv:2304.10573 (2023).

[2] Garg, Divyansh, et al. ""Extreme Q-Learning: MaxEnt RL without Entropy."" arXiv preprint arXiv:2301.02328 (2023).

[3]  Wang, Zhendong, Jonathan J. Hunt, and Mingyuan Zhou. ""Diffusion policies as an expressive policy class for offline reinforcement learning."" arXiv preprint arXiv:2208.06193 (2022).

The paper writing can be further improved.

Limitations:
The limitations of the current method are discussed, that the evaluation policies in MCEP may not be consistent with the value function and can lead to unstable value estimation if used in policy evaluation.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes Mildly Constrained Evaluation Policy (MCEP) for offline reinforcement learning to address the issue of excessively restrictive constraints for action selection during test time inference. MCEP uses a more constrained target policy for value estimation and another less restrictive policy for performance evaluation. Empirical results demonstrate the effectiveness of MCEP.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
MCEP is easy to implement and can be plugged into many policy constraint offline RL methods.

Weaknesses:
Since $\pi_e$ does not participate in the policy evaluation, I think line 7 of Algorithm 1 can be removed and $\pi_e$ can be extracted from Q after actor critic learning to save computational cost. The contribution of MCEP is only to extract a less restrictive policy after RL learning, which is somewhat limited.

The overall idea of the paper is quite simple. However, the notations and descriptions are a bit confusing. For example, the notations in Algorithm 1 lack a clear definition ($\psi, \phi, \tilde \pi, \pi^e, \tilde w, w^e, \mathcal L(.,.)$). And $\psi$ and $\phi$ in line 6 and 7 of Algorithm 1 are reversed, since Q evaluation in Equation 2 is associated with $\phi$. 

No theory supports MCEP in the paper.

Limitations:
More hyperparameters need to be tuned for MCEP compared with the original algorithm.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This work addresses the issue of excessive policy constraints in stabilizing value estimation within the offline RL paradigm. A separate target policy is used solely for evaluation and stabilizing value estimation, which is more constrained than the ""evaluation policy."" The evaluation policy does not participate in policy evaluation and is improved by the value function estimates, with the level of constraint adjusted by the weight of the term.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
Major points:
- This procedure can be easily integrated into offline RL algorithms that utilize policy constraints, and empirical results apply it to TD3+BC and AWAC. The empirical findings demonstrate promising improvements, with baselines encompassing the standard suite of state-of-the-art offline RL algorithms.
- The paper is well-written, easy to comprehend, and thoughtfully structured. 
- The idea itself is intuitive, and the toy experiments convincingly demonstrate that over-constraint poses a significant issue. Figure 2 clearly illustrates the adverse effects of over-constraint, with the policy performing poorly in low state value regions of the maze.
- The ablation studies are extensive and demonstrate the method's effectiveness.

I believe this simple yet intuitive method is worth presenting to the broader offline RL community. I believe this work should be accepted.

Weaknesses:
Major points:
- While the results show promise, they do not indicate substantial improvements across many environments, and there is some inconsistency observed. The method shows a decrease in performance in the medium-expert D4RL tasks and the pen task.

Limitations:
Yes

Rating:
6

Confidence:
3

REVIEW 
Summary:
Offline reinforcement learning (RL) methods frequently involve a policy constraint to mitigate error propagation when learning the Q function. Generally, a single constraint strength is used throughout training. This paper proposes instead to use different constraint strengths for learning the target policy, which is only used for learning the Q function, and the evaluation policy, which is the final policy returned by the algorithm. In particular, a stronger constraint is needed to ensure stability when training the target policy, but weakening the constraint for the evaluation policy can lead to better performance.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
* The idea is fairly general and can be instantiated with various RL algorithms, as shown in the paper.
* The experimental results provide insight into the role of the constraint and the tradeoff between stability and performance.
* Conceptually, the approach allows for a continuum of algorithms between one-step RL and standard actor-critic methods.
* The paper is clearly written and understandable.

Weaknesses:
* The algorithm introduces an additional hyperparameter that requires tuning, which is already a challenge in offline RL.
* The paper found that “in 6 out of the 9 tasks, the $\alpha$ for better inference performance is higher than the $\alpha$ that enables safe Q estimates”. While 6/9 is a majority, this is not convincing evidence that weakening the constraints is always helpful.

Limitations:
Yes, limitations are addressed.

Rating:
6

Confidence:
5

";0
JTKd7zYROf;"REVIEW 
Summary:
The paper proposes a so-called randomized sparse neural galerkin (RSNG) algorithm to solve time-dependent PDEs. The basic idea is to
consider parameterized functions as ansatz spaces, with parameters that vary in time. Consequently, the method is of the sequential-in-time learning algorithms rather than global-in-time learning algorithms such as plain vanilla PINNs. Computing PDE residuals with this class of ansatz functions and collocating the loss (at every time instant) naturally leads to a least-squares type linear algebra problem which can be solved either directly or indirectly and has been previously considered. The author identify problems with this approach and propose a novel remedy by considering randomized sparse instantiations of the projection, loosely along lines of dropout, but in a more L2 sense. The algorithm is illustrated with some benchmark examples for PDEs.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The premise of the paper is interesting. It is well-known that global-in-time methods such as PINNs are ill-conditioned and hard to train and the paper investigates a method of potentially avoiding some of the pitfalls of global-in-time methods. 

2. Randomized sparse algorithms can be efficient in the context described by the authors as long as certain conditions, such as incoherence that they loosely identify, are fulfilled. 

3. Numerical experiments clearly demonstrate that RNSG outperforms the dense least-squares method as well as baselines such as plain vanilla PINNs considerably.

Weaknesses:
1. Clarity: The paper could do with a rewrite where authors elaborate on the rather succinct notation, for instance in the appendix. In particular, to aid the reader, they could write out the neural networks in line 81, for instance for a simple one-hidden layer shallow neural network, as to how the time variable and the space variable exactly enter into Eqn (2) -- the reader will clearly see the distinct roles played by time here. Similarly, further rationale could be provided for the choice n >> p in deriving Eqn (4), one could for instance refer to it a quadrature for Eqn. (3). Caption of Figure 2 could be better explained, in particular what is refit in Figure 2(b) and what does co-adaptation mean and how does Figure 2 (c) shows this property. Quite a lot of heuristics is provided in a hand wavy manner that is potentially inscrutable to a reader -- for instance, what does the lines 173-176 mean ? 

2. Theory or rather the lack of it: The author do not attempt to provide any theoretical justification of their method. Some phrases are included here and there (e.g. line 162, lines 173-179 etc) but the reader feels that the authors know more about the theory then they present here. A discussion on these aspects could be illuminating. In particular, when is the method supposed to work and when not ? The lack of presentation of a theoretical perspective forces this reviewer to judge and criticize the paper on its empirical content as I proceed to do below. 

3. Empirical evaluation: The authors consider three time-dependent PDEs i.e., 1-d Allen-Cahn, 1-d viscous Burgers with a relatively low viscosity and a 2-d Vlasov-Poisson with a given potential and on a Cartesian domain. As such the problems are fairly well-studied benchmarks and the authors clearly show that their RNSG algorithm handily outperforms its dense foundation and an iterative version. However, the following limitations of the experiments can be identified: 

      a. The authors claim that it takes 18 mins for a PINN to obtain 6% errors for Allen-Cahn and almost 2 hours to obtain below 1% error for the simple viscous-Burgers example. This is unacceptably large and could very well be due to the use of ADAM as the optimizer. It is standard in PINNs literature to either use L-BFGS or start with ADAM and then switch to L-BFGS after say Order of 1000 iterations or so. This reviewer is curious if doing so reduces the run-time with PINNs ? In any case, the authors obtain significantly lower errors with RNSG -- so the timing is worth investigating. 

   b. Clearly, PINNs and even Casual PINNs (I am not convinced that it takes more than 6 and half hours to solve Burgers' equation with casual PINNs) are not the best conditioned to be the only baselines here. Rather I would suggest the following baselines: 

             b1) A method of lines version of PINNs (see the Raissi et al original PINNs paper) where the time is discretized with a RK4 solver and space with PINNs -- this is a fairer comparison as time is now iteratively updated with the PINNs and should be compared against your method, at least for some experiments. 

            b2) Alternatives to PINNs such as ODILs (Karnakov et al https://arxiv.org/abs/2205.04611) which directly collocate the PDE residual (with an implicit time-discretization) appear to be much better conditioned and could be a strong baseline for your method. 

         b3) The whole rationale would be out-compete standard numerical methods. Hence, it would be instructive to consider the computational cost of your ground truth generator. How much computational time does it take with this ground truth generator -- for all the experiments -- to reach comparable accuracy. Given that you require 13 mins to solve 1-D Allen-Cahn and 40 minutes to solve 1-D Burgers, traditional numerical methods would solve both these problems in a matter of secs, if not faster -- so where is the rationale for the use of your method ? One could argue that its rationale would be high-dimensional problems or complex geometries, which you don't consider empirically here. 

4. Scope: In its current version, it is unclear how this algorithm scales with spatial (or parametric) dimension -- is it fairly dimension independent ? If so, where is the evidence for that -- for instance, what is the computational time for the 2-D Vlasov-Poisson problem ? 

Another issue pertaining to the scope is that the method relies on an explicit time-stepping integrator such as RK4 -- can it be used for stiff problems, with implicit time stepping ? Otherwise, you will require too many iterations in time, something that PINNs and other global-in-time methods don't have to. It would be useful to see if your algorithm can work with large time steps for such stiff problems. 

5. Comparison: The authors say that other works such as [1,7,14] have considered this approach and yet do not provide a clear comparison to these works. It is essential to distinguish between them and the current paper, in a related work section.

Limitations:
It is a mostly a reiteration of the section on Weaknesses:

-- No theoretical underpinning of the algorithm.

-- Lack of a clear description of the limitations of the algorithm.

--- How are the collocation points $x_i$ chosen ? The authors refer to a prior work but do not contextualize it here.

--- More baselines need to be evaluated -- in particular, the runtime of traditional numerical methods has to be mentioned to provide the reader with an idea of what is the potential pay-off with their method, if any ? 

The basic idea is interesting and I am happy to reconsider my score if the authors address my questions appropriately.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper suggests a simple but effective way to train sequential-in-time models by randomly sampling weights to update. The method avoids overfitting and realizes speedup because only a small part of parameters is updated. The experiments show that the proposed model offers the best performance in terms of speed and accuracy.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* Although the method is simple, the idea is based on the reasonable assumption that parameters are redundant locally in time. Due to its redundancy, the Jacobian matrix is prone to be singular or stiff and takes a huge computational budget for training with the existing sequential-in-time methods. The proposed method elegantly addresses these issues by randomly sampling weights to update.
* The experiments clearly demonstrate that the proposed method works efficiently in terms of computation time and accuracy compared to the considered baseline methods.

Weaknesses:
* Figure 5 (b) and (c) suggest that sampling could lead to instability (see also Questions). The reviewer recommends that the authors show predicted solutions in figures to assess the smoothness of the prediction. Sometimes smoothness is key because some downstream tasks may perform additional differentiation on the prediction.

Minor points:
* In the first equation in Equation (1), the domain of t should be defined in the open set (see, e.g., Jürgen Jost ""Partial Differential Equations ThirdEdition"" (2012) Chapter 1) to make differentiation possible everywhere in the domain. That's why we need the initial condition.

Limitations:
* The authors addressed limitations. Another limitation could be handling boundary conditions other than periodic (e.g., Dirichlet and Neumann).

Rating:
7

Confidence:
4

REVIEW 
Summary:
The authors suggest an alternative optimization scheme for Neural Galerkin methods - a family of methods in which parameters of a neural network are optimized so that the gradients of the solution obey a given PDE with the notable quality being that rather than solving over all of space-time, the parameters of the network evolve in time to match the system evolution equation - in which only a subset of the surrogate model's parameters are updated at each time step. The author's show that this approach seems to lead to improved accuracy at a fixed FLOP budget and improved speed to a fixed accuracy threshold. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper is well written and interesting.
- Jargon is generally defined shortly after it is used.
- As someone who is not very familiar with Neural Galerkin methods, it seemed that most questions that immediately came to mind after reading segments were answered quickly. 
- Overall, the paper was clear, compelling, and all claims were well-supported by the provided evidence. 


Weaknesses:
The main weakness of the paper is that while the paper is clearly providing value to a current research topic, it could do a better job of explaining where Neural Galerkin methods are (or could be) practically useful. The PINN comparison is enough for me to be comfortable recommending approval, but given the method requires knowledge of $f$, it seems like true comparison would be numerical solvers which, as the authors mention in the conclusion, are still quite a bit ahead of the PINN comparisons on these problems. Without that, it's difficult to see whether the paper would be interesting to a wider audience.

Line-item Issues:
- Figure 2 is not very clear. a) makes sense, but (b) could use a better captioning so that it can be understood independently of the text and (c) is unclear even after reading the text. 
- Figures 3, 4 are similar in that the caption is currently insufficient to explain the figure.


Limitations:
Limitations are well covered and no ethical issues are foreseen. 

Rating:
7

Confidence:
2

REVIEW 
Summary:
In this paper, the authors propose a modified Neural Galerkin scheme for the solution of time-dependent PDEs, aiming at reducing computational cost and improving accuracy by avoiding overfitting.

The target solution $u(x,t)$ is approximated with a nonlinear parameterisation $\theta(t)$, so that $u(x,t)\approx \hat{u}(x;\theta(t))$ (this can be interpreted as a nonlinear form of reduced-order modelling). In practice, $\theta(t)$ represents the parameters of a NN responsible for outputting $\hat{u}(x;\theta(t))$ for a given $x$. At each time-step $t$, the goal is to train the network to output the solution at the next time-step. As is classical with Galerkin methods (and derivative thereof), this is achieved by following a variational principle, whereby the norm of the residual of the PDE is minimised. This can be recast as a least-square minimisation problem in the unknown $\dot\theta$: once solved, the gradient of $\theta$ is then used to guide the update of the NN.

The novel contribution of the paper lies in applying sketching to simplify the solution of this least-square problem. This is rooted in the observations that:
- Often, the target matrix in the system $J(\theta)$ (a collation of the gradients of $u$ wrt $\theta$, sampled at specific collocation points) is low-rank, implying redundancy in the components of $\theta$
- Updating the whole $\theta$ tends to overfit, which in turn leads to accumulation of errors and poor quality of the solution as time evolves.
This justifies reducing the space of parameters to update, and has the boosted effect of reducing computational time due to the effective shrinking of the least-square problem.

The performance of the novel method is verified on three benchmark PDEs (two 1D and one 2D), and expressed in terms of runtime and norm of residual. The proposed method outperforms both its dense (non-sketched) counterpart, and the global-in-time method (Causual) PINN.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The sketching simplification at the core of the method is well-justified, also with experimental results. Moreover, I particularly appreciate the fact that the intuition behind using sketching is rooted both in algebraic considerations (pertaining rank of J), but is also interpreted through the lenses of regularisation techniques in NN (dropout). Both views are valid, and contribute to supporting the hypothesis
- The paper is well written, neatly structured, with the right level of rigour and a good introduction to the topic

Weaknesses:
- Possibly the breadth of the contribution: after all, it represents a relatively small modification to an already existing method, and the modification itself takes inspiration from already-established techniques. Still, I appreciate this work, and the contribution seems valid and useful
- Some experiments might be expanded to further substantiate the claims (see questions below), but this should be straightforward to address

Limitations:
The main limitations I could identify (particularly, dependency on properties of the target Jacobian) are addressed and made explicit in the corresponding section

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper introduces Neural Galerkin scheme to sparsely update neural network over the sequential-in-time training. In particular, the proposed method randomly update sparse subsets of network parameters at each time step. With randomized sparse updates, overfitting problem of sequential-in-time methods can be mitigated, leading to better accuracy at test times. Moreover, the computational costs of training is reduced without losing expressiveness of the model, while accelerating the training process. Proposed method is validated on a wide range of evolution equations. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The paper is well organized and easy to follow. It also has a strong motivation inspired by the Neural Galerkin schemes and neural network dropouts.

* The paper has a strong theoretical grounds and corresponding toy examples to support the claim. For instance, two challenges of previous sequential-in-time training are well addressed in the section 2.2 with adequate examples. 

* The paper presents a wide range of ablation studies and various experimental settings to show the effectiveness of the proposed method. Most of the claimed benefits of the proposed method are validated empirically, including lower error rates and faster running time. 

* The proposed method is considered a big contribution for the field of sequential-in-time training methods.

Weaknesses:
* The performance of the model is only validated in standard benchmark equations. Validation in high-dimensional PDE problems can benefit the paper. 

Limitations:
Limitations are addressed by the authors

Rating:
8

Confidence:
3

";1
Ai40Gvt2wj;"REVIEW 
Summary:
The authors propose a SO(3)-equivariant network operating on scalars, vectors and 2-tensors. They identify the corresponding equivariant linear layers and come up with a mixing strategy to mix different representations. Furthermore, SO(2)-equivariant linear layers are proposed to allow the scenario that SO(3) symmetry breaks into a subgroup SO(2) axial symmetry along a specific axis $\hat{j}$.  Finally, they demonstrate how it is appllied to b-tagging in High Energy Physics (HEP), where the data is rotational symmetric w.r.t. jet axis $\hat{j}$.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The authors provide a good and simple implementation of SO(3) equivariant networks on scalars, vectors and 2-tensors. The weight matrix is carefully designed to preserve the symmetry. The way to mix different representations is also intuitive and easy to understand. 
- The discussion on axial symmetry is well motivated and easy to follow, and the analysis is solid and sound. 

Weaknesses:
- [Novelty] The idea of tensor-product-based representations is not new [1]. The main method (except the SO(2) part) looks a simple variation and the technique involved is pretty standard. Add discussion and comparison with existing tensor-product-representation-based methods could make the work more solid.
- [Evaluation] To show the effectiveness of mixing and SO(2) linear layers, I think it is better to put more intermidiate results (e.g., w/ and w/o SO(2) linear layers) in the main table. 
- [Minor issues]: Eq. (1, 2, 3) use Einstein summation without declaration, which may cause confusion to readers without physics background. Line 168 should be ""isotropic linear neuron of Eq. (2)"" instead of ""Eq. (1)"".

[1] Finkelshtein, Ben, et al. ""A simple and universal rotation equivariant point-cloud network."" _Topological, Algebraic and Geometric Learning Workshops 2022_. PMLR, 2022.


Limitations:
Limitations are not included in the manuscript.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This work presents a lightweight architecture based on scalars, vectors, and tensors for learning in 3D, subject to $SO(2)$-equivariance in a known direction that varies by sample. They are motivated by the jet-tagging problem in high energy physics, in which a given batch is equivariant with respect to a certain axis (which may vary between batches), and test their framework on this problem.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
Restricting to cross products and matrix products is new relative to previous work, which tends to focus more on the expressivity via irreducible representations of higher orders. The jet HEP dataset is also not commonly used in similar papers, and could provide a useful dataset for future papers. The proposed operations do indeed seem to be equivariant, and they are written out explicitly.

Weaknesses:
1. The proposed architecture is a simple restriction of many other existing architectures. The “tensor bilinear layer”, is a special case of the more general CG product that is now standard practice in other architectures, and it is not clear what benefits it has over a more general CG architecture. The nonlinearity of scaling by the vector or tensor norm is also not new: it is subsumed by the nonlinearity of applying an arbitrary function to the norm, and then scaling the vector or tensor by this value, which similarly was widespread in foundational works such as Tensor Field Networks (Thomas et al 2018) and subsequent works.
2. It does not seem like this architecture is particularly expressive, due to the use of low order features and the simple nonlinearity. (Note that prior work on the universality of point cloud architectures, and equivariant architectures more generally, usually requires making statements about polynomial approximation, where higher order tensor products are required to approximate higher degree polynomials — see e.g. Lorentz nets, Bogatskiy et al 2020, or Dym et al 2020 on the universality of point cloud architectures. Therefore, it seems to me that these simpler layers will likely have worse approximation properties.)
3. Based on the paper’s description (but the authors can correct if this is not the case), the motivating problem is really SO(2)-equivariant about a known but sample-dependent axis. Therefore, the discussion of the SO(3) CG product and other SO(3) architectures is somewhat misleading/confusing. It is also not explained clearly how using this paper’s SO(2)-equivariant architecture compares to the standard approaches in HEP based on the “transverse and longitudinal projections”.
4. The baselines and experiments are not sufficiently developed. For example, the only baseline is a DeepSet-style permutation invariant architecture, presumably on the raw 3D coordinates. However, the authors should compare to approaches using the apparently more standard “transverse and longitudinal projections”, as well as to SO(2)- or SO(3)-equivariant baselines on the raw coordinates. The paper claims that using a physically intuitive restriction of the space of possible operations is beneficial, but does not demonstrate this with an ablation study or by using e.g. a full CG product.

Limitations:
There is no potential negative societal impact. The paper does not address a potential lack of expressivity of the architecture.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper presents a method to handle complex geometric structured data, specifically SO(3) representations, through SO(3)-equivariance and judicious symmetry breaking. The technique improves computational efficiency and enhances the performance of a network operating on these representations. When applied to b-tagging, a High Energy Physics classification problem, it yielded a 2.7x improvement in rejection score over conventional methods.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. It makes sense in machine learning to explore more efficient representation for data with symmetric structures. It is particularly important in many scientific fields such as HEP and material.
2. Using the proposed method, it shows a significant performance improvement compared with the baseline method.

Weaknesses:
1. Some explanation in the paper is difficult to be followed by ML researchers. This work deeply involves the task of B-jet identification at LHC experiments, but I’m not sure if these are sufficiently interesting for the ML and AI community.
2. In the related works, the authors mentioned that there had been numerous prior works on SO(3) equivariant models, but in the experiment only PFN with simulated datasets is implemented for comparison.  The numerical evidence in this paper looks not sufficient.

Limitations:
N/A

Rating:
5

Confidence:
2

REVIEW 
Summary:
The paper proposes n permutation and SO(3) equivariant network for b-tagging in high energy physics. Permutation equivariance is required since the input data is a _set_ of track particle features, while SO(3) equivariance is required since these features are geometrically scalars or vectors whose rotational transformation law should be respected. The basic architecture is a particle flow network (PFN), which is based on deep sets. It consists of 1) a first SO(3)-equivariant subnetwork, applied to each track particle individually, 2) permutation invariant pooling via summation, 3) a second SO(3)-equivariant subnetwork, and 4) a final layer which extracts scalars. Internally, the network operates on scalars, vectors, and Cartesian 2-tensors. The first two are irreps of order $0$ and $1$, respectively, the third one is reducible (it would in principle decompose according to $1\otimes 1 \cong 0\oplus 1\oplus 2$).

The equivariant subnetworks employ a range of different SO(3)-equivariant mappings:
- Affine layers, i.e. linear maps followed by bias summation. The linear maps are made equivariant by broadcasting weights over the full representation dimension. Biases are only summed to scalars and 2-tensors since equivariant bias summation is impossible for vectors.
- Linear layers that are applied in local frames that are aligned along each individual jet's momentum axis $j$. This alignment is only specified up to rotations around the momentum axis, which is addressed by making the layers $\mathrm{SO}(2)\_j$ _gauge_-equivariant (the subscript labels the axis along which the subgroup is taken). Technically, the network seems to operate on restricted representations $\mathrm{Res}^{\mathrm{SO}(3)}_{\mathrm{SO}(2)} \rho$. Since the momentum axes $j$ are moving along with SO(3) rotations, the operation is as a whole still SO(3)-equivariant. Two explicit constructions of such SO(2) equivariant maps for vectors and 2-tensors are proposed. Due to the restricted equivariance requirement, these maps are less constrained than fully SO(3)-equivariant layers.
- Nine different _bilinear_ maps which map pairs of features of different types again to scalars, vectors and 2-tensors. This step allows (in contrast to the others) to transition between different representations.
- SO(3)-equivariant nonlinearities. For scalars, the authors use conventional ReLUs, while the nonlinearities for vectors and 2-tensors are acting on their norm (a common approach).

There is a single experiment, in which the models are trained as binary classifiers for b-tagging. The full equivariant model improves significantly upon a non-equivariant baseline and a version which ablates 2-tensor features.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
The paper is well written and generally easy to follow. While I am not familiar with the b-tagging task and competing approaches, the empirical improvements over the baseline model seem very significant. Another strength is the use of bilinear mappings - most equivariant networks utilize only linear maps.

I really liked the idea of using locally $SO(2)\_j$ gauge equivariant operations besides fully SO(3) equivariant layers. The authors identified the additional geometric structure given by the momentum axes $j$ and addressed it appropriately.

Weaknesses:
My main concern with the approach is that the linear layers are not shown to be _complete_: in principle one could derive a basis of the most general equivariant linear maps (intertwiners) between the given representations. The authors show only the sufficiency of their layers regarding equivariance, but not the necessity. Indeed, I believe that quite some maps are in fact not the most general ones
(more details listed below).

To address this issue it is most convenient to work in the basis of irreducible representations, which the authors consciously avoid. For the following comments, note that scalars and vectors are irreps of order $0$ and $1$, while Cartesian 2-tensors are an irrep tensor product which decomposes according to $1\otimes 1 \cong 0\oplus 1\oplus 2$. Furthermore, intertwiners exist by Schur's lemma only between irreps of the same order, and are for SO(3) scaled identity matrices $\lambda\mathbb{I}$. For all non-trivial SO(2) irreps over $\mathbb{R}$ the spaces of irreps are 2-dimensional and are spanned by the irrep-endomorphisms ((1,0),(0,1)) and ((0,-1),(1,0)).

- The intertwiners for scalars and vectors (presented as ""broadcasting"") are complete since these are irreps. However, the broadcasting for $1\otimes 1$ tensors is overly restrictive, and there are actually three parameters, one for each irrep in the 2-tensor. The claim that the intertwiner space for 2-tensors is one-dimensional is repeated in line 172.
- The solutions for biases are indeed complete: biases can only be summed to trivial irreps, which exist with multiplicity 1 in scalars and 2-tensors, but not in vectors.
- It is furthermore possible to have intertwiners between scalars or vectors and 2-tensors, since the latter contain irrep orders 0 and 1 as subspaces (trace and antisymmetric part). These solutions are not used.
- The SO(2)-intertwiners between SO(3)-vectors in 3.1.1 seem complete, however, this is not proven but just claimed (""The set of all SO(2)-equivariant maps is _exactly_ ... [proposed parametrization]""). One can easily prove the completeness by observing that order 1 SO(3)-irreps decompose under restriction into the direct sum of an order zero and order 1 SO(2)-irrep, whose intertwiner spaces are 1 and 2-dimensional, respectively. The proposed parametrization has the same dimensionality.
- The SO(2)-intertwiners between Cartesian SO(3) 2-tensors are again not proven to be complete (""[equivariance] is satisfied when ... [proposed parametrization]"" just claims sufficiency). Going to the irrep basis shows again that there are more possible solutions.
- In the case of bilinear maps, there are again some missing operations, e.g. combinations of two 2-tensors that result in a vector. All possible solutions follow directly from Clebsch-Gordan decompositions, which are well known for SO(3).
- An alternative approach to TReLU would be to apply three independent nonlinearities to the irreps contained in the 2-tensor. The equivariance of TReLU is not explicitly shown (this might be quite trivial to show).

As mentioned above, addressing these concerns would be easiest by switching to the irrep basis. As this would require a major revision, I am not sure whether this is the right way forward for this submission, or should rather be addressed in follow-up work. An alternative way to address these concerns would be to explicitly discuss completeness of intertwiner bases in general and prove it for each operation in which it holds. The equivariance of operations like TReLU or Eq. (4) should also be proven.

Another issue is that it is not well explained how the overall network remains SO(3)-equivariant despite intermediate operations only being SO(2)-equivariant. This is one of the most interesting aspects of the paper and deserves more attention.

It should also be discussed how this relates to other _gauge equivariant_ / _coordinate independent_ networks. The alignment of frames along the $j$-axis with remaining SO(2) ambiguity seems very similar to the SO(2)-structure (SO(2)-bundle of frames) considered by (Weiler et al. 2021), specifically their figure 53 (right).

The transition from SO(3) to SO(2) features is currently not sufficiently explained. I believe that the authors assume the restriction function $\mathrm{Res}^{\mathrm{SO}(3)}\_{\mathrm{SO}(2)}$ - please clarify this!

The group actions and the domains and codomains of A and L in sections 3.1.1 and 3.1.2 are nowhere defined. One can read them off between the lines, but they should be stated more clearly.

I am somewhat worried about the extent of the experiments. It would be nice to have a more thorough empirical analysis, for instance giving more ablations, showing training curves, investigating whether the equivariance is exact or due to numerical errors only approximate. Not all ablations discussed in the text of section 5.2 are shown in the table.

Finally, I wondered about the input features of the baseline, are they the same as for vector and tensor PFN? There should really be two baselines, with either set of features, to clarify that the improvement is really coming from the architectural changes instead of the input alone.
How exactly are the different models made comparable? Do they have the same number of parameters or the same computational cost to ensure a fair comparison?



Limitations:
The main limitation is in my opinion that the solutions are incomplete - which should be addressed more clearly. Other limitations, like the limitations of the bilinear ops due to not using Clebsch-Gordan decompositions or the lack of investigating the space of tensor nonlinearities are  adequately addressed.

Negative societal impacts are not to be expected.

Rating:
6

Confidence:
5

REVIEW 
Summary:
Before starting, I must mention that I am not a physicist, and therefore, I have focused on the machine learning aspects of the paper.

### Summary

This paper proposes the use of SO(3) equivariant neural networks for B-tagging. The method proposed builds upon Deep Sets and provides a quite constrained formulation of SO(3) equivariance. The paper further indicates that breaking SO(3) equivariance locally can be used to get better representations for the task at hand, while maintaining global equivariance to that group.


Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper is presented very clearly, well-structured, and it is in general easy to follow and understand.

Weaknesses:
### Concerns
* My main concern is that, to the best of my knowledge, it is not possible to obtain a certain equivariance without all networks being equivariant to that group. In particular, I do not understand how global SO(3) equivariance can be locally broken into SO(2)_j equivariance while preserving global SO(3) equivariance. In the best of my understanding, as shown in roughly all the papers on group equivariance, in order to have a network be equivariant to a certain transformation, all layers need to respect that equivariance. I think that clarifying how this is possible is *crucial* for the paper. It is important to note that the paper simply states this and does not provide proofs or analyses regarding this statement. 
* I am very concerned with regard to the expressivity of the proposed algorithm. For instance, it has been shown in several works that equivariance can be obtained in expressive ways that do not have such hard restrictions as having biases be equal to zero –note that several similar very constraining restrictions are also defined for each of the mappings in the networks–, e.g., E(n)-equivariant Steerable CNNs, among many others. From what I understand, this work builds mostly upon the Deep Sets literature. However, this is by far not the most general way to obtain equivariance to a certain symmetry. I believe that the authors should at least state this clearly in the paper.
* The paper performs multiple ablations that are not found in the submission other than by conclusive statements. For instance, in line 322, the authors state: “Finally, we note that neither family of models performs even as well as the baseline, when no bilinear operations are allowed”. I believe that clearly showing the results of these ablations will strengthen the contributions of the paper. In general, such conclusive statements in their own are often vague and not very informative.
* In the final part of the conclusion it is stated that “it should be also possible to use these models for creating equivariant Graph and attention based networks”. Given that these families of networks –as stated earlier– are much more general than Deep Sets for equivariant formulations, I believe that this statement is not really easy to accomplish in practice. 


Limitations:
See previous responses.

### Conclusion
In conclusion, I believe that the paper needs some work before I am able to support acceptance. I believe that there are several factors that need to be clarified, .e.g, how to get SO(3) equivariance with only SO(2)_j layers, for this to be an strong submission. 

Rating:
4

Confidence:
4

";0
6fXzz8cvTA;"REVIEW 
Summary:
This paper combines the denoising pretraining technique with protein inverse folding models, achieving competitive results to baselines. The denoising pretraining has been proven to be effective in molecule and protein representation learning. Therefore, the rediscovery of the phenomenon in protein design is to some extent straightforward. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper is clearly written. The presentation is good.
- The methodology is reasonable and convincing.
- The experimental results are positive and support the authors' claims.

Weaknesses:
- The innovation seems to be limited. As denoising pretraining has been proven to be effective in molecule [1] and protein [2,3] representation learning, its success in protein design is straightforward.

- The denoising techniques and model design are largely proposed by existing work, which further limits the innovation of this paper

- Compared to the PiFold baseline, the improvement is marginal.

[1] Zaidi, Sheheryar, et al. ""Pre-training via denoising for molecular property prediction."" arXiv preprint arXiv:2206.00133 (2022).

[2] Huang, Yufei, et al. ""Data-Efficient Protein 3D Geometric Pretraining via Refinement of Diffused Protein Structure Decoy."" arXiv preprint arXiv:2302.10888 (2023).

[3] Zhang, Zuobai, et al. ""Physics-Inspired Protein Encoder Pre-Training via Siamese Sequence-Structure Diffusion Trajectory Prediction."" arXiv preprint arXiv:2301.12068 (2023).

Limitations:
N/A

Rating:
4

Confidence:
4

REVIEW 
Summary:
In this work, the authors present DNDesign, a denoising training module atop inverse folding networks (IFNN). The folding physics learning plug-in module (FPLM) is trained following score-matching with noise added to the protein backbone. It also contains five operations, including summation, cross-attention, and gated attention, that integrate the features from FPLM to IFNN. Experimental results show the PiFold with FPLM achieves superior performance on CATH 4.2 and 4.3, when compared to previous IFNNs. Besides, the work introduces a fixed backbone conservation analysis based on potential energy changes to evaluate the performance of IFNNs. 

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
1. The protein inverse folding problem that the paper investigates is an emerging domain to apply deep learning techniques.
2. The fixed backbone conservation analysis based on potential energy changes leverages physical prior to evaluate the performance of IFNNs.
3. The idea of utilizing denoising to IFNNs is also connected to physical prior which is expected to boost the performance on inverse folding problems. 

Weaknesses:
1. The improvement of DNDesign compared to the original PiFold may not be significant. 
2. The paper writing needs to be improved. Some notations and technical details are not clearly explained.
3. DNDesign is claimed to be a plug-in for IFNNs. However, it is only tested with PiFold. 

Please see ""Questions"" for more details. 

Limitations:
The authors have discussed potential limitations in Appendix B.1. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work proposes DNDesign, a denoising-enhanced protein fixed backbone design method that effectively captures the protein energy landscape. By integrating denoising training and a plug-in module, DNDesign demonstrates its ability to generate promising protein sequences based on pre-designed structures. 
In the inverse folding experiments, the method outperforms all the baseline methods. It also compares the diversity with the baseline, which demonstrates that the method can generate diverse suggestions for a design protein.


update: I keep my score.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The authors propose DNDesign, which enables the inverse-folding model to capture the deep understanding of folding physics that previous models do not fully exploit.
They prove how DNDesign learns folding physics directly from data and the method improves the state-of-the-art model on various protein sequence design benchmarks.
The authors further make a fixed backbone conservation task based on potential energy change from newly generated sequences. The analysis proves that DNDesign generates energetically favorable sequences. It is highly likely to work in real wet-lab experiments.

Weaknesses:
Figure 2 is a bit confusing. I think the 'noisy protein' and 'protein' should be a protein backbone without side chains?
The energy function and energy based distribution part is a little confusing to me. The distribution (force) is purely learning from the data? Or, the distribution is initialized with some force field, e.g. Rosetta. If the energy is purely learned from the training data, I think the submission should have some experiments to demonstrate that the energy is meaningful.
some typos, e.g. line 158, DEDesign.

Limitations:
n/a

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper proposed to use denoising diffusion probabilistic model and score-based model to solve the protein inverse folding problem, e.g., generate amino acid sequence given a protein backbone structure. Specifically, this paper used denoising diffusion model to generate unstable protein structure with higher dynamic energy and then used score-based model to learn the physical dynamic forces which drive a protein structure from unstable state to stable one. The learned physical forces are incorporated into a graph-based attention network to predict amino acid sequence. Experiments are conducted on CATH 4.2 and 4.3 datasets and compared with various approaches, showing superior performance (though not by a large margin) over the compared methods.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
(+) Using score-based model to learn physical dynamics in protein folding is novel, informative and reasonable.

(+) Code has been submitted along with the manuscript submission.



Weaknesses:
(-) The paper is not well organized and written, and the logic between each sections/subsections is not obvious and hard to follow.

(-) The texts in Figure 2 are too small to see them clearly with ease.

(-) The experimental results in Table 1 is a little bit marginal compared with PiFold, especial for NAR type.



Limitations:
The authors did not mention any limitations of the proposed method.

Rating:
5

Confidence:
4

";0
pirH9ycaNg;"REVIEW 
Summary:
This work studies RL with kernel function approximation, specifically where it is assumed that the transition dynamics and reward function live in some RKHS. While there has been some work on RL with kernel function approximation, existing work provides bounds which are suboptimal. This work seeks to tighten these bounds, and ultimately obtains a bound scaling as $O(\sqrt{\Gamma(T) T})$ where $\Gamma(T)$ is the maximum information gain, matching the lower bound in certain settings of the bandit case. Their proposed algorithm is a variant of optimistic LSVI.

Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
1. The setting of kernel RL has not been studied as thoroughly as some areas of RL, and optimal bounds do not exist. This work takes a step in obtaining optimal regret, tightening the best-known existing bounds, and obtaining optimal regret in certain special cases (some bandit instances). However, I believe the stated result is incorrect—see below.

Weaknesses:
1. I do not believe the stated result is correct. In the setting of linear bandits/linear MDPs/linear mixture MDPs, the information gain is bounded as $\Gamma_{k,\lambda}(T) \le O(d \log T)$, which would translate to a regret guarantee scaling as $O(\sqrt{d T})$. However, there are well-known lower bounds for these settings which scale as $\Omega(d \sqrt{T})$ (see e.g. [1] and [2]). Thus, the bound stated in this paper is better than the lower bound by a factor of $\sqrt{d}$, which is impossible.

The following are less significant issues, but are also important to address:

2. There is a vast body of literature on RL beyond tabular and linear function approximation which is not referenced or discussed. See references [3-6] below for a start, and the works cited therein. This literature should be cited and discussed.
3. To make the results concrete, it would be helpful to instantiate Theorem 2 in the setting of tabular and linear MDPs.
4. The result is only provably optimal in the setting of bandits with Matern kernels. However, this paper considers deterministic rewards, so it doesn’t actually capture the bandit setting and as such does not handle the setting for the stated lower bound. It’s difficult, then, to make the claim that this result is optimal in any setting. I would suggest modifying the setting to allow for noisy rewards, or showing that there is a reduction from the current setting to the bandit setting (by encoding reward randomness in the transitions).
5. In addition, it would greatly strengthen the paper to show a lower bound for kernelized RL.
6. It was not clear to me what the $\eta$ parameter corresponded to or how it was defined. This should be clarified. 
7. A reference or proof should be given for Lemma 1. 

[1] Zhou, Dongruo, Quanquan Gu, and Csaba Szepesvari. ""Nearly minimax optimal reinforcement learning for linear mixture markov decision processes."" Conference on Learning Theory. PMLR, 2021.

[2] Lattimore, Tor, and Csaba Szepesvári. Bandit algorithms. Cambridge University Press, 2020.

[3] Du, Simon, et al. ""Bilinear classes: A structural framework for provable generalization in rl."" International Conference on Machine Learning. PMLR, 2021.

[4] Jin, Chi, Qinghua Liu, and Sobhan Miryoosefi. ""Bellman eluder dimension: New rich classes of rl problems, and sample-efficient algorithms."" Advances in neural information processing systems 34 (2021): 13406-13418.

[5] Foster, Dylan J., et al. ""The statistical complexity of interactive decision making."" arXiv preprint arXiv:2112.13487 (2021).

[6] Zhong, Han, et al. ""A posterior sampling framework for interactive decision making."" arXiv preprint arXiv:2211.01962 (2022).

Limitations:
Yes.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper proposes a reinforcement learning algorithm called $\pi$-KRVI that achieves order-optimal regret. The algorithm performs local kernelized optimistic least squares value iteration update - specifically, it partitions the state-action space so that each cell contains a small number of observations, and the Q value within a cell is updated to a Gaussian Process based Upper Confidence bound using observations in that cell only. This is motivated by KOVI [10] and $\pi$-GP-UCB [14]. A sublinear upper bound on the regret is given - this seems to be the first sublinear bound in a general setting, and it is claimed to be order optimal in the number of episode $T$.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The sublinear regret seems to be the first one established in a general setting, and this seems to be order-optimal. The paper is generally clear, but a few things could be improved as mentioned in Weaknesses. The idea seem to be sound, but I didn't read the proofs.

Weaknesses:
* The paper only informally refers to [19] when stating the bound is order optimal. This is not immediately clear due to differences in notations, and some ambiguity in the paper's discussion. A more precise and detailed discussion would be helpful.
* It is not clear what ${\cal S}$ and ${\cal A}$ are, but it seems both are cubes? If yes, is this a necessary assumption?
* The reward functions and transition distributions are assumed to have a norm $\le 1$. How will the regret change if the upper bound is larger than 1?
* Lemma 1: $V$ is not used anywhere. Is it supposed to be $V_{h+1}$? If yes, an arbitrary $V_{h+}$ can potentially have a large norm, and Eq. (11) may not hold? A proof of the lemma seems to be missing.
* Line 233: why is the target value constructed using a new random state $s_{h+1}'$, rather than the random state $s^{t}_{h+1}$ from the current episode?
* Line 248: Does the running time refer to the total running time of Algorithm 1? If yes, can you explain why the upper bound is independent of the size of the partitions?

Minor comments
* Kernel ridge regression is generally described as a method that doesn't provide any uncertainty estimate, while the paper describes it as a method that provides uncertainty estimates. In particular, [38] is cited as the reference, but it doesn't seem to provide such an account of kernel ridge regression. What's described as kernel ridge regression is Gaussian process regression.
* A brief explanation on the motivation behind maximum information gain would be helpful.
* Putting the pseudocode of the algorithm in the paper would be helpful. Alternatively, provide a more complete description of the algorithm.
* Algorithm 1, line 10: $x_{h}^{t}$ should be $s_{h}^{t}$?

Limitations:
A discussion on the limitation of Assumption 1 would be beneficial.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper presents an optimism-based online learning algorithm for RL with large state-action spaces (including continuous spaces). It proposes a (Gaussian) kernel-based function approximation + optimism (building on UCBVI) algorithm. It assumes that the reward and transition density functions are representable in  1-bal of an RKHS (with a Gaussian kernel), which is quite reasonable.  It also introduces a domain-partitioning technique  to make the kernel ridge regression part scalable. The regret bound obtained for the algorithm is shown to be an improvement over SOTA [10]. Specifically, regret scales H^2 and sublinear in T. 

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
Online RL algorithms for continuous state and action spaces is a really challenging problem, and until recently was unresolved. This is the best such result I have seen. It makes a very smart (and now seemingly natural) use of kernel-based function approximation. 

Weaknesses:
The authors have not presented any numerical results. So it leaves one wondering whether it is all nice theory, and there is some hope of the making the algorithms practical.

Note: Title has a typo for Kernelized"".

Limitations:
Kernel-based method may limit scalability. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper theoretically studies the performance of a reinforcement learning algorithm under the assumption that the Q function is a member of RKHS with a known kernel. The authors provide cumulative regret bound on the iterative-least value iteration algorithm and specialise their results to the kernel with polynomial decay of eigenvalues. They improve the regret bounds of prior works by refined analysis of confidence sets in this non-parametric setting. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The paper is clearly written in the context of its rather technical nature. 

The work combines the newest understanding of adaptive confidence sets and their analysis for the case of Matern kernels with Linear MDPs, by providing a kernelized variant of thereof. I am not familiar with the other literature utilising the novel variants of the analysis in the context of RL. 

Seems like there is a sub-community interested in this issue given the COLT open problem, however, as somebody not in this community, I have to say it seems certainly a matter of taste rather than importance. 


Weaknesses:
The prior work, [10] indeed does not provide optimal bounds, but arguably the algorithm seems to be more practical than a rather time-varying discretization of the domain in order to facilitate the construction of the order optimal confidence sets. 

To be slightly harsh I wonder if the proper academic solution would not have been informing the authors of [10] of this new technique rather than writing a wholly new paper which utilises the trick with the domain splitting which comes from other prior works e.g. [14]. At the expense of creating more elaborate confidence sets one can indeed improve the bounds, but whether this improves the performance remains unanswered in this paper, as no comparison is provided. The proposed contribution is solely of theoretical nature. 

It is common in online learning to use the doubling trick and mention it in passing in case improved results are desired. I wonder if this is not a similar discretization trick, and whether this deserves an independent publication at NeurISP.


Limitations:
None identified. 

Rating:
5

Confidence:
3

";1
6d9Yxttb3w;"REVIEW 
Summary:
This paper proposes OILCA, which addresses the scarcity of expert data in offline imitation learning by generating counterfactual samples, to imagine what the expert will do under a unobserved state. OILCA takes the perspective of Structual Causal Model (SCM). The algorithm consists of four steps: 

1) A heuristic expert policy is pretrained with the known expert data. 

2) A conditional VAE is trained on the union of expert and supplementary data with the latent variable being ""exogeneous variable"" that integrates the information from task label and transition, to rebuild the possible next state. 

3) Sample transitions with task labels from the expert data. Counterfactual next states are generated by applying a do-intervention on the latent variable, and expert action is then generated by the pretrained policy; the counterfactual transition is added back to the expert data. 

4) Arbitrary offline imitation learning method with supplementary data is applied on the augmented expert data (DWBC is empirically the best choice).

On many environments with both in-distribution and out-of-distribution dataset, OILCA is proved to be better than multiple baselines. The data augmentation technique of OILCA is orthogonal to other offline IL algorithms and can benefit many of them.


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
**1. The proposed method is sound, and can be combined with multiple offline IL methods.** The idea of generating imaginary (counterfactual) samples to combat scarcity of expert data is a still novel direction that has been proved to be successful by several works [1, 2]. Furthermore, the algorithm is orthogonal to many offline IL algorithms and empirically proved to be an amplifier for them, which significantly increases its impact.

**2. Generally, the idea of the paper is clearly conveyed through the authors' writing.** The motivation is clearly presented in the question between line 59 and 60, and the high-level process is clearly conveyed in the pseudocode. The questions asked at the beginning of the experiment section is well-answered and clearly indicates the important takeaways of the section.

**3. The experiment result is solid.** Not only does OILCA works significantly and unanimously better than multiple baselines across in-data distribution and out-of-data distribution dataset on many environments, but it also improves the performance of the baselines. Moreover, the visualization on toy environment clearly shows the property and behavior pattern of the algorithm.

**4. The paper also provides theoretical analysis to prove its generalization ability, which is a very important problem of IL/RL community.** Numerous methods, such as entropy regularizer [3] and pessimism [4], has been proposed to address the problem of generalization onto the vast state-action space without data coverage. Theoretical advantage on this area is a valuable contribution. 

**References:**

[1] D. Hwang et al. Sample Generation for Reinforcement Learning via Diffusion Models. In ICLR, 2023.

[2] C. Lu et al. Synthetic Experience Replay. In ArXiv, 2023.

[3] J. Ho and S. Ermon. Generative Adversarial Imitation Learning. In NIPS, 2016.

[4] G-H Kim et al. LobsDICE: Offline Learning from Observation via Stationary Distribution Correction Estimation. In NeurIPS, 2022.

Weaknesses:

**1. The related work section could be expanded to include more related work.**

a) The work is related to works that uses generative model and tries to do few-shot adaptation on offline IL such as FIST [1], PARROT [2] and CEIP [3] (the latter two has RL finetune, but already works decently well at the offline IL stage). While this work uses generative model in a very different way, those works should be briefly discussed due to the similarity of the problem formulation and the common use of generative model. 

b) The key idea of this work is to generate imaginary data to enhance the imitation learning, and the idea of data generation has already been studied by several works (see [1, 2] in the strength section). It would be great to see a discussion about the relation between this work and the existing works in data generation.

The above two parts are now missing from the related work, and poses a weakness on the investigation of the context.

**2. There are many moving parts in OILCA.** While the authors claim that OILCA is a 2-step solution (line 135-136), the networks that need to be trained include two actors (pretrain & final), one discriminator, and encoder/decoder of the conditional VAE. Two pairs of them needs to be trained jointly. This could be a potential source for instability in training and extra usage of computational resource (and there is no computational resource and training time spent reported in the paper).

**3. About the pseudocode:**

a)  ""$D_E\leftarrow$"" should be added in front of the current line. This is a value assignment rather than an expression;

b) The meaning of $c$ should be reiterated in the pseudocode for quick understanding.

c) overall, I think the update process and generate process in line 3 and 7 can be clarified more clearly (for example, what is sampled in the beginning, and what is fed into a neural network with what output?) such that the do-intervention process on SCM is more clearly presented. Currently the readers can only make analogy from the definition on line 96-106 without explicit confirmation on notations, which is difficult to comprehend (especially when the parameter notation used in Eq. 2 and the pseudocode is different).


**References:**

[1] K. Hakhamaneshi et al. Hierarchical few-shot imitation with skill transition models. In ICLR, 2022.

[2] A. Singh et al. Parrot: Data-driven behavioral priors for reinforcement learning. In ICLR, 2021.

[3] K. Yan et al. CEIP: Combining Explicit and Implicit Priors for Reinforcement Learning with Demonstrations. In NeurIPS, 2022.

Limitations:
The authors have discussed the limitations in the appendix line 582-587, and I think is indeed the most important problem that readers could ask: does counterfactually generated data necessarily improve the performance? Will it instead hinder the performance because of the poor quality of generated data? It is a pity that this paper does not answer the question, despite the fact that it is already a good work.

As for potential negative societal impact of the work, there is no discussion. I suggest the authors to include the discussion somewhere in the paper; though the work is purely on simulated control environment, the work on automated control would inevitably causes some concerns such as misuse of technology and potential job loss.



Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper focuses on the problem of offline Imitation Learning, where an agent aims to learn an optimal expert behavior policy without additional interactions with the online environment. This setting widely exists in the real world as it usually consumes a lot of human effort and cost to collect expert data. Using sub-optimal data could be a valuable way for learning policy. The proposed method uses counterfactual data augmentation to generate high-quality data by learning the underlying data generation mechanism with unlabeled data. Then the new data is combined with expert data to train a policy model. Experiment results demonstrate that the proposed method achieves a large margin over multiple baselines.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. This paper is well-written and easy to follow. Figure 1 is illustrative and helps me understand the core idea of this work.
2. The motivation of using a small portion of expert data with a large portion of unlabeled data is important in real-world tasks. The proposed method that leverages the information in unlabeled data in an unsupervised way helps generate more expert data and therefore trains a better policy model.
3. The proposed method is evaluated in two domains with locomotion and manipulation tasks, which of both are important and have a high impact.

Weaknesses:
1. In the abstract, there is a gap between the statement of poor generation caused by sub-optimal datasets and the elimination of spurious features. No clear connection between the sub-optimal dataset and spurious feature is mentioned. 

2. In line 14 of the abstract, the authors say the in-distribution robustness is improved by their method but robustness is not mentioned before. Do the authors assume that robustness is almost the same as generalization when the testing scenario is “in distribution”?

3. Using counterfactual data augmentation to improve the performance of decision-making models (including RL, offline RL, IL) is already investigated by existing works. The novelty of the proposed method could be limited unless more differences between existing papers and this one are emphasized. 

4. In Definition 2, the notations $\tilde{f}_i$ and $\tilde{PA}_i$ are not defined. These notations are important for understanding this definition.

5. I am a little confused about the three-step pipeline of counterfactual inference discussed after Definition 2. The second step is usually for modifying the SCM by removing the structural equations $f$ and replacing the value of $x$ with others. However, this part is not mentioned and the authors only say “denote the resulted SCM as $M_x$”. So, how do we get this $M_x$? Is the value of x changed? Figure 2 enhances my confusion since the do-intervention is only conducted to the exogenous variable $u_i$ rather than $s_t$ or $a_t$. I am not sure if this is still the standard definition of counterfactuals. It is also not consistent with the statement “What action might the expert take if a different state is observed?” since the state is not changed.

 6. In section 3.1, the authors say “an additionally observed variable c is needed, where c could be, for example, the time index or previous data points in a time series, some kind of (possibly noisy) class label, or another concurrently observed variable”. However, the authors do not mention which one is considered for $c$ in this paper. I can only find some information in the experiment part, which also makes me have new questions. Does the process of selecting the variable $c$ create multiple environments with different contexts? This is usually considered a multi-task learning setting, which may not be true for a general imitation learning setting since the data collection is usually not possible to be clustered with additional context indicators. I hope the authors can provide more information about how to define and section the variable $c$ in the rebuttal.

7. After reading the entire paper, I find that robustness and spurious feature is only mentioned in the introduction. The theoretical analysis and experiment do not provide any evidence of the improvement of robustness and the elimination of spurious correlation. In particular, section 4.2 tries to investigate the robustness but I cannot find the setting that supports it. Robustness should be evaluated with small disturbances to the system, including the state, action, and dynamics. After checking Appendix 5.1, I cannot find any disturbance added to the dataset.

Limitations:
No potential negative societal impact is mentioned in either the main context or the appendix. One theoretical limitation is discussed in the appendix but some empirical limitations might be ignored. The most important one is how to select and obtain variable $c$, which is not discussed in the paper.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper propose a novel learning framework OILCA for offline IL, which generate counterfactual data to augment the scarce expert data. They analyze the disentanglement identifiability of the constructed exogenous variable and the counterfactual identifiability of the augmented counterfactual expert data. The experiments especially in the CausalWorld benchmark demonstrate the effectiveness of the method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. It is in time to introduce causal inference to offline reinforcement learning and offline imitation learning. This topic is valuable to the RL community;
2. The paper is overall well-written and easy to follow for me;
3. The way of data augmentation through counterfactual inference makes sense;
4. The experiment is almost sufficient to demonstrate their method.


Weaknesses:
This paper can be improved in several areas:

Writing and structure:

1. In this paper, it's not clear what the spurious relations under the MDP structure are. Could you explain it based on Figure 2? Notably, it appears that there is a direct causal relationship between $u$ and $s$, which seems should not be categorized as spurious relations.

2. The definition of do-intervention from line 96 to 119 is a little confusing. Specifically, the definition of do on lines 96~96 and in Figure 2(b) is replacing $f$ with another function $\tilde f$ while keeping $u$, but on line 117, the interpretation of do changes to keeping the function $f$ unchanged and replacing $u$ with $\tilde u$. It seems that line 117 is more in line with the actual implementation. Could the author clarify this point?

3. The explanation in Section 2.3 on why IVAE solves the identifiability problem is a bit of vague. This is an important context, especially regarding the introduction of the auxiliary variable (which is c defined in Section 3.1) and its role in identifiability. It might be beneficial to move some content about c from section 3.1 to 2.3 to better inform the reader about the preliminaries.

Related work:

The authors do not mention any related work in the main body of the paper. Consider moving some of the related work from the appendix into the main text. Moreover, there have been several recent studies applying causal inference to data augmentation for RL . The authors can compare their method with these works and discuss the similarities or differences. They might also consider using these methods as baselines for comparison:
- Sample-Efficient Reinforcement Learning via Counterfactual-Based Data Augmentation
- MOCODA: Model-based Counterfactual Data Augmentation
- Counterfactual data augmentation using locally factored dynamics
- Offline Reinforcement Learning with Causal Structured World Models

Method:

1. The authors rely on a pretrained expert policy in the data augmentation process. How was this expert policy obtained? If we assume that we use $\mathcal{D}_E$ to train the expert policy, given that this paper claims $\mathcal{D}_E$ is very limited, it should be challenging to reconstruct the expert policy adequately from $\mathcal{D}_E$. Consequently, the constructed $\tilde s$, $\tilde a$ might be different from the actual data distribution in $\mathcal{D}_E$. How can we ensure that using the augmented dataset from $\tilde pi_e$, which is unreliable, will have a positive effect on the downstream imitation task?

2. The authors state that they ultimately use Equation (2) for data augmentation, but Equation (2) uses $p(u|c)$ to generate $u$, not the posterior-based method described on line 101, which should use $q(u|s,a,s',c)$ instead. The authors need to clarify the inconsistency between these two points.

3. The authors claim that their contribution is to prove that their method can improve the generalizability of imitation learning, which is a bit of overclaim. From their theoretical analysis, it only shows that increasing the amount of data can improve the accuracy of imitation learning, a relatively trivial conclusion that holds for any machine learning task. If we are conducting a theoretical analysis, we need to explain why using data generated by the counterfactual data augmentation method can better improve the algorithm's generalizability. Otherwise, it raises the question: Does data augmentation not reliant on counterfactual inference have the same improvement?

 Experiments:

1. To my knowledge, the original DEEPMIND CONTROL SUITE environment does not have exogenous variables. How did the authors construct the experimental environment and dataset to fit the setting they proposed? Supplementary note: if this is not achievable, I don't think experiments on MuJoCo are necessary. The authors could consider designing more experiments to validate their algorithm's effectiveness on benchmarks designed for causal inference, such as:
    - Alchemy: A benchmark and analysis toolkit for meta-reinforcement learning agents
    - Systematic Evaluation of Causal Discovery in Visual Model Based Reinforcement Learning

2. While the experimental results seem significant, the ablation study and visualizations are insufficient. Perhaps the authors could consider the differences between their data and data generated by different methods, the clustering of reconstructed exogenous variables, and whether the corresponding transitions or policy behaviors meet expectations.

3. The description of the baselines is insufficient. The authors should emphasize how the baselines utilize $\mathcal{D}_E$ and $\mathcal{D}_U$ respectively.

Limitations:
NAN

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper introduces OILCA, a causality-regularized data augmentation method for offline imitation learning tasks. Overall, the idea is novel to me. The empirical results shown in the experiment section seem very promising. However, to support the claims made in the paper, more experiments are needed. Please see details in my suggestions & questions section.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
Overall, the idea is novel to me. The authors provide theoretical analysis to support their results. The empirical results shown in the experiment section seem very promising.

Weaknesses:
Missing reference:

As your work is related to causality + imitation learning, I believe the work of Causal confusion in imitation learning clearly worths a citation and discussion. Also, there is quite a lot of related work that is not discussed, e.g., with a quick Google search on causality + reinforcement learning: 

[1] Pim de Haan, Dinesh Jayaraman, and Sergey Levine. Causal confusion in imitation learning. In NeurIPS, pages 11698–11709, 2019.

[2] Gasse, Maxime, et al. ""Causal reinforcement learning using observational and interventional data."" arXiv preprint arXiv:2106.14421, 2021.

[3] Sun, H., & Wang, T.. Toward Causal-Aware RL: State-Wise Action-Refined Temporal Difference. arXiv preprint arXiv:2201.00354, 2022.




Suggestions:

The flow of the introduction is a bit awkward to me: the authors mentioned the difficulty of learning from a mixed dataset containing both expert and random-quality data, but they then answer the counterfactual question of how the expert could perform under different states, this question is uncorrelated with the previous discussion. I hope the authors can update this part to better convey their idea and motivations.

The notions in Definition 2 are unclear to me. what does the \tilde mean when it is over PA_i? This is not explained in the paper.

Figure 5 is not mentioned in the text and is therefore confusing: how do you increase the percentage of expert data? Why is the proportion always smaller than 1.0? 

It's sort of misleading to call a well-performing method a 'robust' method. (Q2, experiment section). To demonstrate the robustness, the authors should show far more empirical studies rather than a comparison under a single setting.

Limitations:
please see above.

Rating:
6

Confidence:
4

";1
50I7q86igD;"REVIEW 
Summary:
This paper aims to explore the application of a scalable UQ-aware deep learning technique, Deep Evidence Regression, and applies it to predict Loss Given Default. It extends the Deep Evidence Regression methodology to learn target variables generated by a Weibull process and provides the relevant learning framework. By testing on both simulated and real-world datasets in the context of credit risk management, the proposed method exhibits enhanced suitability for applications in which the target variable originates from a Weibull distribution, better capturing the uncertainty characteristics of such data.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1.This paper innovatively extends the Deep Evidence Regression methodology to learn target variables generated by a Weibull process and provides the relevant learning framework.
2.The article provides a clear and coherent description of the proposed method, including a thorough derivation of the relevant formulas.

Weaknesses:
1.The article contains several mathematical formula writing errors, such as the missing ""dθ"" in line 51 and Equation (16), the missing ""-1"" in Equation (10), and the inconsistencies in Equation (31) and Equation (33) with their original definitions.
2.The figure legends in the article do not indicate which parts of the content reference them or provide explanations for their content.
3.The experimental section lacks clear exposition, such as the specific settings of parameters k and λ for the Weibull distribution, as well as the target or subject for the MSE metric.

Limitations:
The proposed approach serves as a valuable tool for capturing and quantifying uncertainty in cases characterized by Weibull distributions. Therefore, the key to using this method lies in determining whether the data is suitable for the Weibull distribution. Furthermore, the applicability of the proposed method to distributions other than the Weibull distribution requires further investigation.

Rating:
4

Confidence:
3

REVIEW 
Summary:
Authors tackle the problem of uncertainty quantification for predicting credit risks. Concretely, they have applied a scalable UQ-aware deep learning technique, Deep Evidence Regression to predicting Loss Given Default with uncertainty. Authors argue that the conventional methods use for uncertainty quantification are too computationally and memory intensive and therefore they adopt Deep Evidence Regression as their statistical model.


They extend the framework of Deep Evidence Regression to predict targets generated with a Weibull distribution. The original method relies on the normally distributed targets which is not fitting for credit risk applications. Authors therefore re-derive the necessary formulas based on Weibull distributed targets; concretely they focus on log likelihood needed for training and the mean/uncertainty needed for predictions. Beyond that, authors adapt the regularizers found in the original paper to their purpose.

The new method is tested empirically on a synthetic dataset with points sampled from a Weibull distributions as well as a single real-world dataset focused on peer to peer mortgage lending data with recovery rate has been used as a proxy for Loss Given Default. Both datasets were used to test the Weibull-based approach against the original method relying on Gaussian distributed targets. The results show the modification made by the authors indeed helps with lower MSE and NLL on both test and train datasets.


Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
1. Authors tackle an important problem of uncertainty quantification for risk assesment where robust, and efficient measures of uncertainty are crucial to ensure fair treatment of customers. 
2. Authors provide an expansion of a well established method to make it much more attractive to specialized domains such as risk assesment. This work can be directly useful for practicioners in the field of risk assesment and indirectly useful for researchers in other fields who can adapt the Deep Evidential Regression to work with different, field-specific targets distributions.

Weaknesses:
1. This paper provides an incremental improvement over the original Deep Evidential Regression (DER) work. The derivation of DER with Webull distribution instead of original gaussian target distribution is interesting but the majority of what makes the method impactful remains unchanged. 
2. Authors provide very limited empirical evaluation of their work. The single study with synthetic data provides a good proof of concept but gives little assurance of real-world impact of the work. The single real-dataset study is relatively limited and suffers from some issues: 1/ Authors do not have any baselines beyond the original methods while other methods, for example based on conformal predictions or quantile regression, could be competitive. Any other approach for uncertainty quantification would be useful to gauge the difficulty of the task. 2/ Authors only use a single dataset making the evaluation process less robust, it is possible that this method works well only for this particular dataset rather than for a generic class of problems. 3/ Authors only consider NLL and MSE as metrics for their evaluation, there are may more metrics to quantify uncertainty quantification (such as coverage) that could be used to make the evaluation more robust.
3. The presentation of the initial method is vague, I undrstand that the DER work is presented in its own paper but presenting it in more detail would make this paper more self-contained, especially given the reliance on the original work.

Limitations:
Authors provide a formulation for extending Deep Evidential Regression for problems where targets are Weibull distributed. However, they do not cover the extension of this method to problems with targets sampled from a different distribution. There is only a small class of problems where Weibull is appropriate and authors do not provide evidence that this approach generalizes beyond Loss Given Default estimation. I would appreciate additional real-world experiments thta could show whether this parametrization (choice of target distribution) can work beyond the single dataset chosen in the paper.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper introduces the utilization and extension of deep evidential regression for uncertainty estimation in credit risk prediction. The approach assumes a Weibull distribution for the target variable (e.g., LGD or a synthetic target). The authors modify the evidential regression mechanism to accommodate targets from this distribution, providing equations to illustrate the training process. Simple experiments are conducted on synthetic and real-world peer-to-peer lending datasets, showcasing potential improvements over vanilla deep evidential regression for credit risk management.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
The incorporation and extension of evidential regression-based UQ into finance-related problems is the main contribution and the key strength of this paper.

Weaknesses:
Refer to Questions. 

Limitations:
The paper's experimental setup is quite limited and requires further motivation. The demonstrations illustrating how uncertainty quantification can benefit credit risk problems lack sufficient substantiation. Moreover, the paper has a limited exploration of related work, and the quality of references needs improvement. In its current stage, the paper requires significant improvement in terms of better problem motivation and comprehensive qualitative and quantitative evaluations prior to publication.

Rating:
3

Confidence:
4

";0
gZiLCwFT61;"REVIEW 
Summary:
This paper presents a new approach to automatic curriculum learning designed specifically for multi-agent coordination problems.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- The main strength of the paper, in my opinion, is the well-formulated approach to the curriculum learning problem. To the best of my knowledge of the related literature, the non-stationary contextual bandit as the teacher and the population-invariant skills for the students are both original and useful contributions to the literature.

Weaknesses:
- My general problem with this paper is that I am finding it hard to evaluate the significance of the work in the automatic curriculum learning sphere without an adequate baseline provided for GRF. Whilst the authors do argue that VACL is not used on GRF due to requiring prior knowledge, it seems unreasonable to therefore provide no baselines that are actually designed for these larger settings. For example, if VACL was unusable, then I would have maybe liked to have seen a comparison to population-based approaches in MARL or any of the other automatic curriculum learning approaches mentioned in Sec. 4. Overall, it is hard to properly evaluate the gains from this automatic curriculum learning framework without seeing the performance of baselines in an environment that actually requires automatic curriculum learning (MPE does not need it according to line 297-298).

I am happy to update my score if the authors can make a reasonable argument against the lack of other baselines in the work.

Limitations:
The authors briefly make mention to the limitations of the work. I agree with the over-design of the framework for simple tasks, so would definitely like to see its performance in more difficult environments that it is designed for.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper presents Skilled Population Curriculum (SPC) which is a method for learning a curriculum to help a team of agents complete a complex task. SPC models the problem of choosing tasks for agents as a contextual bandit problem, and builds on top of the Exp3 algorithm to solve this bandit problem. SPC also uses an attention-based communication approach, and a hierarchical policy framework. Experiments are performed in the Multi-agent Particle Environment (MPE) and Google Research Football environment (GRF). While MPE does not seem to benefit much from SPC, in the more complex GRF domain the authors show using their SPC approach can accelerate training relative to MARL baselines.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The paper is clear: it is well-structured, and provides a good balance of intuition and detail. It is clearly motivated, and addresses an interesting problem.

- The presented results show clear benefits to the authors' approach.

- The authors use suitable baselines approaches, and suitable environments.

Weaknesses:
- SPC seems to add significant computational complexity vs. baselines like IPPO. While the authors can justify focusing on sample complexity, for completeness they should also record information about the wall-clock time / computational resources needed to train their different baselines.

- In their research question stated on lines 52–53, the authors highlight their desire to consider complex sparse-reward settings. However, the MPE domains are a sparse setting, though fairly simple, and the results here show little benefit to using SPC. On the other hand, the GRF experiments appear to have a somewhat dense reward (the GRF checkpoint reward, which while not necessarily active every timestep, it could be argued is 'somewhat dense'). It seems like absent the checkpoint reward, SPC would struggle because there would be little information in the returns for the teacher agent to use — and I expect this would be the case in most complex (very) sparse reward environments

- Line 277 the authors state MADDPG/MAPPO would not be suitable in these experiments. This might be true in general, but for GRF specifically the critic input size actually would be the same across all tasks (as it pads observations if agents are absent). But since GRF is fully observable, MAPPO is equivalent to IPPO so this is not an issue for this work — though the authors may wish to revise their statement.

- Though the GRF environment is complex and difficult to solve, its level of complexity is somewhat deceptive, as evidenced by the video on the project website. The rollouts show that the agents have learned a simple ""force an offside and run in a straight at the goal line"" strategy which exploits deficiencies in the GRF bots. This behaviour has been observed before by Song et. al (http://arxiv.org/abs/2305.09458). However, this is not a fault of the authors, and is more broadly an issue in the MARL research community. Because of this, it's not clear what skills the agents learn in the training tasks that are useful in the target task. It would be interesting to see a plot of training task performance throughout training.

- It's unclear why the IPPO baselines have a sharp step change in performance around 80 and 90 million timesteps. The authors should investigate this, and perhaps make a comment (at least in the appendix) about why it occurs. In my experience, things like this sometimes occur when training runs stop unexpectedly before the full 100M timesteps, and so the remaining timesteps are aggregating over fewer seeds with lower performance. I would encourage the authors to produce plots reporting the interquartile mean of their results, and produce a plot showing the disaggregated training curves for each seed. These can go in the appendix.

- The authors state (line 301): "" InFig. 5b, we omit the curve of QMix as its mean score is low and affects the presentation of the figure"". I don't expect QMix to perform worse than the presumably near-uniform policies at the start of training for the other agents. So it's not clear how including QMix would disrupt the graph. Is it the case that QMix has a worse average goal difference than -2?

- Can the authors clarify: the target distribution for GRF is ""100% 5vs5""? What is the target distribution for the MPE tasks? (I see now that these are mentioned later in the text: they should be mentioned when introducing the environments)

- It doesn't seem like there's a pattern to the task distribution (Fig. 6a) beyond ""academy_pass_and_shoot_with_keeper becomes less common"". It would be good to see the same plot for other trials. This possibly explainable by academy_pass_and_shoot_with_keeper requiring coordinated passing and shooting, whereas the 5vs5 rollouts (see video on project website) show a very simple GRF-bot exploiting strategy which does not closely resemble the behaviour required in academy_pass_and_shoot_with_keeper.

	- Where the authors claim ""For example, the proportions of 3vs1 and Empty-Goal tasks gradually drop as the student becomes proficient in these scenarios"", it is difficult to support this by looking at Fig. 6a.

- In my opinion this approach is over-engineered, but the authors do acknowledge this. Stripping some components (e.g the hierarchical RL) and focusing on deeply investigating the remaining components would improve this work

- Minor writing fixes:

	- line 42/43 ""more scores"" → ""more goals""

	- line 43: ""4v11"" → ""4v1"" (I assume)

	- line 305: ""tons of"" → ""many"" (more formal tone)

Limitations:
- The limitations section is quite limited, and limitations and assumptions could be more clearly stated throughout.

- The authors recognise that their approach is complex and computationally intensive, so might not be applicable in simple environments. Testing in a complex environment like Google Research Football is a good choice, although due to issues with Google Research Football (such as the exploitability of the built-in AI) it is perhaps not as complex as the authors may hope, even though it has presented a challenge to past MARL research. However, this is a broader issue within the MARL community and the authors of this paper cannot fairly be singled out for this.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper introduces a new automatic curriculum learning framework, Skilled Population Curriculum (SPC), for multi-agent reinforcement learning. The algorithm includes three major components: (1) a contextual bandit conditioned by student-policies representation for automatic curriculum learning; (2) An attention-based communication architecture for policies to learn cooperation and behavior skills from distinct tasks with varying numbers of agents; (3) A hierarchical policy architecture to help agents to learn transferable skills between different tasks. The experiments are conducted in Google Research Football environment and Multi-agent Particle environments, which demonstrate the efficiency of the proposed method to IPPO and VACL.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The proposed method is simple yet efficient in the complex Google Research Football environment. 
2. The motivation of the components are also clear and make sence. 
3. In exepriment, several ablation studies demonstrate the effectiveness of the proposed components; 
4. Also, the paper is overall easy to follow to me. The key idea is easy to understand.

Weaknesses:
This paper could benefit from further improvements in the following aspects:

1. It seems that the manuscript introduces various components. While each one appears to be intuitive and rational in isolation, I recommend that the authors should provide a unifying theme or framework to better connect these components. Presently, it appears as if these components are addressing three discrete issues: a) efficient curriculum learning, b) policy architecture development, and c) communication in varying agent scenarios. It is noteworthy that a paper does not necessarily need to devote substantial attention to the innovative aspects of each introduced components. In the case of this paper, the hierarchical structure, for instance, appears to be a standard approach with limited novelty. The authors can highlight how they design efficient automatic curriculum learning in the context of variable agent scenarios.

2. In the section discussing related work (Line 221), the authors mention various curriculum learning mechanisms without a detailed discussion. Could the authors provide an expanded explanation on how these works conduct curriculum learning and how they relate to or differ from the proposed methodology?

3. There is room for improvement in the experiments section. Specific recommendations are detailed in the questions section.

4. The paper could be further polished, for instance:
    - There are several instances where a capital letter follows a comma, such as in line 40: ""For example, In the football environment, when we…""
    - The legend of Figure 6(b) lacks clarity. It would be beneficial if the authors could provide a detailed explanation of what the labels 0,1,2,3 represent.

Limitations:
NAN

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work introduces the Skilled Population Curriculum (SPC), an automated curriculum learning algorithm designed for Curriculum-enhanced Dec-POMDP. The goal of SPC is to enhance the student's performance on target tasks via a sequence of training tasks provided by the teacher. The SPC functions as a nested-HRL method, where the teacher serves as the upper-level policy and is modeled as a contextual multi-armed bandit. At each teacher timestep, the teacher selects a training task from the distribution of bandit actions, with the context derived from the student policy's hidden state. The teacher's bandit is optimized using the student policy's test reward. The lower-level policy, also known as the ""student"", is in itself a hierarchical policy. The high-level policy implements population-invariant communication using a self-attention communication channel to manage messages from a number of agents, and all students share the same low-level policy.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- This paper is well-presented. Figure 1 is well-designed. I can get a good understanding of this paper's method just by reading this figure.
- The algorithm is implemented with Ray RLlib, though the code is not currently available.

Weaknesses:
- **This study seems to be an overcomplicated amalgamation of pre-existing methods.** SPC stacks three layers of hierarchical policies (teacher 1 + student 2), the teacher is modeled as a multi-arm bandit with a fixed output dimension (number of tasks), and the lower-level control policies of the students are shared. The intricacy of this pipeline leads me to question its generalizability and practical applicability.
- **More rigorous comparison with current MARL algorithms, and need benchmark results on SMAC**, which is de facto the most standard benchmark for MARL algorithms. Please consider adding [MAPPO](https://github.com/marlbenchmark/on-policy), [HARL](https://github.com/PKU-MARL/HARL), and their multi-agent communication variant as your baselines.
- Line 236-238, “However, current approaches that extend HRL to multi-agent systems or utilize communication are limited to a fixed number of agents and lack the ability to transfer to different agent counts”, this is an inaccurate claim because it has been done in the ICLR 2022 publication, [*ToM2C*](https://arxiv.org/pdf/2111.09189.pdf), which similarly uses the HRL with a population-invariant multi-agent communication mechanism. AFAIK this cannot be treated as ""communication limited to a fixed number of agents"". Please consider citing this work and changing your statement regarding the previous work.

Limitations:
The limitations of this paper are only briefly mentioned in the last section.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper studies the multi-agent RL problem with sparse reward and a varying number of agents. The authors propose a novel automatic curriculum learning strategy to solve complex cooperation tasks in this setting. Their curriculum strategy involves a teacher component and a student component. The teacher component selects the sequence of training tasks for the student component using the contextual bandit algorithm with predictive representation of the student’s current policy as context. The student component is endowed with a hierarchical skill framework and population-invariant communication. They empirically investigate their proposed strategy in two environments (MPE and GRF).

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper is overall well-written, and the related work is extensively discussed.

The theoretical results in this paper seem correct; I haven’t checked the details of the proofs.

The population invariant communication module is an interesting contribution to dealing with the varying number of agents across tasks. It would be interesting to compare its effectiveness (on its own) against the existing methods to deal with varying numbers of agents [23, 24].

Weaknesses:
I am unsure about the broader applicability of the contextual representation of the student policy using an online clustering algorithm. How much information will be lost in this process for a high-dimensional policy (e.g., that operates on image inputs)?  

Presented experimental results are not sufficient to validate the effectiveness of the proposed curriculum strategy (specifically the teacher component) in complex scenarios, given that in the MPE environment, the impact/necessity of curriculum is negligible.

Limitations:
The paper is of an algorithmic nature and does not have any direct potential negative societal impact.

Rating:
5

Confidence:
4

";0
yaJ4vZPnHX;"REVIEW 
Summary:
This paper considers solving the $H_\infty$ control problem using zero-th-order policy optimization. The main results are sample complexity bounds for both the exact Oracle setting and the model-free setting. Numerical simulations are conducted to demonstrate the effectiveness of the algorithm.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
This paper is well-written. The $H_\infty$ control problem is known to be challenging and the policy optimization results are likely to be of great interest to the learning-in-control community. The sample complexity result, while not necessarily optimal, is the first non-asymptotic result in the literature.

Weaknesses:
(1) Related Work: The related work section could be more structured. I went through [32] to complete the review of this work. It seems that the main technical tools are already developed in [32]. However, [32] does not provide any sample complexity result. What are the major technical challenges (and the ideas used to overcome them) in going beyond the global convergence in [32] to the sample complexity results in this work?

(2) Theorem 3.7 and Theorem 4.2: Usually for high probability bounds, as the tolerance level $v$ decreases, more iterations are needed. However, for Theorem 3.7 and Theorem 4.2, as $v$ decreases, $T$ also decreases, which seems counter-intuitive. Moreover, the bound has a polynomial tail rather than an exponential tail. Is this an artifact of the proof or are exponential tail bounds not achievable?

Limitations:
The sample complexity bounds are not necessarily optimal, which was pointed out by the authors.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper focuses on the structured $H_\infty$ control problem. They provide sample complexity bounds for policy optimization in $H_\infty$ control problem. 
The results are provided for two separate scenarios namely: 
- Exact Oracle Setting  (exact $J(K)$ for any $K$ is available, for the given closed loop system)
- Inexact Oracle Setting (the system matrices are not known)

The theoritical results provide the sample complexity of $H_\infty$ norm estimation.
Finally, the paper provides a few numerical experiments supporting their theoritical results along with comparison to some model-based approaches. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- $H_\infty$ control problem is one of the important setting in linear systems , which is well studied in adaptive control literature but has received less attention in recent learning theory literature unlike the standard LQR setting. The paper highlights the various challenges involved in the analysis of $H_\infty$ control due to non-convexity and non-smoothness.

- The Sample complexity of the $H_\infty$ norm estimation are provided by exploiting the randomized smoothing techniques.



Weaknesses:
- The algorithm relies on access to an oracle. More discussion on the oracle is warranted. 
- The simulation section in the main body (as well as the appendix) of the paper is meager. The presentation quality of plots included can be improved. 
- Authors mention in the appendix that (line 806) that when necessary, one can reinitiate to avoid bad local minima, but how would one know that they are at a 'bad' local minima.


Limitations:
The results rely on access to an oracle.

Social Impact: NA

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies the static output feedback $\mathcal{H}_{\infty}$ control problem. It proposes a derivative-free policy optimization algorithm via randomized smoothing and further provides sample complexity analysis for the cases with exact and inexact zeroth-order oracles. To validate the performance of the new algorithm, the authors also conduct some numerical experiments and compete against the model-based methods in the literature. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. As the authors claim, the proposed algorithm is the first derivative-free policy optimization algorithm for constrained structured $\mathcal{H}_{\infty}$ control problem (there have been some works on the unconstrained setting).
2. The authors also consider the inexact oracle setting.
3. The paper provides both theoretical analysis and numerical experiments.

Weaknesses:
I am not very familiar with the problem studied by this paper and it looks fine to me. However, I feel like the paper is mainly a combination of existing techniques (such as randomized smoothing and gradient sampling with zeroth order feedback). Would the authors highlight any novel techniques they apply in the analysis or explain what makes the problem different from other nonsmooth nonconvex problems such that this paper is not simply A+B?

Limitations:
None.

Rating:
5

Confidence:
2

";1
jOuPR9IH00;"REVIEW 
Summary:
This paper considers variance-weighted least-squared regression for offline RL with general function approximation. Under a uniform data coverage assumption, they show that the proposed algorithm obtains a sub-optimality bound that scales with the $D^2$-divergence of the offline data set, the positive lower-bounded constant of the uniform data coverage, and the complexity of the function class. Their bound obtains the right order when realized in the linear case. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- clear presentation (though some parts can be improved further -- see Weaknesses)
- the obtained result is new and relevant to the offline RL community 


Weaknesses:
- The main weakness is that the uniform data coverage assumption is very strong. In the linear case, this assumption is equivalent to that the behavior policy is exploratory overall dimensions of the linear feature. A question for the authors is that in such a case, why would we even need pessimism?  Pessimism is used when the data coverage is partial thus we become pessimistic about uncertain actions. But when the coverage is uniform, it can eliminate the need for pessimism and we can simply use greedy algorithms. I understand that without such a uniform data coverage assumption, it seems difficult to get a reliable estimation of the variance of the transition kernel and it would be interesting to get rid of this assumption. But if we could not get rid of it yet, the very least expectation is that we need to explain this assumption further, especially regarding where pessimism is really needed with this assumption. 


- Writing can be improved further. For example, the $D^2$-divergence and the definition of the bonus function (Def 4) can be explained and motivated further. The current presentation of these concepts are not very helpful 

- Some claims might be potentially misleading. It's not comfortable to view the proposed algorithm as computationally efficient even in the oracle sense. Specifically, the construction of the bonus function in Definition 4.1 is far from being computationally efficient since it is essentially a constrained optimization over the version space. That said, it is nowhere more computationally efficient than version-space-based algorithms such as the ""Bellman-consistent pessimism"" of Xie et al. 

- Though the main result is new, it appears expected given the already-developed machinery in Argawal et al. 2022 and Xiong et al. 2022. What are the technical challenges in the current problem that the existing techniques cannot resolve? 

- Some minor: PNLSVI is never introduced before used 

Limitations:
Yes 

Rating:
5

Confidence:
5

REVIEW 
Summary:
The paper studies offline RL with non-linear function approximation. The paper is mainly motivated as existing sample complexity guarantees on offline RL algorithms with general function approximation yield suboptimal dependency on the function class complexity, e.g. when the bounds are translated to the linear case. The paper proposes an oracle-efficient algorithm that achieves minimax optimal problem-dependent regret when the bounds are specialized to the linear case. The paper also introduces a new coverage definition.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- The paper appears to be technically sound with some new ideas in the algorithm design and formulation of dataset coverage.
- The approach achieve minimax optimal rate in non-linear function approximation, when bounds are converted to linear.

Weaknesses:
- The main weakness is that the proposed approach either requires uniform coverage or non-linear bonus oracle. The non-linear bonus oracle is a strong requirement and in effect, simply removes the difficulties related to pessimism in offline RL. On the other hand, the uniform coverage assumption is too strong and thus, it is unfair to compare its efficiency to pessimistic offline RL algorithms.
- A clear comparison to prior work is not presented. In particular, there are multiple axes of comparison, such as dependency on $\epsilon$, dependency on function classes, data coverage requirement, type of oracle, computational efficiency/tractability, realizability assumptions, etc. It is difficult to clearly evaluate the results in this paper without such comparisons. For instance, it will be helpful to have a table as well as translating the bounds of the other algorithms into linear case to see in detail. Additionally, there are several pessimistic offline RL algorithms with general function approximation that only require optimization oracles instead of the more difficult bonus oracle, and no comparison with those papers are presented:

Cheng et al. Adversarially trained actor critic for offline reinforcement learning. In International Conference on Machine Learning (pp. 3852-3878). PMLR

Rashidinejad et al. ""Optimal conservative offline rl with general function approximation via augmented lagrangian."" arXiv preprint arXiv:2211.00716 (2022).

Ozdaglar et al. Revisiting the Linear-Programming Framework for Offline RL with General Function Approximation. arXiv preprint arXiv:2212.13861

Zhu et al. Importance Weighted Actor-Critic for Optimal Conservative Offline Reinforcement Learning. arXiv preprint arXiv:2301.12714.

Limitations:
Yes

Rating:
4

Confidence:
3

REVIEW 
Summary:
This paper proposes a pessimistic nonlinear least-squares value iteration algorithm to tackle the offline reinforcement learning problem. The main motivation of the paper is to propose an algorithm that are both computationally efficient and minimax optimal w.r.t. the complexity of nonlinear function class. The proposed pessimism-based algorithm strictly generalizes the existing pessimism-based algorithms for both linear and differentiable function approximation and is oracle efficient. Also, the proposed algorithm is proven to be optimal w.r.t. the function class complexity, closing the gap originated from the previous work on differentiable function approximation.

Soundness:
1

Presentation:
2

Contribution:
1

Strengths:
1) The proposed algorithm is proven to be optimal w.r.t. the complexity of nonlinear function class, closing the gap from the previous work on the differentiable function class and generalizes it to the wider nonlinear function class.
 2) The proposed algorithm is computationally efficient if there exist the efficient oracles for both regression minimization and bonus function optimization/searching.

Weaknesses:
1) The paper's presentation needs some work. For example, the terminology definition is not consistent. The D^2 divergence definition in Definition 3.2 is not consistent with the later terminology of D_F in line 239. The language itself needs some work too. For example, lots of places where it needs 'an', but 'a' is used and vice versa. Please define RL before using it in the abstract. There are also some ambiguities in the definitions that needs clarification in the Question section. 
2) The paper's claimed contribution is a bit exaggerated. Although the proposed algorithm does not need the computationally heavy optimization as previous works in planning phase, it transfers the main computation burden to the Oracle to find the satisfied bonus function, which seems to be a very time-consuming task. It also applies to the claim of being the first statistically optimal algorithm for nonlinear offline RL. Being able to get optimal result in the reduced linear function class does not necessarily mean it's optimal in the broader nonlinear class.
3) Although the considered class is the nonlinear one and general than the previously considered linear or differentiable class, the techniques used in the analysis are nothing new in my opinion, except re-defining the metrics in the nonlinear function class and connect the results together along with additional assumptions.
Overall, I think the paper is well motivated, but given the presentation and the insignificant contribution, it's not ready to be published. 

Limitations:
N/A

Rating:
3

Confidence:
3

";0
xdtBFMAPD2;"REVIEW 
Summary:
The submission proposes an approach to improve model monitoring by rather evaluating changes in explanations instead of input features. The authors provide synthetic examples to justify their method and compare it empirically to existing strategies on tabular datasets. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
 - The paper addresses an important topic as effective model monitoring based on unlabeled data only is a relevant problem.
 - Although rather, simplistic the synthetic examples help to get a rough idea about the potential benefits of explanation monitoring.
 - The authors provide code as well as tutorials on how to apply their method to ensure reproducibility.


Weaknesses:
 - The theoretical analysis is extremely limited such that the overall assumptions under which the proposed method can be expected to yield actual benefits are too vague. Also, the basic notations section seems a bit inflated. 
 - I think the novelty is limited as well. Monitoring feature attributions instead of input data is not new and is already offered by popular ML service providers. See for instance here the functionality implemented by Google (https://cloud.google.com/vertex-ai/docs/model-monitoring/monitor-explainable-ai). I would have also liked to see such an alternative approach to use explanations for monitoring somewhere included in the experiments.
 - The conducted numerical experiments are not sufficient to demonstrate the benefits of the proposed approach. If only considering tabular data I think including more than 3 actual datasets and 4 prediction tasks is necessary to be convincing. This is especially true for methods where rigorous theoretical analysis is challenging. See also the question below for further suggestions. 
 - The evaluation section is hard to follow, and lacks formulation of insights derived from the experimental results, e.g., it is unclear what benefits can be derived from the feature importance in Figure 4. Given the lack of baselines and justification for those explanations, it is also not clear if they represent useful insights into the effect of a distribution shift on the model’s behavior.  Also, Table 1 comes out of nowhere and is not described sufficiently.
 - Given the limited theoretical and empirical investigation the submission does in my opinion not make a significant contribution to the field.


Limitations:
I appreciate the discussion at the end of the paper that hints at some relevant limitations. 

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper introduces a new concept called ""explanation shift"" for detecting shifts in data distributions with the changes of the attribution distributions on machine learning models. The authors argue that current methods for detecting shifts have limitations in identifying changes in model behavior. Explanation shift provides more sensitive and explainable indicators for these changes. The paper also compares the proposed method against other methods for detecting distribution shifts in both synthetic and real datasets.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
+ Leveraging the changes of explanations as a manner of detecting the distribution shift is a novel idea.
+ The authors provide a compreshensive analysis to show the connections between explanation shift and various distribution shifts, which could be helpful for readers to understand how to use explanation shift to detect distribution shift

Weaknesses:
+ The overall presentation is not clear and many key terminologies and notations are not well explained or defined. For example, in Equation (3), what is $x^*$? What is the formal definition of $S(f_{\theta},x)$? What are the definitions of ""sensitivity"" and ""accountability"" which are used as evaluation metrics in Experiments? The lack of clear presentations of these terms makes me extremely hard to understand the key information in this paper
+ Although the authors proposed a new concept called ""explanation shift"", the technical contribution is still very limited. First of all, the method proposed for detecting the explanation shift (i.e., Section 3) is very simple. But the authors failed to justify why this is an effective method from the theoretical perspective by comparing it against other methods. 
+  Some key empirical studies are missing. In Section 5.3, the authors evaluate their methods on some real datasets to detect novel group distribution shift and geopolitical and temporal shift. However, the authors did not perform the same experiments by using baseline methods. Thus it is unclear whether those baseline methods can discover the same types of shifts or not. If yes, then what are the benefits of the proposed method? If not, why are those baseline methods unable to find out those shifts?

Limitations:
Not applicable.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper uses explanation shift as a way to detect different types of distribution shift between the training set and unseen (test) data sets. The method is based on measuring the changes between the explanation provided by an explanation approach such as Shapley values, for the two data sets for a trained model. As such, the two data sets could be statistically similar but appear different from the model’s perspective. Overall, the proposed approach is novel and interesting but the paper needs to be improved. 

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
To the best of my knowledge, this is a novel approach that uses explanation to detection distribution shift. The proposed method is clear and the method seems to be effective in practice.

Weaknesses:
Section 4.1 provides examples where the proposed method works but simple distribution shift evaluation fails. But this does not provide any guarantee whether in general the proposed model is better or not. The same is true for Section 4.3. Section 4.2. provides a disposition but as mentioned by the authors, the prediction shift implies explanation shift, but the opposite is not true. Thus, no conclusion can be ae when there is an explanation shift.  

Even though the authors compared their proposed model with the baselines on the synthetic data set in Section 5.1, they have not done it using any real data sets. The real data set is mainly used to study the sensitivity of the model on the parameters. 

There is lack of consistency in notation used in the paper that makes it more difficult to follow. Notation changes from one section to another, and in some extreme cases from one example to another. Here are some instances:
1-	Val function is defined differently in Equation 1 and 2. 
2-	Equation 3 is not clear and not explained either. What is the expected value is defined on? If it is X, why the notation differs from Equation 2?
3-	There is a sign used in Example 4.2 which is not defined. 


The paper benefits from a round of proof-reading.
Line 143: out approach –> our approach
Line 182: a hard tasks --> a hard task (the sentence that includes this is also not clear and needs explanation)
Line 279: AppendixE.1 --> Appendix E.1


Limitations:
No.

Rating:
5

Confidence:
3

REVIEW 
Summary:
Detecting shifts in data distribution between training and deployment is critical for ensuring models function as intended and operate in their domain of applicability. However, detecting such shifts is challenging. In this paper, the authors propose an approach based on techniques from the explainability literature. They define the concept of explanation shift and introduce an Explanation Shift Detector. They validate their approach on a synthetic data and 4 tabular datasets, demonstrating improved performance over a range of baselines. 

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The paper is well written, the introduction well motivated, and the formalism both precise and easy for the reader to follow. I found the method interesting, and the analysis of explanation shift detailed and informative. I think this work is a meaningful contribution to the literature.  


Weaknesses:
The experiments were only conducted on several, relatively simple, tabular datasets. Demonstrating the method for another modality would strengthen the paper.

Please see Questions below. 


Limitations:
Yes

Rating:
7

Confidence:
3

";0
w91JqNQLwy;"REVIEW 
Summary:
The authors introduce a novel second-order method for sparse phase retrieval. Compared to previous algorithms, it exhibits faster convergence and better recovery. The method leverages sparsity to reduce the size of the linear system that needs to be solved at each iteration in order to determine the approximate Newton direction (reduced from n^3 to s^3), and a second-order approximation of the intensity-based objective.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
This paper presents strong results and a theoretical analysis of the algorithm in both the noisy and noise-free case.

Weaknesses:
The sample complexity required for initialization and refinement is sub-optimal. The experiments are only on toy data.

Limitations:
The authors discuss the limitations of their method.

Rating:
8

Confidence:
4

REVIEW 
Summary:
The authors propose a second-order algorithm based in Newton projection for the sparse phase retrieval algorithm. The proposed algorithm is similar to Hard Thresholding Pursuit, where the free variables (i.e. the support) is first identified by a hard thresholding step, followed by an update on the free variables via a Newton projection step.
As is standard for approaches to phase retrieval, the proposed method first performs an initialisation stage to ensure that the initial guess is sufficiently close to the true signal, then applies the proposed second-order method to obtain global convergence. There is are theoretical results proving quadratic convergence for the proposed method.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The performance show substantial gains compared to previous methods, moreover, it establishes a quadratic convergence rate.


Weaknesses:
This work is incremental compared to HTP of [28]. HTP can already to be interpreted as a second order method. In terms of per-iteration complexity, the proposed method is the same as HTP. The comparison in ‘iteration complexity’ is somewhat unclear because the complexity given in HTP is for exact recovery, whereas the rate given in Table 1 for the proposed method is to obtain accuracy \epsilon — is it just that [28] does not prove a quadratic rate, or do we expect [28] to have worse convergence behaviour in general? Moreover, [28] proves finite convergence for their method, does the proposed method also achieve finite convergence?

In terms of practical performance, the convergence plots show that the proposed method has faster convergence compared to HTP, but the performance for HTP here is worse than the performance reported in [28]. Perhaps it would be useful to replicate the exact experiments in [28] so that a clear comparison can be given? In general, it would be useful to have a discussion on the differences with HTP and an explanation as to why the performance is superior to HTP, given that both are second-order methods. I also had a look at the proof and it is again similar to the proof given in [28], so it would be useful again to have a discussion on the differences and novelty over [28].


Limitations:
Yes

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper focuses on the sparse phase retrieval problem and introduces an efficient second-order algorithm based on Newton‘s method. The algorithm aims to recover sparse signals and offers a quadratic convergence rate while maintaining the same per-iteration computational complexity as first-order methods. Experimental results demonstrate that the proposed algorithm outperforms popular first-order methods in terms of convergence rate and success rate in recovering the true sparse signal.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The authors' algorithm exhibits a lower complexity per iteration and a higher convergence rate compared to popular first-order methods. It is noteworthy that this is the first algorithm to establish a quadratic convergence rate.
2. The experimental results clearly illustrate the superiority of the proposed algorithm.
3. The paper effectively communicates the motivation behind the development of the second-order algorithm and highlights the complexity reduction achieved by restricting Newton's step to a subset of variables.

Weaknesses:
1. The authors mention two prevalent loss functions but do not provide an explanation regarding the difference between these functions in the numerical experiments. It would be beneficial if the authors clearly explain the distinction between the two functions, particularly why the first function is used for initialization and the second one is used in Newton's update.
2. Equation 12 introduces J_{k+1}, which seems to be highly dependent on the choice of S_0, the initial support. This raises concerns about the algorithm's sensitivity to the initial point. It would be valuable for the authors to address this issue and discuss the potential impact of the initial point on the algorithm's performance. 
3. Regarding the overall contribution, this paper focuses on approximating the objective function using a quadratic function, which can be limited. Also, this paper may be interested to only a few people attending this conference. 



Limitations:
N/A

Rating:
3

Confidence:
5

REVIEW 
Summary:
The work proposes a new algorithm for phase retrieval of sparse signals. 
Specifically, it focuses on a faster algorithm targeting quadratic convergence with the same number of measurements that are also needed in other algorithms. A proof of a quadratic convergence rate is established and experments illustrate the benefit also in experiments.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper focuses on aspects in phase retrieval that are often ignored. In particular, a proofable faster convergence rate has not been the focus of other works so far.
It is well-written and easy to follow. 
It may well become a new standard for phase retrieval (or a starting point for other similar algorithms) if other researchers can reproduce the excellent performance.

Weaknesses:
The novelty is limited in the sense that second order algorithms are well known. However, adaptation and convergence proof for the phase retrieval setting are indeed novel and interesting.
It is not clear why this subset of existing algorithms has been used for the experiments.

Limitations:
The authors present no drawbacks of their method compared to the existing algorithms. In particular, the fact that existing algoritms need fewer measurements for refinement but perfom worse in the phase transition Figure 2 is surprising. Eventually, this is an artifact of the restriction to maximally 100 iterations for success (if the initialization is bad, significantly more iterations might be needed and could still make an algorithm successful, although for a significant computational cost). 
It is strange that a faster algorithm is in this sense also more ""robust"".


Rating:
7

Confidence:
3

";0
sW8yGZ4uVJ;"REVIEW 
Summary:
The paper theoretically proves that, for finite-arm bandits with linear function approximation, the global convergence of policy gradient (PG) methods is not dependent on approximation error, but rather, on the ordering properties of the reward representation. The global convergence is achievable for both standard Softmax PG and natural policy gradient (NPG) under linear function approximation. This result is also verified using simple examples and empirical experiments.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
This paper provides a completely new understanding on the convergence of policy gradient method, that is, the global convergence only depends on the specific ordering-based condtions instead of some classical approximation requirements. To my own knowledge, I have not seen such result before. 

This novel insight is also significant. There are many situations where the approximation error could never be sufficiently small. This work provides a key to understand the covnergence of PG-based algorithms. And as the author claims, I agree with that this paper will open a new directions for PG-based methods under function approximation. 

This work is well-written. I can clearly understand which problem this paper is solving; especially, examples given in Section 3 are very helpful to understand the motivation of proposing the ordering-based conditions. This paper also has rigorously defined two new preservation condtions on the ordering, and proved the necessarity or sufficiency for those proposed conditions. 

Weaknesses:
The studied case only contains one state, so it is a simple bandit optimization problem. Though the main result of this paper is inspiring and interesting, this paper lacks of (1) persuading evidence showing that these understandings will also be valid for more general cases, and (2) sufficient discussions on how to generalize this result in a more general Markov decision process. 


Limitations:
This is a theoretical work so there is no negative societal impact.

Rating:
8

Confidence:
3

REVIEW 
Summary:
This work studies the global convergence of standard softmax policy gradient and natural policy gradient for finite-arm bandits with linear function approximation (i.e., considering a log-linear policy). It is shown that the approximation error is not crucial for characterizing global convergence since the latter can be achieved even with non-zero approximation error. Ordering-based conditions are provided instead to guarantee the global convergence of softmax PG and natural PG while the approximation error is non-zero. More precisely, this paper establishes that both NPG and softmax PG converge globally when (a)  for NPG, the optimal action’s rank is preserved by the projection of the true reward onto the space of representable rewards and (b) for softmax PG, there exists a linear function preserving the ranking of actions provided by the true reward function. The case of a linearly realizable reward function is a particular case of the aforementioned reward rank preservation condition. Numerical examples illustrate all these results throughout the paper.  


Soundness:
3

Presentation:
4

Contribution:
4

Strengths:

The main results of this paper provide very interesting insights on the global convergence of some PG methods under the function approximation setting for finite-arm bandits. The paper provides strong and solid contributions.

**(a) Originality**: the paper challenges common existing analysis featuring the approximation error as a natural limit when considering function approximation. Results are new to the best of my knowledge.

**(b) Significance**: as also highlighted by the paper in the conclusion, the results are likely to inspire further important developments for function approximation in the more general setting of MDPs, nonlinear approximation and representation learning. 

**(c) Correctness**: To the best of my knowledge, except for the proof of Theorem 1 which is more involved and may deserve some clarifications in my opinion (see my comments below), the proofs in the appendix are correct, complete and relatively easy to follow. I checked the appendix in details. 

**(d) Clarity and writing**: This paper is very well-written, the structure is clear, the exposition is progressive, the story line is very nice and the illustrations are insightful. This is a solid body of work. 


Weaknesses:
- While the proof of Theorem 2 is very clear to me, some parts of the proof of Theorem 1 (and its sketch l. 266 to 293) deserve some clarifications in my opinion. Please see my comments in the questions section. 

- Comment: The proof of Theorem 2 seems to follow similar lines to the proofs in Khodadadian et al. 2022 even if the latter work does not consider linear function approximation nor does it propose ordering-based conditions (while it applies however to MDPs).  The main difference seems in replacing the  true gap $\Delta$ by the gap induced by the linear approximation $\hat{\Delta}$. For similarities, see for e.g., Appendix C in Khodadadian et al. 2022 for the bandits case or the main part of the paper (Theorem 1, Lemma 1, Proposition 1 p. 3-4 and their proofs). 

Khodadadian, S., Jhunjhunwala, P. R., Varma, S. M., & Maguluri, S. T. (2022). On linear and super-linear convergence of Natural Policy Gradient algorithm. Systems & Control Letters, 164, 105214.

**Minor:**

-  It is not very clear what is $v_2$ in l. 278-279 (especially that there is also $v$ in l. 278) and what is $v_1$ in l. 283 and how do they relate to $v$ in l. 268, I find the notations and the formulation a bit confusing in l. 266-293. 

- Please give the precise reference to Theorem 1 when you reference [8] in lines 116 and 123. 

- $r(a^*)$ introduced in l. 110 does not seem to be defined before (definition of $a^*$ comes later in l. 140).  

- Numerical legends in all the figures are almost invisible in a printed format (although visible when zooming on a computer), it would be nice to increase the size of the numerical characters (and save the figure in pdf format if not already done) to ease the reading. 

- Eq. (27): Why is the inequality strict? This (strict) positivity does not seem to be used anyway. 

- Suggestion: Eq. (102) maybe add $>0$ for clarification. 

- Suggestion: Eq. (149) to Eq. (150) maybe add the splitting sum steps with the order summing exchange to detail a bit more for the convenience of the reader. 

**Typos:** 
- l. 446: $\theta’ \in \mathbb{R}^d$ instead of $\mathbb{R}^K$?

- l. 474: $a \in [K]$ instead of $i \in [a]$. 

- l. 481: Eq. (58) instead of (54).  

- l. 493: Eq. (68) instead of (65). 

- Eq. (97): Eq. (94) instead of (91). 

- Eq. (101): Eq. (97) instead of (95). 

- Eqs. (138) and (139): $\max_a$ instead of $\max_i$. 

- l. 539, Eq. (144): say there exists $\theta_{\zeta} \in [\theta, \theta']$ such that ...




Limitations:
Limitations and opportunities for future work are briefly discussed in the conclusion. 


Rating:
8

Confidence:
5

REVIEW 
Summary:
This paper challenges (arguably) the current best known policy gradient (PG) convergence analysis, which is the conventional approximation error based analysis originally proposed by the seminal work of Agarwal et al. (2021). To this end, the authors consider the finite-arm bandits with log-linear policy and study the conditions of the global convergence of PG and natural policy gradient (NPG). 

**First**, by carefully designing numerical simulations, the authors show that global convergence can be achieved even if the parameterized policy space can not cover the full policy space, or the approximation error is not zero. Consequently, the approximation error is not a key quantity for characterizing global convergence in either algorithm under linear function approximation.

**Second**, the authors establish new conditions of the global convergence of PG and NPG for the same setting, separately. For NPG, the necessary and sufficient condition of the global convergence is whether the projection of the reward vector onto the feature map strictly preserves the top ranking of the optimal action. For PG, the sufficient but not necessary condition of the global convergence is whether there exists a point in the image of the feature map such that it preserves the entire ranking of the reward vector. These conditions are again well supported by numerical simulations.



Agarwal, Alekh, Sham M. Kakade, Jason D. Lee, and Gaurav Mahajan (2021). On the Theory of Policy Gradient Methods: Optimality, Approximation, and Distribution Shift. Journal of Machine Learning Research 22.98, pp. 1–76.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The paper is revolutionary and the results are surprising. This paper may have the same impact on the theoretical RL community as that of Agarwal et al. (2021). The strengths of the paper can be summarized as follows.

- First, the research question itself on the approximation error assumption is important and revolutionary. Challenging the previous pioneering work is always not easy.
- However, the argumentation of the paper is impeccable. Readers will probably be surprised at first, but will quickly be convinced by several simple but sophisticated numerical simulations.
- Not only the authors are able to find negative answers to questions about the role of approximation error for the global convergence of PG methods, they also establish new results characterizing the conditions for the global convergence of PG methods and draw the connection between the new conditions and the approximation error assumption. The novelty of the paper is significant.
- In addition, the paper is very well written. The research question is well formulated. The new conditions are well presented. And the reasoning is detailed with intuitive explanations, figures, many examples and the proof sketch.

I agree with the authors that this work will open many new directions for understanding PG-based methods in the function approximation regime, especially considering the general Markov decision processes (MDPs).

Weaknesses:
Although the paper is well written, I still find some minor points for improvement.

- In the figures, the authors can specify that y-axis is the reward / value function.
- Line 52-53: ""... approximation error ..., diverting attention from feature designs that achieve useful properties beyond small approximation error."" Although one goal of the paper is to claim that approximation error is not necessary, I find this sentence to be a little too dismissive of approximation error. It may leave the reader with the impression that approximation error is rarely useful outside of the tabular case where the approximation error is zero. It is true that, zero approximation error does not fit well with linear function approximation in general, which is the original motivation for the paper. However, when the approximation error is zero, the conventional approximation error based analysis becomes useful. For instance, another interesting case of zero approximation error is the use of neural networks, recently studied by Alfano et al. (2023) in Section 4.2. That is, a sufficiently wide and shallow ReLU network can infinitely approximate the Q-function such that the approximation error is zero. Consequently, their approximation error based analysis leads to a new SOTA sample complexity of PG method under neural network parameterization.
- Line 361-362: ""Extending the results and techniques to general Markov decision processes (MDPs) is another important and challenging next step."" The authors can use an extra page in the revised version to discuss the intuition of possible obstacles to such an extension. See also my question below.

Alfano, Carlo, Rui Yuan, and Patrick Rebeschini (2023). A Novel Framework for Policy Mirror Descent with General Parametrization and Linear Convergence.

Limitations:
The authors have adequately addressed the limitations.

Rating:
9

Confidence:
4

REVIEW 
Summary:
The paper studies softmax policy gradient and natural policy gradient methods for multi-arm bandits problems using linear function approximation. The authors provide examples to illustrate the global convergence of these methods when the standard function approximation error is not zero. To better characterize the global convergence, the authors provide the ordering-based conditions on rewards. Some numerical examples are provided to verify the proposed conditions. 

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
**orginality**

- The studied deterministic policy gradient methods are well-known in the literature. The authors re-revisit the convergence of these methods for multi-arm bandits with linear function approximation. Such convergence hasn't been studied directly. 

- The authors show necessary (and sufficient) conditions for two policy gradient methods to converge in the linear function approximation setting. The analysis generalizes some similar analysis, e.g., reference [18] to linear function approximation. Technical comparisons with the existing analysis are not very clear. 

**quality**

- It seems that the global convergence is abused in some way, since the existing convergence studies in the linear function approximation investigate different sub-optimality gaps, e.g., [4] is based on the regret analysis while [24] utilizes the mirror descent analysis. 

- It is not clear how to interpret 'Approximation error is not a key quantity for characterizing global convergence in either algorithm'. The reference [24] shows that zero approximation error leads to global convergence. Although this result does not show necessity, approximation error is still an important quantity we use in practice.

-  Compared with approximation error, it is less clear how to check optimal action perseveration condition, especially when the action space is large. So, weaknesses haven't been clearly stated. 

**clarity**

- The paper is structured well, but it lacks of technical comparisons with existing results. 

- The visualization is not clearly stated, e.g., axes, error bars, speed. 

**significance**

- Since the function approximation is widely used in reinforcement learning, this work is important. It provides new understandings of policy gradient methods in bandit cases. 

Weaknesses:
- Technical comparisons with existing results are not detailed, sometimes vague. For instance, in line 96 why (4) and (5) for bandits can be used as RL methods; in line 102 which work provides $1/\sqrt{t}$ rate; line 121, why 'insufficiency'; line 152, how to check global convergence; line 191, what of algorithms the quantity must depend on? Please state claims with concrete justifications. 

- Optimal action perseveration generalizes the analysis in reference [18]. It is similar to the gap of optimal action and second optimal action used in literature, e.g., reference [12] and the paper: Regret Analysis of a Markov Policy Gradient Algorithm for Multiarm Bandits. These quantities seem to be known important for global convergence and the authors generalize them to linear function approximation. Therefore, it is important to clarify the connections and position the work in the literature properly. 

- The usefulness of proposed ordering-based conditions is still questionable. First, the generalization to MDPs is not provided. Second, it is not clear how to check such conditions when state/action spaces are large, even infinite. 

- The paper focuses on multi-arm bandits and deterministic policy gradient methods, which have a large gap with reinforcement learning. This work seems to be still in progress and significant effort is needed to generalize this work for serious publication. 

Limitations:
Yes. 

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper considered the problem of global convergence condition of policy gradient (PG) methods with linear function approximation motivated by three observations: i) global convergence under linear function approximation can be achieved without policy or reward realizability; 2) approximation error is not a critical factor for global convergence; and 3) conditions for characterizing global conference should be algorithm-dependent. Based on these observations, the authors developed new ordering-based conditions for global convergence of PG methods: i) For Softmax PG, a sufficient condition for global convergence to occur is that the representation preserves the ranking of the rewards; and ii) For natural PG (NPG), the necessary and sufficient condition of global convergence is that the projection of the reward onto the representation space preserves the optimal action’s rank.


Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
1. This paper develops a new set of global convergence conditions for PG methods with linear function approximation, which advances the state of the art of understanding PG methods (Softmax PG in particular).

2. The proof strategies and algorithm analysis techniques are novel.

3. Motivating examples in this paper are insightful.


Weaknesses:
1. The paper could benefit from constructing a bit more larger-scale experiments.

2. This paper could have some further discussions on the implications of the ordering-based conditions.

Limitations:
N/A

Rating:
7

Confidence:
4

";1
biLgaNKYdB;"REVIEW 
Summary:
The paper introduces a new method called YNN that transforms traditional ANN structures into yoked neural networks, promoting information transfer and improving performance. The authors analyze the existing structural bias of ANN and propose a model YNN to efficiently eliminate such structural bias. In their model, nodes carry out aggregation and transformation of features, and edges determine the flow of information. They further impose auxiliary sparsity constraints to the distribution of connectedness, which promotes the learned structure to focus on critical connections. Finally, based on the optimized structure, they also design a small neural module structure based on the minimum cut technique to reduce the computational burden of the YNN model. The learning process is compatible with the existing networks and different tasks. The obtained quantitative experimental results reflect that the learned connectivity is superior to the traditional NN structure.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. YNN promotes information transfer significantly which helps in improving the performance of the method.
2. The authors propose a model that efficiently eliminates structural bias in ANN.
3. The authors design a small neural module structure based on the minimum cut technique to reduce the computational burden of the YNN model.

Weaknesses:
1. There is a lack of ablation study, e.g., comparing the model performance using different clique size/number of cuts
2. The equations presented in Section 3.3 are excessively complex and challenging to comprehend. The authors have employed ""W"" and ""w"" for too many different variables in their notations, leading to confusion.

Limitations:
N/A

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposed a module called YNN that could exchange the information of the neurons within the same layer. The proposed module can be combined with MLP. The experiments on several small-scale datasets show that their method achieves good performance compared to previous networks.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
- The motivation of this paper is valid and interesting.


Weaknesses:
- I don't think a fundamental difference between the proposed YNN and graph neural networks. This method can be a special case by assigning a fully connect adjacent matrix to a GNN.
- The evaluation is only conducted on small-scale datasets. 

Limitations:
Please refer to the weakness section.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper propose a 'yoked' neural architecture where neurons at the same level are bidirectionally linked. They claim that optimizing this complete graph is superior to current deep neural network architectures that impose a structural bias due to the transfer of knowledge in a way that prevents structural bias. 

Soundness:
2

Presentation:
1

Contribution:
1

Strengths:
* the proposed optimization changes is simple in that it is similar to other methods like DARTS that grow neural networks and regulate their connections, assigning weights to them using ANN optimization algorithms with regularization term 
* it is clear how the forward propagation is done with the addition of the clique nodes that are computed in addition to the regular precursor nodes at each layer

Weaknesses:
* some of the terminology and abbreviations need to be defined / explained in the first appearance in the intro paragraphs (e.g. 'yoked', ANN, and DAG)
* the method of optimization doesn't seem particularly novel, employing both an L1 and L2 term to search for the best architecture. 
* the figure 2 presented does not particularly show much about the method or its justification
* Is it possible to approximate the non-differentiable minimum cut algorithm and absorb it into the training procedure? This would be similar to progressive training methods like http://proceedings.mlr.press/v119/evci20a/evci20a.pdf and other related works)
* please proofread for more typos and such (e.g. 'mata' on l204, other grammatical errors)
* results are on quite toy problems and training with regularization is not necessarily yielding the best results 

Limitations:
Authors do not discuss limitations of their work.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper proposes Yoked Neural Networks (YNN) - an extension of neural networks, which, when calculating the value of a node, in addition to the information from the previous layer of the network, uses information from the nodes on the same layer (i.e. it ""yokes"" nodes from the same layer together). 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The paper describes the approach well, it is clear how it works.

Code has been provided as an attachment, so that it should be reproducible (I have not ran the code or looked at it carefully).

Weaknesses:
(Details about the mentioned weaknesses are given per line, in the field ""Questions"".)

The paper would benefit from describing in more details the contributions of the proposed method. Particularly, across the paper, some strong statements have been used, but they have not been motivated with evidence. 

Overall, the idea and the benefits of using the method needs to be better motivated.

The description of the experiments is not very clear.


Limitations:
No limitations have been addressed. 

Rating:
3

Confidence:
4

REVIEW 
Summary:
In this paper, the authors propose a novel neural network model that exploits a connection between the nodes of a layer. The authors’ goal is to develop a model that overcomes the structural bias posed by the classical layer structure of the NN. To do this they propose to consider the model as a bidirectional complete graph for the nodes of the same level and to define for each layer a clique. 
The authors then test the proposed architecture and compare it with the traditional NN model considering 3 datasets.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper discusses an interesting problem and proposes a novel methodology that seems promising.

Weaknesses:
Overall, the paper is challenging to read, and at times, the concepts being discussed are not adequately introduced, causing difficulty for the reader to comprehend the discussion's progression. Additionally, several critical concepts are unclearly defined.

In the introduction, the authors discuss the neural module without explaining what it is. Even the concept of “yoke”, which is central to the discussion, is not adequately introduced and explained in the context of neural networks.
In the introduction the authors also discuss the impact of the sparsity constraints, also in this case this concept has to be defined and explained to the reader.
In the list of contributions, point 4 says that the authors designed a regularization-based optimization, which at this point of the paper is very difficult to understand. Point 5 of the same list discusses the problem of computational complexity, which also in this case is not discussed before.

Another issue is the experimental campaign where the experimental setting and the metric used to perform comparison are not explained. Indeed the authors use the “variety of nodes” as a metric, but they do not explain why it has to be significant to show the advantage of the proposed approach.
From the tables, 1,2,3 seems the authors fix the number of nodes for the various architectures and train them. In general to me, it does not seem a fair way to compare the models, mainly for 2 reasons: (i) the architectures of the baselines have to be validated (in particular in terms of the number of neurons, but also considering the other hyperparameters of the model and of the optimization algorithm) in order to find the most suitable setting for the task. (ii) the comparison has to consider the number of parameters of the model since the structure of the  YNN will have many more weights than a standard model (fixing the number of neurons).
Even a description of the setting of the three proposed approaches (YNN, YNN&L1 ,YNN&L2) would make it easier for the reader to understand the proposed results.
Finally, a discussion about the computational burden and a comparison with standard NN is missing.
The experimental evaluation and the discussion of the obtained results should be significantly improved and extended

Limitations:
Not applicable

Rating:
3

Confidence:
3

";0
l0zLcLGdcL;"REVIEW 
Summary:
This paper aims to improve previous Universal Domain Adptation (UniDA) methods by further exploting the intra-class discrimination. For that, they propose a Memory-Assisted Sub-Prototype Mining (MemSPM) method. MemSPM learns to retrieve new task-oriented features given the input embedding features, and apply existing UniDA methods to the retrieving features. The paper also proposes an additional reconstruction task for the demonstration to the explainability of its proposed method as the authors claimed. Experiments on four datasets are conducted on three DA settings.

Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
Considering the effect of learning intra-class discrimination for UniDA is indeed an interesting idea to focus on, and such motivation is new in the UniDA community. By exploiting the intra-class structure, the proposed MenSPM is somehow novel to see.

Weaknesses:
Although the motivation from exploiting intra-class structure is interesting to UniDA, the analysis and the evidences to support the effectiveness of such idea is not enough. This is mainly due to the following concerns.

1. Subclasses learning brings additional learning challenge and increases the learning cost to the problem, and not always the case that some classes have obvious subclasses, thus it is hard to say whether forcing subclasses learning would be beneficial to UniDA. To investivage this, I think it should have a solid analysis to the problem.

2. The proposed method introduces too many hyper-parameters to the leanning process, inlcuding $N$, $S$, $K$, $\lambda$, $\lambda_1$, $\lambda_2$, and $\lambda_3$, etc., and there have not sufficient studies to investigate those hyper-parameters for different datasets or tasks. Note that this is important in UniDA since there is no validation set for model selection. Therefore, it is hard to say whether the effectiveness of the method may come from hyper-parameters tunning.

3. Abalation studies are also not enough to understanding the effectiveness of different loss terms in Equation (8). Although improvements have shown when comparing to the DCC method, but to my knowledge with the CLIP models,  a simple baseline of standard training on source data only may already outperform the proposed method. However, this is not compared in the experiments.

4. The results reported in the ResNet50 are meaningless since the proposed method do not run on this backbone. This is also a limitation of the proposed method. 

5. The experiments to verify the effectiveness of the proposed idea only conduct on the DCC method, which is not enough.

The authors claim that the proposed method could make interpretability from Figure 3, but I do not know how it works for the explainability since reconstruction does not imply interpretability. A random noise could also reconstruct the input.

The loss of $\mathcal{L}_{cdd}$ is not illustrated in the paper. It is a bad way to let readers to understand it from other papers as it is not popular. 

Some typos exist in the paper, and please carefully check if some formulas are presented correctly, e.g., Equations (2), (6).

Limitations:
The authors have shown some limitations of the proposd method, but more should consider other that the method itself.

Rating:
3

Confidence:
5

REVIEW 
Summary:
This work proposes to exploit the intrinsic structures for each class, where sub-prototypes are devised to associate domain-common knowledge for universal domain adaptation. Specifically, MemSPM employs a memory module to mine sub-class information, and a corresponding reconstruction module to derive task-oriented representations. Experiments on representative benchmarks are conducted to verify the effectiveness of the proposed approach. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1, This paper is generally well-written and easy to follow, and neat figures are presented to enable a more intuitive understanding. 

2, The motivation for decoupling with subclass structures seems reasonable.

3, The technical details are well explained.  

4, Surpassing previous methods with noticeable margins, justifying its effectiveness.  

Weaknesses:
I think the main drawback of this paper lies in its presentations:

1, Motivations of some designs are not well explained, i.e., why sub-prototypes benefits the universal scenario？ 

2, Some technical details seem missing. 

The details of these concerns are presented in the ‘Questions’ part. 

Minors: 
Page 5 Line 179: missing space ''[17]that''


Limitations:
Yes. 

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper focuses on Universal Domain Adaptation (UniDA), a practical DA setting that does not make any assumptions on the relation between source and target label sets. The goal is to adapt a classifier from source to target domain such that both source and target domains may have their own private classes apart from shared classes. The paper claims that existing UniDA methods overlook the intrinsic structure in the categories, which leads to suboptimal feature learning and adaptation. Hence, they propose memory-assisted sub-prototype mining (MemSPM) that learns sub-prototypes in a memory mechanism to embody the subclasses from the source data. Then, for target samples, weighted sub-prototype sampling is used before passing the embedding to a classifier, which results in reduced domain shift for the embedding. They also propose an adaptive thresholding technique to select relevant sub-prototypes. Finally, they adopt the cycle consistent matching loss objective from DCC [24] along with an auxiliary reconstruction loss for training. They show results on UniDA, Partial DA, and Open-Set DA using standard benchmarks like Office-31, Office-Home, VisDA, and DomainNet.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* The motivating ideas for the approach are interesting and intuitive. Further, the technical contributions are novel as well as effective.

* It is intriguing that the auxiliary reconstruction task provides interpretability, which is usually not possible in existing DA solutions.

* The paper is fairly easy to follow (with the exception of some equations and many typos and grammatical errors, see Weaknesses).

* With their method and the advantages of a CLIP-pretrained ViT model, they achieve large improvements over existing ResNet-based methods. While they also show small improvements over some existing methods using the CLIP-pretrained model, this can serve as a new strong baseline for future UniDA work.

Weaknesses:
* The paper claims that existing UniDA works overlook the internal intrinsic structure in the categories. 
    * However, [W1] aims to resolve the same problem. [W1] proposes to learn lower-level visual primitives that are unaffected by the category shift in the higher-level features. And, in their proposed word-prototype-space, different visual primitives can be shared across domains and classes (including unknown classes).
    * There is a significant overlap in the motivation given by this paper and that of [W1]. Consequently, the high-level conceptual novelty of this paper is overclaimed. However, I do believe that these conceptual ideas are interesting as well as important for UniDA.
    * Please discuss the similarities and differences (both in terms of motivation and the actual approach) of this paper w.r.t. [W1].
    * Another paper with similar conceptual ideas is [W2].

* This paper lacks some mathematical rigor.
    * Eq. 1, 2: $\hat{Z}=W\cdot M$ is shown as matrix multiplication (I assume that it is not element-wise multiplication since dimensions of $W$ and $M$ are different), but the expansion of this matrix multiplication contains an arg-max over the elements of $W$. Then, it does not make sense for the overall computation to be a standard matrix multiplication.
    * Eq. 1, 2: the text mentions that $s_i$ is the index of sub-prototypes in the $i^\text{th}$ item but Eq. 2 implies that $s_i$ is a particular dimension found with arg-max. This seems contradictory and is confusing.
    * Eq. 2: Use $\mathop{\arg\max}_{j}$ instead of using `dim=1` since it is a mathematical equation and not the code implementation.
    * Eq. 5: It is unclear which dimension is used for top-$k$
    * Eq. 6: It should be $\max(... , 0)$ instead of just $\max(...)$.

* The requirement of a CLIP-pretrained backbone is very restrictive since the method cannot be extended to other settings (like medical imaging) where the CLIP-pretraining may be suboptimal. While the paper shows comparisons where prior methods use the CLIP-pretrained model, it should also show comparisons when starting from a random initialization as well as the more widely used ImageNet initialization.
    * The paper claims that a CLIP backbone is needed to retrieve sub-prototypes in early iterations. Why not start retrieving sub-prototypes after a few epochs of normal training?

* L135: “eliminates the domain-specific information from the target domain”. This is a very strong claim which does not seem to be backed by evidence. Performing “domain alignment” is not the same as “eliminating” domain-specific information. Further, as we can see from Fig. 3, the sub-prototypes seem to be retaining domain-specific information.

* There are no sensitivity analyses for the several loss-balancing hyperparameters $\lambda_1, \lambda_2, \lambda_3$ (not even in the Supplementary). While the paper claims to have borrowed them from DCC, this approach is vastly different from DCC, and we need to check for sensitivity to these hyperparameters. Further, DCC does not have a reconstruction loss, so it is unclear how that hyperparameter is selected.

* There is no ablation study for the adaptive threshold $\lambda$. It should be compared to various fixed thresholds and the value of the adaptive threshold should also be plotted over the course of training to obtain more insights into its working.

* Other UniDA works, like OVANet [40] and [W1], study the sensitivity of their methods to the degree of openness (i.e. the number of shared/private classes) which changes the difficulty of the UniDA problem. This analysis is missing in this paper. This should be shown for a better understanding of the capabilities of the proposed method.

* Some more related work [W3-W4] on Open-Set DA and UniDA (apart from [W1, W2]) that is not discussed in this paper.

* Minor problems (typos):
    * L53: “adaption” → “adaptation”
    * L59: “shifts” → “shift”
    * L92: use `unknown’ i.e. use a backquote in LaTeX for it to properly render the opened and closed quotes like in L102. 
    * L119: use math-mode for K in top-$K$.
    * L124: “varies” → “vary”
    * L126, 179: add space between text and \cite{...}
    * L134: “differenciates $\hat{Z}$ with” → “differentiates $\hat{Z}$ from”
    * L151: “max” → “maximum”
    * L166: “only the $K$” → “only the top-$K$”
    * L181: “$max$” → “$\max$”
    * L244: “fellow” → “following”

* Minor problems (grammatical errors):
    * L32: “aims” → “aiming”
    * L40: “Since such kind” → “Since this type”
    * L41: “almost happens in all the” → “occurs in almost all of the”
    * L59: “embedding give into” → “embedding is passed to” 
    * L125: “sometimes is” → “is sometimes”

### References

[W1] Kundu et al., “Subsidiary Prototype Alignment for Universal Domain Adaptation”, NeurIPS22

[W2] Liu et al., “PSDC: A Prototype-Based Shared-Dummy Classifier Model for Open-Set Domain Adaptation”, IEEE Transactions on Cybernetics, Dec. 2022

[W3] Chen et al., “Evidential Neighborhood Contrastive Learning for Universal Domain Adaptation”, AAAI22

[W4] Garg et al., “Domain Adaptation under Open Set Label Shift”, NeurIPS22

Limitations:
I appreciate that the paper provides both limitations and broader societal impact discussions in the Supplementary.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper proposes a Memory-Assisted Sub-Prototype Mining (MemSPM) method that can learn the differences between samples belonging to the same category and mine sub-classes when there exists significant concept shift between them.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
The writing of the article is very good. Graphical expressions such as t-SNE are very clear. The method have achieved relatively high classification H-score.

Weaknesses:
Some training details need to be explained, such as the selection of hyperparameters. How to adjust the N, S and lambda, and what criteria are based on? If it is based on the final experimental effect, it also indirectly depends on the label information of the target domain.
The scalability of the method is relatively poor. If the data set is large and there are many categories, will there be many prototypes required, and how will the method perform? It is crucial to have the Domainnet dataset in the experiments.

Limitations:
This paper has no limitation sections.

Rating:
5

Confidence:
5

REVIEW 
Summary:
This work addresses the problem of universal domain adaptation by focusing on the intra-class structure within categories, which is often overlooked by existing methods.

The main contribution is the proposed Memory-Assisted Sub-Prototype Mining (MemSPM) method, which learns the differences between samples belonging to the same category and mines sub-classes in the presence of significant concept shift. By doing so, the model achieves a more reasonable feature space that enhances transferability and reflects inherent differences among samples.

Experimental evaluation demonstrates the effectiveness of MemSPM in various scenarios, achieving state-of-the-art performance on four benchmarks in most cases.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
S1 : The primary contribution of this work is the introduction of sub-prototypes, learned from samples within the same category but exhibiting significant concept shift.   The utilization of sub-prototypes allows for a more fine-grained adaptation process, which is an intuitive and an interesting idea.  The ablation experiment Figure 3 (graph), supports the notion that mining sub-prototypes is indeed advantageous, as increasing the number of sub-prototypes (S) leads to a substantial performance improvement, from approximately 62% (with one sub-prototype per category) to around 80% (with 40 sub-prototypes per category). 

S2: The results presented in Table 2 and Table 3 demonstrate significant performance improvements compared to previous works, with increases of +4.5% and +6.4% in H-score on DomainNet and Office-31 datasets for UniDA scenario. Additionally, there is a +1.6% improvement in H-score on the Office-Home dataset. It should be noted that the comparisons are not entirely apples-to-apples, as discussed in the weaknesses section.

Weaknesses:
W1: The utilization of CLIP-based embedding as mentioned in line 126 offers semantic capabilities that generalize across various domains (as shown by works such as [1, 2, ..] that build on top of CLIP). However, the importance of using CLIP-based embedding is not clearly demonstrated in the ablation analysis. A comparison between CLIP-based embedding, learned embedding (without pre-training), and ViT-B/16 (pre-trained on ImageNet) would provide valuable insights. Additionally, the lack of utilization of CLIP's semantic capabilities in prior works raises concerns about the apples-to-apples comparison of the results presented in Table 2 and Table 3.

W2: From the experiment section, the impact of different losses, such as cross-entropy (L_ce), domain alignment loss (L_cdd), and auxiliary reconstruction task (L_rec), on model performance is not clearly explained in the experiment section. Understanding the contribution of each loss would enhance the understanding of the paper.

W3: The sensitivity of hyperparameters across different scenarios, such as Open-Set Domain Adaptation (OSDA) and UniDA, is not adequately addressed in this section. Investigating the sensitivity of hyperparameters would provide valuable insights into their impact on model performance.

W4: Section 3.3.3 discusses the ""Adaptive Threshold Technique for More Efficient Memory,"" but there is a lack of experimental details showcasing the memory efficiency of this technique. Without such evidence, it becomes challenging to fully appreciate the technical contribution.

W5: While the motivation and the main idea of mining sub-prototypes are novel, it is worth noting that memory-based prototype mining was explored earlier in works like [3]. This observation slightly diminishes the overall technical contribution..  

W6: Supplementary material Figure 1 reveals that a significant portion (>60%) of the sub-prototype visualizations are not interpretable. This undermines the contribution of interpretability in this work. 
[1] Rinon Gal and Or Patashnik and Haggai Maron and Gal Chechik and Daniel Cohen-Or StyleGAN-NADA: CLIP-Guided Domain Adaptation of Image Generators, ACM Transactions on Graphics
[2] Boyi Li, Kilian Q. Weinberger, Serge Belongie, Vladlen Koltun, René Ranftl, Language-driven Semantic Segmentation, ICLR 2022
[3]Tarun Kalluri , Astuti Sharma, Manmohan Chandraker.\ MemSAC: Memory Augmented Sample Consistency for Large Scale Domain Adaptation, ECCV 2022

Limitations:
A notable limitation of the study is the lack of clarity regarding the contribution of various components of the proposed method to the overall performance. Specifically, the impact of CLIP-based embedding, which has demonstrated generalizable capabilities even in zero-shot scenarios across domains, needs to be thoroughly understood to fully appreciate the proposed components. Gaining insights into the individual contributions of different components would provide a deeper understanding of their influence on the overall performance. Further investigations or additional analyses focusing on these aspects would enhance the comprehensiveness and rigor of the study.

Rating:
6

Confidence:
5

";0
qVMPXrX4FR;"REVIEW 
Summary:
The paper extends the CrossBeam method with synthesizing intermediate lambda
functions to solve the programming by example task. The authors introduce the
Merge operator to construct new lambda functions by choosing an operator from
the DSL, and its arguments from existing terms (variables or lambda
functions). The inputs to the Merge operator are predicted by the neural model,
so the lambda functions are built step-by-step bottom-up, similarly to the whole
synthesized program.

Lambda functions can not be executed on the input/ouput examples so they are
executed on hardcoded canonical argument tuples instead and are encoded using
property signatures computed on the results, the expected output of the program,
and the arguments of the lambda function.


Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
As the authors claim and also as far as I know this is the first neural search
method to synthesize general-purpose (not hardcoded) lambda functions, which has
been an open problem for years. DreamCoder (cited in the Related Work) can also
synthesize lambda functions but it does so by finding common program fragments
in synthesized programs in a Bayesian framework.

Weaknesses:
The paper - understandably - refers to the CrossBeam paper many times. It
contains a summary of CrossBeam in lines 100-121, but reading that paper still
helped a lot to undertand this paper. Also, I think that Figure 2 from the
CrossBeam paper should be included as Section 3.3 talks about parts of that
Figure.

It would be good to include CrossBeam without lambda functions as a baseline for
the evaluations; currently it is hard to know how much of the improvement is due
to the lambda functions.

Part 3.1 could be clearer:
- the example is at the end of the section, maybe an example-first approach
  could be better
- Merge ensures that there are no free variables and also unifies the arguments.
  When reading through the section it seems from line 128 to 147 that the only
  criterion we need is that we have no free variables and that alone ensures the
  unification of the arguments.
- I'm not sure these are correct:
  - line 157 says that Merge runs the function $f$,
  - line 160 says that $a_k(i_k)$ evaluates to a concrete value, I think it
    should be an expression

line 217 says that $S$ contains variable tokens, but I think it also contains
lambda expressions.

I'm not sure how to intepret line 223: ""an embedding of the weight of this value"".

I couldn't find which LLM the authors used as a baseline, it should be cited.

I couldn't find the range of the inputs and outputs, it would be good to include
them.



Limitations:
Limitations are addressed at the end of the Results, maybe they could have their
own section.


Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper presents LAMBDABEAM, a nn-based search method for program synthesis which is built upon CROSSBEAM and can handle lambda functions and higher-order functions. Specifically, to build lambda terms, LAMBDABEAM enforces that every term constructed during search has no free variables by introducing a novel operator called MERGE. Furthermore, to learn lambda expressions, LAMBDABEAM constructs a new generalization of property signatures to represent lambda expressions. The paper shows that LAMBDABEAM outperforms existing techniques in the integer list manipulation domain (a modified DeepCoder).

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. This addresses a meaningful problem, that is how to search for lambdas and higher-order functions which would enable the synthesis of arbitrary looping computations and extend the boundary of neural program synthesis.
2. The experiment result is promising.
3. The writing is clear.
4. In the current era dominated by LLMs in the field of program synthesis and code generation, this paper makes a good attempt towards small and meaningful works. I believe that this type of work and LLMs-related works will inspire and complement each other.

Weaknesses:
1. Placing a figure that illustrates the LAMBDABEAM Model architecture would be better. Although the design largely follows CROSSBEAM, a LAMBDABEAM figure is necessary for showing the differences and for readers unfamiliar with CROSSBEAM.
2. Experimental settings are somewhat confusing. For example, what causes the differing number of I/O examples for list output and integer output in the hand-written?  What is the name of the pre-trained LLM since the performance gap among different LLMs on program synthesis is significant.  Can the authors further explain these settings?
3. Can the authors fine-tune the LLM on the proposed DSL which might be a better comparison?

Limitations:
The authors have discussed the limitations in the results part.

Rating:
6

Confidence:
4

REVIEW 
Summary:
In this work the authors introduce LambdaBeam a method crafted to explicitly handle lambda functions and higher-order functions for neurally guided program synthesis. Towards this goal the authors first introduce a method to represent lambda functions which enables variable order independent canonical representation, and eases creation of lambda functions by merging other lambda functions. Then, the authors adapt CrossBeam a pre-exiting method for program synthesis to synthesize lambda functions, while also employing property signatures to represent lambda functions. When deployed on integer list manipulation tasks, Lambda beam surpasses other competitive baselines, in terms of both speed and success rate.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
### Originality 
Previous works do not explicitly model lambda functions, or learn how to compose them. This is an original contribution of the paper.

### Quality & Clarity
The paper at a paragraph level is well written.

### Significance
The approach towards representation of lambda functions is a interesting, useful and novel contribution that may be useful for many future program synthesis approaches. 
Furthermore, the approach is able to beat strong baselines such as lambda^2 an off-the-shelf program synthesis tool, and a 60B parameter large language model.

Weaknesses:
### Weaknesses

1. I think the paper does not clearly justify *why* prior works cannot model lambda functions or higher-order functions. Particularly, it states Programming by examples (PBE) demands a more systematic search strategy but its unclear why that is the case. The paper also doubles down on the belief that other methods *cannot* synthesize programs with arbitrary looping computation, but its unclear why that is the case. For example, large scale models (GPT 3.5) can indeed produce programs with higher-order functions and lambda functions. Its unclear why the paper strongly posits that other methods cannot do this.

2. The paper is not easy to understand and seems to depend on the reader being familiar with CrossBeam> On that note, LambdaBeam strongly depends on CrossBeam which is a small drawback as well (though the other contribution - representation of lambda functions is a general and useful contribution).

 

Limitations:
The paper does not discuss limitation or potential negative societal impact. Adding information regarding both these aspects in the appendix might further improve the paper.

Rating:
7

Confidence:
2

REVIEW 
Summary:
This paper presents a method for training a neural module to guide a search-based program synthesis procedure that supports lambda functions. This is accomplished by leveraging the existing technique of property signatures, which essentially represent program constructs using a hand-designed vector of features. The authors design features for representing lambda functions, including evaluating the lambda function on a hardcoded set of inputs. This allows lambda functions to be incorporated in a prior bottom up program search technique called CrossBeam, and they introduce a new Merge operator to build lambda terms from the bottom up. The results on a modified version of the DeepCoder benchmark show their approach outperforms several strong baselines, including symbolic search and an LLM fine-tuned on Python.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Representing and synthesizing (lambda) functions is a significant step forward for neural program synthesis. Although the individual techniques are mostly prior art, I consider the combination of techniques to be novel.

The writing is generally clear, though I found the implementation details to be quite sparse in places (see Weaknesses)

Weaknesses:
The biggest weakness is that the approach is tested on only one synthetic dataset and also relies on hand designed features. Furthermore, the lambda functions only contain 2 variables. It remains to be seen if this approach could scale to more realistic datasets with more data types and more complex functions.

Additionally, there are almost no examples in the paper. At the very least, the appendix should include some examples of the synthesis dataset as well as programs synthesized by LambdaBeam.

Finally, many of the implementation details are not listed out fully, which impedes reproducibility (e.g., the architecture of the models) and in severe cases, the reader's ability to contextualize the results (e.g., a list of the property signatures used and the evaluation tasks).

Limitations:
The authors should also address the extent to which the property signatures are tailored to the DSL, and the implications for broader applicability / scalability.

Rating:
7

Confidence:
4

";1
HF6bnhfSqH;"REVIEW 
Summary:
This paper explores the efficiency of training parameterized quantum models, from the perspective of backpropagation scaling. By leveraging some recent developments in shadow tomography and accessing multiple copies of a quantum state, the authors propose an algorithm that matches backpropagation scaling in quantum resources and reduces additional classical computational costs. The results provide valuable insights into the reusability of quantum information and the results are potentially meaningful for the future of quantum machine learning.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper investigates a timely and relevant topic in quantum machine learning, comparing the efficiency of training parameterized quantum models to classical neural networks.
- The authors leverage recent developments in shadow tomography, providing a novel approach to study a meaningful problem on quantum neural networks.
- The proposed algorithm matches backpropagation scaling in quantum resources and reduces classical auxiliary computational costs.
- The angle of this work to study quantum neural networks is novel.

Weaknesses:
- The primary analysis is limited to quantum neural networks based on variational quantum circuits, which restricts the scope of the paper as many other types of quantum neural networks exist.
- The application of the results to general quantum machine learning algorithms is not convincingly demonstrated.
- The paper lacks a clear and well-motivated example demonstrating the application of the proposed methods, making it difficult to assess its practical implications and usefulness.

Limitations:
NA.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper explores whether parameterized quantum models can achieve comparable training efficiency to classical neural networks. From the perspective of reusing quantum information, the paper demonstrates that achieving backpropagation scaling in quantum models is not feasible without access to multiple copies of a state. With access to multi-copies assumption, the authors propose an algorithm that achieves backpropagation scaling using gentle measurement and online learning while reducing classical auxiliary computational costs. These findings shed light on reusing quantum information for the challenges of training large quantum models.


Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper investigates the backpropagation in quantum models which is interesting and of general interest to the community of QML. It combines online learning and shadow tomography to achieve $O(polylog(M))$ sample complexity for gradient estimation. 

Weaknesses:
1. Even though the proposed method achieves $O(polylog(M))$ of sample complexity, it also requires exponential classical resources which is not practical for handling a large system.
2. It only provides the theoretical analysis and does not give some proof-of-principle numerics.
3. Some necessary details and the related brief introduction of the proposed methods should be listed in the manuscript instead of supplementary.


Limitations:
 

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper studies the scaling of computing the gradient of a quantum neural network. While in the classical case we can use backpropagation, which gives the same linear scaling for computing the gradient and the forward pass, in the quantum case, we would naively have to run a circuit for each component of the gradient, leading to a squared complexity in the number of parameters, which prevents studying quantum models with large number $M$ of parameters.
The authors formulate this problem in the language of shadow tomography and apply ideas from that field to the problem at hand.
This shows that while an $M\log M$ scaling is possible using polylog copies of the input state. This comes at a drawback of classical cost that scales as $2^n$ with $n$ the number of qubits. Resolving this exponential scaling would resolve some open problems in shadow tomography.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- Relevant problem in quantum ML.
- Connection with shadow tomography and application to scaling of gradient computation is new and can lead to new ways to think about the problem
- Rigorous statements supporting scaling 
- Well written paper

Weaknesses:
- The paper relies on quantum information concepts that are not necessarily familiar with the ML audience at the conference.
- When talking about memory requirements of backprop in classical neural networks, one needs to store activations for reverse mode autodiff. This leads to memory that scales with the number of layers, while in the quantum case by analogy the number of qubits does not scale with the number of layers. The authors could comment about this.
- I was confused by Prop. 3: is the proof considering the case of a number of parameters $4^n$? I am not sure what we learn from this example since it does not seem to be part of the quantum neural networks we would like to train.
- The classical scaling as $2^n$ required for the proposed algorithm restricts a lot the class of problem for which this protocol can be useful. 


Limitations:
- As the authors say several time, the main limitation is that their algorithm come with a classical exponential scaling that limits its applicability. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors go over the backpropagation scheme for both classical and quantum machine learning methods. They also propose a novel quantum backpropagation algorithm based on quantum shadow tomography to reuse information and reduce the time complexity. 

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
1. This paper provides a link between quantum backpropagation and quantum shadow tomography, which are both important in quantum computing.
2. This paper provides a thorough background check on quantum backpropagation, information reuse scheme in QST, and backpropagation scaling problem.
3. This paper is technically sound.

Weaknesses:
1. This paper is more like a report paper than a research paper to me, since the main contribution is to discuss in detail how reusing information can benefit quantum backpropagation, and the proposed algorithm seems quite trivial.

Limitations:
The major limitation is whether this paper fits the scope of the research paper in NeurIPS. 

Rating:
7

Confidence:
4

";1
uJ3qNIsDGF;"REVIEW 
Summary:
The authors explore the output space of vision models. Specifically, they propose an algorithm (Level Set Traversal, LST) which, starting from a ""source"" input image transforms it into an image that looks completely different (e.g. an image from a different class) while still confidently predicting the ""source"" class. This proves that neural networks have ""blind spots"" they further analyze the behavior of networks, and show that the paths between these inputs are connected.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
Originality: the paper improves upon previous work in the area, which only applied to very specific network architectures (invertible resnets). In contrast, the novel LST algorithm can be applied to virtually any network.

Quality: Both the theoretical and the empirical work appear solid to me.

Clarity: I was not able to follow all the math in detail, but was nonetheless able to follow along nicely.

Significance: I think this paper is a solid contribution to the field. It provides both some theoretical analysis of the output space of vision models, as well as an algorithm for producing ""blind spot"" examples that is easy to understand and implement, and seems to work well.

Weaknesses:
* The authors state that the algorithm takes 200-400 iterations for each input image. To put this into context it would be nice if the authors could give a wall clock time estimate on how long this would take.


Limitations:
The authors have adequately discusses limitations.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The paper studies the under-sensitivity of such deep neural networks, where it is possible to find large perturbations in the input space (such as transforming one image to another) without significant changes to the activations/predictions. Towards this goal, the paper proposes Level Set Traversal, LST, which adds perturbations in an incremental manner to move along regions orthogonal to the gradient (w.r.t input) space computed on the classification loss. Evaluations are done on ImageNet and CIFAR-10, and results show that LST is able to find not only large perturbations that continue to yield high confidence on the source class, but a path of high confidence from the source to the target. Experiments also show that the convex hull formed by a source image and a pair of LST output images for two targets enclose a region of high-confidence for adversarially trained models, unlike non-adversarially trained ones.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
- The explanation and intuition of the method is very clear, and it also appears to be highly effective in finding an entire path of high confidence/similar loss from two highly visually different images.
- Theoretical analysis presented in Sec 4 is also helpful and insightful.
- Additional experiments on CIFAR-10 and ablation studies in the supplementary material are also appreciated.

Weaknesses:
1) The experiment details mention 1000 images from ImageNet are chosen as the source images, but only 5 target images (the ones in the figure) are used. It is difficult to tell whether the results are overfitted to the 5 target images. More extensive evaluation can be done, for example simply randomly choosing source-target pairs from the 1000 chosen images.

2) I do not see any metric measuring the confidence of the output of LST alone. This metric is highly important especially for Table 1, to show that LST can both jointly optimize for minimal distance from target image as well as preserving confidence. Measuring path confidence averages the results across the entire path is not sufficient since the confidence result can be biased by points closer to the source image and hence more likely to exhibit high confidence. 

3) Measurement of computational cost (wall-clock time) of the LST algorithm might be useful.

Limitations:
Limitations are discussed.

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors propose a new adversarial attack on the discriminative vision models called Level Set Traversal (LST). Contrary to the previous attacks, this new algorithm exploits the orthogonal component of the network's gradient to produce samples that can bypass existing adversarially-trained classification networks.

Moreover, the authors showcase that the samples obtained using this algorithm lie on the star-shaped manifold of high-confidence predictions. This is new compared to the previous adversarial attacks, which typically yield samples not connected via high-confidence linear paths.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
+ The presentation of the paper is great.

+ The motivation behind the proposed method is clear. I also like that the paper is theoretically driven and includes experimentally confirmed results on real-world datasets.

+ To me, the results look really convincing and exciting. I was surprised at the existence of the linearly interconnected sets of adversarial examples, even in adversarially trained models.

+ The ablation study of the method is quite extensive and includes robustness tests for multiple hyperparameters.

Weaknesses:
- I would suggest the authors try and make connections to the area of model ensembling, where works such as [1] showcased the existence of piecewise-linear paths with low training error in the space of the model weights.

- I also suggest including section 1 of the appendix in the main paper, as well as extending Figure 4 to include the visualization for multiple attack types. Without reading the appendix, the advantage of the proposed algorithm over previous methods remains unclear. Extended proof of Lemma 1, in my opinion, could be instead moved to the appendix, as it breaks the flow of the text.

- The authors have only validated their approach against one type of adversarial defense for each proposed network. Extending such evaluation would benefit the community and provide a benchmark for future works.

[1] Loss Surfaces, Mode Connectivity, and Fast Ensembling of DNNs, NeurIPS 2018

Limitations:
The limitations were adequately addressed.

Rating:
8

Confidence:
3

REVIEW 
Summary:
This paper introduces the idea of level sets for image classification models. Image classification models are said to be ‘under-sensitive’ when two visually distinct images produce the same output (class). The authors propose a method to compute ‘equi-cconfidence’ level sets such that two images belonging to this set produce the same output when passed through a classifier. 

The Level Set Traversal (LST) algorithm takes a source image and target image (visually distinct from the source, different class) and produces an image visually similar to the target while maintaining the output prediction of the source image. This is done by iteratively updating the source image in the direction perpendicular to the gradient. The paper then goes on to show (theoretically) the behavior or level sets in some basic ML settings (linear classifier, rely neural networks etc).

Additionally, the authors also highlight the complimentary nature of adversarial examples and level sets: Adversarial examples try to change model output while keeping human predictions the same while levels sets try to keep model output the same when human predictions differ.
Experiments are done on ResNet-50 and DeiT on ImageNet. Empirical results show adversarially trained models exacerbate the problem of under sensitivity compared to vanilla models.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The LST algorithm is the main contribution of the paper and is a novel way to calculate equity-confidence sets given a source-target pair.
2. The paper is well written and concepts are introduced in an easy to understand fashion.
3. Significance: Understanding what causes under-sensitivity in a model is a major open problem.
4. I really like the explanation of the under-sensitivity of adversarial models using the LST algorithm. I think that is the major insight from this paper (apart from the algorithm itself of course).

Overall, I really like the idea and the execution of the paper so I'm conflicted whether to recommend acceptance  (see my comments below). I'm looking forward to hearing from the authors and seeing other reviewers' comments.

Weaknesses:
1. I think the main weakness of the paper is the lack of analysis around the level set found for a particular source-target pair. For example, in the adversarial example setting, adversarial examples don’t need to be w.r.t. some target, they just need to change the label with a perturbation. In this paper’s case, the level set itself depends on a target label. This isn’t necessarily an issue on it’s own, however, it is difficult to understand what showing the mere existence of a level set is supposed to show about the under-sensitivity of the model. Is there something about the level set we've found that tells us something about the under-sensitivity of the model? Other questions: How does choice of target affect the level set? Is there a ’size’ for a level set? Are level sets from source -> target and target -> source symmetric?
2. Related to presentation: I think including a figure showing images along a path from source to the final image generated by the level set algorithm would be useful to understand what exactly is within a level set. This figure could potentially replace Figure 3 which, in my opinion, does not add anything new (it shows the same thing as Figure 2 except a change in architecture)

Limitations:
1. (Relatively) Large computational budget required to calculate images close to the target via LST. (Mentioned by the authors)
2. Level Sets are a property of target class in addition to source class (unlike adversarial examples)

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors aim to systematically study specific shortcomings (called blind spots) of vision models caused by their under-sensitivity.
For this purpose, they devise an algorithm that given an arbitrary pair of images (called source and target) can produce inputs that result in the same prediction output as the source image despite being ""perceptually"" similar to the target image (as quantified using LPIPS).
The authors study the geometry of the generated inputs and compare it under different training criteria (e.g. standard vs. adversarially robust models).


Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- Interesting analysis that can potentially shed light into inherent shortcomings of vision models.
- The authors provide their source code

Update after reubttal
------------------------
I would like to thank the authors for their elaborate response to the review remarks. My remarks are largely addressed in these responses, and I am hence leaning for accepting this contribution. In that case, I urge the authors to highlight the significance of their work and its utility to diagnose the sensitivity issues in adversarial training upfront. The manuscript is heavy on technical jargon that, while important, could be simplified in the main text and presented in the appendix.
As an additional suggestion, the following workshop at NeurIPS '23 would be a great fit to present some of the theoretical parts / foundations of this work https://www.neurreps.org/

Weaknesses:
- The contribution is rather limited. It is unclear what actionable insights we can gain using the proposed level-set analysis. There were no sufficient findings that can inform their design and training of vision models.
- The observations with respect to the ""geometry of blind spots"" are highly anecdotal 
- The novelty is rather limited. A wide variety of methods have been devised to minimally perturb an image in order to fool the model to make a specific prediction. In the interpretability domain, various methods have meaningfully utilized minimal perturbation for the purpose of visualizing which image areas are most relevant for the input. In fact, similar perturbations are used in Integrated Gradients (Sundararajan et al. 2017), however, for a tangible application (feature attribution). See also the work by Wagner et al:
""Interpretable and Fine-Grained Visual Explanations for Convolutional Neural Networks"" (CVPR '19)

Minor: There were frequent language issues. Below are the ones I noted:
- sensitivty
- dimnesional
- upto human expectations => unclear (did you mean, compared with?)
- phenonmenon
- atttacks
- near-neighbour training images => nearest-neighbour images?
- simalar



Limitations:
Partially discussed 

Rating:
6

Confidence:
4

";1
W5Clq1bSrR;"REVIEW 
Summary:
The paper presents a theoretical study of the generalization properties of training when the training set is augmented with artificially generated data. The main result of the paper is a theorem which bounds the generalization error by two terms – one representing the divergence between the original training distribution and the augmented distribution, and one representing the generalization error of the mixed distribution. The authors presents two empirical contributions. Firstly they study a gaussian mixture model with synthetic data and find that their theoretical predictions match the measurements. Secondly they consider Resnets trained on cifar and find that generative models are useful when augmentations are not used, and that diffusion models but not GANs are useful with augmentations.


EDIT: increased confidence from 2 to 3 after clarifications from the authors.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:

* The topic is topical and potentially impactful.
* The writing is very good.
* The results of the theorem are natural and intuitive.


Weaknesses:
* It seems like a major assumption is that “because the distribution learned by the generative model is dependent on the sampled train set”. In practice, I don’t think this is necessarily true. If this assumption is removed, the theoretical results might be much easier to derive.
* Gaussian mixture models are not really used in practice, so it’s not a very interesting experiment.
* It doesn’t seem like the theoretical results will apply to deep neural networks.


Limitations:
n/a

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper provides a theoretical analysis of the stability bound for generative data augmentation. The authors provide empirical evidence to validate the proposed theory on bGMM and GNAs.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
Data augmentation plays an important role in deep learning. The paper provides a theoretical analysis of using generative models for data augmentation. Understanding generative data augmentation can potentially benefit machine learning tasks in low-data conditions.

Weaknesses:
- In Section 4.2, the authors use standard augmentation to approximate CIFAR-10 with larger $m_S$. The comparison of GDA on CIFAR-10 and augmented CIFAR-10 is unfair as the standard augmentations induce effective inductive bias in the dataset, e.g., flipping. A more proper way to approximate CIFAR-10 with different $m_S$ is to sample multiple subsets of CIFAR-10 data with different sizes.

- Although the paper proposes a general stability bound on GDA, it is unclear how the stability bound can be utilized to advance existing baselines. The finding that GDA can improve test generalizations on small datasets, where awful overfitting occurs, is somewhat expected. It is still unclear how to set the hyperparameters, like the number of augmented samples given any new dataset.


Limitations:
I find no negative societal impact in this work.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper studies generative data augmentation (GDA), in which the samples from trained generative models are added to the training dataset for training discriminative models. There have been several empirical research reports on GDA, and it is known that GDA is unlikely to be effective when real learning data is abundant. However, there has been no theoretical analysis provided so far to explain this phenomenon. This study assumes a realistic setting in which the distribution imitated by the generative model and the real dataset distribution are different, and analyzes the generalization error bounds of GDA in the non-iid setting in three cases: (i) general case based on the existing algorithmic stability framework, (ii) a binary Gaussian mixture model (bGMM) and linear classifier, and (iii) a deep generative adversarial nets and deep neural classifier. The main findings from these theorems are (a) the divergence between the distribution imitated by the generated model and the true distribution is important for the generalization error, (b) increasing the number of generated samples does not lead to a faster learning rate, and (c) GDA does not achieve faster learning rate in situations affected by the curse of dimensionality. The paper provides simple experiments on bGMM and CIFAR-10 to test the theory, showing the similarity between the upper bound of the generalization error given by the theorem and the measured trends of the error, and the effectiveness of GDA in situations where the curse of dimensionality occurs. While this generalization error analysis is not perfect, as the paper states in the Limitation, this paper will have a significant impact on the research field of GDA, where no theoretical discussion existed.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
+ The paper is well organized and clearly states its arguments. Also, the paper is very readable, with careful notation and explanation of existing frameworks to explain the theory.
+ The paper provides generalized error-bound analysis in three settings ranging from general to realistic settings, establishing a first step toward learning guarantees in GDA.
+ The paper confirms the implications of the theorem through several experiments.

Weaknesses:
- **W1** Some parts of the explanation are difficult to interpret. In Theorem 3.2 and 3.3, the paper explains ""constant-level improvement"" by GDA, but it is not clear in which equation ""constant"" appears, making the argument difficult to understand. Further, in Eq. (2), most of the theorems depend on the explanation in the Appendix, and the paper is not self-contained in this respect.
- **W2** The paper does not mention several important previous studies. For example, Shmelkov et al. [a] were the first to report that GDA with GANs degrades accuracy even in small training dataset settings. Subsequently, Tran et al. [b] and Yamaguchi et al. proposed methods to improve GDA by the principles of Bayesian neural networks and multi-task learning. Although these works use somewhat older generative models and there may be facts that partially contradict the claims of the paper, they are considered important milestones in explaining the motivation for your work. I recommend that the paper cites them appropriately.

**Reference**
- [a] Shmelkov, Konstantin, Cordelia Schmid, and Karteek Alahari. ""How good is my GAN?."" Proceedings of the European conference on computer vision (ECCV). 2018.
- [b] Tran, Toan, et al. ""A bayesian data augmentation approach for learning deep models."" Advances in neural information processing systems 30 (2017).
- [c] Yamaguchi, Shin'ya, Sekitoshi Kanai, and Takeharu Eda. ""Effective data augmentation with multi-domain learning gans."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 34. No. 04. 2020.

Limitations:
This paper adequately discusses the limitation of the tightness of the theoretical guarantee.

Rating:
7

Confidence:
4

REVIEW 
Summary:
Generative data augmentation (GDA) aims to improve model performance by generating artificial labeled samples to enlarge the limited training dataset, but is also highly influenced by the size of training dataset, choices of augmentation methods and the number of augmented data. The paper seeks to develop a theoretical understanding to GDA by reproducing classical results built upon i.i.d. assumptions to the generatively augmented dataset, and establishes a general relationship between the stability bound and the learned distribution divergence besides the augmentation size. To further investigate and verify the proposed upper bound of generalization error, the paper also particularizes specific cases of bGMM and GAN, and provides insights to understand the theoretical results. Finally, empirical experiments on synthetic dataset and CIFAR10 are conducted to study the effect of different factors and validate discovered theoretical findings.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1） Mathematical notations and theoretical assumptions are clearly exhibited and explained in the paper. Details required to understand the problem background and existing results are also provided.
2） The theoretical results are novel and the related analysis seems to be solid and intuitive. The proposed theorems also build connection to previous results under i.i.d. assumption and empirical findings.
3） The idea of extending classical generalization error theories to the generative data augmentation problem is novel and well-motivated.


Weaknesses:
1） Despite theoretical results and corresponding conclusions of this paper are easy to understand and conformable to our common intuition, too many separated compositions of remarks make the paper annoyingly disorganized, forcing the readers to jump around while reading. The paper has to be re-organized to make readers easier to access the content.

2） Some figures (e.g. Figure 1a) are hard to recognize, and some are unnecessarily separated into parallel parts (e.g. Figure 1b and Figure 1c).

3） My greatest concern about this work is that when talking about data augmentation, we are most interested in the difference of model performance trained with and without data augmentation. However, the paper only considers the case where the optimal augmentation number $m_G^*$ is adopted.

4） According to Theorem 3.1, the generalization bound for GDA is composed of the distribution divergence and the generalization error with respect to the mixed distribution, where the former is determined by the model itself and the latter is controlled by the number of generative data. For a given model with fixed generative ability, what we can only do is to find a near optimal number of generative samples. However, the choice of this number is not inspiring in this paper in my opinion.

5） Experiments on real-world dataset seem incomplete and not convincing enough. First, only a single dataset (CIFAR-10) is adopted which fails to evade possible influence by the nature of dataset. Second, the experimental results show limited information and connection to the proposed theories, since the generalization error is inestimable in this case. Moreover, it is unfair to take generative models with totally different architectures for comparison, but fair comparison can be made between the same generative model with different training degrees.

6） The practical contribution of the paper may be limited since many conclusions are intuitive and show little help to practical application.


Limitations:
see above

Rating:
5

Confidence:
3

REVIEW 
Summary:
In this work the authors present new theoretical results for generative data augmentation. In particular, the authors introduce a new result that gives a bound on the generalization error of a model trained with data augmentation provided by a generative model. The authors use this bound to illustrate when GDA may or may not help in generalization performance. The authors then provide specific examples of applications of this bound to common models for generative augmentation; a binary Gaussian mixture and a deep generative model (GAN). For each the authors derive a more specialized bound and perform empirical experiments to validation the theory.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- This work provides useful theoretical insight into the strengths and limitations of generative data augmentation. The authors present novel results that, to my knowledge, are the first results bounding the generalization error for models trained with the aid of generative data augmentation. 
- I have not checked proofs in the appendix, but the theory as presented in the main text seems sound.
- The applications to both the binary Gaussian mixture and GAN setting are helpful for illustrating and expanding upon the main theoretical results.
- The experimental results do support the theoretical results presented and the results on DGM showing the re-affirming the promise of diffusion models for this purpose are interesting.
- The results presented could be a useful foundation for future work.

Weaknesses:
- The deep generative model results presented are narrow in scope, applying to GANs trained in a class-separated manner and to fully-supervised learning, rather than potentially semi-supervised learning or transfer learning via generative models. 
- As mentioned by the authors, they do not investigate the tightness of the proposed bounds.
- I'm not sure this type of data augmentation approach is widely used enough for this analysis to be immediately impactful, though it does appear to be gaining traction.
- The clarity of the writing could be improved in places.

Limitations:
The authors discuss limitations.

Rating:
7

Confidence:
1

";1
IYnsTEVTIb;"REVIEW 
Summary:
This paper studies optimization problem under non-convex and smooth setting, where the smoothness is parametrized by a PSD matrix instead of a scalar. It considers using matrix instead of scalar as stepsize, which can trace back to the Newton's method. In addition, it considers the distributed/federated optimization setting, where the communication efficiency is a major concern. To this end, this paper considers two ways of performing updates: $DS^k \nabla f(x_k)$ and $T^k D \nabla f(x_k)$. The matrix $D$ for the second one can be easily computed, and it requires to solve a convex program for the first one. They show how to extend this formulation to layerwise structure, which is widely applied in neural network training. They further show a federated algorithm using matrix stepsize. Experiments are conducted and results are in favor of the proposed algorithm, compared to DCGD.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
Distributed optimization with matrix smoothness and stepsize is indeed an intriguing topic. This paper further analyzes its compatibility with compressor/sketches, which are widely used in federated learning. It provides explicit construction of the stepsize matrix to obtain the optimal convergence behavior. This is the first work that provides theoretical understanding in the federated setting, which has become one of the most applicable fields in optimization. I believe this framework can be further studied and yield more uses in the context of federated learning.

Weaknesses:
As noted in the paper, one of the first matrix stepsize algorithms is the Newton's method. It is also highlighted the reason why this paper seeks a better matrix stepsize --- as computing and inverting the Hessian is always expensive. However, only the $T^k D \nabla f(x_k)$ case is easy to compute, and the $DS^k \nabla f(x_k)$ requires solving a convex program. It is important to quantify the computation complexity of the $DS^k \nabla f(x_k)$ case, otherwise it might just be better to use the Newton's method. 

The sketching matrices $S^k$ and $T^k$ seem to be only useful when the problem has layerwise or blockwise structure. For a more general purpose optimization framework, how does one design such distributions? Perhaps this correlates to a recent work for first-order federated learning paradigm, where one first sketches the gradient, sends it to the server and uses the transpose of the sketches to recover the averaged gradients for model update [SWYZ22]? This point should be further elaborated, otherwise the proposed method has only limited uses.

As the paper focuses on factoring the sketching matrices into analysis, more literature regarding such approach should be examined. For example, using sketching for second-order method in the case of linear program and empirical risk minimization has been heavily studied before [LSZ19, SY21, QSZZ23]. Can we devise matrix stepsize method for these interior point methods?

[LSZ19]: Y. Lee, Z. Song and Q. Zhang. Solving empirical risk minimization in the current matrix multiplication time. COLT'19.

[SY21]: Z. Song and Z. Yu. Oblivious sketching-based central path method for linear programming. ICML'21.

[QSZZ23]. L. Qin, Z. Song, L. Zhang and D. Zhuo. An online and unified algorithm for projection matrix vector multiplication with application to empirical risk minimization. AISTATS'23.

[SWYZ22]: Z. Song, Y. Wang, Z. Yu and L. Zhang. Sketching for First Order Method: Efficient Algorithm for Low-Bandwidth Channel and Vulnerability. Arxiv'22.

Limitations:
Yes.

Rating:
7

Confidence:
5

REVIEW 
Summary:
This paper analyzed two different non-convex optimization algorithms with matrix step sizes with sketching.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper coupled sketching with matrix stepsize, and detailed a number of conditions that need to be satisfied in order for the algorithm to work.

Weaknesses:
1. (9) and (10) do not clearly communicate why using a matrix stepsize D is better than a constant stepsize. Right now the machine learning community is concerned more with the quality of the local minima (generalization abilities, saddle-escaping, etc), not the speed at which we arrive at any local minima. Therefore this paper does not convince me to use matrix step-size over constant stepsize in its current form.

2. We don't know why matrix stepsize and sketching matrices need to be coupled. What if you only use D, not S and T? Would the results be better? Or simply because someone else already performed that analysis.

Limitations:
N/A

Rating:
5

Confidence:
2

REVIEW 
Summary:
The paper studies matrix step-sized and compressed gradient descent method for non-convex optimization. Two types of update are given (det-CGD1 and det-CGD2) . They are extensions of the vanilla gradient descent from scalar step-sizes to matrix cases, and can be considered as Newton-Type methods with a constant Hessian approximation. Convergence results in the single-node regime are given under generalized step-size conditions. Optimal matrix step-sizes are discussed for two types of update. Particularly, the block diagonal case of matrix smoothness is considered to leverage the layer-wise structure of NN training. In this case, gradient compression with Bernoulli sketches comes for free, compared to the uncompressed version. Two types of update are discussed under distributed setting as well and shown to have different theoretical results which guarantee the convergence when the chosen matrix step-size is bounded by n/K. Experiments are provided to show the advantages of the proposed algorithms.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. proposed two types of CGD with matrix step-sizes for non-convex optimization and analyzed matrix step-size conditions and optimal matrix step-size for convergence.
2. specialized to the block-diagonal case of matrix smoothness to leverage layer-wise structure for NN training, and found gradient compression for free.
3. extended to distributed setting.
4. demonstrated good performance of the proposed algorithm

Weaknesses:
Although this matrix step-size extension may be useful, it seems to be straightforward to get theoretical results on convergence. Also, matrix step-size tuning may be a difficult job in practice, and step-size conditions seem hard to check in the general case. Optimal step-size conditions for det-CGD2 seem computationally hard to meet by CVX and are required for each step.

It needs more efforts to determine the matrix smoothness than in the scalar case. Also, it would be space-inefficient in the general case.

Since matrix step-size was proposed previously, it would be better to make a clear comparison with SHR21.

Experiment section is incomplete in the main text.

There are some typos.

Line 46: first occurrence of $e_ij^T$

Line 133: No definition of $\mathcal{S}$, $\mathcal{T}$

Line 255: ""denominator""

Limitations:
Limitations of the work were discussed in the paper.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper proposes two novel single-node and distributed algorithms for non-convex optimization via SGD with matrix stepsizes and compressed gradients via random sketches. The authors also provide theoretical guarantees for convergence of their methods to a stationary point under a matrix smoothness assumption, and also show improved communication complexities for the distributed algorithms. Further, they specialize their results for the block-diagonal smoothness assumption with block-diagonal stepsizes, which corresponds to multilayer neural networks. Theoretical results are corroborated by experiments.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. The paper proposes an interesting and novel SGD type algorithm for non-convex optimization that combines random sketching for compressed gradients with matrix stepsizes. 
2. The algorithm extends to distributed computation settings, and leads to gains in communication complexity in various settings including multilayer neural networks by leveraging different gradient compression schemes that fall under the model studied by the paper.
3. The presentation is fairly clear and easy to follow.
4. I did not check all the proofs but they seem sound.



Weaknesses:
1. The main convergence result is in the $D$-norm (or the determinant normalized version), and not in the euclidean norm. The authors note this weakness in the limitations section.
2. Some definitions are not clearly stated (like the error $\epsilon^2$)
3. There were no experiments on the neural network setting (specifically with the block-diagonal smoothness assumption). Additionally, more experiments highlighting the improved communication complexity implied by the theory would be nice as well.

Limitations:
As pointed out by the authors, the convergence results do not extend to the Euclidean norm.

Rating:
7

Confidence:
3

REVIEW 
Summary:
1. This work analyzes two compressed gradient descent methods with matrix step sizes for non-convex optimization. 
2. This work designed the sketches and step sizes to take advantage of the layer-wise structure. 
3. The distributed version of the proposed method is also discussed under the Federated Learning framework. 

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
Please refer to the first section. 

Weaknesses:
This paper can be improved in written style and flow. 


Limitations:
As discussed in Section 6.1 and 6.2. 

Rating:
6

Confidence:
1

";0
2pVogxJyDA;"REVIEW 
Summary:
This paper introduces PromptCoT, an enhancer that automatically refines text prompts for diffusion-based generative models, improving their capability to produce high-quality visual content. The system is based on the idea that prompts that resemble high-quality image descriptions from the training set lead to better generation performance. Pre-trained Large Language Models (LLM) are fine-tuned using a dataset of such high-quality descriptions, allowing them to generate improved prompts. To mitigate the tendency of LLMs to generate irrelevant information, contamination, transfer, and even a Chain-of-Thought (CoT) mechanism are used to improve alignment between original and refined prompts. Additionally, to maintain computational efficiency, the system employs adapters for dataset-specific adaptations, leveraging a shared pre-trained LLM. When tested on popular latent diffusion models for image and video generation, PromptCoT showed significant performance improvements.


Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1) This paper shows an insight from the observation that prompts resembling high-quality image descriptions from the training set lead to better generation performance.  It makes sense from my perspective.  Current text-guided image generation is still with limited generalization ability.  The points near the training support samples are transferred better. 
2) The idea to utilize LLM to adapt the original prompt to the one that is more aligned with training samples makes sense. It leverages the ability of LLMs to align the distributions. 
3) Three training methods are proposed to implement the ideas including the continuation, revision, and CoT.  
4) This paper builds the corresponding datasets to help the fine-tuning, which can benefit the following research.
5) The method using the GPT-3 to build datasets is smart.

Weaknesses:
1) The finetuning of LLaMa is time-consuming.  Could it be replaced by prompt learning or LORA? 
2) Please add a discussion (e.g. in related work) with ''Visual Chain-of-Thought Diffusion Models'', though these two methods are clearly different. 
3) If the datasets with neural captions can be built.  How about adding these sentences to the training set of the diffusion model?


Limitations:
It can be found in the F part of the appendix.

Rating:
7

Confidence:
4

REVIEW 
Summary:
 In this manuscript, the authors proposed a simple yet effective framework to improve the generation quality of pretrained generative models. Generally, to align the prompt distribution with large language models, the authors present three individual solutions to align and enhance the original textual inputs, i.e., providing a compelling text continuation with given initial inputs, revising the initial inputs, and using chain-of-thought to enhance the initial inputs by 5 steps. Moreover, the authors also introduce multi-task adaptation to improve the quality of generated textual desctiptions for each dataset. The experimental results demonstrate that the generative models with PromptCoT can achieve better qualitative performance than the baseline method.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The proposed PromptCoT is easy to follow.

2. The qualitative results is promising. 

Weaknesses:
1. Introducing prompting technique to improve the quality of the textual inputs for generative models lacks novelty, since intuitively, providing more detailed information indeed improves the generation quality of the generative models. The authors could clarify the novelty, e.g., how the prompting technique or the CoT technique different from other works, or why the generated text is better than human-refined counterparts.

2. As shown in Figure 4, the proposed text revision technique may generate some uncontrollable additional information (e.g., time, weather and place). I think the prompted results may be influenced by the inherent bias of the language model or the prompting dataset. The authors could provide some analysis to show that the enhanced textual inputs are unbiased with given CoT technique, or show that the enhanced textual inputs are controllable.

Limitations:
The authors have stated the limitation of the proposed PromptCoT, i.e., relying on the capability of the generative models and the quality of inital textual inputs. These limitations can be seen as the future direction of this work.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper aims to improve the images generated by the off-the-shelf diffusion model, such as Stable Diffusion. This is done by fine-tuning a large language model (LLaMA) using text continuation on more high-quality prompts, which are collected by hand-crafted rules on, for example high CLIP similarity and text length. Furthermore, off-the-shelf image caption models (like BLIP) are also used to curate high-quality prompts. While the proposed method seems effective, the lack of technical novelty is a concern of this work.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1.	The overall presentation is good, and the paper is simple to follow.

2.	The visualization is very interesting. For example, Figure 1 is very impressive.

3.	The overall pipeline is well encapsulated in Figure2-4. It is simple for the reader to understand the high-level idea of the work by looking at the figures. 


Weaknesses:
Major 
1.	Novelty is a big concern in this work and there is no technical contribution in this work. The author basically uses many off-the-shelf techniques to augment/modify the input text.  Text-continuation is also commonly used for pretraining large LLM. The adaptation techniques in section 3.4 are also not novel and the author simply leverages the existing techniques. In general, it is unclear what is the technical contribution of this work. 

2.	L258 Is there any reason why t2t-blip booster demonstrates the best performance? Any theoretical reason behind this?

Minor
1.	L224 gpt to “GPT”


Limitations:
Limitation is discussed in appendix

Rating:
5

Confidence:
4

REVIEW 
Summary:
The main motivation of this paper is to better align prompts to the textual information of high-quality images within the training set. The authors propose datasets, instruction templates and use CoT to finetune LLM to achieve this goal. Adapters are also use adapters to facilitate dataset-specific adaptation.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
Application of LLMs to image generation for better text alignment is reasonable. The authors prepared datasets and conducted curated pipeline to achieve this goal. The presentation is clear.

Weaknesses:
1. the observation that prompts aligned with high-quality images withing the training set are more inclined to yield visually superior outputs needs more demonstration. The authors verified this on  LAION dataset. But it does not mean this is always the truth on all datasets. Whether it works on Flickr, Pinterest images?
2. The application of LLMs to refine texts is widely used now. Prompt engineering, Cot, adapters are also widely explored. Only the Application of these techniques seems not novel enough. 
3. The pipeline may needs more explanation on why we need text continuation first then text revision. 
4. the experiments are quite confusing. what are t2t-inter, cot_d, cot? the scores in Table 2 shows the effectiveness of each booster. How do they work together?  Which booster works the best  according to these experiments. what are Table 3 and Table 4 used for? It's really hard to summarize a conclusion that can match the design of the whole method.

Limitations:
limitations are presented in the paper.

Rating:
4

Confidence:
5

";0
PXUHrqIL9O;"REVIEW 
Summary:
The authors demonstrate that selective mixup has a similar effect with resampling. This reduces training dataset bias, which may contribute to settings with distribution shifts. Extensive experiments on five datasets were conducted.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- A new perspective of (selective) mixup has been discussed.
- Extensive experiments were conducted on various benchmarks
- The paper was well written and was easy to follow.

Weaknesses:
The authors have done a great job in providing an extensive list of experiments. However, I have two major concerns:
1. The paper's contribution to the body of knowledge is somewhat weak. I believe the gist of the paper is that ""(selective) mixup has a resampling effect"". Although this is a novel and interesting interpretation of the mixup data augmentation method, I feel this is not enough compared to the recent standard level of NeurIPS papers. I believe that a more concrete methodology that reflects the authors' findings is needed to qualify as ""acceptance level"".
2. The experimental results and their discussions are confusing. While the discussion on the waterbirds dataset says that ""selective mixup performance is entirely due to resampling"", the results on the arxiv dataset says ""selective mixup is affected both by vanilla mixup and resampling"", and results on the civilComments dataset implies that ""resampling is better than mixup"". These different experimental results and interpretations do not conclude the effect of resampling and mixup. Rather, the results seem to imply that ""mixup and resampling are independent methods that may or may not have correlating effects, depending on what dataset is used"". If this is the case, the primary assumption of this paper is undermined.

Limitations:
Yes

Rating:
4

Confidence:
3

REVIEW 
Summary:
The authors focus on explaining the working mechanism of selective Mixup in distribution-shift scenarios. They points out an interesting and novel perspective that selective Mixup benefits not (only) from Mixup itself, but rather the resampling effect it induces. They suggest that such a resampling effect can balance the class and/or domain distribution (depends on the pairing criterion) into uniform distribution, which in turn improves the model's performance.

Theoretically, they show that under the different-class criterion of pairing the data, the classes' distribution does become more uniform. Experimentally, they take the worst-group (class or domain or their combination) performance  as the metric, and conduct experiments on several datasets and models. They investigates a variety of algorithms (e.g. ERM, Mixup, conventional resampling, etc.) and pairing criterions (e.g. different class, different domain, etc.). The results confirms their raised point of view on selective Mixup.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. This paper presents an interesting and novel perspective of  undderstanding the mechanism of selective Mixup. That is, instead of focusing on the data interpolation part of Mixup, which is conventionally considered the key of Mixup and any of its variant, this paper indicates that it's the resampling effect, which averages the data distribution, that enables selective Mixup to success.

2. In all the experiments, the paper has fully investigated all the possible sampling criterions. Furthermore, on the basis of these experiments results, this paper combines the algorithms and criterions that give the best performance respectively and obtains a even better result under its own testing metric.

3. All the figures are well displayed and clearly readable.

Weaknesses:
1. This paper is a experiment-driven work. Not many theories are provided to back the main idea up. While Theorem 3.1 shows how certain selective sampling ""uniformizes"" the class distribution, there is no evidence nor explanation of how it affects the covariate distribution. Also, there is no investigation or explanation of what roles (if any) the Mixup operation itself play in selective Mixup.

2. When performing Mixup and selective Mixup training as baselines, the hyperparameter of the Beta distribution $B(\alpha, \alpha)$ is not carefully determined, or at least the choosing process is not fully explained. If the Mixup baselines failed to reach their optimal states, then the comparison of them with other methods like resampling and selective sampling w/o Mixup might be unqualified.

3. The testing metric only considers the worst-group performance, but not including the overall performance. Normally practitioners are concerned not only with the short board of the bucket, but also all the boards as an entirety (a trade-off between fairness and overall generalization). 



Limitations:
1. The paper is fundamentally based on experimental investigation, but with little theoretical support. 

2. The datasets used doesn't include some of the most popular or commonly used ones like CIFAR or Imagenet. 

3. In the experiments, Mixup (and selective Mixup) are taken as baselines, but the value of $\alpha$ (which supervises the Beta distribution) may not have been carefully determined.

4. I personally imagine that the datasets size and\or the batch size will also have an impact on the performance of selective Mixup and selective sampling. There might be some room for further investigations in the future.

5. What roles (either positive or even negative) the Mixup operation itself plays in selective Mixup exactly have yet to be fully understood.

6. The metric is limited at the worst-group performance with no information on how the models perform in general on all the testing data.



Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper performs an analysis of selective mixup techniques, coming to the conclusion that some of the improvements these techniques provide can be obtained by the sampling procedure and not the mixing strategy. Based on some of the insights, variants of selective mixup are sometimes proposed to fit specific settings and shown to work better.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
- The considered datasets and settings are varied and extensive
- The paper proposes plausible explanations of the improved performance of selective mixup variants, and performs extensive ablations and creates experimental settings to test these explanations
- The observations about the roles of re-sampling and regression to the mean are extremely interesting.
- One of the main interesting points of the paper is carefully studying the forms of shift occurring between the training and the test distributions, and studying how specific forms of selective mixup impact them. This methodology can improve the practitioner's understanding of these techniques and how to improve them (e.g. as shown in some of the cases considered).


Weaknesses:
- As the authors acknowledge, their analysis is limited to the original mixing strategy.   
- The formatting of the paper is a bit bizarre, with plenty of extremely short paragraphs, misplaced image captions etc. I would recommend the authors to fix this issue, as it makes their work look unprofessional. 

Limitations:
Adequately Addressed

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper presented an insight that selective mixup (different class or different domain) is actually equivalent to resampling. A simple mathematical proof is presented showing that the mixed up distribution is closer to uniform distribution in terms of label distribution. Extensive experimental results are presented to demonstrate the findings. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
* novel understanding on selective mixup
* extensive experiments to demonstrate the understanding 


Weaknesses:
The finding is nice but seems value is limited, at least for two-class problems, selective mixup obviously leads to uniform distribution. This paper is more about empirical demonstration.

Limitations:
not new algorithm, but understanding existing algorithm, the value of the finding seems limited. 

Rating:
4

Confidence:
4

";0
hh6azymUaE;"REVIEW 
Summary:
This paper proposes the quadratic gradient for privacy-preserving logistic regression. Such gradient is used together with Nesterov’s accelerated gradient (NAG) and Adagrad on Homomorphic Encryption techniques.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
This paper tackles the privacy-preserving (in the sense of encryption) logistic regression. The paper is clear with introduction and motivation. The algorithm is easy-to-follow and well-supported by experiments.

Weaknesses:
Overall, it is hard to understand the privacy concerns in this work as there are many privacy-preserving techniques available. Without an example or experiment of privacy attack, the motivation of using HE in the first place is not backed up. In addition, only [12] is compared in the empirical results. While the new method seems promising, the lack of other baselines makes it hard to understand the limitations and benefits of the new algorithm.

Limitations:
NA.

Rating:
3

Confidence:
2

REVIEW 
Summary:
This paper proposes a second-order version of Nesterov's accelerated gradient (NAG) descent and Adagrad for logistic regression by incorporating an approximation to the Hessian. The authors call this the ""quadratic gradient"". Specifically, a diagonal approximation to the Hessian for logistic regression is proposed. Some empirical results are shown to illustrate the benefit of the proposed method over vanilla NAG and Adagrad.

Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
The proposed approximation of the diagonal Hessian may be interesting for NAG.

Weaknesses:
Unfortunately, this paper has several weaknesses.

**1.** **Limited novelty**: The proposed approximation to the Hessian in Section 3.2 seems like a trivial and incremental extension of the idea of reference [4] discussed in Section 3.1. 

**2.** **Lack of clarity**: The proposed methods are unclear to me and the presentation needs to be heavily improved.

* The enhanced NAG method described in line 150 is unclear to me -- what is $G$ here and is $\alpha_t$ the step-size here? Moreover, Algorithm 1 seems different from the discussions in Section 3.3. What are $\alpha_0$ and $\alpha_1$? They don't look like the quantity $\alpha_t$ introduced in Section 3.3. Why is $\alpha_1$ chosen to be $0.5(1 + \sqrt{1 + 4 \alpha_0^2})$? I don’t understand lines 31 and 37 in Algorithm 1 and what are $\gamma$ and $\eta$ here? What is the role of $W$ in lines 34 and 35 – it is not being used at all. In summary, the enhanced NAG method/Algorithm 1 has been presented poorly and I don't understand the method at all. 

* What is the enhanced Adagrad algorithm? The two equations after line 154 which are supposed to explain the enhancement are not very clear. What is the difference between $G^{(t)}$ and $g^{(t)}$ in these equations? Is $G = \tilde{B}^{-1} g$ here? There is no algorithm summarizing it like Algorithm 1 for enhanced NAG. Also, suddenly for Adagrad, the authors have a negative sign in front of the gradient which corresponds to minimizing the function whereas for NAG and the previous discussions in the paper, maximizing the function has been considered. Please stick to either minimization or maximization for consistency.

**3.** **Premise of enhanced Adagrad:** One way to interpret Adagrad is that it tries to maintain a diagonal approximation of the Hessian inverse and applies it to the gradients (a.k.a. preconditioning). So I'm not sure why applying a second approximation of the Hessian inverse on the *already preconditioned* gradients makes sense intuitively. Additionally, the authors themselves point out that enhanced Adagrad cannot be applied to general optimization problems (line 182) due to ""learning-rate explosion"". Then why introduce this method at all?

**4.** **Setup and experiments**: The datasets on which experiments are performed are not standard benchmarking datasets in the ML community and appear to have very few features (looking at Table 2). There are no test set statistics provided. I'd be more convinced if the authors showed empirical results in a *standard logistic regression setup without any kind of encryption* (which frankly seems irrelevant to me in this paper) on benchmarking ML datasets.

----

*Some general comments*: The introduction on logistic regression can be compressed. It is standard to consider the *negative* log-likelihood objective and apply gradient *descent* to minimize it. A couple of small typos -- in line 96, I guess it should be ""$\bar{h}_{k i}$ is the $k^\text{th}$ element in the $i^\text{th}$ row of the Hessian"" and in line 216, it should be ""public"".



Limitations:
Not in too much detail but as I mentioned in Weaknesses, the authors point out that enhanced Adagrad cannot be applied to general optimization problems. No foreseeable negative societal impact.

Rating:
2

Confidence:
4

REVIEW 
Summary:
This paper proposed a new approach to improve the gradient used by first-order optimization methods in logistic regression by utilizing a constant bound to the Hessian matrix. The authors demonstrate how to use their method under a fully Homomorphic-Encryption scenario. They test their method on many real-world datasets under non-private settings and Homomorphic-Encryption settings.

Soundness:
2

Presentation:
3

Contribution:
1

Strengths:
1. The paper is written clearly.
2. The experiments are all using real-world datasets which have strong practical implications.

Weaknesses:
1. The `quadratic gradient` method is not very new. As the authors have mentioned in Section 3.1 (Line 95), most parts of the method were proposed by Bonte and Vercauteren. I understand that the missing non-negative restriction is important for using the convergence results by Böhning and Lindsay (Line 92). However, using the absolute value is rather an straightforward solution. 
* A more interesting and critical question remaining to be answered is why this proposed `quadratic gradient` method is faster as the authors claimed in the conclusion (Line 238). 
* Another problem with this method is its usage being restricted in logistic regression: The authors provided a choice of $\bar{H}$ for logistic regression, while it may be very hard to generalize it to other problems, especially neural network training. 
2. The experiments have not shown much advantage of using the `quadratic gradient` method. In Table 1 and Table 2, the accuracy and AUC of the proposed method are almost always lower than the compared baseline method [12]. I understand that the learning time is reduced, but it was not a main problem in [12] as shown in the tables, and we are not sure if there is a tradeoff between the learning time and the accuracy.
* The description of the experiment details in Section 5 is very short. The authors suggest the readers refer to [12]. I think it would be better to have the details in supplementary and a discussion of the weaknesses of [12] in these experiment settings, along with why the proposed method solves those weaknesses.

Minor weaknesses:
1. The maximum likelihood estimation (MLE) is commonly referring to the estimate for $\beta$. The value of the loss function is the negative log-likelihood. That said, the y-axis of the figures could be corrected. Also, the objective function is usually the mean of the loss for each data point, not the sum.
2. Typo: Line 216, pulbic -> public.
---
I have read the rebuttal which answered my questions but did not fully address my concerns on the weakness.

Limitations:
N.A.

Rating:
3

Confidence:
3

REVIEW 
Summary:
The paper proposes a new gradient method that can be efficiently used under homomorphic encryption. The proposed method replaces the gradient $g$ by an approximation of $H^{-1}g$, where $H$ is the Hessian. This approximation is done using a specific diagonal matrix, that speeds up convergence while being possible to use under homomorphic encryption.


Soundness:
2

Presentation:
3

Contribution:
1

Strengths:
1. The paper proposes a new method for privacy-preserving logistic regression under FHE, that achieves reasonable results with less computation than existing methods.
2. The proposed method is quite versatile as it can be applied to a variety of optimization algorithms.


Weaknesses:
1. Experimental results are not convincing. Contrary to the paper's claims, the proposed method often performs way worse than existing ones (especially on iDASH, Edunburgh and pcs). Datasets are also very small, and therefore do not account for how the method scales with the dimension. Given this is an empirical paper, it seems a bit insufficient.
2. The fixed-Hessian method seems to be closer to preconditioning (see e.g. [1]), where gradients are linearly transformed before being used, than to a proper second order method.
3. The proposed method does not seem to be too novel. In particular, the paper refers to [2] (which itself refers to a paper with the same title as this manuscript), which proposes a very similar method.

[1] Preconditioned Stochastic Gradient Descent, Xi-Lin Li 2015.
[2] Quadratic Gradient: Uniting gradient algorithm and newton method as one, by Chiang, 2022.


Limitations:
Limitations are not discussed in the paper. In particular, experiments only consider a specific setting, with a choice of parameters that seems arbitrary (e.g., 3 vs. 7 iterations), and some claims are not supported with evidence.


Rating:
2

Confidence:
4

";0
b4Tr8NWTDt;"REVIEW 
Summary:
The paper addresses the problem of multi-agent RL by using learned world models over multiple potential opponent policies. The technique uses a Dyna-style algorithm to train the core policy with a combination of experiences generated through a world model and experiences playing against opponents. One evaluation demonstrates the world models benefits from training on data from multiple distinct policies. A second evaluation compares ways to use experiences generated from a world model to train a policy, showing pretraining on purely generated experiences is effective to warm-start a policy. An ablation study compares the proposed Dyna-PSRO model to vanilla PSRO on three MARL games, showing improvements over the PSRO model.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
## originality

Modest. The paper extends existing lines of work on MARL and world models, specifically studying the question of the policy diversity for training the world model.

## quality

Modest. The core results (figure 5) show clear improvements over PSRO. This is limited to a small number of games and the games themselves are relatively simple game domains.

The paper does a good job of breaking down specific claims to isolated experiments.

## clarity

Low. It was difficult to interpret many of the figures (questions and suggestions below). Generally the results of each experiment were hard to understand and would benefit from a single clear statement of the core outcomes in each section.

## significance

Low. The core audience of this work is researchers in MARL and particularly those considering world models as a solution.

Weaknesses:
The experimental results are promising, but would benefit from expansion. There are a few experiments that would help:
1. More games from MeltingPot. I hate asking simply for ""more"", but in this case it would help to show how well the agents perform on a wide variety of tasks. The results would help clarify where DynaPSRO benefits and may reveal limitations or areas for improvement. The wider set of results would give others confidence in the generality of the improvements gained by planning against diverse other agents.
1. More complex games. Consider more complex environments from PettingZoo (https://pettingzoo.farama.org/) or SMAC (https://github.com/oxwhirl/smac) that would highlight the potential of these algorithms in more compelx scenarios. This would help address the point that world models can become unstable and the value of strategic diversity in scenarios that support a much wider array of behaviors.
1. Scaling experiments. For example, when do prediction improvements level off when adding more policies? The experiments only examine adding 2 policies, which is a sparse sample of the space of strategies for most games.

The evaluations would benefit from other baselines to compare. What other algorithms could be used aside PSRO?

The full evaluation (last experiment) would benefit from a set of ablation studies. This could easily replace some of the planning experiments as the ablations would examine similar capabilities. I ask for ablations as these will be more convincing that the parts of Dyna being used add benefit over PSRO.


Limitations:
Yes. The work is focused on integrating world models into game playing agents and recognizes the preliminary nature of this work along with the potential risks introduced by using a simulation (world model) for decision making in real world applications.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper introduces a new approach to PSRO algorithms, where a world model of the environment is learned concurrently to the iterative PSRO strategy expansion. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The authors are right that the problem of having to re-learn policies from scratch is a large problem in the PSRO literature. Therefore, the idea to co-learn a world model alongside the expansion of the empirical game, in so taking advantage of the diversity of experience created by agents with slightly different best-response targets is an interesting approach to this problem. To the best of my knowledge this is also a novel solution to this problem.  
- I really like the presentation of the paper, and in general I think the authors do a very good job in terms of analysing the different moving parts of the framework in a reasonable manner.


Weaknesses:
I have a few concerns with the paper, however none of these necessarily game-changing in my evaluation of the work.

- I think the greatest misgiving I have with this work is that the related work seems to miss quite a large collection of PSRO papers that probably deserve mentioning. PSRO-style algorithms is a fairly small research area and I am surprised that the authors fail to make mention to many variants. In particular, as there is a section on strategic diversity itself in the paper, it seems odd that the authors have failed to comment on the line of works on diversity-based PSRO frameworks. For example, [1], [2], [3], [4], [5] are all diversity PSRO approaches. It also fails to place itself in the literature involving PSRO algorithms that attempt to speed up convergence times such as [6], [7].

- Furthermore, I was additionally surprised at the lack of comparison to NeuPL [8] which is another population-based framework attempting to similarly deal with the best ways to transfer information between agents in the population. 

- I do not necessarily believe that the authors need to benchmark against all of the approaches that I have listed. I do however believe the paper still needs work in terms of placing itself within the current literature on PSRO and other population-based frameworks.

- Based on the above, my score is set at a borderline accept. However, I am willing to revise this upwards upon seeing a better framing of this work in the current literature.

REFERENCES  
[1] Policy Space Diversity for Non-Transitive Games - Yao et al. 2023  
[2] Open-ended learning in symmetric zero-sum games - Balduzzi et al. 2019  
[3] Modelling behavioural diversity for learning in open-ended games - Perez-Nieves et al. 2021  
[4] Towards unifying behavioural and response diversity for open-ended learning in zero-sum games - Liu et al. 2021  
[5] A unified diversity measure for multi agent reinforcement learning - Liu et al. 2022  
[6] Pipeline PSRO: A scalable approach for finding approximate Nash equilibria in large games - McAleer et al. 2020  
[7] Neural auto-curricula in two-player zero-sum games - Feng et al. 2021  
[8] NeuPL: Neural Population Learning - Liu et al. 2022  



Limitations:
I think the authors actively engage with the limitations of the work.

Rating:
6

Confidence:
5

REVIEW 
Summary:
The authors consider learning world models for deep reinforcement learning in combination with the construction of empirical games through PSRO. They first show that world models benefit from training on a diverse set of strategy profiles as can be generated through PSRO meta-game solvers. They then empirically show that PSRO best responses can enjoy sample efficiency benefits by training with simulated world model experience. Finally, they present Dyna-PSRO, in which PSRO best responses make use of a world model trained on all available experiences collected thus far in a run of the PSRO algorithm. Dyna-PSRO provides lower-regret solutions with higher sample efficiency than PSRO without a world model.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The paper is very well written and presented, and the experiments are well designed. 
- World models are seeing increased use in the RL community, and PSRO is one of the more practical and general methods currently available for finding approximate game solutions. This paper provides insights on how to properly combine the two and make improvements to PSRO's sample efficiency, which is one of its largest issues.
- The proposed Dyna-PSRO method is sound.
- While many implementation details are not present in the main paper, the appendix describes these details thoroughly.

Weaknesses:
It would have been nice to see how current high-performing world model methods such as Dreamer, which employs latent state spaces [1,2] might perform with the same approach. It's not immediately clear if experiments like in section 3.3.2 would have had the same outcome.

[1] Hafner, Danijar, et al. ""Dream to Control: Learning Behaviors by Latent Imagination."" International Conference on Learning Representations. 2019.

[2] Hafner, Danijar, et al. ""Mastering diverse domains through world models."" arXiv preprint arXiv:2301.04104 (2023).

Limitations:
All limitations have been adequately addressed.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper describes combining two things: training a world model of a game, and doing Policy Space Response Oracles (PSRO) on the game.

Doing PSRO involves getting a lot of episodes from the game (episodes are used to train the RL best-responses, and also to estimate the payoffs of the empirical game). The novel algorithm in this work (Dyna-PSRO) can be thought of as a modification of PSRO where those episodes are **also** used to train a world model (which is essentially a learned simulator of the game engine). Then, the world model is used to improve the training process of the best-response policies. 

Through experimental results, the authors show that this improved training process (based on Dyna) can cause the best-response learners to learn a stronger policy than the normal method when using the same amount of interactions with the real game environment. It does this by training the policy using trajectories from the world model (in addition to the usual trajectories from the real game environment), and by equipping the agents with one-step lookahead planning during training.

This paper showcases experiments on the Dyna-PSRO algorithm in three games, and Dyna-PSRO outperforms PSRO in all three, as measured by an approximation of NashConv.

The paper also shows experiments to measure the quality of the learned world model, to test the hypothesis that Dyna-PSRO results in a good world model.

I think the paper has some flaws* in its current form, but the core work of the paper is good, the charts are beautiful, and the results are strong.

---

*Edit: many of the abovementioned flaws were addressed during the rebuttal/discussion period.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Overall, the paper is well-executed.
    - It is well-written and polished.
    - There has clearly been a lot of time and effort put into the engineering and writing of this paper. There are 4 sets of thorough experiments in the main paper, and more in the appendix.
    - The figures are extremely readable.
- The results concerning the performance of the best-response policies are impressive.
- The effort will surely be helpful to future researchers: the research directions of (1) improving the efficiency of PSRO response calculations and (2) training better world models should continue to flourish, and the work presented in this paper contributes to both.
    - The research direction seems natural, especially in the direction of using world models to improve the performance of PSRO.


Weaknesses:
I think the paper could be better in explaining or hypothesizing the ""why"" for a lot of things, even if just qualitatively.

I think the paper states some conclusions too strongly.

I think some things are not explained well enough and are confusing to the reader (at least, to me):

- SumRegret metric:
    - It's really not clear from the main paper how SumRegret works (even though it is explained in the appendix). This could be clarified by defining the terms ""method"" and ""combined game"".
    - Also, I would feel a lot better if I saw results measured by an alternative metric, where the deviation set is the set of **all** policies. The $max_{\pi_i \in \bar{\Pi}_i}$ could then be approximated by just training one more response policy (as if doing one last epoch of PSRO). This seems like it would be a more accurate approximation of the Nash Conv. Is there any reason to use the metric in the paper instead of this?
- Empirical Game Solution not described
    - Since the settings here are general-sum, it's probably important to specify what solution concept is used for the meta-strategies in the main paper (even though it is included in the appendix).
- Experiments in Section 3.1 Strategic Diversity
    - Looking purely at Figure 2, the conclusion ""Overall, these results support the claim that strategic diversity enhances the training of world models"" does not ring true to me. For example, there are three world models which perform better on the metric (accuracy) used in Figure 2 than the most diverse one, for Observations.
    - Even if I look in the appendix at E.1, there doesn't seem to be significant evidence to support the conclusion: multiple world models have similar recall scores than the most diverse one, and the one trained without the random policy seems to have better scores.
    - I would be interested in seeing the cross-entropy loss instead of (or in addition to) the accuracy.
- The discussion of the Decision-Time Planning results (3.2.2) seems incomplete:
    - ""**The main outcome of these experiments is the observation that multi-faceted planning is unlikely to harm a response calculation,** and has a potentially large benefit when applied effectively. These results support the claim that world models offer the potential to improve response calculation through decision-time planning."" (emphasis mine)
    - However, Figure 4 does show that decision-time planning causes the response to be *worse*: The solid blue line (top) has no decision time planning, and the dashed gray line (second from the top) has decision-time planning, and performs worse.
    - It would be nice if there was some discussion about this, perhaps an intuitive/qualitative reason why this is.
- Dyna-PSRO results need more details (Figure 5):
    - For each experiment, how many policies (iterations of PSRO) were there?
    - Does each policy train for a fixed number of steps, or until some measure of convergence is reached?
- Was ""policy"" vs. ""strategy"" ever defined like this before? In my opinion, we shouldn't define these terms like this, because they are usually considered synonymous. The terms I'm familiar with are ""policy"" or ""strategy"" for the former, and ""meta-policy"" or ""meta-strategy"" or ""meta-strategy distribution"" for the latter. Just my opinion!
- I was very confused by the definition of World Model while reading the paper.
    - Even after reading it through entirely, I was under the impression that each player had their own world model, and that it implicitly modeled the actions of the opponent.
    - If one misses the bold notation of the definition of agent world model from line 137 to 141, it's easy to think that this is the case, especially since the phrasing is that ""the agent learns and uses a ... world model"" (instead of, say, ""the agents learn and use a ... world model).
    - On one hand, the formal definition given for an ""agent world model"" is technically accurate, and I am just dumb. On the other hand, I suspect many of us are dumb, and will be similarly confused upon reading the paper. (Also, I'm not **that** dumb: it's really hard to tell that the O and A are bolded!) (Also, even if some of us are not dumb, we are likely lazy and will gloss over the explanation that boldface means joint.) This is all to say that I would suggest explicitly stating that the world model takes as input an observation and action from **each** player, and returns an observation and reward to each player. And that the world model does NOT model the actions of any player.
- The bolding is nice, but it would be less confusing to **also** say ""strategy profile"" or ""joint strategy"" anytime this is meant instead of just ""strategy"" and something bold, as it's very easy to miss or forget what something bold means (plus, it seems incorrect to call a strategy profile a strategy). Also, maybe emphasize that sentence that explains what boldface means, so that readers don't miss it?
    - For example in line 774 and 775 of the appendix:
        - ""This is typically not tractable, but instead draws are taken from a dataset generated from play of a behavioral strategy **σ**. And the performance of the world model is measured under a target strategy **σ∗**""
        - should be ""strategy profile"" and ""target strategy profile""
    - and same in Line 159 and 160 of the main paper, and throughout section 3.1


Limitations:
Limitations addressed

Rating:
6

Confidence:
4

";0
9SwKSvaCiP;"REVIEW 
Summary:
The paper proposes SING, a simple gradient preprocessing technique that, combined with any optimizer of choice, argues for improved stability and generalization. The paper further provides a theoretical convergence analysis of the approach 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper is clear
2. The technique is simple and easy to implement.


Weaknesses:
Overall the paper proposes a straight forward extension to the gradient centralization method, where the gradients are also normalized in a pointwise fashion as in other adaptive techniques. The main weaknesses of the paper are:

1. Incremental - i do not think the contribution of this paper merits publication due to its incremental nature. Adaptive optimizers already normalize gradients in a similar way and it is not clear what is added in the proposed method. 

2. Non-convincing experiments - Only 1 experiment compares gradient centralization + AdamW (GC + AdamW), which was proposed in [1], which is the closest method to the one proposed in the paper.  By that experiment GC + AdamW already achieves approximately the same performance, hence stripping SING from any practical significance. I do not understand why GC + AdamW is not used as a baseline for other experiments as it clearly shows strong performance. At it stands, it is not clear whether the apparent improvement of SING stems from the GC part of SING, or the added normalization which constitutes it novelty. I would encourage the authors to add this ablation study to the paper to make it more convincing. Finally, results in the paper do not include standard deviation which is be a must for an empirical paper.

3. The theory can be equally applied to other adaptive optimizers, hence it is not special to SING




[1] - Yong et al - ""Gradient Centralization: A New Optimization
Technique for Deep Neural Networks""

Limitations:
Limitations are adequately addressed. 

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper proposed a simple and hyper-parameters-free way to improve the stabilization and generalization properties of optimizers used in deep-learning scenarios. They show that with gradient centralization and gradient normalization methods, SING can escape the local minima with large step sizes theoretically. The authors provide several experiments on datasets like ImageNet-1K, RDE, and some NLP tasks to show the superiority of SING together with popular optimizers like AdamW. They also give out some other theoretical results like convergence and invariance properties.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The experiments on real datasets show that SING+AdamW performs significantly better than other baselines at image classification, depth estimation, and NLP tasks. This efficient method is also simple and not requires additional hyper-parameters.
2. The authors show that SING can escape the basin of attraction of the critical point when the step size is sufficiently large, and the stepsize threshold is inversely proportional to the network's depth, while GD cannot. And the experiments result (Figure 4) show that SING can stabilize the performance of the optimizers like AdamW.
3. The paper is well-writen and easy to follow.

Weaknesses:
1. Although the empirical results are remarkable, the novelty of this paper is limited. As mentioned in the paper, gradient centralization[1] and gradient normalization[2,3,4] are common methods in the previous works, and this paper combines these two methods and systematically investigates the properties of SING.
2. The theoretical analysis just focuses on the gradient rather than combining it with momentum and scheduler, but since the gradient is normalized and the step size is large, the momentum and learning rate scheduler is critical for the global convergence, like in Thm 3.3, 3.4, $\eta$ is small, but $\eta$ needs to be large to escape local minima from Thm 3.1.
3. Thm 3.3, 3.4 requires that the mini-batch size B be some concrete value, this is too strict and it's better to relax this assumption.

[1] Hongwei Yong, Jianqiang Huang, Xiansheng Hua, and Lei Zhang. Gradient centralization: A new optimization technique for deep neural networks. In Computer Vision–ECCV 2020: 16th European Conference, Glasgow, UK, August 23–28, 2020, Proceedings, Part I 16, pages 635–652. Springer, 2020
[2] Ashok Cutkosky and Harsh Mehta. Momentum improves normalized sgd. In International conference on machine learning, pages 2260–2268. PMLR, 2020.
[3] Ryan Murray, Brian Swenson, and Soummya Kar. Revisiting normalized gradient descent: Fast evasion of saddle points. IEEE Transactions on Automatic Control, 64(11):4818–4824, 2019.
[4] Shen-Yi Zhao, Yin-Peng Xie, and Wu-Jun Li. On the convergence and improvement of stochastic normalized gradient descent. Science China Information Sciences, 64:1–13, 2021.

Limitations:
The author mentions that their method cannot be used together with LayerNorm or LayerScale, and the theoretical results not incorperate with AdamW.
I don't find ethical or immediate negative societal consequences in this work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
In this paper, authors have proposed a method (called SING) for stabilizing the optimization algorithms used in training of deep models. The proposed method is based on only a layer-wise standardization of the gradients without introducing any additional hyper-parameters. In addition, a theoretical analysis for convergence to a stationary point is provided. Extensive empirical simulations show improvement of the training performance when the existing optimization algorithms use the proposed approach on various tasks.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
In general, the paper is well-written, and concepts have been presented in an accessible way. Providing theoretical results, including the convergence and invariance properties provide more credibility to the proposed method. Furthermore, experiments on different architectures and on various datasets is another strength point of this paper.

Weaknesses:
I need some clarifications on the followings:

1 - As mentioned in the theorems 3.3 and 3.4, convergence is guaranteed only to a stationary point. On the other hand, Theorem 3.1 states that the algorithm can escape from a narrow local minimum. How can SING guarantee that the stationary point is a local minimum (what happens if the algorithm converges to a saddle point or even local maximum) ?

2 - What defines the narrow local minimum and the wide local minimum. There is no curvature information/notion in Theorem 3.1 to distinguish local flat minimum from the sharp one.

3 - In Theorem 3.3, $\epsilon^2$ is given by $\sigma^2/B$, so to have an arbitrary small error on the expectation of the gradient at some stationary point, $\sigma$ should scale as $\mathcal{O}(\frac{\sqrt{B}}{D})$. My question is for a very large model, (i.e., $D$ is huge), does the assumption (8) hold for every $x\in\mathbb{R}^p$? I am not sure how the assumption holds for a highly non-convex loss in a large deep model? This assumption is stronger assumption than other approaches. Typically, ADAM, and other optimization in deep learning either assume some level of convexity or use somehow reasonable assumptions like small gradient, or bounded sequence of estimates, etc.,

4 - Regrading the previous point, it is a good idea to run an experiment to illustrate the effect of $D$ on the training performance with SING.

5 - Do the results in experiment section (Table 1, 2, and 3) show the validation accuracy or the training accuracy ? Please clarify this.

6 -The convergence result doesn't provide any insight for the generalization to the unseen data. It is a purely an optimization perspective.

Limitations:
Please see my comments for Weaknesses and Questions.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper presents SING (StabIlized and Normalized Gradient), a new method designed to enhance the stability and generalization capabilities of the Adam(W) optimizer. SING involves a layer-wise standardization of the gradients that are input into Adam(W), and does not require the introduction of additional hyper-parameters. This makes it straightforward to implement and computationally efficient.

The authors demonstrate the effectiveness and practicality of SING through improved results across a broad range of architectures and problems, including image classification, depth estimation, and natural language processing. It also works well in combination with other optimizers.

In addition to these experimental results, a theoretical analysis of the convergence of the SING method is provided. The authors argue that due to the standardization process, SING has the ability to escape local minima narrower than a certain threshold, which is inversely proportional to the depth of the network. This suggests that SING may offer significant advantages in training deep neural networks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. As the authors have claimed, the proposed method can be applied in a plug-and-play style with impressive applicability to a lot of tasks, datasets and optimizers. Without additional hyperparameters introduced, I think this work has great potential to become a standardized training technique with big impact in the community. And the authors did provide the elegantly implemented source code in PyTorch which I think is already close to ready to be included in the standard PyTorch library.

2. Empirical performance is impressive with huge improvements over baseline optimizers in many settings.

3. Mostly the paper is written in quality and easy to follow with only a few ambiguities, which I will mention in the weaknesses part.

Weaknesses:
1. Firstly, I believe there is a misalignment between the theoretical analysis and practical method. To be specific, the analysis in Theorem 3.1 compares the learning rate needed for escaping local minima for SING and SGD. However, as the authors claimed previously, the SING algorithm is proposed to overcome the limitations of Adam(W). Therefore, it would be better to directly analyze SING against Adam(W), which is also mainly compared against in the experiment part.

2. Some issues in terms of writing. One is that the authors did not formally formulate the centralize operation in math equations but only in codes, which could be confusing for readers who are not familiar with PyTorch framework. I strongly recommend the authors to provide strict math formulations instead of ambiguous codes only. For example, at least I am still confused the mean operation is executed over which dimension and what the meaning is for that averaging. A second issue in writing is that it seems the authors interchangeably use the terms ""learning rate"" and ""step size"" in Section 1 but treat them as different things in Section 3. I hope the authors could clarify the differences or use one term consistently.

Limitations:
N/A

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes SING, a plug-and-play approach to enhance optimizers without introducing additional hyperparameters.

The idea consists of standardizing gradients in a layer-wise manner prior to the host optimizer’s execution, and is motivated by factors such as easier escaping of narrow minima and invariance properties.

Experiments on image classification, depth estimation, and NLP tasks such as NMT and QA are used to assess the proposed method’s performance and measure improvements over host optimizers.


Soundness:
2

Presentation:
3

Contribution:
1

Strengths:
The paper is written clearly and well-organized.

The proposed method is easy to implement and works in a plug-and-play fashion. The layer-wise gradient standardization can be viewed as a gradient pre-processing step and hence completely agnostic to the host optimizer, making the approach general and flexible.

The authors provide theoretical analyses to motivate and better understand SING. The convergence analysis considers the smooth non-convex bounded variance setting, which I believe to be a good balance between assumptions and how well it captures network training.

Experiments include different tasks from two domains and distinct network architectures, including ResNets and Transformer-based models.


Weaknesses:
The theoretical analysis is not helpful in motivating or better understanding SING’s benefits.

While SING adopts layer-wise normalization, it seems that the analysis holds given any partition of the $p$ many parameters in $D$ many sets – i.e. the fact that each of the $D$ tensors is assumed to correspond to a different layer is not necessary nor used anywhere in the analysis. We can then consider the effect of variable $D$ for a fixed $p$ (grouping the parameters in larger or smaller sets, say filter-wise, kernel-wise, or even parameter-wise), and we recover Normalized Gradient Descent (NGD) with $D=1$ and a form of sign SGD with $D=p$ (this has been studied in previous works to understand how normalizing gradients of coarse/fine-grained parameter sets can affect performance and convergence).

This has two concerning implications:

- For Theorem 3.1

Since $||\frac{g}{\Gamma (g)}|| = \sqrt{D}$, it follows that the post-processed gradients will scale (in norm) as $\sqrt{D}$, and hence $\eta_{SING}$ in Theorem 3.1 is fundamentally ‘undoing’ this scaling, hence claiming that ‘SING can escape local minima narrower than a threshold that is inversely proportional to the network’s depth’ is not very meaningful.

A similar argument would be to pre-process the gradient by scaling it up by 100 and claiming that now we can use a 100x smaller learning rate, which, although technically correct, is not useful.

While I’m aware that the layer-wise normalization will actually change the update direction and not just scale it, it seems that this change in direction does not play any positive role in the presented analysis.

Finally, one could simply set $D=p$ (i.e. artificially view each parameter as an independent layer) and Theorem 3.1 would state that the sufficient learning rate to escape narrow minima would actually decrease way more aggressively (as 1/sqrt(# parameters) instead of 1/sqrt(# layers)) – this is clearly not actually useful since we’re just scaling up the (norm of) post-processed gradients (compared to the layer-wise case) and compensating by scaling down the learning rate.


- For Theorems 3.3. and 3.4

To achieve stationarity $\delta$ independent of $D$ (i.e. $\epsilon \propto \frac{\delta}{D}$) we would set $\eta = \Theta(\frac{\delta}{D})$, $T = \Theta(\frac{D^2}{\delta^2})$, $B = \Theta(\frac{D^2}{\delta^2})$, where we are ignoring dependencies on $F(x_0)$ and $L$.

This means that, in the original case where $D =$ # layers, the guarantee requires both the number of iterations and the batch size to increase quadratically with the depth of the model, which is concerning.

Moreover, if we set $D=1$ (i.e. NGD) we actually minimize the required number of iterations and batch size. Therefore, these results do not motivate or support layer-wise normalization, and actually question this design choice by offering significantly better guarantees for NGD.

- Other points

Although the theoretical analysis considers updates following Eq. 2, the experimental studies heavily focus on AdamW + SING, which is not well-discussed. My main concern in this case is that the normalization from SING affects both $m_t$ and $v_t$ in the numerator and denominator of AdamW, respectively. It is unclear what is really happening in this case.


It seems that for long enough training time windows, if the layer-wise gradient norms remain roughly constant then the normalization effect would cancel out, reducing to AdamW’s updates (that is, if the $\epsilon$ term in the denominator of AdamW is negligible). Accounting for $\epsilon$, on the other hand, yields AdamW’s updates except with different values for $\epsilon$ for each layer, each scaled by the layer’s gradient norm.

Although it is unlikely that layer-wise gradient norms remain roughly constant for many enough iterations, this hints that SING’s normalization might be affecting the size of $\sqrt(v_t)$ compared to $\epsilon$ differently for each layer. This could result in confounding effects since the value of $\epsilon$ (compared to $\sqrt(v_t)$) can play a major role in the behavior of Adam-like methods, potentially improving the performance in multiple settings.

The experimental analysis could also be substantially improved. The hyperparameter tuning strategy (choosing best learning rate, then fixing it to choose best weight decay) can easily lead to suboptimal values, especially for SGD and any adaptive method that does not inherently incorporate AdamW’s weight decay decoupling (see Fig. 1 and 2 of Loshchilov & Hutter).

This can lead to an unfair advantage to AdamW and AdamW + SING over all other methods. The cosine schedule is also known to improve AdamW’s performance and more often than not harm SGD (compared to step-wise), hence it would be valuable to also collect results with a step-wise schedule for a more comprehensive and clear comparison.

There is also some loss in novelty from the fact that the actual method that plays the key role in the experiments also adopts LookAhead and softplus calibration. In particular, centering is not novel although it has been explored more extensively for the 2nd moment estimate (centered RMSProp, AdaBelief, SDProp, ACProp, etc), and layer-wise gradient normalization for adaptive methods has also been studied (AdaShift & AvaGrad – none of the two are cited or discussed). These methods should be included in the comparison to have a clear picture that would allow a proper assessment of SING.

Finally, there are also concerns regarding the other vision tasks. It is unclear what was the exact ResNet-18 model used for CIFAR-100: if it is a ~11M param model, then it is a wider version (DeVries ResNet-18) which differs from the one originally proposed by He et al. and achieves over 77% acc. on CIFAR-100 when trained with SGD (see LookAhead’s paper and DeVries&Taylor).

This would indicate issues with the CIFAR-100 results in Table 1, since results with SGD would be ~1.4% worse even with additional augmentation and 100 extra training epochs. As for depth estimation, the dataset is synthetic and not well studied, hindering a proper assessment of its results. Nonetheless, ViT’s are typically well-trainable with SGD if warmup and gradient clipping are employed (which is common practice for these models).

The fact that SGD achieved 0.25% accuracy suggests that the experimental setup should be revised – warmup and grad clipping should be adopted for SGD since they are common practice, especially if SGD only achieves 0.25% accuracy without them.


Limitations:
The authors discuss limitations satisfactorily.

Rating:
3

Confidence:
4

";0
xHNzWHbklj;"REVIEW 
Summary:
This paper propose DyGFormer, a new Transformer-based architecture for dynamic graph learning, whose novelty mainly includes a neighbor co-occurrence encoding scheme and a patching technique. Moreover, it introduce DyGLib, a unified library to promote reproducible, scalable, and credible dynamic graph learning research. In experiments ,the DyGFormer achieves sota performance on most of the datasets.

Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
1. The paper is clearly written and easy to follow.
2. I think this unified library is high-quality, meaningful and urgently needed, which can well promote the development of dynamic graphs


Weaknesses:
1. In DyGFormer , I think the introduction of transformer is not so novel. which has similarity with the attention mechanism of TGAT and TGN.
2. PINT[1], a work in NeurIPS 2022, is a new and sota model in the area of dynamic graph. Why this paper has not mention it?

[1] Souza A, Mesquita D, Kaski S, et al. Provably expressive temporal graph networks[J]. Advances in Neural Information Processing Systems, 2022, 35: 32257-32269.

Limitations:
The authors adequately addressed the limitations. I have no ethical concerns with this paper.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper studies the problem of continuous-time dynamic graph learning. The authors proposed a Transformer-based architecture DyGFormer to learn the dynamic edge representation, which mainly consists of a neighbor co-occurrence encoding scheme to count the co-occurrence of nodes, a patching technique to split the node’s sequence into multiple patches, and a Transformer as the backbone.  The authors also propose a dynamic graph learning library DyGLib, and re-report the important baselines’ performance based on DyGLib. Experimental results conducted on 13 benchmark datasets verify the effectiveness of the proposed method. 

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1.	The authors propose a co-occurrence encoding scheme and a patching technique to capture the correlation between sequences and the long histories of sequences respectively.  As far as I know, it is less studied than learning the dynamic link representations direct from the sequences of nodes’ first-hop interactions under the dynamic graph learning problem, especially how to model the long histories of the interaction sequence.
2.	The authors conduct extensive experiments on 13 benchmark datasets to verify the effectiveness of the proposed two-component. Besides the proposed method, the authors also re-implemented the important baselines in the area, and point out that some findings of baselines are not in line with previous reports because of their varied pipelines and problematic implementations, which may benefit future research in the continuous-time dynamic graph learning community.
3.	The authors propose a continuous-time dynamic graph learning problem. This library provides a unified pipeline to train and evaluate different baselines. I think this library can provide a tool for researchers in the community to conveniently evaluate different methods in a unified setting.


Weaknesses:
1.	In the problem formalization of section 3, the authors define the target of the dynamic graph learning problem as learning the time-aware representations for each node. But the DyGFormer actually learns the dynamic link representations but not the node representations. Or more accurately, the DyGFormer learns the contextual node representation $h^t_{u|(u,v)}$ but not $h^t_u$, where the node representation $h^t_{u|(u,v)}$ relies on the other node v. It will be better to add some discussion for this in the paper.
2.	In section 5.2, the authors say that “some of our findings of baselines differ from previous reports”, but the authors do not point out which baselines’ performance improves. I suggest the authors explicitly list which baselines’ performance improves.


Limitations:
N/A

Rating:
8

Confidence:
5

REVIEW 
Summary:
In this paper, the authors considered the dynamic graph representation learning (a.k.a. dynamic network embedding) problem and proposed a novel transformer-based architecture - DyGFormer, with several original designs (e.g., a neighbor co-occurrence encoding scheme, a patching technique, etc.) Moreover, the authors also implemented a unified continuous-time dynamic graph learning library, which include several baselines, widely-used datasets, and a common evaluation pipeline. Exhaustive experiments with various settings of dynamic link prediction and dynamic node classification were also conducted to validate the effectiveness of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
(S1) The overall presentation of this paper is clear, which makes it easy to grasp the key ideas.

(S2) There are some original designs (e.g., neighbor co-occurrence encoding scheme and patching of historical neighbor sequence) in the proposed method.

(S3) The authors conducted many experiments on 13 datasets for both dynamic link prediction and dynamic node classification with various settings.

(S4) The authors also implemented a unified library including some typical baselines and a common evaluation pipeline for various task settings (e.g., transductive and inductive dynamic link prediction and node classification), with the code provided for review.

Weaknesses:
(W1) Some statements are with too many citations (e.g., 'continuous-time methods [27, 53, 62, 44, 35, 9, 55, 58, 57, 24, 34, 12]', 'graph convolutions [53, 62, 44, 35, 9, 57]', etc,) which are hard to check their sources. Some references are also repeated in successive sentences (e.g., 'only a few libraries are specifically designed for dynamic graphs [18, 45, 71]. DynamicGEM [18] is a library for dynamic graph embedding methods'). It is better to ensure that there are at most 5 references for each statement. Some of the references can also be replaced with concrete examples or methods introduced in this paper. Furthermore, references of some important statements are also missing, e.g., 'unlike most previous methods that need to retrieve nodes' historical interactions from multiple hops' but what do 'previous methods' refer to?

(W2) Some problem statements regarding model optimization are unclear. Concretely, the formal definition of training loss is not given. In general, we can divided existing dynamic graph representation techniques into the task-dependent and task-independent methods, which are respectively trained via (i) supervised losses related to and (ii) unsupervised losses regardless of the downstream task. It is unclear that the proposed method is task-dependent or task-independent. In the baselines, DyRep and TGAT are task-independent while CAW is task-dependent (with a loss designed for dynamic link prediction), as I can check. From my perspective, it is unfair to compare both types of method in a common experiment setting, where task-dependent methods are expected to have better performance than task-independent approaches, due to the incorporation of supervised information related to the downstream task. For task-independent methods, the authors should also clarify the downstream module (e.g., logistic regression, SVM, MLP, etc.) to support a concrete task. More importantly, the evaluation pipeline of a unified library should also cover both the settings.
  
(W3) Although the authors gave toy examples for some procedures of the proposed method (e.g., the computation of neighbor co-occurrence encoding), several details still need further clarification (e.g., how to pad a derived patch, etc.) It is recommended summarizing the overall procedure to derive all the encoding and patches in terms of pseudo-code (even in supplementary material).
  
(W4) In related research, inductive dynamic link prediction includes the prediction of links between (i) one old (i.e., previously observed) node and one new (i.e., previously unobserved) node as well as between (ii) two new nodes. It is unclear that the inductive setting in this paper refers to both cases or just the latter case.
  
(W5) Although the authors included 7 baselines in experiments, most of them are based on time-encoded deep sequential models (e.g., with RNN and attention). In addition, there are some other related approaches based on temporal point process (e.g., HTNE [1] and TREND [2]) and neural ordinary differential equation (e.g., GSNOP [3]) that are not considered in experiments. A unified library should cover various types of method.
- [1] Zuo, Yuan, et al. Embedding temporal network via neighborhood formation. KDD 2018.
- [2] Wen, Zhihao, and Yuan Fang. Trend: Temporal event and node dynamics for graph representation learning. Web Conference 2022.
- [3] Luo, Linhao, Gholamreza Haffari, and Shirui Pan. Graph sequential neural ode process for link prediction on dynamic and sparse graphs. WSDM 2023.
  
(W6) In supplementary material, although the authors gave some details regarding the datasets, the source information (i.e., where can we download these datasets) is missing. In Table 4 of supplementary material, what does 'duration' means? Does it means the time granularity? Consistent with the statistics of 'duration', what is the total number of time steps for each dataset? From my perspective, the scale of a dynamic graph is related to the (i) number of nodes $N$ and (ii) number of time steps $T$. As I can check, all the datasets are with $N<20,000$, which cannot be considered as large datasets. Can the proposed method be scaled up to larger datasets? Or are there any possible solutions to addressing the scalability issue (perhaps as one future research direction)?
  
(W7) It seems that the authors upload the paper to ArXiv before the formal submission to NeurIPS. Several month ago, google scholar and github recommended the ArXiv version of this paper to me. As a result, I have already known the names and institutions of the anonymous authors, which breaks the double-blind policy.

Limitations:
See W1-W7.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes a new dynamic graph learning architecture and implemented it as part of a new unified graph library with extensive experiment results to verify the effectiveness of the proposed algorithms.
 
The author proposes a new Transformer-based architecture DyGFormer for dynamic graph learning. The architecture is designed to overcome some of the limitations of previous methods by:
- Leveraging a neighbor co-occurrence encoding scheme, which captures the frequency of appearances of each neighbor in the sequences of source and destination nodes. This explicitly explores the correlations between nodes.
- Utilizing a patching technique, which allows splitting each source/destination node’s sequence into multiple patches, enabling the capture of long-term temporal dependencies and reducing the computational complexity to a level that doesn't depend on the input sequence length.

The author also developed a library called DyGLib as a unified library specifically tailored to continuous-time dynamic graph learning. Its key features include:
- Standardized training pipelines to facilitate reproducibility and consistency across different methods.
- Extensible coding interfaces for better adaptability and scalability and integrates with 13 graph datasets and 9 graph algorithms

Soundness:
3

Presentation:
1

Contribution:
2

Strengths:
The paper presents an extensive and technically rigorous evaluation. The authors benchmark their model on 13 dynamic graph datasets against nine state-of-the-art dynamic graph learning models, providing thorough evidence for their conclusions. The comprehensive ablation study further supports the effectiveness of the proposed components.

The proposed DyGFormer architecture performs favorably in comparison to other dynamic graph learning models, with only a few exceptions.

DyGFormer handles long-term node interactions efficiently, thanks to the novel integration of neighborhood co-occurrence encoding and the patching technique.

Weaknesses:
The paper lacks a comprehensive technical discussion motivating the design of the proposed model. The co-occurrence matrix is said to model node correlations better than other models, but it's not entirely clear why explicit modeling of correlations leads to superior performance. Node proximity and similar neighbors are features that even basic models like the Graph Convolutional Network (GCN) can capture without explicit modeling.

The paper's presentation could be improved. The title, for instance, may overpromise what the paper actually delivers - I was expecting a unified learning algorithm along with a software tool. However, on closer reading, it appears that the main contribution is from DyGFormer, while DyGLib, though useful, doesn't seem to bring much novelty or be absolutely essential to the paper's contributions. The preliminaries section seems unfinished, and the section on DyGLib includes several unsupported claims and lacks novelty.

Limitations:
NA

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper presents a transformer-based architecture (DyGFormer) for dynamic graph learning, based on a node co-occurrence encoding scheme and patching. Further, they present DyGLib a library for uniform evaluation of dynamic graph learning techniques. Extensive experimental evaluations over diverse datasets show that DyGFormer performs well.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
**Originality.** While the transformer architecture is well-known, it's application to dynamic graph learning, and the proposed co-occurrence encoding and patching schemes are novel. Various libraries / frameworks (as discussed in 108) exist for dynamic graph learning, so DyGLib is not novel in that regard, but a fresh rigorous and extensible evaluation is appreciated.

**Quality / Clarity.** The paper is well-written and easy to follow. The DyGLib codebase is high-quality and well-documented. At a glance, it looks easy to use. Also, the appendix is thorough and well-put-together.

**Significance.** Dynamic graph learning is an important research problem. This work not only presents a good solution but also paves the way for rigorous future work.

Weaknesses:
While DyGFormer outperforms the baselines in avg. rank, there is a problematic trend that on some datasets DyGFormer can be much worse than the best or second best baseline in terms of absolute performance points (Table 1). This needs some analysis. What characteristics of these datasets make DyGFormer a bad choice against the baselines? What aspects of DyGFormer and the baselines could be causing their poor and better performance respectively? Why does DyGFormer's superiority falter when going from ""rnd"" to ""hist"" to ""ind""?

Limitations:
Limitations are discussed but in the appendix.

Rating:
7

Confidence:
3

";1
ViFTWelHVZ;"REVIEW 
Summary:
The paper presents a new method for improving the performance of neural networks through the design of optimal activation functions. The authors created benchmark datasets by training convolutional, residual, and vision transformer architectures with systematically generated activation functions. They then developed a new surrogate-based method for optimization, which uses the spectrum of the Fisher information matrix and the activation function's output distribution to predict performance. The method was tested on CIFAR-100 and ImageNet tasks, and the results showed significant improvements in accuracy.



Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. This paper introduces an innovative approach to enhancing activation functions, surpassing existing techniques in both efficiency and effectiveness. 

2. The paper is exceptionally well-written, and the experiments conducted are notably thorough.

3. The benchmark datasets created by the authors provide a foundation for future research on activation function properties and their impact on performance.

Weaknesses:
None

Limitations:
Yes

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper introduces three benchmark datasets created by training CNN, ResNet, and ViT architectures using a set of activation functions generated from a three-node computation graph that combines unary and binary operations.

The benchmarks serve to showcase the efficacy of utilizing the 2D UMAP of the Fisher information matrix (FIM) spectrum and/or activation outputs as a cost-effective surrogate for predicting activation performance. Leveraging the 2D feature representation, an efficient activation optimization method, AQuaSurF, is developed by employing regression techniques to model activation accuracy across the 2D feature space, requiring only 100 function evaluations. The benchmark results further demonstrate the effectiveness and statistical reliability of this approach.

The proposed method is successfully applied to various vision tasks, where the discovered activation functions consistently outperform existing baseline activations. Moreover, the top activations identified through this search exhibit successful transferability to a new vision task.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper is well-written and easy to follow. 

The approach of utilizing the UMAP embedding of the FIM spectrum with activation outputs to assess activation performance is novel and interesting. 

In contrast to previous methodologies that relied mostly on evolutionary algorithms and required thousands of function evaluations, the method proposed in this work demonstrates efficiency by outperforming baselines with just 100 function evaluations. 

Furthermore, the benchmark datasets introduced in this work, may potentially help accelerate research on activation optimization. 

Overall, this paper offers valuable insights for assessing activation performance and also introduces a more efficient methodology for activation optimization.


Weaknesses:
In Section 6, the authors apply their proposed method to more challenging datasets and a larger activation search space, compared to those used to create the benchmarks. To further evaluate the effectiveness of the approach it would be beneficial to apply the method (KNR on UMAP embeddings) to vision tasks involving new network architectures as well. 

While the chosen baseline activations in Table.1 already include ReLU and Swish, used in the original three architectures studied in the paper, in order to further strengthen the results it would still be advantageous, and perhaps straightforward, to extend the list of baselines at least to those used in PANGAEA, including GELU, LeakyReLU etc.

Limitations:
Limitations are partly addressed in the Future Work section in the appendix. There are no concerns regarding negative societal impact.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper addresses the optimization of activation functions in neural networks for improved performance in machine learning tasks. The authors create benchmark datasets and propose a surrogate-based optimization method based on a characterization of the benchmark space. They apply this method to discover better activation functions in CIFAR-100 and ImageNet tasks, showcasing its practical effectiveness.

Soundness:
3

Presentation:
3

Contribution:
1

Strengths:
1. The authors create benchmark datasets (Act-Bench-CNN, Act-Bench-ResNet, and Act-Bench-ViT) by training various architectures with numerous activation functions.

2. The paper presents a novel surrogate-based optimization method that characterizes activation functions analytically. By utilizing the Fisher information matrix's spectrum and activation function output distribution, a low-dimensional representation is created.

3. The proposed method, AQuaSurF, efficiently discovers improved activation functions in CIFAR-100 and ImageNet tasks, surpassing previous approaches in terms of evaluation efficiency.

Weaknesses:
1. The motivation and definition of using ""Activation Function Outputs"" as feature in Section 3 is not clearly explained.

2. In Table 1, some widely used human-designed activation functions, such as ELU, ReLU, and Swish, consistently achieve top performance on various tasks with different networks. However, the top activation functions discovered by the proposed method vary across tasks and networks. This suggests a limited generalizability of the searched activation functions. In other words, when faced with a new task or utilizing a new network, the activation function needs to be searched again. Furthermore, this also implies that the searching method may overfit the specific task and network, rather than finding activation functions that are generally effective and meaningful.

3. Related to the previous point, the design of the search space appears overly complicated, which also raises concerns about overfitting. As observed, the top activation functions discovered through the search process often involve complex combinations of existing human-designed activation functions. This complexity reduces their interpretability. Human-designed activation functions, on the other hand, are typically well-reasoned and supported by theory or hypotheses, allowing them to generalize effectively across tasks and networks. However, the searched activation functions are difficult to explain in terms of why they exhibit certain characteristics, and they lack generalizability.

Limitations:
No potential negative societal impact.

Rating:
3

Confidence:
5

REVIEW 
Summary:
This paper introduces a set of benchmark datasets for activation function search, and an efficient search method based on the analysis of the benchmarks.


Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed benchmark datasets are beneficial for further research.

2. The method that searches activation functions through the function outputs of a limited number of samples seems effective and can significantly outperform the random search baseline.


Weaknesses:
1. The paper is hard to follow. The main text refers to many details in Appendix, but it is still complex and hard to get to the method. I suggest the authors refine the structure and make the technical details of the proposed method more clearly.

2. Analysis is limited to show the efficiency of the method. The paper includes ""Efficient"" in the title, but I can only find the evaluation of efficiency in Appendix, and it should be compared with previous search methods to show how efficient it is. Besides, this method still needs to train multiple activation functions independently, which is also computationally expensive. 

3. The improvements are marginal. The authors should compare their method with existing approaches in both benchmark search and new tasks search. Besides, in Table 1, comparing with the popular searched activation Swish, the improvements are marginal.


Limitations:
None

Rating:
4

Confidence:
3

";1
XetXfkYZ6i;"REVIEW 
Summary:
The paper purports to develop a framework of optimal stopping that generalizes the previous approaches by incorporating non-Markovian settings and using a Bayesian network formulation.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The only strength of this paper in this reviewer's opinion is the fact that it tackles an important problem.

Weaknesses:
The paper is written in a manner that makes it very difficult to discern its contents. The presentation and the proofs are ridiculously long-winded even though they could they be easily written in much more succinct manner. Although the reformulation of the optimal stopping problem as a Bayesian is interesting at first sight, I don't see how it is useful unfortunately, especially given the fact that I have concerns about they are even maximizing. See the next section for more concrete examples of the problems this paper has.

Limitations:
Not applicable.

Rating:
6

Confidence:
3

REVIEW 
Summary:
Optimal stopping is the problem of choosing a time to take a given action based on sequentially observed random variables in order to maximize an expected payoff. Previous works used Deep Neural Networks to find the optimal stopping time (e.g. Backward Induction method), however, as the authors mentioned, these approaches have several limitations in non-Markovian settings. The paper presents the Optimal Stopping Problem as an inference problem on a Bayesian Network and they use RNNs to learn the model-free optimal stopping strategies.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Authors introduce a reasonable way to solve the optimal stopping problem and provide the corresponding theoretical justifications.
- The approach is well motivated (lines 43-60).
- The experimental results demonstrate that their method outperforms the baseline.
- The authors compared the training and inference times for DROS and the baseline.
- The paper is well written and organized.
- Experiments were done on real-world benchmarks.


Weaknesses:
- There is a whole literature on how to use deep learning to solve partial differential equations (PDEs) in general, and more specifically option pricing problems. Many papers solve PDEs using deep learning in the context of optimal stopping time.
- There are several papers on optimal stopping problems, I encourage the authors to include more baselines in their comparisons.

Limitations:
- From the tables 2 and 3 (Appendix C), the training time of DROS is bigger compared to the baselines.
- The proposed method can suffer in terms of time complexity when we test it to solve problems in high dimensions.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper proposes an RNN-based approach for optimal stopping which is based on a Bayesian inference view of optimal stopping. The proposed model can be trained with direct optimization via policy gradients, or with expectation-maximization (EM). These two appraoches are shown to be equivalent. This new RNN-based approach is shown to outperform state of the art methods on a couple of commonly used datasets.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The limitations of the existing deep neural network approaches are discussed and contributions of this work are clearly stated. The background on optimal stopping is also introduced in detail.

Weaknesses:
- **Writing and organization of the paper have much room for improvement.** 

**Overuse of abbreviations makes the paper difficult to read.** Although abbreviations like DNN and EM are common and inevitable, I personally would suggest against abbreviating weighted maximum likelihood as WML and policy gradiants as PG in the text. To make things worse, one of the main approaches is named DROS-OSPG, with an unnecessary repeat of ""optimal stopping"" which I find to be cumbersome and confusing.

**Equations with conflicting real and dummy indices.** In equation 2 and theorem 3.1, the index $j$ is both used as a real index and a dummy index for summation. 

**There's too much technical detail in the main paper.** Equivalence of EM and policy gradient is interesting but probably belong better in the supplements. However I do understand that the significance of this fact might have eluded me since I am not an expert in this field. The Keras-specific implementation detail on lines 246-247 can also be saved for the supplements.

**No conclusion or discussion paragraph at the end.** As a consequence, the paper does not find room to discuss its limitations, which is a requirement for NeurIPS papers.

- **Ablations are missing.** Since I am not an expert in this specific field, I do not know if the improvements over the state of the art are significant enough. From the prospective of a research paper, I feel like some design decisions need to be justified by running ablations. For example, how important are the weights in the weighted maximum likelihood objective? Abalation studies like this would justify the various claims put forth throughout the paper.
---
Once again I must say that it is very likely that I do not understand the significance of the paper due to lack of familiarity with the field. My main concerns with the paper lie in its presentation. I do not think the paper at its current state is doing a good job of illustrating the key ideas of the new method and of convincing me that the contributions are novel and significant.

Limitations:
The authors did not adequately address the limitations of the work since there is no conclusion or discussion section.

Rating:
5

Confidence:
2

";1
tSEeRl7ACo;"REVIEW 
Summary:
This paper proposes a method for learning discrete representations whose complexity can be smoothly annealed. Experiments demonstrate that, at the appropriate level of complexity, these representations can be useful for downstream classification tasks involving abstract categories.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The work proposes an interesting method for learning a discrete 'codebook' with a controllable level of complexity.
- Experiments demonstrate a clear non-monotonic relationship between complexity and usefulness in a downstream task involving abstract categories.
- The method outperforms baselines in this setting.

Weaknesses:
I have previously reviewed this work, and it seems that the paper has not been revised in a manner that sufficiently addresses the concerns raised by myself and other reviewers. Specifically, a major concern with this work as currently formulated is that the proposed human-in-the-loop approach does not seem very realistic. In the abstract, the authors note that 'humans learn discrete representations ... at a variety of abstraction levels ... and use the appropriate abstraction based on tasks'. This is a compelling motivation, but unlike humans, the proposed approach does not involve any method for autonomously selecting the appropriate level of abstraction for a given task. It is unclear how this human-in-the-loop approach will be scaled in a realistic manner, and the authors do not devote enough attention toward addressing this limitation.

It would help if the authors could add more discussion of this issue, and also describe more concretely the scenarios in which they envision this approach being useful. Given pre-trained models over a range of complexity levels, and some downstream task on which a user wants to fine-tune, would it not be much more straightforward to simply use a validation set from the downstream task to evaluate the full set of pre-trained models? This also suggests the possibility of automating the process of selecting the right abstraction level in a meta-learning setup.

Limitations:
There are no discernible negative societal impacts related to this work.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper proposes a framework for human-in-the-loop training of machine learning models where humans select among pretrained models with different complexity levels based on prototypes. The authors demonstrate that finetuning performance is significantly impacted by representation complexity in the experiments considered. Moreover, a user study demonstrates that humans are relatively successful at helping choose the correct complexity level for finetuning. 

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
- The idea of using humans to help select abstractions for a given problem is an interesting idea for human in the loop ML. 

- The proposed VQ-VIB approach seems like a reasonable implementation for this use case.

- There seems to be solid empirical verification of the very intuitive connection between representation complexity and fine-tuning performance.

Weaknesses:
- The authors only show transfer performance within single benchmarks rather than across tasks or in realistic pretraining and finetuning settings such as ImageNet or LLMs. 

- The authors do not compare with other methods for human in the loop training in which the amount of human effort can be compared. For example, feature engineering is another way that humans can potentially impact downstream performance -- a technique widely used in practice. 

- The paper also does not discuss the effort involved in providing this human guidance. 

Limitations:
The authors do not really address the limitations of the work. I believe the main limitations are the lack of alternative methods for human-in-the-loop training considered i.e. feature engineering and the use of somewhat synthetic benchmarks for evaluation that are disconnected from real-world use cases. It is natural to wonder if humans can be as helpful at prototype based abstraction selection for transfer tasks that are more distant from each other and not drawn from the same benchmark. 

Rating:
6

Confidence:
3

REVIEW 
Summary:
The authors observe that the downstream tasks for pretrained models can rely on representations of varying level of complexity: as a running example, a birdwatcher relies on significantly more complex representations of images to classify bird species, relative to a child who may want to identify the color of a bird. They thus propose pretraining a variety of models with representations of varying complexity, having a human choose the appropriate pretrained model for the task, and then finetuning that model for the downstream task. Relative to finetuning the model with the most complex representations, this should lead to increased data efficiency.

The authors suggest a modification of VQ-VIB (vector-quantized variational information bottleneck) for the pretraining in this setting, called VQ-VIB_C (C standing for categorical) – my understanding is that the main difference is that in the authors’ method, the representation is divided into n chunks and each of the n chunks is snapped to a quantized vector (the loss remains the same). I have not spent much time delving into the details as the authors say this is not their main contribution: in particular I may be wrong about the differences from VQ-VIB.

The authors implement this idea for a variety of image-based datasets: FashionMNIST, CIFAR-100, and iNaturalist 2019 (iNat). They pretrain an encoder using a reconstruction loss as well as a classification loss for the labels defined by the dataset, and increase the regularization on the representations to generate a variety of checkpoints that produce representations of different complexity. They then evaluate these encoders in a downstream classification task that is coarser than the original classification task (e.g. living vs. non-living for CIFAR-100).

Their experiments show that (1) VQ-VIB_C outperforms two baselines (VQ-VIB, $\beta$-VAE), and (2) when the classification task is simple (e.g. 2-3 classes) and there are very few data points (e.g. 1-5 examples per class), it is better to use a pretrained encoder with lower representational complexity (though in all other cases it is typically fine to use the encoder with maximal representational complexity).

They also perform a human experiment in which they show that, given visualizations of the learned quantized vectors, humans can correctly identify which encoder will perform best during finetuning.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. To my knowledge, the idea of using simpler representations for downstream tasks in order to improve data efficiency is novel, and I think it is an interesting hypothesis to explore.
2. Although the authors don’t consider it their main contribution, their experiments suggest that the VQ-VIB_C performs notably better than other alternatives on their task. (However, I did not investigate in detail, e.g. it is possible that the baselines were not tuned well while the VQ-VIB_C was.)
3. More generally, the experiments are quite detailed and look at the effects of various hyperparameters in the method.
4. The paper has an experiment with real humans to justify its claim that humans could choose an appropriate model to use for a downstream task. While I think the experiment is pretty different from realistic conditions, this is still better than the vast majority of papers on ideas like these, which often don’t bother testing their claims about real humans at all.

Weaknesses:
I have a few concerns:
1. Significance: It seems to me that the author’s experiments suggest that the main idea only has a benefit in very limited settings, which are unlikely to arise in practice.
2. Baselines: For k > 1, the appropriate baseline would be to use the most complex representation, with strong regularization determined using a validation set at finetuning time.
3. External validity: The experiments involve downstream tasks that are pure coarsenings of the pretraining task, whereas in typical settings the downstream tasks will likely not be these “pure coarsenings”, which would reduce performance.
4. Relevance of the human experiment: The author’s experiment seems to “give away” the answer (though there is a case to be made that this reflects a strength of the method, rather than a weakness of the experiment).

Overall I feel conflicted about this paper. On the one hand, it has a nice idea with careful technical work done to flesh it out, and a large set of experiments to understand how the idea works in practice. On the other hand, I think the experiments suggest that the idea is not very practically useful (whereas the paper suggests they validate the idea), despite the experiments having some aspects that bias them towards showing the idea to be good. Overall I’m recommending a borderline reject, but I can see the case for acceptance as well.

**Significance**

Looking at the experiment results (including the ones in the appendix), it appears that if n > 3, or k > 5, or you use any method other than VQ-VIB_C, it is usually best to use the most complex representation (i.e. the one that achieves highest reconstruction loss). Thus the benefits of the idea only occur when n <= 3 and k <= 5, which implies a tiny dataset of 15 examples or fewer. As we might expect from such a small dataset, the resulting finetuned classifiers do not perform particularly well, getting around 70-90% accuracy.

Thus it seems like the idea in this paper is only helpful when (1) there is a very simple classification task, (2) there is very limited finetuning data available, (3) you use VQ-VIB_C rather than a different method, and (4) the user is happy with performance numbers of 70-90% accuracy. This seems extremely restrictive, and I find it hard to think of a realistic setting that satisfies all of these constraints (especially #4). Overall, I would characterize these experiments as providing a negative result.

(I would actually be more inclined to accept a version of this paper that was upfront about this, and framed the paper as a negative result that has taught us that lower representation complexity only buys you a little bit of data efficiency, that is overwhelmed very quickly by a large enough dataset.)

**Baselines**

In the paper’s experiments, the finetuning is done the same way for representations with varying levels of complexity (I believe, at least I didn’t see anything to contrary in Appendix 7). However, a natural baseline would be to use the regular (maximum-complexity) representations, but use stronger regularization to learn a simpler classifier. One might reasonably argue that it is unclear how to choose this hyperparameter – but at least when k > 1, it should be possible to take 1 example (or more) per class to form a validation set that is used to tune the hyperparameter. I think it is plausible that this significantly improves performance for high-complexity representations. If it does work it would be a significant improvement, as there would no longer be any need for human input.

**External validity of experiments**

All of the experiments in the paper have the downstream task labels $Y_t$ being a strict coarsening of the pretraining task labels $Y_p$ (or in other words, $Y_t$ is a deterministic function of $Y_p$). However, this may not be the case in realistic settings – for example, in the running example, there may be birds that are of the same species but have different colors, and so a representation that just identifies the species would be insufficient for predicting the color. Should we expect the method to work even in these cases? I’m not sure; the fact that the pretraining includes a reconstruction loss suggests that it would still work (though likely not as well as in the paper’s experiments). Ideally however the authors would conduct such an experiment to test it empirically.

**Relevance of the human experiment**

Looking at the pdf for the user study (Appendix 11), the instructions seem to “give away” the answer. In particular, the instructions contain:

> Generally, the more images the second robot is using, the less general it will be, and the worse it will do at categorizing the 3 new high-level categories.

and

> If the visualization shows that (1) the robot is not using many images, and (2) they roughly represent the three high-level categories [...] then the robot should perform well

and

> For example, here is one robot’s visualization which is perhaps too general, as there is not even three categories being used.

From which it is easy to infer “choose the option with fewest categories, subject to having at least three categories” – which then leads to the desired answer.

Arguably this represents a strength of the method – the approach for selecting the appropriate model is so simple that it can easily be communicated even to non-experts. However, in this case, it is so simple that there isn’t really any need for human input – we could equally well find the appropriate model using a simple heuristic of choosing the option that has a few more “most important images” than the number of classes in the classification task. (Perhaps this is another baseline which should be compared against.)

**Minor issues**

Line 91:
> Lastly, we assume that the pre-training labels are a sufficient statistic for the task-specific labels: $I(X; Y_p) = I(X; Y_t)$. Intuitively, this states that the pre-training objective must include relevant information for the finetuning task.

I don’t think that’s the right criterion. For example, for reconstruction pretraining with red-yellow classification as the downstream task, we have $Y_p = X$ and $Y_t$ is whether the bird is red or yellow. Then $I(X; Y_p) = I(X; X) = H(X) \geq 1$ (since entropy of natural bird images is large), but $I(X; Y_t) \leq H(Y_t) \leq 1$ (since $Y_t$ is a binary variable).

I think what you mean to say is that $I(X; Y_t \mid Y_p) = 0$, that is, conditioned on knowing $Y_p$, there is no more information about $Y_t$ that can be learned from $X$.

Limitations:
The limitations brought up in the weaknesses section are not mentioned or addressed. In particular, I think the issues raised in “Significance” and “External validity of experiments” should obviously be mentioned (or otherwise addressed); the others are more debatable.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper explores an interesting premise -- how does the level of abstraction captured by a discrete (visual) representation dictate downstream task performance, where downstream tasks can be at arbitrary levels of abstraction. Specifically, the running example from the work that I really like is that of bird-watching; when communicating amongst experts, the extremely specific species names (e.g., ""white osprey"") is very useful, and captures the right level of information. However, when communicating with small children who are looking at things in the wild, coarser descriptions (e.g., ""the white bird"" or ""the red bird"") are much more useful. 

To study this premise, this paper makes two contributions: first, it introduces a new method for learning discrete representations at various levels of abstraction, the Vector-Quantized Variational Informational Bottleneck - Categorical (VQ-VIB_c). Second, and more importantly, it presents a thorough evaluation, including a human-in-the-loop user study (N = 17), showing conclusively for downstream tasks with ""known"" abstraction levels, the ""best"" representation to use is the one that roughly matches that level of abstraction.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
This paper starts with a strong motivating question, introduces a new method to explore the hypotheses formed from this question, and critically performs a comprehensive evaluation and user study that confirms the proposed hypothesis. I think the user study linking ""prototypes"" from different models (capturing different levels of abstraction) to the downstream finetuning task's level of abstraction is the most convincing, and most meaningful result in this work. I really like this type of study, and it really helps support the paper's key contributions.


Weaknesses:
Unfortunately, I have several concerns with this paper, stemming from the very confusing presentation of the paper (it took me a very long time to understand the experimental protocol, and I found that several details about the pretraining/finetuning procedure were missing or placed in scattered places in the Appendix), to the actual useability of such an approach for more practical applications.

First, on the presentation/soundness side -- I ask this question more explicitly below, but what is the exact link between the process of ""degrading"" a high-accuracy model (pretrained to high accuracy on the ""full"" classification task), and emerging levels of abstraction? In other words, how exactly does the process of changing the loss hyperparameters lead to models that capture different levels of granularity? From my read of the paper/appendix, this link is never made explicit, and the process is further left underspecified -- if hyperparameters are changing in a staged way (or you're running for a limited number of additional gradient steps given the ""high accuracy"" model), couldn't a possible confound be the initial batches seen during this ""degradation process""? Another confound could just be the per-example/per-class difficulty, which is known to be uneven across classes in the datasets (e.g., iNaturalist has severe label imbalance out of the box, papers from the Distributionally Robust Optimization literature report huge discrepancies in average vs. worst class accuracy for other datasets)? Is this controlled for in any way? 

Beyond these questions, I find it unclear how such an approach would be useful for practical applications? Is the idea that given some supervised dataset, you should train all models to ""best accuracy"" then selectively degrade the resulting representations depending on your (family of) downstream tasks? Isn't this expensive/redundant? 

---
EDIT (Post-Rebuttal): I think many of my soundness concerns have been addressed during rebuttal, and in discussion with the other reviewers. Raising my score to a 5.

Limitations:
The main body of the paper does not explicitly discuss limitations; the appendix discusses some of the limitation of the VQ-VIB_c approach in passing, but a proper treatment of the pros/cons of this approach is not present.


Rating:
5

Confidence:
4

";1
nkfSodI4ow;"REVIEW 
Summary:
This work introduces XYZ Data Efficiency, a framework that combines curriculum learning and data routing techniques to improve data efficiency in training recent large models. In detail, authors implemented an efficient difficulty metric calculation method for large datasets by utilizing map-reduce, on top of which authors tried various curriculum learning techniques. Furthermore, by analyzing the limitations of existing data routing techniques, authors developed random-LTD that drops different tokens for different Transformer layers. Finally, authors demonstrate their XYZ Data Efficiency framework leads to achieve the baseline accuracy with less data, or achieve the better accuracy with the same amount of data.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. Developing a general, efficient, and easy-to-use framework for curriculum learning for large models hasn't been explored before to the best of my knowledge. Given the high cost of training recent large models, such library can enable more active research in this field.
2. Their data routing technique (i.e. random-LTD) is thoughtfully designed, and seems to improve the final performance of large Transformers across different tasks.

Weaknesses:
This paper touches on multiple aspects of improving training efficiency of large models, especially from the data perspective, but none of them seem to meet the NeurIPS standard.
1. Firstly, I would argue that random-LTD has almost nothing to do with data efficiency. It looks to me that random-LTD is actually closer to some regularization, particularly dropout [1]. Just because one can achieve the same performance with 2x less data with some regularization techniques (e.g. weight decay), calling them as a ""data efficiency trick"" cannot be justified in my opinion. Since it bypasses some computations of some tokens in some layers, I believe it's closer to a computation efficiency rather than data efficiency trick. From the systems perspective, however, random-LTD consistently hurts the overall throughput as shown in Table 3 & 4 somehow.
2. I doubt the practical utility of map-reduce-based data difficulty calculation. The metrics used in this paper are all offline metrics in that they can be calculated only once before training and can be reused later. While I understand even such preprocessing can take a painfully long time with recent large datasets (e.g. Pile or C4), I don't think the value practitioners will get from this paper would be not so significant. If they can show their framework can be combined with some online or dynamic metrics (e.g. loss value for each token), I would be more convinced.
3. XYZ Data Efficiency framework seems to lack the flexibility and/or modularity, a highly important aspect in the framework. For example, the use of certain CL techniques require specific LR schedules to enjoy the maximal improvement. This essentially means that users get a reduced flexibility in choosing their own LR schedulers. Such entanglement between LR schedulers and data sampling strategies can further harm the user experience when they want to implement their custom data sampling strategies. Overall, my impression is that XYZ Data Efficiency doesn't allow much flexibility for users to try out different things, but rather enforces users to follow their predefined pipeline, in this case, composed of random-LTD and several CL strategies.

To summarize, I find two major framing issues in this paper. First, while CL can be approached from the data efficiency perspective, I believe random-LTD (or data routing) has little to do with data efficiency. Second, I believe XYZ Data Efficiency is more of a combination of two algorithms (i.e. CL and random-LTD) rather than a some general framework due to its lack of flexibility and modularity.

[1] Liu et al., Gating dropout: Communication-efficient regularization for sparsely activated transformers

Limitations:
N/A

Rating:
3

Confidence:
4

REVIEW 
Summary:
In this paper, the author proposes XYZ data efficiency framework to improve the data/training efficiency in the foundation model training. The proposed framework mainly consists of two techniques, i.e., (1) the efficient data sampling via general curriculum learning library and (2) efficient data routing via random layer-wise token dropping.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The CL-based library is open-sourced and compatible with PyTorch.

* The proposed method achieves considerable training acceleration with minor or no accuracy degradation.

* The author evaluates their method on several different models, including the large language models.


Weaknesses:
* Missing the full term of “CL” in the introduction section (line 44). The first explanation shows in line 64.

* The author argues that the previous methods require changing the data loader, data sampler etc. However, the proposed method still needs to change then as well.

* Besides the TokenBypass, there are also several data routing techniques for foundation model training. E.g., [1] [2]. The author should also include those works for discussion and comparison.

[1] EViT: Expediting Vision Transformers via Token Reorganizations. ICLR 2022

[2] Peeling the Onion: Hierarchical Reduction of Data Redundancy for Efficient Vision. AAAI 2023

* I think the previous work [2] also explores the efficiency at the data sample-level and data routing level. So, it is probably not appropriate to claim the proposed method is “the first to demonstrate that composing data sampling and routing techniques can lead to even better data/training efficiency …”

* As the author claims the proposed framework is easy-to-use and admits being open-sourced as one of the contributions of this work, it would be better if the author could submit the anonymous code with the supplementary materials.

* Though the proposed method can achieve considerable overall acceleration, it would be great if the author can provide a discussion about the overhead of data sampling and routing part.


Limitations:
N/A

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper draws inspiration from the observation of training costs increasing quadratically with data size, leading to a focus on enhancing data efficiency. To address this issue, the paper presents a framework that optimizes data utilization, improves training efficiency, and enhances model quality. The framework introduces efficient data sampling and data routing methods designed to overcome the challenges associated with data size. Extensive experimental results conducted on various foundation models confirm the effectiveness of the proposed methods, validating their ability to achieve improved data efficiency and overall model performance.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The paper introduces a framework that combines general continual learning (CL) techniques with random layerwise token dropping for data sampling and data routing. This framework aims to address the challenges of CL by incorporating the random dropping of tokens at each layer, enabling efficient data processing and routing during the learning process.

2. The effectiveness of the proposed method is demonstrated through experiments conducted on various foundation models. The results highlight the remarkable data efficiency achieved by the framework, showcasing its ability to handle continual learning tasks effectively while maintaining high performance with limited data.

Weaknesses:
1. While data efficiency is recognized as crucial for various tasks, the paper could provide a more comprehensive study and presentation of how the proposed method enhances models across different data sizes. A more thorough investigation and analysis of the impact of the proposed method on models of varying data sizes would contribute to a deeper understanding of its effectiveness.

2. It is worth noting that the paper's verification of the proposed method is limited to four models. Expanding the experimental evaluation to include a broader range of models would provide a more robust assessment of the method's performance and its applicability across different architectures. This would enhance the credibility and generalizability of the findings.

Limitations:
Please refer to the weakness and question part.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes XYZ Data Efficiency, a framework that makes better use of data, increases training efficiency, and improves model quality. The proposed framework features efficient data sampling, efficient data routing, and an easy-to-use practical framework that is integrated into an existing library. 

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- The targetted application on improving the data and training efficiency is an important problem, especially in the era of large models. This paper makes a practical step in this direction. 

- I like the writing of the introduction and related works, which provides a structured review and highlights the goal of this paper and its difference from other papers. 

- The description of the proposed method is pretty detailed, which can help the reader to have a detailed understanding of how this framework is implemented and part of the time/computation overhead to run this framework. 

- Overall, I would say this paper makes good engineering efforts to make the 

Weaknesses:
- My main concern about this paper is the evaluation observations. Specifically, it seems that under lower-budget training settings (e.g., training with less data and training time), the improvement over the baseline actually shrinks. Will this make this method can only be suitable for teams with a large amount of data and computation resources?

- In the paper, the authors mentioned that previous methods for improving data/training efficiency fail to achieve satisfactory performance under large-scale settings. Is there some numerical evidence for this claim other than the one shown in Table 1? 

Limitations:
The authors may want to address the limitations of this paper in their final version. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper introduces XYZ, a data sampling and routing framework designed to enhance the efficiency of training large transformer models. XYZ incorporates a user-defined curriculum learning metric for data sampling and leverages token dropping to reduce computational overhead. The authors propose random layer-wise token dropping (random LTD) to efficiently apply token dropping per layer, capturing attention dependencies between tokens in intermediate layers with high probability. The framework's effectiveness is validated through experiments on pretraining GPT-3, GPT-3 MoE, and BERT, as well as finetuning GPT-2 and ViT, achieving up to a 12.5x reduction in data/time/cost.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. This paper introduces the random layer-wise token dropping technique, which demonstrates a novel approach to enhance the efficiency of large transformer model training. 
2. The evaluation is across various models of different sizes, including GPT-3, GPT-3 MoE, BERT, GPT-2, and ViT. 
3. The paper provides a comprehensive and detailed account of the training setting used in the experiments. Additionally, the authors present a thorough analysis of the results and observations obtained from the experiments. 
4. The paper shows substantial efficiency gains using the XYZ framework.

Weaknesses:
1. One notable weakness of the proposed framework is its relatively limited performance compared to the baseline when operating at a smaller data scale, as indicated in Figure 6. Further investigation and clarity on the factors contributing to this limitation would be valuable for understanding the framework's practical applicability across various data scales.
2. Despite claims of open-sourcing the XYZ framework, anonymized code or a link to access the implementation is not provided. 
3. The paper does not explicitly address the limitations of their proposed framework or discuss any potential negative societal impact.

Limitations:
The authors have not explicitly discussed the limitations and potential negative societal impact of their work in the paper.

Rating:
5

Confidence:
3

";0
QFcE9QGP5I;"REVIEW 
Summary:
This paper presents first-order iterative methods for solving unconstrained minimization problem. The connection between the proposed methods, quasi-Newton and Anderson accelaration methods are illustrated, which gives an insight of the motivation. Under certain assumptions, the methods exhibit explicit gobal non-asympotic convergence rates adaptively using backtracking strategy. The effiency of the propsed methods is compared to a fine-tuned BFGS algorithm with line search in the numerical experiments.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The problem is well-motivated.

- Under certain assumptions, the proposed adaptive first-order methods achieve explicit global convergence rates that blend those of gradient descent and cubic regularized Newton's method.

- The inputs of the algorithms are carefully discussed. It should be clear for others to implement.

- The algorithm complexity is analyzed and compared in the numerical experiments.

Weaknesses:
- Too many assumptions and requirements in the theoretical part. Can those requirements be verified? Seems the requirements are post to let the proof go through.  

- The algorithm setting for the numerical experiments in section 6 is a bit confusing. For example, what online techniques are used for 'Iterate Only', 'Accelareted Forward Only', 'Forward Estimate Only' and 'Greedy'?

- The performance of the accelarated algorithm is suboptimal and unstable.

Limitations:
- The performance of the accelarated algorithm is suboptimal and unstable.
- Analysis is provided only for Algorithm 1 not Algorithm 2 while the later seems better. 

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes a generic framework for developing new iterative schemes for smooth optimization in the deterministic setting. The derived new methods show some similarity to quasi-Newton methods and Anderson acceleration, while using a backtracking line search for estimating the Lipschitz constant and having step sizes adaptively determined by minimizing an upper bound of the objective function or the gradient norm. The explicit, global and non-asymptotic convergence rates are established for one type of the derived methods. Numerical results are presented for some specific implementations of the framework.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
The paper proposes a novel framework inspired by the recent development of cubic regularized Newton's method. This framework leads to several optimization methods that can be viewed as new variants of quasi-Newton methods and Anderson acceleration. The theory seems to be solid. The explicit, non-asymptotic convergence rates are established under different assumptions of the optimization problems, justifying the introduced techniques. The content is informative and the main idea is easy to follow. Numerical results are presented to support some claims of the paper.

Weaknesses:
The weaknesses are listed as follows:

1. The theory requires the Lipschitz continuity of the Hessian in $\mathcal{R}^d$, which is a strong assumption in practice.

2. The theorems only consider the iteration number towards convergence, which may overestimate the efficiency of the algorithms. Since the number of function evaluations can be large in each iteration, it needs to be clarified whether the claimed convergence rates are still valid when taking these hidden calculations into account.

3. The biggest drawback of the proposed methods is their suitability for solving high-dimensional problems. To well approximate the Hessian which is critical for the convergence, the memory usage needs to be large, which can cause failure of the algorithms when the memory resource is limited. What's worse, the algorithms are very complicated. They use a backtracking line search to estimate the Lipschitz constant and solve a minimization problem to determine the stepsizes in each inner step of the line search. It is unknown how many times the minimization problem needs to be solved during each iteration. It is likely that the computational cost of each iteration is much higher than the classical quasi-Newton methods or Anderson acceleration. However, there is no discussion about this issue.

4. The algorithms were only tested for solving small-scale logistic regression problems. Some numerical results do not support the theory. For instance, the accelerated method does not exhibit any acceleration in practice. In many cases, the simple BFGS with the default setting still achieves the best performance, while the proposed methods are less efficient due to higher costs.

5. Since many additional calculations are hidden in the subroutines of the algorithms, it is more convincing to report comparisons of the considered algorithms with respect to running time. However, these results are missing.

There are also some other concerns about the paper:

1. In Section 2, the paper claims that quasi-Newton methods and Anderson acceleration share the common property that the iterates are combinations of previous iterates and the current gradient. This claim may be true for quadratic optimization but seems to be wrong in general cases. There is no justification for this property throughout the paper. The presented framework is more like a generalized heavy-ball or Nesterov-like method, or can be viewed as an adaptive version of the subspace Newton method. Its connection with quasi-Newton methods and Anderson acceleration is not clear. It is misleading if such a connection is not valid.

2. Although the presented framework does not use the traditional line-search or trust-region technique to guarantee global convergence, it uses a more costly backtracking line-search strategy to estimate the Lipschitz constant. Such complexity can impair the significance of this work.

3. Section 1.2 says that Anderson acceleration does not generalize well outside quadratic minimization. It is not true since Anderson acceleration is well-known for its usefulness in accelerating fixed-point iterations in scientific computing.

4. Some descriptions of the classical methods are not standard. The formulas of BFGS and Anderson acceleration in Section 2 are quite strange. The formula of BFGS below Line 97 seems to be wrong. The formula of Anderson acceleration below Line 104 is incorrect since $d_0$ and $g_0$ are undefined. The formula below Line 513 is not the Anderson acceleration.

5. Many notations are not defined clearly, e.g., $P_i$ in Equation (2), $r_i$ below Line 104, and $R^\dagger$ in Line 600. 

6. The paper discusses many possible methods derived from the framework, but the pros and cons of each method lack clear clarifications. For example, the orthogonalized greedy method seems to outperform BFGS in some cases, but it also doubles the memory usage. So the comparison in the experiments may be unfair.

7. The proofs need to be reorganized to make them easy to follow. It is better to give more examples of the introduced notations and assumptions.

Limitations:
Some limitations have been mentioned in the main paper, but there is no discussion about the total computation cost and memory cost.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The authors introduce a generic framework for developing novel quasi-Newton and Anderson/Nonlinear acceleration schemes, offering a global convergence rate in various scenarios, including accelerated convergence on convex functions. They also provide empirical results in the numerical experiments.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The structure is clear and the theoretical analysis and proof are correct. The results presented in the numerical experiments section are consistent with the theoretical results.

Weaknesses:
There is the following weakness regarding this submission.

1. First, the authors ignores a lot of existing quasi-Newton type methods or accelerated versions that achieve the non-asymptotic global convergence rates of $\mathcal{O}(1/k)$ and $\mathcal{O}(1/k^2)$ such as:

“Practical inexact proximal quasi-Newton method with global complexity analysis”. Katya Scheinberg and Xiaocheng Tang. Mathematical Programming160(2016), pp.495–529.

“Proximal quasi Newton methods for regularized convex optimization with linear and accelerated sublinear convergence rates”. Hiva Ghanbari and Katya Scheinberg. Computational Optimization and Applications69(2018),pp.597–627

“Accelerated Quasi-Newton Proximal Extragradient: Faster Rate for Smooth Convex Optimization”. R. Jiang and A. Mokhtari. https://arxiv.org/abs/2306.02212

Recently there also exists a paper proposing the quasi-Newton type method that could achieve a global explicit superlinear convergence rate:

“Online Learning Guided Curvature Approximation: A Quasi-Newton Method with Global Non-Asymptotic Superlinear Convergence”. R. Jiang, Q. Jin, A. Mokhtari. Conference on Learning Theory (COLT), 2023.

However, this global superlinear convergence rate assumes strong convexity of the objective function. The authors should compare the results in this submission to all these related literature in detail to check if the proposed algorithm could achieve an improvement in the aspect of convergence rate or computational cost per iteration.

2. The authors didn't compare the results of the this paper to the global convergence rates of first-order gradient descent and accelerated gradient descent. The authors claimed that the proposed algorithm could match the results of accelerated gradient descent, but is the constant in the convergence rate of this proposed method better than the constant of accelerated gradient descent? What is the improvement of this algorithm compared with the accelerated gradient descent? Notice that the computational cost per iteration of this QN-type method is worse than the computational cost per iteration of accelerated gradient descent.

3. It's better for the authors to use the notations of $s_t = x_{t + 1} - x_t$ and $y_t = \nabla{f}(x_{t + 1}) - \nabla{f}(x_{t})$. These notations are more commonly-used in the quasi-Newton methods.

4. The notations used from equations (3) to (7) are quietly confusing. What's the exact definition of $y_i$ and $z_i$? Is it $y_i = x_{t - i + 1}$ and $z_i = x_{t - i}$. Also it seems that $D$, $G$ and $\epsilon$ depend on the iterations index $t$. Then it should be $D_t$, $G_t$ and $\epsilon_t$. These notations are messed and make the readers difficult to understand and follow the ideas of the authors.

5. What is the definition or function of the parameters $M_0$, $M_t$ and $M_{min}$ presented in the algorithms? The authors said that the $M_0$ is the initial guess of the smoothness parameter, So $M$ is an estimation of the parameter $L$ in assumption 1? What's the value of $M_0$ in all the numerical experiments presented in this paper? What theoretical conditions should this $M_0$ satisfy?

6. The authors presented some designed requirements in section 3.1. These expressions of requirements are a bit strange for the theoretical results. The authors should either make these requirements as the assumptions of the algorithms or make these requirements as some lemmas/theorems of the theoretical analysis. However, these requirements are too strong or restrictive as assumptions. On the other hand, the authors didn't give any strict mathematical proof to give any conditions to satisfy these requirements. The authors argue that these requirements are not restrictive in the text of section 3.1. But this is not enough for the theoretical analysis of the algorithm. We need strict and clear mathematical proof or empirical results from the numerical experiments. It is not clear how to make these requirements to be satisfied. There is no formal proof.

7. The authors claimed that $N$ could be small around line 149. However, as expressed in the theorem 6, it seems that $N$ should be comparable to the dimension $d$ to reach a good convergence rate. But when $N = \mathcal{O}(d)$, the computational cost of solving the sub-problems could be as expensive as $\mathcal{O}(d^3)$ as presented in line 148. This is as costly as the Newton's method.

8. From line 203 to line 210, the authors argue that some propertied needed to be satisfied to retrieve the convergence rate of Newton’s method with cubic regularization. However, it seems that when these properties are satisfied, the computational complexity is the same as the Newton's method. Then, what is the advantages or improvements of this method compared to Newton’s method with cubic regularization?

9. What are the definitions of $b_i$ and $\lambda_t^{(1, 2)}$ in the equation below line 213? The authors should explain these parameters for the accelerated algorithms.

10. The most significant weakness or drawback of this submission is the lack of formal algorithm for the update of matrices $Y, Z, D, G$. Although the authors presented the online and batch techniques in section 5, these descriptions of the update scheme is just an outline and too introductory. We need a lot of details and formal description of the algorithm for the implementations or updates of matrices $Y, Z, D, G$. Without explicit algorithm like Algorithm 1 in the paper, there are a lot of unclear operations regarding the update of the corresponding matrices. The authors should present a formal algorithm regarding the implementations of these updates in the appendix. There also exists a lot of issues for these algorithms. For example, in the deviations of Iterates, Forward Estimates and Greedy updates after the line 239, it seems that column numbers of matrix $Y$ and $Z$ are $N + 1$ instead of $N$. The dimensions are not consistent. Also to implement the orthogonal iterate in the equation under line 236, it needs the matrix $P_{t - 1}$ defined in equation (12). However, this definition in equation (12) needs the operation of matrix multiplication and matrix inversion, which could be very expensive in high dimension condition. Also, the authors argue that Orthogonal Forward Estimates can ensure that the condition number $\kappa_D = 1$ and the norm of the error vector is small. But this observation is not obvious and need detailed explanations. There is no formal or strict mathematical proof and theoretical analysis to ensure that these updates of matrix $D$ could satisfy the requirements presented in the section 3.1. At last for the batch technique, the authors apply the QR decomposition. However, the computational cost or complexity of QR decomposition could be very high in the high dimension condition and make the implementation very slow in practice.

Limitations:
Please check the weakness section for limitations.

Rating:
4

Confidence:
4

REVIEW 
Summary:
In the paper, the authors propose a new framework that connects Quasi-Newton methods with Anderson acceleration. By exploiting the Cubic Regularization technique, the authors achieve new competitive convergence rates comparable to first-order methods in the worst case and second-order methods in the best case. The paper describes various problem setups with their respective convergence rates: non-accelerated methods for non-convex, star-convex, and convex functions, as well as an accelerated method for convex functions. The authors also propose different variants of approximation. Experiments are presented for both convex and non-convex functions.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
I believe the results presented in the paper are both original and significant. The novel connection between Anderson averaging, Quasi-Newton methods, and subspace sketch-Newton-type methods is both new and prospective. The provided proofs seem mostly correct.
Furthermore, introducing online approximation techniques adds value to the research. The thorough comparison of these techniques in practical applications showcases their effectiveness and underscores their relevance.
In conclusion, this paper contributes valuable insights to the field, and its innovative approach holds great potential for further advancements in optimization methods.

Weaknesses:
I may guess that the paper was created in a hurry before the conference deadline, which might explain why it contains a lot of small mistakes and misprints. While the mathematical results are sound, The mathematical results are sufficient but the overall presentation and clarity need improvement. A thorough review and revision by the authors are necessary to address these issues. Next, I will present some of the mistakes and misprints that I find.

1) Page 6, line 191, Theorem 5. The brackets on the right side are unnecessary. On the right side, $f(x_t)-f^{\ast}$ should be changed to $f(x_0)-f^{\ast}$. The same is in the Appendix. 
2) Page 8, line 239. All $Z_t$ and $Y_t$ contain $N+1$ elements, but they should contain $N$. So, I think the indices should be corrected.
3) Page 36, line 785. $r_i$ was not defined anywhere before, moreover, $r$ is used in other places for different things. It probably should be $G_i$.
4) Page 36, line 787. The last formula in the line is incorrect; the left side is a vector, and the right side is a number.
5) Page 36, line 791. The inequality is incorrect because $(r_i - \nabla^2 f(x) d_i)^2$ is a vector and shouldn’t be squared in such a way.
6) Page 36, line 795. The third transition is incorrect because $\|w\|$ is missing for the $L$ term.
7) Page 37, line 800. I believe it is better to explicitly prove that the term $\alpha^T D^TG\alpha$ could be upper bounded by $\alpha^T (D^T G+G^T D)\alpha/2$; otherwise, it may be confusing. 
8) Page 5, line 174. The notation $\varepsilon_t$ as a vector is confusing because a page before $\varepsilon_i$ was a number, the coordinate of a vector $\varepsilon$. 
9) Page 4, line 127. $x_{+}$ is defined in Theorem 1 but never used inside. So, I think it can be removed. On the other hand, $x$ is used inside but not really specified. 

Next are the moments that caused confusion for the first read. 

10) Page 3, Motivation. The transitions between line 97 to formula (2) and then to formula (4) are very confusing. It may help to specify what are the $\alpha_i$ and $P_t$ for that case. For formula (4), the author may say that it is a special case of formula (2) when $H_0=0$. It may help to understand such a transition.
11) Page 4, line 142, formula 10. The norm of $\|D\|$ is not defined, which may cause some confusion because $D$ is a rectangular matrix and could have different norms. 
12) Page 7, Algorithm 5. The notation $(M_0)_1$ is very confusing, especially when $M_0$ and $M_1$ also exist. Moving index $t$ of the step to the upper level, like $M^t_0$, may solve both this moment and $\varepsilon_t$ moment, but it is up to the authors how to resolve these issues with the notation.
13) Page 15, line 498, formula (19). As a small comment, I would suggest using the keyword “subspaced” or “sketched” instead of “stochastic” to avoid confusion with stochastic optimization methods such as SGD and others. 


Limitations:
Yes


Rating:
6

Confidence:
4

";0
HvWfTrjwWa;"REVIEW 
Summary:
This paper proposes a method for estimating the balancing weights in causal effect estimation. The balancing weights are given by the density ratio of the counterfactual distribution and the observed distribution, and the density ratio can be estimated by solving the optimization of the variational expression of the f-divergence between the two distributions. The authors claim that using the $\alpha$-divergence is useful because it addresses the vanishing-gradient problem when we use a neural network for the model. The paper presents some theoretical results.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
- Expressing $\phi$ as $\exp(T)$ in Eq. (14) is interesting since the domain of the optimization variable becomes simpler.
- The discussion about vanishing gradient is interesting, and the suggested way of setting $\alpha$ may be useful to address this issue.

Weaknesses:
- There are several technical parts that might not be precise. See the Questions section below.
- The writing could be improved. The paper rather looks like a collection of definitions of results. Connections between sections are not smooth, and the message of the paper as a whole is not clear.
- The abstract says, ""we selected $\alpha$-divergence as it presents efficient optimization because it has an estimator whose sample complexity is independent of its ground truth value and unbiased mini-batch gradients; moreover, it is advantageous for the vanishing-gradient problem."" I am not sure what makes the $\alpha$-divergence special in terms of these aspects. Also, the explanation about the vanishing-gradient issue in Section 5.1 is not convincing enough to make me believe that the authors' suggestion really addresses the issue.
- The paper relies on references (including one without open access) for important definitions. The paper could be more self-contained.
- Overall, the paper is mainly about estimating density ratios by the variational expression of the f-divergence, which is not novel.
- There is no experiment in the main part of the paper.

Limitations:
The authors discuss limitations of the work.

Rating:
4

Confidence:
2

REVIEW 
Summary:
This paper proposes an approach for estimating balancing weights. The ultimate target is to use these weights for the estimation of the casual effects of interventions. The search of balancing weights are formulated as density ratio estimation. To tackle this problem, the authors employ a variational representation of the $f$-divergence. . The density ratio is modeled by a neural network. Specifically, this paper advocates the use of $\alpha$-divergence, and provide various ways to improve the practical performances of the proposed weights.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- This paper successfully makes use of the density ratio learning tools based on $f$ -divergence for causal inference applications.
- This paper summarizes various related existing results on density ratio learning via variational form of $f$ -divergence.
- This paper provides detailed discussion on various practical improvement of the proposed algorithm, which would benefit practitioners.

Weaknesses:
- This paper lacks theoretical results on the causal effect estimation (not the density ratio learning), which is regarded as a key target of this work.
	- It is unclear whether the proposed estimator obtain optimal convergence rates and (semi-parametric) efficiency, when degenerated to standard settings like average treatment estimation, and conditional average treatment estimation.
- This work is not impressive in terms of novelty, as many significant components follow more directly from existing work.
	- The variational form of $f$ -divergence and the corresponding estimation of density ratio has been explored by references [15] and [16].
	- The main theoretical result (Theorem 6.1) seems to be a fairly standard dual result (e.g., Theorem 4.4 of https://arxiv.org/pdf/1003.5457.pdf)
- The theoretical results are not stated clearly. Many results are not stated in rigorous statements with careful listing of assumptions. Some seem to be exclude certain important cases like mixture distributions, as claimed to be a major contribution of this proposed work. (see my question below.)

Limitations:
Section 8 discusses limitations on sample size requirements. The authors claim that the sample size required for controlling the error of $\hat{Q}_{K_0}^{(N)}$ has an exponential dependence on the dimension. Although I have questions regarding this theoretical results (as stated above), I would not be surprised about the curse of dimensionality. However, since the ultimate target is the causal effect, I am more interested in a direct error or sample complexity analysis of the causal effect. The lack of such analysis is a major limitation of this work.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This paper derives a novel method of reweighting covariates for the purpose of causal inference. This is done through learning optimal change of measure weights modeled via neural networks and trained using alpha-divergence measures between the true joint distribution and a mutually independent one. Further tweaks are introduced to help enable this procedure to be more practical in its implementation. 

A brief disclaimer: I am not very familiar with causal inference (to be honest I am not quite sure how I received this paper to review); however, I am quite familiar with variational inference and related techniques. I will focus primarily on the latter during my review, but please keep all of this in mind while reading my comments.

Soundness:
4

Presentation:
2

Contribution:
4

Strengths:
All of the work presented is very precise and detailed, with novel contributions that I can see with regards to the learning of the weights via the alpha-divergence objective. I found the result of the main algorithm to be particularly concise and intuitive for such an involved derivation (e.g., almost trivial to describe but definitely not to prove). As far as I could tell, all of the decisions made throughout the proposal were well justified and deliberate.

Weaknesses:
The precision presented definitely came at the cost of readability in my opinion. The paper is very notation heavy and introduces many concepts rapidly and with great precision that lead me to having to re-read sections many times over to understand the message. It feels like the paper could benefit from summarizing some of the technical details in the main paper and reproduce the more exact version in the appendix.

Additionally, I understand the work faced space limitations; however, I believe it should definitely have the numerical experiments presented in the main paper. If pressing for pages, I could see including the experiments in the main paper and moving section 7.1 (either partially or completely) to the appendix.

I found no weaknesses in the technical information itself from what I could understand.

Limitations:
The authors very adequately discussed limitations.

Rating:
4

Confidence:
2

REVIEW 
Summary:
This work presents the way to estimate the balance weight, that represents the causal effects of arbitrary mixture of intervention, by using the neural network. Specifically, authors recognize this balance weight as the density ratio between the source and balanced distributions, and estimates this ratio by optimizing the variational representation of  $\alpha$-divergence with $\alpha \in (0,1)$. In this procedure, authors justify why the $\alpha$-divergence should be used in terms of the property of the estimator, such as the sample complexity and unbiasedness, and the vanishing gradient issue for training.  



Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* This work seems to have a solid theoretical explanation for the balance weight estimation through neural network.

Weaknesses:
First of all, I confess that I am not expert in this field, and thus ask for understanding if my feedback does not make sense in this field.

* The proposed has not been compared with other baselines for balance weights.
> Authors claim that the proposed approach is a general method that can estimate balancing weight even when the datasets are generated from arbitrary mixture of discrete and continuous interventions. This means that the proposed method could be comparable with other baselines if either discrete interventions or continuous interventions exists on datasets. I believe that this comparison seems necessary because it can validate whether the proposed method can be regarded as the general method or not by showing that the proposed method is competitive with the existing methods.  However, since the proposed method was validated only for mixed variable interventions, its effectiveness on single intervention (either discrete or continuous) is not clear.

 
*  The advantages of the proposed approach seems less clear as compared to the existing baseline.
> In appendix, table 3 shows that the baseline approach (Entropy Balancing) outperforms the proposed method in most cases. Therefore, I am skeptical about why the proposed approach is meaningful. Does the proposed method have any other advantages over the baseline approach, that are not shared at current draft ?

Limitations:
See above Weaknesses.

Rating:
4

Confidence:
1

";0
fcYObrixSS;"REVIEW 
Summary:
The paper studies the research problem of shelf(DINO)-supervised articulated 3D shape reconstruction. The key idea of the paper is to factorize shapes into different primitives, and to model the shape primitives using both global and local deformation parameters. The parameters of the model are optimized in a kinematics-inspired way, where the 3D kinematics are projected into 2D to utilize the DINO supervision. The experiments show the proposed model outperforms previous state-of-the-art methods, including both primitive and holistic reconstruction approaches.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
1. The high-level idea of factorizing primitive parameterization into global and local deformations make sense.
2. The performance of the proposed method has been well evaluated and clearly outperforms previous state-of-the-art methods.

Weaknesses:
1. As a non-expert in this field who does not have much knowledge about physics-based deformable model, I find this paper hard to follow and sometimes not self-contained. Not being able to understand some details about kinematics could be caused by my limited knowledge about physics, but, importantly, it's not clear to me at all why the kinematics-based optimization approach is adopted in this paper. Conceptually, I understand optimizing primitives without direct 3D primitive supervision requires some sort of regularization -- but what kind of prior knowledge or physical facts are encoded in this model, given that there are no real forces/materials? What are the assumptions/hypothesis that generate these forces? And what makes the local deformation to be small/local? It is really important to make the high-level intuition clear before dive into the details. I also wonder if one uses the global+local deformable model in this paper but with simple optimization methods (e.g. directly sample points from primitive surface and project to 2D, then minimize the mask L1/MSE/BCE loss), will such simple alternatives not converge/lead to inferior performance?

2. In the quantitative ablation (Fig.6), the performance gap between the model w/o local and the full model does not seem too large (e.g. compared to the gap between LEPARD and Hi-LASSIE in Table 2). What is the typical std/error bar of this evaluation?

Other comments/questions:
1. It would have been beneficial to provide the dataset statistics.
2. How is K decided? (L164)
3. What is the limitation/failure cases of the proposed method?

Limitations:
A detailed discussion about limitation and failure case is missing now and should be included.

Rating:
4

Confidence:
2

REVIEW 
Summary:
The paper introduces LEPARD, a framework for reconstructing the 3D shape of animals from single images. LEPARD reconstructs 3D shapes as parts, which are parameterized primitive surfaces with global and local deformations. LEPARD is trained using off-the-shelf deep features without the need for 2D or 3D annotations. Experimental results demonstrate more detailed shape reconstruction.

Soundness:
2

Presentation:
1

Contribution:
2

Strengths:
**Method:**
- 3D reconstruction as parts is an interesting approach to 3D reconstruction and is generally less explored than holistic reconstruction. Exploration in this direction can be beneficial to the community.
- The proposed method reconstructs details better by modeling coarse shapes as well as detailed deformations of each part. This is an intuitive approach and appears to be effective.

**Experiments:**
- The comparison with SOTA is thorough and complete to the best of my knowledge. The improvements over SOTA appear to be significant.
- The ablation study clearly demonstrates the effectiveness of each component.

Weaknesses:
I have several confusion about the method.

- The model parameters are not defined precisely. It’s unclear how q_c and q_theta are defined ( how are they just c and R? or is there any difference?)
- Details regarding how the local deformation d is obtained from v_s are missing. The concept of stationary velocity field and Gaussian smoothing layer should be explained more in detail to help understanding. Also, the paper should include justification for using stationary velocity field and Gaussian smoothing layer.
- I don’t understand Equation 5. The author introduces the model Jacobian matrix without defining what the model is. It’s also unclear why the velocity x is relevant in this static reconstruction setting. In addition, it’s unclear how Equation 5 is derived.
- In L152-153, what is the practical meaning of “the energy of the primitive” and how is this energy and the force f_3d energy related to 3D shape reconstruction?
- how is the loss in equation 12 derived?
- In equation 14, shouldn’t G^i be part-based (as described in L162 and illustrated in Fig 3) and hence be denoted as G^(i,k)? Otherwise, why is the rendered part mask compared with the mask of the whole object? If G^i is part-based, how are the part labels from the DINO feature associated with the K primitives?

I find it hard to understand the proposed method and hence cannot recommend acceptance given the current state. Please address my questions above

Limitations:
There is no discussion about limitations in the paper.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper presents LEPARD, a framework for reconstructing the 3D articulated shape of animals from a single in-the-wild image. It explicitly represents the parts as parameterized primitive surfaces (superquadrics) with global and local deformations in 3D. The authors employ a kinematics-inspired optimization to guide the deformations. Besides, LEPARD is trained solely using off-the-shelf deep features from DINO, without requiring any 2D or 3D annotations. Experiments on the Pascal-part and LASSIE datasets show the superiority of the proposed method.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
- The paper is clearly written and easy to follow.
- The authors propose to use local non-rigid deformations to capture fine-grained shape details, which is different from previous works.
- The authors propose a framework to compute image-based forces based on the discrepancy of DINO features and the projected primitive parts.

Weaknesses:
There are a limited number of categories in the datasets evaluated. Can the authors qualitatively evaluate their methods on held-out images from other sources for the same training categories and some unseen categories (maybe fine-tuning a small set of images)?

Limitations:
The authors have not adequately addressed the limitations. It will be nice if the authors can discuss, e.g., 1) the effect of the quality of psuedo labels generated by DINO on the final performance; 2) the bottleneck of the current method for the further improvement.

Rating:
8

Confidence:
3

REVIEW 
Summary:
The paper describes a method for fitting K superquadric geometric primitives (enhanced with tapering, bending, and diffeomorphic local deformations) to a set of images of an animal category (e.g. elephant). The main contribution is that the method requires no supervision, and uses 2D feature correspondence to impose constraints on 3D shapes.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
Superquadrics with limited deformations offer a good balance between geometric expressivity and parameter compactness.

The end-to-end image supervision of 3D geometry is fundamentally sound. Thus, relying only on 2D feature correspondences, and translating those to 3D constraints, is a robust way to aggregate information from an unstructured image collection.

The results are compelling and a clear improvement on previous work.

Weaknesses:
Animal bodies are kinematic chains, where one limb affects another. This approach does not take that in account.

Even though the paper focuses on animals, it would be quite useful to see how it performs on humans and how it compares to human-specific baselines.

The paper could use a section discussing the limitations and future work.

Limitations:
The paper does not explicitly address its limitations in a traditional limitations section.

It is not explicitly stated whether the number of primitives, K, is computed automatically or manually specified. It would be a limitation if it has to be manually specified.

The method treats each primitive separately from others, which does not faithfully represent the kinematic chain that is an animal shape.

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper proposes a part-based method to reconstruct 3D shapes in a category-specific manner. Compare against its baseline LASSIE, the paper uses an elegant primitive part representation that could capture both global and local deformations to increase the fidelity of reconstruction. The method also does not need test-time optimization process like LASSIE. The proposed method outperforms LASSIE quantitatively and qualitatively. 

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. The proposed primitive part representation is novel. Compare against modeling deformation implicitly,  the parameterization of the representation is compact and intuitive for understanding. The new representation increase the fidelity of the reconstruction. 

2. No need for test-time processing.  The proposed method does not need per-instance optimization and only requires a forward-pass. 

3. No 3D or input requirements. The method takes only image as input, without the need of mesh template, category skeleton as additional input. For training, the method is self-supervised and does not need 2D/3D annotations and could be easily scale-up to in-the-wild data. 

4. Impressive qualitative results. The visual result shown in Fig. 4 is impressive. The results looks quite strong as the detailed structure of the animal could be fully recovered in a primitive-based approach. 

5. Outperform baseline method in all categories. 

Weaknesses:
1. The motivation of introducing image force in the training objective is not well explained. As LASSIE adopts simple silhouette loss by differentiable rendering, the paper introduces complex Jacobian matrices and generalized forces. What kind of benefit could we get from this and what is the difference and limitation of using silhouette loss against LASSIE. The author should include more experiments to demonstrate the superiority of such kinematic-inspired procedure. 

2. Several questions not fully understand. Please questions below. 

3. No discussion of the limitation of the method and when the method would fail. 

Limitations:
Please see comments above. It would also be quite interesting if the output parts could be further retargeted to novel poses. The part-based approach is hard for animation against parametric models. 

Rating:
6

Confidence:
4

";1
SlXKgBPMPn;"REVIEW 
Summary:
This work proposes another auto-regressive-based graph generative model similar to GRAN. The authors propose a hierarchical generation scheme to un-coarse a graph level by level. In each non-leaf level, the abstract graph is weighted both in nodes and edges. A node represents a community, and its weight represents how many edges should be inside the community. An edge is the ""connection"" between two communities, and its weight represents how many edges should exist between the two communities. The weights of each community are generated through a stick-breaking process. And the number of communities is automatically decided by it. The structure within the community is generated using an AR model. Then the edges between communities are generated using GNN.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The assumption makes sense, and the model decomposition is quite convincing.
2. This method can indeed improve generation efficiency by only auto-regressively generating the diagonal blocks of the adjacency matrix and using GNN (which has O(M) runtime) to predict the off-block entries.
3. The method is simple and straightforward.

Weaknesses:
see questions below

Limitations:
1. one possible limitation is the edge independency of the model.
2. The model has made a strong assumption that the graph should have a community structure, while the experiment datasets are relatively small and may not have such a structure.

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper proposes HiGen a hierarchical generative graph model. The model consists of a clustering process (Louvain), followed by a GNN model (GraphGPS) to estimate probabilities. The generative process is separated by communities and bipartite sub-graphs.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
The paper seems original. The proposition of a new model is always an important contribution. Even though the new model is the combination of a clustering process and GNN. The combination of both ideas is interesting. 

The theoretical quality of the demonstrations is good. Most of them seem fine and no errors were observed during the revision. 

Parts of the papers are quite clear. Figure 1 really helps to understand the main idea of the paper. However, there is room for improvement.

The significance of the paper is high, it seems that this new model is able to reproduce the mean of the distribution quite correctly in comparison to other state-of-the-art methods, as it is shown in the results.

Weaknesses:
The state of the art can be improved. The paper mentions ""there exists no data-driven generative models specifically designed for generic graphs that can effectively incorporate hierarchical structure."". Neville et al. focused on this type of work, generating several papers related to hierarchical graph models (doi.org/10.1145/3161885, doi.org/10.1145/2939672.2939808, doi.org/10.1007/s10618-018-0566-x). 

Parts of the paper are closed related to mKPGM (doi.org/10.1145/3161885). In both cases there is a hierarchical structure, both have the idea of a super-node at the higher level, and the sampling process is also based on a multinomial distribution (doi.org/10.1007/s10618-018-0566-x). Please take a look at the sampling process proposed, because it has similarities to the proposition of this paper, and the authors claimed to sample a network with billions of edges in less than two minutes.

The paper must state its main contribution. In the beginning, it seems to be the model, but after reading the paper, it seems to be the sampling process. Unfortunately, both of them have different issues.

If the main contribution is the model, then the paper should improve the modeling of the main network and be fairly compared in the experiment section against other baselines (not just the mean of the distribution). The main models consider $\ell$ hierarchies, but just two are applied. It is also not clear how the final probabilities are obtained. 

If the main contribution is the sampling process, there are some issues too. The time complexity of the generative model claims to be O(n_c \log n), but this is not demonstrated. The results of the paper are focused on the modeling of networks, not the sampling process. For example, there are no empirical results about the time complexity, and the largest networks have some thousand nodes, rather than millions.

I understand that the papers follow the experimental setup and evaluation metrics of Liao et al. However, this methodology must be stated in the main paper, otherwise, the experiments of the main paper are not reproducible. 

The results of Table 1 are difficult to read because of the lack of explanations. There are no details on the separation of the data in the main paper. I understand that this is explained in the supplementary material (80% for training and 20% for testing), but it must be considered in the main paper too. Moreover, I do not know if the values are the average over the 20% of the testing graphs or, if you just considered it as a single distribution. In the first case, please add the standard deviation, to see if the difference at statistically significant. 

Section 5 claims: ""The results demonstrate that HiGen effectively captures graph statistics"". Considering that, generally speaking, MMD estimates the distance between the means of two distributions, I suggest you change it to ""The results demonstrate that HiGen effectively captures the mean of the graph statistics"". Given the use of MMD, you can not determine if the other part of the distribution are correctly estimated. 

The conclusions state that HiGen ""enables scaling up graph generative models to large and complex graphs"" but this is not demonstrated.

Limitations:
No, the authors did not consider the limitations of the proposed model. For suggestions, please check weaknesses.

Rating:
4

Confidence:
4

REVIEW 
Summary:
The paper introduces an innovative hierarchical method for graph generation, which employs multiple levels of graph coarsening. This approach begins with the first level, representing the most coarse graph, and progressively expands nodes and edges to form new communities and connections between the newly created nodes. At each level, nodes serve as communities for the subsequent level, and the edge weights, including both inter-community edges and self-loops, dictate the total number of edges within each community in the final graph. Consideration of independence among the generation processes of inter and intra-community edges, conditioned on the graph and edge weights from previous levels enables parallel execution of the steps, resulting in acceleration of the generation process.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The paper effectively utilizes hierarchical clustering to enhance the graph generation process, capitalizing on the benefits of this technique.
2. By introducing parallelization in generating distinct clusters at each level, the paper successfully minimizes the number of sequential steps required.
3. The experimental results presented in the paper demonstrate improvements across multiple datasets.
4. Paper for the most part is well-written and easy to follow.

Weaknesses:
1. In lines 35-36 paper mentions that this work is the first hierarchical method for generic graphs. I believe [1] is also a hierarchal method for graph generation. I understand that methods are significantly different but still it would be more accurate to highlight the unique aspects of the proposed method and consider including a comparison between the two approaches.
2. The time complexity analysis provided in the paper focuses solely on the sequential steps, neglecting to consider the computational requirements. It would be valuable to compare the overall computational workload, particularly since the proposed method utilizes the GraphGPS approach, which has a time complexity of $O(n^2)$, in contrast to conventional GNN methods with a complexity of $O(n+m)$. Including such a comparison would provide a more comprehensive analysis.
3. The paper lacks a study examining the distribution of community sizes during the generation process across different datasets. Addressing this limitation by investigating and reporting the distribution of community sizes would enhance the understanding of the method's behavior and its adaptability to various datasets.
4. The paper uses a more advanced GNN compared to methods like GRAN, raising the question of how much of the observed progress is solely due to the change in the GNN architecture. Conducting an ablation study specifically focused on the GNN architecture used would provide valuable insights into its individual contribution to the overall performance of the method.
5. The evaluation metrics commonly employed for graph generative models have their limitations, as discussed in [1] and [2]. It is important to consider these limitations in the evaluation process. The paper mentions the use of random GNNs as an alternative evaluation method, but this approach is only used in the appendix for a few experiments. I would suggest using this in the main body and comparing all models using this metric. [Additionally/Optionally, there are two more recent approaches, one based on contrastive training and another one based on Ricci curvatures that could be incorporated for evaluation purposes.]

[1] Shirzad, H., Hajimirsadeghi, H., Abdi, A. H., & Mori, G. (2022, May). TD-gen: Graph generation using tree decomposition. In International Conference on Artificial Intelligence and Statistics (pp. 5518-5537). PMLR.

[2] O'Bray, Leslie, et al. ""Evaluation metrics for graph generative models: Problems, pitfalls, and practical solutions."" arXiv preprint arXiv:2106.01098 (2021).

Limitations:
Considering the assumed independence among the clusters and the cross-edges connecting them, it is evident that there exist certain graph distributions which the model may struggle to learn.

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper introduces a graph generative model that is analogously structured as the inverse process of graph pooling, where the model first split a single node into a metagraph. This metagraph is further partitioned by utilizing a multinomial scheme, which allows for the division of nodes and edges into intra-community and inter-community connections. The proposed model's performance is evaluated on several benchmarks using various metrics, demonstrating state-of-the-art results.

Soundness:
2

Presentation:
1

Contribution:
3

Strengths:
1. The approach of initially generating a graph's skeleton and subsequently refining its details is a novel and intuitively logical motivation for the proposed model.

2. The proposed methods have demonstrated state-of-the-art performance on some widely adopted benchmarks.

Weaknesses:
1. Some important technical aspects in the paper may require additional clarification or more detailed elaboration. Here are the major concerns regarding specific aspects:

    (1) Can the authors please provide more information on the loss function utilized in the model?

    (2) How is the weight on level 0 determined during model inference?

    (3) On line 188, the node embedding matrix $\mathbf{h}_{\hat{C}}$ is referenced without being defined. Could the authors please explain how this matrix is generated from the node and edge embeddings of prior levels?

    (4) Could the authors please elaborate on how the graph neural network (GNN) is utilized throughout the entire process?

2. The utilization of notations in the paper has resulted in a significant amount of confusion. There are two main issues that need to be addressed:

    (1) Inconsistent notations caused by reusing the same symbols: One notable example is the letter ""t"" used at lines 187-188, which has multiple interpretations. In $\hat{C}_{i,t}^l$, ""t"" represents the ""t-th"" step in the stick-breaking process. In $h{(t, s)}$, ""t"" denotes the node that is associated with community ""i"". Moreover, when referring to the node matrix size as ""$t \times d_h$"", it indicates the total number of nodes in community ""i"". These varying interpretations of the same notation can lead to confusion and should be clearly distinguished or explained consistently throughout the paper.

    (2) Notations used without being defined: An example is the ""r"" symbol in Figure 1 (c). Although it is assumed to represent the acronym for ""remaining (edges),"" its precise definition is not explicitly provided in the paper. To enhance clarity, it would be beneficial to define such notations explicitly or provide a glossary of symbols and their corresponding definitions.

3. In the paper, the specific method for determining the number of mutually exclusive events (i.e., the edges split from the same parent node) when modeling the partition weights using a multinomial distribution is not explicitly mentioned. This aspect requires further clarification or explanation. The paper should provide details on how the number of events is determined, whether it is considered a fixed parameter based on the model's architecture or if it is treated as a latent variable to be inferred during the training process.

Limitations:
1. Please refer to questions 2 & 3.

2. The model is built upon the assumption that the graph contains underlying communities. While this assumption can aid in generating higher-quality graphs with evident community structures, it may come at the expense of the generation quality for graphs where the community structures are less apparent. It would be intriguing to explore how the quality of generated graphs varies with changes in graph modularity or other community metrics.

Rating:
5

Confidence:
3

";0
PITeSdYQkv;"REVIEW 
Summary:
This work designs a generic algorithm that transforms an item-level DP algorithm into a user-level DP one with $\sqrt{m}$ improvement in user complexity. It recovers previous user complexity bounds on various learning tasks, and the transformation works in the example sparse setting, i.e. does not require sufficient number of samples per user.

Soundness:
3

Presentation:
4

Contribution:
4

Strengths:
1. Compared to Ghazi et.al. 2021 which requires sufficient number of samples per user, the generic transformation from item-level to user-level DP in this work applies to the example scarce setting. The authors derive new bounds in the example-scarce setting for PAC learning.
2. The authors recover the $\sqrt{m}$ factor in many known bounds for various learning tasks through a generic algorithm. 
3. The writing is clear and easy to follow.

Weaknesses:
1. The proposed algorithm is not computationally efficient.
2. Although $\sqrt{m}$ matches the tight bounds for a variety of problems, it seems that the lower bound is not addressed in this work.
3. In Theorem 16, it might be helpful to be specific about how sufficiently small should $\varepsilon', \delta'$ be. 

Minor issues:
1. In Lemma 4, I feel that it should be $A\simeq C$ instead of $B$ at the end of the line.
2. In line 242, it appears that the $\delta'$ inside the big O should be $\delta''$, and there should be a factor of $(1+e^{\varepsilon''})$ for the $\delta''$ term according to Lemma 4 (even though it does not affect the final bound on $\delta'$).


Limitations:
Limitations and potential impacts are adequately addressed.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper studies the problem of user-level differential privacy, giving a generic conversion for any item-level approx DP algorithm to a user-level approx DP algorithm, and a clipped scoring function that allows application of the exponential mechanism to obtain user-level privacy. All results are applicable in the few-samples-per-user setting, significantly extending results beyond the case in which users have enough samples to solve a task independently. 

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
This work significantly improves our understanding of when user-level privacy is achievable, though it is unfortunate the transformation from item-level to user-level privacy isn't computationally efficient. The intermediate result that approx DP => sample perfect generalization is also a meaningful contribution to our understanding of the relationships between useful algorithmic stability notions. 



Weaknesses:
In terms of substance, I think this work is very strong, but the clarity of presentation could maybe be improved a little. That said, I do think the authors did a good job overall of assisting the reader with interpretation of their results. 

Notes:

The first paragraph of the technical overview for the approximate DP result was a bit confusing.

Page 4: “it is sufficient get new user-level pure-DP”

Page 6: 	“For example, g is not Lipschitz everywhere anyway anymore”




Limitations:
Yes, the authors adequately address societal impact of their work. 

Rating:
8

Confidence:
4

REVIEW 
Summary:
This paper establishes new generic conversions from item-level DP algorithms to user-level DP algorithms, continuing a line of work initiated by Ghazi, Kumar, and Manurangsi. Previous state-of-the-art due to BGHILPSS '23 shows that any item-level $(\varepsilon, \delta)$-DP algorithm using N samples can be converted to a user-level DP algorithm with $n = O(\log(1/\delta)/\varepsilon$ users and $m \approx N^2$ samples per user. The present submission observes that this previous result is just one end of a spectrum of possible item-to-user-level conversions, and generalizes it to hold for the full range of $1 \le m \le N^2$. That is, the main result (somewhat simplifying) shows a conversion from any item-level DP algorithm to a user-level DP algorithm with $n \approx (N/\sqrt{m}) \cdot \log^{O(1)} ({1/\delta})/\varepsilon^2$ users and $m$ samples per user. In addition to generalizing the conversions of GKM'21 and BGHILPSS'23 to the regime of few examples per user, this result unifies previous work on specific problems such as mean estimation, stcochastic convex optimization, and discrete distribution learning for which a $n \propto 1/\sqrt{m}$ dependence was previously observed.

As an auxiliary result, the paper also gives a tight characterization of the sample complexity of user-level pure $(\varepsilon, 0)$-differentially private PAC learning in terms of the probabilistic representation dimension.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- This is a nice contribution to the emerging general theory around user-level DP learning, in particular, building a bridge between one thread of work on general item-level vs. user-level connections and another on understanding the sample complexity of user-level DP for specific problems.

- As part of their analysis, the authors more carefully investigate the connection between DP and perfect generalization, in particular giving a clean solution to an old open question of CNLRW'16 that is likely to have further applications.

Weaknesses:
- The main technical ideas going into the algorithms and their analysis appeared in prior work on this topic. (E.g., the passage from DP to perfect generalization appears in GKM'21/BGHILPSS'23, and a similar PTR-based argument appeared in KL'21/GKKMMZ'23). This paper's technical contribution is to carefully refine each of these steps and piece them together in the right way.

- Neither of the paper's new algorithms is computationally efficient in general, even if the task admits a computationally efficient item-level DP learner. Note that this is the case for previous general item-to-user-level conversions as well.

- The final main result (Theorem 9) is somewhat messy to state and potentially suboptimal in its dependence on $\log(1/\delta)$ and $\varepsilon$. This is likely in part due to stacking various generic tools (DP => PG, amplification by subsampling, PTR) that could potentially be avoided with a simpler algorithm.

Limitations:
Limitations (overlapping with points identified as weaknesses) are nicely described as directions for future work in the conclusion.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This work develops novel generic algorithms that convert algorithms under item-level differential privacy (DP) to user-level DP in the low user sample regime, where each user holds a small number of samples, and obtains new results for user sample complexity (aka., min # users to achieve a certain utility of the algorithm). Based on a novel observation that connects sample perfect generalization to local deletion DP, the proposed algorithm under approximate-DP leads to tight user sample complexity bound for several privacy-preserving machine learning applications. The proposed algorithm under pure-DP is based on the exponential mechanism with a new score function and it also leads to tight and improved user sample complexity bounds for several applications.

Soundness:
4

Presentation:
4

Contribution:
4

Strengths:
The improved user sample complexity results and proposed algorithms under both approximate DP and pure DP are interesting and are widely applicable to several important privacy preserving machine learning tasks. 

The paper is well presented and intriguing to read. The notations are well-defined throughout the paper. Problems and results are well defined and clearly stated. The contribution and overview of techniques are well summarized. 

The paper tries hard to give the readers intuition on the techniques developed and does a good job explaining the observations.

The paper also explains well its difference from the previous works, especially why the techniques from previous works do not apply when the number of samples per user is low.


Weaknesses:
The only places that might need a bit more explanation is why local deletion DP is natural to consider based on sample perfect generalization results, and how local deletion DP is then converted to user level DP?

Limitations:
Limitations (esp. the runtime of the proposed algorithm) are well discussed in the paper.

Rating:
8

Confidence:
3

";1
vBx0yNQmik;"REVIEW 
Summary:
This paper introduces an approach on Federated learning using dataset distillation techniques

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The idea of using dataset distillation for FL is interesting
2. The solution is reasonable 
3. The experimental results show the effectiveness of the proposed approach 

Weaknesses:
1. In a few equations, the details is not provided. For instance $L_CE$ in 3, $Dist$ in 5. The paper should be self-contained 
2. The technical contribution is low
3. In the experimental results, Table 1, can you highligh both first and second place? In MNIST-M, the winner should be VHL/R, 85.7> FedLGD 85.2. 

Limitations:
The limitation on using virtual data should be discussed. Is there any drawbacks on using fake data instead of real data?

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper proposes a method called FedLGD that utilizes distilled virtual data on both clients and the server to train FL models. To address the synchronization issue and class imbalance, the authors use iterative distribution matching to distill the same amount of local virtual data on the clients for local model training, thereby improving the efficiency and scalability of FL. The authors also reveal that training on local virtual data exacerbates the heterogeneity issue in FL. To address this problem, they use federated gradient matching to distill global data on the server and add a regularization term to the local loss function to promote the similarity between local and global features. They evaluate the proposed FedLGD method on benchmark and real datasets and show that FedLGD outperforms existing heterogeneous FL algorithms.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The authors use visualization to reveal the limitation of local data distillation in federated virtual learning, which makes the motivation of the proposed method clear.
2. The proposed method preserves local data privacy by leveraging averaged local gradients to distil global virtual data.
3. The experiment results validate performance improvement and privacy protection.


Weaknesses:
1. The initialization for data distillation requires each client to calculate the data statistics and the server to aggregate these statistics, which still raises privacy concerns since the statistics contain some private information. How about using random initialization or other strategies? The authors need to justify it. 
2. Compared with VHL that uses untrained StyleGAN without further updates, the proposed FedLGD method needs to update the global virtual data iteratively.  Therefore, it is not surprising that FedLGD outperforms VHL. If the StyleGAN can be updated the same number of times as FedLGD, does FedLGD still outperform it? This requires justification or experimental validation.
3. The structure of the proposed method is not clear enough, which makes it difficult to follow. The authors first present the overall pipeline and then describe each component. However, the connection between the components and the overall pipeline is not clear. This requires significant revision.
4. The presentation quality is not satisfactory. There are too many typos and grammatical errors. Some notations are unclear, e.g., $i$ represents both the data index and client index; the subscript $t$ disappears in many places; $\tau$ is a set but denoted as a scalar in the caption of Figure 2.


Limitations:
1. As pointed out by the authors, data distillation incurs additional communication and computation cost. Further investigation is required to enhance the efficiency.
2. The proposed method performs well but lacks of theoretical analysis to support its performance improvement.


Rating:
6

Confidence:
4

REVIEW 
Summary:
This work proposes a method to address data heterogeneity from the perspective of dataset distillation, named FedLGD. Specifically, the proposed iterative distribution matching and federated gradient matching strategies are used to iteratively update the local balanced data and the global shared virtual data, and the global virtual regularization is applied to coordinate the data domain drift between clients effectively. This method can effectively solve the problem of client data imbalance and domain drift in heterogeneous data scenarios. Extensive experimental results show the effectiveness of the proposed method. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The problem of data heterogeneity studied in this paper is important for applying federated learning in real-world scenarios.
2. The idea of this paper to solve the problem of data heterogeneity in federated learning through the dataset distillation method is novel.
3. The authors perform various experiments to analyze the proposed method


Weaknesses:
1. Some symbols are written differently. The authors should unify these symbols. In section 3.1, local virtual data is written as $\widetilde{D}_{I}$, but in section 3.2 and 3.3 is written as $\widetilde^{D}{c}$, in Figure 2 is $\widetilde{D}_{t}^{c_{I}}$. In Eq. (3), $L_ {Con} $is about \ widetilde ^ {D} {g} and \ widetilde {D} ^ {c} function. But they do not appear in Eq. (4). The authors should give more details about Eq. (3) and (4). 
2. other approaches to address heterogeneity are personalized federated learning, e.g. FedAMP[1] Ditto[2] KT-pFL[3], etc. However, it is not mentioned in related work, and there is no comparison in experimental methods.
3. The legend and curve in Figure 3a do not match. In Table 1, ResNet18 generally performs worse than CNN. The authors mention that overfitting may be happening (at line 275). The authors should increase the dataset size or use a smaller model to make the results more convincing.
4. According to the results in Figure 4, the visualization results of FedLGD do not define the boundaries of each class well. Although FIG. 4 can prove that FedLGD solves the data drift of both clients, the degree of clustering of each class looks reduced. The authors should analyze it further. 


[1] Huang, Yutao, et al. ""Personalized cross-silo federated learning on non-iid data."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 35. No. 9. 2021.
[2] Li, Tian, et al. ""Ditto: Fair and robust federated learning through personalization."" International Conference on Machine Learning. PMLR, 2021.
[3] Zhang, Jie, et al. ""Parameterized knowledge transfer for personalized federated learning."" Advances in Neural Information Processing Systems 34 (2021): 10092-10104.


Limitations:
The authors discuss the limitations in section 5.

Rating:
6

Confidence:
3

REVIEW 
Summary:
To solve the challenges of synchronization, efficiency, and privacy, this paper presents a local-global distillation mechanism for FL (FedLGD). In FedLGD, an iterative distribution matching scheme is proposed to distill global virtual data to alleviate the heterogeneous problem. Experiments have shown superiority of FedLGD compared with existing FL methods. 

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The whole pipeline of FedLGD is well depicted in Figure 2. Each component involved in the pipeline is carefully designed.

2. It is an interesting idea to solve the existing FL challenges from the virtual learning perspective. This can inspire future studies in this direction. 

3. Experimental results look solid with sufficient implementation details. 


Weaknesses:
1. It seems that only feature heterogeneity is considered in this work. How the proposed method performs on different heterogeneous cases should be discussed. 

2. The definition of small distilled dataset is not very clear, which can affect the readers’ understanding towards the motivation and detailed technical parts. 

3. Privacy concern. Since there are image-level data transferred between the server and clients, it is better to further discuss the potential privacy-preserving risks. 


Limitations:
Yes. The authors have addressed the limitations. 

Rating:
4

Confidence:
4

";0
9Rhopbm4qu;"REVIEW 
Summary:
The paper addresses the overlap violation problem in observational datasets for causal inference by presenting an interpretable balancing method for overlap violation identification and causal effect estimation for binary treatments. The method BICauseTree adapts decision tree classifiers to the stated problem by recursively splitting the data population into non-overlap-violating subgroups based on covariate dissimilarity and treatment heterogeneity. The major advantage of the presented method in comparison to existing balancing methods is the interpretability of the prediction process.

Soundness:
1

Presentation:
1

Contribution:
1

Strengths:
• The authors evaluate their method on both synthetic and real-world benchmarking datasets.

Weaknesses:
- The proposed method is highly similar to the work in reference 12. Furthermore, related work is not discussed appropriately (section 2). It is thus unclear how this work significantly differs from previous contributions in the literature. The originality of the submission has to be considered very limited.
- The manuscript presents a complete piece of work. Claims about the performance of the proposed method are supported by experimental results.  Nevertheless, the an experimental study with sophisticated baseline methods is missing. A performance comparison with a Causal Forest model would be desirable.
- Claims aiming at motivating the method are neither supported quantitatively nor experimentally (e.g., an analysis with different levels of overlap violation; unbiased estimation). 
- The submission lacks clarity due to multiple grammar errors and many nested arguments. The mathematical notation (section 3.1) lacks formal correctness.
- The citation style does not agree with the required format for NeurIPS  submissions.
- The statement of a high ASMD indicating a confounder (line 164) needs to be justified.
- The paper would profit from a revision of the language and the consistency of the presented arguments (e.g., lines 84, 181/182). Furthermore, the figures and sections need to be referenced correctly, as also recognized by the authors in the appendix.
- The authors claim interpretability of the method but do not provide evidence for the statement. Here, a user study would be desirable etc. 
- The method is limited to ATE. 


Limitations:
- The authors have stated the limitations of their work. The section could be improved by highlighting the general weaknesses of single trees in prediction settings which naturally devolve to the proposed method.

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper proposes a new method called BICauseTree for interpretable causal effect estimation. BICauseTree is a hierarchical bias-driven stratification method that identifies clusters where natural experiments occur locally. The method is designed to reduce treatment allocation bias and improve interpretability. The authors evaluate the performance of BICauseTree on several datasets and compare it to existing approaches. They find that BICauseTree performs well in terms of bias-interpretability tradeoff and outperforms existing methods in some cases. Overall, the paper presents a novel and promising approach to causal effect estimation that could have important applications in various fields.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Novelty: The paper proposes a novel method called BICauseTree for estimating causal effects from observational data. The method is based on a hierarchical bias-driven stratification approach that identifies clusters where natural experiments occur locally. The method builds on decision trees to reduce treatment allocation bias and provides a covariate-based definition of the target population. The method is interpretable and outperforms other state-of-the-art methods in reducing treatment allocation bias while maintaining interpretability.

2. Significance: Causal effect estimation from observational data is an important analytical approach for data-driven policy-making. However, due to the inherent lack of ground truth in causal inference, accepting such recommendations requires transparency and explainability. The proposed method addresses this issue by providing an interpretable and unbiased method for causal effect estimation. The method has the potential to be applied in various domains, including healthcare, social sciences, and economics.

3. Experimental Evaluation: The paper provides a thorough experimental evaluation of the proposed method using synthetic and realistic datasets. The authors compare the performance of their method with other state-of-the-art methods and show that their method has lower bias and comparable variance. They also conduct sensitivity analyses to evaluate the robustness of their method to violations of the assumptions. The experimental evaluation provides strong evidence to support the claims made in the paper.

Weaknesses:
1. Limited Scope: The paper focuses on a specific method for causal effect estimation from observational data, and the scope of the paper is relatively narrow (especially related to the tree-based models). While the proposed method is novel and has some advantages over other methods, it may not be of interest to a broad audience: (1) The method relies on the quality of the data and the assumptions made in the model. If the data is noisy or contains missing values, the method may produce biased estimates.; (2)The method may not be suitable for high-dimensional data, as the number of covariates may increase the complexity of the decision tree and lead to overfitting; (3) The method may not be suitable for datasets with small sample sizes, as the stratification may lead to small sample sizes in some subgroups, which may affect the accuracy of the estimates;(4)The method may not be suitable for datasets with complex interactions between the covariates, as the decision tree may not capture these interactions effectively.

2. Experimental Evaluation: While the paper provides an experimental evaluation of the proposed method, the evaluation is limited in scope and does not provide a comprehensive comparison with other state-of-the-art methods. The experimental evaluation would benefit from a more comprehensive comparison with other methods (such as TARNet from the machine learning domain) and a more detailed analysis of the results.


Limitations:
See weakness.

Rating:
5

Confidence:
3

REVIEW 
Summary:
This paper focuses on achieving interpretable causal effect estimation, where the goal is to ensure that each decision within the algorithm is explicit and traceable. The authors propose a decision tree-based balancing method to address this problem, which identifies clusters where local natural experiments occur. The effectiveness of the proposed algorithm is empirically evaluated using synthetic and semi-synthetic data. The paper also did several ablation studies on the trade-off between interpretability and bias and the consistency of the decision tree.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Overall, this paper is well-executed and demonstrates several notable strengths.
- This paper is very clear. It effectively presents complex ideas in an easily understandable manner.
- The problem addressed in the paper is well-motivated and interesting.
- The authors display a strong grasp of the related work in the field, effectively positioning their contribution within the existing literature.
- The empirical analysis conducted in the paper is thorough and yields valuable insights.
- The method is intuitive and well explained.

Weaknesses:
- Style file: One issue with this paper is that it doesn't follow the NeurIPS style guidelines, specifically regarding paragraph spacing. The paragraphs are not well-separated, which makes it harder to read and understand the content. This affects the overall flow and coherence of the paper. Additionally, the excessive content allowed due to the spacing issue may be seen as unfair to authors who followed the style guide correctly. This may be a potential ground for rejection.


- Method: The rationale behind considering features with the highest ASMD as potential confounders is not well-explained. This is an important assumption in the paper, but it lacks a clear justification or empirical investigation. Providing additional explanations or conducting empirical studies would strengthen this aspect of the paper.

- The experiment section of the paper is not self-contained. Although the motivating problem revolves around identifying subpopulations with natural experiments, this aspect is not adequately illustrated in the experiment section. Instead, the focus is primarily on bias analysis, with the analysis related to interpreting the causal effect estimation process deferred to the appendix. This undermines the fulfillment of the paper's fundamental promise to the readers. To address this issue, it is highly recommended that the authors integrate the analysis into the main paper, ensuring that the key components align with the paper's core premise.


Limitations:
n/a

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper introduces a decision tree methodology to identify regions where selection bias no longer ensures covariate balance. These regions, which have some level of interpretability, can then be removed in subsequent analysis.

Soundness:
1

Presentation:
2

Contribution:
2

Strengths:
The paper presents an interesting decision tree methodology.

The paper contains a significant amount of simulation experiments to validate the procedure.

Weaknesses:
Despite a focus on covariate balance, there is no guarantee of balance unlike competing methods (rerandomization, matching, etc). Furthermore, there is limited analysis to show the claims of balance are fulfilled, especially for high dimensions.

The paper explores a bias-interpretability tradeoff, but provides no rigorous definition. The proposed model often is more biased than alternative models, most notably IPW, and it's not clear that the resulting decision trees, or their interpretations, are actually sensible. No discussion of estimator variance is given or how that might factor into a tradeoff, despite the high variance generally expected from decision tree estimators.


Limitations:
The paper touches on some limitations, most notably the increases bias that is expected. The paper does not discuss the variance of the estimator in detail relative to other methods, which is another potential limitation.

The method advocates for trimming nodes that violate positivity. These nodes could contain sensitive subpopulations and could lead to fairness concerns.

Rating:
3

Confidence:
3

";0
01GQK1gwe3;"REVIEW 
Summary:
In this paper the authors explore whether better optimization solutions can be found by jointly optimizing several inverse problems together. These inverse problems share a connection as they all can be formulated through a differentiable function $F(\xi_i | x_i)$. The authors implement the joint optimization through a set of NN parameters $\theta$ which connect all the different inverse problem variables $\xi_i = \hat{\xi}_i (\theta)$.

Upon reading the authors' rebuttal to the questions I posed, I am inclined to adjust my score accordingly.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The main problem that the paper is trying to solve is interesting and relevant. As the authors mention in lines 101-105 ""generic optimizers often fail to find the global optimum due to local optima, flat regions, or chaotic regions"".
* Also the setting expressed in section 3 is not typical which opens up many potential future work leveraging this setting.
* The experimental results are interesting in how they all improve the solutions by increasing $n$ which does provide evidence of cross-talk between the problems.

Weaknesses:
* The experiments generate inverse problems synthetically. I would like to see a real-life example of a problem that follows the settings exposed in section 3. I understand that several of the equations have practical applications but in this case I'm referring to a real-life problem that has a naturally occurring (not sampled from a known distribution) set of inverse problems that can be connected through a function $F$.
* The method is not applicable to many problems. I'm not familiar with any ML application that has similar optimization problems that can be pooled together. Put differently, it is unclear to me how restrictive is the setting in section 3 of having a function $F(\xi_i | x_i)$ that expresses all the inverse problems.

Limitations:
* The limitations that I see are encapsulated on the questions that I raised above.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper develops a novel approach to gradient-based non-convex optimization. The proposed methodology begins with the reparameterization of the parameter space utilizing neural networks, followed by the application of classical techniques such as BFGS, or alternative Neural Network surrogate models for the forward function, to accomplish the optimization task.

The efficacy of these methods is verified through four distinctive experiments, which include applications to the Kuramoto-Sivashinsky (K-S) equation and the Incompressible Navier-Stokes equation. The results indicate that the reparameterized optimizer delivers enhanced convergence overall.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper presents a compelling concept of reparametrizing the parameter space of the optimization problem using a neural network and accomplishing optimization via a two-step process that employs the trained/optimized neural network as a preconditioner. However, it remains unclear to the me as to why such a reparameterization is likely to benefit the non-convex optimization procedure. Despite this, the experimental results appear to indicate an enhancement, as evidenced by the four case studies investigated by the author.

Weaknesses:
My primary concern regarding this paper pertains to its lack of rigor. The authors do not clearly define the inverse problem that they are attempting to solve, nor do they provide cogent proofs or insights explaining why the introduced reparameterization would aid the optimization process. It is quite plausible that the limited experimental studies offered in this paper lack generalizability, and it's conceivable that there are counterexamples where optimizers, without the incorporation of reparameterization, achieve superior convergence.

Limitations:
While the authors recognize that they have not extend the method for constrained optimization problems, I believe there is an inherent limitation in the approach, as it lacks a mechanistic understanding of why such a method would be effective. This comprehension is fundamental for a method to be broadly applicable and reliable.

Rating:
4

Confidence:
2

REVIEW 
Summary:
The manuscript presents a method to reparameterize and solve multiple inverse problems jointly using neural networks. The manuscript tests the proposed method on multiple inverse problems (including some chaotic problems) and compares against Neural Adjoint and BFGS baselines to show measurable performance improvements.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
* The method is simple, and the authors haven't tuned architecture for problems, which would allow their usage as drop-in replacements.
* Comparison against baselines shows the method provides noticeable improvements.

Weaknesses:
* The main downside of these methods is the added training cost (which the authors have mentioned in the limitations section)
    * I would recommend adding the training wall clock times + solving times in a table to give potential users of this method a proper estimate.
* Adding benchmarks for the same problems used in the Neural Adjoint paper would strengthen the paper.

Limitations:
All limitations are clearly stated.

Rating:
7

Confidence:
3

REVIEW 
Summary:
This paper discusses a novel approach to finding model parameters from data, a crucial task in science. Traditional iterative optimization algorithms like BFGS can accurately solve simple inverse problems, but their reliance on local information can limit their effectiveness in complex situations with local minima, chaos, or zero-gradient regions.

To overcome these issues, the study proposes the idea of jointly optimizing multiple examples. The authors use neural networks to reparameterize the solution space and utilize the training procedure as an alternative to classical optimization. This method is as versatile as traditional optimizers and does not require additional information about the inverse problems, making it compatible with existing general-purpose optimization libraries.

The paper evaluates the effectiveness of this novel approach by comparing it to traditional optimization on a variety of complex inverse problems involving physical systems, such as the incompressible Navier-Stokes equations. The findings show significant improvements in the accuracy of the solutions obtained, suggesting that this method could be a powerful tool for tackling complex inverse problems.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
1. This paper points out a potential new use case of neural networks and deep learning for optimization instead of existing learning to optimize (L2O) methods, that is to use neural networks as part of classic optimization, trying to learn unknown common structures among problem instances of interest. A major difference is that generalization to unseen instances does not matter.

2. The paper is written in crystal clarity with information in every details about the methods, the experiments and the results. The authors discuss about the results of different methods for each setting. Limitations and outlook to future work are also faithfully discussed.

3. Improvements without refinements look impressive on all settings. Improvements after refinements still look great on the first three settings.

Weaknesses:
1. For the 4th setting, Incompressible Navier-Stokes, the proposed reparameterization method gives much higher mean losses than BFGS despite the fact that the majority of problems actually improve over BFGS. Could the authors elaborate more on the potential reasons specific to this experiment setting?

2. Since the mean losses could change with different dataset sizes because of the varying instance difficulty, would it be a better presentation of results with relative error or relative loss? (also a relative improvement could be better for results like Figure 4 in the Appendix.

3. 3~6 times more computational cost for the first three settings and up to 22 times for the fluids could be too high for the benefits achieved. This could be subjective but I hope the authors could provide some discussions or justification.

4. Would the ""similarity"" requirement be too strict to make the proposed method practically useful? For example, in the wave packet localization setting, the parameters A and $\sigma$ are fixed. Is there a practical application scenario that corresponds to this setting?

Limitations:
High computation cost and potential lack of practical applicability.

Rating:
6

Confidence:
3

";0
muFvu66v7u;"REVIEW 
Summary:
- The paper investigates the use of Lipschitz constrained networks to replace clipping functions and limit gradient sensitivity in DP-SGD.
- Lipschitz constrained networks are utilized as an alternative to clipping in order to address the issues of clipping's impact on convergence and performance in DP-SGD.


Soundness:
3

Presentation:
3

Contribution:
4

Strengths:
- The idea of removing clipping as an alternative to clipping itself is promising, as clipping is known to have detrimental effects on convergence and performance of DP-SGD, even without noise addition [1].
- The paper introduces the replacement of Vector-Jacobian product with Scalar-Scalar product to reduce computational complexity. The proposed methods outperform existing SGD approaches by a significant margin in terms of speed, which is crucial as memory usage and time inefficiency are major drawbacks of DP-SGD.

[1] Differntially Private Shaprness-Aware Training (ICML’23)

Weaknesses:
- Please refer to the questions.
- (Minor) There are several typos, such as the use of ""cotangeant vector"" which sounds little awkward, and inconsistencies in figure references (e.g., Fig 4 vs. Figure 5). Please carefully review the grammar and correct the typos.

Limitations:
The paper provides a detailed discussion of its limitations.


Rating:
5

Confidence:
4

REVIEW 
Summary:
Differentially Private (DP) Deep Neural Networks (DNNs) face challenges in estimating tight bounds on the sensitivity of the network’s layers. Instead, they rely on a per-sample gradient clipping process (as argued by the authors). This process not only biases the direction of the gradients but also proves costly in both memory consumption and computation. To provide sensitivity bounds and avoid the drawbacks of the clipping process, the authors provide a theoretical analysis of Lipschitz constrained networks, and uncovers a previously unexplored link between the Lipschitz constant with respect to their input and the one with respect to their parameters. By bounding the Lipschitz constant of each layer with respect to its parameters, the authors argue it will guarantee DP training of these networks.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper is well-structured and clearly written.

The theoretical part is simple and easy to follow.

Weaknesses:
Estimating Lipschitzness with respect to parameters may not be necessary. If the network is Lipschitz continuous with respect to the input, its gradient will be bounded, and thus the weight update will also be bounded. So, the motivation may not be rational.

Experimental results do not support the arguments. The validation accuracy of the DP-SGD is lower than several referenced works.



Limitations:
See weaknesses and my questions

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper addresses the problem of efficiently bounding the sensitivity of gradients in DP-SGD by using special architectures the layers of which can be proven to be Lipschitz with respect to the parameters, hence bounded gradient. They show how to recursively calculate the sensitivity of a sequence of layers, and incorporate the method in an algorithm to perform private SGD without clipping.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
The writing is exceptionally lucid. The concept is original and potentially significant, although there are at present many limitations.

Weaknesses:
There are a lot of constraints on the architecture that severely limit the potential of the method for short-term impact. I'm torn, because introducing the concept at this stage is of value, but far more work must be done -- both theoretical, in establishing the requisite bounds for popular architectures -- and experimental, in demonstrating that the approach achieves good points on the privacy/utility/efficiency Pareto frontier -- before we can assess the significance of the work.

I'm not convinced that it isn't a major problem that the gradients can vanish during training. This is the reason for the success of adaptive (layer-wise) clipping strategies. In particular see ""EXPLORING THE LIMITS OF DIFFERENTIALLY PRIVATE DEEP LEARNING WITH GROUP-WISE CLIPPING"" which would seem to enjoy the efficiency of your approach without the drawbacks of vanishing gradients or restricted architecture class.

Limitations:
Limitations are honestly and adequately discussed. No potential negative societal impact.

Rating:
3

Confidence:
4

REVIEW 
Summary:
The paper studies the question of how to do differentially private optimization without using per-sample gradient clipping, in order to simplify and speedup the iteration cost.
The paper proposes to restrict the class of functions to feed-forward neural networks for which it is feasible to compute bound on the gradient norm (Lipshitz constant), and proposes to compute adaptively the bound on the gradient norm (layer-wise) at every step of DP-SGD depending on the current iterate point. Paper provides the description of the algorithm, as well as evaluates its practical behavior.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- An efficient implementation of the algorithm is provided.
- Experiments show that per-iteration runtime of the proposed algorithm is indeed faster.
- Overall the paper is interesting and novel and provides a new direction for future research.

Weaknesses:
1. No clear comparison of the proposed algorithm to the baseline method (DP-SGD) is given in terms of the final accuracy. When restricting to the same architecture, it is unclear if the proposed algorithm can still reach the good accuracy compared to the classical DP-SGD with gradient clipping. Without clipping the gradients, the amount of the added DP noise to each gradient is larger than if you clip the gradients, which might hurt the final performance. 
2. From the experiments on CIFAR10 one might conclude that for the same privacy $\epsilon$ the final accuracy of the baselines is much better than of the proposed algorithm, which makes the proposed algorithm not applicable.
3. In the “local” strategy (line 201), how exactly did you calculate the amount of the noise to be added? I did not find a clear description of the “local” strategy, and how it is different from the “global” strategy.
4. Some parts of the paper are not very clearly written (see questions below).

Limitations:
yes

Rating:
6

Confidence:
3

";0
ldulVsMDDk;"REVIEW 
Summary:
The authors provided a theoretical analysis for independent subnet training and provided a convergence analysis in the case where communication compression is present.

The authors discussed the scenario when bias is not present and provided two analyses in the homogeneous and heterogeneous case, respectively before extending their theorems to the case with bias.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. Sections 3 and 4 are well written and clear, which break the scenarios down in an intuitive way and presented the theorems and outlines clearly.

Weaknesses:
1. Introduction could be more straightforward and can dive straight into the main technical contributions of the work. It was not clear why the problem is well motivated and what are the main technical hurdles until reading section 2 and onwards. A clearer presentation in the intro can make the paper much more readable and well-motivated.

Limitations:
The work is theoretical and does not seem to have any potential negative societal impact.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper provides a theoretical analysis of the convergence properties of Independent Subnetwork Training (IST), for distributed Stochastic Gradient Descent (SGD) optimization with a quadratic loss function. The analysis considers both the cases of homogeneous and heterogeneous distributed scenarios, without restrictive assumptions on the gradient estimator.

The work characterizes situations where IST converges very efficiently, and cases where it does not converge to the optimal solution but to an irreducible neighbourhood. Experimental results that validate the theory are provided in the Appendix.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper provides a solid analytical treatment of the important problem of distributed optimization with reduced communication overhead by means of Independent Subnetwork Training (IST).

Compared to previous work, the analysis of the paper does not rely on the restrictive assumption of a bounded stochastic gradient norm.

The paper is well written – the exposition is clear, and the material is well structured and well presented.

Weaknesses:
The work considers distributed Stochastic Gradient Descent (SGD) training with a quadratic loss. As mentioned by the authors in Section 3, a simple quadratic loss function has been used in other work to analyze properties of neural networks. While this loss function can still provide interesting theoretical insights, it would be valuable to extend the analysis and the experimental results to more generally used loss functions.

Minor comments:

-- Line 17: ""drives from"" may be changed to ""derives from"".

-- Equation after line 217: it seems that the first part of the equation
""$\mathbb{E}[g^k] = \bar{\mathbf{L}}^{-1} \bar{\mathbf{L}} x^k \pm \bar{\mathbf{L}}^{-1} \bar{b} - \frac{1}{\sqrt{n}} \tilde{\mathbf{D} b} = $ ..."" 
should be rewritten as
""$\mathbb{E}[g^k] = \bar{\mathbf{L}}^{-1} \bar{\mathbf{L}} x^k - \frac{1}{\sqrt{n}} \tilde{\mathbf{D} b} = $ ..."".

-- Equation after line 221: it seems that the first part of the equation
""$\mathbb{E}[x^{k+1}] = x^k - \gamma \mathbb{E}[g^k] = $ ...""
should be
""$\mathbb{E}[x^{k+1}] = \mathbb{E}[x^k] - \gamma \mathbb{E}[g^k] = $ ..."".



Limitations:
The work relates to distributed training of large-scale models, which generally correspond to significant power consumption and CO2 emissions. However, the IST method studied in the paper aims at allowing distributed training with reduced communication overhead, corresponding to potentially reduced power consumption.

Rating:
7

Confidence:
4

REVIEW 
Summary:
Independent Subnetwork Training (IST) is a technique that divides the neural network into smaller independent subnetworks, trains them in a distributed parallel way, and aggregates the results of each independent subnetwork to update the weights of the whole model.
This paper aims to analyze the behavior of IST theoretically. Specifically, it considers a quadratic model trained by IST. It conducts convergence analysis under both homogeneous and heterogeneous scenarios and shows that IST can only converge to an irreducible neighborhood of optimal solution.


Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
This is probably the first work providing a thorough theoretical analysis of IST.

Weaknesses:
1. This paper should include a more comprehensive motivation for the theoretical study of IST. This could involve discussing the potential limitations of current IST architectures and how a theoretical analysis can guide future modifications to improve their performance. By doing so, reviewers will have a clearer understanding of the significance of the paper's findings and how they can be applied in practice.
2. The main body of this paper does not have any experimental results. The authors should include some key experiments to validate their theoretical analysis.
3. The authors should consider expanding the scope of their experiments beyond quadratic models to include other types of models that are commonly used in SOTA IST papers, e.g., ResNet, and Graph Convolutional Networks, as listed in this paper's reference. This would allow reviewers to better understand the generality of the paper's findings and how they can be applied to real-world applications.


Limitations:
Future works are discussed in the conclusion section. 

Rating:
4

Confidence:
3

REVIEW 
Summary:
This submission presents a theoretical analysis for the independent subnetwork training (IST) algorithm for training models under both data and model parallel settings. The convergence guarantees are analyzed for quadratic loss functions, when using permutation sketches as the model compressors.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Optimization using data and model parallelization is an important practical problem, for which more theoretical insights are welcome 
- The submission presents a fairly thorough analysis of the convergence guarantees for IST for the quadratic loss function analyzed
- The authors highlight some limitations of IST, which would be useful to be aware of (e.g. the fact that in the general case of the quadratic function the algorithm does not converge to the solution)
- The submission is overall well written, and the authors are careful to introduce the setup and all the assumptions used in their analysis


Weaknesses:
- Overall the analysis presented in this submission is very limited, as it only deals with a specific type of loss function, which is not a practical instance where model parallelization would be useful
- While the authors argue that the quadratic model has been previously used in the literature for studying neural networks (lines 140-144), in my understanding this model still relies on a Taylor approximation for the loss function of a neural network (for non-linear models). It is therefore not clear how the error introduced through this approximation would translate into the convergence analysis presented in the submission
- The authors use additional simplifying assumptions, such as the fact that each node can compute the true gradient of its submodel, which would be infeasible in the case of large datasets. Additionally, the results are only presented for Perm-1 sketches when the number of nodes matches the dimension of the model, which is again not a practical use-case. While the authors argue that their results can generalize beyond these limitations, a more general formulation is not provided in the submission.
- Other works have analyzed the convergence guarantees for IST, notably [28] has done so for a one hidden layer network with ReLU activations, which is a more general case than the one from this submission. It is not clear what are the additional insights presented here, compared to the previous work.


Limitations:
I believe the authors have properly addressed the limitations of their analysis. However, I am not convinced that the contributions presented in this submission are broad enough, which ultimately motivates my score.


---------------------------------
**Edited after rebuttal**

After reading the authors' answers, and the other reviews, I decided to increase my overall rating to 5, as well as the scores for Soundness, Presentation and Contribution.

Rating:
5

Confidence:
3

";0
JIYdbHDonF;"REVIEW 
Summary:
$k$-means clustering is a classical and fundamental method in machine learning. This paper proposes a very simple method that projects the $n$ data points onto a one-dimensional space by generating a random Gaussian vector. Moreover, the one-dimensional projections can be used for coreset construction and the seeding in $k$-means++. The experimental results have proven the efficiency of the proposed algorithm. 

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
(1) The proposed method is very simple and easy to understand.  
(2) The idea of projecting the $n$ input points onto one-dimensional space is kind of novel.  
(3) This paper provides strict proofs for the running time and approximation ratio.  
(4) Experimental results show the efficiency for coreset construction and $k$-means++ implementation. 

Weaknesses:
(1) The essence of the proposed method is dimension reduction. To be specific, the pairwise distances of the $n$ data points are roughly preserved by projecting onto one-dimensional space. The projected distances are used to $k$-means++ seeding and an approximate clustering. By the sorted $n$ scalars, the seeding can be implemented in time $O(n \log{n})$ independent of $k$ but at the cost of $\widetilde{O}(k^4)$ approximation ratio. For me, I think the technique is straightforward and the idea's novelty is a little limited.  
(2) The colors of the curves in Figure 2 and Figure 3 are not easy to distinguish. 

Limitations:
See the Weaknesses part. 

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper proposes a new clustering algorithm that seeks to have the accuracy of k-means++ while avoiding the time complexity of O(ndk). In particular, the paper focuses on settings where k is large and seeks a O(nd+nlogn) time complexity. This is a practically relevant problem.

The main algorithm proposed in the paper is a simple algorithm based on projecting all data points onto a random one-dimensional subspace and performing the k-means++ seeding procedure on the dimensionality-reduced data. Interestingly, it is shown that this can be done in time O(nd+nlog(n)) and it has a poly(k) approximation guarantee with respect to the optimal k-means clustering. While the poly(k) guarantee is worse than the O(log k) guarantee of k-means++, a log(k) approximation guarantee can be obtained if we use the proposed algorithm to build a coreset of size poly(kd)*log(n), and then use k-means++ on the resulting coreset.

While the techniques in [26] (based on approximate nearest neighbor search) achieve similar goals to those of the present paper (a fast alternative to k-means++ with a complexity that doesn’t depend on k), the algorithm in the present paper is attractive for its surprising simplicity.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
The proposed algorithm is natural and intuitive, but the theoretical analysis is quite involved.

The fact that a one-dimensional projection can be shown to provide an approximation guarantee (although a poly(k) one) is interesting and nontrivial, since a Johnson-Lindenstrauss type approach typically requires a projection onto log(n) dimensions. 

After the one-dimensional projection, a standard implementation of k-means++ seeding does not avoid the O(nk) dependence. As such, the paper proposes a careful data structure to perform k-means seeding on a one-dimensional dataset. The proof that this approach runs in time O(n logn) is technically sophisticated and could be of independent interest. The potential function must be carefully defined, and its use to show that each point is only updated O(log n) times is elegant.

The paper is well written and clear, and the results should be of interest to people working on the theory of clustering algorithms and to people looking for practical approaches for performing k-means clustering on large datasets with large k.


Weaknesses:
The empirical results in the paper are generally positive, but perhaps a bit underwhelming, as the proposed approach doesn’t seem to have a strong advantage over the Lightweight coreset approach from [10]. In particular, the proposed algorithm is slower than the Lightweight approach, and the accuracy of the two approaches is similar (except on the Gaussian dataset case, which was designed to adversarially exploit a weakness of Lightweight).

I found it a little confusing to distinguish between the different variants of the algorithm, and it may be helpful to give them clear names (rather than saying “Ours”). In particular, in Section 1, there is a brief discussion about how the approximation can be reduced from poly(k) to O(log k), by incurring an additional poly(nk)*log(n) time. Are the results in 3 and Table 1 for the variant with the O(log k) approximation? Also, in section 1, it says that this improvement is further discussed in the supplementary material, but it wasn’t very clear where. Is this just referring to the general discussion on coresets in Appendix A? It may be useful to have an explicit theorem about the algorithm that combines the proposed algorithm with k-means++ on a coreset (and that algorithm should have a name). I apologize if I missed this somewhere.

It may be useful to discuss some of the literature on fast clustering algorithms beyond k-means. For instance, there are “fast” implementations of PAM for k-medoids (such as FastPAM, Clarans and BanditPAM) and of dbscan (such as dbscan++). It may be the case that all those incur the O(ndk) time that this paper tries to avoid, but a mention of that would be useful to the community (specially as it relates to the claim that there are no other clustering algorithms with O(n*log(n)) complexity).

Some other minor comments/typos:

- line 144: k-measn -> k-means
- line 278: “compared a” -> “compared to a”
- line 279: “entre dataset” -> “entire dataset”
- line 344: “running independent” -> “running time independent”

Limitations:
The limitations of the proposed approach are discussed in a fair manner.


Rating:
7

Confidence:
4

REVIEW 
Summary:
The submitted paper focuses on the problem of unsupervised clustering and aims at improving the $\Omega(ndk)$ runtime of the ```kmeans++``` algorithm in the clustering of $n$ $d$-dimensional datapoints in $k$ clusters. The proposed algorithm relies on the intuition of running ```kmeans++``` on a one-dimensional projection of the dataset along a random direction $\boldsymbol v$: the authors prove that such strategy leads to a clustering in $O(nd+n\ln n)$ steps with an approximation ratio scaling as $\tilde O(k^4)$ (to be compared with the $O(\ln k)$ of ```kmeans++```). Numerical experiments compare the performance of the algorithm both with ```kmeans++``` and with other strategies for the production of coresets.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The proposed algorithm is very simple to implement (reminiscent of sliced optimal transportation strategies, in which a low-dimensional projection is adopted to solve a high-dimensional problem with a subsequent polynomial gain in speed). The approach proposed by the authors is indeed compatible with a remarkable speed-up and, given its price in quality, suitable for adoption in a pipeline. One of the strengths of the paper is that the authors can provide theoretical guarantees on the performance obtained by this simple trick.

Weaknesses:
The proposed algorithm pays the price of better performances producing clusterings of lower quality, as the authors themselves point out. This drawback can be mitigated by adopting the algorithm for the construction of coresets within a pipeline (as discussed in the paper). In the case in which the algorithm is adopted as a stand-alone solution, the (possibly deteriorating) effect of structure in the dataset is only partially investigated and addressed.

Limitations:
Limitations of the contribution have been addressed, see also the *Weakness* section.

Rating:
6

Confidence:
2

REVIEW 
Summary:
The paper presents a new technique for clustering data such that the running time of the method is independent of the number of clusters, and the approximation error is polynomial only in the number of clusters and not the number of data points. 

The paper experimentally tests the method and finds that it is a good balance of speed and quality of the result. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
There are many strengths.


1) The runtime being independent of $k$ is a significant speed up. Further, the constants seem to be better as well. Since the method is not iterative and only makes a single pass over the data 
2) The experimental results on corsets are compelling. 
3) The presentation of the method is easy to follow. 

Weaknesses:
The approximation of ratio of $O(k^4)$ is significantly worse than $O(\log k)$. I was hoping this was only theoretical and that we would see better results experimentally, but the experimental results are not clear. 

Specifically for Figure 3, I have no idea what the difference between the bottom and the top row is. This is partly due to the fact that the paper never actually writes down formally the problem or the metrics used. From my understanding, we have $n$ data points $x_1, \ldots, x_n$ in high dimensional space. We want to find $k$ clusters $C_1, \ldots, C_k$ where each $C_i$ is a subset of the $n$ data points such that if $c_i$ is the center of the mass of those points, then 

$$ \sum_{i=1}^k \sum_{j \in C_i} \|c_i - x_j\|^2 $$

is minimized. This is what I interpret to be the cost. Hence it is very unclear to me what Figure 3 is showing. 



Limitations:
N/A

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper introduces a randomized clustering algorithm that provably runs in expected time O(nnz(X) + n log n) for arbitrary k. In theoretical analysis, the proposed algorithm achieves approximation ratio O(k^4 ) on any input dataset for the k-means objective. In the experiments, the quality of the clusters found by the proposed algorithm is usually much better than this worst-case bound. They use the algorithm for k-means clustering and coreset construction. This work shows that the approximation ratio achieved after a random one-dimensional projection can be lifted to the original points and that k-means++ seeding can be implemented in expected time O(n log n) in one dimension. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
This paper has the following contributions: 
It designs a simple, practical algorithm for k-means that runs in time roughly O(nd), independent of k, and produces high-quality clusters. In theoretical analysis, they give a randomized clustering algorithm with provable guarantees on its running time and approximation ratio without making any assumptions about the data. In experiments, they run two types of experiments, highlighting various aspects of the algorithm: Coreset Construction Comparison and Direct k-means++ comparison.

Weaknesses:
1.Insufficient research on related work. 
There are many methods for accelerating clustering. This paper did not conduct comprehensive research. In the most relevant random projection clustering, this work is not the first random projection clustering work, such as paper [1]. It is not cited in this paper. The superiority of this work cannot be verified. Please compare it from the theoretical analysis, experimental results, algorithm complexity, etc.

[1]Liu W, Shen X, Tsang I. Sparse Embedded $ k $-Means Clustering[J]. Advances in neural information processing systems, 2017.

2.The organization and presentation of this paper can be further improved. The second half of the abstract is logically chaotic.

3.The experiment is insufficient to verify the superiority of this work. The experimental results show that the performance of the proposed algorithm is not excellent, such as in Figure 2,3.

Limitations:
N/A

Rating:
4

Confidence:
4

";1
ARJG1kr8A7;"REVIEW 
Summary:
Authors propose a way to prompt GPT-3 to exhibit behavior simulating execution of iterative programs. Authors propose the following prompt constructs: providing structured examples of program execution; using fragments of execution; not using self-attention on some parts of the generated text. Authors compare the results to baselines and show significant improvements

Soundness:
3

Presentation:
2

Contribution:
1

Strengths:
- Authors introduce a method that enables GPT-3 to mimic the execution of iterative programs. They achieve this by supplying the model with intermediate steps and outcomes.
This is somewhat novel and could be useful for using LLMs to solve problems that require iterative processing. 
- The use of path fragments may prove beneficial in situations where the context size is insufficient for comprehensive examples.
- The strategy of confining self-attention to particular segments of the output might be advantageous when the context size needs to be considered
- The examples provided in the paper are well written

Weaknesses:
- Authors' approach requires the manual construction of prompts for each problem at hand. It is not automated and not scalable. This limits the usefulness of the approach in practice.
- Authors compare their approach to simple baselines. There should be a comparison to at least chain-of-thought reasoning. 
- Paper is hard to read and accept as a standalone without appendices. Authors refer to the content in the appendices too much.
- The significance of the work is low. It is known that LLMs can produce iterative output. Although the authors have enhanced the quality of such outputs via structured prompting, structured prompting is not entirely novel. Same can be said about using fragments in prompt/context.

EDIT: I have raised my evaluation of the paper from 3 to 4. Authors have promised to address the issues I and other reviewers have raised. However, in my opinion, such changes would require a major rewrite of the paper. I am not confident if these changes can be done well for the publication.

I have read the author’s rebuttal and further discussion with authors based on my questions and feedback. 
Authors' rebuttal addressed some of my concerns by more in depth discussion of IRSA relationship to chain-of-thought reasoning and possible automated generation of IRSA prompts.

Limitations:
It would be good if authors explored the limitations of what can be achieved by their approach. At some point the LLM ""execution"" (simulation really) of the program should fail. The points at which the simulation would fail likely depend on the input/output/context size. It may also depend on the algorithm semantic and/or algorithmic complexity. (Those are different complexities and may affect the breaking point differently). These questions could be explored and would be useful to know.

Rating:
4

Confidence:
4

REVIEW 
Summary:
Making LLMs follow procedural rules precisely, as done when executing a program, is been a challenging task. In this paper, the authors introduce iterations by regimenting self-attention (IRSA), a prompting technique to make large-language models (LLMs) *execute* a hand coded programs (on novel inputs) precisely. The authors propose three techniques for IRSA: 1) by prompting the LLM with a step-by-step example showing many state-transitions in details, 2) By prompting with fragments of the state-to-state transitions only (along with the latest state for which it predicts the transition), and finally 3)skipping attention on intermediate state-to-state transitions. LLMs are shown to be significantly more successful at tasks such as sorting arrays, finding longest sub-sequence in a string, or simpler logical puzzles etc. when prompted with IRSA.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
### Originality

Recently, many new proposals for prompting LLMs, including scratchpad, and Chain-of-Though prompting have been proposed. However, most approaches focus on making LLMs reason and solve problems/puzzles. Instead, in this paper, the focus is on making LLMs *follow instructions accurately* (which then can be used for solving certain puzzle). This is a novel and original direction.

### Quality
The paper is well presented, including the figures, and the tables. Additionally, the experiments have been conducted on many tasks to sho

### Clarity
The paper is very clearly written and well presented. Specifically, I found the prompt examples very useful in understanding the paper's ideas.

### significance
Getting LLMs to precisely execute procedural rules is of intereset to the research community at large. LLMs inability to successfully tackle procedural problems (such as multiplication) of complexity beyond the training set complexity has raised questions regarding its ability tackle compositional problems. This paper provide an important perspective and countering result that will further enrich this discussion.


Weaknesses:
The two main drawbacks of the paper are motivation and experiments. 

### Motivation
The paper does not sufficiently motivate the problem statement. Why should we care about making LLMs execute programs - perform iterative behavior? When the process is deterministic and easy to programmatically describe, why would we prefer LLMs over a deterministic typical program (one can even use programs which explicitly *show* transition rules applied as well)? The authors mention education or software engineering vaguely, but there is no concrete motivation in these use-cases (when would a LLM be more suitable for this task over a REPL-like loop with python/cpp?).


### Experiments

1) The authors do not evaluate on significantly larger sequences sizes. Since fragmented prompting seems to allow arbitrarily large sequence of state-transitions, I believe authors can indeed use IRSA on larger sequence problems. The trend between success rate and sequence length would be insightful.
2) I strongly appreciate the authors for showing the negative result in Figure A.1 (Appendix section A.3.2). This shows that despite using IRSA, the model may end up performing wrong state-transitions based on its correlation to patterns in recent history. If multiple previous state transitions contain sequences where the statement ""2 < x = True"" appears, then when asked ""2 < 1 ="" the LLM has higher likelihood of filling True than False. This seems to directly negate the claim of this paper that we can make LLMs execute programs precisely. It clearly seems to be affected by the prompt history which can make them act in unreliable ways.

Limitations:
Yes the authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper proposed several novel prompting methods that could trigger GPT-3 to perform iterative behavior for executing algorithms with loops. The main technique the paper presented, IRSA, is to use highly structured prompts that contains information about unrolled execution trace, program states, and a detailed explanation of the motivation of a specific action, thus bringing strict attention controls to LLMs and help them reason about the procedures to get the solution. 

Based on the proposed IRSA method, the authors also introduced two alternative prompting methods: Fragmented prompting and Skip attention. The fragmented prompting technique strips out some of the iterations in the full execution trace, thus enabling the prompt to contain more diverse scenarios under a limited prompt length. The skip attention technique explicitly marks program state information with a special token and puts emphasis on the original prompt that serves as a demonstration to the LLM and the last execution state where the LLM could continue its execution from, thus also bringing LLM server side and client side optimization opportunities. Meanwhile, the authors also briefly discussed the automatic generation of the proposed prompts using  LLMs. 

The proposed prompting methods were evaluated on various tasks whose solutions involved loops. It is shown by the evaluations that the proposed methods could achieve state-of-the-art results on multiple tasks.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:

1. The paper showed great originality and insights in that the authors identified the failing reason of the LLM on tasks involving iterations, considered related Turing machine concepts, and designed several novel prompting methods incorporating highly structured information about unrolled execution trace, program states, and a detailed explanation of the motivation of a specific action, bringing strict attention controls to LLMs and help them reasoning about the procedures to get the solution. 

2. The proposed prompting methods showed state-of-the-art results on multiple loop-involving tasks.

3. The proposed Fragmented prompting method can be a promising technique that could encode diverse scenarios while keeping the prompt relatively short. This can be helpful for working with LLM APIs.

4. The proposed Skip attention prompting method can also be promising in that it emphasizes the concept of program state in the context of using LLMs as general Turing machines. This technique could also help reduce prompt length, and with adequate supporting modifications and implementations on LLMs, this can be a solid base for future works.

Weaknesses:
The overall presentation of the work can be relatively hard to comprehend for the readers. Especially for the presentation of the proposed Skip attention, it could be better if the authors provided a figure that briefly demonstrates the idea of the server and client-side implementations of the method.

Limitations:
None

Rating:
7

Confidence:
4

REVIEW 
Summary:
The paper explores the use of regimented self-attention (IRSA) to prompt GPT-3 to perform iterative behaviors necessary for executing programs involving loops. The authors investigate three approaches to trigger the execution and description of iterations. The results suggest that IRSA leads to larger accuracy gains than using the more powerful GPT-4 for dynamic program execution. The authors highlight the potential applications of IRSA in education and discuss the implications for evaluating large language models (LLMs). While LLMs have limitations in complex reasoning tasks, prompt design plays a crucial role in their performance.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
+ Interesting problem and approach
+ Providing examples of prompts

Weaknesses:
- Presentation
- Concern about reliability
- Concern about ""Turning machine"" claims


Limitations:
* The authors make huge claims, but do not discuss the limitations of their work.

Rating:
4

Confidence:
3

REVIEW 
Summary:
This work introduces Iterations by Regimenting Self-Attention (IRSA) which is a set of LLM prompting techniques for producing repetitive, algorithm-like behavior that can be useful for a range of tasks, such as carrying out a sorting algorithm or solving a logic puzzle. There are 3 techniques discussed: 1) ""Basic IRSA"" which is a chain of thought prompt that looks a bit like an execution trace of some natural-language-like pseudocode - theres a lot of repetitive structure, the current state is verbosely repeated after each step, and changes to the state are explicitly describe before the happen. 2) ""Fragments"" which is the idea that instead of prompting with a full trace of an algorithm you can just prompt with random unordered individual steps of the algorithm to prepare the model for executing a random step. 3) ""Skip Attention"" where only the most recently produced state is attended to (plus the original, fragment-based prompt), since changes to the state should be independent of the history of states – this cuts down on computation and helps the LLM not get confused by patterns in its recent output.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- Skip attention and fragments (which pair well together) are great ideas, and are original as far as I know – other reviewers can correct me if I'm wrong. In many algorithms (and in fact, in the execution of interpreted code in general) only the current state matters as opposed to the history of how the state has changed. Only showing the most recent state to the LLM makes a lot of sense. It saves on computation cost and makes long running algorithms feasible, since there's no need to attend over the whole history of generations (which would become a huge problem as an algorithm runs for dozens or hundreds of steps). As the authors point out, this Markovian setup of not looking at the history of states also means that the LLM won't get confused by patterns in its recent history.
  - The ""fragments"" approach is a clever way to get the LLM used to this idea of seeing somewhat random states and needing to do a single algorithmic step for each one.
  - Skip attention makes so much sense, I'm surprised past work like ""Show Your Work"" (Nye et al 2021) didn't take an approach like this, since I imagine it would work fine with executing interpreted Python programs (where the state is the set of local variables/values along with the current line number in the program, and the LLM just has to output a next set of local variables and next line number).
- More generally, getting LLMs to do things that look more like rigid computation can be difficult and I think that this is a paper with a pretty good evaluation of a particular approach to this problem, and would be useful for the NeurIPS community to see.
- The evaluation is reasonable and shows unsurprisingly that the skip attention method can work great and generalize to very long sequences (eg bubble sort with 25 steps).

Weaknesses:
- The descriptions of what IRSA is were quite difficult for me to understand. The first line of section 2, the section describing IRSA, is ""Prompt 1, as well as the prompts 2, A.4, A.5, and A.6 in the Appendix, illustrate the basic IRSA."" (line 66). Written as is this feels a bit overwhelming as it suggests that I need to look at 5 different prompts (including 3 in the appendix) and try to look for the common features among them to figure out the method. Also, in reality many of these references (2, A.4, A.5, A.6) are actually going to show up later on in places where they're discussed so it's okay if I don't look in detail at them now, but since I haven't been told that I feel some need to dig them all up before continuing.
    - A flow for section 2 that would be much more understandable to me (and I believe others) would be the following. This is just one suggested way of doing it and I think there are many valid ways that would be widely understandable (you dont need to do the below), but the current flow is difficult to understand:
        - Give a brief but concise description of the key feature of IRSA (similar to lines 71-73 right now) so we're primed with looking for that *before* we're told about any prompts to look at. I might even suggest that instead of putting the CoT comparison at the end (lines 78-82), it might flow nicer to actually frame it *in terms of CoT / as an extension building on CoT* since that is a closely related framework many readers know about. In general after reading the paper I actually still find I have trouble precisely articulating what makes something count as ""basic IRSA"", so presenting it from the start in terms of its relation to CoT might be helpful.
        - Tell us to look at Prompt 1, and briefly walk us through what we're looking at / why this is IRSA. 
        - Mention that the precise keywords/format of Prompt 1 (""EXECUTION"", ""Prep"", ""EndPrep"" ""Iteration"", the indentations, ""State:"") are not important (IRSA is not a set of specific keywords to use) and point to Prompt 2 as an example of something that looks different on the surface level but is still IRSA.
        - At this point, you might parenthetically refer to the 3 appendix prompts as additional examples used in the evaluation that the reader can look to if they want more.
    - Again, to be totally clear, I'm not prescribing this format, I just find the current flow difficult to understand so I took a stab at restructuring it, but there are many other ways of doing so that would also flow well.
- I'd like to see some discussion of how this relates to the paper ""Show Your Work: Scratchpads for Intermediate Computation with Language Models"" (Nye et al 2021) which is currently just referenced by the paper in the list of CoT related works without specific discussion. In that work, the authors showed that while LLMs are bad at directly predicting the output of a Python function called on certain inputs, they could instead have the LLM repeatedly output the current state (what the variables are set to) plus the next line of the program to run. That was essentially a form of CoT with extra structure. IRSA seems somewhere in between the strict state/instruction format of Show Your Work and the free flowing reasoning of more general CoT. I think an explicit comparison to that paper (and/or any other paper that does some form of rigid CoT) is important, so it's clear how this work should be viewed in relation to others that have structured CoT.

- I'm generally coming out of this paper still somewhat unsure what precisely ""basic IRSA"" is (i.e., without fragmenting or skip attention), and I felt I could only gesture towards some of its important features when writing the Summary section above.
    - It feels related to CoT and I'd like to understand it in terms of that. One section comparing CoT to IRSA says ""a significant distinction lies in the number of reasoning steps, which is limited and fixed in *usual* CoT applications"" (emphasis is mine) (lines 78-82) but this is not always true (e.g. in the Show Your Work paper above – so does that make Show Your Work and instance of IRSA and this present paper is proposing a general framework encompassing that?).

- (minor weakness) Section 2.4 sounds interesting but is largely confined to the appendix. It doesn't really flow with the rest of the story and as far as I can tell isn't used in the evaluation alter. But I'm a bit torn because it is actually quite cool and maybe it doesn't hurt to have as just an aside (though maybe with a slightly more clear verbose explanation). I don't terribly hold this one against the paper, it just feels a little out of place. 

**Overall** I think that, though it is difficult to follow the flow of this paper in places so it took me quite a while to understand, and I'm still not totally clear on what makes something ""basic IRSA"" or how it relates to prior work like Show Your Work, I think that in particular given the contributions of skip attention and fragments this would still be valuable work for the NeurIPS community to see. The positives outweigh the negatives in my view, but with revisions around the points mentioned above I would be more supportive.


Limitations:
The authors address limitations adequately 

Rating:
6

Confidence:
3

";0
SJw4Da8BuR;"REVIEW 
Summary:
The paper proposes a subspace projection algorithm spanned by the features' principal components. 
The projection's fusion with the different network layers is presented.
A gradient-free pruning approach is further suggested based on the parameters and activation statistics.
Finally, the proposed framework is experimented on BERT and T5 and achieves a compression ratio of 44% with at most 10 1.6% degradation.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
The paper is fairly written.
The apparent main novelties of the paper are the low-rank approximation of the features and a statically based pruning approach. 

Weaknesses:
Low-Rank approximation:\
The main weaknesses seem to arise from the comparison to prior/competing works.
For example, low-rank approximations of features have already been presented see for example 
https://cs.nju.edu.cn/wujx/paper/AAAI2023_AFM.pdf \
Also, I am unsure why lines 37-38 are true: performing PCA (/Kosambi–Karhunen–Loève) is a pretty old technique for model acceleration, the subspace being defined by the principal components of the parameters or activations, this is a low-rank approximation.\
Thus, the low-rank approximation contribution of the paper should be narrowed to the definition of the data matrix.

Pruning: \
it's unclear how these simple statistics perform compared to other pruning methods or heuristics.

Experiments: \
the subspace dimension as well as the compression ratio are not given which leaves the speed-up metric subjective.
The method performance on mid-size LLM is not very good compared to old methods. \
Random projection as ablation is a very weak baseline.

Clarity:\
The paper can be refined in terms of clarity (also typos (e.g., lines 235, 282))

Limitations:
No limitations discussed

Rating:
3

Confidence:
4

REVIEW 
Summary:
This paper presents TCSP, a model compression approach for transformers by reducing hidden size via low-rank factorization. In addition, TCSP is compatible with other compression methods such as model pruning and head size compression. Experiment results demonstrate the effectiveness of proposed method, achieving a high compression ratio while incurring rare performance drop.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
$\cdot$ This paper is well-structured and clear to understand.

$\cdot$ The algorithm is general enough, and is compatible with other compression strategies.

$\cdot$ Experiment results verify the effectiveness of the proposed method.

Weaknesses:
$\cdot$  The novelty of this paper is limited, the core idea resembles low-rank factorization with SVD, and the approach is more like a combination of SVD and model pruning.

$\cdot$ The author claims it is the first work to reduce the hidden size, but I doubt if the method can be successfully implemented in the industry since the lack of experimental results related to inference speed of compressed model.


Limitations:
The author has addressed the limitations and social impacts in Appendix.

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper proposes a decomposition-based method, called Transformer Compression via Subspace Projection (TCSP), for compressing transformers. By decomposing the feature matrix extracted by some sample data, the model is projected onto a subspace to reduce the size of hidden dimensions. Experimental results on the datasets GLUE and SQuAD show that TCSP enables 44\% parameters reduction with at most 1.6\% accuracy loss and surpassing existing methods.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
This paper compresses the hidden dimension of the transformers, which is less explored.

The overall presentation of the paper is easy to understand.

Weaknesses:
TCSP is indeed just principal component analysis (PCA) or compressed sensing (CS), all working with the dominant subspace derived from SVD. Why another name?

I have concerns about the following aspects:

1. Motivation: From lines 51-59, this paper discusses the compression methods for transformers. Also, it mentions ""We do not delve into knowledge distillation and weight sharing, as they involve training models from scratch"". However, knowledge distillation (KD) includes both task-agnostic and task-specific schemes. For task-agnostic KD methods, they do not involve training models from scratch, see e.g., Wu T, Hou C, Zhao Z, et al. Weight-Inherited Distillation for Task-Agnostic BERT Compression. Meanwhile, it is a normal setting in KD to reduce the hidden size of the transformer model. Therefore, task-agnostic KD methods should be compared, too.

2. Computation: As TCSP requires SVD decomposition for a large matrix, more discussion about the computing cost and scalability is needed, especially in Table 2.

3. Robustness: Regarding the quality of subspace, how is the performance if we add noise or adversaries to the input data when generating the projection matrix? How do you ensure the sample data are representative? The timing overhead and complexity of the SVD to ensure a good projection subspace should be explicitly characterized and quantified.

Indeed, there are recent decomposition-based compression algorithms applied to transformers which the authors may benchmark against, e.g., Ren, Y., Wang, B., Shang, L., Jiang, X., \& Liu, Q. (2022). Exploring extreme parameter compression for pre-trained language models. arXiv preprint arXiv:2205.10036. 



Limitations:
The models used are relatively small in size, e.g. T5-base, BERT-base. There are no experiments on the Large Language Models.

Rating:
3

Confidence:
3

REVIEW 
Summary:
This paper proposes an approach to compress the hidden size of a transformer model using subspace projection. On a high level, the paper aims at projecting the transformer model into a lower dimensional subspace using a projection matrix that is computed using a sample of the training data. This method is compared against other compression techniques using the T5 and BERT models on GLUE and SQuAD datasets and it is shown that the proposed method performs on par or better then the methods under comparison. The highlight of the experimental result is that the proposed transformer compression via subspace projection technique is able to compress models by as mush as 44% with only 1.6% degradation in performance. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper addresses an important problem of transformer compression. In the age of ever-increasing model sizes, it is vital to develop methods that compress large models with minimal loss in performance, if any. This paper presents a simple yet effective approach to leverage linear subspace projection for compression. The paper is easy to follow, offers sufficient literature review, and presents convincing experimental results. The experimental results are particularly strong -- 44% compression with only 1.6% loss in performance. 

Weaknesses:
Some notation is used before definition. Could include more recent literature in Related Work section -- see below.

Limitations:
NA

Rating:
7

Confidence:
4

";0
m2WR1yJ8N9;"REVIEW 
Summary:
This work found that more data does not necessarily improve the pre-training of GNNs and then propose a method to adaptively select data for pre-training GNNs. Experimental results show the proposed method is effective.

Soundness:
2

Presentation:
2

Contribution:
3

Strengths:
1. this work is well motivated by the observation that more data does not necessarily lead to better pretrained graph encoder

2. that the experiments seems to be convincing since APT generally outperforms other baselines

Weaknesses:
There are several weaknesses mainly about the method:

1. The optimization problem in equation 5 is confusing, what is the variable you are optimizing, a series of binary variable indicating whether to select a graph or not? Based on description in subsequent part, you are basically selecting graphs with the highest score, in that case it is unnecessary to formulate an optimization problem, just state your scoring function.

2. The description is confusing, eg, what is M in line 296? The number of subgraphs being selected?

3. Efficiency is important in data selection, but the authors do not discuss it in the main body of the paper. I assume the method is slower than baseline pretraining method but how slow is it?

4. The motivation of the other four property criteria is unclear, especially when they are highly correlated with the network entropy. Can we just drop the other four criteria?

5. The authors claim in the conclusion that APT can enhance the model with a fewer number of input data, but I can't find evidence for that. Is APT only trained on fewer data than baselines?

Overall, the description of the proposed method could be improved. In addition, the authors claimed they are not curriculum learning but I don't think so. Time-adaptive selection strategy is just curriculum learning in a broader sense, I think easy-first is just one type of curriculum.

Limitations:
The authors do not state limitations in either the appendix or the main body of the paper, please add them.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper introduced an novel graph pre-training pipeline from a data-centric view by proposing a graph data selector. The proposed approach involved sequentially feeding representative data into the model for pre-training. These representative data were determined by a data selector, which utilized prediction uncertainty and graph statistics to suggest the most relevant examples. This novel methodology offers a fresh perspective on graph pre-training and demonstrates the importance of data selection in the process.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The idea is novel and interesting.
2. The proposed training pipeline is tested on many tasks from different domains.

Weaknesses:
1. The utilization of predictive uncertainty based on pre-training loss lacks convincing evidence.

2. The proposed training pipeline requires input graphs to be processed order-by-order, which may introduce unnecessary bias and is not scalable for large graph datasets.

Limitations:
See above.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper presents a data-centric framework, abbreviated as APT, for cross-domain graph pre-training. APT is composed of a graph selector and a graph pre-training model. The core idea is to select the most representative and informative data points based on the inherent properties of graphs, as well as predictive uncertainty. For the pre-training stage, when fed with the carefully selected data points, a proximal term is added to prevent catastrophic forgetting and remember all the contributions of previous input data.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.This paper proposes that big data is not a necessity for pre-training GNNs. Instead of training on a massive amount of data, it is more reasonable to select a few suitable samples for pre-training. This approach can also reduce the amount of data and computational costs. Compared to pre-training on the entire dataset, selecting a more carefully selected subset of data for pre-training can indeed achieve better results.
2.This paper provides theoretical justification for the connection between uncertainties by establishing a provable connection between the proposed predictive uncertainty and the conventional definition of uncertainty. The predictive uncertainty is defined in the representation space, which enables the identification of more challenging samples that can benefit the model training more significantly.
3.The entire framework seems very reasonable, and the process is described clearly. From the experiments, it appears that good results have been achieved.


Weaknesses:
1.This paper mentions both ""data-active"" and ""data-centric"" concepts. It may be helpful to clarify the relationship between these two to avoid confusion. Maybe only using ""data-active"" in the paper.
2.There is no individual ablation experiment about the graph selector to demonstrate the effectiveness for each of the ﬁve graph properties.
3.There is a spelling error in the title, 'Prespective' should be corrected to 'Perspective'.


Limitations:
Yes

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper proposes a novel approach to pre-training graph neural networks (GNNs) called the data-active graph pre-training (APT) framework. This framework introduces a unique method that involves a graph selector and a pre-training model. These two components work together in a progressive and iterative way to choose the most representative and instructive data points for pre-training.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The key insight of the paper is that using fewer, but carefully selected data can lead to better downstream performance compared to using a massive amount of input data. This approach challenges the common practice in machine learning of using large datasets for pre-training, a phenomenon the authors refer to as the ""curse of big data"" in graph pre-training.

2. The paper is well-motivated, well-written and easy to understand.

3. The experimental results demonstrate that the proposed method outperformed GCC, which is the previous SOTA and a direct ablation to the proposed APT framework.

Weaknesses:
1. The proposed method is incremental. The proposed “predictive uncertainty” is adapted from curriculum learning, and “graph properties” are common statistics. 

2. The proposed methods are not able to handle node features. It only provides structural information which does not align with most real-world scenarios. As a result, performance is not comparable with a supervised model trained on node features.

3. The node classification performance on homophily graphs (which is an important family of graphs) are much lower than ProNE. The improvement in graph classification tasks seems limited.

4. Selection based on graph properties might cause potential test data leakage because the graph properties are selected according to test performance (as shown in Figure 2).

Limitations:
Yes

Rating:
6

Confidence:
4

";1
o6yTKfdnbA;"REVIEW 
Summary:
Recursion in Recursion combines the efficiency characteristics of Binary Balanced Tree Recursive Neural Networks (BB-Tree RvNNs) and quality characteristics of Beam Tree RvNNs by applying the recursion in a hierarchical fashion.

Doing so is not straightforward and requires several ""modifiers"" such as Beam Alignment and being careful about chunk preprocessing in the outer recursion.

Table 1 demonstrates that the method is efficient in both time and memory. Table 2 demonstrates that the model quality is compromised but not by much.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
The paper provides necessary background which I appreciated since I am not too familiar with RvNNs. Experiments seem well done and reproducible. The method is efficient as promised and still maintains the ability to generalize to sequence length.

While the method is still slow and seem less mature than other methods like transformers, I think RvNNs are interesting and worthy of discussion because of their explainability and ability to generalize to longer sequences.

Weaknesses:
While interesting, the method seems rather complex to apply generally with 2 levels of recursion and necessary modifiers to integrate them effectively.

The beam alignment section I found particular confusing. Maybe there could be a figure to help understanding?

The inability to generalize on argument length could use more discussion.

Limitations:
The authors have a limitations section that discuss slowness and lack and difficulty to generalize.

To me the the method seems complex. I am not sure if it is broad applicable given that it is still slow and model quality is still lacking.

Rating:
4

Confidence:
3

REVIEW 
Summary:
The paper studies a “recursion in recursion” (RIR) approach, the outer loop being a balanced k-tree recursive NN, and the inner loop a general recursive NN that is based on a beam tree. The goal is to obtain O(k*log_k(N)) time complexity. O(2*log_2(N)) is a special case of k=2 (for binary trees) and O(N) is the special case of k=N for RNNs. The authors study (mostly) the tradeoff between performance on ListOps and Long Range Arena (LRA). Namely, RIR seems competitive to strong baselines on LRA (e.g. state space models), yet it generalizes better on longer lengths in ListOps.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
S1. The paper is very well written. The research process is embedded in the structure of the paper: “motivation -> problem -> solution -> new problem -> new solution -> etc.”

S2. The  modifications are quite reasonable and the experiments are thorough (the appendix was helpful too).

S3. The papers’ evidence matches the claims quite well and the limitations are well-acknowledged.

Weaknesses:
W1. It is very hard to read through the tables. Since you are studying a trade-off of time/ memory efficiency and performance on ListOps, and likewise ListOps vs. LRA performances, I would expect scatter plots that trace out a Pareto frontier and where your RIR contributions lie on this frontier. 

W2. Despite the work being very engineeringly sound and well-presented, one could argue that the contributions as “combinatorial” and mostly heuristic vs. fundamental. That is OK, but one could consider it a weakness within the context of NeurIPS. 

W3. Following up on 2., it may be good to discuss the significance of your contribution to larger tasks and broader / realistic data distributions. For example, how would RIR might be a part of a state-of-the-art LLM architecture in the future?

W4. Perhaps the most exciting discussion for me is about different inference strategies (whether to use RIR or not). I acknowledge that you have addressed some of that in the SM, but I would have expected to see a bit more discussion in the main text.

Limitations:
the authors adequately addressed the limitations

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors propose a new tree-style RNN that combines the computational efficiency of tree RNNs with the computational power of more complex tree RNNs. The method makes each node perform a recursive computation, so there are logarithmically many nodes but they are more expressive than the standard tree network. The resulting model can operate on long sequences comparably to state space models and much better than vanilla transformers.

**Ethics Review**: It appears to me that the authors broke the integrity of the double-blind reviewing procedure. In lines 144-146, the authors write: ""we use an efficient variant of BT-RvNN (EBT-RvNN). We propose and explore EBT-RvNN in a concurrent work"". NeurIPS policy states: ""If you need to cite one of your own papers, you should do so with adequate anonymization to preserve double-blind reviewing.  For instance, write “In the previous work of Smith et al. [1]…” rather than “In our previous work [1]...”). If you need to cite one of your own papers that is in submission to NeurIPS and not available as a non-anonymous preprint, then include a copy of the cited anonymized submission in the supplementary material and write “Anonymous et al. [1] concurrently show...”)."". As such, I am flagging it for an ethics review.

**Edit after rebuttal**: I have increased my score from 3 to 4.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
The paper improves on other tree-based RNNs by reducing the computation.

Weaknesses:
This may be my lack of understanding on this subject area, but I think this is an overly complicated method that does not outperform existing methods or provide a clear benefit over them. There is some value to advancing a subfield of ML (i.e., tree-based RNNs), but I find it difficult to see the significance of this work. It may also be because of issues with the writing, see below. 

Besides the complexity of the method and the performance issues, the authors also do not benchmark on many tasks. They focus on the ListOps task from the Long Range Arena benchmark and don't consider Mega, which has achieved SoTA on this task.

**Writing**: Overall the writing is very difficult for someone who has not been actively thinking about this problem and working with tree-style RNNs. I found it very difficult to read the introduction and abstract because there is too much jargon. Below are some suggestions and questions that may help the authors improve their writing for an audience that is not working on tree-based RNNs. I generally recommend refactoring the writing into formal definitions so it is clear to see how the inner and outer recursions work together. As it stands, I couldn't even write down how your method would actually process a given input.
- Can you make a figure illustrating these different trees instead of trying to describe them in the text?
- I don't know what a ""strong RvNN"" is or what it means to process only some arguments that are sent in from an outer loop. (lines 75-79)
- Can you introduce clearly what ListOps is and why it is important to be length-generalizable for that task? 
- There is not enough information about these other methods in the tables and how they work.

Limitations:
The authors discuss the limitations clearly, which I appreciate. They mention that their architecture improves on tree-based RNNs but is much slower than other, seemingly stronger models. It also seems to not be able to learn from long sequences, which may be the point of a long-context model. One thing they do not mention is that their architecture seems to be very complicated and has many tricks involved. They also only evaluate on a few tasks, and they seem to be very focused on ListOps in particular. For other tasks, their method performs comparably to existing tree-based models (lines 325-326).

Rating:
4

Confidence:
2

REVIEW 
Summary:
The paper proposes a novel framework - Recursion in Recursion (RIR) for tree recursive neural networks (Tree-RvNNs) so as to get around the issue of computational infeasibility of typical RvNN models (Beam tree RvNNs are $\mathcal{O}(n)$) while still being able to exhibit length generalization on simple arithmetic tasks like ListOps. The models based on RIR have recursive depth bounded by $k\log_k n$ and still demonstrates over $90\%$ length generalization performance. They explain their method thoroughly and also compare on ListOps and LRA with a fairly large suite of baselines showing that their method perfoms comparably with the best.

Soundness:
3

Presentation:
4

Contribution:
2

Strengths:
1. The paper is very well written.
    - The method and related work is explained thoroughly while providing sufficient intuition.
    - If anything, I would urge the authors to explain the results a little more thoroughly even if it were at the cost of pushing some of the other content to the appendix.
2. The experiments are thorough.
    - The authors compare with a sufficiently large suite of baselines.
3. Length generalization is an important and difficult problem that existing methods (such as Transformers) struggle with. Getting RNN length generalization performance without the associated cost blowup is very interestin for the field.
4.  The authors adequately list the limitations of their work, which I appreciate.

Weaknesses:
1. The authors discuss the choice of $k$ somewhat briefly explaining that small $k$ surely hurts the performance of the resulting model.
    - On a few examples they try to suggest that $k=\mathcal{O}(\log n)$, however, without seeing this varied over several values of $n$, it is hard to justify this claim.
    - Note that if $k=\mathcal{O}(n)$, the method loses it's benefits.
2. The authors build on unpublished work which is cited as ""Anonymous"" and shared in the appendix. This is highly unusual and I have some concerns about this.
    - While there is nothing inherently wrong with this, it puts the burden of evaluating some of the claims within the paper to yet another unverified paper.
3. Some of the notation in the paper can be improved. Such as the example used for explanation in Sections 2 and 3: ""$7 + 8 \times 5 - 2$"". I feel this would be better if explained symbolically like $a_1 \cdot op_1\cdot a_2 \dots$""
4. Some claims are not justified very well (such as the Problem with th String-it solution on page 6)
5. I feel the results need more careful explanation. Table 1 seems like a very important result to justify the proposed method however, I don't understand why the table is split into two. (ListOps competitive and ?) Can you explain this?
6. In table 4, the proposed method RIR-GRC performs quite poorly, however this is not called out or explained.
7. Overall, the results are not particularly impressive. And while MEGA is mentioned multiple times, it does not appear to be listed in the benchmarks.
8. Efficiency and accuracy are compared in separate tables. Since the main contribution of the paper is an efficient implementation of a model that shows length generalization, I would like to see a computation vs performance tradeoff curve. I feel this would go a long way in proving the superiority of the proposed method.
9. This is very minor but I would recommend the authors include a short explanation of ListOps in the main paper. I understand that the authors chose to push it to the appendix due to space limitations but since it is such an essential part of the paper, I would recommend adding a few lines about it.

Limitations:
Yes. I commend the authors for including an honest and thorough limitations section.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper proposes Recursion in Recursion (RIR) to address the shortcomings that computationally efficient models like Binary Balanced Tree Recursive neural networks have in solving arithmetic tasks like ListOps. RIR also seeks to alleviate the computational burden brought forth by structure-aware ListOps-competent models such as the Beam tree RvNN. The approach, relying on 2 levels of computation, break down a sequence into chunks (of length k) on which an inner loop uses another network (e.g. Beam tree RvNN) to compute local representations. The representations from the inner loop are subsequently taken in by an outer loop. The authors suggest that this approach can help scale structure-aware inference from (n) to (k*log_k(n)), where n is the input length. The authors conducted experiments to show that their RIR-based models use much less time and memory for the ListOps task than baseline approaches. The results on ListOps, logical inference and long range arena (LRA) show that even with this efficiency, RIR’s performance is competitive with current baselines. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
The paper proposes a scaleable solution that can be structure-aware and effective at tasks like ListOps.
Experiments have shown that this approach (RIR) is promising, with performance competitive with less-efficient baselines. 
Overall, the paper is well-written and the contribution is clear.


Weaknesses:
Modifications are required to integrate existing models into the RIR framework, e.g. the EBT-RvNN, which can limit the widespread use of this approach.

Limitations:
Yes, addressed.

Rating:
6

Confidence:
2

REVIEW 
Summary:
In this work, the authors try to combine the best of two worlds in proposing Recursion in Recursion, where outer recursion is K-array balanced binary tree and inner implements its cell function for recursive neural networks for sequential inputs. The proposed framework is tested on various logical inference and NLP tasks to show model can reach respectable performance and scale to longer sequences. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. Good paper on scaling.
2. Paper is well written
3. Good sets of experiments.

Weaknesses:
1. Novelty is limited. The paper is more focused on scaling and several works are already published in the literature
2. Why inner cell uses BT-RvNNs, given they are similar to CYK-based RvNNs, and what advantage it offers should be mentioned in depth. Since selecting top-k is still heuristic and based on beam size, results will vary.
3. Huge standard deviation, indicating potential instability in the model.

Limitations:
1. Language can be improved, some sequence, some score, some function such terms should be avoided while explaining any mathematical concepts.
2. The variance of the proposed model is big leading to instability, hence what benefits it offers is questionable. What about the generalization effect, how does the model work on longer sentences? What is the limit? These questions are still unclear.
3. More analysis will benefit the work.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper addresses the long-sequence modeling problem. A recursion in recursion strategy is proposed to balance the advantages between BB-Tree RvNNs and RvNN models. The idea is straightforward but achieves competitive performance on LRA tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The paper is easy to follow.

Weaknesses:
1. There are large numbers of related works missing. For long sequence modeling, there are at least 4 types of methods, including Linear attention, SSM, Linear RNN, and LongConv. Since these methods are implemented for the same goal, they should be included in related work as well as the experiment section.
2. The experiments are inadequate. LRA is a toy benchmark for assessing long-sequence modeling. It is insufficient to use it as the sole indicator of effectiveness. In fact, this work focuses solely on a sub-task in LRA, making the experiments even weaker. I would encourage the author to verify the actual long sequence modeling capabilities in real-world scenarios such as language modeling, image classification, etc.
3. Linear RNN has achieved STOA performance in many benchmarks, including LRA, language modeling, and image classification, as demonstrated in recent papers. Why should we consider non-linear RNNs, which are slow to train and perform no better than linear RNNs?
4. The concept is straightforward and straightforward. By forming a RIR structure, it combines the advantages of the two methods. As a result, the processing time is lengthened. It would be more appealing if the processing time could be shortened. Furthermore, the competitive methods are ineffective. I don't see any standard benchmark LRA results, so it's difficult for me to justify the effectiveness of the proposed method.
5.The maximum sequence length used in this paper for an efficient long sequence modeling method is 2K, which is a standard sequence length for transformer LLM. It would be preferable to see the proposed method in long sequence tests, such as 32K and higher. Furthermore, long sequence modeling takes much longer to process than transformer, making the method less appealing in real-world scenarios.

Limitations:
Yes.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper introduces the recursion-in-recursion (RIR) framework for balancing the tradeoffs between
1. sequential processing, which offers better inductive bias and stronger solutions for many types of symbolic processing and logical inference tasks, but can be very expensive
2. balanced tree recursion, which shortens the length of the computational graph and can be fast and scalable, but struggles with the aforementioned tasks

The RIR framework is thoroughly evaluated on a simple set of arithmetic tasks (ListOps), where it shows the ability to solve and generalize on this task, while being much faster computationally than previous methods with this ability.


Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
- The paper provides a very well-explained exposition of a lesser-known model approach.
- The proposed method/framework is novel and makes intuitive sense.
- The method performs very well on symbolic processing tasks that it was designed for. It almost preserves the performance of much more expensive methods (e.g. full beam search) while being an order of magnitude faster.
- Results on other LRA tasks involving language are also shown, indicating that the method is not necessarily specialized to synthetic symbolic processing tasks but could be a viable more general approach


Weaknesses:
A weakness of the method itself is that the RIR framework adds complexity because of the two separate levels of hierarchy which can be freely chosen. Additionally the $k$ hyperparameter seems very important and there doesn't seem to be a first-principles way to choose it well. It seems like even if there is a model that can perfectly solve a given task, once RIR is introduced there are no guarantees about whether the task can be perfectly solved. Thus it becomes a heuristic tradeoff between efficiency and strength, with many hyperparameters that must be managed.


Limitations:
Main limitations are properly addressed. It might be worth being more explicit about the fact that the proposed family of methods is not meant to address other types of sequential data such as perceptual signals (e.g. images/audio) and likely doesn't work in those settings.  (Otherwise people may also wonder why the particular subset of LRA was chosen.)

Rating:
7

Confidence:
4

";1
x5aH1we8Bb;"REVIEW 
Summary:
This work proposes a new attack on monocular 3D object detectors utilising NERF representation to create multi-view consistent attacks utilising Lift3D [26]. The authors show successful attacks on nuScenes dataset and the effect of their design choices on the attack success. Furthermore, a mitigating strategy is shown how to make monocular 3D object detection robust to these types of attacks.

Soundness:
2

Presentation:
4

Contribution:
3

Strengths:
This manuscript includes a comprehensive set of ablation studies and detailed analyses, which effectively highlight the influence of different components in the proposed pipeline. Figure 4 is especially illustrative and insightful. Notably, the successful tackling of numerous molecular 3D detectors, each with varying detection strategies, is commendable.

The authors present an effective strategy to counteract the proposed adversarial attacks on molecular detection systems. 

The overall presentation of the paper is lucid and the figures contribute significantly to accurately conveying the intended ideas.

Weaknesses:
1- A comparison against baselines is missing, most notably the mesh-based baseline [43]. This comparison is very important since the proposed setup in Lift3D [26] is very similar when constraining the Nerf to shape and texture latents , resembling the mesh generation and texturing scheme in [43]. 

2- A crucial evaluation protocol has been overlooked. In the context of adversarial attacks, the imperceptibility of the attack is a significant factor; how discernible is the attack in contrast to the clean sample? This critical information is absent in this work. It leaves us questioning the perceptibility of image corruption in terms of pixel alteration. If the corruption is so pronounced that even a human observer fails to detect the cars, can it still be considered a successful attack? Previous studies typically provide this information [43]. Moreover, the inclusion of tests on KITTI should be considered vital to the evaluation process.

Limitations:
N/A

Rating:
7

Confidence:
5

REVIEW 
Summary:
This work develops adversarial attacks against 3D object detectors by utilizing instance-level NeRFs.  
They start with a representation of a vehicle, parameterized by a NeRF that predicts both geometry and texture and render the vehicle into a image, which they compose into the original image by copy-pasting. They use the composited image to adversarially attack 3D object detectors, which provides a gradient signal used to optimize the NeRF (texture only).  
Experiments show their adversarial examples are effective against a variety of different 3D object detectors, and they show that training on these samples improves robustness (and even overall performance).

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* Novel application of NeRFs, utilizing the fully differentiability to optimize for adversarial texture.
* Work is well written and overall clear to follow.  
* Analysis provides good insights (referring to Sec 5.3 analysis of 3D detector architecture robustness and Sec 5.4 adversarial training actually boosts performance).
* Multiple architectures used in experiments.

Weaknesses:
* Section 4.4 could use more elaboration - this is a key section for the overall work and in the current revision is quite vague.
* Some of the attacks (Fig. 3) do not look photorealistic - how can the reader be convinced these adverarial samples would actually work in the real world?

Limitations:
Authors provide discussion of limitations (real world safety).

Rating:
5

Confidence:
4

REVIEW 
Summary:
Deep neural networks (DNNs) have shown susceptibility to adversarial examples, which raises significant safety concerns, particularly in safety-critical applications like DNN-based autonomous driving systems and 3D object detection. While there is a wealth of research on image-level attacks, most of them focus on the 2D pixel space, which may not always translate into physically realistic attacks in our 3D world. In this paper, the authors present Adv3D, the first exploration of modeling adversarial examples as Neural Radiance Fields (NeRFs).

The utilization of NeRFs allows for the generation of adversarial examples that possess photorealistic appearances and accurate 3D generation, thereby enabling more realistic and realizable adversarial attacks in the 3D domain. The authors train their adversarial NeRF by minimizing the confidence of surrounding objects predicted by 3D detectors on the training set. They evaluate Adv3D on an unseen validation set and demonstrate its ability to significantly degrade performance when rendering the NeRF in various sampled poses.

To ensure the practicality of the adversarial examples, the authors propose primitive-aware sampling and semantic-guided regularization techniques, which facilitate 3D patch attacks with camouflage adversarial textures. The experimental results showcase the generalizability of the trained adversarial NeRF across different poses, scenes, and 3D detectors. Additionally, the authors provide a defense mechanism against these attacks through adversarial training via data augmentation.

In summary, the authors introduce Adv3D as a novel approach that models adversarial examples using NeRFs, resulting in more realistic and realizable attacks in the 3D domain. They demonstrate the effectiveness of their method through extensive evaluations and propose a defense strategy to mitigate the impact of these attacks.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
1. The writing and presentation of this paper are good and clear.
2. The idea of leveraging NeRF in generating adversarial examples is interesting
3. The authors conducted sufficient evaluation of the proposed method

Weaknesses:
1. The practicality of the proposed attack is questionable
2. There is a lack of real-world experiments
3. The study seems to lack technical insights as the adversarial attacks are well established. There is no surprise that using some fancy new idea can lead to adversarial examples, but the essence is the same.

Limitations:
The authors has discussed the potential negative societal impact of this study, so it should be fine.

Rating:
4

Confidence:
5

REVIEW 
Summary:
The authors proposed new generative adversarial examples in the form of NeRFs, in the context of driving scenarios. The training objective is minimizing the 3D detection confidence from a variety of views. The parameters to optimize are the latent input to the NeRF, that encodes shape and texture info. Rendering is naturally differentiable due to the usage of NeRF. To improve the physical realizability, they propose three methods: primitive-aware sampling, NeRF disentanglement, and semantic-guided regularization. The authors conducted experiments on the widely used nuScenes dataset to evaluate the performance drop. The results show that their method is able to reduce the detection performance of various detectors, whether they are FOV-based detectors or birdview-based ones. They also evaluated the transferability of their method, and the adversarial training defense method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
Using NeRF as 3D adversarial example representation seems novel and interesting. The NeRF representation naturally is differentiable in terms of rendering, so it makes the adversarial attack problem easier. Also, with more uses of NeRF in 3D vision, it is important to explore the vulnerability in NeRF itself. Such adversarial attacks may highlight the potential security issues in NeRF.

The attacking framework (expectation over transformation), the NeRF rendering framework they use (Lift3D) are standard. The method is mostly built upon existing works; it seems not hard to implement their method.

The writing is clear.

Weaknesses:
My major concern is that whether the formulation of NeRF is necessarily, from the motivation perspective. In line 175, they fixed the shape and only optimized the texture latent code. The optimization is essentially finding the color, density of the volume. However, I believe most vehicle objectives are not translucent; the optimized 3D object is very hard to realize. This is evident as authors need to improve the physical realizability (line 180).

This leads to the Occam's razor principle: do we really need NeRFs to reach the effectiveness/realizability of the 3D attack? So we are missing a baseline here: optimizing the surface texture as a 3D mesh, using existing differentiable mesh renderers (such as Neural Mesh Renderer). The latter is easier to optimize (2D texture space), and more physically realizable (because it is a texture map rather than a volume). In line 166, the authors said ""enables patch attacks in a 3D-aware manner by lifting the 2D patch to a 3D box"", so we really need a baseline to showcase such lifting is necessary. Also it is not clear how rendering the NeRF into 3D scenes is done. In Fig. 3, the lighting of the NeRF object is not consistent with the environment, and we can see typical blurriness of NeRF.

Another weakness is that the setting is not sophisticated enough to be ""Driving Scenarios"". At first glance, it looks like attacking self-driving algorithms, but the point clouds are not used (correct me if I am wrong). The detection methods (FCOS3D, PGD-Det etc) are based on monocular/multi-view 2D images instead of multi-sensor. In Fig. 3, the inserted adversarial example does not seem to block the LiDAR rays. The experiment is not done through a full driving simulation software, but by rendering 3D objects into existing 3D data. Whether such mixed environment can represent the real-world driving scenario is not clear. It'll be better to claim general 3D detection scenario and do more experiments with other objects, instead of only claiming driving-specific scenarios.

In general, my decision largely depends on the first point: the NeRF representation may not be necessary under the current settings. Optimizing the texture image should just work; such volume formulation will make it harder to physically realize and does not bring much benefit other than differentiable rendering.

Limitations:
The authors addressed the limitations about dataset annotations and potential harmful consequences.

Rating:
4

Confidence:
4

REVIEW 
Summary:
This work proposes to generate 3D adversarial examples for attacking 3D object detectors in driving scenarios using NeRF. In particular, it integrates a series of techniques, including primitive-aware sampling and semantic-guided regularization, to ensure the physical realism and realizability of the generated adversarial examples. Extensive experiments have validated the effectiveness of the proposed method in reducing detection performance and serving as data augmentation.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
1. As an early attempt of generating 3D adversarial examples using NeRF, this work could offer a new perspective for the community in understanding and tackling real-world 3D adversarial attacks. 

2. The extensive experiments validate the superiority of NeRF as a 3D adversarial attack generator. In particular, it is interesting to see that the generated adversarial examples can serve as a data augmentation to improve clean performance, which aligns with the previous observations in classification.

Weaknesses:
1. My major concern is the assumed attacking setting of this work, i.e., how to leverage the proposed method in real-world driving scenarios. If only a static adversarial example is attached to the scene, generating other static objects on the road may be more practical than generating a vehicle; Otherwise, the authors are expected to show a video under an egocentric view to demonstrate the attack effectiveness, i.e., whether the dynamically moving adversarial vehicles can consistently mislead the 3D detectors from different view directions.

2. The claim ""the first exploration of modeling adversarial examples as Neural Radiance Fields (NeRFs)"" in the abstract may not be accurate. ViewFool [1] also models adversarial examples using NeRF although only the view direction is adversarially optimized. It will be more accurate if the authors highlight this work as the first 3D adversarial example generator using NeRF.

3. Missing references regarding the early attempts of marrying NeRF and adversarial attacks (which are mostly orthogonal with this work):

[1] ""ViewFool: Evaluating the Robustness of Visual Recognition to Adversarial Viewpoints"", Y. Dong et al., NeurIPS'22.

[2] ""NeRFool: Uncovering the Vulnerability of Generalizable Neural Radiance Fields against Adversarial Perturbations"", Y. Fu et al., ICML'23.

[3] ""Aug-NeRF: Training Stronger Neural Radiance Fields With Triple-Level Physically-Grounded Augmentations"", T. Chen et al., CVPR'22.

4. Minor issue: There exists some inconsistency in terms of tense and punctuation, which could be improved in the final version.

Limitations:
Although the developed adversarial attacks may cause security concerns, this work intended to gain a deeper understanding of 3D adversarial examples and improve the achievable robustness on them, thus not suffering from negative societal impact.

Rating:
5

Confidence:
4

";0
td6xbEOPLr;"REVIEW 
Summary:
This paper proposes to attack different fairness definitions for a variety of graph learning models. The task is formulated as a bi-level optimization problem, which is solved in a meta learning manner. The major advantages of the proposed framework include 1) it is feasible to any fairness notion and graph learning model, 2) it can support continuous and discrete perturbation on the graph topology. Experiments demonstrate the attack efficacy and classification utility of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
1. The proposed framework provides a good coverage for multiple types of (differentiable) fairness notions (e.g., statistical parity, individual fairness), and graph learning models (e.g., non-parametric, parametric);
2. Extensive experiments are conducted, considering possible fairness defenses (FairGNN and InFoRM-GNN) and transferability.

Weaknesses:
1. The clarity and rationale of methodology can be improved: there are inconsistent definitions (of the budget constraint), and the realization of budget constraint in the optimization procedure is not validated. See detailed questions;
2. The IID assumption for kernel density estimation may not hold in graph data;
3. Important baseline should be compared (a modified version of DICE based on the sensitive group); and more related works should be discussed.

Limitations:
The authors provide reasonable discussion about the limitations of this work (i.e., other fairness notions, space efficiency).

Rating:
5

Confidence:
4

REVIEW 
Summary:
The authors propose an attacking framework called FATE. Existing research in algorithmic fairness aims to prevent bias amplification but neglects fairness attacks. This paper fills this gap by formulating the fairness attack problem as a bi-level optimization and introducing a meta-learning-based attack framework. The authors present two instantiated examples, demonstrating the expressive power of the framework in terms of statistical parity and individual fairness, and validate the model's capability of attacking fairness through experimental verification. The paper contributes by providing insights into adversarial robustness and the design of robust and fair graph learning models.

Soundness:
2

Presentation:
4

Contribution:
3

Strengths:
1. This paper is well-structured and easy to follow.
2. The originality of the article deserves emphasis as it formulates the fairness attack on graph data as a bi-level optimization problem. This novel approach contributes to understanding the resilience of graph learning models to adversarial attacks on fairness.
3. The experimental results show that the proposed method is capable of attacking fairness without decreasing too much on accuracy.

Weaknesses:
The motivation provided is somehow insufficient in persuading me. The authors state that “an institution that applies the graph learning models are often utility-maximizing” (line 82), which I totally agree with, but then concludes that “minimizing the task-specific loss function … for deceptive fairness attacks” (line 88). While I do agree that pursuing utility will lead to a preference for models with superior performance, and if the objective of the attack is to deceive victims into selecting an unfair learning model, then it does make sense to enhance the utility of the malicious model. But it’s not the case in this article, where the attack’s aim is to poison the graph data. Unlike models, we don't have much discretion when it comes to the data, and it is exceedingly challenging for me to envision a real-life scenario wherein an institution would discard the data due to unsatisfactory performance, as a more practical solution is data cleansing or a more capable model. Consequently, my concern is that, is it truly necessary to maintain the utility for “deception”, leaving aside that I’m not convinced that preserving the utility is necessary for successful deception. Please provide a more detailed explanation.
 
The experimental findings reveal the limitations of the attack's effectiveness. Although I agree that FATE is capable of attacking fairness and is relatively more stable, in 2 out of 3 datasets, FATE exhibits incompetence in reducing statistical parity compared to FA-GNN. Although the authors argue that the victim model achieved the best performance under the attack FATE, I’m skeptical about the cost-effectiveness of this trade-off.

Limitations:
Yes, the authors have addressed the limitations and potential negative societal impact of their work.

Rating:
5

Confidence:
3

REVIEW 
Summary:
The paper proposes a novel framework named Fate, which is capable of attacking any fairness definition on any graph learning model, as long as the corresponding bias function and the task-specific loss function are differentiable. Fate is equipped with the ability for either continuous or discretized poisoning attacks on the graph topology.studies. The paper provides insights into the adversarial robustness of fair graph learning and sheds light on designing robust and fair graph learning in future studies. The empirical evaluation on three benchmark datasets shows that Fate consistently succeeds in fairness attacks while being the most deceptive (achieving the highest micro Fl score) on semi-supervised node classification.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
(1) This paper addresses an important problem in graph learning — fairness attacks. While previous work in the field focused on ensuring that bias in not perpetuated or amplified during the learning process, the proposed framework, FATE, allows for the study of adversarial attacks on fairness.

(2) FATE, a meta-learning based framework, is versatile and can be used to attack different fairness definitions and graph learning models.

(3)The experimental evaluation shows that the proposed framework can successfully attack statistical parity and individual fairness on real-world datasets with the ability for poisoning attacks on both graph topology and node features while maintaining the utility on the downstream task.

(4) This article is well-written and well-organized. It starts with an introduction that highlights the importance of fair graph learning and the need for resilience against adversarial attacks. Then it provides some background information and defines the problem of fairness attacks in graph learning. The paper then proposes the Fate framework as a solution to this problem, providing a detailed explanation of its design and mechanism. It also presents experimental results to evaluate the efficacy of Fate. Finally, the paper concludes with a summary of its contributions and future research directions. Overall, the writing logic is clear and easy to follow.

(5) The paper provides detailed information on how they implemented the proposed framework, including the optimization process, the selection of the bias function and the task-specific loss function, and the hyperparameter tuning process. Additionally, they provide a detailed description of their experimental setup, including the datasets used, the graph learning models, the evaluation metrics, and the implementation details of the Fate framework and other baselines. The authors also provide a thorough evaluation of the proposed framework through extensive experiments and analysis.


Weaknesses:
(1) One weakness of this paper could be the limited evaluation of the framework on only three benchmark datasets and one task (semi-supervised node classification).
 Further evaluations on various graph learning tasks and datasets could provide more insights into the effectiveness and generalizability of the proposed framework.

(2) The proposed Fate framework may not be effective in attacking other fairness definitions beyond statistical parity and individual fairness, and it may not work well on graph learning models with very large graphs. 

(3) The paper assumes that the attacker has access to sensitive attributes of all nodes, which may not always be feasible in real-world scenarios.


Limitations:
see comments above

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper presents a novel approach for introducing fairness attacks in graph learning, which is impressive. To address this issue, the article proposes an attack framework for graphs and conducts experiments on the classic GCN model. Compared to two baseline methods, DICE and FA-GNN, the proposed method is more effective in attacking graph neural networks.

The strengths of the paper include:

1. The research problem is novel and interesting. There is little prior work on attacking graph models, and this article is the first to define this type of problem.
2. The paper provides code for the proposed method, which makes it more reproducible.

The weaknesses of the paper includes:

1. Some of the content organization is not optimal, such as placing the descriptions of the baseline methods in the appendix. Including them in the main text would have made it more convenient for readers.
2. The article does not mention the limitations of the work. More discussions are required.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. The research problem is novel and interesting. There is little prior work on attacking graph models, and this article is the first to define this type of problem.
2. The paper provides code for the proposed method, which makes it more reproducible.

Weaknesses:
1. Some of the content organization is not optimal, such as placing the descriptions of the baseline methods in the appendix. Including them in the main text would have made it more convenient for readers.
2. The article does not mention the limitations of the work. More discussions are required.

Limitations:
The article does not mention the limitations of the work. More discussions are required.

Rating:
7

Confidence:
2

REVIEW 
Summary:
This paper studies an interesting problem, attacking fairness on GNN. Specifically, the authors aim to amplify the unfairness while maintaining the performance of the downstream tasks. They propose a bi-level optimization scheme with a meta-gradient poisoning attack to achieve this goal. Experiments on both statistic fairness and individual fairness show the effectiveness of the proposed method. Overall, good problem and a solid framework.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1.	Fairness attack on graph learning is an interesting problem, this paper gives an intuitive problem definition, which can extend to other bi-level optimization goal attacking scenarios.
2.	Given the bi-level optimization goal, the authors design a meta-gradient graph poisoning attack and corresponding Low-level and High-level loss functions.
3.	Experiments on statistical fairness and individual fairness demonstrate the effectiveness of the proposed method.


Weaknesses:
1. It will be more interesting if the authors provide more experiments on group fairness and Rawls fairness. How to attack some specific groups, such as best/worst accuracy group fairness.

Limitations:
None

Rating:
6

Confidence:
3

";0
KAlSIL4tXU;"REVIEW 
Summary:
The paper proposes PromptIR, a prompt-based learning approach for all-in-one image restoration. The method utilizes prompts to encode degradation-specific information and dynamically guide the restoration network. It achieves state-of-the-art results on various image restoration tasks.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
(1) The proposed method offers a generic and efficient plugin module with a few lightweight prompts that can be used to restore images of various types and levels of degradation with no prior information of corruption.

(2) The method achieves state-of-the-art results in image denoising, deblurring, and dehazing.

(3) The paper is well-written and easy to follow.


Weaknesses:
(1) The paper lacks discussions on the limitations of the proposed method, such as the sensitivity of the results to the number of prompts used and how the method performs on combined types of degradation, such as noise + rain.

(2) The paper lacks the visualization of the learned prompt and an explanation of how and why it could work.

(3) The paper lacks comparisons on complexity, including runtime and parameter number.


Limitations:
See the weakness and question part.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper proposes an all-in-one image restoration method that can restore images from various types and levels of degradation. Specifically, it utilizes prompts to encode degradation-specific information, which is used to dynamically guide the restoration network. Experimental evaluations on image denoising, deraining, and dehazing are conducted to demonstrate the effectiveness of the proposed method. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
Prompts are used to encode degradation-specific information for all-in-one image restoration.

Experimental evaluations and ablation studies are conducted to demonstrate the superior performance of the proposed method.

Weaknesses:
There are several unclear statements listed as follows.

Limitations:
This paper does not claim the limitations and potential negative societal impact of their work. It would be better to discuss about the limitations in real applications and potential negative societal impact of their work.

Rating:
5

Confidence:
5

REVIEW 
Summary:
In this paper, authors study to restore images with unknown degradation types with a unified framework. To achieve it, a prompt-based block is proposed which generates suitable prompt first and then interact them with original features. Experiments show that combining the proposed block with restormer network can achieve new SOTA performance.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. Prompt learning is recent a hot method in both language and vision domains. Different from previous works focus on high-level understanding tasks, this paper tempts to explore its application on low-level image restoration task.
2. The writing is good and easy to understand.

Weaknesses:
1. Typos or confused explanations: 
     a. In Eq. 2, subscript of w should be the same with P in $\sum_{c=1}^{N} w_{I}P_{c}$? And the subscript of w in the second formula should be removed?
     b. Line 208, ""Table 1 Similarly on the image Deraining task, "".
     c. Line 204, the second best approach is not DL in Table 1, is MRPNet. 
     d. Line 231-232, it mentions that using only one prompt block in the latent space (36.76 dB) degrades the performance. However, the baseline is 36.74 dB, which lower than 36.76 dB.
2. Lack some comparisons. For example, the performance restormer is needed in Table 1. And other all-in-one weather image removing methods[1][2] are still very related to this work and needed to be compared. In addition, all results are in the synthesized images, how about its general performance on real world images (e.g., real rain images).
3. Ablation study can be further improved. For example, the number of prompt is not studied. And the additional computational cost and model parameters should be discussed. Is the improvement gains come from the additional parameters?
[1] Transweather: Transformer-based restoration of images degraded by adverse weather conditions;
[2] Learning Multiple Adverse Weather Removal via Two-Stage Knowledge Learning and Multi-Contrastive Regularization: Toward a Unified Model;

Limitations:
Authors have not discussed the limitations. In my opinions, one of the potential limitations is the unproved generalization ability in terms of networks (e.g., utilization on AirNet) and data (e.g., real world images).

Rating:
5

Confidence:
5

REVIEW 
Summary:
This paper restores images from various types and levels of degradation. In detail, degradation information is encoded as prompts and used to guidance the restoration. Based on the classical UNet architecture for restoration, there are two added blocks: prompt generation module and prompt interaction module. With a single network, it achieves state-of-the-art performance on image denoising, deraining and dehazing.

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1, A prompting-based single-model blind restoration framework.

2, A plug-in module with prompt generation and prompt interaction, which guide the backbone model to remove corruptions effectively.

3, State-of-the-art performance on multiple tasks with one single model

Weaknesses:
1, The ablation studies cannot prove this basic concern of the main contribution and intuition in this paper. Is the prompt (5 learned tensors as a dictionary) really working? What have the prompts learned? Do we really need the prompts? Since there are only 6 different types of degradations, I guess $softmax(Conv_{1\times 1}(GAP(F_1)))$ learns a classification model to decide which degradation the input image has. If we directly inject this information to the UNet, it should work as well. This paper still follows the pipeline of ""degradation estimation + degradation-aware restoration"".

2, Missing comparison on model size, runtime, FLOPs, etc. According to the model file sizes, I suspect there exists unfair comparison. The model size of PromptIR (file size: 388MB) is about 4 times of Restormer (file size: 99.9MB) and 11 times of Airnet (file size: 35.6MB). This means that the model might use 100MB for each sub-task and the rest 88MB for estimating the degradation type.

3, For the single-task setting, for example, image denoising with sigma=50, what's the meaning of using prompt generation and interaction?

4, Missing ablation studies on prompt design, such as number of prompts.

5, Why adding prompt blocks at level 4 is nearly of no use (36.76 v.s. 36.74dB)? Note that it has more parameters in the prompt generation module and prompt interaction module. Besides, how about using level 2 only?

6, In table 7 and 8, why PromptIR is significantly better than Airnet on unseen noise level, but is only slightly better on spatially variant degradation?



Limitations:
See weakness

Rating:
4

Confidence:
5

REVIEW 
Summary:
The paper proposes prompt based image restoration method that can handle different degradations. the proposed method uses prompts to encode degradation-specific information, which is then used to dynamically guide the restoration network. Given any instance, prompt generation module (PGM) and a prompt interaction module (PIM) are used generate input-conditioned prompts (via PGM) that are equipped with useful contextual information to guide the estoration network (with PIM) to effectively remove the corruption.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- Paper is well organized 
- PGM, and PIM modules are used to generate the prompts and recover the image. Proposed method is clearly explained 


Weaknesses:
- Can the proposed method method handle degradations like snow, night, blur, and turbulence conditions, since the proposed method mentions it is generic image restoration.
- experiments are limited, since the experiments showed in the paper covered only haze, rain, and noise, specifically these the datasets used for comparison are synthetic and very small datasets. it is hard understand the limits of the proposed method from these experiments
- Additionally the experiments performed contains only single degradation at any instance, can the proposed method handle multiple degradations in the single input image
- Can authors performs the experiments showing if the wrong prompt was propagated (instead of the prompt from the PGM) to understand the importance of PGM module.


Limitations:
Limitations of the proposed method were not discussed

Rating:
5

Confidence:
5

";1
tRKimbAk5D;"REVIEW 
Summary:
This paper proposes an image-computable model of human motion perception, bridging the gap between biological computation and CV models. The proposed model contains a two-stage approach that combines trainable motion energy sensing with a recurrent self-attention network for adaptive motion integration and separation. The similarity of the proposed model to human visual motion processing is demonstrated by computer neurophysiology experiments and psychophysics experiments.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
S1. This study applies DNNs to construct a model of human motion perception that extracts informative motion flows for a wide range of inputs. Specifically, a two-stage model is proposed that simply mimics mammalian V1 and MT functions, respectively. 
S2. The two-stage model’s neurons exhibit direction and speed tunings similar to those observed in mammalian physiological recordings in V1 and MT.
S3. Human-like responses are generated to several traditional motion stimuli and illusions, including the global motion pool and the barbershop illusion, showing good generalization from texture-free stimuli (e.g., drifting Garbo) to complex natural scenes.
S4. This paper is clear and well-written overall.

Weaknesses:
W1: Although the primary purpose of this paper is to better model human motion perception, there is a gap between the two-stage model and state-of-the-art models in the CV field regarding the optical flow prediction performance, which needs more explanations/analyses.
W2: The proposed two-stage model is relatively complicated, but this study lacks complexity analysis for each part. Considering the low energy consumption property of the human brain, energy efficiency may be another advantage of this model compared with other SOTA models.

Limitations:
This paper has discussed its limitations in modeling human motion perception and its potential influence on future research. Maybe it’s better to add some limitations regarding to its performance on optical flow prediction, compared to other SOTA models.

Rating:
7

Confidence:
4

REVIEW 
Summary:
I have read the authors' rebuttal and will maintain my already high rating.

This paper proposes a new model of the dorsal pathway (V1->MT) using a two-stage architecture. The first stage uses spatiotemporal filters tuned by supervised learning, while the second stage uses a dynamic connection between motion detectors based on the similarity of motion responses. The model is able to capture several important aspects of human motion processing, including both neurophysiological properties and psychophysical responses.

Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
-The paper is very well written, although there are a few places where it is not clear. 

-The model is quite convincing in its fits to the data, both neurophysiological and psychophysical. 

-The models fits to data are compared to 8 different SOTA models of optical flow, and when the correlation to the ground truth is factored out, the model is superior to all others (by having a high correlation to the ground truth motion, the models will necessarily	have a high correlation with the human data). This is an important test.

-The model can not only account for responses to low-level stimuli (drifting gabors), but can also give convincing responses to natural and artificial movies. I am not an expert in motion processing, but to my knowledge, this is the first model to do this. Perhaps other reviewers will know of other models with this capability.

-The model is a sophisticated one, using a dynamic graph construction that connects the motion detectors in stage 2, integrating the motion detectors in stage 1 based on similarity of response. In this way, the model can capture global motion, solving the aperture problem.

Weaknesses:
-The model’s sophistication is also a weakness: The dynamic graph construction assumes all responses can be influenced by all other responses, depending on the response similarity. That is, dynamically, any response integrator can be connected to any other, no matter how far away spatially. This is what allows the network to integrate the local motion into global motion. How this could be implemented neurally is very unclear. However, this is an argument from lack of imagination. Also, the paper acknowledges this limitation.

-Some of the presentation is unclear/unconventional: For example, figure captions don’t label all components of the figure (e.g., the caption to Figure 5 only describes 5D), although they may be discussed in the text. It is not always clear what is being shown in the Figures, which are relatively complex.

Limitations:
Barber pole illusion is not quite captured properly (Figure 4D), although the end points are.

The neural implementation of this model is unclear. This is mentioned as a limitation.

Rating:
8

Confidence:
3

REVIEW 
Summary:
This paper proposes a novel model of motion analysis. It takes inspiration from biological motion processing to try and solve the aperture problem, leveraging a combination of biologically inspired constrained structure with learned parameters. This produces strong partial correlations of motion components in empirical tests, but also interesting emergent phenomenon when investigating the distribution of unit responses between the different network components.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
- This paper does a good job of anchoring model development with a strong motivation and justifications based on biological understanding of motion processing. This is not an easy task, and the paper does a good job of finding compromise between the performance-driven approach of data fitting with the desire for an explanatory model that provides insight into why a certain performance or behaviour is achieved.

- The paper is quite dense with a wide range of ways to explore the problem and the model. This is both a strength and a weakness, as it sometimes makes the paper difficult to follow, but the range of analysis is impressive.

Weaknesses:
- Missing reference: Tsotsos et al., ""Attending to visual motion"", CVIU 2005
  -- This paper provides a biologically motivated model of motion including attentional effects and structure informed by our understanding of visual areas V1, MT, MST, and 7a in the primate visual system.

- Figure 1 is rather confusing; there's a lot going on, and I found flow of information rather unclear. If possible I would recommend breaking it up into separate figures or finding a way to show how the different components connect in a clearer manner. For example, is C a sub-component of A, or just some example receptive fields? If the latter, why is it part of the network diagram? Similarly, in the text F is given as having dimensions HWx256 (Line 140), but in the RMIB section of Figure 1 it looks like it is supposed to have HxWxC dimensions. Is this a different F? This was not clear.

- The partial correlation metric should be clearly defined; I did not see a definition for this in the paper. It would be better to have more discussion of the aspects that the model does not excel at compared to competing models (e.g. FlowFormer for v.s. Human and RAFT for vs. GT in Table 1).

Limitations:
Limitations seem to be adequately discussed.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper proposes a new V1-MT model using a normalized Gabor model of V1, followed by a recurrent self-attention stage. It uses dense optic flow as a supervised objective. The authors perform extensive in silico neurophysiology to show the model units qualitatively look like V1 and MT. They also show that this model is a better match to human visual perception on natural scenes than computer-vision models.

Soundness:
3

Presentation:
3

Contribution:
2

Strengths:
- Well-written and clear
- Breadth of in silico experiments
- The model makes sense and I’m excited about the idea of segregation vs. integration as an explanation for complex receptive fields in MT

Weaknesses:
- Pretty incremental in a crowded field
- Little comparison to SOTA
- Mostly qualitative and little quantification

I really want to like this paper: full disclosure, I’m a big fan of the work of Orban et al. and more recently Cui et al. (2013) that shows that MT cells have complex receptive fields capable of integration and segregation. However, this paper kind of rubbed me the wrong way by ignoring the state-of-the-art in this field: “Despite extensive research in cognitive neuroscience, image-computable models that can extract informative motion flow from natural scenes in a manner consistent with human visual processing have yet to be established”. There are plenty of image-computable models that can extract information relevant to dense optic flow from natural scenes. Everything from the old work of Simoncelli and Heeger to the receptive field models empirically derived from MT of Nishimoto and Gallant (2011) to MotionNet from Rideaux et al. and especially to Mineault et al. (2021), inexplicably uncited in this manuscript despite being published in these very pages and having very similar motivation to this manuscript. The onus is on the manuscript to show us some failings of these previous models and how the model does better according to some axis. 

To be fair, the paper could say that some of these networks–especially generic 3d CNNs without multi-scale representations and explicitly organized direction and speed tuning–do not extract quantitative optic flow information *explicitly*. However, consider this thought experiment: if I wanted to read information from a patch of MT, as a psychophysical observer presumably needs to do to solve the tasks in the paper, I wouldn’t have access to a neat organization of direction and speed tuning, either; I would need to do a readout on top of the patch. I don’t buy the premise that the brain needs to form an explicit estimate of dense optic flow at every point in space, e.g. in MT. I think it’s easy enough to turn these old models into ones that estimate dense optic flow using a linear readout, and these should be compared to the proposed model.

The paper spends a lot of time rehashing the same kind of qualitative receptive field exploration that’s a hallmark of this field: tuning curves, component vs. pattern, speed tuning curves, distribution of preferences for direction tuning, barber poles, reverse phi, etc. This is a very crowded field between all the papers from Rideaux, Welchman, Fleming, Mineault, Bakhtiari and Pack: these kinds of demonstrations have been done over and over again. It’s nice to have it in the paper but I really consider these a sanity check, not a finding, especially since this paper bakes in Gabor receptive fields in the first layer: how could they not learn direction selectivity? My advice, speed run through these to give more space for the end of the paper, which is where things get interesting.

Definitely the most interesting thing about the paper, in my view, is Table 1. It shows that the network knows something about how human motion estimation operates that is not captured by CV models. It’s a bit buried in the paper and explained a bit fast. The authors should add ablations to figure out what it is about this network that makes it more brain-like - the Gabors? the normalization? the attention mechanism? I also think they need to add in quantitative comparisons with MotionNet from Rideaux et al. and DorsalNet from Mineault et al. with a linear decoder on top–to be clear, I’d be fine with a linear decoder trained on another task. You could avoid doing a linear decoder by using an RSA-based analysis, or you could use other methods of alignment which are more restrictive than a linear decoder (e.g. from Alex Williams et al.), if you believe this is not an apples-to-apples comparison. 

Overall, I think this could be a valuable contribution to the field, but it needs to be very explicit about its specific contribution in light of plentiful previous work, and it needs explicit comparisons to this previous work. I would be happy to accept granted the authors cite and address previous literature and include quantitative comparisons to MotionNet and DorsalNet, provided their model comes out on top either according to the metrics they have in Table 1 and Figure 5 or some other relevant metric they find.

Nitpicks:

- I liked the convention of using red color for trainable parameters, but the authors only use that once and that drop that later. Would recommend doing it consistently throughout the methods.
- Bottom of Page 4: “Top side of Fig. 3” → Should be a reference to figure 1. There are a couple more instances of this, so the authors should go carefully through the manuscript to find if there are more instances of that.
- The model is fairly bespoke but it’s not particularly biologically plausible with its attention mechanism. If the point is to implement recurrence, why not just use a plain transformer with tied weights at every layer instead? To be clear, the authors don’t have to fit this model, just a sentence or two to justify why they picked this architecture rather than something more off-the-shelf.

Limitations:
-

Rating:
6

Confidence:
4

REVIEW 
Summary:
In this paper, the author tries to build up a visual system like human eyes. In this study, the authors propose a two-stage model that combines trainable motion energy sensing with a recurrent self-attention network to capture the computations in V1-MT, the core structure for motion perception in the biological visual system. The model's unit responses are similar to mammalian neural recordings regarding motion pooling and speed tuning and replicate human responses to various stimuli. The model outperforms several state-of-the-art computer vision models in explaining human responses that deviate from the ground truth.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
1. This paper has a deep exploration of the human visual system based on on-the-shelf computer vision modules.The idea of using two stage framework to build up this system is insightful. 
2. The design of this paper is intuitive and this framework achieves reasonable results.



Weaknesses:
1. In this paper, the authors do not provide much visual evidence to prove their system.
2. In some experiments, their method is still worse than state-of-the-art

Limitations:
Please refer to weaknesses

Rating:
7

Confidence:
2

";1
dnGEPkmnzO;"REVIEW 
Summary:
Fully dynamic k-clustering in O(k) update time

This paper studies fully dynamic k-clustering. It gives a fully dynamic algorithm that maintains O(1)-approximate solutions to k-median and k-means with \tilde O(k) amortized update time and \tilde O(k^2) worst-case query time. On the negative side, the authors showed that the Omega(k) amortized update time is required if one needs to achieve O(1)-approximation ratio and poly(k) query time. So they gave the optimal update time, and the time improves the prior best-known update time of O(k^2).  The authors also did experiments to complement the theoretical analysis. 

The dynamic algorithm is built on the static algorithm of [27]. It runs for many iterations. In each iteration i, the algorithm samples a set S_i of points and creates a set of smallest-radius balls around the samples so that the balls cover at least beta fraction of the remaining points, for a constant beta. The algorithm then removes the covered points and repeats the process.  This gives a partition of the points into \tilde O(k) balls. They construct an assignment that maps every point to its ball center. Then a k-median or k-means solution can be constructed using the centers only. 

The dynamic algorithm maintains the tree-structure constructed in the static algorithm in a lazy manner by allowing some slack at many places. It needs to rebuild the tree from some layer if the cost of the assignment at that layer or above becomes bad. On average, it needs many updates for the algorithm to rebuild a tree from some layer, giving a good amortized update time. 



Soundness:
4

Presentation:
3

Contribution:
4

Strengths:
Overall, the paper gives a tight update time of \tilde O(k) for the fully dynamic k-clustering problem, using elegant techniques. This is a solid accept.  

Weaknesses:
The hidden approximation ratio is a little big. 

Limitations:
No limitations.

Rating:
7

Confidence:
4

REVIEW 
Summary:
This paper considers the dynamic version of the $k$-median and the $k$-means clustering problem in an arbitrary metric space. The authors provide an $O(k)$ amortized time (which is near optimal based on a lower bound that the authors provide) for insertions and deletions of points and a query time of $O(k^2)$. This is an improvement of the recent result by Henzinger and Kale, ESA 2020 which provided a dynamic algorithm with a worst case $O(k^2)$ update time. The algorithm in this paper is based on making the algorithm of Mettu and Plaxton dynamic. The authors also provide implementations of their algorithm as well as prior works (including a coreset construction algorithm) and then provide an empirical performance analysis of these algorithms.

Soundness:
4

Presentation:
4

Contribution:
3

Strengths:
This paper is providing a near-optimal bound for amortized update time for dynamic k-means and k-median clustering problems.

 The authors provide implementations of their algorithm as well as prior works which is very useful.


Weaknesses:
In terms of techniques, the methods used are adaptation of an existing algorithm of Mettu and Plaxton and perhaps somewhat incremental in nature.

Limitations:
NA

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper studies dynamic algorithms for k-median (and k-means) in general metrics. The main result is an \tilde{O}(k) update time, poly(k) query time algorithm that achieves O(1)-approximation. Here, the update model is point insertion/deletion, and the algorithm has access to the distance oracle. A query operation asks to return a list of k points that is O(1)-approximate.

The result improves over the k^2 update time achieved in HK20. This is achieved using a somewhat different but more direct approach. In particular, in HK20, a general reduction to coreset via merge-and-reduce framework was employed, but this work employs a direct dynamization of MP04.

Comprehensive experiments are also provided. Compared with HK20 as a baseline, the new algorithm runs faster and achieves an overall better accuracy. 

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
- The paper achieves near-linear in k update time, which is a nice improvement over the previous k^2
- The experiments are convincing, and justify the theoretical improvement
- A nearly-matching lower bound is provided

Weaknesses:
- This work only yields O(1)-approx, while HK20 uses coreset approach and hence can build an eps-coreset even for general metrics (even though it does not imply an efficient algorithm for finding a solution). This also means this work cannot be easily generalized to the Euclidean case, where near-linear time PTAS was known.
- The query time is k^2, which may be improved

Limitations:
I didn't find these explicitly discussed. A discussion of limitations, and possibly mentioning future directions, could be helpful. This paper is a theory paper and I don't see a potential negative societal impact.

Rating:
6

Confidence:
3

REVIEW 
Summary:
This paper studies the $k$-median/means problems in fully dynamic settings. Clustering in fully dynamic setting is a recent hot topic, where fully dynamic $k$-center has been well studied in literature. However, little was known for fully dynamic $k$-median/means problems. Inspired by the static framework proposed by Mettu and Plaxton, where a minimum radius ball coverage strategy is used to obtain $t=O(logn)$ layers of representations for good approximation of the given $k$-median/means instance. This paper modifies the classic framework by Mettu and Plaxton and presents an $O(1)$-approximation algorithm for the fully dynamic $k$-median/means problems with $\tilde{O}(k)$ amortized update time and $\tilde{O}(k^2)$ worst case query time, which improves the previous coreset-based method with $\tilde{O}(k^2)$ worst case update time. 

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
1. This paper proposes simple but efficient approximation algorithm for the fully dynamic $k$-median/means problems with $\tilde{O}(k)$ amortized update time and $\tilde{O}(k^2)$ worst case query time, which improves the previous result with $\tilde{O}(k^2)$ update time and $\tilde{O}(k^2)$ query time. The authors also show that the update time of our algorithm is optimal up to polylogarithmic factors.

2. This paper gives detailed experimental evaluation of fully dynamic k-median algorithms for general metrics and shows that the proposed framework is more efficient than previous ones.

Weaknesses:
1. The techniques used in this paper seem to rely heavily on the minimum ball coverage method by Mettu and Plaxton. 

2. The challenges for obtaining good update time and query time is not well discussed.

3. The theoretical analysis is not an easy read in a limited time. The intuition behind the analysis and algorithm before going into the details of lemmas and proofs should be given before the proofs. 

Limitations:
Since this is a theoretical paper, I don't think there is potential negative societal impact of this paper.

Rating:
5

Confidence:
2

";1
fY7dShbtmo;"REVIEW 
Summary:
Looking to tackle the lack of temporal granularity in existing world models, the paper proposes a multi-time scale linear Gaussian state space model (MTS3). The model uses an efficient closed-form inference scheme on multiple time scales for highly accurate long-horizon predictions and uncertainty estimates over longer horizons. The experiments focus on action conditional long horizon future predictions, showing that MTS3 outperforms recent methods on several system identification benchmarks, including the D4RL dataset, a simulated mobile robot, and real manipulators including data from heavy machinery excavators.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
-	Multi-Time Scale Predictions: Training scalable hierarchical world models that operate at multiple timescales is an open challenge in multiple fields including compressed video prediction, reinforcement learning, control theory, etc. The paper presents one way to interleave between different levels conditioned on a task descriptor. The low-level learns the dynamics conditioned on a particular task and the higher-level is trained to predict the next task. 
-	Efficient Inference Scheme via factorised formulation: Following Becker, et al, 2019, and Volpp, et al 2020, the paper proposes multi-scale inference via closed-form solutions using simplifying locally linear assumptions.
-	I like the way the two levels are connected: through the specification of the prior belief $p(l_k| B_{1:k-1}, \alpha_{1:k-1})$ that defines the $p(l_k)$ in the fast-time scale.
-	The use of a probabilistic formalism allows the model to handle uncertainty in predictions, particularly in prediction of changes to change, which is a common challenge across continual learning task settings. 
-	Robust Performance: The model has been shown to outperform recent methods on several system benchmarks for long horizon predictions as noted in the figure 3. 
-	Captures Complex Dynamics: The model can better capture the complex, non-linear dynamics of a system in a more efficient and robust way than models that learn on a single time scale (Fig. 5)


Weaknesses:
-	In the formulation, the slow time scale SSM is only updated every step, i.e., the slow time $H$ scale time step is given by $H \triangle t$. Therefore, the results are contingent on a design choice. I am cognizant that the this was evaluated for the range $0.2 – 0.5 $. However, how dramatically could results / predictions change if the wrong discretization step was chosen. Is there a way to systematically infer this through the system? 
-	The results have presented do not show how well the prediction settings vary across different tasks. 
-	The results haven’t shown how well large deviances in the dynamics would be encoded? Example some large spike. 
-	Higher level encoded space is based on the observation space. Therefore, the higher-level is inherently conditioned on the first level. So, how would the scaling up work for multiple levels? Would $o \to B$ change to $o \to z \to B$?


Limitations:
The authors have appropriately addressed the limitations of MTS3, specifically regarding the consideration of only two timescales and the model's exclusive evaluation for predictions. Typically, it is customary to validate methods in a controlled setting when assessing the performance of the world model. However, I appreciate the learning the policy from such formulations is tricky and requires further consideration. 

Rating:
6

Confidence:
4

REVIEW 
Summary:
The paper addresses learning predictive world models that operate at multiple (i.e. 2) time scales. At the slower time scale, the belief over the “task” (i.e. the high-level state) is updated at every H time steps by aggregating the influence of the low-level observations and actions received over that period. The transition dynamics for the fast time scale model resemble those for a standard state space model, with the exception that the they are conditioned upon the high-level state. Throughout, the authors utilise linear transition models in latent space, with Gaussian process and observation noise.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
* The paper addresses an important problem that is under-explored.
* For the most part the writing is quite clear.
* The experimental results appear strong in comparison to a number of baselines.

Weaknesses:
* It is unclear whether the experimental results support the authors’ claim that their model can more accurately capture system dynamics because of the two-time scale approach, or whether their method outperforms the baselines to due to other factors (such as the imputation-based training). An obvious baseline of updating both the low-level and high-level states at the same frequency (i.e. setting H = 1, and therefore removing the multi time scale aspect) is missing. Please see the questions.
* In the preliminaries, the authors imply that they are building upon the locally-linear state space model from [1], that learns linear dynamics that are *conditional upon the state*. However, the authors model the dynamics in latent space to be linear in the latent state, action, and task context (Equation 5), and this linear transition model is *state-independent* (Line 150, i.e. it is the same linear dynamics for all states). It is unclear how the approach can achieve such strong performance with a completely linear model in a fairly low-dimensional latent space. This is further confused by the fact that Section 3.2 is contradicted by Appendix A.3 which instead tells us that the dependence on the actions is in fact non-linear, and learnt by a multi-layer MLP. Please see the questions.
* I think the paper could be stronger if it presented a more general framework (e.g. arbitrary number of hierarchy levels, arbitrary non-linear dynamics (non-Gaussian)), and then presented their 2-timescale, linear Gaussian model as one example of that framework that is computationally simple. Presenting a more general framework would help to set the groundwork for future works that consider the problem of multi time scale world models.

*Minor comments*
- Covariance matrices Q and R do not appear to be defined.
- Line 31: “under non-stationary” words appear to be missing from this sentence

Limitations:
The limitations section in the appendix is appreciated. A discussion of any limitations associated with the assumption of state-independent linear Gaussian latent dynamics would be useful.

Rating:
6

Confidence:
3

REVIEW 
Summary:
The paper proposes a formalized multi-scale world model, which works at two timescales: a fast-timestep module that predicts individual timesteps, and a slower one that is only updated over fixed number of steps. The slower module defines a ""task"" that controls how the fast module functions, and the slower module reads abstract observations/actions over the time window. The paper derives Bayesian updates and ways to optimize the models. Experiments on three different datasets indicate that the proposed method is significantly better at predicting steps long into the future over baselines, and is also able to better predict the uncertainty.

Soundness:
3

Presentation:
2

Contribution:
2

Strengths:
- Formal derivation of the method, including the derivation of the uncertainty bounds.
- Comparison against large number of valid baselines in multiple different settings, and all results indicate the proposed method is better
- Ablation studies on which components of the proposed method are important
- Experiments on both simulators and robots

Weaknesses:
- Some weaknesses in the baselines:
	- Only the proposed method had ""two-layer"" approach timewise, while others methods were ""single-layer"". While the formal ""two-layer"" approach was the main selling point of the proposed method, one can also naively create similar effect with RNNs or transformers
		- E.g., see this paper for two-layer RNN in Section 2.1: Jaderberg, Max, Wojciech M. Czarnecki, Iain Dunning, Luke Marris, Guy Lever, Antonio Garcia Castaneda, Charles Beattie et al. ""Human-level performance in 3D multiplayer games with population-based reinforcement learning."" Science 364, no. 6443 (2019): 859-865.
		- Alternatively, a ""single-layer"" approach of the proposed method could also be included to better demonstrate the value of ""two-layer"" approach. However, if two-layer version outperforms single-layer approach, then two-layer results of the other baselines is even more warranted to ensure the benefit is not alone from two-layer approach.
	- Transformers used were rather small (4 layers, ~100 dimension), but many of the previous work has shown that larger transformers perform better (e.g., + 8 layers, 500 dimensions). While larger models are hard to use in robotics (not much compute / tight time constraints for control), testing the scalability of the different methods in terms of parameters to train would provide a better picture of the methods.
		- If larger transformer turns out to be better at world modelling without big impact in inference time, this method still has the benefit of capturing uncertainty, and can potentially be a better fit in planning.
- While paper does mention the potential applications of the world modelling approach, there are no experiments to demonstrate this usefulness. The results seem positive for the proposed method (better at modelling the world), but it is unclear how useful this is down the line. For example, how accurate do you have to be to perform good planning for control?
- No code available. The paper does detail the algorithm and setup used to great detail, but the code might contain details that researchers would need to replicate experiments. Small changes to the underlying libraries or how data is pre-processed may have big effects, or there might be parts in the code that were not reported in the paper. Any code, even if messy, is better than no code. 
	- Given the complexity of the algorithm, having at least a pseudocode to refer to is crucial for correct implementation in the future.
	- Example of how code-level implementation details matter: Engstrom, Logan, Andrew Ilyas, Shibani Santurkar, Dimitris Tsipras, Firdaus Janoos, Larry Rudolph, and Aleksander Madry. ""Implementation matters in deep rl: A case study on ppo and trpo."" In International conference on learning representations. 2019.

Limitations:
Authors acknowledge the limitations of the work, and propose good paths to extend the work. Authors also discuss the broader impact (no immediate societal impact from the work).


## Rebuttal acknowledgement

I have read authors' rebuttal which addressed my concerns, and raised my score from 4 to 7 and confidence from 2 to 3 (before discussion period closed).

Rating:
7

Confidence:
3

REVIEW 
Summary:
The authors introduce the Multi Time Scale State Space (MTS3) model in this work. The model uses closed-form equations derived using exact inference, spread across two time-scales, to produce long-horizon predictions and uncertainty estimates. They demonstrate the superiority/competitiveness of their inference approach across a number of offline datasets, both in terms of long-term deterministic predictions, and long-term uncertainty quantification.

Soundness:
4

Presentation:
3

Contribution:
3

Strengths:
- To the best of my knowledge, this closed-form multi time-scale inference approach using SSMs is novel, and has not be explored in previous works. The produced inference model is a principled (but non-trivial) integration of pre-existing components, and the results demonstrate the promise of such an approach
- Generally, the work is of high quality. The writing is clear, and the paper is well-organized. The related work section is brief, but appears to adequately address prior work relevant to the aims of this paper.

Weaknesses:
- In the Figure 3 ablation plots, MTS3 should remain as Red in (c) to improve clarity. It would also be much clearer if the ablations were displayed in their own figure. It also does not make much sense, beyond organization of the plots onto separate lines, why Figure 3a and 3b are separated, as they are displaying the same findings across different environments. The a/b grouping contains no semantic difference. I understand the need for conserving space, but the way the figures are grouped and color-coded, it is not clear at a first glance what the relationship between a, b, and c are.
- Spacing between Table 1 and the caption is too tight.
- Figure 1 should be raised so that the caption does not overlap with 3.1 (try to align with line 79 paragraph).

Limitations:
- The modeling assumptions constitute most of the limitations, but these are also what enable the convenient closed-form updates.
- The authors note that the their model is limited to two levels of temporal abstraction, and that for certain tasks (e.g. Maze2D) more hierarchies may help. The method allows for addition of more complex abstractions such as Feudal hierarchies.
- The authors are restricting their application to action conditional long-horizon future predictions. Although they note that future work can use these predictions for hierarchical control, they leave this for future work—it would have been nice to demonstrate this possibility with real control results.
- Their method relies on reconstruction loss, which may have limited direct application to image-based domains. However, as the authors note, non-reconstruction based losses can be integrated. Again, it would have been nice to see this integration of different losses directly. 
- Overall, while I believe the authors have extensively noted the major limitations, especially for listed limitations (ii) and (iii), it would have been nice to see results in this paper to demonstrate that these limitations can indeed be relieved with simple substitution/introduction of new components.

Rating:
7

Confidence:
3

";1
Cs74qIBfiq;"REVIEW 
Summary:
This paper introduces RoboShot, a method that improves the robustness of pretrained model embeddings in a fully zero-shot manner. The key idea is to leverage insights obtained from language models based on task descriptions. These insights are used to modify the embeddings, removing harmful components and enhancing useful ones, without the need for supervision. Technically, the method ensures invariance to spurious features by projecting pretrained model embeddings onto the subspace orthogonal to the subspace spanned by spurious feature descriptions. Experiments demonstrate that RoboShot improves multi-modal and language model zero-shot performance. 


Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. **Novel and useful setting:** The setting of improving the robustness of pretrained model embeddings with task description is novel. RoboShot offers a unique approach that preserves the out-of-the-box usability of pretrained models, which is a key advantage.

2. **Extensive experiments and analyses:** The authors demonstrated the efficacy of the proposed method and setting with extensive experiments and analyses, in terms of both datasets and settings. 

3. The paper is well-written. 

Weaknesses:
1. **Limitation of method:** The robustification relies on the insights provided by language models. However, if the language model does not identify the potential failure cases of the model, the method cannot remedy it. 

2. **Gender bias:** Some experiments are targeted at gender bias. It's better to discuss the scope of evaluation here, e.g., what genders are considered and what biases remain unresolved. 



Limitations:
Limitation needs to be elaborated on. 

Rating:
6

Confidence:
5

REVIEW 
Summary:
The paper presents an innovative approach to enhance zero-shot classification inference without the need for fine-tuning pre-trained models. The authors introduce a method that partitions input embeddings into three components: harmful, helpful, and benign. By leveraging task descriptions and querying language models with harmful and helpful prompts, they successfully extract harmful and helpful components. This leads to the removal of harmful components and a boost in the helpful ones, ultimately improving robustness in zero-shot classification. The proposed method demonstrates promise and contributes to the field of zero-shot classification without extensive model finetuning.

Soundness:
3

Presentation:
4

Contribution:
3

Strengths:
The paper is well-structured and easily understandable, with a strong and compelling motivation. As model sizes continue to increase, fine-tuning LLMs becomes increasingly expensive. This paper presents a compelling alternative by enhancing zero-shot performance while retaining the power of LLMs without the need for fine-tuning.  The innovative method of partitioning embeddings into three concepts and leveraging task descriptions and LLMs to strengthen or weaken them is intriguing and holds promise for embedding-based zero-shot text classification.

Weaknesses:
1. The proposed method is primarily applicable to embedding-based zero-shot classification approaches, while prompt-based methods like ChatGPT3.5/4 have gained popularity recently. Prompt-based methods allow humans to directly query language models based on downstream task knowledge, resulting in impressive performance. Although the proposed method is interesting for embedding-based zero-shot classification, its impact may be limited due to the current research trend.

2. Considering the above point, it would be beneficial if the authors compare their method to a more reasonable baseline, such as asking ChatGPT about predictions. This approach could be employed, for instance, by asking ChatGPT to identify if a given text contains gender bias. While ChatGPT's performance might not extend to image-based tasks, applying this baseline to text datasets would provide valuable insights. If ChatGPT achieves accurate predictions, it questions the necessity of an embedding-based zero-shot text classification method.

3. Although the method exhibits significant improvement in worst group performance (WG), it would strengthen the findings if the overall average performance also demonstrated improvement. It is worth noting that in some datasets, the average performance did not improve. This implies that the proposed method sacrifices performance for some classes to achieve better results in the WG. Ensuring a balance between overall average performance and WG improvement would bolster the method's effectiveness.

Overall, I am inclined to accept the paper if all weaknesses are properly addressed but I am also not strongly against rejecting the paper.

Limitations:
Please refer to weaknesses.

Rating:
6

Confidence:
4

REVIEW 
Summary:
This paper introduces a novel approach called ROBOSHOT that aims to enhance the robustness of pre-trained models for zero-shot classification tasks. The key idea is to leverage task-specific queries to prompt the large language model (LLM) to generate textual insights about the task, which can be classified as helpful or harmful concepts. These textual insights are then used to obtain ""insight representations"" based on the corresponding encoded embeddings, which are utilized to calibrate the vector of input representation to predict the class. The proposed method is evaluated through experiments on zero-shot image and text classification tasks. The paper also includes a theoretical analysis of the bound of the coefficient of targeted harmful concept post-removal. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
1. The motivation of this paper is clear. The problem of ""robustifying zero-shot models without labels, training, or manual specification"" is challenging and interesting. 
2. The paper proposes a simple method to calibrate the input embedding to make predictions.
3. The paper is generally easy to read and easy to follow. The settings of experiments in this paper are clear.
4. The theoretical proof of the coefficient bound of the targeted harmful concept is a plus.




Weaknesses:
1. The paper aims to robustify zero-shot models without labels, training, or manual specification. However, the proposed method still requires the use of manually-designed helpful/harmful queries (shown in Table 6&7) to query the LLM. 
2. The paper did not explore the impact of using different prompts to query textual insights. Given that the experiments were conducted on small pre-trained models such as BERT, which are less robust than LLMs, it is unclear whether the resulting different text insights would significantly affect the model's performance.
3. The paper employs LLMs (e.g., chatgpt/LLAMA) to generate textual insights for calibrating the input embedding of a small pre-trained model for prediction. It raises the question of why not directly prompt the LLM for zero-shot text classification if it already possesses sufficient knowledge about each class. The paper would benefit from including such a baseline comparison.
4. The paper did not include and compare some relevant works, such as [1] [2], which calibrate the logits of predictions for zero-shot/few-shot text classification. 
5. Table 1 indicates that the proposed method does not improve the ALIGN model on the Waterbirds dataset, but it does improve ALIGN's performance on CelebA, VLCS, and CXR14. The paper attributes this discrepancy to the harmful and helpful insight embeddings of Waterbirds being indistinguishable in the text embedding space of ALIGN. However, it is unclear how to determine when the ROBOSHOT method is applicable to different combinations of models and datasets. The paper could benefit from providing some metrics to predict the conditions under which ROBOSHOT is effective.

[1]Surface Form Competition: Why the Highest Probability Answer Isn’t Always Right

[2]Calibrate Before Use: Improving Few-Shot Performance of Language Models

Limitations:
please refer to weakness point 5

Rating:
4

Confidence:
4

REVIEW 
Summary:
The main objective of this research is to enhance the robustness of image/text classification by taking into account the relationships between labels. The authors utilize Large Language Model (LLM) to incorporate prior knowledge about these labels. They also propose methods to amplify useful features and eliminate harmful features induced from these labels.

In contrast, prior work require manual identification of biased features for debiasing, whereas this work leverages LLM to automatically suggest such features, thereby reducing the need for manual effort.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
- The paper is well written and can be easily understood.
- Using LLM to propose useful and spurious features is somewhat novel.

Weaknesses:
- This work aims to incorporate the knowledge of LLM into image classification. However, the method used, which is based on CLIP, does not seem to fully utilize the knowledge from LLM. The author's approach involves generating insight descriptions to establish a connection between LLM and CLIP's text encoder.  A more intuitive alternative would be to link CLIP's image encoder directly with LLM through an adapter, similar to the Flamingo (https://arxiv.org/abs/2204.14198) and LLaVA (https://arxiv.org/abs/2304.08485) methods. By doing so, the knowledge from LLM can be more comprehensively utilized, rather than solely the generated insight descriptions.
- The subspace-based debiasing methods have already been used under various contexts as also mentioned in the paper, which cannot be viewed as novel to some extent.

Limitations:
The limitations part is missing in the paper.

Rating:
4

Confidence:
3

";0
LJ4CYEagg3;"REVIEW 
Summary:
This work introduces two attention modules for video frame interpolation that achieve promising results in terms of quantitative and qualitative evaluation. The researchers conducted numerous experiments to provide a comprehensive evaluation of the proposed approach.

Soundness:
2

Presentation:
2

Contribution:
1

Strengths:
The proposed attention mechanism enhances the interpolation quality of intermediate frames.


Weaknesses:
(1) While this work does not present many theoretical contributions or fundamental insights, it is technically sound and offers improvements and modifications to existing methods. For example, the shift windows attention applied to spatial-temporal aggregation is a relatively simple and incremental approach that builds upon similar ideas explored extensively in previous research.

(2) Table 1/2 should specify the training setups of previous state-of-the-art video frame interpolation methods, as the proposed method uses six input frames. As far as I know, existing models are generally trained on Vimeo-Triplets, which only consist of 51K three-frame samples.

(3) Additionally, the specific contributions of SGuTA are unclear, as SCubA significantly outperforms it with fewer parameters and lower multi-adds. Finally, while many previous works have demonstrated arbitrary interpolation ability, this point does not appear to have been discussed in this work.

Limitations:
Please see the weakness and question parts.
No significant negative societal impact in this work.

Rating:
4

Confidence:
5

REVIEW 
Summary:
In video tasks, the computational complexity and memory requirements of Transformer is challenging. This paper employ two different Transformers in VFI task resulting in new state-of-the-art results. Furthermore, this paper conduct an analysis of existing embedding strategies, and put forth a novel half-overlapping embedding strategy. The author carried out the experimental analysis carefully.

Soundness:
2

Presentation:
3

Contribution:
3

Strengths:
1. I believe the author contributed efficient components to the multi-frame video transformer.
2. The author has open sourced the relevant code.
3. The resulting visuals look good. Overall, the paper has a good impression.

Weaknesses:
1. The author did not discuss and compare a series of works on single frame interpolation. CVPR20-SoftSplat, CVPR22-IFRNet, CVPR22-Many-to-many Splatting, ECCV22-RIFE, CVPR23-AMT. This greatly weakens the credibility of model evaluation. Overall, the authors cite very little recent relevant literature.
The model mainly compares the ""video frame interpolation transformer"", which is not as popular as SoftSplat or RIFE as far as I know, so I hope the author will add more comparison experiments to make the evaluation more solid.
2. The video submitted by the author has only one 720p scene, and there is no other method to compare it.
3. The author has discussed that this model currently does not support multi-frame interpolation.

Limitations:
Previous video transformers often performed mediocrely at 2k or 4k resolutions, or had a large memory overhead, and it is unclear how well this work in HD scenes.

Rating:
5

Confidence:
4

REVIEW 
Summary:
This paper aims to address the computational complexity and memory requirements of Transformers to enhance their suitability for the video frame interpolation (VFI) task. The authors propose two novel methods, SCuTA and SGubA, which are integrated into a multi-stage multi-scale framework. SCuTA leverages the correlation between spatial information and temporal sequences, while SGubA incorporates a 3D local self-attention mechanism. Through extensive experiments, the proposed methods demonstrate superior performance in terms of peak signal-to-noise ratio (PSNR) compared to other approaches, while also exhibiting a reduced parameter count.

Soundness:
3

Presentation:
2

Contribution:
3

Strengths:
1. Intuitive motivation: The paper provides a clear and intuitive motivation for modifying Transformers to better adapt to the VFI task, addressing the computational complexity and memory requirements.
2. Reasonable design: The introduction of SCuTA and SGubA, which leverage the correlation between spatial information and temporal sequences, is a sensible approach. Additionally, SGubA's utilization of a 3D local self-attention mechanism aligns well with the requirements of the VFI task.
3. Comprehensive experimentation: The paper presents a substantial number of experiments to support the proposed methods. The results consistently demonstrate that the proposed approaches achieve the highest PSNR while also reducing the parameter count, effectively addressing the computational and memory challenges of Transformers in the VFI task.

Weaknesses:
1. Lack of clarity in Figure 2: Figures 2d and 2f require further clarification. It is recommended that the authors explain G-MSA and SC-MSA before referring to these figures to enhance reader understanding.
2. Inference time comparison: The paper should provide information on the inference time for all the methods evaluated. This will provide a more comprehensive evaluation of the proposed approaches in terms of both performance and computational efficiency.

Limitations:
The authors do not address the limitations. Please refer to weaknesses.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper tackles video frame interpolation (VFI). It particularly aims to deploy a transformer architecture for VFI tasks. To address the computational complexity and memory requirements in transformers, it proposes two transformer networks, SGuTa and SCubA. While SGuTa uses the spatial (global) information of video frames to establish temporal correlation, SCubA focuses on local attention. Both methods exhibit linear computational complexities. The authors also introduce a half-overlapping embedding strategy to balance the trade-off between computational complexity and memory usage. Experimental comparisons are presented on several VFI benchmarks. 

Soundness:
2

Presentation:
2

Contribution:
2

Strengths:
* The work attempts to tackle an important trade-off (performance vs. computation) in using transformers for the VFI task
* The proposed transformers exhibit linear time complexity which makes them easily deployable
* The qualitative results look good
* The ablation studies are thorough and show the effect of the different design choices in the proposed networks
* Code is shared in the supplementary material for reproducibility 

Weaknesses:
* The motivation of the work is not convincingly justified

   There are several works [2,3] that used a transformer architecture for VFI. However, the authors introduce two new transformer networks without drawing any motivation from the progress that has been made in this line of research. It is also not very clear why the authors need to introduce two different networks in one paper. The introduction part of the paper should be rewritten by positioning the proposed framework in comparison with existing literature and hence filling in the big jump in L38-39.

* The technical novelty of the work is limited

   The proposed modules are heavily copied from other relevant works such as VideoSwin Transformer [1].
   
   It is also not clear why the proposed multi-head self-attention (MSA) is very different from other MSAs in previous literature. As far as I understand, the key difference is in reshaping the input tensor (HW X TD instead of HWT X D) in SGuTa. The remaining operations are the same as other MSAs. The local attention in ScubA is also similar to the one used in [1]. 

* The experimental comparisons are limited and unfair

   Experimental results are simply copied (quoted) from previous works. However, the experimental settings the authors used to conduct experiments are very different (6 adjacent frames, different patch sizes) from previous works. Hence, how can the authors convincingly justify that the performance gain is coming from the proposed method and not the different experiment settings?

   Intuitively speaking, using more adjacent frames (6 versus 2 or 4) should provide more context during training, hence, it benefits the proposed method. A fair comparison would follow the common experiment protocol used in previous VFI works. 

   The benefit of half-overlapping in Table 3 is not clear. The performance gain is really negligible (0.04dB) compared to the computation overhead (significant increase in memory usage and FLOPs compared to non-overlapping baselines).  how did the authors come to the conclusion in Eq. 11? There has to be a more convincing explanation (proof) than simple observation, otherwise, the claim in L62-64 should be toned down. 

   The video presented in the supplementary file only shows one video result for the proposed method. The authors should provide a side-by-side video comparison of their method and competing approaches on different test videos.

* The writing of the paper needs improvement 
  
   The methodology part is very confusing with too many coined terms. It would be better to rewrite this part in a clearer manner.
   
   It is also not a recommended practice to introduce new acronyms in the title of the paper. 

References

[1]. Ze Liu, Jia Ning, Yue Cao, Yixuan Wei, Zheng Zhang, Stephen Lin, and Han Hu. Video swin transformer, CVPR 2022

[2]. Zhihao Shi, Xiangyu Xu, Xiaohong Liu, Jun Chen, and Ming-Hsuan Yang. Video frame interpolation transformer, CVPR 2022

[3]. Liying Lu, Ruizheng Wu, Huaijia Lin, Jiangbo Lu, and Jiaya Jia. Video frame interpolation with transformer, CVPR 2022

Limitations:
The authors do not discuss the limitations of their work. 

Rating:
4

Confidence:
5

REVIEW 
Summary:
This paper proposed two types of Transformer for video frame interpolation: Spatially-Guided Temporal Attention (SGuTA) and
Shifted-Cube Attention (SCubA). SGuTa merges the temporal dimension and the embedding dimension during the self-attention process to explore the inherent correlations between the spatial and temporal dimensions. On the other hand, SCubA is focused on reducing the computational complexity and adapts Video Swin Transformer for frame interpolation. Both approaches show significant performance improvements compared with the recent VFI models.

Soundness:
4

Presentation:
3

Contribution:
2

Strengths:
1. Good performance. Both of the proposed Transformer models---SGuTA and SCubA---outperform the existing works by a notable margin. As the authors have mentioned, given that the performance of the existing works for VFI is almost plateaued, it seems that the proposed architectures are quite effective.

2. Writing is clear and easy to understand. Also, the figures are neat, and the experiments logically match the paper's claims. The motivations of the proposed approaches are sensible.

Weaknesses:
1. Main focus of the paper is a bit confusing.

This paper has two main contributions - SGuTA and SCubA - which are separate contributions, and how to combine these two ideas are not introduced, which makes the paper look incomplete. I would like to suggest three options to remedy this issue:

a) If both SGuTA and SCubA can be combined into a single framework, then this will be the best approach - each module is shown to be effective and can show synergies when combined together.

b) If SGuTA and SCubA cannot be combined, then we should at least discuss when to use which method. It seems like SCubA is quite consistently better than SGuTQ in terms of PSNR/SSIM and also computational complexity - then what is the point of proposing SGuTA? Should we just use SCubA all the time?

c) If neither a) nor b) option is available, then I think we should consider separating SGuTA and SCubA into two distinct papers.

2. Use of more input frames and incapability of generating arbitrary intermediate time step.

The proposed method uses more number of input frames (thus, more information) to predict the middle frame. While methods like QVI [30] also uses more input frames, for truly fair comparison with the other works, I think the authors should also report the performance with triplet-based evaluations.
Also, I have one simple question: it is written in the paper that ""septuplets"" are used - does this mean that the from the frames 1~7, the input frames are [1, 2, 3, 5, 6, 7] and the GT is 4-th frame? Or, do you use [1, 3, 5, 7]-th frames as the input?

Also, most the existing flow-based works can generate arbitrary time-step intermediate frames, while the proposed SGuTA and SCubA cannot. In my opinion, at least a short discussion regarding this issue is needed.

Limitations:
The limitations of the current work and its potential negative societal impact is not adequately discussed.
The authors are encouraged to think about these issues more seriously and write them, even if the potential societal impact seems to be minimal.

Rating:
4

Confidence:
5

";0
VMAgvbBBts;"REVIEW 
Summary:
This paper investigates the task of data pre-selection by learning a better representation from the joint feature space of both vision and text in an unsupervised manner. The paper focuses on training text prompts to extract joint features with enhanced representation, specifically with the BLIP-2 parameters kept fixed. The aim is to achieve a diverse cluster structure that encompasses the entire dataset.

Soundness:
2

Presentation:
3

Contribution:
2

Strengths:
**[New task]** This paper tackles data pre-selection for labelling without accessing the information of downstream tasks, which is quite new to the community.

**[Well-illustrated figures]** The figures shown in this paper are clear enough for better understanding.

**[Good presentation]** The paper is well-written and easy to follow.


Weaknesses:
**[Unconvincing illustration]** In Figure 1, BLIP-2 is pre-trained with prompt and image together. This explains why using only image features yields poor performance. Consequently, the evidence presented does not convincingly demonstrate the superiority of multimodal features.

**[Need in-depth analysis]** (i) It is unclear why the self-trained model can be used for sample selection. (ii) The motivation of medoid selection is not given. It would be nice to see the rationale. 

**[Missed ablation studies]** The ablation study for two hyperparameters are not given.

**[Disorganized reference format]** Please reformat the references as per some published papers.


Limitations:
Please refer to the weaknesses part.

Rating:
5

Confidence:
5

REVIEW 
Summary:
The paper addresses data pre-selection (akin to active learning) problem using the highly successful vision-language models (VLMs) of late. In relation to existing approaches, the proposed approach has a few advantages, e.g., no need to have a small initial set of labeled data, no need to have multiple rounds of selection, labeling and retraining, once selected the data can be used for multiple future, unknown downstream tasks etc. The authors start with a BLIP-2 model and an unlabeled set of data $D$. The BLIP-2 model is extended to have learnable context/prompt and a few MLPs, namely instance-level and cluster-level heads. The instance-level head is employed to produce a contrastive training between two views of each unlabeled instance. Cluster-level head, on the other hand, first assigns cluster memberships to the instances and then helps to train the model with a cluster-level contrastive loss. After training for a few epochs using these two losses, cluster-level MLP along with the learned contexts/prompts are used to get the cluster assignment of unlabeled data (which can come from different downstream dataset). Finally, the medoids from each cluster form the representative selections for active learning. Experiments performed on linear probing and domain generalization show the efficacy of the proposed approach over the state-of-the-arts on benchmark datasets.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
1. The use of VLMs for unsupervised active learning is appreciable. VLMs. Now-a-days, are known for good zero-shot transfer. The already well-learned representations can and did help the active learning cause.
2. The use of learnable contexts/prompts has been shown to be useful for few-shot transfer. The use of these for active learning is interesting.
3. The use of cluster-level contrastive loss cleverly avoids the use of any initial labeled set that is required in traditional cluster level losses in getting the initial clustering (akin to group contrastive loss in semisupervised literature e.g., [a]).
4. Experimental analysis and ablations show the efficacy of the proposed approach compared to sota approaches and the importance of different components of the approach as well.

[a] Singh et al., Semi-supervised action recognition with temporal contrastive learning, CVPR 2021.

Weaknesses:
1. One important ablation that could be useful is running the approach without the learnable prompts/contexts. What I mean is updating only $g_I$ and $g_C$ but not employing $V$ in Algorithm 1. This will help gauge the importance of the contexts/prompts vis-à-vis the instance and cluster level MLPs. Does ‘Initi_Prompt’ row in Table 2 do this?
2. Line 260+: This is more of a clarification query. When the datasets are described, I don’t see any mention of which dataset is used for pre selection. I am assuming these 7 datasets are downstream task datasets. The question is coming from Table 1. While in Table 2, it seems that the first column tells what is the dataset on which the prompts are learnt, in table 1, it is not clear. Is it that the learning is done on the same datasets on which the linear probing performances are shown for Table 1?
3. Line274+: I am not getting what is meant by 'with the learned prompts' in the baseline using USL. Does it mean everything else here is same as BLIP-2, but in addition a few prompts are learned also?

Limitations:
The limitations are described well in the paper. At the same time, the authors tried to address the limitation in the supplementary material.

Rating:
6

Confidence:
5

REVIEW 
Summary:
This paper presents an unsupervised approach for data preselection, which aims to select instances for labeling from an unlabeled dataset in a single pass. The authors leverage the text features in multimodal models, specifically BLIP2, to enhance the representation for data preselection. They argue that a well-designed joint feature space of vision and text can yield improved results. To achieve this, they train text prompts to extract joint features with enhanced representation, ensuring a diverse cluster structure that covers the entire dataset. The authors employ two loss functions, namely instance-level contrastive and cluster-level contrastive, to train the learnable text prompts. These loss functions encourage the adapted multimodal model's joint representation space to be more diverse and well-separated, suitable for clustering. Experimental results on seven different datasets, along with a comparison against three baselines, demonstrate the effectiveness of the proposed approach.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
I believe the strength of the paper is as follows:

- The integration of vision and language models for data pre-selection holds promise due to the added benefits of leveraging text modality. 

- This paper introduces a novel approach that promotes diverse and clustered representations, addressing limitations in the current state-of-the-art BLIP2 model through the lens of prompt learning. 

- The proposed method is lightweight and more efficient in terms of training costs. The paper makes a significant contribution, evident in its clear presentation and compelling results.

Weaknesses:
While the paper's results are strong, there are two notable aspects that could be addressed:

1. Missing CLIP baseline: Comparing the proposed approach with a CLIP baseline would provide a fundamental point of reference. Since the authors employ contrastive loss functions and prompt learning, which can be applied to any vision and language model, including CLIP as a baseline would enhance the comparative analysis.

2. Lack of integrability: A limitation of the prompt tuning method is the lack of interpretability. While prompt tuning improves model performance, it does not provide a clear explanation of why and how the model works in the combined language model embedding space. Addressing this limitation would enhance the understanding and justification of the proposed approach.

3. Performance/training-time trade-off: The paper utilizes the best-performing model of BLIPV2, which has over 7 billion parameters. However, it does not explore the performance and training-time trade-off with different BLIPV2 models. Investigating the performance of the proposed approach on BLIPV2 models with fewer parameters would provide insights into its scalability and suitability for models of varying sizes.

Limitations:
There are not any limitations. 

Rating:
5

Confidence:
4

REVIEW 
Summary:
The paper studies the problem of data pre-selection, which aims to select instances for labeling from an unlabeled dataset to enhance performance for downstream tasks with a limited annotation budget. The authors suggest that combining visual and textual features in a joint space can result in a better representation for data pre-selection. They introduce UP-DP, an unsupervised prompt learning approach that adapts vision-language models(specifically BLIP-2) for data pre-selection. The proposed approach outperforms the state-of-the-art on benchmark datasets and exhibits generalizability.

Soundness:
1

Presentation:
3

Contribution:
2

Strengths:
1. The paper introduces a novel approach for data pre-selection that incorporates unsupervised prompt learning in the vision-language model. This approach effectively exploits the multimodal features and enhances the discrimination among classes.
2. The authors provide a clear motivation for the task of data pre-selection and highlight the unique challenges it poses compared to semi-supervised learning and active learning.
3. The paper compares with the state-of-the-art on multiple benchmark datasets, demonstrating its effectiveness and superior performance. Also, it highlights the generalizability of the learned prompts across different datasets.

Weaknesses:
1. The authors claim that the purpose of the data pre-selection is to optimize performance for undefined diverse downstream tasks; however, they only conduct experiments on the image classification task. This limited scope is insufficient to demonstrate the effectiveness of UP-DP for various downstream tasks such as detection or segmentation. The paper would benefit from conducting more experiments on other tasks. And on top of this, it would be interesting to investigate the generalizability of the learned prompts across different tasks.
2. In Table 1, the ""Zero-Shot BLIP-2"" setting is unreasonable. It lacks a justifiable rationale to use the prompt for the CLIP model to evaluate the zero-shot performance of the BLIP-2 model. If the authors intend to use this baseline, they should train the learnable prompt for BLIP-2 from scratch.
3. In Table 1, the author does not include a baseline that utilizes only image features extracted from BLIP-2. By comparing with this baseline, the authors can demonstrate the efficiency of the proposed method.
4. In Table 1, the authors compare ""Random"", ""USL-I/M"", but it is suggested to compare with more approaches in the field. Including additional comparisons would strengthen the paper's evaluation and provide a better context for understanding the performance of the proposed approach.
5. The ablation studies of the proposed method are limited. It would be beneficial to conduct more comprehensive experiments to analyze the performance of UP-DP under different settings. For instance,  the authors should provide a detailed analysis of the impact of the instance-level and cluster-level contrastive loss.
6. All the experiments are carried out using the BLIP-2 model during the data pre-selection stage. However, it remains unclear whether the proposed method is exclusively effective for this particular model. It is important to consider other visual-language models, such as CLIP, to establish the efficacy of the approach.
7. The paper contains several grammar issues. For example, on page 5, line 191, it states ""presents an efficient pre-training,"" and on page 6, line 237, it states ""Thus we can from a positive pair."" These sentences require revision for improved clarity and grammatical correctness.

Limitations:
The authors need to analysis the limitations of their work.

Rating:
5

Confidence:
2

REVIEW 
Summary:
This paper presents a novel method to perform data preselection for the task of image classification. Data preselection refers to the task of finding the images for annotating labels and then used for training. The paper builds around the powerful visual-language model BLIP, and proposes learnable prompts as inputs to BLIP to help perform unsupervised clustering for data preselection. The paper then presents results on seven image classification benchmarks, showing the superb performance of the proposed method.

Soundness:
3

Presentation:
3

Contribution:
3

Strengths:
+ The presented method is a novel application of visual language model to data preselection. 
+ I find the proposed learnable prompting in conjunction with unsupervised clustering novel, and as suggested by experiment effective. 
+ The presented method is effective in data preselection, as demonstrated by the comparison against baseline methods.


Weaknesses:
- The decision to annotate 200 images per benchmark (LINE 265 - 273) seems arbitrary. Why this number? It would be great if the number can be varied and then plot the model performance accordingly to understand the effect of annotated data set size on model performance.
- USL-M, which shares the same multimodal features from BLIP-2 as the proposed UP-DP method, isn't really outperforming the baseline USL-L on EuroSAT (Table 1). Such a result contradicts the claimed effectiveness. What is the explanation?
- No results on linear probe on BLIP-2 using Random Sampling. This is needed in order to showcase the effectiveness of the proposed method. 

Limitations:
Yes

Rating:
5

Confidence:
3

";1
