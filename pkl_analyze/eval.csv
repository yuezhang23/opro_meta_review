raw_prompt,raw_answer,raw_prompt_second_round,raw_answer_second_round,parsed_answer,true_answer,accuracy,f1_score
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposes the Retentive Network (RetNet) as a foundation architecture for large language models. RetNet has a multi-scale retention mechanism with three computation paradigms: parallel, recurrent, and chunkwise recurrent. 
The retention mechanism starts with a recurrent modeling formulation and derives a parallel formulation. It maps input vectors to state vectors recurrently and implements a linear transform to encode sequence information. Then, it makes the projection content-aware by using learnable matrices. The retention layer is defined using these matrices and a complex position embedding, combining causal masking and exponential decay along relative distance. 
It achieves low-cost inference, efficient long-sequence modeling, comparable performance to Transformers, and parallel training. Experimental results show its superiority in language modeling, inference cost, and training throughput.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The RetNet also shows competitive performance in language modeling and knowledge-intensive tasks compared to other Transformer variants and has the potential to replace Transformers for large language models.
2. Achieves significantly better inference efficiency in terms of memory, speed, and latency.

Weaknesses:
1. The paper presents the scaling curves of RetNet and Transformer with model sizes ranging from 1.3B to 6.7B, concluding that RetNet is favorable in terms of size scaling and starts to outperform Transformer when the model size is larger than 2B. However, it does not provide a detailed explanation for this trend. Understanding the underlying reasons for this performance difference with increasing model size could provide more insights into the effectiveness of RetNet and its potential advantages over Transformer.
2. The use of $\gamma$ in the RetNet may appear somewhat heuristic. The paper assigns different $\gamma$ for each head in the multi-scale retention (MSR) module and keeps them fixed among different layers. While this approach is used to achieve certain effects, such as enhancing the non-linearity of the retention layers and improving the model's performance, the specific rationale for choosing these values and the potential impact on the model's behavior could be further explained.

Limitations:
none

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a linear attention model called RetNet for language modeling, which has a linear training complexity and constant inference complexity.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. RetNet has both linear time complexity and constant inference memory complexity. 
2. RetNet has a chunk recurrent form which can be beneficial for speculative decoding.

Weaknesses:
1. The authors introduce a new term called ""Retention,"" but this is essentially the same as Linear Attention without the denominator, which has already been proposed in [1].
2. Lack of comparison with the baselines on open source pretraining data. All the training experiments are conducted on in-house data mixtures, which harms the reproducibility.
3. The paper doesn't compare RetNet with other linear attention model (such as GLA, RWKV, Mamba) on downstream tasks with standard metrics instead of perplexity. Table 2 only include RetNet and Transformer. The efficiency measurment of RetNet+ is absent.
4. The evaluation on MMLU/Qasper is using perplexity but not the widely-used accuracy/F1 metric. The perplexity results don't necessarily mean that the model can make correct choices for the samples in MMLU, and has less guidance for the model's downstream performance.
5. Missing citations: The authors should also cite [1] for the normalization after retention, and discuss the details of the triton implementation of RetNet and its difference from the implementation in the Flash Linear Attention [2] library.

[1] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7025–7041, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics.

[2] Yang, Songlin and Zhang, Yu. FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism. https://github.com/sustcsonglin/flash-linear-attention

Limitations:
No, the authors should have a limitation section to point out the strong assumptions of their approximation of self-attention and relative position embedding.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents Retentive Network (RetNet), a family of efficient models that incorporate exponential decay within a linear attention-like structure. RetNet shares similarities with state-space models and linearized attention, enabling both training parallelism and O(1) inference cost. Additionally, RetNet supports chunk-wise parallel computation for efficient long-sequence training. Experimental results demonstrate RetNet achieves performance comparable to Transformers and outperforms other efficient variants on language modeling and vision tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The structure of RetNet is easy to understand and follow
- RetNet exhibits promising training and inference efficiency, and is able to scale up to 6B.
- Comprehensive evaluation on both language and vision tasks, highlighting its generalizability.

Weaknesses:
- Some experiments could be improved
- Some claims may be misleading
- RetNet's performance lags behind Transformers at smaller model scales, suggesting it might be more demanding in terms of capacity and compute resources for optimal performance. This trade-off should be carefully considered and analyzed.

Limitations:
I didn't see serious problems.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposes the Retentive Network (RetNet) as a foundation architecture for large language models. RetNet has a multi-scale retention mechanism with three computation paradigms: parallel, recurrent, and chunkwise recurrent. 
The retention mechanism starts with a recurrent modeling formulation and derives a parallel formulation. It maps input vectors to state vectors recurrently and implements a linear transform to encode sequence information. Then, it makes the projection content-aware by using learnable matrices. The retention layer is defined using these matrices and a complex position embedding, combining causal masking and exponential decay along relative distance. 
It achieves low-cost inference, efficient long-sequence modeling, comparable performance to Transformers, and parallel training. Experimental results show its superiority in language modeling, inference cost, and training throughput.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The RetNet also shows competitive performance in language modeling and knowledge-intensive tasks compared to other Transformer variants and has the potential to replace Transformers for large language models.
2. Achieves significantly better inference efficiency in terms of memory, speed, and latency.

Weaknesses:
1. The paper presents the scaling curves of RetNet and Transformer with model sizes ranging from 1.3B to 6.7B, concluding that RetNet is favorable in terms of size scaling and starts to outperform Transformer when the model size is larger than 2B. However, it does not provide a detailed explanation for this trend. Understanding the underlying reasons for this performance difference with increasing model size could provide more insights into the effectiveness of RetNet and its potential advantages over Transformer.
2. The use of $\gamma$ in the RetNet may appear somewhat heuristic. The paper assigns different $\gamma$ for each head in the multi-scale retention (MSR) module and keeps them fixed among different layers. While this approach is used to achieve certain effects, such as enhancing the non-linearity of the retention layers and improving the model's performance, the specific rationale for choosing these values and the potential impact on the model's behavior could be further explained.

Limitations:
none

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a linear attention model called RetNet for language modeling, which has a linear training complexity and constant inference complexity.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. RetNet has both linear time complexity and constant inference memory complexity. 
2. RetNet has a chunk recurrent form which can be beneficial for speculative decoding.

Weaknesses:
1. The authors introduce a new term called ""Retention,"" but this is essentially the same as Linear Attention without the denominator, which has already been proposed in [1].
2. Lack of comparison with the baselines on open source pretraining data. All the training experiments are conducted on in-house data mixtures, which harms the reproducibility.
3. The paper doesn't compare RetNet with other linear attention model (such as GLA, RWKV, Mamba) on downstream tasks with standard metrics instead of perplexity. Table 2 only include RetNet and Transformer. The efficiency measurment of RetNet+ is absent.
4. The evaluation on MMLU/Qasper is using perplexity but not the widely-used accuracy/F1 metric. The perplexity results don't necessarily mean that the model can make correct choices for the samples in MMLU, and has less guidance for the model's downstream performance.
5. Missing citations: The authors should also cite [1] for the normalization after retention, and discuss the details of the triton implementation of RetNet and its difference from the implementation in the Flash Linear Attention [2] library.

[1] Zhen Qin, Xiaodong Han, Weixuan Sun, Dongxu Li, Lingpeng Kong, Nick Barnes, and Yiran Zhong. The devil in linear transformer. In Proceedings of the 2022 Conference on Empirical Methods in Natural Language Processing, pages 7025–7041, Abu Dhabi, United Arab Emirates, Dec. 2022. Association for Computational Linguistics.

[2] Yang, Songlin and Zhang, Yu. FLA: A Triton-Based Library for Hardware-Efficient Implementations of Linear Attention Mechanism. https://github.com/sustcsonglin/flash-linear-attention

Limitations:
No, the authors should have a limitation section to point out the strong assumptions of their approximation of self-attention and relative position embedding.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents Retentive Network (RetNet), a family of efficient models that incorporate exponential decay within a linear attention-like structure. RetNet shares similarities with state-space models and linearized attention, enabling both training parallelism and O(1) inference cost. Additionally, RetNet supports chunk-wise parallel computation for efficient long-sequence training. Experimental results demonstrate RetNet achieves performance comparable to Transformers and outperforms other efficient variants on language modeling and vision tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The structure of RetNet is easy to understand and follow
- RetNet exhibits promising training and inference efficiency, and is able to scale up to 6B.
- Comprehensive evaluation on both language and vision tasks, highlighting its generalizability.

Weaknesses:
- Some experiments could be improved
- Some claims may be misleading
- RetNet's performance lags behind Transformers at smaller model scales, suggesting it might be more demanding in terms of capacity and compute resources for optimal performance. This trade-off should be carefully considered and analyzed.

Limitations:
I didn't see serious problems.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper introduces a novel augmentation method based on diffusion models. A latent diffusion model conditioned on a text prompt generates hard negatives, by adjusting the noise level. The hard negatives can be used as challenging augmentations. The authors demonstrate the effectiveness of their approach on long-tail and few-shot settings.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Well-written paper with clear contributions and presentation.
- Extensive experiments and evaluation.
- Interesting and useful idea.
- Code included in the supplementary.

Weaknesses:
I am generally happy with the paper, experiments, and presentation. A weakness seems to be the selection of the noise ratio r.  The authors propose an algorithm for this. However, I am concerned how sensitive it is for different datasets or classification settings. This might affect performance in other settings or in real-world scenarios. If this is true, it might degrade the overall method's usefulness.

Limitations:
The authors have added a section for limitations and a section for broader impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes to employ text-to-image latent diffusion models to augment images through a controlled modification such that the resultant class is different from the source class. Such augmented images are referred to as hard negative images. Building upon SDEdit style image modification, the paper controls the extent of modification by adaptively determining the appropriate noise-scale for each image separately. The benefits of this type of augmentation have been demonstrated on few-shot and long-tailed imagenet classification tasks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The paper is very well written presenting the core idea of generating hard-negative images by modifying an image with a caption of another class. This idea is simple, intuitive and interesting. 
- Furthermore, the algorithm to determine the optimal noise-level for each image adaptively is not only simple and intuitive but also effective in eliminating dependence on hyperparameters. I feel that a connection can be made to the recent work [1] on phase-transition in diffusion models since this algorithm is attempting to find the diffusion-time when phase-transition occurs. 
- The evaluation is comprehensive considering a variety of diffusion-augmentation baselines as well as traditional augmentations.
- The paper illustrates the effectiveness of the adaptive search procedure through separate experiments with DINO-v2 and visualisations.
- In many cases, synthetic data generation with a diffusion model may be replaced by a simpler retrieval baseline [2]. However, the goal of this work is to use a diffusion model to search for and generate hard negatives, which is an interesting deviation from some of the previous synthetic data augmentation approaches. 

[1] Sclocchi, Antonio, Alessandro Favero, and Matthieu Wyart. ""A phase transition in diffusion models reveals the hierarchical nature of data."" arXiv preprint arXiv:2402.16991 (2024).

Weaknesses:
- From the various results in the paper, it seems that the Text2Image, GeNIe, and GeNIe-Ada achieve comparable performance with respect to each other on average. This seems to suggest that the majority of the gains can be attributed to the increased number of _distinct_ examples --- as compared to regular augmentations which simply apply different transformations to the same image --- for each class rather than the hard-negatives in GeNIe/GeNIe-Ada. 
- Additionally, it seems that beyond some threshold, any value of $r$ that changes the source-image to the target image yields comparable performance indicating that it may be sufficient to generate an augmentation that is similar to source-image and it need not specifically be a _hard-negative_.  It may be useful to consider some other applications where images lying in the boundary of the classifier may be informative: for example, see recent work on generating outliers [1] for OOD detection. 
- GeNIe-Ada algorithm is compute-intensive as compared to a simple Text2Image augmentation since it requires generating several augmentations for each source image before selecting one optimal augmentation that lies on the decision boundary. Given how close the text2image and genie-ada performances are in some cases, it may be possible that we could generate more augmentations using text2image in the same compute budget and improve over GeNIe. 
- (minor) GeNIe is applicable to the fine-tuning stage rather than the pretraining stage.

[1] Du, X., Sun, Y., Zhu, J. and Li, Y. Dream the impossible: Outlier imagination with diffusion models. NeurIPS 2024.

Limitations:
Yes, limitations are addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces GeNIe, a data augmentation method for training vision models using synthetic images. GeNIe generates images by combining a source category image with a target category text prompt, selecting those that feature source characteristics but belong to the target category as negative samples. Experimental results show that GeNIe improves performance in both few-shot and long-tail distribution settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
* The proposed GeNIe improves the performance in few-shot and long-tail distribution settings.
* The paper provides extensive experiments to support the claims,  including the selection of noise levels.
* The paper is well-written and easy to follow.

Weaknesses:
The key idea of GeNIe is to use image editing to combine features from two categories. Here are several questions:

* Regarding controllable image augmentation
  * Line 9 mentions that GeNIe ""retains low-level and background features from the source image."" How does GeNIe control which features are retained or changed?
  * To combine features from different categories, how about adding the attribute from the target category to the prompt? For example, a ""[dog] with [wings]"".  This method does not require carefully selection of denoise steps. 
  * Other image editing methods, such as those in [1] and [2], efficiently control image changes using prompts or user instructions.  For example, they can transform a car into a motorcycle in Figure 2, while keeping the background unchanged for more challenging negative samples. What advantages does GeNIe offer over these methods?



* GeNIe generates images ""using images from all other classes as the source image"" (line 227). Will all (source image, target prompt) pairs lead to effective image generation? Which types of pairs contribute the most to the final accuracy?

     [1] Prompt-to-Prompt Image Editing with Cross-Attention Control

     [2] InstructPix2Pix: Learning to Follow Image Editing Instructions, CVPR 2023

Limitations:
The paper has discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the idea is to generate data for data augmentation by utilizing a pre-trained diffusion model. The method employs different text prompts and an adjusted noise scheduler to generate hard negative samples for the source distribution. ""GeNIe"" creates new augmentations using diffusion by leveraging source images and contradictory target prompts. ""GeNIe-Ada"" adjusts noise levels on a per-sample basis, using the classifier as the condition boundary to select the right threshold.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The method offers infinite possibilities to separate the source from the target.
- The idea is simple, original, and convincing.
- The ablation studies and experiments demonstrate strong performance.

Weaknesses:
- The method is slow, particularly GeNIe-Ada, as it requires generating an image through multiple forward passes of a diffusion model and using a classifier to select the appropriate threshold $r$.

- The number of steps required to retain low-level features is crucial for optimizing the method's performance.

- The method relies on access to a foundational text-to-image model trained on billions of images.

Limitations:
/

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper introduces a novel augmentation method based on diffusion models. A latent diffusion model conditioned on a text prompt generates hard negatives, by adjusting the noise level. The hard negatives can be used as challenging augmentations. The authors demonstrate the effectiveness of their approach on long-tail and few-shot settings.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Well-written paper with clear contributions and presentation.
- Extensive experiments and evaluation.
- Interesting and useful idea.
- Code included in the supplementary.

Weaknesses:
I am generally happy with the paper, experiments, and presentation. A weakness seems to be the selection of the noise ratio r.  The authors propose an algorithm for this. However, I am concerned how sensitive it is for different datasets or classification settings. This might affect performance in other settings or in real-world scenarios. If this is true, it might degrade the overall method's usefulness.

Limitations:
The authors have added a section for limitations and a section for broader impact.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes to employ text-to-image latent diffusion models to augment images through a controlled modification such that the resultant class is different from the source class. Such augmented images are referred to as hard negative images. Building upon SDEdit style image modification, the paper controls the extent of modification by adaptively determining the appropriate noise-scale for each image separately. The benefits of this type of augmentation have been demonstrated on few-shot and long-tailed imagenet classification tasks.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The paper is very well written presenting the core idea of generating hard-negative images by modifying an image with a caption of another class. This idea is simple, intuitive and interesting. 
- Furthermore, the algorithm to determine the optimal noise-level for each image adaptively is not only simple and intuitive but also effective in eliminating dependence on hyperparameters. I feel that a connection can be made to the recent work [1] on phase-transition in diffusion models since this algorithm is attempting to find the diffusion-time when phase-transition occurs. 
- The evaluation is comprehensive considering a variety of diffusion-augmentation baselines as well as traditional augmentations.
- The paper illustrates the effectiveness of the adaptive search procedure through separate experiments with DINO-v2 and visualisations.
- In many cases, synthetic data generation with a diffusion model may be replaced by a simpler retrieval baseline [2]. However, the goal of this work is to use a diffusion model to search for and generate hard negatives, which is an interesting deviation from some of the previous synthetic data augmentation approaches. 

[1] Sclocchi, Antonio, Alessandro Favero, and Matthieu Wyart. ""A phase transition in diffusion models reveals the hierarchical nature of data."" arXiv preprint arXiv:2402.16991 (2024).

Weaknesses:
- From the various results in the paper, it seems that the Text2Image, GeNIe, and GeNIe-Ada achieve comparable performance with respect to each other on average. This seems to suggest that the majority of the gains can be attributed to the increased number of _distinct_ examples --- as compared to regular augmentations which simply apply different transformations to the same image --- for each class rather than the hard-negatives in GeNIe/GeNIe-Ada. 
- Additionally, it seems that beyond some threshold, any value of $r$ that changes the source-image to the target image yields comparable performance indicating that it may be sufficient to generate an augmentation that is similar to source-image and it need not specifically be a _hard-negative_.  It may be useful to consider some other applications where images lying in the boundary of the classifier may be informative: for example, see recent work on generating outliers [1] for OOD detection. 
- GeNIe-Ada algorithm is compute-intensive as compared to a simple Text2Image augmentation since it requires generating several augmentations for each source image before selecting one optimal augmentation that lies on the decision boundary. Given how close the text2image and genie-ada performances are in some cases, it may be possible that we could generate more augmentations using text2image in the same compute budget and improve over GeNIe. 
- (minor) GeNIe is applicable to the fine-tuning stage rather than the pretraining stage.

[1] Du, X., Sun, Y., Zhu, J. and Li, Y. Dream the impossible: Outlier imagination with diffusion models. NeurIPS 2024.

Limitations:
Yes, limitations are addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces GeNIe, a data augmentation method for training vision models using synthetic images. GeNIe generates images by combining a source category image with a target category text prompt, selecting those that feature source characteristics but belong to the target category as negative samples. Experimental results show that GeNIe improves performance in both few-shot and long-tail distribution settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
* The proposed GeNIe improves the performance in few-shot and long-tail distribution settings.
* The paper provides extensive experiments to support the claims,  including the selection of noise levels.
* The paper is well-written and easy to follow.

Weaknesses:
The key idea of GeNIe is to use image editing to combine features from two categories. Here are several questions:

* Regarding controllable image augmentation
  * Line 9 mentions that GeNIe ""retains low-level and background features from the source image."" How does GeNIe control which features are retained or changed?
  * To combine features from different categories, how about adding the attribute from the target category to the prompt? For example, a ""[dog] with [wings]"".  This method does not require carefully selection of denoise steps. 
  * Other image editing methods, such as those in [1] and [2], efficiently control image changes using prompts or user instructions.  For example, they can transform a car into a motorcycle in Figure 2, while keeping the background unchanged for more challenging negative samples. What advantages does GeNIe offer over these methods?



* GeNIe generates images ""using images from all other classes as the source image"" (line 227). Will all (source image, target prompt) pairs lead to effective image generation? Which types of pairs contribute the most to the final accuracy?

     [1] Prompt-to-Prompt Image Editing with Cross-Attention Control

     [2] InstructPix2Pix: Learning to Follow Image Editing Instructions, CVPR 2023

Limitations:
The paper has discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the idea is to generate data for data augmentation by utilizing a pre-trained diffusion model. The method employs different text prompts and an adjusted noise scheduler to generate hard negative samples for the source distribution. ""GeNIe"" creates new augmentations using diffusion by leveraging source images and contradictory target prompts. ""GeNIe-Ada"" adjusts noise levels on a per-sample basis, using the classifier as the condition boundary to select the right threshold.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The method offers infinite possibilities to separate the source from the target.
- The idea is simple, original, and convincing.
- The ablation studies and experiments demonstrate strong performance.

Weaknesses:
- The method is slow, particularly GeNIe-Ada, as it requires generating an image through multiple forward passes of a diffusion model and using a classifier to select the appropriate threshold $r$.

- The number of steps required to retain low-level features is crucial for optimizing the method's performance.

- The method relies on access to a foundational text-to-image model trained on billions of images.

Limitations:
/

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,No,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposed GSAAL to simultaneously address three changeling problems in outlier detection: inlier assumption (IA), curse of dimensionality (CD), and multiple views (MV).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper has a good flow.
The paper proposed the first outlier detection method that explicitly addresses IA, CD, and MV simultaneously. 
The paper has strong theoretical and empirical evidence to show the advancement of the proposed method. 
The experimental design is solid and the numerous visual examples help to facilitate understanding.
The paper has good reproducibility with open codes.

Weaknesses:
some (but few) places to improve.

Limitations:
The authors have analyzed the limitations sufficiently.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents this generalization of GAAL Generative Subspace Adversarial Active Learning (GSAAL) for outlier detection to address the limitation of the previous work such as multi-view and the curse of dimensionality, where the theoretical convergence, the scalability of the algorithm are discussed. Experiments on real dataset and synthetic tabular dataset are carried out to establish the validity of the approaches.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The manuscript presents a method called generative subspace adversarial active learning for outlier detection in multiple views. The proposed method called GSAAL provides the proof of convergence, the computation complexity and aims to address the curse of dimensionality. The outlier detection in high dimensional space indeed is an important and challenging solution. Thus, the proposed method can be a good solution to address this difficult problem.

The manuscript has compared the performance of GSAAL with other outlier detection approaches with detailed visual illustration and AUC. The experiments show advantages of the proposed solution over other competing methods. The experiments seems to be detailed.

Weaknesses:
The novelty of the work appears to be small. Theoretically, the derivation of theorem 1 is very similar to GAN derivation. 

In this case, the paper needs to compare their solution both theoretically and experimentally with the related work for outlier detection using GAN such as [1] https://arxiv.org/pdf/1906.11632 such as AnoGAN, BioGAN and EGBAD. 
[2] https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-022-00943-7
If we compare the main equation (2) in the manuscript with the formulation in reference [1] with conditional GAN and BioGAN, it seems the main difference are the proposed method used multiple detectors and accumulated the performance, which should not be considered a large distinction.

Due to the lack of the comparison with generative adversarial network based approaches such as AnoGAN and EGBAD, the potential improvement of the purposed method against the state-of-the-art approaches is not clear. The novelty of the paper does not stand on the safe ground. The theoretical derivation is also similar to GAN derivation.

Limitations:
Limited innovation and lack of critical comparison with important reference are the main issues of the current manuscript.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The main contribution of this paper is to improve existing work on Generative Adversarial Active Learning (GAAL) by using multiple discriminators for multiple views to detect outliers in tabular data. The training mechanism is similar to existing works. The paper also introduces a theoretical analysis on Multiple Views (MV). As claimed by the authors, GAAL addressed the problems of Inlier Assumption (IA) and Curse of Dimensionality (CD), but missed Multiple Views (MV), which is the main focus of this paper. The experimental results compare the proposed method to GAAL and some other classical methods such as OCSVM and KNN, ....

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper introduces an interesting view about MV and proposes a new method to address this MV problem together with theoretical analysis.

Weaknesses:
* The empirical results are not strong (or at least unclear in the way the authors presented in the main paper); most of the experiments are on synthetic datasets.
* The results on the real dataset do not seem to show significant improvements compared to existing work (or at least it is hard to observe this when reading the paper). Perhaps the authors could improve the writing and highlight the results better. It is unclear to me why the experiments on the real dataset were put in the Appendix, as it is an important result.
* The paper claims at the beginning that it not only improves the MV problem but also the IA and CD problems, but this is hard to see with the current writing of the paper. Could the authors highlight the experiments in the paper to prove that claim?

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposed GSAAL to simultaneously address three changeling problems in outlier detection: inlier assumption (IA), curse of dimensionality (CD), and multiple views (MV).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper has a good flow.
The paper proposed the first outlier detection method that explicitly addresses IA, CD, and MV simultaneously. 
The paper has strong theoretical and empirical evidence to show the advancement of the proposed method. 
The experimental design is solid and the numerous visual examples help to facilitate understanding.
The paper has good reproducibility with open codes.

Weaknesses:
some (but few) places to improve.

Limitations:
The authors have analyzed the limitations sufficiently.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents this generalization of GAAL Generative Subspace Adversarial Active Learning (GSAAL) for outlier detection to address the limitation of the previous work such as multi-view and the curse of dimensionality, where the theoretical convergence, the scalability of the algorithm are discussed. Experiments on real dataset and synthetic tabular dataset are carried out to establish the validity of the approaches.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The manuscript presents a method called generative subspace adversarial active learning for outlier detection in multiple views. The proposed method called GSAAL provides the proof of convergence, the computation complexity and aims to address the curse of dimensionality. The outlier detection in high dimensional space indeed is an important and challenging solution. Thus, the proposed method can be a good solution to address this difficult problem.

The manuscript has compared the performance of GSAAL with other outlier detection approaches with detailed visual illustration and AUC. The experiments show advantages of the proposed solution over other competing methods. The experiments seems to be detailed.

Weaknesses:
The novelty of the work appears to be small. Theoretically, the derivation of theorem 1 is very similar to GAN derivation. 

In this case, the paper needs to compare their solution both theoretically and experimentally with the related work for outlier detection using GAN such as [1] https://arxiv.org/pdf/1906.11632 such as AnoGAN, BioGAN and EGBAD. 
[2] https://asp-eurasipjournals.springeropen.com/articles/10.1186/s13634-022-00943-7
If we compare the main equation (2) in the manuscript with the formulation in reference [1] with conditional GAN and BioGAN, it seems the main difference are the proposed method used multiple detectors and accumulated the performance, which should not be considered a large distinction.

Due to the lack of the comparison with generative adversarial network based approaches such as AnoGAN and EGBAD, the potential improvement of the purposed method against the state-of-the-art approaches is not clear. The novelty of the paper does not stand on the safe ground. The theoretical derivation is also similar to GAN derivation.

Limitations:
Limited innovation and lack of critical comparison with important reference are the main issues of the current manuscript.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The main contribution of this paper is to improve existing work on Generative Adversarial Active Learning (GAAL) by using multiple discriminators for multiple views to detect outliers in tabular data. The training mechanism is similar to existing works. The paper also introduces a theoretical analysis on Multiple Views (MV). As claimed by the authors, GAAL addressed the problems of Inlier Assumption (IA) and Curse of Dimensionality (CD), but missed Multiple Views (MV), which is the main focus of this paper. The experimental results compare the proposed method to GAAL and some other classical methods such as OCSVM and KNN, ....

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper introduces an interesting view about MV and proposes a new method to address this MV problem together with theoretical analysis.

Weaknesses:
* The empirical results are not strong (or at least unclear in the way the authors presented in the main paper); most of the experiments are on synthetic datasets.
* The results on the real dataset do not seem to show significant improvements compared to existing work (or at least it is hard to observe this when reading the paper). Perhaps the authors could improve the writing and highlight the results better. It is unclear to me why the experiments on the real dataset were put in the Appendix, as it is an important result.
* The paper claims at the beginning that it not only improves the MV problem but also the IA and CD problems, but this is hard to see with the current writing of the paper. Could the authors highlight the experiments in the paper to prove that claim?

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
In this work, the authors conceptualize the evaluation process as a decision tree, where each node represents an evaluation action, and each path from the root to a leaf node represents a trajectory of evaluation reasoning. The authors demonstrate that within a limited search space, there exist better decision-making behaviors that facilitate the model in making reasonable and accurate judgments.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The idea of branching LLM evaluation is interesting and novel.

Weaknesses:
1. The authors missed a lot of key related works, including close-ended benchmarks such as MMLU, MMLU-pro, MixEval, GSM8k, GSM1k, etc; open-ended benchmarks such as Arena-Hard, AlpacaEval, WildBench, Chatbot Arena, etc.
2. I think the writing needs improvement. Now it's not easy for a reader to get what you are focusing on. If you are doing evaluation, then try to use some pipeline figures and comprehensive captions to describe the core idea. Besides, all captions in this paper is misleading, not telling the reader about what is happening in the table or figure; also, there lacks some key sections such as conclusion.
3. How to measure the quality of the proposed evaluation? I think just evaluating 5-6 models is far from enough. Beyond that, how is the model rankings related with Chatbot Arena or some other popular benchmarks such as MMLU?

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a novel approach to efficiently evaluate LLMs using branching preference learning. The authors conceptualize the evaluation process as a decision tree, where each path represents an evaluation reasoning trajectory. They introduce a tree-based data sampling method and preference learning based on the DPO algorithm to improve evaluation capabilities. The method is tested in three settings: in-distribution, out-of-distribution, and transfer evaluation. The authors claim their model significantly reduces dependency on labeled data and demonstrates strong performance across different evaluation settings while reducing inference costs by 90% compared to searching the entire evaluation tree.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper's novel approach of framing LLM evaluation as a decision tree problem is a significant strength. This allows for a more nuanced and flexible evaluation process that can adapt to different scenarios and criteria. The use of branching preference learning enables the model to prioritize critical evaluation criteria.
- The authors test their model in multiple settings (in-distribution, out-of-distribution, and transfer evaluation), providing a thorough assessment of its performance. Applaud to that.

Weaknesses:
- The biggest concern I have is that in-distribution performance is not better than other baselines it compares to. This begs the question of where the improvement gain is from. If in-distribution evaluation performance is mediocre but out-of-distribution does better, then doesn't the most gain come from a better dataset? 
- Another concern is the unnaturalness of using evaluation criteria as individual nodes. How to ensure the coverage of those criteria across different nodes. Are they overlapping each other or completely different? The paper is quite vague on this.
- Why is each criteria subtree only a binary tree? If using a tree structure, it seem like it can be easily extend to multiple nodes rather than just 2 at each layer.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
They present an approach to improving LM evaluation by having models first generate an evaluation criteria, then a scoring guideline, and then finally a final judgement. They then develop a procedure for collecting training data corresponding to these three steps by applying branching/pruning approach (sample multiple criteria, from each sample multiple guidelines, etc...). They then use the generated data to train a DPO and SFT model. They find that their method outperforms baseline evaluation approaches according to correlation with human judgement on dialogue evaluation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The problem of improving LM evaluation is important
* The idea of enabling language models to hierarchically sample evaluations (e.g. fist criteria, then guideline, then judgement) is very neat, and similarly the idea of applying a tree-based sampling procedure to automatically generate data is quite cool.
* I think they do fairly thorough experiments and compare to quite a few baselines.

Weaknesses:
* The paper is honestly pretty hard to follow. There's a lot of moving parts and it's not explained in an easy to digest way.
* The specific method presented seems a little bit ad-hoc, and could be justified better in the paper (e.g. why use criteria, then guideline, then judgement, why not some other sequence of steps?).
* Looking at Figure 1, it doesn't seem that their method improves all that much over the baseline

Nits:
* The related work seems pretty sparse. There's lots of work on improving LM evaluation in math reasoning settings that isn't discussed.
* Figure 4, the text is really small and hard to read.

Limitations:
They do a good job of discussing the limitations. I would also note that it is unclear how effective this is when applied to more challenging tasks like mathematical reasoning (e.g. MATH benchmark) as a limitation.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates how to improve the quality of automated evaluation through fine-tuning (SFT and DPO). The main algorithm proposed by the paper is to construct an search tree which consists of node of (criterion, scoring guide, and judgment). This tree is later pruned and modified and the different paths serve as fine-tuning data for SFT and DPO.

My current rating is tentative. If the authors can kindly clarify the details of the paper, I'm happy to raise the score.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is very clear and easy to read.
2. The investigation is very thorough. Experiment is comprehensive (the in-distribution, out-of-distribution evaluation setup is great).
3. The main claim of the paper is substantiated (I.e., improving efficiency through fine-tuning).

Weaknesses:
I don't think this paper has substantial weaknesses.

1. There are some imperfections of text -- mostly just need to be clarified. Missing notation definitions, etc. 
2. The performance improve over Auto-J on AGR is minor (55.13 -> 57.18). OOD evaluation, Zephyr-7B AGR is 56.75 and GPT-4 is 62.28 (which is close, but not quite close). CNS however is beating GPT-4. This would be very helpful for me to understand a bit more about what CNS is, and whether beating GPT-4 on this metric is meaningful or not (see Q6).

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this work, the authors propose a tree-based data sampling method to conceptualize the evaluation process as a decision tree, where each node represents an evaluation action, and each path from the root to a leaf node represents a trajectory of evaluation reasoning. The proposed method involves generating supervised data and preference pairs derived from the evaluation tree for SFT and DPO training. This approach aims to reduce the dependency on labeled data and improve the performance of the evaluation model across in-distribution, out-of-distribution, and transfer evaluation settings. Experimental results demonstrate that the proposed model can enhance evaluation efficiency and performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method reduces the dependency on human labeled data by generating supervised data and preference pairs from the evaluation tree.
2. The paper is well-written.

Weaknesses:
1. Potential Biases --- The initial multi-branch training data is generated using only GPT4, which could introduce bias to the training data. Moreover, the branch ensemble method could also introduce bias to the training data. If the training data is biased or unrepresentative, the model's evaluations may also be biased. The authors should consider labeling a small annotation set to validate the branch ensemble approach.

Limitations:
Yes, limitation discussed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
In this work, the authors conceptualize the evaluation process as a decision tree, where each node represents an evaluation action, and each path from the root to a leaf node represents a trajectory of evaluation reasoning. The authors demonstrate that within a limited search space, there exist better decision-making behaviors that facilitate the model in making reasonable and accurate judgments.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The idea of branching LLM evaluation is interesting and novel.

Weaknesses:
1. The authors missed a lot of key related works, including close-ended benchmarks such as MMLU, MMLU-pro, MixEval, GSM8k, GSM1k, etc; open-ended benchmarks such as Arena-Hard, AlpacaEval, WildBench, Chatbot Arena, etc.
2. I think the writing needs improvement. Now it's not easy for a reader to get what you are focusing on. If you are doing evaluation, then try to use some pipeline figures and comprehensive captions to describe the core idea. Besides, all captions in this paper is misleading, not telling the reader about what is happening in the table or figure; also, there lacks some key sections such as conclusion.
3. How to measure the quality of the proposed evaluation? I think just evaluating 5-6 models is far from enough. Beyond that, how is the model rankings related with Chatbot Arena or some other popular benchmarks such as MMLU?

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a novel approach to efficiently evaluate LLMs using branching preference learning. The authors conceptualize the evaluation process as a decision tree, where each path represents an evaluation reasoning trajectory. They introduce a tree-based data sampling method and preference learning based on the DPO algorithm to improve evaluation capabilities. The method is tested in three settings: in-distribution, out-of-distribution, and transfer evaluation. The authors claim their model significantly reduces dependency on labeled data and demonstrates strong performance across different evaluation settings while reducing inference costs by 90% compared to searching the entire evaluation tree.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper's novel approach of framing LLM evaluation as a decision tree problem is a significant strength. This allows for a more nuanced and flexible evaluation process that can adapt to different scenarios and criteria. The use of branching preference learning enables the model to prioritize critical evaluation criteria.
- The authors test their model in multiple settings (in-distribution, out-of-distribution, and transfer evaluation), providing a thorough assessment of its performance. Applaud to that.

Weaknesses:
- The biggest concern I have is that in-distribution performance is not better than other baselines it compares to. This begs the question of where the improvement gain is from. If in-distribution evaluation performance is mediocre but out-of-distribution does better, then doesn't the most gain come from a better dataset? 
- Another concern is the unnaturalness of using evaluation criteria as individual nodes. How to ensure the coverage of those criteria across different nodes. Are they overlapping each other or completely different? The paper is quite vague on this.
- Why is each criteria subtree only a binary tree? If using a tree structure, it seem like it can be easily extend to multiple nodes rather than just 2 at each layer.

Limitations:
Yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
They present an approach to improving LM evaluation by having models first generate an evaluation criteria, then a scoring guideline, and then finally a final judgement. They then develop a procedure for collecting training data corresponding to these three steps by applying branching/pruning approach (sample multiple criteria, from each sample multiple guidelines, etc...). They then use the generated data to train a DPO and SFT model. They find that their method outperforms baseline evaluation approaches according to correlation with human judgement on dialogue evaluation.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The problem of improving LM evaluation is important
* The idea of enabling language models to hierarchically sample evaluations (e.g. fist criteria, then guideline, then judgement) is very neat, and similarly the idea of applying a tree-based sampling procedure to automatically generate data is quite cool.
* I think they do fairly thorough experiments and compare to quite a few baselines.

Weaknesses:
* The paper is honestly pretty hard to follow. There's a lot of moving parts and it's not explained in an easy to digest way.
* The specific method presented seems a little bit ad-hoc, and could be justified better in the paper (e.g. why use criteria, then guideline, then judgement, why not some other sequence of steps?).
* Looking at Figure 1, it doesn't seem that their method improves all that much over the baseline

Nits:
* The related work seems pretty sparse. There's lots of work on improving LM evaluation in math reasoning settings that isn't discussed.
* Figure 4, the text is really small and hard to read.

Limitations:
They do a good job of discussing the limitations. I would also note that it is unclear how effective this is when applied to more challenging tasks like mathematical reasoning (e.g. MATH benchmark) as a limitation.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper investigates how to improve the quality of automated evaluation through fine-tuning (SFT and DPO). The main algorithm proposed by the paper is to construct an search tree which consists of node of (criterion, scoring guide, and judgment). This tree is later pruned and modified and the different paths serve as fine-tuning data for SFT and DPO.

My current rating is tentative. If the authors can kindly clarify the details of the paper, I'm happy to raise the score.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is very clear and easy to read.
2. The investigation is very thorough. Experiment is comprehensive (the in-distribution, out-of-distribution evaluation setup is great).
3. The main claim of the paper is substantiated (I.e., improving efficiency through fine-tuning).

Weaknesses:
I don't think this paper has substantial weaknesses.

1. There are some imperfections of text -- mostly just need to be clarified. Missing notation definitions, etc. 
2. The performance improve over Auto-J on AGR is minor (55.13 -> 57.18). OOD evaluation, Zephyr-7B AGR is 56.75 and GPT-4 is 62.28 (which is close, but not quite close). CNS however is beating GPT-4. This would be very helpful for me to understand a bit more about what CNS is, and whether beating GPT-4 on this metric is meaningful or not (see Q6).

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this work, the authors propose a tree-based data sampling method to conceptualize the evaluation process as a decision tree, where each node represents an evaluation action, and each path from the root to a leaf node represents a trajectory of evaluation reasoning. The proposed method involves generating supervised data and preference pairs derived from the evaluation tree for SFT and DPO training. This approach aims to reduce the dependency on labeled data and improve the performance of the evaluation model across in-distribution, out-of-distribution, and transfer evaluation settings. Experimental results demonstrate that the proposed model can enhance evaluation efficiency and performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method reduces the dependency on human labeled data by generating supervised data and preference pairs from the evaluation tree.
2. The paper is well-written.

Weaknesses:
1. Potential Biases --- The initial multi-branch training data is generated using only GPT4, which could introduce bias to the training data. Moreover, the branch ensemble method could also introduce bias to the training data. If the training data is biased or unrepresentative, the model's evaluations may also be biased. The authors should consider labeling a small annotation set to validate the branch ensemble approach.

Limitations:
Yes, limitation discussed in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper addresses the issue of the flow matching method's lack of dependence on the data population. It proposes incorporating the initial population density into the vector field through amortization—using a Graph Neural Network (GNN) to embed the populations and adding this embedding to the input of the vector field network. The paper argues that this dependence would better model the data due to sample interactions, demonstrating improved generalization on unseen initial distributions. The method's application is showcased in perturbation drug screening.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
### Originality
The problem setting of adding population dependence to flow matching is novel. The model framework of adding input to the vector field network using a GNN as a population encoder is also novel.

### Clarity

The paper is clearly written, with rigorous mathematical notations. The related work and introductions are especially well-written.

### Quality

The writing is good, and the experiment involves many baselines.

### Significance

The proposed method excels at generalizing to unseen populations, which is a significant improvement over existing methods, particularly when the conditions for generation are complex. The application on drug screening addresses a significant scientific problem and holds promise for personalized healthcare.

Weaknesses:
1. The paper could explain more about the meta-learning aspect of this method.
2. It could include explanations and/or ablation studies on the role of meta-learning and the GNN, especially in the synthetic experiment.
3. More detail is needed on what properties of the Wasserstein manifold of probabilities are used in the model. It is unclear how the model proposed in section 3.2 depends on the properties of the Wasserstein manifold described in section 3.1.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed an extension of the Conditional Generative Modeling via Flow Matching (CGFM) framework. Taken inspiration from the theory of Wasserstein Gradient Flow, this new framework, Meta Flow Matching, proposed to learn the push-forward mapping of multiple measures in the same measures-space. This is motivated by the realistic problem of modeling single-cell perturbation data where we want to see the response of populations of cells of patients when receiving different treatments. A novelty of Meta Flow Matching is that by combining amortized optimization and CGFM, the trained MFM velocity network can model newly observed populations _without_ knowing their labels/conditions. Two empirical benchmarks were performed to showcase the effectiveness of MFM compared to the Flow Matching (FM) and CGFM.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method proposed is novel enough, and the problem is well-motivated. I also find the idea of integration of GNN to model the conditional variable quite neat. The method is based on the well-studied theory of Wasserstein gradient flow on measure spaces and amortized optimization framework.

- The empirical benchmark, especially on real biological data, seems to showcase the strength of MFM.

- Overall the paper is quite well written and easy to follow.

Weaknesses:
- The first part of the methodology section seems to be phrased as a new methodological contribution, but if I'm not mistaken this is just more or less restating the already established theory of W2 gradient flow and continuity equation (eq 14). I think the authors should put Section 3.1 into the background section (2nd Section).

- There is a lack of discussion on whether the 3 crucial assumptions (line 145-152) are satisfied in a realistic biological setting. For example, in theory, Assumption (iii) on the unique existence of the Cauchy problem stands when the velocity field satisfies some regularity conditions -- I'm not sure this can be extended to a parameterized neural network that takes input from another (graph) neural network as an embedding function, which is hardly Lipschitz smooth in most of the case.

- Algorithm boxes at the end of section 3 is highly welcome. Or if the authors cannot allocate the space, I highly recommend putting two (one for training and one for sampling) into the Appendix. It is quite hard to follow how the velocity is trained in reality. For example, what function $f_t(x_0, x_1)$ did the authors take for this work? Is it still linear interpolation as vanilla flow matching? Or does it involve adding some form of stochasticity as in stochastic interpolant or VP-SDE as in diffusion model? Is the coupling $(x_0, x_1)$ sampled to match randomly, or they are sampled to some form of alignment as in the multisample flow matching paper (Pooladian et al. 2023)?

- This might not be the original purpose of this work, but I would love to see how MFM perform on conditional image generation task. One can pick a simple small dataset such as CIFAR10 that already includes class labels, or better yet ImageNet dataset. The performance in this takse will be much more convincing than the synthetic experiment, where I would argue would target the same type of task.

Limitations:
See weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper discussed the novel problem setup of generative modeling of the dynamics of probability distributions. The paper proposed Meta Flow Matching (MFM), an extension of the flow matching framework for implicitly learning the vector fields on the Wasserstein manifold of probability distributions. The paper demonstrated better transferability of the proposed framework on unseen distributions on both synthetic datasets and real-world drug-screen datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The problem setup of learning a flow matching model for mappings between distributions (i.e., a probability path on the Wasserstein manifold), to the best of my knowledge, is novel and has not been explored in previous work.
- The idea of using distribution-specific embeddings (the population embeddings) is well explained and motivated in the paper.
- The proposed method demonstrates better transferability on both synthetic and real-world datasets compared to other baselines.

Weaknesses:
- The proposed method seems to be a special case of a conditionally trained flow matching model where the conditions are continuous learnable embeddings. Such an idea has already been applied in various diffusion or flow matching models including image generation (conditioned on text embedding in the latent space), protein co-design [1] (conditioned on sequence, generate protein structure, or vice versa), and peptide design [2] (conditioned on receptor proteins, generate peptides).

- The idea of population embedding in the paper is similar to task embedding, which has been well-explored in the meta learning (e.g. [3]). Although the authors claimed their framework to be *meta* flow matching, related work in meta learning seems to lack.


[1] Campbell, Andrew, et al. ""Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design."" arXiv preprint arXiv:2402.04997 (2024).

[2] Li, Jiahan, et al. ""Full-Atom Peptide Design based on Multi-modal Flow Matching."" arXiv preprint arXiv:2406.00735 (2024).

[3] Achille, Alessandro, et al. ""Task2vec: Task embedding for meta-learning."" Proceedings of the IEEE/CVF international conference on computer vision. 2019.

Limitations:
The authors have adequately and properly discussed the limitations and potential societal impact of the paper in the Appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Meta Flow Matching (MFM), a flow matching framework modeling interacting samples evolving over time by integrating vector fields on the Wasserstein manifold. The authors leverage a Graph Neural Network to embed populations of samples and thus generalize the method over different initial distributions. The authors demonstrate the method on individual treatment responses predictions on a large multi-patient single-cell drug screen dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novelty: The method uniquely considers population interactions, unlike previous flow matching methods that model samples individually.

Generalization: The authors extended conditioning on latent variables to conditioning on population index in section 3.2. The proposition in section 3.2 demonstrates that conditional flow matching can fit well within the MFM framework. The experiments show that MFM can generalize to unseen data, outperforming other methods in this regard.

Weaknesses:
In Table 1 of the synthetic experiment, the authors compared the performance of FM, CGFM and MFM of k=0,1,10,50. MFM doesn't seem to beat existing methods on the metrics and from the visualizations, it's hard to tell MFM is actually doing better than FM. Also, for the various values of k, some explanations on how performance correlates with values of k and why might be necessary for readers to understand this table.

In both experiments, the authors only compared FM, CGFM, and in Table 2 also ICNN. Probably more methods, like diffusion, should also be taken into comparison. Also, in experiment 2, only W1, W2 and MMD are computed as metrics. While these are useful when modeling distributions, more metrics, especially those specific to this application may be applied.

Limitations:
The authors have not addressed limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper addresses the issue of the flow matching method's lack of dependence on the data population. It proposes incorporating the initial population density into the vector field through amortization—using a Graph Neural Network (GNN) to embed the populations and adding this embedding to the input of the vector field network. The paper argues that this dependence would better model the data due to sample interactions, demonstrating improved generalization on unseen initial distributions. The method's application is showcased in perturbation drug screening.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
### Originality
The problem setting of adding population dependence to flow matching is novel. The model framework of adding input to the vector field network using a GNN as a population encoder is also novel.

### Clarity

The paper is clearly written, with rigorous mathematical notations. The related work and introductions are especially well-written.

### Quality

The writing is good, and the experiment involves many baselines.

### Significance

The proposed method excels at generalizing to unseen populations, which is a significant improvement over existing methods, particularly when the conditions for generation are complex. The application on drug screening addresses a significant scientific problem and holds promise for personalized healthcare.

Weaknesses:
1. The paper could explain more about the meta-learning aspect of this method.
2. It could include explanations and/or ablation studies on the role of meta-learning and the GNN, especially in the synthetic experiment.
3. More detail is needed on what properties of the Wasserstein manifold of probabilities are used in the model. It is unclear how the model proposed in section 3.2 depends on the properties of the Wasserstein manifold described in section 3.1.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposed an extension of the Conditional Generative Modeling via Flow Matching (CGFM) framework. Taken inspiration from the theory of Wasserstein Gradient Flow, this new framework, Meta Flow Matching, proposed to learn the push-forward mapping of multiple measures in the same measures-space. This is motivated by the realistic problem of modeling single-cell perturbation data where we want to see the response of populations of cells of patients when receiving different treatments. A novelty of Meta Flow Matching is that by combining amortized optimization and CGFM, the trained MFM velocity network can model newly observed populations _without_ knowing their labels/conditions. Two empirical benchmarks were performed to showcase the effectiveness of MFM compared to the Flow Matching (FM) and CGFM.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The method proposed is novel enough, and the problem is well-motivated. I also find the idea of integration of GNN to model the conditional variable quite neat. The method is based on the well-studied theory of Wasserstein gradient flow on measure spaces and amortized optimization framework.

- The empirical benchmark, especially on real biological data, seems to showcase the strength of MFM.

- Overall the paper is quite well written and easy to follow.

Weaknesses:
- The first part of the methodology section seems to be phrased as a new methodological contribution, but if I'm not mistaken this is just more or less restating the already established theory of W2 gradient flow and continuity equation (eq 14). I think the authors should put Section 3.1 into the background section (2nd Section).

- There is a lack of discussion on whether the 3 crucial assumptions (line 145-152) are satisfied in a realistic biological setting. For example, in theory, Assumption (iii) on the unique existence of the Cauchy problem stands when the velocity field satisfies some regularity conditions -- I'm not sure this can be extended to a parameterized neural network that takes input from another (graph) neural network as an embedding function, which is hardly Lipschitz smooth in most of the case.

- Algorithm boxes at the end of section 3 is highly welcome. Or if the authors cannot allocate the space, I highly recommend putting two (one for training and one for sampling) into the Appendix. It is quite hard to follow how the velocity is trained in reality. For example, what function $f_t(x_0, x_1)$ did the authors take for this work? Is it still linear interpolation as vanilla flow matching? Or does it involve adding some form of stochasticity as in stochastic interpolant or VP-SDE as in diffusion model? Is the coupling $(x_0, x_1)$ sampled to match randomly, or they are sampled to some form of alignment as in the multisample flow matching paper (Pooladian et al. 2023)?

- This might not be the original purpose of this work, but I would love to see how MFM perform on conditional image generation task. One can pick a simple small dataset such as CIFAR10 that already includes class labels, or better yet ImageNet dataset. The performance in this takse will be much more convincing than the synthetic experiment, where I would argue would target the same type of task.

Limitations:
See weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper discussed the novel problem setup of generative modeling of the dynamics of probability distributions. The paper proposed Meta Flow Matching (MFM), an extension of the flow matching framework for implicitly learning the vector fields on the Wasserstein manifold of probability distributions. The paper demonstrated better transferability of the proposed framework on unseen distributions on both synthetic datasets and real-world drug-screen datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The problem setup of learning a flow matching model for mappings between distributions (i.e., a probability path on the Wasserstein manifold), to the best of my knowledge, is novel and has not been explored in previous work.
- The idea of using distribution-specific embeddings (the population embeddings) is well explained and motivated in the paper.
- The proposed method demonstrates better transferability on both synthetic and real-world datasets compared to other baselines.

Weaknesses:
- The proposed method seems to be a special case of a conditionally trained flow matching model where the conditions are continuous learnable embeddings. Such an idea has already been applied in various diffusion or flow matching models including image generation (conditioned on text embedding in the latent space), protein co-design [1] (conditioned on sequence, generate protein structure, or vice versa), and peptide design [2] (conditioned on receptor proteins, generate peptides).

- The idea of population embedding in the paper is similar to task embedding, which has been well-explored in the meta learning (e.g. [3]). Although the authors claimed their framework to be *meta* flow matching, related work in meta learning seems to lack.


[1] Campbell, Andrew, et al. ""Generative flows on discrete state-spaces: Enabling multimodal flows with applications to protein co-design."" arXiv preprint arXiv:2402.04997 (2024).

[2] Li, Jiahan, et al. ""Full-Atom Peptide Design based on Multi-modal Flow Matching."" arXiv preprint arXiv:2406.00735 (2024).

[3] Achille, Alessandro, et al. ""Task2vec: Task embedding for meta-learning."" Proceedings of the IEEE/CVF international conference on computer vision. 2019.

Limitations:
The authors have adequately and properly discussed the limitations and potential societal impact of the paper in the Appendix.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces Meta Flow Matching (MFM), a flow matching framework modeling interacting samples evolving over time by integrating vector fields on the Wasserstein manifold. The authors leverage a Graph Neural Network to embed populations of samples and thus generalize the method over different initial distributions. The authors demonstrate the method on individual treatment responses predictions on a large multi-patient single-cell drug screen dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Novelty: The method uniquely considers population interactions, unlike previous flow matching methods that model samples individually.

Generalization: The authors extended conditioning on latent variables to conditioning on population index in section 3.2. The proposition in section 3.2 demonstrates that conditional flow matching can fit well within the MFM framework. The experiments show that MFM can generalize to unseen data, outperforming other methods in this regard.

Weaknesses:
In Table 1 of the synthetic experiment, the authors compared the performance of FM, CGFM and MFM of k=0,1,10,50. MFM doesn't seem to beat existing methods on the metrics and from the visualizations, it's hard to tell MFM is actually doing better than FM. Also, for the various values of k, some explanations on how performance correlates with values of k and why might be necessary for readers to understand this table.

In both experiments, the authors only compared FM, CGFM, and in Table 2 also ICNN. Probably more methods, like diffusion, should also be taken into comparison. Also, in experiment 2, only W1, W2 and MMD are computed as metrics. While these are useful when modeling distributions, more metrics, especially those specific to this application may be applied.

Limitations:
The authors have not addressed limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposes a method for neural network-based learning to incorporate expert knowledge in the neural network architecture by building rules and utilizing them in ""rule-based"" layers of the learned neural networks. It introduces RuleGNNs as a concrete application of the proposed method and evaluates its performance against a few other SOTA methods. Empirical studies show competitive performance of RuleGNNs compared with other alternative methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The idea of having dynamic rule-based layers in a neural network and especially for graph neural network learning is interesting. Although some existing methods in the literature including WL labeling could be considered doing the same, the proposed method builds on top of these building blocks and extends their ideas.
- Theoretical discussions in the paper and the assumptions behind them are clear.
- Experimental results cover an adequate set of alternative methods.

Weaknesses:
- The performance of RuleGNNs is expected to heavily rely on the quality of the rules generated from additional information or domain knowledge, however, the paper solely focuses on application of such rules without adequately discussing the challenges of building quality rules and feasibility of this fundamental step in the proposed method.
- Lack of clarity around how rules in RuleGNNs look like and how they can influence learning model parameters. 
- Experimental results are not fully discussed. For example, WL-Kernel shows superior performance in three data sets and it would have been useful to provide more insights about what data set characteristics contribute to this.

Limitations:
Authors have adequately addressed the limitations of their work by listing the following limitations:
- They have only considered 1 dimensional input signals and labels.
- They have not considered graphs with multi-dimensional node features.
- Edge features are not considered.
- Computation and storage limitation for large/dense graphs.
In addition, authors have clearly discussed structure, Combinatorics, and Implementation limitations of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel model architecture rule-based layer, which induces different parameters given different inputs. Theoretical analysis demonstrates how the proposed architecture reduces back to classical feed-forward layers, and empirical results on both synthetic and real-world data sets demonstrate that the proposed method can improve upon existing works.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The idea of rule-based layers and rule-based GNN is novel and interesting.

Weaknesses:
- The implementation in this work may need further elaboration to make the proposed method easier to understand. 
- Empirical results may need further improvements to better support the proposed method.

Limitations:
The authors discuss about possible limitations in the conclusion part, and no direct negative societal impact exists for this work from my perspective.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces rule-based (dynamic) neural network layers. The basic idea is to have a common set of parameters, i.e., weights and biases, where, depending on a certain rule, only a subset of these parameters are used in the forward pass. They show that certain fully connected and convolutional layers can be regarded as a type of static rule-based neural network layer. In the remainder of the paper, the authors introduce three dynamic rules for graph classification tasks and perform experiments on synthetic and real-world datasets.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Overall, the concept of using rules based on expert knowledge to select different subsets of weights for various data samples or tasks seems useful and promising. This approach could offer significant benefits, such as training the same model on different tasks or on different datasets. Moreover, an approach which is able to learn on variably sized input data could be valuable on its own. The proposed rule-based layers for graph classification tasks outperform standard message-passing graph neural networks on synthetic and real-word datasets.

Weaknesses:
One of my primary concerns is that the main theoretical result of the paper, Theorem 1, is not proven. Specifically, while the authors show in Prop. 1 and Prop. 2 that fully connected layers _without bias_ and convolutional layers _without bias, padding, stride of one, and quadratic kernels_ can be expressed using their proposed (static) rule-based layer, the following paragraph leading to Theorem 1 claims this can be generalized to arbitrary convolutional layers. Although this might be straightforward to prove (and could be included in the appendix), the lack of a complete and formal proof severely undermines the soundness of the submission. If Theorem 1 is intended as a summary of Proposition 1 and Proposition 2, I suggest making this explicit by clearly stating the specific types of FC and CNN layers and renaming Theorem 1 to Corollary 1, or merging Prop. 1 and Prop. 2 into Theorem 1. Moreover, while the paper introduces some mathematical framework and formalizes existing concepts within this framework, it lacks proofs demonstrating what this framework can achieve and fails to establish connections to existing work. Given the lack of substantial theory, I think a more thorough empirical investigation could strengthen the submission. Comparisons with more expressive architectures are missing (e.g., in Table 1 there are no results reported for more expressive architectures for almost half of the datasets; for the synthetic datasets no comparison is done with more expressive architectures), making it difficult to appreciate the practical advantages of using the rule-based layers in practice. The practical relevance is limited further by the fact that the rule-based layer can only process one-dimensional features, and the higher space complexity for dense graphs.

Regarding clarity, there is considerable room for improvement. The concept of how a rule-based layer works was not fully clear to me until page 4. If my understanding is correct, we have a matrix $\mathcal{W}$ that contains all possible weights (and similarly a bias vector $\mathcal{B}$ with all possible biases). A rule restricts $\mathcal{W}$ to a subset of weights; applying rule *R* means setting some entries in $\mathcal{W}$ to zero. If my understanding is incorrect, this indicates that the writing lacks some clarity. I suggest shortening the introduction and preliminaries, which are at times verbose, and including a briefer example from Appendix A.4 earlier in the paper, or providing a clearer definition sooner. Additionally, the notation for the rule-based layer presentation is somewhat convoluted. The readability of the paper is also hindered by the inconsistent use of formal definitions and natural language. While both approaches can be fine (as long as they are precise), there is a noticeable mismatch between the rigor in the preliminaries and, for example, Section 4. Many aspects of the paper are thus unclear; please refer to the *Questions* and *Minor Remarks* for specific examples.

Overall, I think this paper presents promising ideas in a preliminary manner. As also stated by the authors, the dynamic rule-based layer seems to be reasonable for graphs, but is more difficult to devise for other structures. One approach could be to revise the paper from a graph learning perspective, and, if the authors have novel results which hold for general structures, present these results in a follow-up paper. Another exciting direction could be to use rules to create flexible machine learning models for different tasks and input data.

*Minor remarks*:

* line 33: each new information -> each new piece of information
* line 34: the essence -> a bit vague, what is the essence of dynamic NNs?
* Fig. 1 is too small and difficult to parse in general; there is also and typo in the last sentence
* line 75: dot missing after end of sentence
* line 95: concatentation -> should this be ""composition""?
* line 111: dot missing after end of sentence
* Somewhat inflationary use of ""respectively""
* line 123, 140: I would strongly advise to not use $y$ here for $x, y \in D$, as $y$ is already used to denote labels earlier
* It would be helpful to refer to equations as eq. (1) (instead of just (1))
* Could it simplify presentation if you define $\Theta$ as tuple $(\mathcal{W}, \mathcal{B})$?
* Last sentence of Prop. 1 is difficult to read
* Why do we call the learnable parameters $\Theta$ in Prop. 1 and $W^i$ in Prop. 2?
* line 190: higher dimensions -> higher dimensional
* line 202: network -> network architecture
* lines 206-214: I suggest to consider moving this to the preliminaries
* line 221: either rule function (singular) or rule functions R_W, R_B
* line 225: circle -> cycle
* Prop. 3: ""its"" -> not clear what it refers to
* line 231: If R permutation-equivariant -> language sounds off, maybe ""For permutation-equivariant R"" or ""If R is permutation-equivariant""
* line 255: typo in isomorphism
* Pattern counting rule: $d$ is never defined
* line 347: missing space

Limitations:
One of the main limitations, as the authors point out themselves, is that their proposed rule-based layer can only process one-dimensional node features, and no edge features, which impacts the practical value of their method. For more limitations, please refer to *Weaknesses* and *Questions*.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors develop a broad framework for adding expert knowledge to Neural Networks. They formalize this by extending the learnable parameterized functions with an additional parameter consisting of the set of formal rules. In general, these rules maybe learnable as well. However, the authors focuses on these rules being given in the form of expert-knowledge. The authors then introduce the set of rule based layers. And shows that fully connected NN layers and CNN layers are special cases of the rule based layers. They introduce three rule based layers for graphs: Weisfeiler-Leman Layer, Pattern-Counting layer and Aggregation layer. The author shows that there exists a GNN with rule based layers that can distinguish any two isomorphic graphs. Finally the author introduces some examples of rule based layers for specific molecule graphs. And presents experimental results on synthetic and real-world data.

Soundness:
3: good

Presentation:
1: poor

Contribution:
1: poor

Strengths:
-- The idea of adding expert knowledge to NNs and GNNs specifically is quite interesting and widely investigated.

-- The presented theory is very general and simple

Weaknesses:
-- The author has used the notion of rules rather broadly. There is no formal language (logic or matrix language) for the rules. They are just arbitrary functions. This basically means that any existing NN model, in one way or another, can be seen as a special case of Rule based NN. In my understanding, this makes the introduced framework a rather simple formalization of how expert knowledge maybe added to NNs. However, this formalization is so loose, that it does not really admit any meaningful analysis or provide any meaningful guidance to the user for adding knowledge.

-- None of the examples presented by the author are beyond what would be anyway possible by adding some simple graph features to the node features. This could be an interesting direction to investigate. But just formally stating that this is possible is not very interesting.

Limitations:
The authors have indeed touched upon most of the points I mention as weaknesses.
However, as mentioned earlier, the proposed framework is very broad and does not provide a meaningful way to proceed.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposes a method for neural network-based learning to incorporate expert knowledge in the neural network architecture by building rules and utilizing them in ""rule-based"" layers of the learned neural networks. It introduces RuleGNNs as a concrete application of the proposed method and evaluates its performance against a few other SOTA methods. Empirical studies show competitive performance of RuleGNNs compared with other alternative methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The idea of having dynamic rule-based layers in a neural network and especially for graph neural network learning is interesting. Although some existing methods in the literature including WL labeling could be considered doing the same, the proposed method builds on top of these building blocks and extends their ideas.
- Theoretical discussions in the paper and the assumptions behind them are clear.
- Experimental results cover an adequate set of alternative methods.

Weaknesses:
- The performance of RuleGNNs is expected to heavily rely on the quality of the rules generated from additional information or domain knowledge, however, the paper solely focuses on application of such rules without adequately discussing the challenges of building quality rules and feasibility of this fundamental step in the proposed method.
- Lack of clarity around how rules in RuleGNNs look like and how they can influence learning model parameters. 
- Experimental results are not fully discussed. For example, WL-Kernel shows superior performance in three data sets and it would have been useful to provide more insights about what data set characteristics contribute to this.

Limitations:
Authors have adequately addressed the limitations of their work by listing the following limitations:
- They have only considered 1 dimensional input signals and labels.
- They have not considered graphs with multi-dimensional node features.
- Edge features are not considered.
- Computation and storage limitation for large/dense graphs.
In addition, authors have clearly discussed structure, Combinatorics, and Implementation limitations of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a novel model architecture rule-based layer, which induces different parameters given different inputs. Theoretical analysis demonstrates how the proposed architecture reduces back to classical feed-forward layers, and empirical results on both synthetic and real-world data sets demonstrate that the proposed method can improve upon existing works.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The idea of rule-based layers and rule-based GNN is novel and interesting.

Weaknesses:
- The implementation in this work may need further elaboration to make the proposed method easier to understand. 
- Empirical results may need further improvements to better support the proposed method.

Limitations:
The authors discuss about possible limitations in the conclusion part, and no direct negative societal impact exists for this work from my perspective.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces rule-based (dynamic) neural network layers. The basic idea is to have a common set of parameters, i.e., weights and biases, where, depending on a certain rule, only a subset of these parameters are used in the forward pass. They show that certain fully connected and convolutional layers can be regarded as a type of static rule-based neural network layer. In the remainder of the paper, the authors introduce three dynamic rules for graph classification tasks and perform experiments on synthetic and real-world datasets.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
Overall, the concept of using rules based on expert knowledge to select different subsets of weights for various data samples or tasks seems useful and promising. This approach could offer significant benefits, such as training the same model on different tasks or on different datasets. Moreover, an approach which is able to learn on variably sized input data could be valuable on its own. The proposed rule-based layers for graph classification tasks outperform standard message-passing graph neural networks on synthetic and real-word datasets.

Weaknesses:
One of my primary concerns is that the main theoretical result of the paper, Theorem 1, is not proven. Specifically, while the authors show in Prop. 1 and Prop. 2 that fully connected layers _without bias_ and convolutional layers _without bias, padding, stride of one, and quadratic kernels_ can be expressed using their proposed (static) rule-based layer, the following paragraph leading to Theorem 1 claims this can be generalized to arbitrary convolutional layers. Although this might be straightforward to prove (and could be included in the appendix), the lack of a complete and formal proof severely undermines the soundness of the submission. If Theorem 1 is intended as a summary of Proposition 1 and Proposition 2, I suggest making this explicit by clearly stating the specific types of FC and CNN layers and renaming Theorem 1 to Corollary 1, or merging Prop. 1 and Prop. 2 into Theorem 1. Moreover, while the paper introduces some mathematical framework and formalizes existing concepts within this framework, it lacks proofs demonstrating what this framework can achieve and fails to establish connections to existing work. Given the lack of substantial theory, I think a more thorough empirical investigation could strengthen the submission. Comparisons with more expressive architectures are missing (e.g., in Table 1 there are no results reported for more expressive architectures for almost half of the datasets; for the synthetic datasets no comparison is done with more expressive architectures), making it difficult to appreciate the practical advantages of using the rule-based layers in practice. The practical relevance is limited further by the fact that the rule-based layer can only process one-dimensional features, and the higher space complexity for dense graphs.

Regarding clarity, there is considerable room for improvement. The concept of how a rule-based layer works was not fully clear to me until page 4. If my understanding is correct, we have a matrix $\mathcal{W}$ that contains all possible weights (and similarly a bias vector $\mathcal{B}$ with all possible biases). A rule restricts $\mathcal{W}$ to a subset of weights; applying rule *R* means setting some entries in $\mathcal{W}$ to zero. If my understanding is incorrect, this indicates that the writing lacks some clarity. I suggest shortening the introduction and preliminaries, which are at times verbose, and including a briefer example from Appendix A.4 earlier in the paper, or providing a clearer definition sooner. Additionally, the notation for the rule-based layer presentation is somewhat convoluted. The readability of the paper is also hindered by the inconsistent use of formal definitions and natural language. While both approaches can be fine (as long as they are precise), there is a noticeable mismatch between the rigor in the preliminaries and, for example, Section 4. Many aspects of the paper are thus unclear; please refer to the *Questions* and *Minor Remarks* for specific examples.

Overall, I think this paper presents promising ideas in a preliminary manner. As also stated by the authors, the dynamic rule-based layer seems to be reasonable for graphs, but is more difficult to devise for other structures. One approach could be to revise the paper from a graph learning perspective, and, if the authors have novel results which hold for general structures, present these results in a follow-up paper. Another exciting direction could be to use rules to create flexible machine learning models for different tasks and input data.

*Minor remarks*:

* line 33: each new information -> each new piece of information
* line 34: the essence -> a bit vague, what is the essence of dynamic NNs?
* Fig. 1 is too small and difficult to parse in general; there is also and typo in the last sentence
* line 75: dot missing after end of sentence
* line 95: concatentation -> should this be ""composition""?
* line 111: dot missing after end of sentence
* Somewhat inflationary use of ""respectively""
* line 123, 140: I would strongly advise to not use $y$ here for $x, y \in D$, as $y$ is already used to denote labels earlier
* It would be helpful to refer to equations as eq. (1) (instead of just (1))
* Could it simplify presentation if you define $\Theta$ as tuple $(\mathcal{W}, \mathcal{B})$?
* Last sentence of Prop. 1 is difficult to read
* Why do we call the learnable parameters $\Theta$ in Prop. 1 and $W^i$ in Prop. 2?
* line 190: higher dimensions -> higher dimensional
* line 202: network -> network architecture
* lines 206-214: I suggest to consider moving this to the preliminaries
* line 221: either rule function (singular) or rule functions R_W, R_B
* line 225: circle -> cycle
* Prop. 3: ""its"" -> not clear what it refers to
* line 231: If R permutation-equivariant -> language sounds off, maybe ""For permutation-equivariant R"" or ""If R is permutation-equivariant""
* line 255: typo in isomorphism
* Pattern counting rule: $d$ is never defined
* line 347: missing space

Limitations:
One of the main limitations, as the authors point out themselves, is that their proposed rule-based layer can only process one-dimensional node features, and no edge features, which impacts the practical value of their method. For more limitations, please refer to *Weaknesses* and *Questions*.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors develop a broad framework for adding expert knowledge to Neural Networks. They formalize this by extending the learnable parameterized functions with an additional parameter consisting of the set of formal rules. In general, these rules maybe learnable as well. However, the authors focuses on these rules being given in the form of expert-knowledge. The authors then introduce the set of rule based layers. And shows that fully connected NN layers and CNN layers are special cases of the rule based layers. They introduce three rule based layers for graphs: Weisfeiler-Leman Layer, Pattern-Counting layer and Aggregation layer. The author shows that there exists a GNN with rule based layers that can distinguish any two isomorphic graphs. Finally the author introduces some examples of rule based layers for specific molecule graphs. And presents experimental results on synthetic and real-world data.

Soundness:
3: good

Presentation:
1: poor

Contribution:
1: poor

Strengths:
-- The idea of adding expert knowledge to NNs and GNNs specifically is quite interesting and widely investigated.

-- The presented theory is very general and simple

Weaknesses:
-- The author has used the notion of rules rather broadly. There is no formal language (logic or matrix language) for the rules. They are just arbitrary functions. This basically means that any existing NN model, in one way or another, can be seen as a special case of Rule based NN. In my understanding, this makes the introduced framework a rather simple formalization of how expert knowledge maybe added to NNs. However, this formalization is so loose, that it does not really admit any meaningful analysis or provide any meaningful guidance to the user for adding knowledge.

-- None of the examples presented by the author are beyond what would be anyway possible by adding some simple graph features to the node features. This could be an interesting direction to investigate. But just formally stating that this is possible is not very interesting.

Limitations:
The authors have indeed touched upon most of the points I mention as weaknesses.
However, as mentioned earlier, the proposed framework is very broad and does not provide a meaningful way to proceed.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The authors propose diffusion Thompson sampling, which uses a diffusion model to leverage reward under similar actions for more efficient exploration. The authors derive efficient posterior approximations under a diffusion model prior and prove a regret bound in linear instances. To efficiently compute and sample posterior distribution, the authors provide an approximation that relies on close-form solutions for case where both the score functions of the diffusion model and the likelihood are linear. For nonlinear diffusion model, the authors approximate posteriors by a Gaussian distribution.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The proof of Theorem 4.1 requires novel techniques such as recursive total variance decomposition and refined arguments such as quantifying not only the posterior information gain for the taken action but also for every learnt latent parameter. 

The paper is well-written. The main contributions and key observations from the regret bound are nicely summarized. Experimental results for all four combinations of linear and nonlinear reward, linear and nonlinear diffusion model are provided. In experiments, the authors made a number of insightful observations, accompanied by ablation results.

Weaknesses:
The authors discussed how the number of layers L affect the regret bound. A higher L increases regret bound and a smaller L may fail to capture a more complex prior. It would improve the paper to provide a heuristics on choosing an appropriate L along with justifications for the heuristics.

Limitations:
The authors addressed limitations and societal impact at Appendix E and F.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work presents the use of Diffusion models as priors for Thompson sampling.

Namely, they propose to learn diffusion models (as replacement to other parametric priors) to accommodate more complex correlations between context, action and reward functions than with simple parametric form priors.

Given that Thompson sampling requires sampling from the posterior of the model, the authors derive a linear-Gaussian posterior approximation (under the proposed diffusion model prior).

The authors analyze the proposed algorithm for the linear-Gaussian reward case, which enables them to provide a Bayes regret bound.

Experimental results demonstrate some of the benefits of the proposed diffusion-based Thompson sampling: learning the correct latent-structure is beneficial, learning more parameters (as a function of $d$, $K$ and $L$) is a harder problem, hence incurs in higher regret.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The use of diffusion models to learn complex priors for their use within MAB problems is of interest and significant.

- The authors provide a theoretical analysis of their proposed algorithm (only for the linear-Gaussian case), for which they: 
    - use the recursive total covariance decomposition,
    - showcase the dependency over K ---induced by the hierarchical parameter learning--- and
    - demonstrate the dependency with $L$, inherent to having more parameters to learn.
    
- The theoretical analysis and the experiments showcase the benefits of learning the true hierarchical model (as specified by a diffusion model) in comparison to LinTS.

Weaknesses:
- The proposed diffusion-based algorithm does not learn the diffusion model as it sequentially interacts with the world
    - Instead, using the diffusion model as a complex prior requires offline learning, so that non-trivial prior distributions can be learned, before it can be used within Thompson sampling.
    - The cost of learning such a diffusion model is not acknowledged nor discussed.

- The proposed posterior approximation seems to be equivalent to the well known Laplace approximation, i.e., a linear-Gaussian approximation to a (non-linear and non-Gaussian) posterior. See questions below.

- The provided Bayesian regret is limited to the linear-Gaussian case, and in fact is acknowledged to be similar to ""L + 1 sequential linear bandit instances stacked upon each other"".

- The empirical evaluation is executed on synthetic experiments simulated from the assumed model prior, with $L$ latent parameters. Hence, the benefits of learning the true model are somehow expected.

Limitations:
The authors do present general limitations of their work, although the cost associated with learning a diffusion model prior is less clear.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work provides a great example of diffusion modeling on bandit action parameter for better exploration.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The work provides a comprehensive description on how to employ diffusion modeling on bandit parameters for contextual bandit problems. 

The discussion on linear and non-linear diffusion model is clean and precise for readers with background in Thompson Sampling 

The analysis part also provide comprehensive discussion on how the regret of the  proposed diffusion Thompson Sampling scales with main dimension of contextual bandit problems.

Weaknesses:
I am satisfied with current version of the paper.

Limitations:
Yes, the author clearly states the assumptions to address the limitation of the theoretical analysis.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper considers the problem of contextual bandits in large action spaces.  In this problem, the reward of an arm is a function of the context and an unknown, arm specific parameter vector. To efficiently learn good policies in such large action spaces, the paper places a structured-prior distribution on the unknown arm parameters that can effectively capture the correlations between the arms. The specific form of the prior distribution considered in the paper resembles a diffusion model. The main contribution of the paper is to provide a computationally efficient heuristic for performing Thompson sampling with this prior. Experiments on synthetic data show that the proposed technique is much better at learning optimal policies than other popular baselines such as LinTS, LinUCB.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The problem of handling large action spaces in contextual bandits seems interesting. The empirical evaluation shows promise in the proposed approach

Weaknesses:
- **Related Work:** There are several ways in which large action spaces are typically handled in contextual bandits. One popular approach that is used in practice is to associate a feature vector to each arm (this feature vector is known to the learner ahead of time), and the reward of pulling certain arm for a context is a function of both the context and arm features. In the absence of arm features, the other approach is to impose some structure on the unknown arm parameter vectors. There are several works which do this, and the current paper falls in this line of research. Some of these works assume the arms can be clustered into a small number of groups or can be embedded in a low-dimensional latent space and learn the low-dimensional features during the course of the online learning (https://arxiv.org/pdf/2010.12363, https://arxiv.org/abs/2209.03997, https://arxiv.org/pdf/1810.09401, https://proceedings.neurips.cc/paper_files/paper/2023/file/f334c3375bd3744e98a0ca8eaa2403b0-Paper-Conference.pdf). The diffusion prior used in the current work resembles the low-dimensional embedding assumption. In particular, it is assumed there is a latent vector (psi_1) from which all the arm parameter vectors are generated; this is a form of rank-1 assumption on the arm features. Unfortunately, none of these works were brought up in the paper. It would be great if the authors perform a thorough literature review and better position their work.

- **Linear Setting**: A lot of emphasis has been placed on the linear model in the paper. I understand it is used to derive the heuristic for the non-linear setting. Beyond that, I do not find the regret bounds derived in section 4 to be interesting. In the linear setting, there isn't a need to work with the complex hierarchical diffusion prior. It looks like one could totally remove the latent variables psi_{*, L}, .... psi_{*, 2} and simply place a Gaussian prior on psi_{*,1} and get an equally powerful model. This would also improve the regret bounds, by removing the L factor in the regret. Given this, I'm not sure about the utility of section 4.
  - Section 4.1 compares the regret bounds obtained in this work with other baselines. But this comparison is only meaningful under the assumption that the diffusion prior is properly specified. This raises the following question: why is this a reasonable prior to use in practice? How do various techniques compare if this prior is misspecified? (There are some experiments section 5.2 on prior misspecification, but the misspecifications considered there seem to be very minor)

- **Quality of Heuristics:** How good is the heuristic used for non-linear diffusion model? There is no discussion on this in the paper (In my opinion, this needs to be thoroughly discussed in the paper, as it is the primary novelty of the work). Some empirical evaluation comparing it with other standard estimation techniques (such as variational techniques, and other posterior estimation techniques) would have been helpful in understanding his question.

Limitations:
See my comments above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The authors propose diffusion Thompson sampling, which uses a diffusion model to leverage reward under similar actions for more efficient exploration. The authors derive efficient posterior approximations under a diffusion model prior and prove a regret bound in linear instances. To efficiently compute and sample posterior distribution, the authors provide an approximation that relies on close-form solutions for case where both the score functions of the diffusion model and the likelihood are linear. For nonlinear diffusion model, the authors approximate posteriors by a Gaussian distribution.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The proof of Theorem 4.1 requires novel techniques such as recursive total variance decomposition and refined arguments such as quantifying not only the posterior information gain for the taken action but also for every learnt latent parameter. 

The paper is well-written. The main contributions and key observations from the regret bound are nicely summarized. Experimental results for all four combinations of linear and nonlinear reward, linear and nonlinear diffusion model are provided. In experiments, the authors made a number of insightful observations, accompanied by ablation results.

Weaknesses:
The authors discussed how the number of layers L affect the regret bound. A higher L increases regret bound and a smaller L may fail to capture a more complex prior. It would improve the paper to provide a heuristics on choosing an appropriate L along with justifications for the heuristics.

Limitations:
The authors addressed limitations and societal impact at Appendix E and F.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work presents the use of Diffusion models as priors for Thompson sampling.

Namely, they propose to learn diffusion models (as replacement to other parametric priors) to accommodate more complex correlations between context, action and reward functions than with simple parametric form priors.

Given that Thompson sampling requires sampling from the posterior of the model, the authors derive a linear-Gaussian posterior approximation (under the proposed diffusion model prior).

The authors analyze the proposed algorithm for the linear-Gaussian reward case, which enables them to provide a Bayes regret bound.

Experimental results demonstrate some of the benefits of the proposed diffusion-based Thompson sampling: learning the correct latent-structure is beneficial, learning more parameters (as a function of $d$, $K$ and $L$) is a harder problem, hence incurs in higher regret.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The use of diffusion models to learn complex priors for their use within MAB problems is of interest and significant.

- The authors provide a theoretical analysis of their proposed algorithm (only for the linear-Gaussian case), for which they: 
    - use the recursive total covariance decomposition,
    - showcase the dependency over K ---induced by the hierarchical parameter learning--- and
    - demonstrate the dependency with $L$, inherent to having more parameters to learn.
    
- The theoretical analysis and the experiments showcase the benefits of learning the true hierarchical model (as specified by a diffusion model) in comparison to LinTS.

Weaknesses:
- The proposed diffusion-based algorithm does not learn the diffusion model as it sequentially interacts with the world
    - Instead, using the diffusion model as a complex prior requires offline learning, so that non-trivial prior distributions can be learned, before it can be used within Thompson sampling.
    - The cost of learning such a diffusion model is not acknowledged nor discussed.

- The proposed posterior approximation seems to be equivalent to the well known Laplace approximation, i.e., a linear-Gaussian approximation to a (non-linear and non-Gaussian) posterior. See questions below.

- The provided Bayesian regret is limited to the linear-Gaussian case, and in fact is acknowledged to be similar to ""L + 1 sequential linear bandit instances stacked upon each other"".

- The empirical evaluation is executed on synthetic experiments simulated from the assumed model prior, with $L$ latent parameters. Hence, the benefits of learning the true model are somehow expected.

Limitations:
The authors do present general limitations of their work, although the cost associated with learning a diffusion model prior is less clear.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work provides a great example of diffusion modeling on bandit action parameter for better exploration.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The work provides a comprehensive description on how to employ diffusion modeling on bandit parameters for contextual bandit problems. 

The discussion on linear and non-linear diffusion model is clean and precise for readers with background in Thompson Sampling 

The analysis part also provide comprehensive discussion on how the regret of the  proposed diffusion Thompson Sampling scales with main dimension of contextual bandit problems.

Weaknesses:
I am satisfied with current version of the paper.

Limitations:
Yes, the author clearly states the assumptions to address the limitation of the theoretical analysis.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper considers the problem of contextual bandits in large action spaces.  In this problem, the reward of an arm is a function of the context and an unknown, arm specific parameter vector. To efficiently learn good policies in such large action spaces, the paper places a structured-prior distribution on the unknown arm parameters that can effectively capture the correlations between the arms. The specific form of the prior distribution considered in the paper resembles a diffusion model. The main contribution of the paper is to provide a computationally efficient heuristic for performing Thompson sampling with this prior. Experiments on synthetic data show that the proposed technique is much better at learning optimal policies than other popular baselines such as LinTS, LinUCB.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The problem of handling large action spaces in contextual bandits seems interesting. The empirical evaluation shows promise in the proposed approach

Weaknesses:
- **Related Work:** There are several ways in which large action spaces are typically handled in contextual bandits. One popular approach that is used in practice is to associate a feature vector to each arm (this feature vector is known to the learner ahead of time), and the reward of pulling certain arm for a context is a function of both the context and arm features. In the absence of arm features, the other approach is to impose some structure on the unknown arm parameter vectors. There are several works which do this, and the current paper falls in this line of research. Some of these works assume the arms can be clustered into a small number of groups or can be embedded in a low-dimensional latent space and learn the low-dimensional features during the course of the online learning (https://arxiv.org/pdf/2010.12363, https://arxiv.org/abs/2209.03997, https://arxiv.org/pdf/1810.09401, https://proceedings.neurips.cc/paper_files/paper/2023/file/f334c3375bd3744e98a0ca8eaa2403b0-Paper-Conference.pdf). The diffusion prior used in the current work resembles the low-dimensional embedding assumption. In particular, it is assumed there is a latent vector (psi_1) from which all the arm parameter vectors are generated; this is a form of rank-1 assumption on the arm features. Unfortunately, none of these works were brought up in the paper. It would be great if the authors perform a thorough literature review and better position their work.

- **Linear Setting**: A lot of emphasis has been placed on the linear model in the paper. I understand it is used to derive the heuristic for the non-linear setting. Beyond that, I do not find the regret bounds derived in section 4 to be interesting. In the linear setting, there isn't a need to work with the complex hierarchical diffusion prior. It looks like one could totally remove the latent variables psi_{*, L}, .... psi_{*, 2} and simply place a Gaussian prior on psi_{*,1} and get an equally powerful model. This would also improve the regret bounds, by removing the L factor in the regret. Given this, I'm not sure about the utility of section 4.
  - Section 4.1 compares the regret bounds obtained in this work with other baselines. But this comparison is only meaningful under the assumption that the diffusion prior is properly specified. This raises the following question: why is this a reasonable prior to use in practice? How do various techniques compare if this prior is misspecified? (There are some experiments section 5.2 on prior misspecification, but the misspecifications considered there seem to be very minor)

- **Quality of Heuristics:** How good is the heuristic used for non-linear diffusion model? There is no discussion on this in the paper (In my opinion, this needs to be thoroughly discussed in the paper, as it is the primary novelty of the work). Some empirical evaluation comparing it with other standard estimation techniques (such as variational techniques, and other posterior estimation techniques) would have been helpful in understanding his question.

Limitations:
See my comments above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,No,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The manuscript introduces a neural compression paradigm for effectively compressing diverse sets of 3D geometry models. The authors propose a two-stage framework that first converts irregular mesh models into a regular 4D TSDF-Def volume representation and then employs a quantization-aware auto-decoder network to achieve redundancy elimination and compact representation. The method claims to compress a large number of 3D mesh models with high accuracy and preservation of geometric details, outperforming state-of-the-art methods both quantitatively and qualitatively.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper presents a unique method for compressing 3D geometry sets by leveraging neural networks, which is a significant advancement in the field. NeCGS achieves an impressive compression ratio, which is a critical metric for 3D geometry data compression.
- The method maintains high accuracy and preserves detailed geometric structures even at high compression ratios. The authors have conducted comprehensive experiments and ablation studies across various datasets, demonstrating the effectiveness of their approach.
- The inclusion of source code in the supplemental material enhances the reproducibility and transparency of the research.
- The paper is well-organized, with clear explanations of the methodology and results.

Weaknesses:
- The manuscript mentions that the optimization process for TSDF-Def volumes is time-consuming (over 15 hours), which could be a limitation for practical applications. The manuscript should address the long optimization time required for the TSDF-Def volumes. Future work could focus on accelerating this process to make the method more practical.
- While the method performs well on tested datasets, it is unclear how well it generalizes to other, more complex, or varied 3D geometry sets, such as some geometry with thin structures or open boundaries (cloth). 
- The choice of an auto-decoder network is effective, but the paper could benefit from a more detailed explanation of why this architecture was chosen over others.
- While the method outperforms existing techniques, a more thorough comparison in terms of trade-offs, especially related to computational resources, would be insightful.
- The paper could provide more insights into how the method scales with the size and complexity of the 3D geometry sets. The paper should include scalability tests to understand how the method performs with larger and more complex datasets.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a neural compression algorithm, NeCGS to significantly compress geometry datasets. The algorithm mainly consists of 2 components, 1) regular geometry representation: This is an optimization algorithm to optimize the TSDF field such that the error between the original geometries and the geometries reconstructed by the deformable marching cube algorithm is minimized and 2) compact neural representation: regresses the optimized TSDF-def fields from compressed latent states, quantizes the latent states and compresses them further into bitstreams. The trained decoder can then be used to reconstruct the TSDF-def fields and the geometries can be reconstructed using the DMC algorithm.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The NeCGS algorithm can provide high compression ratios with impressive reconstruction capability of the geometries. Better geometry representations can be achieved using the proposed optimization algorithm. This is evident from the ability of the DMC method to accurately reconstruct surfaces. The DMC algorithm is also significant and seems to provide better reconstruction of detailed structure in the geometries. Overall, the developed compression method has high potential and the results presented in the paper are very impressive.

Weaknesses:
The biggest weakness of the proposed approach is the computational cost of the method. The exorbitantly large times required to compress the datasets reduce the value proposition. Additionally, it is not clear how much the computational cost scales with the size of the geometry dataset.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method to compress 3D geometry of diverse categories of objects. In the first step, the paper proposes a method to first convert an irregular mesh to a regular representation like a 4D TSDF-Def volume that implicitly describes the geometry. After this, an auto-decoder is trained that learns to reconstruct the 4D TSDF-Def volume from a compressed feature vector which is unique for each shape. Hence, with this design the model can summarize the similarity of local geometric structures within and across different 3D meshes resulting in a compact representation. Results on AMA, DT4D and Thingi10K datasets shows that the model can achieve compression of 3D models to a reasonable extent.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) **Clarity:** the paper is well written with each component of the method explained clearly which is easy to understand.
2) **Reproducibility:** All the details to replicate the results are provided along with the code and architecture details in the supplementary material.

Weaknesses:
1. The intuition behind preferring TSDF-Def 4D volume over TSDF 3D volume is unclear, even though an ablation study shows better reconstruction for thin structures. The quantitative results in Table 2 only show marginal improvements. An brief intuitive explanation of the design choice is helpful.
2. There are lot of methods which try to compress a neural field. For e.g. Triplanes[1], HashGrid [2], Vector Quantization [3], TensoRF [4], Dictionary Fields [5]. It is not very clear why this method does not compare with all these techniques which can be used for compression? 
3. Can this method generalize? Can I use the trained auto-decoder setting to compress a new 3D mesh on which the model is not trained on? How about other methods with which the method compares.
4. The paper does not do a relative comparison of the compression time with the baseline methods. Given the optimization time shown in Table 3, I have concerns about the practical usage of this method.

[1] Peng, Songyou, et al. ""Convolutional occupancy networks."" ECCV, 2020. \
[2] Müller, T., Evans, A., Schied, C., & Keller, A. (2022). Instant neural graphics primitives with a multiresolution hash encoding. ACM TOG, 2022. \
[3] Takikawa, Towaki, et al. ""Variable bitrate neural fields."" ACM SIGGRAPH, 2022. \
[4] Chen, Anpei, et al. ""Tensorf: Tensorial radiance fields."" ECCV, 2022. \
[5] Chen, Anpei, et al. ""Dictionary fields: Learning a neural basis decomposition."" ACM TOG, 2023.

Limitations:
Limitations are adequately discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
this paper looks at the problem of compressing 3d shapes (esp geometry). this paper proposes a two stage approach. the first stage is regular geometry representation. the second stage is compact neural compression. results show some improvements.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. compressing 3d shapes is important to many applications

Weaknesses:
1. this paper over claims what it does. in L1-3, it says that they made the first attempt to tackle the problem of compressing 3D geometry sets containing diverse categories. this isn't true. there are at least two papers doing geometry compression of 3D geometry [a], [b].  

[a] On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes https://arxiv.org/abs/2009.09808
[b] Neural Progressive Meshes https://arxiv.org/abs/2308.05741

2. [a] and [b] are very important references but they are not cited nor discussed. it's not necessary to compare the proposed method with [a] and [b], but at least the authors should acknowledge the existence of these two papers.

3. optimization time is too long

4. it is unclear whether the proposed method is reproducible

5. typo L43: Matching cubes -> Marching cubes

Limitations:
yes

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The manuscript introduces a neural compression paradigm for effectively compressing diverse sets of 3D geometry models. The authors propose a two-stage framework that first converts irregular mesh models into a regular 4D TSDF-Def volume representation and then employs a quantization-aware auto-decoder network to achieve redundancy elimination and compact representation. The method claims to compress a large number of 3D mesh models with high accuracy and preservation of geometric details, outperforming state-of-the-art methods both quantitatively and qualitatively.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper presents a unique method for compressing 3D geometry sets by leveraging neural networks, which is a significant advancement in the field. NeCGS achieves an impressive compression ratio, which is a critical metric for 3D geometry data compression.
- The method maintains high accuracy and preserves detailed geometric structures even at high compression ratios. The authors have conducted comprehensive experiments and ablation studies across various datasets, demonstrating the effectiveness of their approach.
- The inclusion of source code in the supplemental material enhances the reproducibility and transparency of the research.
- The paper is well-organized, with clear explanations of the methodology and results.

Weaknesses:
- The manuscript mentions that the optimization process for TSDF-Def volumes is time-consuming (over 15 hours), which could be a limitation for practical applications. The manuscript should address the long optimization time required for the TSDF-Def volumes. Future work could focus on accelerating this process to make the method more practical.
- While the method performs well on tested datasets, it is unclear how well it generalizes to other, more complex, or varied 3D geometry sets, such as some geometry with thin structures or open boundaries (cloth). 
- The choice of an auto-decoder network is effective, but the paper could benefit from a more detailed explanation of why this architecture was chosen over others.
- While the method outperforms existing techniques, a more thorough comparison in terms of trade-offs, especially related to computational resources, would be insightful.
- The paper could provide more insights into how the method scales with the size and complexity of the 3D geometry sets. The paper should include scalability tests to understand how the method performs with larger and more complex datasets.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a neural compression algorithm, NeCGS to significantly compress geometry datasets. The algorithm mainly consists of 2 components, 1) regular geometry representation: This is an optimization algorithm to optimize the TSDF field such that the error between the original geometries and the geometries reconstructed by the deformable marching cube algorithm is minimized and 2) compact neural representation: regresses the optimized TSDF-def fields from compressed latent states, quantizes the latent states and compresses them further into bitstreams. The trained decoder can then be used to reconstruct the TSDF-def fields and the geometries can be reconstructed using the DMC algorithm.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The NeCGS algorithm can provide high compression ratios with impressive reconstruction capability of the geometries. Better geometry representations can be achieved using the proposed optimization algorithm. This is evident from the ability of the DMC method to accurately reconstruct surfaces. The DMC algorithm is also significant and seems to provide better reconstruction of detailed structure in the geometries. Overall, the developed compression method has high potential and the results presented in the paper are very impressive.

Weaknesses:
The biggest weakness of the proposed approach is the computational cost of the method. The exorbitantly large times required to compress the datasets reduce the value proposition. Additionally, it is not clear how much the computational cost scales with the size of the geometry dataset.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method to compress 3D geometry of diverse categories of objects. In the first step, the paper proposes a method to first convert an irregular mesh to a regular representation like a 4D TSDF-Def volume that implicitly describes the geometry. After this, an auto-decoder is trained that learns to reconstruct the 4D TSDF-Def volume from a compressed feature vector which is unique for each shape. Hence, with this design the model can summarize the similarity of local geometric structures within and across different 3D meshes resulting in a compact representation. Results on AMA, DT4D and Thingi10K datasets shows that the model can achieve compression of 3D models to a reasonable extent.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) **Clarity:** the paper is well written with each component of the method explained clearly which is easy to understand.
2) **Reproducibility:** All the details to replicate the results are provided along with the code and architecture details in the supplementary material.

Weaknesses:
1. The intuition behind preferring TSDF-Def 4D volume over TSDF 3D volume is unclear, even though an ablation study shows better reconstruction for thin structures. The quantitative results in Table 2 only show marginal improvements. An brief intuitive explanation of the design choice is helpful.
2. There are lot of methods which try to compress a neural field. For e.g. Triplanes[1], HashGrid [2], Vector Quantization [3], TensoRF [4], Dictionary Fields [5]. It is not very clear why this method does not compare with all these techniques which can be used for compression? 
3. Can this method generalize? Can I use the trained auto-decoder setting to compress a new 3D mesh on which the model is not trained on? How about other methods with which the method compares.
4. The paper does not do a relative comparison of the compression time with the baseline methods. Given the optimization time shown in Table 3, I have concerns about the practical usage of this method.

[1] Peng, Songyou, et al. ""Convolutional occupancy networks."" ECCV, 2020. \
[2] Müller, T., Evans, A., Schied, C., & Keller, A. (2022). Instant neural graphics primitives with a multiresolution hash encoding. ACM TOG, 2022. \
[3] Takikawa, Towaki, et al. ""Variable bitrate neural fields."" ACM SIGGRAPH, 2022. \
[4] Chen, Anpei, et al. ""Tensorf: Tensorial radiance fields."" ECCV, 2022. \
[5] Chen, Anpei, et al. ""Dictionary fields: Learning a neural basis decomposition."" ACM TOG, 2023.

Limitations:
Limitations are adequately discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
this paper looks at the problem of compressing 3d shapes (esp geometry). this paper proposes a two stage approach. the first stage is regular geometry representation. the second stage is compact neural compression. results show some improvements.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. compressing 3d shapes is important to many applications

Weaknesses:
1. this paper over claims what it does. in L1-3, it says that they made the first attempt to tackle the problem of compressing 3D geometry sets containing diverse categories. this isn't true. there are at least two papers doing geometry compression of 3D geometry [a], [b].  

[a] On the Effectiveness of Weight-Encoded Neural Implicit 3D Shapes https://arxiv.org/abs/2009.09808
[b] Neural Progressive Meshes https://arxiv.org/abs/2308.05741

2. [a] and [b] are very important references but they are not cited nor discussed. it's not necessary to compare the proposed method with [a] and [b], but at least the authors should acknowledge the existence of these two papers.

3. optimization time is too long

4. it is unclear whether the proposed method is reproducible

5. typo L43: Matching cubes -> Marching cubes

Limitations:
yes

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The authors propose a bit-switching quantization method using Double Rounding, which applies rounding twice to achieve nearly lossless switching without storing a full-precision model. They also introduce Adaptive Learning Rate Scaling (ALRS) to adjust learning rates dynamically across precisions, ensuring consistent quantization updates. Additionally, they develop Hessian-Aware Stochastic Bit-switching (HASB) for one-shot mixed-precision training, optimizing bit-width distribution based on layer sensitivity, thus eliminating retraining stages.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. ALRS heuristic can help practitioners who wish to train mutli-precision model

2. Authors made extensive experiments on vision models and compare to previse methos

3. Most sections are well written

4. Code is given

Weaknesses:
**Novelty** is limited and I am not highly motivated that the problem is important.

1.	The main contribution is to not same 32bit weight and different quantization parameters but only the high bidwith using a pretty straightforward idea of double rounding during training
2.	The ALRS is based on observation and heuristic to fix it. It is nice and helps for when trying to use 2 bits as well. Yet, I am not sure it is important for methods that don’t use the double rounding.

**Motivation**

3.	Since we usually don’t switch models based on data I am not sure why this is important. Do we really have edge device that switch on a daily base model precision and thus need to store in small local memory the 32bit model? Can you elaborate why and where multi precision is really important.

4. No results on more recent models (LLMs)

Limitations:
The authors partially discuss limitation

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses challenges in model quantization for deep neural networks (DNNs), focusing on optimizing quantization-aware training (QAT) across multiple bit-widths with weight-sharing. To this end, this paper introduces a novel quantization method that exploits the highest integer precision to achieve nearly lossless bit-switching, reducing storage without relying on full precision. Key contributions include: (1) Adaptive Learning Rate Scaling: A technique that dynamically adjusts learning rates for different precisions to address competitive interference and inconsistent gradient issues during one-shot joint training. (2) Double Rounding: An extension for one-step rounding quantizer in fixed-precision quantization to improve accuracy. Experimental results on the ImageNet-1K dataset show that the proposed methods surpass state-of-the-art approaches in both multi-precision and mixed-precision scenarios, achieving higher efficiency and accuracy.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- This submission is well-written, as well as with good figures in Sec.4.
- The authors conduct extensive experiments on multiple datasets and multiple networks.

Weaknesses:
- Some analysis is missing. For example, I'm wondering whether the second rounding leads to more quantization errors, as the first rounding is used to produce INT8 weights and second rounding is then performed to quantize lower bit-width, the twice quantization is possible to cause more clipping errors and rounding errors, some analysis could enhance the strength of proposed methods. 
- Some designs should be further clarified, e.g., why ALRS is applied only for the scaling factors? Intuitively, weights of small bit-width is induced large gradient variance by STE, and thus the weights of small bit-width should also benefit from using smaller LR. 
- Fig. 1 is a bit confusing, some colored arrows are not well explained. 
- This works essentially lies in the research of mixed-precision quantization, so I think it is better to compare more MPQ (e.g., HAQ, DNAS, LIMPQ, etc) research in the Sec.4. Moreover, some recent papers on multi bit-width quantization are missed on the , e.g., [1] (PTQ-based) and [2][3] (QAT-based), which could be included into the Related Work. 

[1] Xu, Ke, et al. ""PTMQ: Post-training Multi-Bit Quantization of Neural Networks."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 14. 2024.

[2] Tang, Chen, et al. ""Retraining-free model quantization via one-shot weight-coupling learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024. 

[3] Zhong, Yunshan, et al. ""MultiQuant: A Novel Multi-Branch Topology Method for Arbitrary Bit-width Network Quantization."" arXiv preprint arXiv:2305.08117 (2023).

Limitations:
Please refer to the weaknesses. Overall, this paper currently needs more experiments and analysis to reveal some designs are reasonable.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a QAT scheme to jointly optimize a single model with different precisions. The authors apply their scheme on various CNN-based models on CIFAR-10 and ImageNet datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written
2. The ablation study is strong in my opinion and they evaluate various aspects of their scheme

Weaknesses:
1. I think the main limitation of the paper is the models and datasets. I believe that the study should be done on larger models (LLMs for example) as a architecture goal. For example, the authors show that they do not save a FP32 master copy of the model in their scheme. However, ResNet style models (or MobileNet) are easy to fit in even moderate GPUs and I don't think FP32 master copy is a big problem in that case (please correct me if I'm wrong).

2. I couldn't find a source-code to reproduce the results of the paper in my side.

Limitations:
yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper discusses advanced methods in multi-bit model quantization. Specifically, this paper proposes a method for one-shot joint training of multiple precisions. To this end, the authors introduce a double-rounding quantizer that leverages the highest integer precision to achieve nearly lossless bit-switching while reducing storage requirements. Moreover, they also propose an Adaptive Learning Rate Scaling technique that adjusts learning rates dynamically for different precisions. Two proposed techniques mitigate the competitive interference between bit-widths caused by inconsistent gradients of different precisions during biased gradient estimation. They also extend their Double Rounding method to support one-shot mixed precision training and develop a Hessian-aware Bit-witdh sampling strategy. Experimental results on the ImageNet-1K classification task show that their methods outperform state-of-the-art one-shot joint QAT in both multi-precision and mixed-precision scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Eliminating the costs of retraining for mixed-precision quantization is a meaningful and challenging topic.

- The end-to-end experiments are sufficient, and the presentation is good.

Weaknesses:
- More uniquness analysis needed. The using of Hessian information seems a bit trivial, each layer's Hessian is just used to compare with the averaged Hessian trace. Firstly, as shown in recent zero-cost NAS research [1], the architectural proxies will be less effective as the training goes on, I'm not sure the Hessian information obtained on the initial full-precision model will remain useful as the quantization-aware training continues. Moreover, the sampling probability is modified with a simple ascending heuristic, which is not Hessian-aware. 

- Also applies here: the design of the double-rounding quantizer is similar to Bit-Mixer, Adabits, and ABN. Specifically, ABN also uses 

- ALRS needs further ablations. In ALRS, the authors use a fixed scaling ratio to bit-widths, e.g., 8-bit is 1, 6-bit is 0.1, and 4-bit is 0.01, the choice of these scaling factors still requires more ablation studies and discussions. 

- More comparisons needed. Since this paper adopts an ILP-based search algorithm to find optimal subnets, it is better to compare with these ILP-based mixed-precision quantization papers, e.g., [2] and [3]. 



[1] A Deeper Look at Zero-Cost Proxies for Lightweight NAS 
[2] Mixed-precision neural network quantization via learned layer-wise importance, ECCV 2022. 
[3] Hawq-v2: Hessian aware trace-weighted quantization of neural networks, NIPS 2020.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The authors propose a bit-switching quantization method using Double Rounding, which applies rounding twice to achieve nearly lossless switching without storing a full-precision model. They also introduce Adaptive Learning Rate Scaling (ALRS) to adjust learning rates dynamically across precisions, ensuring consistent quantization updates. Additionally, they develop Hessian-Aware Stochastic Bit-switching (HASB) for one-shot mixed-precision training, optimizing bit-width distribution based on layer sensitivity, thus eliminating retraining stages.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. ALRS heuristic can help practitioners who wish to train mutli-precision model

2. Authors made extensive experiments on vision models and compare to previse methos

3. Most sections are well written

4. Code is given

Weaknesses:
**Novelty** is limited and I am not highly motivated that the problem is important.

1.	The main contribution is to not same 32bit weight and different quantization parameters but only the high bidwith using a pretty straightforward idea of double rounding during training
2.	The ALRS is based on observation and heuristic to fix it. It is nice and helps for when trying to use 2 bits as well. Yet, I am not sure it is important for methods that don’t use the double rounding.

**Motivation**

3.	Since we usually don’t switch models based on data I am not sure why this is important. Do we really have edge device that switch on a daily base model precision and thus need to store in small local memory the 32bit model? Can you elaborate why and where multi precision is really important.

4. No results on more recent models (LLMs)

Limitations:
The authors partially discuss limitation

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper addresses challenges in model quantization for deep neural networks (DNNs), focusing on optimizing quantization-aware training (QAT) across multiple bit-widths with weight-sharing. To this end, this paper introduces a novel quantization method that exploits the highest integer precision to achieve nearly lossless bit-switching, reducing storage without relying on full precision. Key contributions include: (1) Adaptive Learning Rate Scaling: A technique that dynamically adjusts learning rates for different precisions to address competitive interference and inconsistent gradient issues during one-shot joint training. (2) Double Rounding: An extension for one-step rounding quantizer in fixed-precision quantization to improve accuracy. Experimental results on the ImageNet-1K dataset show that the proposed methods surpass state-of-the-art approaches in both multi-precision and mixed-precision scenarios, achieving higher efficiency and accuracy.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- This submission is well-written, as well as with good figures in Sec.4.
- The authors conduct extensive experiments on multiple datasets and multiple networks.

Weaknesses:
- Some analysis is missing. For example, I'm wondering whether the second rounding leads to more quantization errors, as the first rounding is used to produce INT8 weights and second rounding is then performed to quantize lower bit-width, the twice quantization is possible to cause more clipping errors and rounding errors, some analysis could enhance the strength of proposed methods. 
- Some designs should be further clarified, e.g., why ALRS is applied only for the scaling factors? Intuitively, weights of small bit-width is induced large gradient variance by STE, and thus the weights of small bit-width should also benefit from using smaller LR. 
- Fig. 1 is a bit confusing, some colored arrows are not well explained. 
- This works essentially lies in the research of mixed-precision quantization, so I think it is better to compare more MPQ (e.g., HAQ, DNAS, LIMPQ, etc) research in the Sec.4. Moreover, some recent papers on multi bit-width quantization are missed on the , e.g., [1] (PTQ-based) and [2][3] (QAT-based), which could be included into the Related Work. 

[1] Xu, Ke, et al. ""PTMQ: Post-training Multi-Bit Quantization of Neural Networks."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 14. 2024.

[2] Tang, Chen, et al. ""Retraining-free model quantization via one-shot weight-coupling learning."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024. 

[3] Zhong, Yunshan, et al. ""MultiQuant: A Novel Multi-Branch Topology Method for Arbitrary Bit-width Network Quantization."" arXiv preprint arXiv:2305.08117 (2023).

Limitations:
Please refer to the weaknesses. Overall, this paper currently needs more experiments and analysis to reveal some designs are reasonable.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes a QAT scheme to jointly optimize a single model with different precisions. The authors apply their scheme on various CNN-based models on CIFAR-10 and ImageNet datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written
2. The ablation study is strong in my opinion and they evaluate various aspects of their scheme

Weaknesses:
1. I think the main limitation of the paper is the models and datasets. I believe that the study should be done on larger models (LLMs for example) as a architecture goal. For example, the authors show that they do not save a FP32 master copy of the model in their scheme. However, ResNet style models (or MobileNet) are easy to fit in even moderate GPUs and I don't think FP32 master copy is a big problem in that case (please correct me if I'm wrong).

2. I couldn't find a source-code to reproduce the results of the paper in my side.

Limitations:
yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper discusses advanced methods in multi-bit model quantization. Specifically, this paper proposes a method for one-shot joint training of multiple precisions. To this end, the authors introduce a double-rounding quantizer that leverages the highest integer precision to achieve nearly lossless bit-switching while reducing storage requirements. Moreover, they also propose an Adaptive Learning Rate Scaling technique that adjusts learning rates dynamically for different precisions. Two proposed techniques mitigate the competitive interference between bit-widths caused by inconsistent gradients of different precisions during biased gradient estimation. They also extend their Double Rounding method to support one-shot mixed precision training and develop a Hessian-aware Bit-witdh sampling strategy. Experimental results on the ImageNet-1K classification task show that their methods outperform state-of-the-art one-shot joint QAT in both multi-precision and mixed-precision scenarios.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Eliminating the costs of retraining for mixed-precision quantization is a meaningful and challenging topic.

- The end-to-end experiments are sufficient, and the presentation is good.

Weaknesses:
- More uniquness analysis needed. The using of Hessian information seems a bit trivial, each layer's Hessian is just used to compare with the averaged Hessian trace. Firstly, as shown in recent zero-cost NAS research [1], the architectural proxies will be less effective as the training goes on, I'm not sure the Hessian information obtained on the initial full-precision model will remain useful as the quantization-aware training continues. Moreover, the sampling probability is modified with a simple ascending heuristic, which is not Hessian-aware. 

- Also applies here: the design of the double-rounding quantizer is similar to Bit-Mixer, Adabits, and ABN. Specifically, ABN also uses 

- ALRS needs further ablations. In ALRS, the authors use a fixed scaling ratio to bit-widths, e.g., 8-bit is 1, 6-bit is 0.1, and 4-bit is 0.01, the choice of these scaling factors still requires more ablation studies and discussions. 

- More comparisons needed. Since this paper adopts an ILP-based search algorithm to find optimal subnets, it is better to compare with these ILP-based mixed-precision quantization papers, e.g., [2] and [3]. 



[1] A Deeper Look at Zero-Cost Proxies for Lightweight NAS 
[2] Mixed-precision neural network quantization via learned layer-wise importance, ECCV 2022. 
[3] Hawq-v2: Hessian aware trace-weighted quantization of neural networks, NIPS 2020.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces DiGGR (Disentangled Generative Graph Representation Learning), a self-supervised learning framework that aims to guide graph mask modeling through disentangled latent factors to enhance the disentanglement of learned representations. Extensive experiments across 11 public datasets for node and graph classification tasks demonstrate the framework's effectiveness, significantly outperforming many existing self-supervised methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Innovative Approach: The DiGGR framework innovatively utilizes disentangled latent factors to guide graph mask modeling, a novel contribution in generative graph representation learning that significantly enhances the model's explainability and robustness.
Comprehensive Experiments: The paper conducts extensive experiments on multiple datasets and tasks, showing significant performance improvements over existing methods, thus providing strong empirical support for the proposed approach.

Weaknesses:
Complexity and Scalability: The framework appears computationally complex, which might limit its scalability to very large graphs or real-time applications. Unfortunately, this aspect is not extensively discussed in the paper.
Lack of Theoretical Analysis: While the empirical results are strong, the paper lacks a detailed theoretical analysis of why the disentanglement process improves performance, which could provide deeper insights into the method’s efficacy and limitations.

Limitations:
See Weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a framework called DiGGR, aimed at improving the robustness and explainability of generative graph models by addressing the issue of entangled graph representations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper tells the story in an easy-to-read way, and the whole paper is quite easy to follow.
2. The problem of disentangled learning is a very popular yet important task.
3. The paper conducts comprehensive experiments to evaluate their method.

Weaknesses:
1. Lack of novelty. Graph disentangled learning is not a new task. There are tons of existing methods for disentangled representation learning, such as those maximizing KL divergence or minimizing mutual information between two sets of representations. A lot of related works such as [1], [2], [3] and [4] are not discussed. Also node factorization is not a new idea, such as node clustering in [3].

[1] Disentangled graph collaborative filtering. SIGIR 2020.
[2] Disentangled Graph Convolutional Networks. ICML 2019.
[3] Deep Generative Model for Periodic Graphs. NeurIPS 2022.
[4] Disentangled contrastive learning on graphs. NeurIPS 2021.

2. The motivation of the proposed method is not clear to me. For example, why should we use mask? Also, why the proposed method sticks to GAE, not VGAE or other types of GNN, such as GCN, GIN or GAT?

Limitations:
Yes. Limitations have been discussed in the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work proposes a disentangled generative self-supervised learning method for graphs. The authors introduce a latent factor learning module to capture the heterogeneous factors in the nodes. The proposed method factorizes the graph into factor-specific subgraphs, and jointly trains a disentangled Graph MAE applying distinct masks for each subgraph. Experimental results demonstrate that DiGGR outperforms traditional methods that treat the graph holistically, without accounting for its latent structure.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method first explores a factorization method for generative graph SSL. 
2. The authors provide extensive experimental results and analysis on both node and graph-level tasks to show the improved effectiveness, interpretability, and generalization by using the proposed method.

Weaknesses:
- The computation complexity of the proposed method is quite high. Could the author pride training time comparison to the baseline methods to help us get a sense of the real complexity?
- Could the author provide more insights on how to find an optimal factor number K according to the statistics of diverse datasets? This might be useful for real-world applications.

Limitations:
Yes, the authors discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a self-supervised learning framework DiGGR, aimed at enhancing the disentanglement of learned graph representations. The authors argue that existing generative graph models tend to overlook the entanglement of learned representations, leading to non-robust and non-explainable models. DiGGR addresses this by introducing a latent factor learning module and a disentangled graph masked autoencoder, allowing for factor-wise graph representations. The framework is tested on various benchmarks, demonstrating its effectiveness in outperforming previous self-supervised methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper studies an interesting research problem that is disentangled graph representation learning. This research problem is very hot recently.

2. The model design is easy to understand. The paper provides a detailed explanation of the proposed model.

3. The experiments demonstrate the effectiveness of the model. The performance improvement on some comparisons seems to be significant.

Weaknesses:
1. One of my concerns is from the novelty. I think the model design is a little similar to the works [1-2]. The authors should make more comprehensive discussions to show the differences between them.

2. The experiments ignore some recent or related contrastive baselines [1-4] for comparisons.  The improvements on some datasets seem to be not significant. 

3.  More large-scale benchmarks should also be considered, e.g., OGB. The experimental settings are not very clear for reproducing the results.

[1] Disentangled contrastive learning on graphs. NeurIPS 2021.

[2] Disentangled Graph Contrastive Learning With Independence Promotion. TKDE 2022.

[3] Augmentation-Free Graph Contrastive Learning of Invariant-Discriminative Representations. TNNLS 2023.

[4] MA-GCL: Model Augmentation Tricks for Graph Contrastive Learning. AAAI 2023.

Limitations:
n/a

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces DiGGR (Disentangled Generative Graph Representation Learning), a self-supervised learning framework that aims to guide graph mask modeling through disentangled latent factors to enhance the disentanglement of learned representations. Extensive experiments across 11 public datasets for node and graph classification tasks demonstrate the framework's effectiveness, significantly outperforming many existing self-supervised methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Innovative Approach: The DiGGR framework innovatively utilizes disentangled latent factors to guide graph mask modeling, a novel contribution in generative graph representation learning that significantly enhances the model's explainability and robustness.
Comprehensive Experiments: The paper conducts extensive experiments on multiple datasets and tasks, showing significant performance improvements over existing methods, thus providing strong empirical support for the proposed approach.

Weaknesses:
Complexity and Scalability: The framework appears computationally complex, which might limit its scalability to very large graphs or real-time applications. Unfortunately, this aspect is not extensively discussed in the paper.
Lack of Theoretical Analysis: While the empirical results are strong, the paper lacks a detailed theoretical analysis of why the disentanglement process improves performance, which could provide deeper insights into the method’s efficacy and limitations.

Limitations:
See Weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a framework called DiGGR, aimed at improving the robustness and explainability of generative graph models by addressing the issue of entangled graph representations.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper tells the story in an easy-to-read way, and the whole paper is quite easy to follow.
2. The problem of disentangled learning is a very popular yet important task.
3. The paper conducts comprehensive experiments to evaluate their method.

Weaknesses:
1. Lack of novelty. Graph disentangled learning is not a new task. There are tons of existing methods for disentangled representation learning, such as those maximizing KL divergence or minimizing mutual information between two sets of representations. A lot of related works such as [1], [2], [3] and [4] are not discussed. Also node factorization is not a new idea, such as node clustering in [3].

[1] Disentangled graph collaborative filtering. SIGIR 2020.
[2] Disentangled Graph Convolutional Networks. ICML 2019.
[3] Deep Generative Model for Periodic Graphs. NeurIPS 2022.
[4] Disentangled contrastive learning on graphs. NeurIPS 2021.

2. The motivation of the proposed method is not clear to me. For example, why should we use mask? Also, why the proposed method sticks to GAE, not VGAE or other types of GNN, such as GCN, GIN or GAT?

Limitations:
Yes. Limitations have been discussed in the paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work proposes a disentangled generative self-supervised learning method for graphs. The authors introduce a latent factor learning module to capture the heterogeneous factors in the nodes. The proposed method factorizes the graph into factor-specific subgraphs, and jointly trains a disentangled Graph MAE applying distinct masks for each subgraph. Experimental results demonstrate that DiGGR outperforms traditional methods that treat the graph holistically, without accounting for its latent structure.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The proposed method first explores a factorization method for generative graph SSL. 
2. The authors provide extensive experimental results and analysis on both node and graph-level tasks to show the improved effectiveness, interpretability, and generalization by using the proposed method.

Weaknesses:
- The computation complexity of the proposed method is quite high. Could the author pride training time comparison to the baseline methods to help us get a sense of the real complexity?
- Could the author provide more insights on how to find an optimal factor number K according to the statistics of diverse datasets? This might be useful for real-world applications.

Limitations:
Yes, the authors discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a self-supervised learning framework DiGGR, aimed at enhancing the disentanglement of learned graph representations. The authors argue that existing generative graph models tend to overlook the entanglement of learned representations, leading to non-robust and non-explainable models. DiGGR addresses this by introducing a latent factor learning module and a disentangled graph masked autoencoder, allowing for factor-wise graph representations. The framework is tested on various benchmarks, demonstrating its effectiveness in outperforming previous self-supervised methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper studies an interesting research problem that is disentangled graph representation learning. This research problem is very hot recently.

2. The model design is easy to understand. The paper provides a detailed explanation of the proposed model.

3. The experiments demonstrate the effectiveness of the model. The performance improvement on some comparisons seems to be significant.

Weaknesses:
1. One of my concerns is from the novelty. I think the model design is a little similar to the works [1-2]. The authors should make more comprehensive discussions to show the differences between them.

2. The experiments ignore some recent or related contrastive baselines [1-4] for comparisons.  The improvements on some datasets seem to be not significant. 

3.  More large-scale benchmarks should also be considered, e.g., OGB. The experimental settings are not very clear for reproducing the results.

[1] Disentangled contrastive learning on graphs. NeurIPS 2021.

[2] Disentangled Graph Contrastive Learning With Independence Promotion. TKDE 2022.

[3] Augmentation-Free Graph Contrastive Learning of Invariant-Discriminative Representations. TNNLS 2023.

[4] MA-GCL: Model Augmentation Tricks for Graph Contrastive Learning. AAAI 2023.

Limitations:
n/a

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This work creates a hybrid LLM and classic planning algorithm, by integrating a LLM into the GraphPlan algorithm. The GraphPlan is an algorithm that solves a relaxed planning problem (forward expansion), and then traverses the created graph to find a valid plan (backtracking). Both steps are expensive. In the hybrid approach, a LLM is prompted in the forward expansion to limit the exploration of states deemed irrelevant. In the backtracking phase, the LLM is used to sort actions to explore first. Experiments with corrupted domain files show that LLMs can better handle corruption than the GraphPlan algorithm.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- A very interesting novel idea of a hybrid planning approach with a fundamental classic planning algorithm.
- The paper provides an introduction to an interesting research area of classical planning (e.g., Figure 4).

Weaknesses:
- Multiple missing experiments and discussions severely undermine the results of the paper.
    - It is not clearly motivated why experiments with corrupted pddl domain files are interesting. This was introduced quite suddenly in the *results section* (lines 261-262) without enough details and without providing motivation.
    - The paper is missing important discussion and experiments about the trade-off between the hybrid approach and the classic GP algorithm. Experiments with valid pddl domain files are not included, which could have alleviate it.  
    - The effect of hyperparameters on the results, such as the number of iterations (N) in Algorithm 1, is not discussed.
    - The failure of LLMs4PLAN-GPT3.5 compared to the phenomenal success of LLMs4Plan-GPT4 is somewhat unexpected and undermines the results of the paper.
- Multiple details are missing regarding the experimental setups. (see questions below)
- The paper's writing needs to be improved. (see suggestions below)

Limitations:
The authors did not discuss limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates how large language models (LLMs) can be integrated into established planning frameworks, specifically graph-based planning. The authors propose a novel framework called LLMs4Plan, which incorporates LLMs at two critical stages of the planning process: action selection during graph expansion and candidate action set generation during backtracking. The framework is tested across various planning domains, demonstrating improved efficiency and effectiveness in planning tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper's approach of embedding LLMs into graph-based planning is innovative and contributes to the field of automated planning.
2. The technical implementation of LLMs4Plan is well-detailed, with descriptions of how LLMs are utilized in action selection and candidate set generation.
3. The effectiveness of the proposed framework is empirically validated across ten planning domains, showcasing its practical applicability.

Weaknesses:
1. The proposed integration of LLMs into planning frameworks in LLMs4Plan may be complex and difficult to scale.
2. Comparisons with more recent LLM integrated planning baselines is limited.

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
There have been debates about the fundamental planning abilities of LLMs in planning tasks. To achieve more reliable performance, several recent works have embedded an LLM into a search framework (e.g., MCTS, BFS) and viewed LLMs as heuristics. Along this line, this work take a closer look at the roles LLMs can play in Planning Graph. It considers two tasks for LLMs: pruning actions and sorting actions (as heuristics).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written, with precise language and formalism.
- The experiment is conducted on over 10 domains, making it quite comprehensive.

Weaknesses:
1. My biggest concern with this work is that it restricts the use of LLMs to specific roles within a classical planning algorithm. There are many other roles LLMs can play in planning. For instance, see the recent LLM-modulo framework below. Instead of just filtering and ranking actions, LLMs have also been used to evaluate state values or rank plans (i.e., action sequences rather than individual actions).

    - Kambhampati, Subbarao, et al. ""Position: LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks."" ICML 2024

2. The evaluation based on the number of nodes explored is partial. We should not ignore the time cost (e.g., latency of calling LLMs) + financial cost of using commercial LLMs. It could be very likely that, although LLM+Graph Planning expands fewer nodes, it may take a longer wall-clock time to give the final outputs. I understand that the evaluation could be tricky and it remains an open question for a while. However, the authors should at least make an attempt to address this.

3. In the abstract, this statement is inaccurate: “works have been proposed to investigate the planning effectiveness of LLMs, without considering any utilization of off-the-shelf planning techniques in LLMs.” There have been quite some paper embedding LLMs in off-the-shelf planning algos

    - Zhao, Zirui, Wee Sun Lee, and David Hsu. ""Large language models as commonsense knowledge for large-scale task planning."" NeurIPS 2023.
    - Yao, Shunyu, et al. ""Tree of thoughts: Deliberate problem solving with large language models."" NeurIPS 2023.


4. While the corrupted domain model experiment looks interesting, it is unclear what messages it tries to convey. Specifically, why would one run the algo on top of a corrupted domain model when there exists approaches that can leverage LLMs to help complete the domain model before starting the search?

    - Guan, Lin, et al. ""Leveraging pre-trained large language models to construct and utilize world models for model-based task planning."" NeurIPS 2023
    - Wong, Lionel, et al. ""Learning adaptive planning representations with natural language guidance."" ICLR 2024.


5. The step of LLM-based action pruning can make the search incomplete, since an LLM may keep ignoring the required action(s) -- in other word, there is no guarantee that the LLM can produce a goal-reaching plan. I notice the authors mention this at a later section (which should be moved to earlier part) that including pruning probabilities could address the problem. I don’t fully agree with this. Can the authors give more detail on how pruning probabilities could guarantee completeness?

6. In the prompt (fig. 3), only the proposition set at the current state is provided. Did the authors consider including the running history of actions (i.e., the partial plan)? Would this affect the overall performance?

7. Line 109: typo in “Algorithm ??”

8. Several works (mentioned earlier) already show that LLMs can be useful heuristics. Can the authors elaborate on the new insights this work provides?

-----
Overall, this study provides a thorough evaluation of LLMs within the Planning Graph algorithm. I appreciate the comprehensiveness of the experiments. However, I also have concerns over the scope of this study (i.e., restricting itself to a limited set of roles). I need to discuss with other reviewers and the authors before finalizing my recommendation.

Limitations:
See the Weakness section.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to investigate integrating large language models (LLMs) into classical planning frameworks to enhance the planning effectiveness. The authors proposed a novel method named LLMs4Plan which integrates LLMs into action selection and mutual constraints solving within the graph-based planning framework. Evaluated across ten classic planning problems, this approach demonstrates improved success rates and reduced computational complexity compared to traditional methods. The study concludes that while LLMs alone are insufficient for planning, their integration into classical frameworks significantly boosts performance,.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper investigates an intriguing topic: the performance of LLMs in classical planning problems. While the impressive performance of LLMs in natural language processing and coding tasks is well-investigated, their efficacy in planning tasks remains largely unexplored. Understanding whether LLMs can replace classical planning algorithms is a significant and meaningful research question.
2. The paper conducts extensive experiments on ten classical planning problems, which enhances the credibility of its findings and conclusions. This comprehensive evaluation demonstrates the robustness of the proposed approach.
3. The paper reveals that LLMs still cannot surpass classical planning algorithms, thereby highlighting a valuable direction for future research. This insight encourages further investigation into how LLMs can be effectively integrated with traditional planning methods.

Weaknesses:
1. Although the authors point out that LLMs cannot outperform classical planning algorithms on their own and need to be integrated with classical methods to perform well, the paper lacks detailed insights on this integration. For example, specific strategies for integrating LLMs with the classic planning algorithms and the roles where LLMs excel within planning problems are not thoroughly discussed. The designed ""expandGraph"" and ""sortActions"" may not be the best practice manner. Future research directions to enhance the planning capabilities of LLMs should be more explicitly outlined.
2. The experiments are conducted in simulated planning domains, and the paper does not provide real-world applications or case studies to validate the practical utility of the approach. Including experimental results from more realistic scenarios would strengthen the paper.
3. While the method is effective for graph-based planning, its applicability to other planning frameworks or domains is not thoroughly investigated. A broader analysis could reveal the versatility of the proposed approach.
4. Typos: Algorithm ?? in Line 109.

Limitations:
See the Weaknesses part

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This work creates a hybrid LLM and classic planning algorithm, by integrating a LLM into the GraphPlan algorithm. The GraphPlan is an algorithm that solves a relaxed planning problem (forward expansion), and then traverses the created graph to find a valid plan (backtracking). Both steps are expensive. In the hybrid approach, a LLM is prompted in the forward expansion to limit the exploration of states deemed irrelevant. In the backtracking phase, the LLM is used to sort actions to explore first. Experiments with corrupted domain files show that LLMs can better handle corruption than the GraphPlan algorithm.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- A very interesting novel idea of a hybrid planning approach with a fundamental classic planning algorithm.
- The paper provides an introduction to an interesting research area of classical planning (e.g., Figure 4).

Weaknesses:
- Multiple missing experiments and discussions severely undermine the results of the paper.
    - It is not clearly motivated why experiments with corrupted pddl domain files are interesting. This was introduced quite suddenly in the *results section* (lines 261-262) without enough details and without providing motivation.
    - The paper is missing important discussion and experiments about the trade-off between the hybrid approach and the classic GP algorithm. Experiments with valid pddl domain files are not included, which could have alleviate it.  
    - The effect of hyperparameters on the results, such as the number of iterations (N) in Algorithm 1, is not discussed.
    - The failure of LLMs4PLAN-GPT3.5 compared to the phenomenal success of LLMs4Plan-GPT4 is somewhat unexpected and undermines the results of the paper.
- Multiple details are missing regarding the experimental setups. (see questions below)
- The paper's writing needs to be improved. (see suggestions below)

Limitations:
The authors did not discuss limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates how large language models (LLMs) can be integrated into established planning frameworks, specifically graph-based planning. The authors propose a novel framework called LLMs4Plan, which incorporates LLMs at two critical stages of the planning process: action selection during graph expansion and candidate action set generation during backtracking. The framework is tested across various planning domains, demonstrating improved efficiency and effectiveness in planning tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper's approach of embedding LLMs into graph-based planning is innovative and contributes to the field of automated planning.
2. The technical implementation of LLMs4Plan is well-detailed, with descriptions of how LLMs are utilized in action selection and candidate set generation.
3. The effectiveness of the proposed framework is empirically validated across ten planning domains, showcasing its practical applicability.

Weaknesses:
1. The proposed integration of LLMs into planning frameworks in LLMs4Plan may be complex and difficult to scale.
2. Comparisons with more recent LLM integrated planning baselines is limited.

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
There have been debates about the fundamental planning abilities of LLMs in planning tasks. To achieve more reliable performance, several recent works have embedded an LLM into a search framework (e.g., MCTS, BFS) and viewed LLMs as heuristics. Along this line, this work take a closer look at the roles LLMs can play in Planning Graph. It considers two tasks for LLMs: pruning actions and sorting actions (as heuristics).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written, with precise language and formalism.
- The experiment is conducted on over 10 domains, making it quite comprehensive.

Weaknesses:
1. My biggest concern with this work is that it restricts the use of LLMs to specific roles within a classical planning algorithm. There are many other roles LLMs can play in planning. For instance, see the recent LLM-modulo framework below. Instead of just filtering and ranking actions, LLMs have also been used to evaluate state values or rank plans (i.e., action sequences rather than individual actions).

    - Kambhampati, Subbarao, et al. ""Position: LLMs Can't Plan, But Can Help Planning in LLM-Modulo Frameworks."" ICML 2024

2. The evaluation based on the number of nodes explored is partial. We should not ignore the time cost (e.g., latency of calling LLMs) + financial cost of using commercial LLMs. It could be very likely that, although LLM+Graph Planning expands fewer nodes, it may take a longer wall-clock time to give the final outputs. I understand that the evaluation could be tricky and it remains an open question for a while. However, the authors should at least make an attempt to address this.

3. In the abstract, this statement is inaccurate: “works have been proposed to investigate the planning effectiveness of LLMs, without considering any utilization of off-the-shelf planning techniques in LLMs.” There have been quite some paper embedding LLMs in off-the-shelf planning algos

    - Zhao, Zirui, Wee Sun Lee, and David Hsu. ""Large language models as commonsense knowledge for large-scale task planning."" NeurIPS 2023.
    - Yao, Shunyu, et al. ""Tree of thoughts: Deliberate problem solving with large language models."" NeurIPS 2023.


4. While the corrupted domain model experiment looks interesting, it is unclear what messages it tries to convey. Specifically, why would one run the algo on top of a corrupted domain model when there exists approaches that can leverage LLMs to help complete the domain model before starting the search?

    - Guan, Lin, et al. ""Leveraging pre-trained large language models to construct and utilize world models for model-based task planning."" NeurIPS 2023
    - Wong, Lionel, et al. ""Learning adaptive planning representations with natural language guidance."" ICLR 2024.


5. The step of LLM-based action pruning can make the search incomplete, since an LLM may keep ignoring the required action(s) -- in other word, there is no guarantee that the LLM can produce a goal-reaching plan. I notice the authors mention this at a later section (which should be moved to earlier part) that including pruning probabilities could address the problem. I don’t fully agree with this. Can the authors give more detail on how pruning probabilities could guarantee completeness?

6. In the prompt (fig. 3), only the proposition set at the current state is provided. Did the authors consider including the running history of actions (i.e., the partial plan)? Would this affect the overall performance?

7. Line 109: typo in “Algorithm ??”

8. Several works (mentioned earlier) already show that LLMs can be useful heuristics. Can the authors elaborate on the new insights this work provides?

-----
Overall, this study provides a thorough evaluation of LLMs within the Planning Graph algorithm. I appreciate the comprehensiveness of the experiments. However, I also have concerns over the scope of this study (i.e., restricting itself to a limited set of roles). I need to discuss with other reviewers and the authors before finalizing my recommendation.

Limitations:
See the Weakness section.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper aims to investigate integrating large language models (LLMs) into classical planning frameworks to enhance the planning effectiveness. The authors proposed a novel method named LLMs4Plan which integrates LLMs into action selection and mutual constraints solving within the graph-based planning framework. Evaluated across ten classic planning problems, this approach demonstrates improved success rates and reduced computational complexity compared to traditional methods. The study concludes that while LLMs alone are insufficient for planning, their integration into classical frameworks significantly boosts performance,.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper investigates an intriguing topic: the performance of LLMs in classical planning problems. While the impressive performance of LLMs in natural language processing and coding tasks is well-investigated, their efficacy in planning tasks remains largely unexplored. Understanding whether LLMs can replace classical planning algorithms is a significant and meaningful research question.
2. The paper conducts extensive experiments on ten classical planning problems, which enhances the credibility of its findings and conclusions. This comprehensive evaluation demonstrates the robustness of the proposed approach.
3. The paper reveals that LLMs still cannot surpass classical planning algorithms, thereby highlighting a valuable direction for future research. This insight encourages further investigation into how LLMs can be effectively integrated with traditional planning methods.

Weaknesses:
1. Although the authors point out that LLMs cannot outperform classical planning algorithms on their own and need to be integrated with classical methods to perform well, the paper lacks detailed insights on this integration. For example, specific strategies for integrating LLMs with the classic planning algorithms and the roles where LLMs excel within planning problems are not thoroughly discussed. The designed ""expandGraph"" and ""sortActions"" may not be the best practice manner. Future research directions to enhance the planning capabilities of LLMs should be more explicitly outlined.
2. The experiments are conducted in simulated planning domains, and the paper does not provide real-world applications or case studies to validate the practical utility of the approach. Including experimental results from more realistic scenarios would strengthen the paper.
3. While the method is effective for graph-based planning, its applicability to other planning frameworks or domains is not thoroughly investigated. A broader analysis could reveal the versatility of the proposed approach.
4. Typos: Algorithm ?? in Line 109.

Limitations:
See the Weaknesses part

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper presents an off-policy hierarchical RL method, based on the HiT-MDP formulation of a Semi-MDP. The HiT-MDP formulation treats the option $o$ as an extension of the original state $s$ (which can be chosen by an extended action), and combines initialization-, termination- and option-policy in a single Markovian master policy $p(o\_{t}|s\_t,o\_{t-1})$. The policy in the extended state-action space, thus, decomposes into the high-level and low-level policies, $p(o\_{t}, a\_t | s\_t, o\_{t-1}) = p(o\_{t} | s\_t, o\_{t-1}) p(a\_t | s\_t, o\_{t})$, which can be trained using standard RL algorihms. 
Compared to the prior work, the paper makes the following contributions:
- Whereas previously PPO was used for reinforcement learning, the paper proposes to use SAC, resulting in improved sample efficiency
- The paper motivates the algorithm from a control-as-inference perspective

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The proposed method seems to be technically sound, and using off-policy agents for HiT-MDPs seems sensible. (Quality)

The provided code clarifies the implementation which helps reproducibility. (Quality)

The presentation is mostly clear. (Clarity)

Applying an off-policy agents to HiT-MDPs seems to be novel and effective (Origingality, Significance)

Weaknesses:
Originality
-----------
One of the main weaknesses of the submission is the limited novelty. Replacing the PPO agent of MOPG by a SAC agent seems to be straightforward, so this contribution is quite incremental. Indeed, the authors of HiT-MDP stated, that their ELBO ""can easily be extended to a SAC-like algorithm"" [35]. Furthermore, given that MaxEnt-RL was already derived from a control-as-inference perspective, deriving the special case of an HiT-MDP using this technique does not seem to be significant contribution either. I also don't see the value of this derivation that would justify devoting so much space on it; couldn't we just argue that we apply SAC to such particular form of an MDP?


Quality
---------
The experimental evaluation seems to be another weakness of the submission. While the method is evaluated on a reasonable number of MuJoCo environments, where it outperforms a reasonable number of baselines, the choice of baselines is not convincing because it looks like the method is only compared to on-policy algorithm. The submission claims that there method ""significantly outperforms existing on-policy and off-policy option variants"", but it is not clear to me to which off-policy baselines this claim refers to. It would be important to focus to flat and hierarchical off-policy methods in the experiments, such as [19], [50], [33] and Hao et. al (2023).  Furthermore, the choice of environments is not convincing, because it does not include more challenging long-horizon tasks that are typically used for evaluating HRL methods, such as Ant-Maze. While the performance on the standard locomotion environments is reasonable, the reported numbers don't seem to improve on the SOTA of flat-RL methods.   

The paper does not discuss the hyperparameter search although it states in the questionnary  that these details are provided in the main content and the appendix.

The paper argues that it did not perform any ablations due to limited computational resources. However, I don't find this argument very convincing, since the experiments are performed on simple vision-free locomotion tasks, that can be run on standard workstation, not even requiring any GPU. Ablations on the number of options would be very useful.



Clarity
---------
I found the background material on control-as-inference a bit confusing. In particular, line 106 which states states policy improvement constitues an M-Step of an EM algorithm that *maximizes* the KL towards $P(\tau|\mathcal{E})$. I don't think any practical algorithm involves such maximization, since the optimum would correspond to a delta distribution on the least-likely trajectory. (

Visually, the presentation is rather bad. Figures are not on the top, and in particular Fig. 2 seems to hide some text, since the sentence in line 271 ends with "", which"". Fig. 2 itself could be improved by increasing the plot sizes (there are some unnecessary white spaces) and by making the legend more readable.  


Significance
-----------------
While I think that the proposed combination of the HiT-MDP formulation and SAC is somewhat interesting, the submission does not provide a convincing argument for the method. When should I use it, instead of existing (hierarchical or flat) methods?

References
----------
* Hao, C., Weaver, C., Tang, C., Kawamoto, K., Tomizuka, M., & Zhan, W. (2023). Skill-critic: Refining learned skills for reinforcement learning. arXiv preprint arXiv:2306.08388.

Limitations:
The limitations are adequately discussed and I don't have any concerns regarding negative societal impact of the work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the Variational Markovian Option Critic (VMOC), an off-policy algorithm for hierarchical reinforcement learning. VMOC aims to address exploration inefficiency and update instability in existing methods. Key contributions include: 1. Use of variational inference for update stabilization 2. Low-cost option embeddings for improved scalability. The authors evaluate VMOC on Mujoco environments, comparing it to other on-policy and off-policy methods. They report improved performance in learning option sets for complex tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. The paper is well-written, and the proposed method is theoretically justified.
2. The empirical evaluations show favorable results compared with existing methods.

Weaknesses:
1. Very similar ideas of the variational option framework have been proposed in [33] (off-policy) and [35] (on-policy). While [35] proposes an on-policy version, its off-policy version is also straightforward to deduce following [ref1]. The use of option embeddings is following [35].
2. The empirical evaluations are very limited; there is no ablative evaluation reported, which makes it hard to determine the contribution of the proposed method to the overall performance gain over various baselines.

References: 
[ref1] Levine, Sergey. ""Reinforcement learning and control as probabilistic inference: Tutorial and review."" arXiv preprint arXiv:1805.00909 (2018).

Limitations:
The empirical evaluations, especially ablation studies, are somewhat limited in scope.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces the Variational Markovian Option Critic (VMOC) which combines variation policy iteration and the option critic. VMOC also modifies HiT-MDPs, where options are represented as latent embeddings rather than triples of (init states, policy, termination condition), to the off-policy setting. The paper performs comparisons to option-based methods and PPO on 10 Mujoco environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and easy-to-read. The figures clearly highlight the performance of the method. The translation from theory to the practical algorithm is well detailed.

2. The advantage in sample-efficiency over other option methods and PPO is clearly seen in Figure 2 across Mujoco environments. In fact, this gain looks to be in atleast two orders-of-magnitude (of fewer steps required by VMOC) which is amazing. The underlying MaxEnt objective in VMOC appears to be very useful with exploration in the high-dim mujoco envs.

Weaknesses:
1. It is not clear if this gain in sample-efficiency will transfer to discrete environments or is somehow applicable only in continuous envs. Perhaps the authors can perform comparisons on Atari or Procgen to demonstrate the same? It would be great if the authors could also discuss the changes in the algorithm in the discrete and continuous settings (perhaps such as the sampling of a_t from the replay buffer?)

2. It is unclear if all methods use the same number of options (e.g. the value used in VMOC appears to be 4). A clear ablation of various design choices like number of options would help demonstrate that VMOC is thoroughly better than the other option methods and is not brittle to hyperparameter choice. 

The analysis of the actual options learnt is also missing (this is for example seen in the option critic paper). This, alongside an analysis of the number of options, is crucial to understand if the method is actual learning composed actions that are further composable and generalizable or degenerating to something simple like learning the action primitives (although the latter would apply more to a discrete rather than continuous env).

3. Minor comment: The location of Theorem 1 in the preliminaries makes it unclear if it is a contribution of the authors or well-known statement. Perhaps the authors can clarify?

4. Another minor comment: It would be great if the authors could discuss other ways of combining options in the related work such as in [1] and [2].

[1] The Option Keyboard: Combining Skills in Reinforcement Learning, Barreto et al, NeurIPS 2019

[2] Exploring with Sticky Mittens: Reinforcement Learning with Expert Interventions via Option Templates, Dutta et al, CoRL 2022

Limitations:
The authors have addressed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces e Variational Markovian Option Critic (VMOC), which learns actions and options simultatenously. They build upon the Hidden Temporal Markovian Decision Process (HiT-MDP) [1] to build a novel off-policy algorithm that utilizes entropy augmented rewards. Their method learns options’ embedding vectors (rather than conventional option tuples utilized in Semi-MDP [2]). They benchmark the learning performance of their method against several competitors on many classic control benchmark environments. 

*References*
1. Li, C., Song, D., & Tao, D. (2023). Hit-MDP: learning the SMDP option framework on MDPs with hidden temporal embeddings. In The Eleventh International Conference on Learning Representations.
2. Sutton, R. S., Precup, D., & Singh, S. (1999). Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2), 181-211.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- Extensive comparison against 8 competitor algorithms on 10 benchmark tasks
- A novel Soft Option Policy Iteration Theorem

Weaknesses:
The paper does offer a potentially interesting contribution to the wider research community. But it is held back by the lack of clarity and polish in writing. For example, two glaring signs of a hasty submission:
1. Sec 4 and Sec 5 are both titled experiments. Sec 4 is only 1 paragraph, and it essentially repeats the same information from the introductory paragraphs of Sec 5
2. In Sec. 5, line 271 just trails off without completion. I believe the authors moved around the images to correct for vertical space and accidentally hid the text.

While the experimental results focus on learning curves, where VMOC does well, they fail to provide other relevant evaluation metrics:
1. What do the learned options look like? A good evaluation could follow Fig. 5 and Fig. 6 from [1]
2. How many options are learned? Digging through the appendix, it says that they learned 4 option vectors. This leads to another question: how do they choose the number of options to learn?
3. The VMOC algorithm listed in the appendix only describes the gradient update process. No details about action sampling or other hyper-parameter tuning are described here
4. The environments used are challenging for model-free RL algorithms. That said, they may not be satisfactory for showcasing the potential of learned options. 


*References*
1. Li, C., Song, D., & Tao, D. (2023). Hit-MDP: learning the SMDP option framework on MDPs with hidden temporal embeddings. In The Eleventh International Conference on Learning Representations.

Limitations:
Much like Soft Actor-Critic, this work develops a novel Soft Option Critic style algorithm. I believe this line of work is very interesting and potentially impactful in the near future. However, their current draft is not well-written and hard to follow. Their experimental evaluation is also insufficient.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper presents an off-policy hierarchical RL method, based on the HiT-MDP formulation of a Semi-MDP. The HiT-MDP formulation treats the option $o$ as an extension of the original state $s$ (which can be chosen by an extended action), and combines initialization-, termination- and option-policy in a single Markovian master policy $p(o\_{t}|s\_t,o\_{t-1})$. The policy in the extended state-action space, thus, decomposes into the high-level and low-level policies, $p(o\_{t}, a\_t | s\_t, o\_{t-1}) = p(o\_{t} | s\_t, o\_{t-1}) p(a\_t | s\_t, o\_{t})$, which can be trained using standard RL algorihms. 
Compared to the prior work, the paper makes the following contributions:
- Whereas previously PPO was used for reinforcement learning, the paper proposes to use SAC, resulting in improved sample efficiency
- The paper motivates the algorithm from a control-as-inference perspective

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
The proposed method seems to be technically sound, and using off-policy agents for HiT-MDPs seems sensible. (Quality)

The provided code clarifies the implementation which helps reproducibility. (Quality)

The presentation is mostly clear. (Clarity)

Applying an off-policy agents to HiT-MDPs seems to be novel and effective (Origingality, Significance)

Weaknesses:
Originality
-----------
One of the main weaknesses of the submission is the limited novelty. Replacing the PPO agent of MOPG by a SAC agent seems to be straightforward, so this contribution is quite incremental. Indeed, the authors of HiT-MDP stated, that their ELBO ""can easily be extended to a SAC-like algorithm"" [35]. Furthermore, given that MaxEnt-RL was already derived from a control-as-inference perspective, deriving the special case of an HiT-MDP using this technique does not seem to be significant contribution either. I also don't see the value of this derivation that would justify devoting so much space on it; couldn't we just argue that we apply SAC to such particular form of an MDP?


Quality
---------
The experimental evaluation seems to be another weakness of the submission. While the method is evaluated on a reasonable number of MuJoCo environments, where it outperforms a reasonable number of baselines, the choice of baselines is not convincing because it looks like the method is only compared to on-policy algorithm. The submission claims that there method ""significantly outperforms existing on-policy and off-policy option variants"", but it is not clear to me to which off-policy baselines this claim refers to. It would be important to focus to flat and hierarchical off-policy methods in the experiments, such as [19], [50], [33] and Hao et. al (2023).  Furthermore, the choice of environments is not convincing, because it does not include more challenging long-horizon tasks that are typically used for evaluating HRL methods, such as Ant-Maze. While the performance on the standard locomotion environments is reasonable, the reported numbers don't seem to improve on the SOTA of flat-RL methods.   

The paper does not discuss the hyperparameter search although it states in the questionnary  that these details are provided in the main content and the appendix.

The paper argues that it did not perform any ablations due to limited computational resources. However, I don't find this argument very convincing, since the experiments are performed on simple vision-free locomotion tasks, that can be run on standard workstation, not even requiring any GPU. Ablations on the number of options would be very useful.



Clarity
---------
I found the background material on control-as-inference a bit confusing. In particular, line 106 which states states policy improvement constitues an M-Step of an EM algorithm that *maximizes* the KL towards $P(\tau|\mathcal{E})$. I don't think any practical algorithm involves such maximization, since the optimum would correspond to a delta distribution on the least-likely trajectory. (

Visually, the presentation is rather bad. Figures are not on the top, and in particular Fig. 2 seems to hide some text, since the sentence in line 271 ends with "", which"". Fig. 2 itself could be improved by increasing the plot sizes (there are some unnecessary white spaces) and by making the legend more readable.  


Significance
-----------------
While I think that the proposed combination of the HiT-MDP formulation and SAC is somewhat interesting, the submission does not provide a convincing argument for the method. When should I use it, instead of existing (hierarchical or flat) methods?

References
----------
* Hao, C., Weaver, C., Tang, C., Kawamoto, K., Tomizuka, M., & Zhan, W. (2023). Skill-critic: Refining learned skills for reinforcement learning. arXiv preprint arXiv:2306.08388.

Limitations:
The limitations are adequately discussed and I don't have any concerns regarding negative societal impact of the work.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes the Variational Markovian Option Critic (VMOC), an off-policy algorithm for hierarchical reinforcement learning. VMOC aims to address exploration inefficiency and update instability in existing methods. Key contributions include: 1. Use of variational inference for update stabilization 2. Low-cost option embeddings for improved scalability. The authors evaluate VMOC on Mujoco environments, comparing it to other on-policy and off-policy methods. They report improved performance in learning option sets for complex tasks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. The paper is well-written, and the proposed method is theoretically justified.
2. The empirical evaluations show favorable results compared with existing methods.

Weaknesses:
1. Very similar ideas of the variational option framework have been proposed in [33] (off-policy) and [35] (on-policy). While [35] proposes an on-policy version, its off-policy version is also straightforward to deduce following [ref1]. The use of option embeddings is following [35].
2. The empirical evaluations are very limited; there is no ablative evaluation reported, which makes it hard to determine the contribution of the proposed method to the overall performance gain over various baselines.

References: 
[ref1] Levine, Sergey. ""Reinforcement learning and control as probabilistic inference: Tutorial and review."" arXiv preprint arXiv:1805.00909 (2018).

Limitations:
The empirical evaluations, especially ablation studies, are somewhat limited in scope.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces the Variational Markovian Option Critic (VMOC) which combines variation policy iteration and the option critic. VMOC also modifies HiT-MDPs, where options are represented as latent embeddings rather than triples of (init states, policy, termination condition), to the off-policy setting. The paper performs comparisons to option-based methods and PPO on 10 Mujoco environments.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-written and easy-to-read. The figures clearly highlight the performance of the method. The translation from theory to the practical algorithm is well detailed.

2. The advantage in sample-efficiency over other option methods and PPO is clearly seen in Figure 2 across Mujoco environments. In fact, this gain looks to be in atleast two orders-of-magnitude (of fewer steps required by VMOC) which is amazing. The underlying MaxEnt objective in VMOC appears to be very useful with exploration in the high-dim mujoco envs.

Weaknesses:
1. It is not clear if this gain in sample-efficiency will transfer to discrete environments or is somehow applicable only in continuous envs. Perhaps the authors can perform comparisons on Atari or Procgen to demonstrate the same? It would be great if the authors could also discuss the changes in the algorithm in the discrete and continuous settings (perhaps such as the sampling of a_t from the replay buffer?)

2. It is unclear if all methods use the same number of options (e.g. the value used in VMOC appears to be 4). A clear ablation of various design choices like number of options would help demonstrate that VMOC is thoroughly better than the other option methods and is not brittle to hyperparameter choice. 

The analysis of the actual options learnt is also missing (this is for example seen in the option critic paper). This, alongside an analysis of the number of options, is crucial to understand if the method is actual learning composed actions that are further composable and generalizable or degenerating to something simple like learning the action primitives (although the latter would apply more to a discrete rather than continuous env).

3. Minor comment: The location of Theorem 1 in the preliminaries makes it unclear if it is a contribution of the authors or well-known statement. Perhaps the authors can clarify?

4. Another minor comment: It would be great if the authors could discuss other ways of combining options in the related work such as in [1] and [2].

[1] The Option Keyboard: Combining Skills in Reinforcement Learning, Barreto et al, NeurIPS 2019

[2] Exploring with Sticky Mittens: Reinforcement Learning with Expert Interventions via Option Templates, Dutta et al, CoRL 2022

Limitations:
The authors have addressed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces e Variational Markovian Option Critic (VMOC), which learns actions and options simultatenously. They build upon the Hidden Temporal Markovian Decision Process (HiT-MDP) [1] to build a novel off-policy algorithm that utilizes entropy augmented rewards. Their method learns options’ embedding vectors (rather than conventional option tuples utilized in Semi-MDP [2]). They benchmark the learning performance of their method against several competitors on many classic control benchmark environments. 

*References*
1. Li, C., Song, D., & Tao, D. (2023). Hit-MDP: learning the SMDP option framework on MDPs with hidden temporal embeddings. In The Eleventh International Conference on Learning Representations.
2. Sutton, R. S., Precup, D., & Singh, S. (1999). Between MDPs and semi-MDPs: A framework for temporal abstraction in reinforcement learning. Artificial intelligence, 112(1-2), 181-211.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- Extensive comparison against 8 competitor algorithms on 10 benchmark tasks
- A novel Soft Option Policy Iteration Theorem

Weaknesses:
The paper does offer a potentially interesting contribution to the wider research community. But it is held back by the lack of clarity and polish in writing. For example, two glaring signs of a hasty submission:
1. Sec 4 and Sec 5 are both titled experiments. Sec 4 is only 1 paragraph, and it essentially repeats the same information from the introductory paragraphs of Sec 5
2. In Sec. 5, line 271 just trails off without completion. I believe the authors moved around the images to correct for vertical space and accidentally hid the text.

While the experimental results focus on learning curves, where VMOC does well, they fail to provide other relevant evaluation metrics:
1. What do the learned options look like? A good evaluation could follow Fig. 5 and Fig. 6 from [1]
2. How many options are learned? Digging through the appendix, it says that they learned 4 option vectors. This leads to another question: how do they choose the number of options to learn?
3. The VMOC algorithm listed in the appendix only describes the gradient update process. No details about action sampling or other hyper-parameter tuning are described here
4. The environments used are challenging for model-free RL algorithms. That said, they may not be satisfactory for showcasing the potential of learned options. 


*References*
1. Li, C., Song, D., & Tao, D. (2023). Hit-MDP: learning the SMDP option framework on MDPs with hidden temporal embeddings. In The Eleventh International Conference on Learning Representations.

Limitations:
Much like Soft Actor-Critic, this work develops a novel Soft Option Critic style algorithm. I believe this line of work is very interesting and potentially impactful in the near future. However, their current draft is not well-written and hard to follow. Their experimental evaluation is also insufficient.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The authors present two potential sources of error which can arise when composing sub-sampled DP mechanisms. On one hand, they discuss cases in which the composition of worst-case datasets does not yield the expected result, on the other hand, they disambiguate guarantees for mechanisms with Poisson sampling vs. sampling WOR.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
I appreciate that this paper points out some of the subtleties which arise regarding the distinctions between worst-case databases and dominating pairs and regarding the correct accounting of specific sampling schemes (Poisson/WOR). These subtleties can pass unnoticed, and lead to errors which compromise the privacy of individuals.

Weaknesses:
There is nothing particularly wrong with the paper. The facts stated are valid, and they are interesting, especially since they point out potential sources of confusion. However, none of this is surprising or even particularly novel. Most of this information is implied by earlier work (Zhu et al. in particular), and some of the facts stated here would be better suited as GitHub issues on the relevant accountants, followed by a technical report at a venue like TPDP or Journal of Privacy and Confidentiality. That is to say: I am not against this paper in general, but this is not a NEURIPS paper to me. It is a highly specialised piece of technical writing with a very narrow scope, and is likely to be of interest only to a very small community. I would recommend the authors to submit it to a venue which is better suited to its content.

Limitations:
The discussion on limitations is a bit lacking in my opinion. The authors state (in the checklist) that the ""main limitation is expressed in Conjecture 12"". Not being able to find a counterexample for a proposition is not really what one understands under the term ""limitation"" of a work. In particular, the supposed ""limitation"" is --by the authors' own admission-- easily resolved by just running the accountant on the two curves separately and taking the supremum. I would have much preferred an experimental section where the consequences of the pitfalls stated in the work are actually shown to affect the real-world use of DP-SGD or other mechanisms, and/or to see that specific privacy threats are practically enabled by overlooking these subtleties (e.g. through auditing).

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper examines the discrepancies between privacy accounting methods and their implementations, highlighting several cases where these mismatches lead to incorrect results. Specifically, it compares the noise requirements for achieving privacy guarantees under Poisson sampling versus sampling without replacement, and explores the limitations of worst-case dataset assumptions in subsampled mechanisms. Additionally, the authors address challenges in computing tight differential privacy (DP) bounds under the substitution relation of neighboring datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper addresses an important and timely topic in the field of differential privacy regarding privacy accounting.

- The findings have strong practical implications, potentially preventing unintended privacy breaches.

- The authors' message is well articulated, promoting better practices among DP practitioners.

- Despite critiquing existing methods, the authors maintain a respectful and constructive tone.

Weaknesses:
- The different messages of the paper may be convoluted sometimes, which makes the paper hard to follow.

- No viable technical solutions are provided for the identified issues, which might be a difficult research problem.

Limitations:
The authors discuss the limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The two main contributions of the paper is as follows:

the privacy guarantee of composition of subsampled mechanism may not be defined by worst-case dataset(s) for the underlying mechanism
Poisson subsampling and sampling without replacement may not have similar privacy guarantee.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper studies a very important problem of composition of subsampled privacy mechanism. There has been a lot of work in the recent past that performs a tight privacy accounting. This work is in the line of these works. These accounting results are used in deployment as well to show how much privacy loss has happened during training when using a prescribed noise scale. Based on these bounds, the training is stopped once we have expired the privacy budget. In this regard, their second result is very important because we definitely use subsampling without replacement in DP-SGD.

Weaknesses:
There are some typos, and the result for the gap is shown empirically. I have to state that I have not seen the Appendix so if the authors have a provable guarantee for this gap in the Appendix, please point me that. To me, the selling point of the paper is this result and it should be placed front and center. Most of the results that are given in the form of propositions and lemma are from previous works.

Limitations:
Mostly seem like an empirical study of the composition result.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the notion sampling with replacement for differential privacy. Most of the literature on machine learning with differential privacy benefits from privacy amplification by poisson sampling in the privacy analysis. However, when implementing the mechanisms, engineers ofter use the sub-sampling with replacement as a substitude for poisson sampling, mainly due to efficiency issues. This paper studies the gap between these two settings. Their main contributions are as follows: 

 - Identifying the Problem with DP-SGD Implementations: The authors highlight a critical issue with implementations of (DP-SGD). They argue that many implementations incorrectly assume that Poisson sampling and batch-sampling yield similar privacy guarantees, which is not necessarily true.

- Gap between Batch-Sampling and Poisson Sampling: The paper demonstrates a significant privacy gap between these two sampling methods. They provide an example showing that for certain hyperparameters, Poisson subsampling can result in an ϵ≈1, whereas batch-sampling without replacement can result in an ϵ>10. This discrepancy is critical for privacy accounting in DP-SGD.  Authors compare the privacy guarantees of Poisson subsampling and batch-sampling. They show that the privacy guarantees can differ significantly depending on the sampling technique used. Their analysis reveals that the method of sampling batches (Poisson vs. fixed-size) significantly impacts the resulting privacy guarantees, cautioning against the interchangeable use of different sampling techniques in privacy analysis.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Identifying an important problem with implementation of DP-SGD

Weaknesses:
- I have some concerns about the correctness of the results.

- There is not much technical novelty.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The authors present two potential sources of error which can arise when composing sub-sampled DP mechanisms. On one hand, they discuss cases in which the composition of worst-case datasets does not yield the expected result, on the other hand, they disambiguate guarantees for mechanisms with Poisson sampling vs. sampling WOR.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
I appreciate that this paper points out some of the subtleties which arise regarding the distinctions between worst-case databases and dominating pairs and regarding the correct accounting of specific sampling schemes (Poisson/WOR). These subtleties can pass unnoticed, and lead to errors which compromise the privacy of individuals.

Weaknesses:
There is nothing particularly wrong with the paper. The facts stated are valid, and they are interesting, especially since they point out potential sources of confusion. However, none of this is surprising or even particularly novel. Most of this information is implied by earlier work (Zhu et al. in particular), and some of the facts stated here would be better suited as GitHub issues on the relevant accountants, followed by a technical report at a venue like TPDP or Journal of Privacy and Confidentiality. That is to say: I am not against this paper in general, but this is not a NEURIPS paper to me. It is a highly specialised piece of technical writing with a very narrow scope, and is likely to be of interest only to a very small community. I would recommend the authors to submit it to a venue which is better suited to its content.

Limitations:
The discussion on limitations is a bit lacking in my opinion. The authors state (in the checklist) that the ""main limitation is expressed in Conjecture 12"". Not being able to find a counterexample for a proposition is not really what one understands under the term ""limitation"" of a work. In particular, the supposed ""limitation"" is --by the authors' own admission-- easily resolved by just running the accountant on the two curves separately and taking the supremum. I would have much preferred an experimental section where the consequences of the pitfalls stated in the work are actually shown to affect the real-world use of DP-SGD or other mechanisms, and/or to see that specific privacy threats are practically enabled by overlooking these subtleties (e.g. through auditing).

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper examines the discrepancies between privacy accounting methods and their implementations, highlighting several cases where these mismatches lead to incorrect results. Specifically, it compares the noise requirements for achieving privacy guarantees under Poisson sampling versus sampling without replacement, and explores the limitations of worst-case dataset assumptions in subsampled mechanisms. Additionally, the authors address challenges in computing tight differential privacy (DP) bounds under the substitution relation of neighboring datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The paper addresses an important and timely topic in the field of differential privacy regarding privacy accounting.

- The findings have strong practical implications, potentially preventing unintended privacy breaches.

- The authors' message is well articulated, promoting better practices among DP practitioners.

- Despite critiquing existing methods, the authors maintain a respectful and constructive tone.

Weaknesses:
- The different messages of the paper may be convoluted sometimes, which makes the paper hard to follow.

- No viable technical solutions are provided for the identified issues, which might be a difficult research problem.

Limitations:
The authors discuss the limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The two main contributions of the paper is as follows:

the privacy guarantee of composition of subsampled mechanism may not be defined by worst-case dataset(s) for the underlying mechanism
Poisson subsampling and sampling without replacement may not have similar privacy guarantee.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper studies a very important problem of composition of subsampled privacy mechanism. There has been a lot of work in the recent past that performs a tight privacy accounting. This work is in the line of these works. These accounting results are used in deployment as well to show how much privacy loss has happened during training when using a prescribed noise scale. Based on these bounds, the training is stopped once we have expired the privacy budget. In this regard, their second result is very important because we definitely use subsampling without replacement in DP-SGD.

Weaknesses:
There are some typos, and the result for the gap is shown empirically. I have to state that I have not seen the Appendix so if the authors have a provable guarantee for this gap in the Appendix, please point me that. To me, the selling point of the paper is this result and it should be placed front and center. Most of the results that are given in the form of propositions and lemma are from previous works.

Limitations:
Mostly seem like an empirical study of the composition result.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the notion sampling with replacement for differential privacy. Most of the literature on machine learning with differential privacy benefits from privacy amplification by poisson sampling in the privacy analysis. However, when implementing the mechanisms, engineers ofter use the sub-sampling with replacement as a substitude for poisson sampling, mainly due to efficiency issues. This paper studies the gap between these two settings. Their main contributions are as follows: 

 - Identifying the Problem with DP-SGD Implementations: The authors highlight a critical issue with implementations of (DP-SGD). They argue that many implementations incorrectly assume that Poisson sampling and batch-sampling yield similar privacy guarantees, which is not necessarily true.

- Gap between Batch-Sampling and Poisson Sampling: The paper demonstrates a significant privacy gap between these two sampling methods. They provide an example showing that for certain hyperparameters, Poisson subsampling can result in an ϵ≈1, whereas batch-sampling without replacement can result in an ϵ>10. This discrepancy is critical for privacy accounting in DP-SGD.  Authors compare the privacy guarantees of Poisson subsampling and batch-sampling. They show that the privacy guarantees can differ significantly depending on the sampling technique used. Their analysis reveals that the method of sampling batches (Poisson vs. fixed-size) significantly impacts the resulting privacy guarantees, cautioning against the interchangeable use of different sampling techniques in privacy analysis.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Identifying an important problem with implementation of DP-SGD

Weaknesses:
- I have some concerns about the correctness of the results.

- There is not much technical novelty.

Limitations:
Yes

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper implements a framework for evaluating LLMs’ social cognitive biases. The social science experiments are automatically collected by LLMs and then verified by humans. The framework includes two communication mode for interaction between multiple humans and multiple LLMs. The experiments include seven LLMs, across seven social science experiments. Results show that most LLMs show cognitive biases in the designed scenarios.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	The paper investigates a relatively underexplored area, which is the social cognitive biases in LLMs. This evaluation plays an important role in assessing the human-like ability of LLMs.
2.	The experiments include seven different models, providing insights into the comparison of their difference in abilities.
3.	The paper develops a framework and collects corresponding data for future use by more LLMs.

Weaknesses:
1.	The paper treats the scenario where LLMs have wrong beliefs (for example, apple is blue) as where LLMs have cognitive bias because of some external influence (observing many others’ choices, authority, etc.) However, there are no experiments showing that the wrong beliefs are caused by the external influence. I mean (though with a pretty low probability), what if without the external influence, LLMs themselves hold the belief that apple is blue? I think a better way is to measure the change of LLMs’ belief towards a concept without and with the external influence.
2.	The paraphrasing may not be a good choice to evaluate rumor chain effect, since continuing paraphrasing a sentence will indeed lower the similarity with the original one, and this is nothing to do with how message spreads in LLM agents. I believe the authors should design a scenario closer to daily communication.
3.	Since the constructed dataset is an essential part in the framework, how do you construct the dataset becomes important. Further explanations are needed about: How do LLMs automatically do the literature search? What is your manual selection criteria about the social science experiments?
4.	Presentation of the paper needs further improvement. There are too many module names in section 3 and readers can easily get confused with these messy concepts. Also, how the modules are organized is not clearly illustrated. A big problem is that some names in Figure 2 cannot match those in texts. For example, is “Mirror Settings” the same as “Environmental Settings?”

Minor suggestions to presentation:

1.	Please be consistent in the terminology. Currently some terms are “presocial” while others are “pre-social.”
2.	It will be better to make the four titles in Fig. 2 the same as introduced in section 3.

Limitations:
I think the author can mention that the current method does not verify LLMs’ original beliefs towards the knowledge in the proposed datasets.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates the potential for Large Language Model (LLM) agents to exhibit prosocial behavior through irrational decision-making, paralleling human cognitive biases. It introduces the CogMir framework, which leverages the hallucination properties of LLMs to simulate and assess social intelligence through various cognitive biases. Experimental results demonstrate that LLM agents and humans show high consistency in irrational and prosocial decision-making under uncertain conditions.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Innovative Framework: The introduction of the CogMir framework is a novel approach to studying social intelligence in LLMs by mirroring human cognitive biases.
2. Comprehensive Evaluation: The paper provides a detailed evaluation of multiple cognitive biases, such as Herd Effect, Authority Effect, and Confirmation Bias, among others.
3. Interdisciplinary Approach: Combining insights from social sciences and evolutionary psychology enriches the study and provides a broader context for understanding LLM behavior.

Weaknesses:
1. Why use hallucinations to mirror human cognitive biases? I think more explanation is required.
2. How to manipulate hallucination?
3. Why use all new datasets in experiments? Do existing datasets all don't have the data you want?
4. I don't think the conclusion is interesting.

Limitations:
Please compare with existing multi-agent social system and point out your advantages.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces **CogMir**, a novel framework designed to assess the social intelligence of LLM agents to mirror human cognitive biases. Through an evolutionary sociology perspective, the authors systematically evaluate the social intelligence of LLM agents, revealing their tendencies towards prosocial behaviors and irrational decision-making. The CogMir framework is applied to various cognitive bias scenarios, demonstrating high consistency between LLM agents and human behavior under uncertain conditions. The paper contributes to the understanding of LLM agents' social intelligence and provides a platform for further research.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Important research question: With the rapid development and application of LLM agents, the behavior studies of LLM agents especially under uncertain situations are getting more and more important.
2. Innovative framework: The introduction of CogMir is a significant contribution, offering a new way to evaluate and understand the social intelligence of LLM agents.
3. Open-ended design: CogMir's modular and dynamic design allows for continuous interpretative study and adaptation to future research needs.

Weaknesses:
1. The paper tries to demonstrate *LLM Agents can leverage hallucinations to mirror human cognitive biases*, while the experiments do not show how to measure hallucinations and what role hallucinations play here.
2. Human subjects are included in the experiments, while the recruitment and the details of them are missing. The paper claims LLM agents' behaviors are similar to humans, but there is no quantitative comparison between these two.

Limitations:
The paper includes the study of human-AI interaction but the ethical risks are not clearly stated. Such risks may also limit the research and application of this field.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores the potential of LLM agents to exhibit irrational social intelligence by mirroring human cognitive biases through their hallucination properties. The authors propose CogMir, a modular and dynamic multi-LLM agent framework that utilizes hallucination to assess and enhance social intelligence through cognitive biases.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1: The experiments explicitly compare LLM agent responses with known human cognitive biases, providing valuable insights into the similarities and differences between human and LLM decision-making processes.

S2: CogMir’s modular structure allows for flexibility in configuring experiments and exploring different social scenarios, making it adaptable for various research needs.

S3: CogMir’s open-ended nature encourages collaboration and further research, promoting the development and refinement of LLM agent social intelligence evaluation methodologies.

Weaknesses:
W1: The framework primarily focuses on language-based interactions, neglecting the simulation of non-verbal behaviors and their impact on social intelligence, limiting the scope of the analysis.

W2: The framework primarily focuses on language-based interactions, neglecting the simulation of non-verbal behaviors and their impact on social intelligence, limiting the scope of the analysis.

Limitations:
See limitation

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper implements a framework for evaluating LLMs’ social cognitive biases. The social science experiments are automatically collected by LLMs and then verified by humans. The framework includes two communication mode for interaction between multiple humans and multiple LLMs. The experiments include seven LLMs, across seven social science experiments. Results show that most LLMs show cognitive biases in the designed scenarios.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1.	The paper investigates a relatively underexplored area, which is the social cognitive biases in LLMs. This evaluation plays an important role in assessing the human-like ability of LLMs.
2.	The experiments include seven different models, providing insights into the comparison of their difference in abilities.
3.	The paper develops a framework and collects corresponding data for future use by more LLMs.

Weaknesses:
1.	The paper treats the scenario where LLMs have wrong beliefs (for example, apple is blue) as where LLMs have cognitive bias because of some external influence (observing many others’ choices, authority, etc.) However, there are no experiments showing that the wrong beliefs are caused by the external influence. I mean (though with a pretty low probability), what if without the external influence, LLMs themselves hold the belief that apple is blue? I think a better way is to measure the change of LLMs’ belief towards a concept without and with the external influence.
2.	The paraphrasing may not be a good choice to evaluate rumor chain effect, since continuing paraphrasing a sentence will indeed lower the similarity with the original one, and this is nothing to do with how message spreads in LLM agents. I believe the authors should design a scenario closer to daily communication.
3.	Since the constructed dataset is an essential part in the framework, how do you construct the dataset becomes important. Further explanations are needed about: How do LLMs automatically do the literature search? What is your manual selection criteria about the social science experiments?
4.	Presentation of the paper needs further improvement. There are too many module names in section 3 and readers can easily get confused with these messy concepts. Also, how the modules are organized is not clearly illustrated. A big problem is that some names in Figure 2 cannot match those in texts. For example, is “Mirror Settings” the same as “Environmental Settings?”

Minor suggestions to presentation:

1.	Please be consistent in the terminology. Currently some terms are “presocial” while others are “pre-social.”
2.	It will be better to make the four titles in Fig. 2 the same as introduced in section 3.

Limitations:
I think the author can mention that the current method does not verify LLMs’ original beliefs towards the knowledge in the proposed datasets.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper investigates the potential for Large Language Model (LLM) agents to exhibit prosocial behavior through irrational decision-making, paralleling human cognitive biases. It introduces the CogMir framework, which leverages the hallucination properties of LLMs to simulate and assess social intelligence through various cognitive biases. Experimental results demonstrate that LLM agents and humans show high consistency in irrational and prosocial decision-making under uncertain conditions.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Innovative Framework: The introduction of the CogMir framework is a novel approach to studying social intelligence in LLMs by mirroring human cognitive biases.
2. Comprehensive Evaluation: The paper provides a detailed evaluation of multiple cognitive biases, such as Herd Effect, Authority Effect, and Confirmation Bias, among others.
3. Interdisciplinary Approach: Combining insights from social sciences and evolutionary psychology enriches the study and provides a broader context for understanding LLM behavior.

Weaknesses:
1. Why use hallucinations to mirror human cognitive biases? I think more explanation is required.
2. How to manipulate hallucination?
3. Why use all new datasets in experiments? Do existing datasets all don't have the data you want?
4. I don't think the conclusion is interesting.

Limitations:
Please compare with existing multi-agent social system and point out your advantages.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces **CogMir**, a novel framework designed to assess the social intelligence of LLM agents to mirror human cognitive biases. Through an evolutionary sociology perspective, the authors systematically evaluate the social intelligence of LLM agents, revealing their tendencies towards prosocial behaviors and irrational decision-making. The CogMir framework is applied to various cognitive bias scenarios, demonstrating high consistency between LLM agents and human behavior under uncertain conditions. The paper contributes to the understanding of LLM agents' social intelligence and provides a platform for further research.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Important research question: With the rapid development and application of LLM agents, the behavior studies of LLM agents especially under uncertain situations are getting more and more important.
2. Innovative framework: The introduction of CogMir is a significant contribution, offering a new way to evaluate and understand the social intelligence of LLM agents.
3. Open-ended design: CogMir's modular and dynamic design allows for continuous interpretative study and adaptation to future research needs.

Weaknesses:
1. The paper tries to demonstrate *LLM Agents can leverage hallucinations to mirror human cognitive biases*, while the experiments do not show how to measure hallucinations and what role hallucinations play here.
2. Human subjects are included in the experiments, while the recruitment and the details of them are missing. The paper claims LLM agents' behaviors are similar to humans, but there is no quantitative comparison between these two.

Limitations:
The paper includes the study of human-AI interaction but the ethical risks are not clearly stated. Such risks may also limit the research and application of this field.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper explores the potential of LLM agents to exhibit irrational social intelligence by mirroring human cognitive biases through their hallucination properties. The authors propose CogMir, a modular and dynamic multi-LLM agent framework that utilizes hallucination to assess and enhance social intelligence through cognitive biases.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
S1: The experiments explicitly compare LLM agent responses with known human cognitive biases, providing valuable insights into the similarities and differences between human and LLM decision-making processes.

S2: CogMir’s modular structure allows for flexibility in configuring experiments and exploring different social scenarios, making it adaptable for various research needs.

S3: CogMir’s open-ended nature encourages collaboration and further research, promoting the development and refinement of LLM agent social intelligence evaluation methodologies.

Weaknesses:
W1: The framework primarily focuses on language-based interactions, neglecting the simulation of non-verbal behaviors and their impact on social intelligence, limiting the scope of the analysis.

W2: The framework primarily focuses on language-based interactions, neglecting the simulation of non-verbal behaviors and their impact on social intelligence, limiting the scope of the analysis.

Limitations:
See limitation

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper tries to formalize the concept of generalizability in experimental studies in machine learning research. It relies on three different types of kernels in order to quantify difference between the rankings in an experiment output. A core contribution is the development of an algorithm for estimating the number of experimental studies needed in order to generalize the results at a desired level.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The topic is interesting and worthwhile.
- The paper is clearly written.
- The formalization of generalizability is well-defined and nicely parameterized through the use of kernels.
- The practical usefulness of the algorithm is somewhat unclear to me.

Weaknesses:
- There is no discussion on the computational costs of the algorithm (except for a vague statement that it is very fast in the checklist).
- The empirical evidence for the algorithm's effectiveness appears somewhat weak to me (see respective item in _Questions_).
- The python package is not properly configured I think. I see this after having installed the package into a virtual environment with the correct python version and
  using `pip install . -r requirements.txt`:

  ```python
  In [1]: import genexpy
  ---------------------------------------------------------------------------
  ImportError                               Traceback (most recent call last)
  Cell In[1], line 1
  ----> 1 import genexpy

  File ~/.pyenv/versions/genexpy/lib/python3.11/site-packages/genexpy/__init__.py:4
        2 from .src import lower_bounds
        3 from .src import mmd
  ----> 4 from .src import probability_distributions
        5 from .src import rankings_utils
        6 from .src import relation_utils

  File ~/.pyenv/versions/genexpy/lib/python3.11/site-packages/genexpy/src/probability_distributions.py
  :11
        8 from typing import Literal
      10 from genexpy import kernels as ku
  ---> 11 from genexpy import rankings_utils as ru
      12 from genexpy import relation_utils as rlu
      15 def sample_from_sphere(na: int, n: int, rng: np.random.Generator) -> np.ndarray[float]:

  ImportError: cannot import name 'rankings_utils' from partially initialized module 'genexpy' (most l
  ikely due to a circular import) (/home/<anonymous_reviewer>/.pyenv/versions/genexpy/lib/python3.11/site-packages
  /genexpy/__init__.py)

Limitations:
Some of the limitations are discussed but I still think the paper could be more self-critical of for instance $n^*$. Possible computational costs are also not discussed.

There are no potential negative societal impacts of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper deals with experimental studies. After providing a mathematical formalization, it focuses on the generalizability of these studies. The main contribution is a quantitative estimate of the the size of the study to obtain generalizable results. Experiments on LLMs are conducted.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- [mathematical formulation] It is nice to have a solid formulation of experimental studies, this is quite relevant to the community.

Weaknesses:
- [train / test split] A concrete problem in machine learning practical experimentation is that of train / test split, and more particularly its absence (that is, training on the test). I do not see this issue discussed in the paper. Can it be incorporated in the setting? Is it possible to clarify whether the paper assumes that the training is done on a training set without calibration on a validation set or is this hidden somewhere? What would then be the influence on the number of experiments?
- [testing between rankings] If I understand correctly, the paper proposes (in Section 4.1) to check whether rankings are consistent by performing kernel two-sample test, with adapted kernels. This does not seem standard to me: there exists some ad-hoc statistical tests (e.g., Kendall's \tau, Spearman's \rho, etc.). Why not use them directly? Is there an advantage to using MMD?  

- [minor comments]:
  - missing ref line 111
  - repeated word ('of') line 300

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tries to formalise the notion of an experimental study by considering the sampling process of acquiring a dataset.  It then uses this notion to argue about generalisability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The problem of understanding the performance of machine learning when tested on new data is a very important problem.  The authors use some technically sophisticate methods to tackle this problem.

Weaknesses:
For me the authors model of an experiment is too simplistic and does not capture the problems faced by machine learning.  If we collect medical data then that data is likely to vary depending on the equipment used, the clinicians running the equipment and population where the data comes from.  These kinds of variations are the bugbear for machine learning, but not captured at all by the model.  Another issue is that a lot of data is non-stationary.  Even in the much used example of checkmate in one.  If a machine learns this very well, then players against the machine are likely to learn their mistake and alter their play.  Thus, I am not convinced that the model being proposed is particularly interesting.

Limitations:
This is fine.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors provide a formalism for the generalizability of experimental studies in ML.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Anything pushing to get better practices in evaluation of ML is very important.

Weaknesses:
I could quibble with some of the setup, which is a bit confusing to me: design factors being properties of the context rather than of the alternative, for example, is kind of odd, but I don't think this is very important.

The big problem is that there is a huge literature on a very similar problem and I have very little sense of how this work connects to it: starting with the Neyman-Pearson lemma and going down through the standard corpus of decision theory, we have a lot of statistical tools for thinking about this problem in very broad strokes. After reading this paper, I have a sense that you're trying to solve a very similar problem (given a sample from some population, what can I say about the reliability of my estimate? How many samples would I need to be sure that it's reliable?)

Section 4.3 seems to be rederiving some form of power analysis.

Looking at A.3.3, it appears that the procedure is essentially the following:
(1) [the inner loop from 1...n_rep] construct a null hypothesis at a given sample size, find the upper alpha quantile of that null hypothesis
(2) Repeat this at a variety of sample sizes
(3) Estimate a power-law relationship between sample size and the upper-alpha quantiles
(4) Predict the sample size which would have such an upper-alpha quantile

This procedure is an empirical version of power analysis where the null distribution is not known but simulated and extrapolated. If I know the type-I error rate, type-II error rate and a distribution under the null and under the alternative, deriving the required sample size is straightforward. Indeed, we have a CLT for MMD (at least under some kernels) [1], so these distributions are known asymptotically, which is likely plenty for the purposes of sample size determination. Do \alpha^* and \delta^* map onto concepts from Neyman-Pearson? It's entirely possible.

This is an important question because decision theory has very well established results on things like uniformly most powerful tests. When we just invent a new framework rather than relying on well-trod ones, we are likely to derive suboptimal procedures unless we compare very carefully to these existing procedures. There's no similarly sophisticated discussion of error properties in this paper, which would be reasonable if this were truly the first paper in its vein, but I don't think that's the case.

Further, it's not clear to me why these similarities between rankings should be the target of inference. Rather, shouldn't I care about whether, based on the sample of allowed-to-vary factors I've used, alternative A is preferred to alternative B? This is an extremely standard matter of decision theory as far as I can tell. By moving to these more complicated research questions about rankings it clouds this fact, but I'm not sure it needs to. If the target of inference were instead to be a rank of K alternatives, I believe a decision theorist would take a somewhat similar approach to what you've done here: define a similarity metric based on the research question. An example solution to a problem like this would be [2], [3]. I just don't see why we need this new framework to accomplish a task I think we already have the tools for.

It's entirely possible that there's a contribution here, but it can't just be ""this is a new task"". We have methods from decision theory that have been designed for a wide range of decision tasks, and its incumbent upon the authors to demonstrate why those existing tools do not fit the task in front of them.

[1] https://www.jmlr.org/papers/volume24/22-1136/22-1136.pdf
[2] https://onlinelibrary.wiley.com/doi/abs/10.1002/mcda.313
[3] https://www.sciencedirect.com/science/article/abs/pii/S0377221715008048

Limitations:
see above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a new mathematical framework and a corresponding new algorithm to evaluate the generalizability of published experimental studies, by adapting Montgomery's classification of experimental factors [44].
They demonstrate the efficacy of this framework in evaluating the generalizability of two popular published experimental studies.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper appears to be theoretical strong. 
It goes beyond the standard notion of reproducibility of an experimental study and comes up a definition of generalizability of an experimental study.
Morever, Proposition 4.2 provides a theoretical result on the sample size $n$ necessary to obtained a desired generalizability $\alpha^*$ for a desired similarity $\epsilon^*$, and the authors also provide an algorithm (A.3.3) to compute this sample size.
Since the similarity $\epsilon^*$ is hard to specify, they make it a function of the kernelized distance between rankings $\delta^*$.

2.
The empirical evaluation in Fig. 2 and Fig. 3, on the categorical encoder comparison from [41] or the BIG-bench framework for LLM comparison from [55], respectively, demonstrates the practical utility of the proposed approach in determining sample sizes to guarantee generalizability.

Weaknesses:
1. The clarity in the writing can be significantly improved:

1.A. Symbols are used before defining them, typos exist, and symbols are not used consistently:

1.A.a. On line 118, the symbol $\mathcal{R}_{n_a}$ is mentioned, but the relation of this symbol to the ranking on alternatives only becomes clear later in Definition 3.1.

1.A.b. The Section number is missing on line 111.

1.A.c. The symbol $m$, is defined as the number of shots, on line 115, whereas line 114 uses the symbol $n$ rather than $m$. Moreover, on line 88, $n$ is defined as the number of shots.

1.A.d. In contrast to 1.A.c, in eq. (1), after line 170, the symbol $n$ is now used without providing a definition. It now appears to be the size of any study, in a general definition, rather than the number of shots, as defined on line 88. 

1.A.e. On Sec. 5.3, line 315, $N$ is defined as the number of preliminary experiments, whereas on line 154, it is defined as the size of the sample of valid experimental conditions. Do these mean the same thing ?

1.B. Sec. 3.1 defines a ranking of alternatives as the primary result of an experimental study.
However, the effect size, i.e., the magnitude and sign of the difference between two alternatives, can be important in certain experiments.
The MMD kernel, used in Sec. 4.2, actually allows measuring this effect size, as discussed in [27], but the limitations imposed by the usage of this MMD kernel within the author's generalizability framework, are not clear despite the somewhat cryptic discussion in Sec. 6.

2. 
The experimental evaluation is limited to a comparison of ranking differences between alternatives, and does not include a measurement of the practical differences between alternatives, or the significance of these differences.

Limitations:
Please refer to the potential limitation underlying weakness #2. Is it possible to quantify the magnitude of differences between alternatives using the generalizability framework provided by the authors ? The authors mention this limitation in Sec. 6, but it is not clear why the MMD kernel cannot quantify magnitude of differences.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper tries to formalize the concept of generalizability in experimental studies in machine learning research. It relies on three different types of kernels in order to quantify difference between the rankings in an experiment output. A core contribution is the development of an algorithm for estimating the number of experimental studies needed in order to generalize the results at a desired level.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The topic is interesting and worthwhile.
- The paper is clearly written.
- The formalization of generalizability is well-defined and nicely parameterized through the use of kernels.
- The practical usefulness of the algorithm is somewhat unclear to me.

Weaknesses:
- There is no discussion on the computational costs of the algorithm (except for a vague statement that it is very fast in the checklist).
- The empirical evidence for the algorithm's effectiveness appears somewhat weak to me (see respective item in _Questions_).
- The python package is not properly configured I think. I see this after having installed the package into a virtual environment with the correct python version and
  using `pip install . -r requirements.txt`:

  ```python
  In [1]: import genexpy
  ---------------------------------------------------------------------------
  ImportError                               Traceback (most recent call last)
  Cell In[1], line 1
  ----> 1 import genexpy

  File ~/.pyenv/versions/genexpy/lib/python3.11/site-packages/genexpy/__init__.py:4
        2 from .src import lower_bounds
        3 from .src import mmd
  ----> 4 from .src import probability_distributions
        5 from .src import rankings_utils
        6 from .src import relation_utils

  File ~/.pyenv/versions/genexpy/lib/python3.11/site-packages/genexpy/src/probability_distributions.py
  :11
        8 from typing import Literal
      10 from genexpy import kernels as ku
  ---> 11 from genexpy import rankings_utils as ru
      12 from genexpy import relation_utils as rlu
      15 def sample_from_sphere(na: int, n: int, rng: np.random.Generator) -> np.ndarray[float]:

  ImportError: cannot import name 'rankings_utils' from partially initialized module 'genexpy' (most l
  ikely due to a circular import) (/home/<anonymous_reviewer>/.pyenv/versions/genexpy/lib/python3.11/site-packages
  /genexpy/__init__.py)

Limitations:
Some of the limitations are discussed but I still think the paper could be more self-critical of for instance $n^*$. Possible computational costs are also not discussed.

There are no potential negative societal impacts of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper deals with experimental studies. After providing a mathematical formalization, it focuses on the generalizability of these studies. The main contribution is a quantitative estimate of the the size of the study to obtain generalizable results. Experiments on LLMs are conducted.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- [mathematical formulation] It is nice to have a solid formulation of experimental studies, this is quite relevant to the community.

Weaknesses:
- [train / test split] A concrete problem in machine learning practical experimentation is that of train / test split, and more particularly its absence (that is, training on the test). I do not see this issue discussed in the paper. Can it be incorporated in the setting? Is it possible to clarify whether the paper assumes that the training is done on a training set without calibration on a validation set or is this hidden somewhere? What would then be the influence on the number of experiments?
- [testing between rankings] If I understand correctly, the paper proposes (in Section 4.1) to check whether rankings are consistent by performing kernel two-sample test, with adapted kernels. This does not seem standard to me: there exists some ad-hoc statistical tests (e.g., Kendall's \tau, Spearman's \rho, etc.). Why not use them directly? Is there an advantage to using MMD?  

- [minor comments]:
  - missing ref line 111
  - repeated word ('of') line 300

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tries to formalise the notion of an experimental study by considering the sampling process of acquiring a dataset.  It then uses this notion to argue about generalisability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The problem of understanding the performance of machine learning when tested on new data is a very important problem.  The authors use some technically sophisticate methods to tackle this problem.

Weaknesses:
For me the authors model of an experiment is too simplistic and does not capture the problems faced by machine learning.  If we collect medical data then that data is likely to vary depending on the equipment used, the clinicians running the equipment and population where the data comes from.  These kinds of variations are the bugbear for machine learning, but not captured at all by the model.  Another issue is that a lot of data is non-stationary.  Even in the much used example of checkmate in one.  If a machine learns this very well, then players against the machine are likely to learn their mistake and alter their play.  Thus, I am not convinced that the model being proposed is particularly interesting.

Limitations:
This is fine.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors provide a formalism for the generalizability of experimental studies in ML.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Anything pushing to get better practices in evaluation of ML is very important.

Weaknesses:
I could quibble with some of the setup, which is a bit confusing to me: design factors being properties of the context rather than of the alternative, for example, is kind of odd, but I don't think this is very important.

The big problem is that there is a huge literature on a very similar problem and I have very little sense of how this work connects to it: starting with the Neyman-Pearson lemma and going down through the standard corpus of decision theory, we have a lot of statistical tools for thinking about this problem in very broad strokes. After reading this paper, I have a sense that you're trying to solve a very similar problem (given a sample from some population, what can I say about the reliability of my estimate? How many samples would I need to be sure that it's reliable?)

Section 4.3 seems to be rederiving some form of power analysis.

Looking at A.3.3, it appears that the procedure is essentially the following:
(1) [the inner loop from 1...n_rep] construct a null hypothesis at a given sample size, find the upper alpha quantile of that null hypothesis
(2) Repeat this at a variety of sample sizes
(3) Estimate a power-law relationship between sample size and the upper-alpha quantiles
(4) Predict the sample size which would have such an upper-alpha quantile

This procedure is an empirical version of power analysis where the null distribution is not known but simulated and extrapolated. If I know the type-I error rate, type-II error rate and a distribution under the null and under the alternative, deriving the required sample size is straightforward. Indeed, we have a CLT for MMD (at least under some kernels) [1], so these distributions are known asymptotically, which is likely plenty for the purposes of sample size determination. Do \alpha^* and \delta^* map onto concepts from Neyman-Pearson? It's entirely possible.

This is an important question because decision theory has very well established results on things like uniformly most powerful tests. When we just invent a new framework rather than relying on well-trod ones, we are likely to derive suboptimal procedures unless we compare very carefully to these existing procedures. There's no similarly sophisticated discussion of error properties in this paper, which would be reasonable if this were truly the first paper in its vein, but I don't think that's the case.

Further, it's not clear to me why these similarities between rankings should be the target of inference. Rather, shouldn't I care about whether, based on the sample of allowed-to-vary factors I've used, alternative A is preferred to alternative B? This is an extremely standard matter of decision theory as far as I can tell. By moving to these more complicated research questions about rankings it clouds this fact, but I'm not sure it needs to. If the target of inference were instead to be a rank of K alternatives, I believe a decision theorist would take a somewhat similar approach to what you've done here: define a similarity metric based on the research question. An example solution to a problem like this would be [2], [3]. I just don't see why we need this new framework to accomplish a task I think we already have the tools for.

It's entirely possible that there's a contribution here, but it can't just be ""this is a new task"". We have methods from decision theory that have been designed for a wide range of decision tasks, and its incumbent upon the authors to demonstrate why those existing tools do not fit the task in front of them.

[1] https://www.jmlr.org/papers/volume24/22-1136/22-1136.pdf
[2] https://onlinelibrary.wiley.com/doi/abs/10.1002/mcda.313
[3] https://www.sciencedirect.com/science/article/abs/pii/S0377221715008048

Limitations:
see above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a new mathematical framework and a corresponding new algorithm to evaluate the generalizability of published experimental studies, by adapting Montgomery's classification of experimental factors [44].
They demonstrate the efficacy of this framework in evaluating the generalizability of two popular published experimental studies.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper appears to be theoretical strong. 
It goes beyond the standard notion of reproducibility of an experimental study and comes up a definition of generalizability of an experimental study.
Morever, Proposition 4.2 provides a theoretical result on the sample size $n$ necessary to obtained a desired generalizability $\alpha^*$ for a desired similarity $\epsilon^*$, and the authors also provide an algorithm (A.3.3) to compute this sample size.
Since the similarity $\epsilon^*$ is hard to specify, they make it a function of the kernelized distance between rankings $\delta^*$.

2.
The empirical evaluation in Fig. 2 and Fig. 3, on the categorical encoder comparison from [41] or the BIG-bench framework for LLM comparison from [55], respectively, demonstrates the practical utility of the proposed approach in determining sample sizes to guarantee generalizability.

Weaknesses:
1. The clarity in the writing can be significantly improved:

1.A. Symbols are used before defining them, typos exist, and symbols are not used consistently:

1.A.a. On line 118, the symbol $\mathcal{R}_{n_a}$ is mentioned, but the relation of this symbol to the ranking on alternatives only becomes clear later in Definition 3.1.

1.A.b. The Section number is missing on line 111.

1.A.c. The symbol $m$, is defined as the number of shots, on line 115, whereas line 114 uses the symbol $n$ rather than $m$. Moreover, on line 88, $n$ is defined as the number of shots.

1.A.d. In contrast to 1.A.c, in eq. (1), after line 170, the symbol $n$ is now used without providing a definition. It now appears to be the size of any study, in a general definition, rather than the number of shots, as defined on line 88. 

1.A.e. On Sec. 5.3, line 315, $N$ is defined as the number of preliminary experiments, whereas on line 154, it is defined as the size of the sample of valid experimental conditions. Do these mean the same thing ?

1.B. Sec. 3.1 defines a ranking of alternatives as the primary result of an experimental study.
However, the effect size, i.e., the magnitude and sign of the difference between two alternatives, can be important in certain experiments.
The MMD kernel, used in Sec. 4.2, actually allows measuring this effect size, as discussed in [27], but the limitations imposed by the usage of this MMD kernel within the author's generalizability framework, are not clear despite the somewhat cryptic discussion in Sec. 6.

2. 
The experimental evaluation is limited to a comparison of ranking differences between alternatives, and does not include a measurement of the practical differences between alternatives, or the significance of these differences.

Limitations:
Please refer to the potential limitation underlying weakness #2. Is it possible to quantify the magnitude of differences between alternatives using the generalizability framework provided by the authors ? The authors mention this limitation in Sec. 6, but it is not clear why the MMD kernel cannot quantify magnitude of differences.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposes a variant of Multi-Armed Bandits (MAB) named EdNetRMABs that model the interdependencies between learning contents to meet the real-world education scenarios. Subsequently, the authors introduce EduQate, an interdependency-aware Q-learning algorithm to optimize content recommendation given the EdNetRMABs. The paper demonstrates the theoretical and empirical effectiveness of EduQate with the experimental results of synthetic and real-world data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work uses RMABs in education to model the learning process with interdependent educational content as the different knowledge concepts have relevance.
2. The paper introduces EduQate employs Q-learning to make decisions on arm selection that do not require knowledge of the transition matrix to compute an optimal policy and provides related theoretical analysis.

Weaknesses:
1. As this work considers interdependency awareness in content recommendation in educational scenarios, the generated group in the experiment is not clear. The authors need to clarify how to capture relevance between the selected exercises of each topic in different datasets. Furthermore, does the different number of topics influence the experimental results?
2. The definition of state space representing a student's knowledge states a binary value. However, in real-world scenarios, the knowledge states are multi-level. A fine-grained knowledge state estimation result is essential for the content recommendation in adaptive learning.
3. The paper lacks the details of EdNetRMABs. Some illustrations would help to understand.

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces **EduQate**, a system that generates adaptive educational curricula using restless multi-armed bandits (RMABs). This method aims to efficiently achieve mastery across multiple interdependent educational contents. Unlike traditional methods that assume learning contents are independent, EduQate acknowledges and leverages the interdependencies between different educational concepts.

The paper addresses key challenges in modeling and optimizing the learning process in educational environments where the learning of different topics is interconnected. By considering these interdependencies, the proposed EduQate system ensures more accurate and effective personalized learning.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
**Originality** The paper introduces the concept of EdNetRMABs, a model that considers the interdependencies among learning contents in educational settings. This approach contrasts with existing methods that typically assume independence among different educational topics. By employing the Whittle index and Q-learning to develop the EduQate algorithm, the paper further demonstrates originality in adapting and integrating these techniques to address the specific challenges of personalized education.

**Quality** The authors provide clear definitions and a well-structured EdNetRMABs model. The optimality guarantees of the EduQate algorithm are well-explained and supported through rigorous proofs. Additionally, the empirical results are compelling, demonstrating the effectiveness of the proposed method over benchmark strategies using both synthetic and real data.

**Clarity** The paper is clearly written and well-organized, making complex concepts easier to understand. The authors systematically introduce the problem, the proposed solution, and both theoretical and empirical validations. The use of figures helps to elucidate the model and results. However, the clarity could be further enhanced by including more detailed explanations of the algorithm's implementation and practical applications.

**Significance** The significance of the paper lies in its potential impact on the field of educational technology. By addressing the interdependencies among learning contents, EduQate offers a more accurate and efficient approach to personalized education.

Weaknesses:
1. **Clarity of Explanations**: Due to the limited length, many explanations in the main text are not very clear. For instance, the survival and design of the student model and the content and information of the datasets are not thoroughly explained. For example, there is a dataset mentioned later that lacks similarity content, which should have been noted when introducing the dataset.

2. **Inconsistency in Notation**: In Section 4.1, ""Analysis of EduQate,"" the variable k in the second line has a different font from the k mentioned later. Additionally, the content related to k mentioned earlier is too far from this section, making it difficult for readers to understand and potentially leading to misunderstandings.

3. **Simplified Modeling**: The model uses only one pseudo-state, where if any content within a group is learned, all unlearned content in the group is marked as pseudo-state. This approach seems overly simplified. For example, in Case 1, if only one piece of content in a group is learned, all other content is marked as pseudo-state. In Case 2, if many pieces of content in a group are learned, the remaining content is also marked as pseudo-state. Although both cases result in a pseudo-state, their actual significance differs, and it seems the authors did not consider this distinction.

4. **Bidirectional Relationships**: The relationships between knowledge points in the paper are bidirectional, with only grouping relationships. However, in practice, many relationships between knowledge points within the same group are unidirectional. Recommending subsequent courses without recommending prerequisite courses first can lead to students needing to self-study prerequisite courses when learning subsequent courses, increasing their learning burden. The modeling in the paper does not seem to account for such unidirectional structures, instead using groups to link knowledge points, which could lead to this reverse learning issue.

5. **User Experience**: The discussion on user experience for educators and students is insufficient. Including qualitative and quantitative feedback from pilot implementations would be useful. The authors should conduct user research to gather insights on the intuitiveness and user-friendliness of the system for the target users. This could include surveys, interviews, and usability testing, focusing on ease of use, effectiveness, and areas needing improvement.

6. **Scalability of the EduQate Algorithm**: Although the paper mentions the efficiency of the EduQate algorithm, it lacks an in-depth exploration of its scalability in large-scale educational environments. Detailed performance benchmarking of the algorithm, including analysis of computation time and resource utilization, would be beneficial. The authors should provide a comprehensive performance analysis of the algorithm as data scale and complexity increase, including potential optimization techniques for handling large datasets.

Limitations:
The authors have explicitly acknowledged the limitations of their work, providing explanations for these constraints and discussing their impact on the research. The paper does not pose any negative social impact, as it aims to improve educational practices through a more personalized approach to learning.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes EduQate, an innovative framework that leverages EdNetRMABs to achieve interdependence among knowledge points. By using Q-learning, EduQate implements optimal strategies for personalized learning, offering optimality guarantees without needing explicit knowledge of transition functions governing student learning states. This approach dynamically adapts educational content to individual progress, optimizing learning experiences and outcomes. The effectiveness of EduQate is validated using three real-world datasets, demonstrating its capability to identify and implement the most effective teaching strategies.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Overall, this article provides valuable insights and has a good overall quality. It has a certain impact on the AI education domain. The proposed approach, EduQate, adapts the Q-learning method of the multi-armed bandit model and introduces active and passive arms to establish correlations between knowledge points, which demonstrates a level of innovation. The article ensures the validity of the approach at the theoretical level, making it more persuasive. In terms of narrative, the article maintains logical coherence and is comprehensible. The discussed topics are indeed important in the field of education.

Weaknesses:
The formatting of the article is messy, and the images are not very clear. The layout on the sixth page is problematic, with formulas and text mixed together, making it confusing. The images on the seventh page and some of the appendix images are blurred. I suggest using a different format to redraw them.

There are too few baselines in the experimental section, and there are very few validation experiments. Merely presenting outstanding performance in one experimental metric is insufficient to demonstrate the superiority of your method. It would be beneficial to include some reinforcement learning baselines for comparison. Other reinforcement learning methods, such as the Bayesian network approach mentioned in the citations, can also accomplish the task. Therefore, the necessity of your method is unclear in the paper.

Your method does not consider the specific circumstances of practical problems. It raises fairness concerns. Ensuring that all knowledge points are fairly selected in a specific educational context is a crucial issue. Although the article presents a good framework, I believe it is not feasible to use it.

Limitations:
As mentioned above, there are many factors to consider in the specific field of education. These factors include fairness, mapping each question's knowledge points to the ARMs mentioned above, and the presence of cold-start problems, among others. Time is also a crucial factor to consider during real-time usage, although the paper does not mention specific algorithmic time information in practice.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a solution to generate personalized learning curricula in educational settings, focusing on the challenge of accounting for interdependencies between learning topics. It argues that existing approaches, often based on the Restless Multi-Armed Bandit (RMAB) framework, fall short by assuming independence of learning content which is unrealistic in educational settings. To address this the paper introduces a new model Restless Multi-armed Bandits for Education (EdNetRMABS) enabling the capture of relationships between independent learning items. Building upon this model, the authors propose a new algorithm called EduQate, which leverages Q-learning and the Whittle index to compute an interdependency-aware teacher policy for recommending educational content. Notably, EduQate doesn't require prior knowledge of the transition matrix, unlike traditional Whittle index methods.

The authors provide a theoretical analysis demonstrating the optimality of EduQate for the case of recommending a single item at each time step (k=1). While finding the optimal solution for recommending multiple items (k>1) is proven to be NP-hard, a heuristic greedy algorithm is proposed to find solutions.

Through experiments on synthetic and real-world datasets (Junyi and OLI), the paper demonstrates the superiority of EduQate over baseline policies, including Threshold Whittle (TW), WIQL, Myopic, and Random. EduQate consistently achieves higher intervention benefits and average rewards across all datasets. Further analysis reveals the effectiveness of the replay buffer in EduQate, mitigating the ""cold-start problem"" common in reinforcement learning applications.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well organized and very well written with comprehensive explanations of the model, algorithm and the experimental setup.
2. Addressing the crucial issue of interdependency in learning content is a significant contribution. EdNetRMABs offer a realistic model for educational settings, moving beyond the simplifying assumption of independence prevalent in traditional RMAB approaches.
3. The paper presents a rigorous theoretical analysis of EduQate, proving its optimality for the k=1 case and providing complexity bounds for the k>1 case.
4. The authors address the cold-start problem by incorporating experience replay and perform ablations to show its effectiveness. This is a practical enhancement relevant for real-world applications.

Overall, this paper presents a strong contribution to the field of adaptive learning by introducing a novel and effective approach for generating personalized curricula that account for interdependencies in learning content.  The theoretical analysis, empirical results, and focus on practical considerations make this work both insightful and impactful.

Weaknesses:
The assumption of fully observable knowledge states is a significant limitation. Future work should explore extending the model and algorithm to handle partial observability, a more realistic scenario in education.
While the complexity analysis is provided, further investigation on the scalability of EduQate to larger datasets and more complex networks would strengthen its practical applicability.
The current work primarily focuses on maximizing long-term rewards. Further analysis on balancing exploration and exploitation in EdNetRMABs could offer valuable insights for curriculum design.

Limitations:
1. The experiments are based on simulated students and existing datasets. Real-world classrooms involve numerous factors not accounted for in the model, such as student motivation, engagement or diverse learning styles. This may limit the practical utility of this method.
2. The paper does not address the interpretability of EduQate's recommendations.
3. The assumption that the student states are fully observable is a major limitation of this work. This can result in overconfident recommendations.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a way to extend RMAB with Q-learning to accommodate the fact that some items/arms belong to a group. RMAB can be considered as a weakened version of contextual bandit CB but also a strong version of CB since it considers state transitions (explicitly defined on arms).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This is a very solid paper with quite a few reasons to accept it:
1. The contribution of education simulators. Can the authors comment on whether they will release their simulators publicly for other education or bandit communities to use in the future? Currently, there is a lack of high-quality education simulators that are based on real-world data. This could be a great contribution. I hope the authors make an effort to make such resources public.
2. The technical contribution of introducing a pseudo-action that extends RMAB and WIQL.
3. Clean writing and presentation.

MAB has been used in many different real-world settings and is the main algorithm behind many learning platforms. Any innovation in this space will have a huge impact on students around the world. Unfortunately, this area is very niche and requires high technical sophistication. Unless someone can convince me that this paper's algorithm has already been published elsewhere or is fully derivative of some other work, I think it's a great paper to present at NeurIPS.

I might also be a bit concerned if the authors decide not to share their code/algorithm/simulators to encourage more future research in this direction.

Weaknesses:
1. Can add some more ablation studies.
2. Some clarifications (see question section).

Limitations:
The authors discussed limitations in a section.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposes a variant of Multi-Armed Bandits (MAB) named EdNetRMABs that model the interdependencies between learning contents to meet the real-world education scenarios. Subsequently, the authors introduce EduQate, an interdependency-aware Q-learning algorithm to optimize content recommendation given the EdNetRMABs. The paper demonstrates the theoretical and empirical effectiveness of EduQate with the experimental results of synthetic and real-world data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work uses RMABs in education to model the learning process with interdependent educational content as the different knowledge concepts have relevance.
2. The paper introduces EduQate employs Q-learning to make decisions on arm selection that do not require knowledge of the transition matrix to compute an optimal policy and provides related theoretical analysis.

Weaknesses:
1. As this work considers interdependency awareness in content recommendation in educational scenarios, the generated group in the experiment is not clear. The authors need to clarify how to capture relevance between the selected exercises of each topic in different datasets. Furthermore, does the different number of topics influence the experimental results?
2. The definition of state space representing a student's knowledge states a binary value. However, in real-world scenarios, the knowledge states are multi-level. A fine-grained knowledge state estimation result is essential for the content recommendation in adaptive learning.
3. The paper lacks the details of EdNetRMABs. Some illustrations would help to understand.

Limitations:
NA

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces **EduQate**, a system that generates adaptive educational curricula using restless multi-armed bandits (RMABs). This method aims to efficiently achieve mastery across multiple interdependent educational contents. Unlike traditional methods that assume learning contents are independent, EduQate acknowledges and leverages the interdependencies between different educational concepts.

The paper addresses key challenges in modeling and optimizing the learning process in educational environments where the learning of different topics is interconnected. By considering these interdependencies, the proposed EduQate system ensures more accurate and effective personalized learning.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
**Originality** The paper introduces the concept of EdNetRMABs, a model that considers the interdependencies among learning contents in educational settings. This approach contrasts with existing methods that typically assume independence among different educational topics. By employing the Whittle index and Q-learning to develop the EduQate algorithm, the paper further demonstrates originality in adapting and integrating these techniques to address the specific challenges of personalized education.

**Quality** The authors provide clear definitions and a well-structured EdNetRMABs model. The optimality guarantees of the EduQate algorithm are well-explained and supported through rigorous proofs. Additionally, the empirical results are compelling, demonstrating the effectiveness of the proposed method over benchmark strategies using both synthetic and real data.

**Clarity** The paper is clearly written and well-organized, making complex concepts easier to understand. The authors systematically introduce the problem, the proposed solution, and both theoretical and empirical validations. The use of figures helps to elucidate the model and results. However, the clarity could be further enhanced by including more detailed explanations of the algorithm's implementation and practical applications.

**Significance** The significance of the paper lies in its potential impact on the field of educational technology. By addressing the interdependencies among learning contents, EduQate offers a more accurate and efficient approach to personalized education.

Weaknesses:
1. **Clarity of Explanations**: Due to the limited length, many explanations in the main text are not very clear. For instance, the survival and design of the student model and the content and information of the datasets are not thoroughly explained. For example, there is a dataset mentioned later that lacks similarity content, which should have been noted when introducing the dataset.

2. **Inconsistency in Notation**: In Section 4.1, ""Analysis of EduQate,"" the variable k in the second line has a different font from the k mentioned later. Additionally, the content related to k mentioned earlier is too far from this section, making it difficult for readers to understand and potentially leading to misunderstandings.

3. **Simplified Modeling**: The model uses only one pseudo-state, where if any content within a group is learned, all unlearned content in the group is marked as pseudo-state. This approach seems overly simplified. For example, in Case 1, if only one piece of content in a group is learned, all other content is marked as pseudo-state. In Case 2, if many pieces of content in a group are learned, the remaining content is also marked as pseudo-state. Although both cases result in a pseudo-state, their actual significance differs, and it seems the authors did not consider this distinction.

4. **Bidirectional Relationships**: The relationships between knowledge points in the paper are bidirectional, with only grouping relationships. However, in practice, many relationships between knowledge points within the same group are unidirectional. Recommending subsequent courses without recommending prerequisite courses first can lead to students needing to self-study prerequisite courses when learning subsequent courses, increasing their learning burden. The modeling in the paper does not seem to account for such unidirectional structures, instead using groups to link knowledge points, which could lead to this reverse learning issue.

5. **User Experience**: The discussion on user experience for educators and students is insufficient. Including qualitative and quantitative feedback from pilot implementations would be useful. The authors should conduct user research to gather insights on the intuitiveness and user-friendliness of the system for the target users. This could include surveys, interviews, and usability testing, focusing on ease of use, effectiveness, and areas needing improvement.

6. **Scalability of the EduQate Algorithm**: Although the paper mentions the efficiency of the EduQate algorithm, it lacks an in-depth exploration of its scalability in large-scale educational environments. Detailed performance benchmarking of the algorithm, including analysis of computation time and resource utilization, would be beneficial. The authors should provide a comprehensive performance analysis of the algorithm as data scale and complexity increase, including potential optimization techniques for handling large datasets.

Limitations:
The authors have explicitly acknowledged the limitations of their work, providing explanations for these constraints and discussing their impact on the research. The paper does not pose any negative social impact, as it aims to improve educational practices through a more personalized approach to learning.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes EduQate, an innovative framework that leverages EdNetRMABs to achieve interdependence among knowledge points. By using Q-learning, EduQate implements optimal strategies for personalized learning, offering optimality guarantees without needing explicit knowledge of transition functions governing student learning states. This approach dynamically adapts educational content to individual progress, optimizing learning experiences and outcomes. The effectiveness of EduQate is validated using three real-world datasets, demonstrating its capability to identify and implement the most effective teaching strategies.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Overall, this article provides valuable insights and has a good overall quality. It has a certain impact on the AI education domain. The proposed approach, EduQate, adapts the Q-learning method of the multi-armed bandit model and introduces active and passive arms to establish correlations between knowledge points, which demonstrates a level of innovation. The article ensures the validity of the approach at the theoretical level, making it more persuasive. In terms of narrative, the article maintains logical coherence and is comprehensible. The discussed topics are indeed important in the field of education.

Weaknesses:
The formatting of the article is messy, and the images are not very clear. The layout on the sixth page is problematic, with formulas and text mixed together, making it confusing. The images on the seventh page and some of the appendix images are blurred. I suggest using a different format to redraw them.

There are too few baselines in the experimental section, and there are very few validation experiments. Merely presenting outstanding performance in one experimental metric is insufficient to demonstrate the superiority of your method. It would be beneficial to include some reinforcement learning baselines for comparison. Other reinforcement learning methods, such as the Bayesian network approach mentioned in the citations, can also accomplish the task. Therefore, the necessity of your method is unclear in the paper.

Your method does not consider the specific circumstances of practical problems. It raises fairness concerns. Ensuring that all knowledge points are fairly selected in a specific educational context is a crucial issue. Although the article presents a good framework, I believe it is not feasible to use it.

Limitations:
As mentioned above, there are many factors to consider in the specific field of education. These factors include fairness, mapping each question's knowledge points to the ARMs mentioned above, and the presence of cold-start problems, among others. Time is also a crucial factor to consider during real-time usage, although the paper does not mention specific algorithmic time information in practice.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a solution to generate personalized learning curricula in educational settings, focusing on the challenge of accounting for interdependencies between learning topics. It argues that existing approaches, often based on the Restless Multi-Armed Bandit (RMAB) framework, fall short by assuming independence of learning content which is unrealistic in educational settings. To address this the paper introduces a new model Restless Multi-armed Bandits for Education (EdNetRMABS) enabling the capture of relationships between independent learning items. Building upon this model, the authors propose a new algorithm called EduQate, which leverages Q-learning and the Whittle index to compute an interdependency-aware teacher policy for recommending educational content. Notably, EduQate doesn't require prior knowledge of the transition matrix, unlike traditional Whittle index methods.

The authors provide a theoretical analysis demonstrating the optimality of EduQate for the case of recommending a single item at each time step (k=1). While finding the optimal solution for recommending multiple items (k>1) is proven to be NP-hard, a heuristic greedy algorithm is proposed to find solutions.

Through experiments on synthetic and real-world datasets (Junyi and OLI), the paper demonstrates the superiority of EduQate over baseline policies, including Threshold Whittle (TW), WIQL, Myopic, and Random. EduQate consistently achieves higher intervention benefits and average rewards across all datasets. Further analysis reveals the effectiveness of the replay buffer in EduQate, mitigating the ""cold-start problem"" common in reinforcement learning applications.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The paper is well organized and very well written with comprehensive explanations of the model, algorithm and the experimental setup.
2. Addressing the crucial issue of interdependency in learning content is a significant contribution. EdNetRMABs offer a realistic model for educational settings, moving beyond the simplifying assumption of independence prevalent in traditional RMAB approaches.
3. The paper presents a rigorous theoretical analysis of EduQate, proving its optimality for the k=1 case and providing complexity bounds for the k>1 case.
4. The authors address the cold-start problem by incorporating experience replay and perform ablations to show its effectiveness. This is a practical enhancement relevant for real-world applications.

Overall, this paper presents a strong contribution to the field of adaptive learning by introducing a novel and effective approach for generating personalized curricula that account for interdependencies in learning content.  The theoretical analysis, empirical results, and focus on practical considerations make this work both insightful and impactful.

Weaknesses:
The assumption of fully observable knowledge states is a significant limitation. Future work should explore extending the model and algorithm to handle partial observability, a more realistic scenario in education.
While the complexity analysis is provided, further investigation on the scalability of EduQate to larger datasets and more complex networks would strengthen its practical applicability.
The current work primarily focuses on maximizing long-term rewards. Further analysis on balancing exploration and exploitation in EdNetRMABs could offer valuable insights for curriculum design.

Limitations:
1. The experiments are based on simulated students and existing datasets. Real-world classrooms involve numerous factors not accounted for in the model, such as student motivation, engagement or diverse learning styles. This may limit the practical utility of this method.
2. The paper does not address the interpretability of EduQate's recommendations.
3. The assumption that the student states are fully observable is a major limitation of this work. This can result in overconfident recommendations.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a way to extend RMAB with Q-learning to accommodate the fact that some items/arms belong to a group. RMAB can be considered as a weakened version of contextual bandit CB but also a strong version of CB since it considers state transitions (explicitly defined on arms).

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
This is a very solid paper with quite a few reasons to accept it:
1. The contribution of education simulators. Can the authors comment on whether they will release their simulators publicly for other education or bandit communities to use in the future? Currently, there is a lack of high-quality education simulators that are based on real-world data. This could be a great contribution. I hope the authors make an effort to make such resources public.
2. The technical contribution of introducing a pseudo-action that extends RMAB and WIQL.
3. Clean writing and presentation.

MAB has been used in many different real-world settings and is the main algorithm behind many learning platforms. Any innovation in this space will have a huge impact on students around the world. Unfortunately, this area is very niche and requires high technical sophistication. Unless someone can convince me that this paper's algorithm has already been published elsewhere or is fully derivative of some other work, I think it's a great paper to present at NeurIPS.

I might also be a bit concerned if the authors decide not to share their code/algorithm/simulators to encourage more future research in this direction.

Weaknesses:
1. Can add some more ablation studies.
2. Some clarifications (see question section).

Limitations:
The authors discussed limitations in a section.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper addresses unsupervised anomaly detection by proposing a method named UniCAD. The authors aim to enhance anomaly detection performance by establishing a theoretical connection between representation learning, clustering, and anomaly detection. They introduce a unified framework that jointly optimizes these three components, using a probabilistic mixture model and a Student's-t distribution for robust representation learning and clustering. The framework also includes an anomaly-aware data likelihood objective, which reduces the impact of anomalous data on the learning process. Additionally, the authors propose a gravity-inspired anomaly scoring method that leverages relationships between samples and clusters.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Modeling the connection between representation learning, clustering, and anomaly detection is highly relevant. This paper effectively demonstrates how these three tasks are interrelated and can be jointly optimized to improve anomaly detection performance.

2. The paper is well-written, presenting its hypothesis and method clearly. 

3. The results are impressive and demonstrate the effectiveness of UniCAD.

Weaknesses:
1. The ablation study on the hyperparameters $k$ and $l$ is insufficient. The authors only present results from a single dataset, satimage-2, where their method achieves an almost perfect score. It would be more informative to perform ablation studies across all 30 datasets or at least a subset where the model also shows lower performance. This broader analysis would demonstrate how these hyperparameters affect the average ranking of the method, similar to the results reported in the paper's table.

2. The authors introduce a $g(\Theta)$ term to prevent shortcut solutions, mentioning it in Equation 15. However, they do not discuss its importance or impact on performance after its introduction. Key questions remain unanswered, such as how the $g(\Theta)$ term affects the model's performance, what happens if it is removed, and how the autoencoder is implemented. These details are crucial, as the regularization term may significantly influence the results.

Limitations:
The authors mention some of the limitations, but they do not address the potential negative impact of the work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes UniCAD, a theoretically unified framework for representation learning, clustering, and anomaly detection. This paper first introduces the mixture of Student-t distribution $p(x|\Theta, \Phi)$ with degree of freedom $\nu=1$ based on a representation learner $f_\Theta$ using NN. Then, this paper combines with an anomaly indicator $\delta$ for maximum likelihood estimation. Parameters $(\Theta, \Phi)$ are optimized by EM algorithm and SGD. In addition, when detecting anomalies, an improved score is used with reference to gravity. The UniCAD achieved good performance on experiments with various datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is well written and easy to follow.
- Good experimental results.

Weaknesses:
- We have several questions about the proposed method and experiments. Please see Qustions.
- The comparison with DeepSVDD and DIF is excellent, but I think the paper also needs to be compared with other Deep anomaly detection methods. For example, DROCC [1].

[1] Goyal, Sachin, et al. ""DROCC: Deep robust one-class classification. ""International conference on machine learning. PMLR, 2020.

Limitations:
- Hyper-parameter sensitivity seems to be one limitation.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel probabilistic mixture model for unsupervised anomaly detection (UAD) that unifies representation learning, clustering, and anomaly detection into a single theoretical framework. The proposed UniCAD model addresses the lack of a unified approach in existing methods, which often consider these components separately or in pairs. The experimental results show that UniCAD consistently outperformed other methods in terms of AUC-ROC and AUC-PR. The model’s iterative optimization process using EM was also highlighted as effective and convergent.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- This paper introduces a novel integration of a probabilistic mixture model that unifies representation learning, clustering, and anomaly detection into a single theoretical framework.
- The proposed approach is well-motivated (Fig. 1) and supported by a robust theoretical foundation that maximizes anomaly-aware data likelihood, ensuring the model effectively leverages the interplay between representation learning, clustering, and anomaly detection.
- The paper is well-written, offering clear and comprehensive explanations of the proposed method, including detailed theoretical derivations and intuitive motivations for the design choices. The methodology section is particularly well-structured, logically outlining the steps and equations involved in the proposed model.
- The comprehensive evaluation design underscores the robustness of the proposed method.

Weaknesses:
- The connection between force analysis and anomaly detection, particularly between Equations 7 and 8 in Section 3.2.1, could benefit from further justification. While the analogy is interesting, it may not be immediately intuitive for all readers.
- The iterative optimization process may pose scalability issues for large datasets. An in-depth analysis and discussion of this would further strengthen the quality of this research.
- Although the model maps data to a low-dimensional representation space, the effectiveness of this mapping for very high-dimensional datasets could be explored further.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose UniCAD to jointly model representation learning,
clustering and anomaly detection.  The main objective is maximizing
the product of anomaly indicator (1 is normal, 0 is anomaly) and the
joint probability of instance x_i and cluster c_k given parameters for
representation learning theta and clustering phi.  The joint
probability is decomposed into the prior of c_k and likelihood of
p(x_i|c_k), which is modeled by a Student's-t distribution on the
distance between representation z_i and mean mu_k with covariance
Sigma_k.  p(x_i) is the marginal over c_k.  Anomaly indicator delta is
zero for p(x_i) in the lowest l percentage.  The anomaly score is
1/p(x_i).

Compared to Newton's law of Universal Gravitation, the anomaly score
function has similar components, except for the unit vector r_ik
(which indicates the directions of forces, beyond the
magnitudes). Hence, they incorporated r_ik into their anomaly score
function.

For updating the clustering parameter phi (mixture weights, means,
covariance), they use EM.  In the E-step, they estimate the posterior
p(c_k|x_i).  In the M-step, they estimate phi.  For updating
representation parameters theta, they use gradient descent to minimize
negative log likelihood of instances, together with a reconstruction
loss via an autoencoder to prevent shortcut solutions.

For empirical comparisons, they use 30 tabular data sets and 17
existing algorithms.  The proposed approach generally outperforms the
others in terms of average rank in AUCROC.  The vector version of
anomaly score function is ranked higher than the scalar version.  On
computation time, UniCAD is in the middle among 5 algorithms.  Ablation
studies indicate the contributions of the different components.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main contribution is combining representation learning,
clustering, and anomaly detection in a unified single probabilistic
formulation, which is interesting.

The empirical results indicate that UniCAD compares favorably against
17 existing techniques on 30 tabular datasets.  Compared to four
existing algorithms, computation is not the most intensive.

The paper is generally well written.

Weaknesses:
The clustering part is similar to a typical Gaussian mixture model for clustering via EM, except for t-distribution instead of Gaussian and the scaling factor.

Two neural-network-based approaches were compared.  As UniCAD utilizes
representation learning, comparing with more approaches that utilize
representation learning would be significant.  Approaches without
representation learning have an inherent disadvantage.

Some parts could be clarified--see Questions.

Limitations:
Limitations of the proposed approach do not seem to be mentioned.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper addresses unsupervised anomaly detection by proposing a method named UniCAD. The authors aim to enhance anomaly detection performance by establishing a theoretical connection between representation learning, clustering, and anomaly detection. They introduce a unified framework that jointly optimizes these three components, using a probabilistic mixture model and a Student's-t distribution for robust representation learning and clustering. The framework also includes an anomaly-aware data likelihood objective, which reduces the impact of anomalous data on the learning process. Additionally, the authors propose a gravity-inspired anomaly scoring method that leverages relationships between samples and clusters.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Modeling the connection between representation learning, clustering, and anomaly detection is highly relevant. This paper effectively demonstrates how these three tasks are interrelated and can be jointly optimized to improve anomaly detection performance.

2. The paper is well-written, presenting its hypothesis and method clearly. 

3. The results are impressive and demonstrate the effectiveness of UniCAD.

Weaknesses:
1. The ablation study on the hyperparameters $k$ and $l$ is insufficient. The authors only present results from a single dataset, satimage-2, where their method achieves an almost perfect score. It would be more informative to perform ablation studies across all 30 datasets or at least a subset where the model also shows lower performance. This broader analysis would demonstrate how these hyperparameters affect the average ranking of the method, similar to the results reported in the paper's table.

2. The authors introduce a $g(\Theta)$ term to prevent shortcut solutions, mentioning it in Equation 15. However, they do not discuss its importance or impact on performance after its introduction. Key questions remain unanswered, such as how the $g(\Theta)$ term affects the model's performance, what happens if it is removed, and how the autoencoder is implemented. These details are crucial, as the regularization term may significantly influence the results.

Limitations:
The authors mention some of the limitations, but they do not address the potential negative impact of the work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes UniCAD, a theoretically unified framework for representation learning, clustering, and anomaly detection. This paper first introduces the mixture of Student-t distribution $p(x|\Theta, \Phi)$ with degree of freedom $\nu=1$ based on a representation learner $f_\Theta$ using NN. Then, this paper combines with an anomaly indicator $\delta$ for maximum likelihood estimation. Parameters $(\Theta, \Phi)$ are optimized by EM algorithm and SGD. In addition, when detecting anomalies, an improved score is used with reference to gravity. The UniCAD achieved good performance on experiments with various datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is well written and easy to follow.
- Good experimental results.

Weaknesses:
- We have several questions about the proposed method and experiments. Please see Qustions.
- The comparison with DeepSVDD and DIF is excellent, but I think the paper also needs to be compared with other Deep anomaly detection methods. For example, DROCC [1].

[1] Goyal, Sachin, et al. ""DROCC: Deep robust one-class classification. ""International conference on machine learning. PMLR, 2020.

Limitations:
- Hyper-parameter sensitivity seems to be one limitation.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel probabilistic mixture model for unsupervised anomaly detection (UAD) that unifies representation learning, clustering, and anomaly detection into a single theoretical framework. The proposed UniCAD model addresses the lack of a unified approach in existing methods, which often consider these components separately or in pairs. The experimental results show that UniCAD consistently outperformed other methods in terms of AUC-ROC and AUC-PR. The model’s iterative optimization process using EM was also highlighted as effective and convergent.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- This paper introduces a novel integration of a probabilistic mixture model that unifies representation learning, clustering, and anomaly detection into a single theoretical framework.
- The proposed approach is well-motivated (Fig. 1) and supported by a robust theoretical foundation that maximizes anomaly-aware data likelihood, ensuring the model effectively leverages the interplay between representation learning, clustering, and anomaly detection.
- The paper is well-written, offering clear and comprehensive explanations of the proposed method, including detailed theoretical derivations and intuitive motivations for the design choices. The methodology section is particularly well-structured, logically outlining the steps and equations involved in the proposed model.
- The comprehensive evaluation design underscores the robustness of the proposed method.

Weaknesses:
- The connection between force analysis and anomaly detection, particularly between Equations 7 and 8 in Section 3.2.1, could benefit from further justification. While the analogy is interesting, it may not be immediately intuitive for all readers.
- The iterative optimization process may pose scalability issues for large datasets. An in-depth analysis and discussion of this would further strengthen the quality of this research.
- Although the model maps data to a low-dimensional representation space, the effectiveness of this mapping for very high-dimensional datasets could be explored further.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose UniCAD to jointly model representation learning,
clustering and anomaly detection.  The main objective is maximizing
the product of anomaly indicator (1 is normal, 0 is anomaly) and the
joint probability of instance x_i and cluster c_k given parameters for
representation learning theta and clustering phi.  The joint
probability is decomposed into the prior of c_k and likelihood of
p(x_i|c_k), which is modeled by a Student's-t distribution on the
distance between representation z_i and mean mu_k with covariance
Sigma_k.  p(x_i) is the marginal over c_k.  Anomaly indicator delta is
zero for p(x_i) in the lowest l percentage.  The anomaly score is
1/p(x_i).

Compared to Newton's law of Universal Gravitation, the anomaly score
function has similar components, except for the unit vector r_ik
(which indicates the directions of forces, beyond the
magnitudes). Hence, they incorporated r_ik into their anomaly score
function.

For updating the clustering parameter phi (mixture weights, means,
covariance), they use EM.  In the E-step, they estimate the posterior
p(c_k|x_i).  In the M-step, they estimate phi.  For updating
representation parameters theta, they use gradient descent to minimize
negative log likelihood of instances, together with a reconstruction
loss via an autoencoder to prevent shortcut solutions.

For empirical comparisons, they use 30 tabular data sets and 17
existing algorithms.  The proposed approach generally outperforms the
others in terms of average rank in AUCROC.  The vector version of
anomaly score function is ranked higher than the scalar version.  On
computation time, UniCAD is in the middle among 5 algorithms.  Ablation
studies indicate the contributions of the different components.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The main contribution is combining representation learning,
clustering, and anomaly detection in a unified single probabilistic
formulation, which is interesting.

The empirical results indicate that UniCAD compares favorably against
17 existing techniques on 30 tabular datasets.  Compared to four
existing algorithms, computation is not the most intensive.

The paper is generally well written.

Weaknesses:
The clustering part is similar to a typical Gaussian mixture model for clustering via EM, except for t-distribution instead of Gaussian and the scaling factor.

Two neural-network-based approaches were compared.  As UniCAD utilizes
representation learning, comparing with more approaches that utilize
representation learning would be significant.  Approaches without
representation learning have an inherent disadvantage.

Some parts could be clarified--see Questions.

Limitations:
Limitations of the proposed approach do not seem to be mentioned.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,No,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
Training RBMs is challenging and slow due to the multiple second-order phase transitions and associated slow mixing of MCMC sampling. The paper introduces a pre-training method, consisting in integrating the principal directions of the dataset into a low-rank RBM through a convex optimization procedure. The Gibbs-Boltzmann equilibrium distribution of the pre-trained model can be efficiently sampled via a static Monte Carlo process. Starting from the pre-trained model, the standard Persistent Contrastive Divergence (PCD) training procedure for RBMs partially overcomes the problem of second-order phase transitions. The pre-training method is tested on the MNIST 01 dataset, a synthesized “Mickey” dataset, and the Human Genome dataset (HGD). The method is shown to outperform the PCD algorithm and the Jarzynski reweighting method (JarRBM).
The paper also introduces a new method to sample from the trained model, called Parallel Trajectory Tempering (PTT), and compares it with the Annealed Importance Sampling (AIS) method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Technically sound.

The proposed pre-training method is shown to improve on other training schemes for RBMs (namely the standard PCD method, and the Jarzynski reweighting method).

The novel PTT method is shown to improve on standard Gibbs sampling.

Weaknesses:
My main concern is about the usefulness of the RBM approach to generative modeling. (cf question Q1 below).

The PTT sampling method seems to require more memory than standard methods for RBMs.

It is unclear how novel the proposed pre-training method is. (cf question Q2 below)

The paper contains a link to a github repository that reveals the author’s identity (section 8 page 9):
https://github.com/nbereux/fast-RBM

Minor. Line 465: Appendix A.2 references itself.

Limitations:
See Weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work proposes to pretrain RBMS with a recently developed convex approximation, the restricted coulomb machine and then fine-tune the model using standard techniques like PCD. Further, a novel sampling technique, PTT is proposed that can sample from the final trained model by employing a sequence of model snaphsots during training, that are then connected via replica exchange in the style of parallel tempering.

Experiments are conducted and results show that the sampled distributions, when projected to the first few principal components, match the true distribution better. Moreover, log-likelihood comparisons based on single training runs show that the proposed method starts at much higher likelihood values due to the initialisation and there is some evidence that the resulting model als reaches higher likelihood values. Further experiments for PTT show that it is more likely to jump between clusters of the distribution than PCD based on gibbs sampling

Disclaimer: This is an emergency review. I have not had the time to do a detailled analysis, or implement/reproduce any of the results. While I am expert in the field, I will adapt my confidence accordingly. I will not fullfy abide to the review format.

Edit: An edit has been performed that only included changes of the format, but not the content.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The use of the approximation using the restricted coulomb machine as an initialisation is an interesting idea that is worth investigating and the results in Fig 3C suggest that this pretraining approach is very effective.

- The idea of using the training models as sampling steps is an interesting approach as well.

In general, i can see that both techniques can become tools in a more general RBM toolbox, however both of them seem to be incremental changes, even though they could impact RBM training a lot.

Weaknesses:
The main weaknesses of the present work are in two areas: experiments/comparisons and language, of which I only deem the former critical, while the latter will limit the potential impact of the article in the machine-learning community. As a result of the weaknesses, this paper has a number of misleading claims or claims that are not supported by the data presented in the work.

Experiments/Comparisons:

- The authors dismiss using PT for being ""expensive"" and do not compare against it. This is against evidence that even single steps of PT chains with a moderate number of parallel chains can be more efficient than PCD using similar resources. This is especially interesting, since the authors use PCD-100 for training, which allows a lot of resources for PT.  Note that the authors themselves reference [40] which shows an order of magnitudes improvement over Gibbs sampling already with 10 chains.
Another example is given by 
[*] https://www.sciencedirect.com/science/article/pii/S0004370219301948
where the authors used PT for training with k=50 chains and only a single sampling step per iteration. Note that this paper uses a very simple heuristic to improve PT sampling: choose the reference (temperature ->infty) distribution as not the uniform, but the marginal data distribution. This is in contrast to [40] that used the uniform distirbution, leading to a significantly worse baseline.

- There are almost no comparisons of log-likelihood values or values of normalisation constants for the proposed technique. While some are given in Figure 3C, they are only single run and only using approximated likelihoods. Due to this, the phrase "" significantly higher log-likelihoods"" in the conclusion is NOT supported by the data, given there is not enough data to test for significance or even measure the variability.

- The training is also cut short, or the RBMs trained are not powerful enough, since 3B shows clear artefacts in all samples, indicating that none of the machines approximate the dataset well. Since RBMS with enough latent variables are universal function approximators, we can not get a definitive statement of whether the proposed pretraining does allow for better likelihoods. Since the experiments are not very expensive, this reviewer would propose to at least repeat experiments in order to obtain error bars on Fig 3C. 

- The learning rate of 0.01 used in the experiments seems to be on the high end. This is not only bad for PCD training, but also for obtaining high likelihood values, and could explain big parts of the leveling out of the graph in Fig 3C. While it is okay to use a high learning rate in the beginning, keeping it constant over the course of training seems like an oversight.

- For PTT especially, there are no good comparisons that compare the quality of the samples in terms of representing the true distributions. While visual examples are shown that show visually good mixing, the baseline to compare to is again PCD, and not PT, nor stacked tempering. Since, again [40] showed that both alternatives clearly beat gibbs sampling given the same amount of resources, we do not know how good this sampling scheme really is, compared to strong baselines. This reviewer suggests to compare in at least one experiment the proposed approach to an approach with known normalisation constant and then measure how well estimators based on these samples work, similar to [*]. This would also partially verify 3F. 
- As an addendum to the previous point: [*] showed that the performance of AIS improves significantly with the choice of reference distribution. Using the same distribution as [*] or the pretrained distribution proposed in this work, might diminish any performance gains by PTT. This would still be a major improvement to the state of the art.



Language:
- While in general well written, this work reads like targeting a physics community, not the ML community. As a result, this paper includes  slang terms mostly encountered in statistical physics/thermodynamics, which do not have a clear meaning in the statistics community. This includes terms like ""phase transitions"" (first and second order), ""equilibrium models"" (not consequential for the article, nor the review, but this reviewer genuinely does not know what this term is supposed to mean), ""critical transitions"", ""relaxation times"", ""free energy"". Since most of these terms appear in sections where the authors try to explain the method and/or its consequences, a significant number of readers will not be able to understand those reasonings as they do not have a grasp of the physical analogies. This reviewer would propose to replace some of the terms by the statistical equivalent, or to introduce them. 
- Some of the explainations and reasonings are misleading. The initial paragraph highlights that RBMs are supposedly interpretable. While they are simple models, Binary RBMs are still universal function approximators and thus it is highly unlikely that the latent space has any meaning that aligns with any human interpretable semantics. If the authors disagree with this, This reviewer encourages them to add a citation to line 25.
- Missing citations: the datasets used should be cited.
- Figure references missing: the article does not always refer to the figure they are talking about, e..g, line 205. In general references should include the subfigure letter as the article does later, e.g., lines 263+

Limitations:
The authors mark ""Yes"" for point 2 ""Limitations"" on the checklist. This reviewer has not found that the authors discussed the limitations of their work. This especially includes the guidelines that say that authors should ""reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs."" However, in fairness, the authors answer ""[NO]"" in question 7, discussing the presence of error bars or significance tests to measure significance. However, since this is not part of the final publication, and the authors include misleading claims about significance of results, this must be discussed in the main text, or the phrasing weakened.

I have not found any information on runtime or CPU/GPU hours, but the CPU/GPU were reported, so point 8 is partially fulfilled.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The manuscript suggests to apply 

Aurélien Decelle and Cyril Furtlehner. Exact training of restricted Boltzmann machines on intrinsically low dimensional data. Physical Review Letters, 127(15):158303, 2021.

to initialise persistent contrastive divergence (PCD) learning for RBM training and estimating the log likelihood / partition function of RBMs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
While one may argue that the novelty is limited because the interesting theoretical work was done in the abovementioned paper by Aurélien Decelle and Cyril Furtlehner, the idea is sound and the approach may be useful in practice.

Weaknesses:
The novelty is limited because the interesting theoretical work was done in the abovementioned paper by Aurélien Decelle and Cyril Furtlehner.

My **main criticism** refers to the empirical evaluation, and I think these questions should be addressed:
- Was enough effort out into the baseline methods (including hyperparameter choice)?
- What would be an example where the PCA decomposition is misleading (i.e., not helping or even slowing down the process)?
- What about MNIST with all 10 digits?

**Details** (not ordered by importance):

* „On the diametrically opposite side (on interpretability)  are generative ConvNets [9, 10], where the energy function is formulated as a deep neural network, which are capable of synthesizing photorealistic images but are almost impossible to interpret as a physical model.“: Not clear, perhaps add half a sentence to elaborate.

* „second-order phase transition“: define what this is already in the beginning

* Beginning of section 2: I suggest to add the analysis in 

  Fischer, Igel. Bounding the Bias of Contrastive Divergence Learning. Neural Computation, 2011
  https://direct.mit.edu/neco/article-abstract/23/3/664/7646/Bounding-the-Bias-of-Contrastive-Divergence?redirectedFrom=fulltext

  to the discussion of the limitations of CD.

* „much better than those obtained with the standard Annealing Important Sampling (AIS) techniques“:
In [42], several methods are discussed, in particular one based on Bennett’s Acceptance Ratio method (BAR), which performed in general better than standard AIS. How does the proposed method perform in comparison to BAR?

* What if linear PCA is not well suited to find a good representation of the data because of a highly non-linear latent structure?

* I am not fully happy with the selected benchmark tasks. 
In particular: How does the method perform on MNIST with all 10 digits?

**Minor** comments: 

The reference list should be revised. Inconsistent capitalisation, author first name abbreviations, etc.

Limitations:
I think the limitations should have been explored in more depth.
What would be an example where the PCA decomposition is misleading (i.e., not helping or even slowing down the process)?

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
Training RBMs is challenging and slow due to the multiple second-order phase transitions and associated slow mixing of MCMC sampling. The paper introduces a pre-training method, consisting in integrating the principal directions of the dataset into a low-rank RBM through a convex optimization procedure. The Gibbs-Boltzmann equilibrium distribution of the pre-trained model can be efficiently sampled via a static Monte Carlo process. Starting from the pre-trained model, the standard Persistent Contrastive Divergence (PCD) training procedure for RBMs partially overcomes the problem of second-order phase transitions. The pre-training method is tested on the MNIST 01 dataset, a synthesized “Mickey” dataset, and the Human Genome dataset (HGD). The method is shown to outperform the PCD algorithm and the Jarzynski reweighting method (JarRBM).
The paper also introduces a new method to sample from the trained model, called Parallel Trajectory Tempering (PTT), and compares it with the Annealed Importance Sampling (AIS) method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Technically sound.

The proposed pre-training method is shown to improve on other training schemes for RBMs (namely the standard PCD method, and the Jarzynski reweighting method).

The novel PTT method is shown to improve on standard Gibbs sampling.

Weaknesses:
My main concern is about the usefulness of the RBM approach to generative modeling. (cf question Q1 below).

The PTT sampling method seems to require more memory than standard methods for RBMs.

It is unclear how novel the proposed pre-training method is. (cf question Q2 below)

The paper contains a link to a github repository that reveals the author’s identity (section 8 page 9):
https://github.com/nbereux/fast-RBM

Minor. Line 465: Appendix A.2 references itself.

Limitations:
See Weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The work proposes to pretrain RBMS with a recently developed convex approximation, the restricted coulomb machine and then fine-tune the model using standard techniques like PCD. Further, a novel sampling technique, PTT is proposed that can sample from the final trained model by employing a sequence of model snaphsots during training, that are then connected via replica exchange in the style of parallel tempering.

Experiments are conducted and results show that the sampled distributions, when projected to the first few principal components, match the true distribution better. Moreover, log-likelihood comparisons based on single training runs show that the proposed method starts at much higher likelihood values due to the initialisation and there is some evidence that the resulting model als reaches higher likelihood values. Further experiments for PTT show that it is more likely to jump between clusters of the distribution than PCD based on gibbs sampling

Disclaimer: This is an emergency review. I have not had the time to do a detailled analysis, or implement/reproduce any of the results. While I am expert in the field, I will adapt my confidence accordingly. I will not fullfy abide to the review format.

Edit: An edit has been performed that only included changes of the format, but not the content.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The use of the approximation using the restricted coulomb machine as an initialisation is an interesting idea that is worth investigating and the results in Fig 3C suggest that this pretraining approach is very effective.

- The idea of using the training models as sampling steps is an interesting approach as well.

In general, i can see that both techniques can become tools in a more general RBM toolbox, however both of them seem to be incremental changes, even though they could impact RBM training a lot.

Weaknesses:
The main weaknesses of the present work are in two areas: experiments/comparisons and language, of which I only deem the former critical, while the latter will limit the potential impact of the article in the machine-learning community. As a result of the weaknesses, this paper has a number of misleading claims or claims that are not supported by the data presented in the work.

Experiments/Comparisons:

- The authors dismiss using PT for being ""expensive"" and do not compare against it. This is against evidence that even single steps of PT chains with a moderate number of parallel chains can be more efficient than PCD using similar resources. This is especially interesting, since the authors use PCD-100 for training, which allows a lot of resources for PT.  Note that the authors themselves reference [40] which shows an order of magnitudes improvement over Gibbs sampling already with 10 chains.
Another example is given by 
[*] https://www.sciencedirect.com/science/article/pii/S0004370219301948
where the authors used PT for training with k=50 chains and only a single sampling step per iteration. Note that this paper uses a very simple heuristic to improve PT sampling: choose the reference (temperature ->infty) distribution as not the uniform, but the marginal data distribution. This is in contrast to [40] that used the uniform distirbution, leading to a significantly worse baseline.

- There are almost no comparisons of log-likelihood values or values of normalisation constants for the proposed technique. While some are given in Figure 3C, they are only single run and only using approximated likelihoods. Due to this, the phrase "" significantly higher log-likelihoods"" in the conclusion is NOT supported by the data, given there is not enough data to test for significance or even measure the variability.

- The training is also cut short, or the RBMs trained are not powerful enough, since 3B shows clear artefacts in all samples, indicating that none of the machines approximate the dataset well. Since RBMS with enough latent variables are universal function approximators, we can not get a definitive statement of whether the proposed pretraining does allow for better likelihoods. Since the experiments are not very expensive, this reviewer would propose to at least repeat experiments in order to obtain error bars on Fig 3C. 

- The learning rate of 0.01 used in the experiments seems to be on the high end. This is not only bad for PCD training, but also for obtaining high likelihood values, and could explain big parts of the leveling out of the graph in Fig 3C. While it is okay to use a high learning rate in the beginning, keeping it constant over the course of training seems like an oversight.

- For PTT especially, there are no good comparisons that compare the quality of the samples in terms of representing the true distributions. While visual examples are shown that show visually good mixing, the baseline to compare to is again PCD, and not PT, nor stacked tempering. Since, again [40] showed that both alternatives clearly beat gibbs sampling given the same amount of resources, we do not know how good this sampling scheme really is, compared to strong baselines. This reviewer suggests to compare in at least one experiment the proposed approach to an approach with known normalisation constant and then measure how well estimators based on these samples work, similar to [*]. This would also partially verify 3F. 
- As an addendum to the previous point: [*] showed that the performance of AIS improves significantly with the choice of reference distribution. Using the same distribution as [*] or the pretrained distribution proposed in this work, might diminish any performance gains by PTT. This would still be a major improvement to the state of the art.



Language:
- While in general well written, this work reads like targeting a physics community, not the ML community. As a result, this paper includes  slang terms mostly encountered in statistical physics/thermodynamics, which do not have a clear meaning in the statistics community. This includes terms like ""phase transitions"" (first and second order), ""equilibrium models"" (not consequential for the article, nor the review, but this reviewer genuinely does not know what this term is supposed to mean), ""critical transitions"", ""relaxation times"", ""free energy"". Since most of these terms appear in sections where the authors try to explain the method and/or its consequences, a significant number of readers will not be able to understand those reasonings as they do not have a grasp of the physical analogies. This reviewer would propose to replace some of the terms by the statistical equivalent, or to introduce them. 
- Some of the explainations and reasonings are misleading. The initial paragraph highlights that RBMs are supposedly interpretable. While they are simple models, Binary RBMs are still universal function approximators and thus it is highly unlikely that the latent space has any meaning that aligns with any human interpretable semantics. If the authors disagree with this, This reviewer encourages them to add a citation to line 25.
- Missing citations: the datasets used should be cited.
- Figure references missing: the article does not always refer to the figure they are talking about, e..g, line 205. In general references should include the subfigure letter as the article does later, e.g., lines 263+

Limitations:
The authors mark ""Yes"" for point 2 ""Limitations"" on the checklist. This reviewer has not found that the authors discussed the limitations of their work. This especially includes the guidelines that say that authors should ""reflect on the scope of the claims made, e.g., if the approach was only tested on a few datasets or with a few runs."" However, in fairness, the authors answer ""[NO]"" in question 7, discussing the presence of error bars or significance tests to measure significance. However, since this is not part of the final publication, and the authors include misleading claims about significance of results, this must be discussed in the main text, or the phrasing weakened.

I have not found any information on runtime or CPU/GPU hours, but the CPU/GPU were reported, so point 8 is partially fulfilled.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The manuscript suggests to apply 

Aurélien Decelle and Cyril Furtlehner. Exact training of restricted Boltzmann machines on intrinsically low dimensional data. Physical Review Letters, 127(15):158303, 2021.

to initialise persistent contrastive divergence (PCD) learning for RBM training and estimating the log likelihood / partition function of RBMs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
While one may argue that the novelty is limited because the interesting theoretical work was done in the abovementioned paper by Aurélien Decelle and Cyril Furtlehner, the idea is sound and the approach may be useful in practice.

Weaknesses:
The novelty is limited because the interesting theoretical work was done in the abovementioned paper by Aurélien Decelle and Cyril Furtlehner.

My **main criticism** refers to the empirical evaluation, and I think these questions should be addressed:
- Was enough effort out into the baseline methods (including hyperparameter choice)?
- What would be an example where the PCA decomposition is misleading (i.e., not helping or even slowing down the process)?
- What about MNIST with all 10 digits?

**Details** (not ordered by importance):

* „On the diametrically opposite side (on interpretability)  are generative ConvNets [9, 10], where the energy function is formulated as a deep neural network, which are capable of synthesizing photorealistic images but are almost impossible to interpret as a physical model.“: Not clear, perhaps add half a sentence to elaborate.

* „second-order phase transition“: define what this is already in the beginning

* Beginning of section 2: I suggest to add the analysis in 

  Fischer, Igel. Bounding the Bias of Contrastive Divergence Learning. Neural Computation, 2011
  https://direct.mit.edu/neco/article-abstract/23/3/664/7646/Bounding-the-Bias-of-Contrastive-Divergence?redirectedFrom=fulltext

  to the discussion of the limitations of CD.

* „much better than those obtained with the standard Annealing Important Sampling (AIS) techniques“:
In [42], several methods are discussed, in particular one based on Bennett’s Acceptance Ratio method (BAR), which performed in general better than standard AIS. How does the proposed method perform in comparison to BAR?

* What if linear PCA is not well suited to find a good representation of the data because of a highly non-linear latent structure?

* I am not fully happy with the selected benchmark tasks. 
In particular: How does the method perform on MNIST with all 10 digits?

**Minor** comments: 

The reference list should be revised. Inconsistent capitalisation, author first name abbreviations, etc.

Limitations:
I think the limitations should have been explored in more depth.
What would be an example where the PCA decomposition is misleading (i.e., not helping or even slowing down the process)?

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This work extrapolates the concept of Linear Mode Connectivity (LMC) modulo model invariances to differentiable tree ensembles (DTE). The authors revealed that, in contrast to neural networks (NNs), permutation invariance is insufficient to provide LMC in DTE and propose two additional tree-specific invariances that enable LMC after taking them into account: subtree flip invariance and splitting order invariance. In addition, they provide a modified DTE architecture that does not posses these additional invariances, however still enjoys LMC with only permutation invariance akin to neural network models. This work proposes two algorithms for building LMC given two independently trained DTEs, based on similar methods from NN LMC literature. The claims are supported by a detailed empirical evaluation.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
Honestly, I enjoyed reading this paper. Although I am not specialized in tree ensembles, I have certain expertise in LMC, and was pleased to find that it is also relevant for DTE models. I think that this contribution is novel and significant.

The paper is very well-structured. It was very easy to follow despite having no significant experience in decision trees, the authors did a good job preparing the reader in Sec. 2. 

Section 3 presents the main contributions of this work, which is done very well using both detailed and intuitive text description and auxiliary images illustrating the main concepts.

Empirical evaluation is excellent, involving multiple datasets, hyperparameter options, and random seeds. The authors tackled many important questions concerning the study of LMC in DTEs and even compared with NN LMC, which I specifically liked.

Weaknesses:
It is hard for me to formulate substantial flaws in this work but a couple of remarks that I put in the next section. 

The main weakness of this work is lack of theoretical support and practical implications. However, I acknowledge that these are the same limitations that are attributed to LMC in neural networks, which is a significantly more broad and well-studied field than LMC in tree ensembles. I hope that future work will address these disadvantages in some way. 

Also, I believe that the text could be slightly polished to eliminate typos and small inaccuracies. For instance, the value $D$ in line 127 is not defined at its first occurrence.

Limitations:
The authors discuss the limitations of their methods in Section 3.2.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides an analysis of types of neural networks called soft trees from the linear mode connectivity point of view. The authors enumerate 3 types of invariances inherent to soft trees and study linear mode connectivity between different solutions (by solution they understand a trained ensemble of soft tree models) after weights or activations matching that account for these invariances. They also study linear mode connectivity for a special case of soft trees - decision list-based tree - that has only one type of invariance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well written
- Authors claim that it is the first paper to study linear mode connectivity for soft trees

Weaknesses:
## Insufficient contribution
- In my opinion, the main contribution of this paper is a showcase that different architectures need to account for different invariances when LMC is analyzed, e.g. MLP and soft trees have different invariances. I think that this insight alone is not enough for a paper, because it sounds quite obvious even without analysis.

## Questionable results
- It is very important to make sure that interpolation results are not computed between the models which are almost identical (that can happen if there is not enough diversity in training recipes). Could you please provide results with distances (any kind of them, e.g. L2 or cosine similarity) between the solutions in Figure. 5 for ""Naive"", ""Tree Permutation"" and ""Ours"" parameter transformations?
- I would expect decision list trees to be much weaker than soft trees because they have less parameters. Could you please report its performance or show me where I can find it?
- Model merging is mentioned as one of the applications for linear mode connectivity (LMC), however, no results for model merging are provided.
  - line 32: ""In addition, LMC also holds significant practical importance, enabling techniques such as model merging [6, 7] by weight-space parameter averaging.""

## Questionable explanation
- I could not find a related work section.
- What is ""Ours"" in Table 2?
- I did not find in the main text any explanation (even after looking into algorithms in appendix, which I found very confusing) for the operation of weights matching (WM) and activation matching (AM) in case of such invariances as ""Perm"", ""Order"" and ""Flip"" (Notation is from Table 1). Since invariances are the main part of the whole analysis, could you please elaborate more?
- Another important part of parameter transforms includes Linear Assignment Problem (LAP), but I could not find any details for it neither.

Limitations:
- There is no theoretical justification for why and in which scenarios linear mode connectivity exists for soft trees.
- The paper does not propose any practical application for the linear mode connectivity between soft trees. While it can be argued that this paper is an analysis paper, some practical applications can be useful in motivating this kind of analysis.
- I did not find the code of the project while in the survey it is written that code is provided in supplementary material.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper empirically shows that separately trained tree emsemble models can show Linear Mode Connectivity (LMC) when considering tree invariant operations.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The exploration of LMC on tree emsemble models is interesting.
- The computational process is clearly stated which makes this paper easy to follow.

------

After reading author rebuttal and discussion with other reviewers, I decide to increase my rating of this paper to borderline reject.

Weaknesses:
This paper does not provide any insights into the question of LMC in neural networks, as it is exploring a totally different model. Although it is always interesting to consider LMC in another senario, I find the contribution of this paper rather insignificant and incremental, since it is basically applying the same idea of [1] to another model. I do not want to deny the author's valueable efforts in exploring symmetry in a new model and using it to achieve LMC, but I just feel that the contribution of this paper may not be sufficient for it to be accepted by this conference.

One possible direction I can suggest for the authors to enhance the current paper is, if any non-trivial theory about LMC can be made on the tree ensemble model setting, then this work will be much more exciting. The underlying reason why neural networks can be made linear connected is not yet clear, and the is hard to study due to the non-linear nature of deep NNs. If the authors can show that the tree ensemble model can be an alternative model to study LMC from a theoretical perspective, then this will make the current work more valuable and intresting.

[1] Git Re-Basin: Merging Models modulo Permutation Symmetries

**Regard writting**
The intro is kind of confusing for readers who are not familiar with the tree ensemble models. It's even unclear whether 1) it is a new model ensembling method for neural networks, or 2) it is a new model, or 3) it is a training method. Although those questions are addressed after reading the detailed definition of tree ensemble in Section 2.2, I think it is better to make it clear in the intro to avoid any confusing.

Limitations:
Authors discussed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to achieve LMC for soft tree ensembles. Akin to achieve LMC for neural network after accounting for permutation invariance, the authors introduce three different kinds of invariance in soft tree ensembles: tree permutation invariance, subtree flip invariance, splitting order invariance. Additionally, the authors demonstrate that better LMC can be achieved after considering all three kinds of invariance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea of extending LMC from neural networks to differentiable tree ensembles is interesting. 
2. Invariances beyond permutation variance are identified for differentiable tree ensembles. The authors demonstrate the effectiveness of accounting for these invariances when doing matching.

Weaknesses:
1. I am not familiar with differentiable tree ensembles, therefore, I would suggest the authors put more efforts on explaining tree ensembles and illustrating the invariances.
2. Another concern is about the motivation. This study is motivated by the question ""Can LMC be achieved for soft tree ensembles?"" but why would we achieve LMC for the tree ensembles? I would expect more elaboration on the motivation.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This work extrapolates the concept of Linear Mode Connectivity (LMC) modulo model invariances to differentiable tree ensembles (DTE). The authors revealed that, in contrast to neural networks (NNs), permutation invariance is insufficient to provide LMC in DTE and propose two additional tree-specific invariances that enable LMC after taking them into account: subtree flip invariance and splitting order invariance. In addition, they provide a modified DTE architecture that does not posses these additional invariances, however still enjoys LMC with only permutation invariance akin to neural network models. This work proposes two algorithms for building LMC given two independently trained DTEs, based on similar methods from NN LMC literature. The claims are supported by a detailed empirical evaluation.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
Honestly, I enjoyed reading this paper. Although I am not specialized in tree ensembles, I have certain expertise in LMC, and was pleased to find that it is also relevant for DTE models. I think that this contribution is novel and significant.

The paper is very well-structured. It was very easy to follow despite having no significant experience in decision trees, the authors did a good job preparing the reader in Sec. 2. 

Section 3 presents the main contributions of this work, which is done very well using both detailed and intuitive text description and auxiliary images illustrating the main concepts.

Empirical evaluation is excellent, involving multiple datasets, hyperparameter options, and random seeds. The authors tackled many important questions concerning the study of LMC in DTEs and even compared with NN LMC, which I specifically liked.

Weaknesses:
It is hard for me to formulate substantial flaws in this work but a couple of remarks that I put in the next section. 

The main weakness of this work is lack of theoretical support and practical implications. However, I acknowledge that these are the same limitations that are attributed to LMC in neural networks, which is a significantly more broad and well-studied field than LMC in tree ensembles. I hope that future work will address these disadvantages in some way. 

Also, I believe that the text could be slightly polished to eliminate typos and small inaccuracies. For instance, the value $D$ in line 127 is not defined at its first occurrence.

Limitations:
The authors discuss the limitations of their methods in Section 3.2.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides an analysis of types of neural networks called soft trees from the linear mode connectivity point of view. The authors enumerate 3 types of invariances inherent to soft trees and study linear mode connectivity between different solutions (by solution they understand a trained ensemble of soft tree models) after weights or activations matching that account for these invariances. They also study linear mode connectivity for a special case of soft trees - decision list-based tree - that has only one type of invariance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well written
- Authors claim that it is the first paper to study linear mode connectivity for soft trees

Weaknesses:
## Insufficient contribution
- In my opinion, the main contribution of this paper is a showcase that different architectures need to account for different invariances when LMC is analyzed, e.g. MLP and soft trees have different invariances. I think that this insight alone is not enough for a paper, because it sounds quite obvious even without analysis.

## Questionable results
- It is very important to make sure that interpolation results are not computed between the models which are almost identical (that can happen if there is not enough diversity in training recipes). Could you please provide results with distances (any kind of them, e.g. L2 or cosine similarity) between the solutions in Figure. 5 for ""Naive"", ""Tree Permutation"" and ""Ours"" parameter transformations?
- I would expect decision list trees to be much weaker than soft trees because they have less parameters. Could you please report its performance or show me where I can find it?
- Model merging is mentioned as one of the applications for linear mode connectivity (LMC), however, no results for model merging are provided.
  - line 32: ""In addition, LMC also holds significant practical importance, enabling techniques such as model merging [6, 7] by weight-space parameter averaging.""

## Questionable explanation
- I could not find a related work section.
- What is ""Ours"" in Table 2?
- I did not find in the main text any explanation (even after looking into algorithms in appendix, which I found very confusing) for the operation of weights matching (WM) and activation matching (AM) in case of such invariances as ""Perm"", ""Order"" and ""Flip"" (Notation is from Table 1). Since invariances are the main part of the whole analysis, could you please elaborate more?
- Another important part of parameter transforms includes Linear Assignment Problem (LAP), but I could not find any details for it neither.

Limitations:
- There is no theoretical justification for why and in which scenarios linear mode connectivity exists for soft trees.
- The paper does not propose any practical application for the linear mode connectivity between soft trees. While it can be argued that this paper is an analysis paper, some practical applications can be useful in motivating this kind of analysis.
- I did not find the code of the project while in the survey it is written that code is provided in supplementary material.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper empirically shows that separately trained tree emsemble models can show Linear Mode Connectivity (LMC) when considering tree invariant operations.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The exploration of LMC on tree emsemble models is interesting.
- The computational process is clearly stated which makes this paper easy to follow.

------

After reading author rebuttal and discussion with other reviewers, I decide to increase my rating of this paper to borderline reject.

Weaknesses:
This paper does not provide any insights into the question of LMC in neural networks, as it is exploring a totally different model. Although it is always interesting to consider LMC in another senario, I find the contribution of this paper rather insignificant and incremental, since it is basically applying the same idea of [1] to another model. I do not want to deny the author's valueable efforts in exploring symmetry in a new model and using it to achieve LMC, but I just feel that the contribution of this paper may not be sufficient for it to be accepted by this conference.

One possible direction I can suggest for the authors to enhance the current paper is, if any non-trivial theory about LMC can be made on the tree ensemble model setting, then this work will be much more exciting. The underlying reason why neural networks can be made linear connected is not yet clear, and the is hard to study due to the non-linear nature of deep NNs. If the authors can show that the tree ensemble model can be an alternative model to study LMC from a theoretical perspective, then this will make the current work more valuable and intresting.

[1] Git Re-Basin: Merging Models modulo Permutation Symmetries

**Regard writting**
The intro is kind of confusing for readers who are not familiar with the tree ensemble models. It's even unclear whether 1) it is a new model ensembling method for neural networks, or 2) it is a new model, or 3) it is a training method. Although those questions are addressed after reading the detailed definition of tree ensemble in Section 2.2, I think it is better to make it clear in the intro to avoid any confusing.

Limitations:
Authors discussed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to achieve LMC for soft tree ensembles. Akin to achieve LMC for neural network after accounting for permutation invariance, the authors introduce three different kinds of invariance in soft tree ensembles: tree permutation invariance, subtree flip invariance, splitting order invariance. Additionally, the authors demonstrate that better LMC can be achieved after considering all three kinds of invariance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea of extending LMC from neural networks to differentiable tree ensembles is interesting. 
2. Invariances beyond permutation variance are identified for differentiable tree ensembles. The authors demonstrate the effectiveness of accounting for these invariances when doing matching.

Weaknesses:
1. I am not familiar with differentiable tree ensembles, therefore, I would suggest the authors put more efforts on explaining tree ensembles and illustrating the invariances.
2. Another concern is about the motivation. This study is motivated by the question ""Can LMC be achieved for soft tree ensembles?"" but why would we achieve LMC for the tree ensembles? I would expect more elaboration on the motivation.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The authors consider the Langevin process for sampling from a target distribution $\pi$. This process is known to be slow-converging for multimodal targets: in practice, it has been observed that the process gets ""stuck"" in some modes of the target, and do not ""reach"" other modes of the target. The authors provide theoretical results for this behavior. In Theorem 1, they prove that by evolving a particle with the Langevin process during exponential time (in the dimension), the particle will still be far away (in probability) from some modes. They also prove, in Theorem 2, that this negative result holds even when using the popular heuristic of ""annealing"" the Langevin process using intermediate distributions, obtained by adding different levels of Gaussian noise to the target samples. 

Instead, the authors propose running an alternative sampling process which they call ""Chained Langevin dynamics"". This consists in running ""annealed"" Langevin processes for each component of the target distribution, that is, for each $\pi(x_i | x_{-i})$. The authors estimate the score of each of these conditional targets using a score-matching loss, and empirically demonstrate the ability of their process to reach the different target modes in a limited time. Theoretically, they prove their process approximates the target (in TV divergence) in linear time (in the dimension).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors provide an interesting perspective on popular sampling processes, Langevin and its annealed counterpart. The paper is clearly written and the results are an interesting contribution to the sampling community.

Weaknesses:
See questions.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study Langevin dynamics (as well as its annealed counterpart) for gaussian mixtures and sub-gaussian mixtures. In Sec. 4, they prove that Langevin remains stuck in the ""dominant mode"" for an at least exponential time, a claim that is often made in the ML literature but which is never formally proved. In Sec. 5, they provide a sequential method to get rid of this dependence.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
It is healthy to finally have a paper that explicitly prove the claims made in the ML literature and that were known in practice for a long time.  Furthermore, it shows that, unlike what was believed, annealed Langevin also fails.

Weaknesses:
I do not understand why it is sensible to say that initially, $p_0$ should follow $P_0$, one of the component of the mixture, isn't it a rather strong assumption?

Limitations:
It seems like the assumption $p_0 \sim P_0$ is not enough justified. Also, I would have like an insight of the proof of the Theorems in Sec. 4.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
A new algorithm is proposed, called Chained Langevin Dynamics, to improve on the mode-seeking properties of Langevin Dynamics, after annleade Langevin Dynamics had been proposed but did not give significant improvements.
Results about the mode-seeking properties of the three algorithms are obtained.
The results of numerical experiments on synthetic and real image datasets are also shown.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
Very inspiring idea on how to improve mode-search for multimodal distributions.
Very clear presentation of premises and of the old and new algorithms.
The new algorithm looks very powerful.

Weaknesses:
No evident connection has been established between experiments and mathematical results.
The description/comment of experiments could have been more accurate (see Questions).

Limitations:
There is a section about limitations in the text.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies Langevin-based algorithms for sampling from multimodal distributions, motivated by generative modeling. The main content of the paper are lower bounds on the convergence of both Langevin and annealed Langevin for mixtures of Gaussian and sub-Gaussian distributions, as well as a proposed modification of the annealed Langevin dynamics to operate on coordinate patches one-at-a-time.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Sampling from multimodal distributions is an importnat problem both theoretically and practically.

- The Chained Langevin Dynamics algorithm that is proposed appears to be novel.

- The empirical results are promising, albeit in a rather contrived setting.

Weaknesses:
- The lower bounds hold only for the distance between the sample and the mean, rather than any standard notion of distance or divergence between probability measures. Moreover, I do not expect that these bounds imply such a quantity is large.

- Related to the above point, it is difficult to appreciate the significance of the lower bound since the lower bound does not depend on the separation between the means. In particular, it seems the lower bounds only show that the iterate remains roughly on the order of the larger variance which is, for example, not surprising in the case where the variances are all of the same order.

- The hidden constants in the $\Omega$ notation are important but difficult to find (as they are suppressed in the main text and some of the appendix). In particular, there should be dependence on the mixture weights but this can't be seen from their result.

- It is unclear if the upper bound in Theorem 5 can be instantiated for their algorithm (see question below).

Limitations:
The limitations of the work have been adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The authors consider the Langevin process for sampling from a target distribution $\pi$. This process is known to be slow-converging for multimodal targets: in practice, it has been observed that the process gets ""stuck"" in some modes of the target, and do not ""reach"" other modes of the target. The authors provide theoretical results for this behavior. In Theorem 1, they prove that by evolving a particle with the Langevin process during exponential time (in the dimension), the particle will still be far away (in probability) from some modes. They also prove, in Theorem 2, that this negative result holds even when using the popular heuristic of ""annealing"" the Langevin process using intermediate distributions, obtained by adding different levels of Gaussian noise to the target samples. 

Instead, the authors propose running an alternative sampling process which they call ""Chained Langevin dynamics"". This consists in running ""annealed"" Langevin processes for each component of the target distribution, that is, for each $\pi(x_i | x_{-i})$. The authors estimate the score of each of these conditional targets using a score-matching loss, and empirically demonstrate the ability of their process to reach the different target modes in a limited time. Theoretically, they prove their process approximates the target (in TV divergence) in linear time (in the dimension).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The authors provide an interesting perspective on popular sampling processes, Langevin and its annealed counterpart. The paper is clearly written and the results are an interesting contribution to the sampling community.

Weaknesses:
See questions.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study Langevin dynamics (as well as its annealed counterpart) for gaussian mixtures and sub-gaussian mixtures. In Sec. 4, they prove that Langevin remains stuck in the ""dominant mode"" for an at least exponential time, a claim that is often made in the ML literature but which is never formally proved. In Sec. 5, they provide a sequential method to get rid of this dependence.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
It is healthy to finally have a paper that explicitly prove the claims made in the ML literature and that were known in practice for a long time.  Furthermore, it shows that, unlike what was believed, annealed Langevin also fails.

Weaknesses:
I do not understand why it is sensible to say that initially, $p_0$ should follow $P_0$, one of the component of the mixture, isn't it a rather strong assumption?

Limitations:
It seems like the assumption $p_0 \sim P_0$ is not enough justified. Also, I would have like an insight of the proof of the Theorems in Sec. 4.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
A new algorithm is proposed, called Chained Langevin Dynamics, to improve on the mode-seeking properties of Langevin Dynamics, after annleade Langevin Dynamics had been proposed but did not give significant improvements.
Results about the mode-seeking properties of the three algorithms are obtained.
The results of numerical experiments on synthetic and real image datasets are also shown.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
Very inspiring idea on how to improve mode-search for multimodal distributions.
Very clear presentation of premises and of the old and new algorithms.
The new algorithm looks very powerful.

Weaknesses:
No evident connection has been established between experiments and mathematical results.
The description/comment of experiments could have been more accurate (see Questions).

Limitations:
There is a section about limitations in the text.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper studies Langevin-based algorithms for sampling from multimodal distributions, motivated by generative modeling. The main content of the paper are lower bounds on the convergence of both Langevin and annealed Langevin for mixtures of Gaussian and sub-Gaussian distributions, as well as a proposed modification of the annealed Langevin dynamics to operate on coordinate patches one-at-a-time.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Sampling from multimodal distributions is an importnat problem both theoretically and practically.

- The Chained Langevin Dynamics algorithm that is proposed appears to be novel.

- The empirical results are promising, albeit in a rather contrived setting.

Weaknesses:
- The lower bounds hold only for the distance between the sample and the mean, rather than any standard notion of distance or divergence between probability measures. Moreover, I do not expect that these bounds imply such a quantity is large.

- Related to the above point, it is difficult to appreciate the significance of the lower bound since the lower bound does not depend on the separation between the means. In particular, it seems the lower bounds only show that the iterate remains roughly on the order of the larger variance which is, for example, not surprising in the case where the variances are all of the same order.

- The hidden constants in the $\Omega$ notation are important but difficult to find (as they are suppressed in the main text and some of the appendix). In particular, there should be dependence on the mixture weights but this can't be seen from their result.

- It is unclear if the upper bound in Theorem 5 can be instantiated for their algorithm (see question below).

Limitations:
The limitations of the work have been adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,No,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
After rebuttal

While I think there are still some problems with this paper, e.g. the short training duration, and the slight exaggeration of claims (that SHED outperforms UED). I think, however, that the idea is nice, and getting RL environment design to work better is a good goal.


-----


This paper aims to improve Unsupervised Environment Design in two ways.
First, it introduces a hierarchical MDP formulation, where the top level corresponds to the teacher, and the lower level corresponds to the learning agent. Each transition in the top-level MDP involves training the lower level agent on generated levels. Related to this, they develop a state representation for the adversary, which is the performance of the agent on a fixed set of diverse levels.

Separately to this, they use a diffusion model to upsample the number of experiences for the teacher, effectively training on synthetic data, to improve sample efficiency.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- I think the H-MDP formulation itself is very valuable; it moves away from treating the generation of environments as a black-box, sparse reward, multi-step generation process (as in PAIRED), and towards a more informed process, where the teacher gets feedback in terms of the state (i.e., the performance vector).
- The analysis in the appendix investigating the ability of the model to generate good synthetic trajectories is useful.

Weaknesses:
- Major
	- The results do not very convincingly demonstrate that SHED is better than current SoTA. Looking at figure 3 particularly, I would say that ACCEL has about the same performance as SHED. However, comparing against the RL baseline, SHED does do better.
	- The method is limited in the types of environments it can generate. For instance, mazes are generated using an LLM instead of directly placing blocks. This method therefore is not quite as broad in scope as PAIRED or ACCEL, which can generate arbitrary environments.
	- Relatedly, in the minigrid experiments, do all methods generate the levels in the same way using an LLM, providing the difficulty numbers? It would be good to compare this against the standard version of ACCEL that directly places blocks in the maze level, as it does not have the same restriction as SHED.
- Minor
	- The figures can be improved:
		- Make the alpha value on the error bars a bit less
		- Keep colours consistent across figures, so the same method has the same colour
		- Keep capitalisation consistent across the figure labels.
	- line 80, the period ends on the line after the maths, it should end on the same line.
	- Footnote 1: Jiang et al. (2021) use (amongst others) the positive value loss, which is not quite the GAE, as it clips it at zero before summing.
	- equation one, you use $\beta_t$ but $t$ does not seem to be defined? Should this be $\beta_k$?
	- Line 159, PARIED should be PAIRED
	- There is no reward scale in figure 4
	- Figure 9's caption can be made clearer. I understand it to be the performance of each method in different testing environments. 
	- Line 718 does not link to a figure.
	- Figure 11's caption: zero-shot and not zeros-shot
	- Capitalise the first word in the title of appendix C.2
	- In terms of notation, in line 96, $\pi^*$ usually has a dependence on $\theta$ (e.g. $\pi^*_\theta$) to indicate it is optimal w.r.t. that particular level.
	- Line 217, maybe add a citation to the first sentence, as I thought that is what you do, which confused me for a second.
	- line 237 space after period.
	- Line 241 ""given"" instead of giving?
	- Lines 296 - 297 are a bit confusing, as the word environment is used three times.
	- The assumption in theorem 1 is pretty strong.

Limitations:
I think the authors can list a few more limitations. 
Primarily, the restriction on the type of environment that can be generated, i.e., it needs numerical parameters, and generating a maze outright is challenging. This is quite a large difference to prior settings.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel approach to Unsupervised Environment Design (UED) that addresses the challenges of efficiency by introducing a hierarchical MDP framework and using synthetic data. This framework involves an upper-level RL teacher agent that generates training environments tailored to a lower-level student agent's capabilities. The paper proposes the Synthetically-enhanced Hierarchical Environment Design (SHED) method, which uses generative modeling to create synthetic trajectory datasets, thereby reducing the resource-intensive interactions between agents and environments. The effectiveness of SHED is demonstrated through empirical experiments across various domains, showing superior performance compared to existing UED methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The use of diffusion models to generate synthetic trajectories is a novel approach that effectively reduces the computational burden of training the teacher agent.
- The paper provides comprehensive experiments across different domains, demonstrating the effectiveness and robustness of the proposed method compared to state-of-the-art UED approaches.

Weaknesses:
- The proposed method introduces significant complexity, particularly in the implementation of the hierarchical MDP and the generative modeling components. This might limit the accessibility and reproducibility of the approach.
- While the empirical results are promising, the evaluation is limited to a few specific domains. It would be beneficial to see broader applicability across more diverse and complex environments.
- Figure 4 is not properly formatted (no values on the axes).

Limitations:
The limitations are discussed in Appendix F.1 but I think the authors should discuss the limitations in the main paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers the Unsupervised Environment Design problem, where a teacher agent seeks to design environments to train a student. Methods such as PLR, PAIRED and ACCEL have recently shown promising performance for random, RL and evolutionary generators. This paper proposes a handful of modifications, using RL with a different objective vs. PAIRED (performance on held out set vs. regret) and also proposes to add synthetic data to accelerate the RL process.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* This is an interesting method in a relevant area of research. UED seems to be one of the most active areas of research with plenty of opportunities for impact.
* The use of evaluation environments is sensible and novel.
* The idea of combining this with Genie is incredibly exciting. It would be interesting to hear how this could be possible or could work. Is there any way to show a simple proof of concept?

Weaknesses:
* There appear to be two confounding features of the method, the new objective for PAIRED and then the synthetic data. Why do they make sense to combine in this way? It just feels like the authors tried to do ""enough for a paper"" rather than contribute something meaningful that people can build on. I say this because its unclear how these two independent features interact with other existing algorithms. Maybe we should just do ACCEL with synthetic data for instance? Did the authors try that? If it is in the Appendix already and I missed it then I will increase my score.
* The performance gains are fairly minor, and presented in an unclear fashion with just a bunch of curves on a single plot. Can we get some more rigorous analysis for example using the recommendations from Agarwal et al, ""Deep Reinforcement Learning at the Edge of the Statistical Precipice""?
* The Maze experiment seems to have many inductive biases and seems distinct from the diffusion based approach for BipedalWalker and LunarLander. What happens if ACCEL has access to ChatGPT as an editor and then uses replay? This seems like a simpler extension that alone could be a strong paper - although it would resemble ELM (Lehman et al 2022) so it wouldn't be particularly novel.
* The related work is very light. This is disappointing since the paper builds on so many related areas, such as synthetic data, diffusion models, UED, language models for evolution, procedural content generation etc.

Limitations:
Covered in the Appendix.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors of this paper use hierarchical MDP formulation and a teacher agent trained by RL to perform curriculum learning. To address the sparse data available for the teacher agent, this paper uses diffusion models to synthesize datasets for training. This paper performs experiments on lunar lander and bipedal walker environments to validate their claim.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Data sparsity is one of the main limitations of using a teacher agent in curricular RL. This paper uses diffusion models to synthesize a dataset for the teacher agent.

Weaknesses:
- This paper designs the teacher agent via hierarchical MDP to model the learning process of the student agent to perform curricular RL. However, Fingerprint Policy Optimization (Paul et al, 2019) also has a similar idea of modeling the learning process of the student agent. It would be interesting to explain more about how this paper's idea is related and contributes to this line of thought.

- A fully trained algorithm on the BipedalWalker should approach a cumulative reward of 300. Even the modified version used in the ACCEL paper is measured on a scale of 0 out of 300. However, from Figure 3, it appears all baselines perform less than 50 on the BipedalWalker benchmark. It is questionable whether all baselines were fully trained with the right settings. Also, the performance of the proposed algorithm and those of the baselines are statistically too similar to see whether SHED improves over the baselines in Lunar Lander and BipedalWalker benchmarks. Finally, other than the version of ACCEL in this paper not performing as well as the ACCEL in the original paper, I am curious whether ACCEL can be considered state-of-the-art in the benchmarks as written in line 324. Genetic Curriculum (Song et al, 2022) reports higher cumulative reward on the BipedalWalkerHardcore environment.

- Figure 4 has no scale on timestep and reward.

Limitations:
The authors has addressed the limitations of this paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
After rebuttal

While I think there are still some problems with this paper, e.g. the short training duration, and the slight exaggeration of claims (that SHED outperforms UED). I think, however, that the idea is nice, and getting RL environment design to work better is a good goal.


-----


This paper aims to improve Unsupervised Environment Design in two ways.
First, it introduces a hierarchical MDP formulation, where the top level corresponds to the teacher, and the lower level corresponds to the learning agent. Each transition in the top-level MDP involves training the lower level agent on generated levels. Related to this, they develop a state representation for the adversary, which is the performance of the agent on a fixed set of diverse levels.

Separately to this, they use a diffusion model to upsample the number of experiences for the teacher, effectively training on synthetic data, to improve sample efficiency.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- I think the H-MDP formulation itself is very valuable; it moves away from treating the generation of environments as a black-box, sparse reward, multi-step generation process (as in PAIRED), and towards a more informed process, where the teacher gets feedback in terms of the state (i.e., the performance vector).
- The analysis in the appendix investigating the ability of the model to generate good synthetic trajectories is useful.

Weaknesses:
- Major
	- The results do not very convincingly demonstrate that SHED is better than current SoTA. Looking at figure 3 particularly, I would say that ACCEL has about the same performance as SHED. However, comparing against the RL baseline, SHED does do better.
	- The method is limited in the types of environments it can generate. For instance, mazes are generated using an LLM instead of directly placing blocks. This method therefore is not quite as broad in scope as PAIRED or ACCEL, which can generate arbitrary environments.
	- Relatedly, in the minigrid experiments, do all methods generate the levels in the same way using an LLM, providing the difficulty numbers? It would be good to compare this against the standard version of ACCEL that directly places blocks in the maze level, as it does not have the same restriction as SHED.
- Minor
	- The figures can be improved:
		- Make the alpha value on the error bars a bit less
		- Keep colours consistent across figures, so the same method has the same colour
		- Keep capitalisation consistent across the figure labels.
	- line 80, the period ends on the line after the maths, it should end on the same line.
	- Footnote 1: Jiang et al. (2021) use (amongst others) the positive value loss, which is not quite the GAE, as it clips it at zero before summing.
	- equation one, you use $\beta_t$ but $t$ does not seem to be defined? Should this be $\beta_k$?
	- Line 159, PARIED should be PAIRED
	- There is no reward scale in figure 4
	- Figure 9's caption can be made clearer. I understand it to be the performance of each method in different testing environments. 
	- Line 718 does not link to a figure.
	- Figure 11's caption: zero-shot and not zeros-shot
	- Capitalise the first word in the title of appendix C.2
	- In terms of notation, in line 96, $\pi^*$ usually has a dependence on $\theta$ (e.g. $\pi^*_\theta$) to indicate it is optimal w.r.t. that particular level.
	- Line 217, maybe add a citation to the first sentence, as I thought that is what you do, which confused me for a second.
	- line 237 space after period.
	- Line 241 ""given"" instead of giving?
	- Lines 296 - 297 are a bit confusing, as the word environment is used three times.
	- The assumption in theorem 1 is pretty strong.

Limitations:
I think the authors can list a few more limitations. 
Primarily, the restriction on the type of environment that can be generated, i.e., it needs numerical parameters, and generating a maze outright is challenging. This is quite a large difference to prior settings.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a novel approach to Unsupervised Environment Design (UED) that addresses the challenges of efficiency by introducing a hierarchical MDP framework and using synthetic data. This framework involves an upper-level RL teacher agent that generates training environments tailored to a lower-level student agent's capabilities. The paper proposes the Synthetically-enhanced Hierarchical Environment Design (SHED) method, which uses generative modeling to create synthetic trajectory datasets, thereby reducing the resource-intensive interactions between agents and environments. The effectiveness of SHED is demonstrated through empirical experiments across various domains, showing superior performance compared to existing UED methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The use of diffusion models to generate synthetic trajectories is a novel approach that effectively reduces the computational burden of training the teacher agent.
- The paper provides comprehensive experiments across different domains, demonstrating the effectiveness and robustness of the proposed method compared to state-of-the-art UED approaches.

Weaknesses:
- The proposed method introduces significant complexity, particularly in the implementation of the hierarchical MDP and the generative modeling components. This might limit the accessibility and reproducibility of the approach.
- While the empirical results are promising, the evaluation is limited to a few specific domains. It would be beneficial to see broader applicability across more diverse and complex environments.
- Figure 4 is not properly formatted (no values on the axes).

Limitations:
The limitations are discussed in Appendix F.1 but I think the authors should discuss the limitations in the main paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers the Unsupervised Environment Design problem, where a teacher agent seeks to design environments to train a student. Methods such as PLR, PAIRED and ACCEL have recently shown promising performance for random, RL and evolutionary generators. This paper proposes a handful of modifications, using RL with a different objective vs. PAIRED (performance on held out set vs. regret) and also proposes to add synthetic data to accelerate the RL process.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* This is an interesting method in a relevant area of research. UED seems to be one of the most active areas of research with plenty of opportunities for impact.
* The use of evaluation environments is sensible and novel.
* The idea of combining this with Genie is incredibly exciting. It would be interesting to hear how this could be possible or could work. Is there any way to show a simple proof of concept?

Weaknesses:
* There appear to be two confounding features of the method, the new objective for PAIRED and then the synthetic data. Why do they make sense to combine in this way? It just feels like the authors tried to do ""enough for a paper"" rather than contribute something meaningful that people can build on. I say this because its unclear how these two independent features interact with other existing algorithms. Maybe we should just do ACCEL with synthetic data for instance? Did the authors try that? If it is in the Appendix already and I missed it then I will increase my score.
* The performance gains are fairly minor, and presented in an unclear fashion with just a bunch of curves on a single plot. Can we get some more rigorous analysis for example using the recommendations from Agarwal et al, ""Deep Reinforcement Learning at the Edge of the Statistical Precipice""?
* The Maze experiment seems to have many inductive biases and seems distinct from the diffusion based approach for BipedalWalker and LunarLander. What happens if ACCEL has access to ChatGPT as an editor and then uses replay? This seems like a simpler extension that alone could be a strong paper - although it would resemble ELM (Lehman et al 2022) so it wouldn't be particularly novel.
* The related work is very light. This is disappointing since the paper builds on so many related areas, such as synthetic data, diffusion models, UED, language models for evolution, procedural content generation etc.

Limitations:
Covered in the Appendix.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors of this paper use hierarchical MDP formulation and a teacher agent trained by RL to perform curriculum learning. To address the sparse data available for the teacher agent, this paper uses diffusion models to synthesize datasets for training. This paper performs experiments on lunar lander and bipedal walker environments to validate their claim.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Data sparsity is one of the main limitations of using a teacher agent in curricular RL. This paper uses diffusion models to synthesize a dataset for the teacher agent.

Weaknesses:
- This paper designs the teacher agent via hierarchical MDP to model the learning process of the student agent to perform curricular RL. However, Fingerprint Policy Optimization (Paul et al, 2019) also has a similar idea of modeling the learning process of the student agent. It would be interesting to explain more about how this paper's idea is related and contributes to this line of thought.

- A fully trained algorithm on the BipedalWalker should approach a cumulative reward of 300. Even the modified version used in the ACCEL paper is measured on a scale of 0 out of 300. However, from Figure 3, it appears all baselines perform less than 50 on the BipedalWalker benchmark. It is questionable whether all baselines were fully trained with the right settings. Also, the performance of the proposed algorithm and those of the baselines are statistically too similar to see whether SHED improves over the baselines in Lunar Lander and BipedalWalker benchmarks. Finally, other than the version of ACCEL in this paper not performing as well as the ACCEL in the original paper, I am curious whether ACCEL can be considered state-of-the-art in the benchmarks as written in line 324. Genetic Curriculum (Song et al, 2022) reports higher cumulative reward on the BipedalWalkerHardcore environment.

- Figure 4 has no scale on timestep and reward.

Limitations:
The authors has addressed the limitations of this paper.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
Proposed a graph based learnable multi-agent framework. The framework consists of multiple stages : Forwarding: Election (K: Answer agents; R: Reviewer) -> Review -> K Discuss till a final conclusion is reached. Proposed a mechanism to learn the graph connections dynamically. 

The major Contributions Introduced in the paper: (A)  A new swarm intelligence geo-local framework smileGeo; (B) Dynamic learning strategy; (C) A new Geo-dataset (test mainly).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The major strengths of the proposed smileGeo frameworks are: 

 (a) the learnable Graph based communication strategy seems works well empirically. In table 2, authors demonstrated that it helps achieve better acc, but lower average token costs. 

(b) The proposed method is also scalable as shown in table 3. 

(c) Used attention-based GNN to predict optimal connections and optimal election. Also empirically justified the effectiveness of attention based GNN.

(d) Also constructed Simple rules of updating edges(connections) that works well in practice.

Weaknesses:
The major weaknesses are as follows:

(a) Comparisons with baselines seems unfair. 

(b) Missing details of the evaluation setup, metrics, etc.

Limitations:
yes, the authors adequately addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This works proposes a new visual geo-localization framework with multiple LVLM (Large Vision Language Model) agents. The agents communicate with each other to estimate the geo-location of the input image. A dynamic learning strategy is proposed to optimize the communication patterns among agents to improve efficiency. The method is evaluated on the proposed GeoGlobe dataset.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
+ The idea of tacking worldwide city-level geo-localization with multiple LVLM agents is very interesting. 
+ The result is surprisingly good with zero-shot setting, which is even better than powerful close-source models.
+ Detailed comparison with other agent-based methods is provided. The ablation study on the number of agents is also very detailed.
+ The writing is easy to follow.

Weaknesses:
- The authors could make the geo-localization setting more clear in the introduction, for example, the paper focuses on worldwide city-level geo-localization. There are lots of different settings for geo-localization problem and this could be confusing for some researchers.
- This paper provides a comparison with three traditional geo-localization methods, i.e., NetVLAD, GeM, and CosPlace. However, these three methods are either retrieval-based landmark matching methods or fine-grained classification-based place recognition methods. It would be better to provide a direct comparison with worldwide geo-localization method on city-level setting, e.g., [A]. Although I believe LVLM-based method is better at this setting, a comparison can make it more convincing.

[A] Pramanick, Shraman, et al. ""Where in the world is this image? transformer-based geo-localization in the wild."" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.
- There are only two qualitative results in the appendix. Given that the accuracy is over 60%, it should be easy to find successful and failed cases to demonstrate the actual output cases of the proposed methods. It can also better illustrate how multiple agents help the geo-localization process.
- There are also some existing worldwide geo-localization datasets that could be used for more comprehensive evaluation, e.g., IM2GPS3K, YFCC4K.

Limitations:
The authors mentioned the limitations in the checklist.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces smileGeo, a novel framework for visual geo-localization, which involves identifying the geographic location of an image. The authors argue that while Large Vision-Language Models (LVLMs) show promise in this area, their individual performance is limited. SmileGeo leverages the concept of ""swarm intelligence"" by enabling multiple LVLMs to collaborate and refine their location predictions through a multi-stage review process. To enhance efficiency, the framework incorporates a dynamic learning strategy that optimizes the selection of LVLMs for each image. Furthermore, the paper introduces ""GeoGlobe,"" a new dataset designed to evaluate visual geo-localization models in open-world scenarios where many images depict locations not seen during training. Experimental results demonstrate that smileGeo outperforms existing single LVLMs and image retrieval methods, highlighting the effectiveness of collaborative learning for visual geo-localization.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* The idea of using an ensemble of networks/agents for geolocalization is interesting and novel. The authors propose a graph-based social network to enable collaboration between the agents.
* The ability to search the internet and provide the agents with relevant information is interesting and improves the performance on the task of geolocalization.
* The paper proposes GeoGlobe, a new dataset for benchmarking models on the task of geo-localizing landmarks. The dataset could be utilized in future for other learning based geospatial tasks.

Weaknesses:
* The paper only seems to tackle the problem of geolocalizing **landmark images**. While this is a challenging problem, the current literature [1, 2, 3] has already tried to address the problem of geolocalizing arbitrary ground-level images. The latter problem requires learning sophisticated geographic and visual features. I think even searching the internet cannot effectively solve the geolocalization problem for non-landmark images.
* Limited applicability: The framework is built entirely upon the capabilities of different LVLMs (e.g. GPT4, LLaVA, etc). It seems the framework cannot generalize beyond the training data used for training LLMs.
* The work fails to address the practical applications and real-life use cases of the framework. Why do we require such a framework?
* The limitation and failure cases are not adequately mentioned in the paper.

[1] Vivanco Cepeda, Vicente, Gaurav Kumar Nayak, and Mubarak Shah. ""Geoclip: Clip-inspired alignment between locations and images for effective worldwide geo-localization."" Advances in Neural Information Processing Systems 36 (2023).

[2] Haas, Lukas, Michal Skreta, Silas Alberti, and Chelsea Finn. ""Pigeon: Predicting image geolocations."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12893-12902. 2024.

[3] Berton, Gabriele, Carlo Masone, and Barbara Caputo. ""Rethinking visual geo-localization for large-scale applications."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4878-4888. 2022.

Limitations:
Limitations are insufficiently addressed in the paper. The future works mentioned in the conclusion are vague and fail to specify specific future directions for the work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
Proposed a graph based learnable multi-agent framework. The framework consists of multiple stages : Forwarding: Election (K: Answer agents; R: Reviewer) -> Review -> K Discuss till a final conclusion is reached. Proposed a mechanism to learn the graph connections dynamically. 

The major Contributions Introduced in the paper: (A)  A new swarm intelligence geo-local framework smileGeo; (B) Dynamic learning strategy; (C) A new Geo-dataset (test mainly).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The major strengths of the proposed smileGeo frameworks are: 

 (a) the learnable Graph based communication strategy seems works well empirically. In table 2, authors demonstrated that it helps achieve better acc, but lower average token costs. 

(b) The proposed method is also scalable as shown in table 3. 

(c) Used attention-based GNN to predict optimal connections and optimal election. Also empirically justified the effectiveness of attention based GNN.

(d) Also constructed Simple rules of updating edges(connections) that works well in practice.

Weaknesses:
The major weaknesses are as follows:

(a) Comparisons with baselines seems unfair. 

(b) Missing details of the evaluation setup, metrics, etc.

Limitations:
yes, the authors adequately addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This works proposes a new visual geo-localization framework with multiple LVLM (Large Vision Language Model) agents. The agents communicate with each other to estimate the geo-location of the input image. A dynamic learning strategy is proposed to optimize the communication patterns among agents to improve efficiency. The method is evaluated on the proposed GeoGlobe dataset.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
+ The idea of tacking worldwide city-level geo-localization with multiple LVLM agents is very interesting. 
+ The result is surprisingly good with zero-shot setting, which is even better than powerful close-source models.
+ Detailed comparison with other agent-based methods is provided. The ablation study on the number of agents is also very detailed.
+ The writing is easy to follow.

Weaknesses:
- The authors could make the geo-localization setting more clear in the introduction, for example, the paper focuses on worldwide city-level geo-localization. There are lots of different settings for geo-localization problem and this could be confusing for some researchers.
- This paper provides a comparison with three traditional geo-localization methods, i.e., NetVLAD, GeM, and CosPlace. However, these three methods are either retrieval-based landmark matching methods or fine-grained classification-based place recognition methods. It would be better to provide a direct comparison with worldwide geo-localization method on city-level setting, e.g., [A]. Although I believe LVLM-based method is better at this setting, a comparison can make it more convincing.

[A] Pramanick, Shraman, et al. ""Where in the world is this image? transformer-based geo-localization in the wild."" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.
- There are only two qualitative results in the appendix. Given that the accuracy is over 60%, it should be easy to find successful and failed cases to demonstrate the actual output cases of the proposed methods. It can also better illustrate how multiple agents help the geo-localization process.
- There are also some existing worldwide geo-localization datasets that could be used for more comprehensive evaluation, e.g., IM2GPS3K, YFCC4K.

Limitations:
The authors mentioned the limitations in the checklist.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces smileGeo, a novel framework for visual geo-localization, which involves identifying the geographic location of an image. The authors argue that while Large Vision-Language Models (LVLMs) show promise in this area, their individual performance is limited. SmileGeo leverages the concept of ""swarm intelligence"" by enabling multiple LVLMs to collaborate and refine their location predictions through a multi-stage review process. To enhance efficiency, the framework incorporates a dynamic learning strategy that optimizes the selection of LVLMs for each image. Furthermore, the paper introduces ""GeoGlobe,"" a new dataset designed to evaluate visual geo-localization models in open-world scenarios where many images depict locations not seen during training. Experimental results demonstrate that smileGeo outperforms existing single LVLMs and image retrieval methods, highlighting the effectiveness of collaborative learning for visual geo-localization.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* The idea of using an ensemble of networks/agents for geolocalization is interesting and novel. The authors propose a graph-based social network to enable collaboration between the agents.
* The ability to search the internet and provide the agents with relevant information is interesting and improves the performance on the task of geolocalization.
* The paper proposes GeoGlobe, a new dataset for benchmarking models on the task of geo-localizing landmarks. The dataset could be utilized in future for other learning based geospatial tasks.

Weaknesses:
* The paper only seems to tackle the problem of geolocalizing **landmark images**. While this is a challenging problem, the current literature [1, 2, 3] has already tried to address the problem of geolocalizing arbitrary ground-level images. The latter problem requires learning sophisticated geographic and visual features. I think even searching the internet cannot effectively solve the geolocalization problem for non-landmark images.
* Limited applicability: The framework is built entirely upon the capabilities of different LVLMs (e.g. GPT4, LLaVA, etc). It seems the framework cannot generalize beyond the training data used for training LLMs.
* The work fails to address the practical applications and real-life use cases of the framework. Why do we require such a framework?
* The limitation and failure cases are not adequately mentioned in the paper.

[1] Vivanco Cepeda, Vicente, Gaurav Kumar Nayak, and Mubarak Shah. ""Geoclip: Clip-inspired alignment between locations and images for effective worldwide geo-localization."" Advances in Neural Information Processing Systems 36 (2023).

[2] Haas, Lukas, Michal Skreta, Silas Alberti, and Chelsea Finn. ""Pigeon: Predicting image geolocations."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 12893-12902. 2024.

[3] Berton, Gabriele, Carlo Masone, and Barbara Caputo. ""Rethinking visual geo-localization for large-scale applications."" In Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition, pp. 4878-4888. 2022.

Limitations:
Limitations are insufficiently addressed in the paper. The future works mentioned in the conclusion are vague and fail to specify specific future directions for the work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This work introduces a novel approach to diagnosing Alzheimer's Disease using a decentralized expert system. This system leverages blockchain technology and Federated Learning to enhance data privacy and manage large volumes of MRI data effectively. The key innovation lies in integrating these technologies to address the challenges of traditional diagnostic methods, which often suffer from delays and inaccuracies, especially in the early stages of the disease.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. This work presents a pioneering integration of blockchain technology and Federated Learning to enhance Alzheimer's Disease (AD) diagnostics, addressing privacy concerns and data management challenges.
2. The proposed decentralized expert system architecture, which includes anomaly detection for patient-submitted data, showcases a comprehensive approach to AD diagnostics, emphasizing AI-driven MRI analysis.

Weaknesses:
1. While the system shows promising results, the article does not provide extensive comparative data against traditional centralized systems or other decentralized approaches, which could validate its superiority more robustly. This work lacks of comparative performance data.
2. The complexity of the blockchain and Federated Learning components might pose usability challenges for less technically adept users, potentially affecting the system's adoption.
3. There are no more details of the algorithms this work used, maybe give out more meaningful algorithm design for the specific model you are using.

Limitations:
1. The accuracy of the AI model heavily depends on the quality and consistency of input data, which might vary significantly across different healthcare settings.
2. The use of advanced technologies such as blockchain might limit the accessibility of the system for users not familiar with such technology, potentially restricting its applicability.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors assume that applying blockchain platforms to combine datasets for Alzheimer’s Disease and then using federated learning for multi-centralized training can improve diagnostic performance. However, the manuscript lacks technical details and experimental evidence. All descriptions are conceptual, making the manuscript a proposal rather than a technical paper.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
It is interesting to apply blockchain platforms to combine datasets for Alzheimer’s Disease and then using federated learning for multi-centralized training.

Weaknesses:
There are no technical details and no experiments. Details can be found in Questions and Limitations.

Limitations:
1.	No technical details and experiments.
2.	The literature review of Alzheimer’s Disease diagnosis is not complete, especially regarding AI-based approaches.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a decentralized expert system designed to predict early-stage Alzheimer's Disease using AI-driven MRI analysis. The system leverages blockchain technology and Federated Learning to ensure data privacy and security while performing anomaly detection on patient-submitted data. The architecture includes a Web3 application for patients to upload biological information and MRI images securely. The decentralized approach aims to improve early detection and intervention for Alzheimer's Disease, providing a more comprehensive representation of AD patterns and enhancing model performance through data diversity.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper encapsulates a few novel ideas. They can be summarized as follows:

1. Handling the security and sensitivity of patient medical information is of paramount importance. The authors were motivated by a very relevant problem and presented an approach to blockchain technology with stated aim of providing robust data privacy and security. By building on decades on research on this topic, this approach has the potential to be extended in future with the general updates in this domain.
2. While there are some confusion around their use case (see weakness below), the authors leveraging Federated Learning and a decentralized system to mitigate the challenges associated with model training on centralized data repositories, such as data bottlenecks and privacy concerns
3. The system aims to provide early-stage prediction of Alzheimer's Disease, which is crucial for timely intervention and improved patient outcomes.

Weaknesses:
However given the commendable motivations there are several challenges with the current paper,

1. First and perhaps the most important aspect is that the paper fails to present the real-world challenges associated with the adoption of such decentralized approaches, especially as it pertains to patients engaging with blockchain wallets and data submission interfaces. Also, the primary use-case for the decentralized approach is not evident - is model training the prime use-case or is the main use case patients being able to generate inferences on their own medical records. Overall, the usage scenario around the setup needs to be better motivated and established
2. The paper also lacks formalism around the presentation. For example, if the primary contribution is the architecture around the decentralized AI approach, the design principles needs to be better justified and articulated. A system architecture diagrams needs to be established as well. Similarly, the ""proof"" around the decentralized approach is not a rigorous mathematical proof. Rather the logic is derived from a hypothesis that more diverse data should lead to a better model. This is a hypothesis at the best and needs to be experimentally validates
3. Finally, the paper is lacking in experimental validation. For example, the proof needs to be backed by real world experiments. Also, this is not the first paper to posit a federated learning approach to medical AI prediction. Some of the SOTA methods in this space needs to be compared against

Limitations:
Please see the weakness above

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces an innovative decentralized expert system designed for early prediction of Alzheimer's Disease (AD), leveraging blockchain technology and Federated Learning. Traditional diagnostic methods often result in delays and imprecision, particularly in early-stage AD detection, while centralized data repositories face challenges in managing vast volumes of MRI data and maintaining patient privacy. The proposed system addresses these issues by combining blockchain for secure, decentralized data management and Federated Learning for collaborative AI model training across multiple institutions. The system includes robust anomaly detection mechanisms to ensure data quality and integrity, enabling precise early-stage AD predictions. This comprehensive approach aims to revolutionize disease diagnostics by enhancing data privacy, security, and collaborative efforts in the medical community.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents a novel integration of blockchain technology and Federated Learning for early AD prediction, which is innovative in addressing data privacy, security, and collaborative AI model training.
- The proposed system is well-conceived, with a detailed architecture and implementation strategy. The inclusion of anomaly detection mechanisms to ensure data quality adds robustness to the system.
- The approach has significant potential to improve early-stage AD detection, which is crucial for timely intervention and better patient outcomes. The decentralized nature of the system promotes data privacy and security, addressing major concerns in medical data management.

Weaknesses:
- The integration of blockchain and Federated Learning introduces significant computational complexity and potential delays due to off-chain processing and communication overhead.
- The system's scalability is a concern as the volume of data and the number of users increase, necessitating ongoing optimization to ensure efficient performance.

Limitations:
The authors have addressed several limitations, including data quality and consistency, computational complexity, and model generalizability. However, further discussion may be needed on:
- Ensuring that the AI model is unbiased and fair across different demographic groups can be difficult, especially if the training data is not representative.
- Real-time processing and predictions might be challenging due to the decentralized nature and the need for off-chain processing.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This work introduces a novel approach to diagnosing Alzheimer's Disease using a decentralized expert system. This system leverages blockchain technology and Federated Learning to enhance data privacy and manage large volumes of MRI data effectively. The key innovation lies in integrating these technologies to address the challenges of traditional diagnostic methods, which often suffer from delays and inaccuracies, especially in the early stages of the disease.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. This work presents a pioneering integration of blockchain technology and Federated Learning to enhance Alzheimer's Disease (AD) diagnostics, addressing privacy concerns and data management challenges.
2. The proposed decentralized expert system architecture, which includes anomaly detection for patient-submitted data, showcases a comprehensive approach to AD diagnostics, emphasizing AI-driven MRI analysis.

Weaknesses:
1. While the system shows promising results, the article does not provide extensive comparative data against traditional centralized systems or other decentralized approaches, which could validate its superiority more robustly. This work lacks of comparative performance data.
2. The complexity of the blockchain and Federated Learning components might pose usability challenges for less technically adept users, potentially affecting the system's adoption.
3. There are no more details of the algorithms this work used, maybe give out more meaningful algorithm design for the specific model you are using.

Limitations:
1. The accuracy of the AI model heavily depends on the quality and consistency of input data, which might vary significantly across different healthcare settings.
2. The use of advanced technologies such as blockchain might limit the accessibility of the system for users not familiar with such technology, potentially restricting its applicability.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors assume that applying blockchain platforms to combine datasets for Alzheimer’s Disease and then using federated learning for multi-centralized training can improve diagnostic performance. However, the manuscript lacks technical details and experimental evidence. All descriptions are conceptual, making the manuscript a proposal rather than a technical paper.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
1: poor

Strengths:
It is interesting to apply blockchain platforms to combine datasets for Alzheimer’s Disease and then using federated learning for multi-centralized training.

Weaknesses:
There are no technical details and no experiments. Details can be found in Questions and Limitations.

Limitations:
1.	No technical details and experiments.
2.	The literature review of Alzheimer’s Disease diagnosis is not complete, especially regarding AI-based approaches.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper presents a decentralized expert system designed to predict early-stage Alzheimer's Disease using AI-driven MRI analysis. The system leverages blockchain technology and Federated Learning to ensure data privacy and security while performing anomaly detection on patient-submitted data. The architecture includes a Web3 application for patients to upload biological information and MRI images securely. The decentralized approach aims to improve early detection and intervention for Alzheimer's Disease, providing a more comprehensive representation of AD patterns and enhancing model performance through data diversity.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper encapsulates a few novel ideas. They can be summarized as follows:

1. Handling the security and sensitivity of patient medical information is of paramount importance. The authors were motivated by a very relevant problem and presented an approach to blockchain technology with stated aim of providing robust data privacy and security. By building on decades on research on this topic, this approach has the potential to be extended in future with the general updates in this domain.
2. While there are some confusion around their use case (see weakness below), the authors leveraging Federated Learning and a decentralized system to mitigate the challenges associated with model training on centralized data repositories, such as data bottlenecks and privacy concerns
3. The system aims to provide early-stage prediction of Alzheimer's Disease, which is crucial for timely intervention and improved patient outcomes.

Weaknesses:
However given the commendable motivations there are several challenges with the current paper,

1. First and perhaps the most important aspect is that the paper fails to present the real-world challenges associated with the adoption of such decentralized approaches, especially as it pertains to patients engaging with blockchain wallets and data submission interfaces. Also, the primary use-case for the decentralized approach is not evident - is model training the prime use-case or is the main use case patients being able to generate inferences on their own medical records. Overall, the usage scenario around the setup needs to be better motivated and established
2. The paper also lacks formalism around the presentation. For example, if the primary contribution is the architecture around the decentralized AI approach, the design principles needs to be better justified and articulated. A system architecture diagrams needs to be established as well. Similarly, the ""proof"" around the decentralized approach is not a rigorous mathematical proof. Rather the logic is derived from a hypothesis that more diverse data should lead to a better model. This is a hypothesis at the best and needs to be experimentally validates
3. Finally, the paper is lacking in experimental validation. For example, the proof needs to be backed by real world experiments. Also, this is not the first paper to posit a federated learning approach to medical AI prediction. Some of the SOTA methods in this space needs to be compared against

Limitations:
Please see the weakness above

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces an innovative decentralized expert system designed for early prediction of Alzheimer's Disease (AD), leveraging blockchain technology and Federated Learning. Traditional diagnostic methods often result in delays and imprecision, particularly in early-stage AD detection, while centralized data repositories face challenges in managing vast volumes of MRI data and maintaining patient privacy. The proposed system addresses these issues by combining blockchain for secure, decentralized data management and Federated Learning for collaborative AI model training across multiple institutions. The system includes robust anomaly detection mechanisms to ensure data quality and integrity, enabling precise early-stage AD predictions. This comprehensive approach aims to revolutionize disease diagnostics by enhancing data privacy, security, and collaborative efforts in the medical community.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper presents a novel integration of blockchain technology and Federated Learning for early AD prediction, which is innovative in addressing data privacy, security, and collaborative AI model training.
- The proposed system is well-conceived, with a detailed architecture and implementation strategy. The inclusion of anomaly detection mechanisms to ensure data quality adds robustness to the system.
- The approach has significant potential to improve early-stage AD detection, which is crucial for timely intervention and better patient outcomes. The decentralized nature of the system promotes data privacy and security, addressing major concerns in medical data management.

Weaknesses:
- The integration of blockchain and Federated Learning introduces significant computational complexity and potential delays due to off-chain processing and communication overhead.
- The system's scalability is a concern as the volume of data and the number of users increase, necessitating ongoing optimization to ensure efficient performance.

Limitations:
The authors have addressed several limitations, including data quality and consistency, computational complexity, and model generalizability. However, further discussion may be needed on:
- Ensuring that the AI model is unbiased and fair across different demographic groups can be difficult, especially if the training data is not representative.
- Real-time processing and predictions might be challenging due to the decentralized nature and the need for off-chain processing.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper focuses on Zero-Shot Composed Image Retrieval (ZS-CIR), which requires retrieving an image matching a reference image while incorporating specified textual modifications. The authors argue that a key challenge in ZS-CIR is training models on limited intention-relevant datasets to understand human intention implicitly expressed in textual modifications for accurately retrieving target images. Therefore, they introduce an image-text dataset incorporated with pseudo-manipulation intentions to enhance the training of ZS-CIR models in understanding human manipulation intents, based on LLaVA. They also propose to use Q-former to compress the features generated by CLIP for retrieval. The experimental results show the improvements of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors propose a large-scale pretraining dataset for ZS-CIR.
2. The experimental results show the improvements of the proposed method.

Weaknesses:
1. The concept of ""intention"" discussed throughout the whole paper is unclear. Based on Figure 1, the authors haven't explained what's ""intention"" to the MLLM. Moreover, the pseudo-manipulation description is the same as the rewritten caption in semantics. I can't find any ""intention"" added into this pseudo-manipulation description. The key novelty of considering human ""intention"" is farfetched.
2. The proposed model lacks novelty. In the model architecture, the authors just add a Q-Former [1] after the CLIP encoder, which is prevalent in existing research based on CLIP-like models. And the authors even do not cite any relevant work.
3. Existing work on CLIP-based ZS-CIR generally compares the experimental results with different CLIP variants, and different methods may be superior with different CLIP variants. The authors only experimented with one CLIP variant, which is insufficient.
4. In Figure 4, the compared method also accurately captures the ""intention"" in the modification text, which cannot show the superiority of the proposed method in capturing ""intention"".

References:

[1] Li, J., Li, D., Savarese, S., & Hoi, S. (2023, July). Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning (pp. 19730-19742). PMLR.

Limitations:
The authors addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces De-MINDS, a novel framework for Zero-Shot Composed Image Retrieval (ZS-CIR) that aims to bridge the gap between pre-training and retrieval by incorporating intention-based pseudo-manipulation descriptions. The authors propose intent-CC3M, a dataset featuring these descriptions generated through chain-of-thought prompting by a Multi-modal Large Language Model (MLLM). They also introduce a manipulation intention understanding network that uses learnable queries to enhance the model's ability to understand user intentions from manipulation descriptions. The paper demonstrates significant performance improvements across four ZS-CIR tasks compared to state-of-the-art models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The introduction of intent-CC3M as a dataset for training mapping networks to align intention-relevant visual information is innovative and potentially impactful.
- The proposed De-MINDS framework shows significant performance improvements over state-of-the-art models across multiple ZS-CIR tasks.
- The approach addresses the challenge of understanding manipulation intentions in user descriptions, which is crucial for accurate image retrieval.
- The ablation studies provide insights into the contributions of different components of the proposed method.

Weaknesses:
Major Weaknesses:

1. Experimental Gaps:
   - The paper lacks experimental evidence to support the claim that caption redundancy leads to inaccurate retrieval, as mentioned in the introduction.
   - There's no evaluation of the method's performance with longer text encoders like LongCLIP, which could potentially address some of the stated limitations of CLIP.
   - The comparison with a baseline (other than CIRR and Fashion-IQ) using only f_theta (trained on Intent-CC3M) without De-MINDS (ablation model '4') is missing, which would provide a fairer comparison.

2. Methodological Concerns:
   - The justification for using CC3M as the base dataset for creating intent-CC3M is not clearly explained.
   - There's no exploration of De-MINDS' performance when prompt options are mismatched with their intended tasks or in scenarios where the task is not known in advance.

3. Incomplete Ablation Studies:
   - The ablation study for the T sampling ratios (50%, 30%, 20%) is missing, and there's no explanation why concatenation of them wasn't considered as an alternative.
   - The ablation study lacks an exploration of the impact of the number of learnable queries, despite its apparent significance.

Minor Weaknesses:

1. Presentation Issues:
   - The prompt types (a), (b), and (c) are not clearly explained in the context they are introduced, requiring readers to refer back to previous sections.
   - There are inconsistencies between the notation in the text and figures (e.g., X vs q in Figure 2).

2. Comparative Analysis:
   - The paper doesn't include evaluations on CIRCO and GeneCIS datasets, which were used in baseline studies.

3. Clarity:
   - More details are needed on certain aspects, such as the ""cos distill"" mentioned in ablation model '9'.

Limitations:
The authors acknowledge the computational intensity of generating pseudo-manipulation descriptions using MLLMs and the potential introduction of irrelevant details in these descriptions. However, they could further discuss the implications of these limitations on the practical applicability of their method in real-world scenarios.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an image-text dataset (intent-CC3M) for Zero-Shot Composed Image Retrieval (ZS-CIR) models to make better understanding of human manipulation intentions. Specifically, captions are re-written with LLaVA model to provide more details, and additional manipulation reasoning prompt is applied to make pseudo-manipulation description. With this dataset, the paper proposes De-MINDS framework (unDErstanding of Manipulation INtention from target Description before Searching), which utilizes pseudo-manipulation descriptions. The model training involves reasoning distillation and cross-modal alignment. The method shows state-of-the-art performance with ViT-L backbone comparisons.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper proposes to leverage LLaVA model to elaborate the image caption and further utilize LLaVA's reasoning capability to build a pseudo manipulation. The proposal is intuitive and clear, and the presentation of this paper is also clear. Extensive results including various ablations and qualitative results demonstrate the proposed method.

Weaknesses:
The proposed method of utilizing LLM, referred to as MLLM, is not entirely novel, as it has been previously addressed in works [1, 2] (please also refer to [1]). Furthermore, the evaluation of the proposed method is limited to the ViT-L backbone, which raises concerns about its effectiveness with other, more robust backbones (such as ViT-G).

[1] Jang, et al. Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval, CVPR2024
[2] Karthik, et al, Vision-by-Language for Training-Free Compositional Image Retrieval, ICLR2024

Limitations:
The paper handles possible limitations properly.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper focuses on Zero-Shot Composed Image Retrieval (ZS-CIR), which requires retrieving an image matching a reference image while incorporating specified textual modifications. The authors argue that a key challenge in ZS-CIR is training models on limited intention-relevant datasets to understand human intention implicitly expressed in textual modifications for accurately retrieving target images. Therefore, they introduce an image-text dataset incorporated with pseudo-manipulation intentions to enhance the training of ZS-CIR models in understanding human manipulation intents, based on LLaVA. They also propose to use Q-former to compress the features generated by CLIP for retrieval. The experimental results show the improvements of the proposed method.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors propose a large-scale pretraining dataset for ZS-CIR.
2. The experimental results show the improvements of the proposed method.

Weaknesses:
1. The concept of ""intention"" discussed throughout the whole paper is unclear. Based on Figure 1, the authors haven't explained what's ""intention"" to the MLLM. Moreover, the pseudo-manipulation description is the same as the rewritten caption in semantics. I can't find any ""intention"" added into this pseudo-manipulation description. The key novelty of considering human ""intention"" is farfetched.
2. The proposed model lacks novelty. In the model architecture, the authors just add a Q-Former [1] after the CLIP encoder, which is prevalent in existing research based on CLIP-like models. And the authors even do not cite any relevant work.
3. Existing work on CLIP-based ZS-CIR generally compares the experimental results with different CLIP variants, and different methods may be superior with different CLIP variants. The authors only experimented with one CLIP variant, which is insufficient.
4. In Figure 4, the compared method also accurately captures the ""intention"" in the modification text, which cannot show the superiority of the proposed method in capturing ""intention"".

References:

[1] Li, J., Li, D., Savarese, S., & Hoi, S. (2023, July). Blip-2: Bootstrapping language-image pre-training with frozen image encoders and large language models. In International conference on machine learning (pp. 19730-19742). PMLR.

Limitations:
The authors addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces De-MINDS, a novel framework for Zero-Shot Composed Image Retrieval (ZS-CIR) that aims to bridge the gap between pre-training and retrieval by incorporating intention-based pseudo-manipulation descriptions. The authors propose intent-CC3M, a dataset featuring these descriptions generated through chain-of-thought prompting by a Multi-modal Large Language Model (MLLM). They also introduce a manipulation intention understanding network that uses learnable queries to enhance the model's ability to understand user intentions from manipulation descriptions. The paper demonstrates significant performance improvements across four ZS-CIR tasks compared to state-of-the-art models.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The introduction of intent-CC3M as a dataset for training mapping networks to align intention-relevant visual information is innovative and potentially impactful.
- The proposed De-MINDS framework shows significant performance improvements over state-of-the-art models across multiple ZS-CIR tasks.
- The approach addresses the challenge of understanding manipulation intentions in user descriptions, which is crucial for accurate image retrieval.
- The ablation studies provide insights into the contributions of different components of the proposed method.

Weaknesses:
Major Weaknesses:

1. Experimental Gaps:
   - The paper lacks experimental evidence to support the claim that caption redundancy leads to inaccurate retrieval, as mentioned in the introduction.
   - There's no evaluation of the method's performance with longer text encoders like LongCLIP, which could potentially address some of the stated limitations of CLIP.
   - The comparison with a baseline (other than CIRR and Fashion-IQ) using only f_theta (trained on Intent-CC3M) without De-MINDS (ablation model '4') is missing, which would provide a fairer comparison.

2. Methodological Concerns:
   - The justification for using CC3M as the base dataset for creating intent-CC3M is not clearly explained.
   - There's no exploration of De-MINDS' performance when prompt options are mismatched with their intended tasks or in scenarios where the task is not known in advance.

3. Incomplete Ablation Studies:
   - The ablation study for the T sampling ratios (50%, 30%, 20%) is missing, and there's no explanation why concatenation of them wasn't considered as an alternative.
   - The ablation study lacks an exploration of the impact of the number of learnable queries, despite its apparent significance.

Minor Weaknesses:

1. Presentation Issues:
   - The prompt types (a), (b), and (c) are not clearly explained in the context they are introduced, requiring readers to refer back to previous sections.
   - There are inconsistencies between the notation in the text and figures (e.g., X vs q in Figure 2).

2. Comparative Analysis:
   - The paper doesn't include evaluations on CIRCO and GeneCIS datasets, which were used in baseline studies.

3. Clarity:
   - More details are needed on certain aspects, such as the ""cos distill"" mentioned in ablation model '9'.

Limitations:
The authors acknowledge the computational intensity of generating pseudo-manipulation descriptions using MLLMs and the potential introduction of irrelevant details in these descriptions. However, they could further discuss the implications of these limitations on the practical applicability of their method in real-world scenarios.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces an image-text dataset (intent-CC3M) for Zero-Shot Composed Image Retrieval (ZS-CIR) models to make better understanding of human manipulation intentions. Specifically, captions are re-written with LLaVA model to provide more details, and additional manipulation reasoning prompt is applied to make pseudo-manipulation description. With this dataset, the paper proposes De-MINDS framework (unDErstanding of Manipulation INtention from target Description before Searching), which utilizes pseudo-manipulation descriptions. The model training involves reasoning distillation and cross-modal alignment. The method shows state-of-the-art performance with ViT-L backbone comparisons.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper proposes to leverage LLaVA model to elaborate the image caption and further utilize LLaVA's reasoning capability to build a pseudo manipulation. The proposal is intuitive and clear, and the presentation of this paper is also clear. Extensive results including various ablations and qualitative results demonstrate the proposed method.

Weaknesses:
The proposed method of utilizing LLM, referred to as MLLM, is not entirely novel, as it has been previously addressed in works [1, 2] (please also refer to [1]). Furthermore, the evaluation of the proposed method is limited to the ViT-L backbone, which raises concerns about its effectiveness with other, more robust backbones (such as ViT-G).

[1] Jang, et al. Visual Delta Generator with Large Multi-modal Models for Semi-supervised Composed Image Retrieval, CVPR2024
[2] Karthik, et al, Vision-by-Language for Training-Free Compositional Image Retrieval, ICLR2024

Limitations:
The paper handles possible limitations properly.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper studies the effect of rescaling symmetry in SGD and shows SGD tends to favor solutions with balanced gradient noises. The authors then derive an exact solution of the stationary distribution of a toy model trained by SGD.  The derived solution shed lights on problems observed in deep learning such as fluctuation inversion and edge of stability.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper contributes to the understanding of SGD properties. The noise balance theorem is novel and important. The analytical solution as well as the interpretation is interesting and insightful.

Weaknesses:
The results of the paper are interesting and important, the writing needs refinement to improve clarity and precision. The conditions under which the results hold is sometimes omitted, leading to confusion. The language should also be made more precise.

Minor points:
1.	The first paragraph in related works appears to overstate the novelty of the results. Specifically, our result is the first to derive an exact solution to the stationary distribution of SGD without any approximation. (Line 55-56) This is a strong claim, but it seems inaccurate. There are previous results showing exact solution of stationary distribution of SGD (e. g. Liu Ziyin 2021). Corollary I.1 in arXiv:2306.04251 (2023) also states the stationary distribution on a deep learning setup similar to the D=1 model discussed in this paper. Also, the solution given in the paper is for a specific model. These should be made clear.
2.	It seems that eq. 15 takes D=1, which has not been stated and thus is confusing.
3.	It is unclear why the left figure of Fig. 5 has only two theory lines instead of three.

Major points:
1.	The related works on symmetry and SGD dynamics are insufficient. There are a few related works that are missing, e. g. arXiv:2309.16932 (2023).
2.	The paper has not discussed convergence to the stationary distribution. The authors seem to assume convergence to stationary distribution and use interchangeably the SGD properties and the stationary solution properties (e. g. line 97-98). However, the properties of SGD can be very different from the properties of stationary solutions unless convergence to the stationary solutions is guaranteed. The authors should clarify this.
3.	The authors fail to discuss uniqueness of the stationary solutions. For example, it is unclear to me why eq (3) is a necessary and efficient condition for stationarity. Eq (3) is a critical result in the paper, and it would be better to make it a theorem or corollary. However, since eq (2) cannot be interpreted as a deterministic ODE. The unique condition for a stationary distribution should be justified, especially considering that C1 and C2 are not constant but depend on u and w.
4.	The equivalence of SGD bias and weight decay is not rigorous. (line 155-158) The C0 term is not constant but depends on u and w, while the weight decay rate is constant.

Limitations:
The authors have listed limitations at the end. The major limitations are the simplicity of the model and lack of experiments on deep neural networks.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
For ReLU networks trained by gradient flows, it is classical that a type of Minkowski inner product between the coefficients of consecutive layers is preserved. The authors demonstrate a monotonicity of the same quantity for stochastic gradient descent in continuous time. They use this to study the invariant distribution of parameters trained by (continuous time) SGD.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The topic is well-chosen and the results - if correct - are very interesting.

Weaknesses:
* In its current form, I find the article a bit unpolished and the results not easy to access. Many questions remained unanswered when I tried reading the article (see questions).

* Important quantities are defined throughout the plain text. I understand that reading as a reviewer under time pressure is different from normal reading, but for instance in Theorem 3.1, I would have hoped for a more self-contained statement on relations and properties $L, C, \ell$ and the distribution of $x$ have to satisfy. As far as I can tell, the statement is fairly general and not specific to machine learning.

* I have serious doubts about Theorem 3.1. It is derived in Appendix A from Itô's Lemma without the diffusion term. This is valid *in expectation over $\theta$*, but not pointwise in $\theta$. Pointwise in $\theta$, there should be white noise in the 'time derivatives', i.e. the ODE identity should be written as an SDE. In the proof, equations (27) and (28) appear to be wrong.

* The authors do not pay any attention to whether solutions to the evolution equations exist (or are unique). Problems with regularity can sometimes be alleviated if the distribution in $x$ is sufficiently regular, but I would appreciate a short discussion.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tries to analyze the specific features that carry the noise of SGD (through a continuous model). The authors show that there is a certain 'law of balance' across the layers when some invariance is assumed. Going further, they derive a toy model to push their study, showing that there is an analytic stationary solution to it. They finally propose a phenomenology related to the role of the noise of SGD when analyzing this precise stationary distribution.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea that a conservation law for the gradient flow implies an asymptotic balancedness condition for the stochastic flow is a good and striking idea.

The one-dimensional examples that are given in the text are very pleasant to follow and they are good exercices to display the ability of the stochastic flow to diverge from the gradient flow.

The example given in Eq.(13) is thoroughly analyzed.

Weaknesses:
The paper present the following weaknesses:

- The law of balance is an interesting phenomenon, yet considering it with a closer look, it seems that not much can be said generally and that one has to understand it case by case. In one dimension, sure, it is possible to conclude that balancedness will occur at exponential speed, yet in dimension more than $2$, it seems impossible to predict it surely.

- I have to say that I was a bit bothered by the general overselling of the paper : 
     - As said before the law of balance is truly valid asymptotically in one-dimension
     - The stationary distribution that the authors claim to be the first to derive is for a very specific model, which is not standard and does not resemble a diagonal network! 
     - The fact that the stationary distribution can be computed is also very inherent to $1d$ calculation and is simply a recognition of a Pearson diffusion that already made in way in ML (at least in https://arxiv.org/pdf/2402.01382 and https://arxiv.org/pdf/2407.02322).

Minor typos/flaws:

- l.41: Fokker Planck is not inherently high-dimensional
- l.44: Go to the line for new paragraph 
- l.165: The law of balance is not strictly applicable here since $\ell$ is not scale invariant because of the regularization.
- Section **4.1 Depth - 0**: I think that $\Delta > 0$ is not currently the ""most practical example"" since it corresponds to a underparametrized model.

Limitations:
As said before, all conclusion are drawn for models that live intrinsically in one dimension.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper studies the effect of rescaling symmetry in SGD and shows SGD tends to favor solutions with balanced gradient noises. The authors then derive an exact solution of the stationary distribution of a toy model trained by SGD.  The derived solution shed lights on problems observed in deep learning such as fluctuation inversion and edge of stability.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper contributes to the understanding of SGD properties. The noise balance theorem is novel and important. The analytical solution as well as the interpretation is interesting and insightful.

Weaknesses:
The results of the paper are interesting and important, the writing needs refinement to improve clarity and precision. The conditions under which the results hold is sometimes omitted, leading to confusion. The language should also be made more precise.

Minor points:
1.	The first paragraph in related works appears to overstate the novelty of the results. Specifically, our result is the first to derive an exact solution to the stationary distribution of SGD without any approximation. (Line 55-56) This is a strong claim, but it seems inaccurate. There are previous results showing exact solution of stationary distribution of SGD (e. g. Liu Ziyin 2021). Corollary I.1 in arXiv:2306.04251 (2023) also states the stationary distribution on a deep learning setup similar to the D=1 model discussed in this paper. Also, the solution given in the paper is for a specific model. These should be made clear.
2.	It seems that eq. 15 takes D=1, which has not been stated and thus is confusing.
3.	It is unclear why the left figure of Fig. 5 has only two theory lines instead of three.

Major points:
1.	The related works on symmetry and SGD dynamics are insufficient. There are a few related works that are missing, e. g. arXiv:2309.16932 (2023).
2.	The paper has not discussed convergence to the stationary distribution. The authors seem to assume convergence to stationary distribution and use interchangeably the SGD properties and the stationary solution properties (e. g. line 97-98). However, the properties of SGD can be very different from the properties of stationary solutions unless convergence to the stationary solutions is guaranteed. The authors should clarify this.
3.	The authors fail to discuss uniqueness of the stationary solutions. For example, it is unclear to me why eq (3) is a necessary and efficient condition for stationarity. Eq (3) is a critical result in the paper, and it would be better to make it a theorem or corollary. However, since eq (2) cannot be interpreted as a deterministic ODE. The unique condition for a stationary distribution should be justified, especially considering that C1 and C2 are not constant but depend on u and w.
4.	The equivalence of SGD bias and weight decay is not rigorous. (line 155-158) The C0 term is not constant but depends on u and w, while the weight decay rate is constant.

Limitations:
The authors have listed limitations at the end. The major limitations are the simplicity of the model and lack of experiments on deep neural networks.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
For ReLU networks trained by gradient flows, it is classical that a type of Minkowski inner product between the coefficients of consecutive layers is preserved. The authors demonstrate a monotonicity of the same quantity for stochastic gradient descent in continuous time. They use this to study the invariant distribution of parameters trained by (continuous time) SGD.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The topic is well-chosen and the results - if correct - are very interesting.

Weaknesses:
* In its current form, I find the article a bit unpolished and the results not easy to access. Many questions remained unanswered when I tried reading the article (see questions).

* Important quantities are defined throughout the plain text. I understand that reading as a reviewer under time pressure is different from normal reading, but for instance in Theorem 3.1, I would have hoped for a more self-contained statement on relations and properties $L, C, \ell$ and the distribution of $x$ have to satisfy. As far as I can tell, the statement is fairly general and not specific to machine learning.

* I have serious doubts about Theorem 3.1. It is derived in Appendix A from Itô's Lemma without the diffusion term. This is valid *in expectation over $\theta$*, but not pointwise in $\theta$. Pointwise in $\theta$, there should be white noise in the 'time derivatives', i.e. the ODE identity should be written as an SDE. In the proof, equations (27) and (28) appear to be wrong.

* The authors do not pay any attention to whether solutions to the evolution equations exist (or are unique). Problems with regularity can sometimes be alleviated if the distribution in $x$ is sufficiently regular, but I would appreciate a short discussion.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tries to analyze the specific features that carry the noise of SGD (through a continuous model). The authors show that there is a certain 'law of balance' across the layers when some invariance is assumed. Going further, they derive a toy model to push their study, showing that there is an analytic stationary solution to it. They finally propose a phenomenology related to the role of the noise of SGD when analyzing this precise stationary distribution.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The idea that a conservation law for the gradient flow implies an asymptotic balancedness condition for the stochastic flow is a good and striking idea.

The one-dimensional examples that are given in the text are very pleasant to follow and they are good exercices to display the ability of the stochastic flow to diverge from the gradient flow.

The example given in Eq.(13) is thoroughly analyzed.

Weaknesses:
The paper present the following weaknesses:

- The law of balance is an interesting phenomenon, yet considering it with a closer look, it seems that not much can be said generally and that one has to understand it case by case. In one dimension, sure, it is possible to conclude that balancedness will occur at exponential speed, yet in dimension more than $2$, it seems impossible to predict it surely.

- I have to say that I was a bit bothered by the general overselling of the paper : 
     - As said before the law of balance is truly valid asymptotically in one-dimension
     - The stationary distribution that the authors claim to be the first to derive is for a very specific model, which is not standard and does not resemble a diagonal network! 
     - The fact that the stationary distribution can be computed is also very inherent to $1d$ calculation and is simply a recognition of a Pearson diffusion that already made in way in ML (at least in https://arxiv.org/pdf/2402.01382 and https://arxiv.org/pdf/2407.02322).

Minor typos/flaws:

- l.41: Fokker Planck is not inherently high-dimensional
- l.44: Go to the line for new paragraph 
- l.165: The law of balance is not strictly applicable here since $\ell$ is not scale invariant because of the regularization.
- Section **4.1 Depth - 0**: I think that $\Delta > 0$ is not currently the ""most practical example"" since it corresponds to a underparametrized model.

Limitations:
As said before, all conclusion are drawn for models that live intrinsically in one dimension.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposes a novel variant of the generative adversarial network that uses the Kolmogorov-Smirnov distance to align the generated distribution with the target distribution. This distance is calculated using the quantile function, which acts as the critic in the adversarial training process. Experiments are conducted on synthetic distributions and small image datasets to show that the proposed KSGAN performs on par with the existing adversarial methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The paper is well-presented and easy to follow.

2. The claims and methodology designs are well supported by theoretical analysis.

Weaknesses:
1. It is still unclear why we need another adversarial design based on KS distance. The vanilla GAN paper shows that the designed bi-level optimization process can already be seen as optimizing the distance between the generated and the target distribution. Then, what are the specific advantages KS distance can bring within the adversarial framework?

2. The experiments are merely conducted on synthetic datasets and small image datasets. It is unclear whether the proposed method can be adapted to larger-scale datasets or incorporated into more advanced frameworks like StyleGAN. Moreover, the compared baselines are limited to early works, and the experimental results of KSGAN are worse than those of WGAN-GP. Thus, I do not see many advantages of KSGAN in terms of the presented experiments.

Limitations:
Please see the discussions above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper first generalizes the Kolmogorov–Smirnov (KS) distance from one-dimensional spaces to multidimensional spaces and proposes the Kolmogorov-Smirnov GAN, which formulates the generative model by minimizing the Kolmogorov-Smirnov (KS) distance. Theoretical results are also given in this paper and the experiments also show the superiority of stability during training, resistance to mode dropping and collapse, and tolerance to variations in hyperparameter settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
I like the idea of generalizing the one-dimensional KS distance to multi-dimensional, which I had thought about before but failed to achieve. The motivation and writing are good, and the experiments of KSGAN seem to have achieved good results.

Weaknesses:
1. I'm skeptical about some parts of the theory. (See questions for details)
2. It seems that the advantages of KS distance over JS divergence and Wasserstein Distance are not explained.
3. The idea of reformulating a distance between distributions to a GAN model seems to be old and is now unlikely to attract readers' interest.
4. The experimental setup is relatively simple, only comparing with vanilla GAN and WGAN on Synthetic, MNIST, and CIFAR10 datasets
5. According to the experimental results, the advantages of KSGAN lie in the stability of training and resistance to mode dropping. A significant issue is that with current network architectures and training techniques, these two problems are rarely encountered.

Limitations:
The authors have discussed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce a generalized KS distance applicable to high dimensional spaces, formulate the corresponding dual problem, and use adversarial training to construct a generative model that minimizes the GKS between data and generated distributions.

The paper is well presented and appears technically correct through what I've seen, though I didn't check the proofs in details.

The main problem is that there's no clear motivation for why using the GKS is beneficial at all (either theoretically or practically). As such, despite being novel, I don't see any clear impact from the paper. Furthermore, the final algorithm is quite complicated and the results are fairly underwhelming, so at the end of the day the cons dramatically outweigh the pros of the newly introduced algorithm.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The paper is well written. It reads easily and it is clear what they want to do. The contribution of a generalized KS distance to multidimensional spaces and the algorithm to approximate it are to the best of my knowledge, novel.

Weaknesses:
The main problem I have with the paper is that I don't see any clear advantages of using the KS distance (gan) as a replacement of other distances like the Wasserstein one, or their GAN equivalent. The only mention of this, which should arguably be the most important thing in a paper introducing a new GAN, is in lines 224-228 of page 7. The authors claim there that they don't need to maximize the supremum in (5) which is false depending on how to interpret it, if you just take any set C in (5) you end up with |P_F(C) - P_G(C)| which is just measuring one moment for a given characteristic function, and far from being anything meaningful (and the same holding true for most IPMs). The results are also not particularly interesting to merit the claim that there's anything particularly different or benefitial on using this new formulation.

Limitations:
No clear motivation or benefit from using their algorithm.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposed a new kind of GAN training method called KS-GAN. The method is based on minimizing the Kolmogorov–Smirnov distance. The KSGAN updates the generator by minimizing an upper bound of the generalized KS distance. It updates the discriminator (or the critic network) by using energy-based model training with regularization terms. The KSGAN is a novel attempt to explore new approaches to train generative adversarial networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* (1) The paper studied using generalized KS distance to train GAN generators is a novel attempt to extend the GAN literature. 

* (2) Some theoretical arguments about implementing the empirical KS distance using neural networks is novel yet constructive.

Weaknesses:
My main concern about the paper is its weak evaluation baselines and questionable practical usage:

* (1) Though the idea of using new objectives for training GAN generators is attractive, the practical usage of KSGAN seems questionable, especially for high-dimensional data. For instance, in the CIFAR10 generation experiment, the author compares KSGAN with WGAN-GP and Vanilla GAN, which have shown weak empirical performances. However, it is well-known that, for CIFAR10 data, the StyleGAN2-ADA[1] model is a strong baseline GAN model. I think it would strengthen the paper a lot if the authors could somehow show strong performances of KSGAN using StyleGAN2's architectures and implementation techniques. However, I do admit that such a requirement may be too tough for new methods.

* (2) The KSGAN's critic function is constructed with EBMs. However, even with regularization terms, energy-based models are well-known for poor scaling ability to high-dimensional data. This may prevent the practical usage of KSGAN for real-world high-dimensional data.

Limitations:
The author has addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposes a novel variant of the generative adversarial network that uses the Kolmogorov-Smirnov distance to align the generated distribution with the target distribution. This distance is calculated using the quantile function, which acts as the critic in the adversarial training process. Experiments are conducted on synthetic distributions and small image datasets to show that the proposed KSGAN performs on par with the existing adversarial methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
1. The paper is well-presented and easy to follow.

2. The claims and methodology designs are well supported by theoretical analysis.

Weaknesses:
1. It is still unclear why we need another adversarial design based on KS distance. The vanilla GAN paper shows that the designed bi-level optimization process can already be seen as optimizing the distance between the generated and the target distribution. Then, what are the specific advantages KS distance can bring within the adversarial framework?

2. The experiments are merely conducted on synthetic datasets and small image datasets. It is unclear whether the proposed method can be adapted to larger-scale datasets or incorporated into more advanced frameworks like StyleGAN. Moreover, the compared baselines are limited to early works, and the experimental results of KSGAN are worse than those of WGAN-GP. Thus, I do not see many advantages of KSGAN in terms of the presented experiments.

Limitations:
Please see the discussions above.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper first generalizes the Kolmogorov–Smirnov (KS) distance from one-dimensional spaces to multidimensional spaces and proposes the Kolmogorov-Smirnov GAN, which formulates the generative model by minimizing the Kolmogorov-Smirnov (KS) distance. Theoretical results are also given in this paper and the experiments also show the superiority of stability during training, resistance to mode dropping and collapse, and tolerance to variations in hyperparameter settings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
I like the idea of generalizing the one-dimensional KS distance to multi-dimensional, which I had thought about before but failed to achieve. The motivation and writing are good, and the experiments of KSGAN seem to have achieved good results.

Weaknesses:
1. I'm skeptical about some parts of the theory. (See questions for details)
2. It seems that the advantages of KS distance over JS divergence and Wasserstein Distance are not explained.
3. The idea of reformulating a distance between distributions to a GAN model seems to be old and is now unlikely to attract readers' interest.
4. The experimental setup is relatively simple, only comparing with vanilla GAN and WGAN on Synthetic, MNIST, and CIFAR10 datasets
5. According to the experimental results, the advantages of KSGAN lie in the stability of training and resistance to mode dropping. A significant issue is that with current network architectures and training techniques, these two problems are rarely encountered.

Limitations:
The authors have discussed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce a generalized KS distance applicable to high dimensional spaces, formulate the corresponding dual problem, and use adversarial training to construct a generative model that minimizes the GKS between data and generated distributions.

The paper is well presented and appears technically correct through what I've seen, though I didn't check the proofs in details.

The main problem is that there's no clear motivation for why using the GKS is beneficial at all (either theoretically or practically). As such, despite being novel, I don't see any clear impact from the paper. Furthermore, the final algorithm is quite complicated and the results are fairly underwhelming, so at the end of the day the cons dramatically outweigh the pros of the newly introduced algorithm.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The paper is well written. It reads easily and it is clear what they want to do. The contribution of a generalized KS distance to multidimensional spaces and the algorithm to approximate it are to the best of my knowledge, novel.

Weaknesses:
The main problem I have with the paper is that I don't see any clear advantages of using the KS distance (gan) as a replacement of other distances like the Wasserstein one, or their GAN equivalent. The only mention of this, which should arguably be the most important thing in a paper introducing a new GAN, is in lines 224-228 of page 7. The authors claim there that they don't need to maximize the supremum in (5) which is false depending on how to interpret it, if you just take any set C in (5) you end up with |P_F(C) - P_G(C)| which is just measuring one moment for a given characteristic function, and far from being anything meaningful (and the same holding true for most IPMs). The results are also not particularly interesting to merit the claim that there's anything particularly different or benefitial on using this new formulation.

Limitations:
No clear motivation or benefit from using their algorithm.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposed a new kind of GAN training method called KS-GAN. The method is based on minimizing the Kolmogorov–Smirnov distance. The KSGAN updates the generator by minimizing an upper bound of the generalized KS distance. It updates the discriminator (or the critic network) by using energy-based model training with regularization terms. The KSGAN is a novel attempt to explore new approaches to train generative adversarial networks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* (1) The paper studied using generalized KS distance to train GAN generators is a novel attempt to extend the GAN literature. 

* (2) Some theoretical arguments about implementing the empirical KS distance using neural networks is novel yet constructive.

Weaknesses:
My main concern about the paper is its weak evaluation baselines and questionable practical usage:

* (1) Though the idea of using new objectives for training GAN generators is attractive, the practical usage of KSGAN seems questionable, especially for high-dimensional data. For instance, in the CIFAR10 generation experiment, the author compares KSGAN with WGAN-GP and Vanilla GAN, which have shown weak empirical performances. However, it is well-known that, for CIFAR10 data, the StyleGAN2-ADA[1] model is a strong baseline GAN model. I think it would strengthen the paper a lot if the authors could somehow show strong performances of KSGAN using StyleGAN2's architectures and implementation techniques. However, I do admit that such a requirement may be too tough for new methods.

* (2) The KSGAN's critic function is constructed with EBMs. However, even with regularization terms, energy-based models are well-known for poor scaling ability to high-dimensional data. This may prevent the practical usage of KSGAN for real-world high-dimensional data.

Limitations:
The author has addressed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces a learning-based method to address the Capacitated Arc Routing Problem (CARP). It involves breaking undirected edges into directed arcs and utilizing a graph attention network to build a Direction-aware Attention Model. In the training process, supervised learning is used to create the initial policy, followed by reinforcement learning based on policy gradients using Proximal Policy Optimization (PPO) to refine strategies. Lastly, dynamic programming is applied to optimize depot placements for path enhancement. Experimental outcomes show notable benefits of this algorithm in evaluation criteria.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
In general, the paper exhibits a well-organized structure with detailed experimental outcomes showcase through graphs and tables, facilitating readers in comprehending and visualizing the results effortlessly. The dataset employed comprises real-world scenarios, thereby boosting its practical relevance.

Weaknesses:
Converting the graph G from arcs to nodes represents a common approach in many heuristics for addressing CARP. This process adds complexity to the problem and increases its scale. The proposed method appears to lack enough novelty, with most components bearing resemblance to neural models designed for CVRP.

Limitations:
It appears that the paper focuses on an unlimited number of vehicles. How would the approach adapt to a specific set of vehicles?

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors skillfully address challenges posed by non-Euclidean graphs, traversal direction, and capacity constraints with their novel NN-based solver in solving capacitated arc routing problem. The introduction of the direction-aware attention model and a supervised reinforcement learning scheme is particularly commendable. These innovations significantly narrow the gap with advanced metaheuristics, achieving superior efficiency and competitive decision quality.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The manuscript employs numerous innovative methods to solve the capacitated arc routing problem, achieving impressive results. 
2. It also shows promising performance in generalizing to larger problem instances.
3. The combination of supervised and reinforcement learning is quite interesting. Using supervised learning for pre-training followed by fine-tuning with reinforcement learning is a noteworthy approach.
4. The qualitative comparisons in real street scenes presented in Figure 4 are particularly interesting.

Weaknesses:
1. It's better to redraw the first part of Figure 1 to enhance its aesthetic quality.
2. The baseline is not very recent. After S2V-DQN and S2V-DQN, there are still some excellent works that can be used to address the CARP problem.
3. Some writing errors have been identified, such as in line 2 of Algorithm 1. Please review the entire manuscript to check.
4. The completeness of the manuscript still requires supplementation and refinement.

Limitations:
The approach of decomposing undirected edges into directed ones introduces additional decision elements, which complicates the problem. It's better that the authors can find a more efficient graph processing method.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new learning-based constructive heuristic for capacitated arc routing problems. In contrast to node routing problems such as the TSP and VRP, arc routing problems received comparably little attentition. To address the specific
challenges in the capacitated arc routing problems, the authors propose a Neural Network-based approach that uses a graph attention
model considering arc directionality, a reinforcement learning approach with supervised pre-training and PPO-based fine-tuning. In
order to improve solutions obtained by an RL-based construction approach, they propose a beam search approach for path optimization which, after turning the set of routes into a giant tour, splits the tour into routes by adding returns to the depot. A set of experiments
shows that the proposed approach consistently yields better results than traditional hand-crafted constructive heuristics, and that their solutions almost match the quality of a time-consuming memetic algorithm that is only capable of solving small instance in a reaonable amount of time.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors propose one of the first learning-based approaches for the capacitated arc routing problem (CARP). Their approach, in particular their graph embedding, explicitly addresses one of the challenges of learning-based construction algorithms for this problem by explicitly replacing the undirected edges by directed arcs. This idea is original and turns out to be helpful to create a well-performing heuristic. 

The online performance of the approach surpasses hand-crafted constructive heuristics both in terms of runtime and efficiency. While for small instances, other metaheuristic approaches are better, it can be assumed that for large-scale instances, the proposed approach surpasses the state-of-the art of heuristic approaches. This is a significant result, since for node routing problems such as the CVRP, researcher have been struggling for years to design learning-based heuristics that achieve a performace that is comparable to hand-crafted heuristics. It should be mentioned, though, that in general, arc routing problems receive much less attention than node routing problems in the literature.

The paper comprises several insightful and, as far as I can tell, reasonably designed experiments, in particular showing the generalization capability to larger instances. 

The paper provides both code and instances.

Weaknesses:
The presentation of the CARP routing problem, the solutions approaches and related work lacks clarity in many places.
As an example, in the abstract, we find that the CARP consists in finding ""the minimum-cost tour""hat covers all required edges on a graph, while within capacity constraints"". This is a a bit misleading description since we look for a set of routes instead of a tour.

The paper distinguishes ""heuristics"" and ""metaheuristics"", while clearly metaheuristics are a type of heuristics. Actually, what the authors appear to have in mind is ""constructive heuristics"" which sequentially construct a solution by adding edges to form routes. I suggest to formulate more precisely here.

Similarly, it would enhance the understanding of the paper to introduce the notion of ""route-first, cluster second"" and the related
notion of a ""giant tour"" which is commonplace in routing applications, to characterize respective existing work. It would even facilitate the
presented path optimization which actually turns the presented approach into a route-first, cluster second approach. 


The computational results are convincing, but the discussion should emphasize that a fair comparison can only be made between their
approach wihout path optimization and the other constructive heuristics. It would indeed be interesting to see how the far the path
optimization is able to improve the results of the other constructive heuristics.

The claims ""NN-based approaches tend to lag behind advanced metaheuristics"" (abstract) and ""NN-based methods usually lags far
behind the traditional ones in solving CARP"", ""they still lag significantly behind traditional methods"" are not valid. Actually, (Rahmamoorty et. al 2024) (reference 20) report that on average, they improve upon the memetic algorithm by 11% on average.

When it comes to the evaluation of the path scanning approaches in the experiments, it is unclear how they are parameterized. From reading the paper (Aarakaki 2019) one sees that the parameter alpha and the number of iteration have a considerable impact both on solution time and solution quality, and (albeit on different instances), the average gaps for the path scanning approaches to the optimal (and to the memetic algrithm) reported in (Aarakaki 2019) are smaller than those found in the submission.

The description of the path improvement is not very clear; in particular the definition of the state used in the Dynamic Programming
algorithm. Is it a path? Is it the length of a path? Also, the statement ""f(*) denotes a state featuring dynamic programming"" is hard
to decipher.

Training time is not discussed at all.

Limitations:
Limitations are mostly addressed in a reasonable way. I suggest to add the following aspects:

I think that for small instances, the approach by (Rahmamoorty et. al 2024) may surpass the results reported here, which should be mentioned. 

Also, you should at least briefly mention the training time, since this makes it easier to assess the trade-off between offline effort and online performace.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a novel neural network-based CARP solver that uses a direction-aware attention model to incorporate directionality into the embedding process. It then applies supervised reinforcement learning for subsequent fine-tuning.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. A learning-based CARP solver is proposed.  
2. The performance of the proposed solver on large-scale data is discussed.

Weaknesses:
1. The comparison algorithms were published five years ago, and there is no discussion of existing methods aimed at big data.  
2. The experiments only tested the self-constructed dataset and did not evaluate on public datasets.

Limitations:
The amount of data required for algorithm training needs to be discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces a learning-based method to address the Capacitated Arc Routing Problem (CARP). It involves breaking undirected edges into directed arcs and utilizing a graph attention network to build a Direction-aware Attention Model. In the training process, supervised learning is used to create the initial policy, followed by reinforcement learning based on policy gradients using Proximal Policy Optimization (PPO) to refine strategies. Lastly, dynamic programming is applied to optimize depot placements for path enhancement. Experimental outcomes show notable benefits of this algorithm in evaluation criteria.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
In general, the paper exhibits a well-organized structure with detailed experimental outcomes showcase through graphs and tables, facilitating readers in comprehending and visualizing the results effortlessly. The dataset employed comprises real-world scenarios, thereby boosting its practical relevance.

Weaknesses:
Converting the graph G from arcs to nodes represents a common approach in many heuristics for addressing CARP. This process adds complexity to the problem and increases its scale. The proposed method appears to lack enough novelty, with most components bearing resemblance to neural models designed for CVRP.

Limitations:
It appears that the paper focuses on an unlimited number of vehicles. How would the approach adapt to a specific set of vehicles?

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors skillfully address challenges posed by non-Euclidean graphs, traversal direction, and capacity constraints with their novel NN-based solver in solving capacitated arc routing problem. The introduction of the direction-aware attention model and a supervised reinforcement learning scheme is particularly commendable. These innovations significantly narrow the gap with advanced metaheuristics, achieving superior efficiency and competitive decision quality.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The manuscript employs numerous innovative methods to solve the capacitated arc routing problem, achieving impressive results. 
2. It also shows promising performance in generalizing to larger problem instances.
3. The combination of supervised and reinforcement learning is quite interesting. Using supervised learning for pre-training followed by fine-tuning with reinforcement learning is a noteworthy approach.
4. The qualitative comparisons in real street scenes presented in Figure 4 are particularly interesting.

Weaknesses:
1. It's better to redraw the first part of Figure 1 to enhance its aesthetic quality.
2. The baseline is not very recent. After S2V-DQN and S2V-DQN, there are still some excellent works that can be used to address the CARP problem.
3. Some writing errors have been identified, such as in line 2 of Algorithm 1. Please review the entire manuscript to check.
4. The completeness of the manuscript still requires supplementation and refinement.

Limitations:
The approach of decomposing undirected edges into directed ones introduces additional decision elements, which complicates the problem. It's better that the authors can find a more efficient graph processing method.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new learning-based constructive heuristic for capacitated arc routing problems. In contrast to node routing problems such as the TSP and VRP, arc routing problems received comparably little attentition. To address the specific
challenges in the capacitated arc routing problems, the authors propose a Neural Network-based approach that uses a graph attention
model considering arc directionality, a reinforcement learning approach with supervised pre-training and PPO-based fine-tuning. In
order to improve solutions obtained by an RL-based construction approach, they propose a beam search approach for path optimization which, after turning the set of routes into a giant tour, splits the tour into routes by adding returns to the depot. A set of experiments
shows that the proposed approach consistently yields better results than traditional hand-crafted constructive heuristics, and that their solutions almost match the quality of a time-consuming memetic algorithm that is only capable of solving small instance in a reaonable amount of time.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
The authors propose one of the first learning-based approaches for the capacitated arc routing problem (CARP). Their approach, in particular their graph embedding, explicitly addresses one of the challenges of learning-based construction algorithms for this problem by explicitly replacing the undirected edges by directed arcs. This idea is original and turns out to be helpful to create a well-performing heuristic. 

The online performance of the approach surpasses hand-crafted constructive heuristics both in terms of runtime and efficiency. While for small instances, other metaheuristic approaches are better, it can be assumed that for large-scale instances, the proposed approach surpasses the state-of-the art of heuristic approaches. This is a significant result, since for node routing problems such as the CVRP, researcher have been struggling for years to design learning-based heuristics that achieve a performace that is comparable to hand-crafted heuristics. It should be mentioned, though, that in general, arc routing problems receive much less attention than node routing problems in the literature.

The paper comprises several insightful and, as far as I can tell, reasonably designed experiments, in particular showing the generalization capability to larger instances. 

The paper provides both code and instances.

Weaknesses:
The presentation of the CARP routing problem, the solutions approaches and related work lacks clarity in many places.
As an example, in the abstract, we find that the CARP consists in finding ""the minimum-cost tour""hat covers all required edges on a graph, while within capacity constraints"". This is a a bit misleading description since we look for a set of routes instead of a tour.

The paper distinguishes ""heuristics"" and ""metaheuristics"", while clearly metaheuristics are a type of heuristics. Actually, what the authors appear to have in mind is ""constructive heuristics"" which sequentially construct a solution by adding edges to form routes. I suggest to formulate more precisely here.

Similarly, it would enhance the understanding of the paper to introduce the notion of ""route-first, cluster second"" and the related
notion of a ""giant tour"" which is commonplace in routing applications, to characterize respective existing work. It would even facilitate the
presented path optimization which actually turns the presented approach into a route-first, cluster second approach. 


The computational results are convincing, but the discussion should emphasize that a fair comparison can only be made between their
approach wihout path optimization and the other constructive heuristics. It would indeed be interesting to see how the far the path
optimization is able to improve the results of the other constructive heuristics.

The claims ""NN-based approaches tend to lag behind advanced metaheuristics"" (abstract) and ""NN-based methods usually lags far
behind the traditional ones in solving CARP"", ""they still lag significantly behind traditional methods"" are not valid. Actually, (Rahmamoorty et. al 2024) (reference 20) report that on average, they improve upon the memetic algorithm by 11% on average.

When it comes to the evaluation of the path scanning approaches in the experiments, it is unclear how they are parameterized. From reading the paper (Aarakaki 2019) one sees that the parameter alpha and the number of iteration have a considerable impact both on solution time and solution quality, and (albeit on different instances), the average gaps for the path scanning approaches to the optimal (and to the memetic algrithm) reported in (Aarakaki 2019) are smaller than those found in the submission.

The description of the path improvement is not very clear; in particular the definition of the state used in the Dynamic Programming
algorithm. Is it a path? Is it the length of a path? Also, the statement ""f(*) denotes a state featuring dynamic programming"" is hard
to decipher.

Training time is not discussed at all.

Limitations:
Limitations are mostly addressed in a reasonable way. I suggest to add the following aspects:

I think that for small instances, the approach by (Rahmamoorty et. al 2024) may surpass the results reported here, which should be mentioned. 

Also, you should at least briefly mention the training time, since this makes it easier to assess the trade-off between offline effort and online performace.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presents a novel neural network-based CARP solver that uses a direction-aware attention model to incorporate directionality into the embedding process. It then applies supervised reinforcement learning for subsequent fine-tuning.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. A learning-based CARP solver is proposed.  
2. The performance of the proposed solver on large-scale data is discussed.

Weaknesses:
1. The comparison algorithms were published five years ago, and there is no discussion of existing methods aimed at big data.  
2. The experiments only tested the self-constructed dataset and did not evaluate on public datasets.

Limitations:
The amount of data required for algorithm training needs to be discussed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposes the first diffusion-based point cloud compression method called Diff-PCC.  
A dual-space latent representation is devised in this paper, where a compressor composed of two independent encoding backbones is used to extract expressive shape latents from different latent spaces.    
At the decoding side, a diffusion-based generator is devised to produce high-quality reconstructions by considering the shape latents as guidance to stochastically denoise the noisy point clouds.    
Experiments demonstrate that the proposed Diff-PCC achieves state-of-the-art compression performance (e.g., 7.711 14 dB BD-PSNR gains against the latest G-PCC standard at ultra-low bitrate) while attaining superior subjective quality.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Novelty is a strength. To my knowledge, diffusion model is used in point cloud compression for the first time. And the dual-latent design is also novel for learned point cloud compression.   

The manuscript is well written and easy to follow. Especially, the author did a good job in introducing related works on image compression, point cloud compression, point cloud analysis and diffusion model.

Weaknesses:
More work on diffusion model for data compression could be discussed, like ‘Idempotence and Perceptual Image Compression, ICLR 2024’. In addition, although this paper focuses on point cloud compression, the way of applying diffusion model should be compared with those learned image compression works in the related work part. From my impression, the method in this paper is still novel compared with those learned image compression paper using diffusion model.    

More recent learned point cloud compression method [30][14] should be compared in Table 1, Figure 3 and Figure 4, regarding rate distortion and encoding/decoding speed. Besides, only object point cloud is considered currently, large scale point cloud like SemanticKITTI could be compared [30][14].  
 
It is not clear how the speed is measured in Table 1. The hardware and commend line shoud be provided in the supplementary material.  

Minor:  
L86, Point·E[] is a typo.  
[30] and [31] are the same.   
L202, the reference should be fixed.   
What is the FPS in eq 14? farthest point sampling?

Limitations:
The limitation is addressed in the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the authors propose a diffusion-based point cloud compression framework. Low frequency and high frequency features are extracted via PointNet and PointPN from input point clouds, which are quantized and encoded for compression. During decompression, the quantized features would be decoded to condition a diffusion model to construct the decompressed results. The experiments on 15K points data from ShapeNet, ModelNet10, and ModelNet40 show superiority over compared methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea to introduce diffusion models for point cloud compression is different with former works;

2. The paper is easy to follow, while the disgrams are also good;

3. The performances show improvements on sparse point clouds.

Weaknesses:
1. The comparison is not convincing enough. Some commonly used compression methods are not compared, while the evaluation is limited to sparse point clouds with relatively simple structures from ShapeNet, ModelNet;

2. The motivation of using diffusion model for compression is questionable. As a sampling-based framework, diffusion models may construct different results during decompression from variant sampled noises each time. I am not so sure if the diffusion model is more appropriate than existing AE or VAE-based frameworks for the compression task, which may need decompression as accurate as possible;
Please check the questions for more details, thanks.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, they introduce the diffusion-based point  cloud compression method, dubbed Diff-PCC, to leverage the expressive power of the diffusion model for generative and aesthetically superior decoding. They get better performance than G-PCC and two deep learning methods.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Encoding point clouds using diffusion models is a good idea. The article is easy to understand.

Weaknesses:
Firstly, how do we obtain a point cloud with added noise in the decoder? We have no knowledge of any other information about the original point cloud, except for the information in the bitstream. This will result in the inability to decode.
This manuscript claims to achieve state-of-the-art compression performance, but it only compares with two deep learning methods from the past two years. It does not compare with the most advanced methods such as CNet, SparsePCGC, and so on.

Limitations:
The decorder will not work

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposes the first diffusion-based point cloud compression method called Diff-PCC.  
A dual-space latent representation is devised in this paper, where a compressor composed of two independent encoding backbones is used to extract expressive shape latents from different latent spaces.    
At the decoding side, a diffusion-based generator is devised to produce high-quality reconstructions by considering the shape latents as guidance to stochastically denoise the noisy point clouds.    
Experiments demonstrate that the proposed Diff-PCC achieves state-of-the-art compression performance (e.g., 7.711 14 dB BD-PSNR gains against the latest G-PCC standard at ultra-low bitrate) while attaining superior subjective quality.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Novelty is a strength. To my knowledge, diffusion model is used in point cloud compression for the first time. And the dual-latent design is also novel for learned point cloud compression.   

The manuscript is well written and easy to follow. Especially, the author did a good job in introducing related works on image compression, point cloud compression, point cloud analysis and diffusion model.

Weaknesses:
More work on diffusion model for data compression could be discussed, like ‘Idempotence and Perceptual Image Compression, ICLR 2024’. In addition, although this paper focuses on point cloud compression, the way of applying diffusion model should be compared with those learned image compression works in the related work part. From my impression, the method in this paper is still novel compared with those learned image compression paper using diffusion model.    

More recent learned point cloud compression method [30][14] should be compared in Table 1, Figure 3 and Figure 4, regarding rate distortion and encoding/decoding speed. Besides, only object point cloud is considered currently, large scale point cloud like SemanticKITTI could be compared [30][14].  
 
It is not clear how the speed is measured in Table 1. The hardware and commend line shoud be provided in the supplementary material.  

Minor:  
L86, Point·E[] is a typo.  
[30] and [31] are the same.   
L202, the reference should be fixed.   
What is the FPS in eq 14? farthest point sampling?

Limitations:
The limitation is addressed in the manuscript.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the authors propose a diffusion-based point cloud compression framework. Low frequency and high frequency features are extracted via PointNet and PointPN from input point clouds, which are quantized and encoded for compression. During decompression, the quantized features would be decoded to condition a diffusion model to construct the decompressed results. The experiments on 15K points data from ShapeNet, ModelNet10, and ModelNet40 show superiority over compared methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea to introduce diffusion models for point cloud compression is different with former works;

2. The paper is easy to follow, while the disgrams are also good;

3. The performances show improvements on sparse point clouds.

Weaknesses:
1. The comparison is not convincing enough. Some commonly used compression methods are not compared, while the evaluation is limited to sparse point clouds with relatively simple structures from ShapeNet, ModelNet;

2. The motivation of using diffusion model for compression is questionable. As a sampling-based framework, diffusion models may construct different results during decompression from variant sampled noises each time. I am not so sure if the diffusion model is more appropriate than existing AE or VAE-based frameworks for the compression task, which may need decompression as accurate as possible;
Please check the questions for more details, thanks.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, they introduce the diffusion-based point  cloud compression method, dubbed Diff-PCC, to leverage the expressive power of the diffusion model for generative and aesthetically superior decoding. They get better performance than G-PCC and two deep learning methods.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
1: poor

Strengths:
Encoding point clouds using diffusion models is a good idea. The article is easy to understand.

Weaknesses:
Firstly, how do we obtain a point cloud with added noise in the decoder? We have no knowledge of any other information about the original point cloud, except for the information in the bitstream. This will result in the inability to decode.
This manuscript claims to achieve state-of-the-art compression performance, but it only compares with two deep learning methods from the past two years. It does not compare with the most advanced methods such as CNet, SparsePCGC, and so on.

Limitations:
The decorder will not work

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The authors proposed a novel compression strategy of transformer based trackers. Unlike previous works, it divides the teacher network into multiple segments, each segment corresponds a single transformer layer of student network, then train each student layer separately. It also introduced some training strategies to enhance performance including (progressive) replacement training, prediction guidance and feature mimicking. Such compression framework is insensitive to the change of architecture of teacher network.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. Effectiveness. The experiment results clearly demonstrated significant improvement of inference speed while reserving the majority of tracking accuracy.
2. Flexibility. The proposed compression strategy is insensitive to the change of architecture of tracking models, making it easy to apply on almost any transformer based trackers. The segmentation strategy and the size of student network also supports user customization, which enables the user to design student network according to their unique demands. Such flexibility shows excellent application prospects in end-side scenarios.

Weaknesses:
The detailed strategy of dividing the teacher network is not stated clearly in the paper. Base on the pseudo code provided in page 13, it seems that the segmentation strategy is simply mapping the list of transformer blocks of student network to that of the teacher network base on the lengths of the two lists. This could be too simple.

For example, assume teacher network has 8 transformer blocks in module 1 and 2 blocks in module 2, while student network consists of 2 blocks, then the second student block would have to emulate the last 3 blocks of module 1 and the 2 blocks of module 2, while module 1 and  module 2 might have been trained separately and possess different knowledge. Empirically, this would result in sub-optical performance.

A brief discuss on the divide strategy could help this paper become more informative.

Limitations:
The paper clearly addressed its limitations including inefficient training process and the performance gap between teacher and student network.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to  distill knowledge from larger teacher models into more compact student trackers. Three techniques are proposed: A stage division strategy that segments the transformer layers of the teacher model. Replacement training technique. Prediction guidance and stage-wise feature mimicking. Experiment verifys the effectiveness of the method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The proposed techniques are comprehensive and include a bunch of methods to improve the performance and efficiency of the trackers, 
2.	The experiments are extensive which includes 5 VOT benchmarks. 
3.	The speed is fast when applying the 2 layer tracker variants.

Weaknesses:
1.	The most obvious weakness is that the whole method consists of many distilling techniques, including training strategies, feature mimicking, and loss guidance. It is hard to see the inherent consistency between those techniques. This may harm the generalization ability and transferability of the proposed framework, as the author claims the framework is general. 
2.	The overall method is complex. I am worried about its application to other researchers.
3.	When applied to the Mixformer v2, which has only 2 layers, performance can be improved marginally while speed is unchanged. This may indicate the method's shortcomings. Complex techniques only bring a little improvement.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces CompressTracker, a novel general model compression framework that enhances the efficiency of transformer-based object tracking models. It innovatively segments transformer layers into stages, enabling a more effective emulation of complex teacher models by lightweight student models. The framework incorporates a unique replacement training technique, prediction guidance, and feature mimicking to refine the student model's performance. Extensive experiments demonstrate CompressTracker's effectiveness in significantly speeding up tracking models with minimal loss of accuracy, showcasing its potential for real-time applications on resource-constrained devices.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1）	Innovative Approach: The paper presents a novel compression framework, CompressTracker, which innovatively addresses the challenge of deploying transformer-based trackers on resource-limited devices by significantly reducing model size and computation cost without substantial loss of accuracy.

2）Structural Flexibility: A key advantage of the proposed framework is its structural agnosticism, allowing it to be compatible with any transformer architecture. This flexibility enables the adaptation of CompressTracker to various student model configurations, catering to diverse deployment environments and computational constraints.

3）Efficiency and Performance: The paper demonstrates through extensive experiments that CompressTracker achieves a remarkable balance between inference speed and tracking accuracy. It notably accelerates the tracking process while maintaining high performance levels, as evidenced by the nearly 96% retention of original accuracy with a 2.17× speedup.

Weaknesses:
1）The concept of ""prediction guidance and stage-wise feature mimicking"" and the idea of BEVDistill [1] seem somewhat similar.

2）Despite the model's efficiency in inference, the training process for CompressTracker is relatively inefficient.

3）While the paper shows promising results on certain benchmarks, there may be concerns about how well these findings generalize across different types of tracking tasks and real-world scenarios.

4）The paper does not compare with other model compression techniques, such as knowledge distillation, model quantization, and pruning.

5）According to the results in Table 3, I observed that the outcomes of CompressTracker-2 are inferior to those of MixFormerV2-S. What could be the reason for this?

6）It is necessary to apply compression to other tracking models in order to further validate the efficacy of the CompressTracker presented in this paper.

7）The authors lack a sufficiently comprehensive review of the related work. The authors should give more reasonable related work by carefully introducing the recent approaches to tracking with compression, such as [2].

[1] BEVDistill: Cross-Modal BEV Distillation for Multi-View 3D Object Detection, ICLR 2023.

[2] Distilled Siamese Networks for Visual Tracking, TPAMI 2021.

Limitations:
Please refer to weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors proposed a general model compression framework for efficient Transformer object tracking, named CompressTracker. The method adopts a novel stage partitioning strategy to divide the Transformer layers of the teacher model into different stages, enabling the student model to more effectively simulate each corresponding teacher stage. The authors also designed a unique replacement training technique, which involves randomly replacing specific stages in the student model with specific stages in the teacher model. Replacement training enhances the student model's ability to replicate the behavior of the teacher model. To further force the student model to simulate the teacher model, we combine predictive guidance and staged feature imitation to provide additional supervision during the compression process of the teacher model. The authors conducted a series of experiments to verify the effectiveness and generality of CompressTracker.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The author has clear ideas and the article is easy to understand. He proposes a general compression framework for single object tracking. This method can efficiently compress large object tracking models into small models. The author has conducted a large number of experiments to prove the effectiveness of this method.

Weaknesses:
The font size of the pictures in the article is too small. The author can adjust the font size appropriately to facilitate reading. The training time line in Figure 1a is blocked, resulting in incomplete display. The font size of the tables is inconsistent, for example, the font size of Tables 5, 6, 7, and 8 is too large. The abstract is redundant and can be appropriately deleted.

Limitations:
For lightweight tracking models, the training time is too long. The author can try to find new ways to reduce the time spent on training.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces CompressTracker, a general model compression framework for efficient transformer-based object tracking. CompressTracker divides the teacher model into stages corresponding to student model layers and randomly replaces student stages with teacher stages during training. It also aligns the teacher and student models using prediction guidance and feature mimicking. The framework gradually increases the probability of using student stages throughout training. CompressTracker achieves significant speed improvements while maintaining high accuracy. For example, CompressTracker-4 accelerates OSTrack by 2.17x while preserving 96% of its accuracy on LaSOT.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Versatility: Compatible with various transformer architectures for student models.
- Efficiency: Achieves a good balance between inference speed and tracking accuracy.
- Streamlined training: Offers a single-step, end-to-end training process, simplifying the compression pipeline.

Weaknesses:
- Limited theoretical analysis: The paper focuses on empirical results without providing much theoretical justification for the proposed methods.
- Lack of ablation on some components: Some components of the framework are not thoroughly explored. For instance, the impact of different feature mimicking strategies is not extensively analyzed.
- Performance and Efficiency Trade-off: While CompressTracker maintains high accuracy, there's a slight performance drop compared to the original model. Training time for CompressTracker-4 (with only 4 blocks) exceeds that of the original OSTrack. This trade-off between training efficiency, inference speed, and model performance requires further optimization.
- The core idea of reducing the number of Transformer blocks is not new. Similar approaches have been used in other models like TinyViT[1] and MiniViT[2].


[1] Wu K, Zhang J, Peng H, et al. Tinyvit: Fast pretraining distillation for small vision transformers[C]//European conference on computer vision. Cham: Springer Nature Switzerland, 2022: 68-85.
[2] Zhang J, Peng H, Wu K, et al. Minivit: Compressing vision transformers with weight multiplexing[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 12145-12154.

Limitations:
No

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The authors proposed a novel compression strategy of transformer based trackers. Unlike previous works, it divides the teacher network into multiple segments, each segment corresponds a single transformer layer of student network, then train each student layer separately. It also introduced some training strategies to enhance performance including (progressive) replacement training, prediction guidance and feature mimicking. Such compression framework is insensitive to the change of architecture of teacher network.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. Effectiveness. The experiment results clearly demonstrated significant improvement of inference speed while reserving the majority of tracking accuracy.
2. Flexibility. The proposed compression strategy is insensitive to the change of architecture of tracking models, making it easy to apply on almost any transformer based trackers. The segmentation strategy and the size of student network also supports user customization, which enables the user to design student network according to their unique demands. Such flexibility shows excellent application prospects in end-side scenarios.

Weaknesses:
The detailed strategy of dividing the teacher network is not stated clearly in the paper. Base on the pseudo code provided in page 13, it seems that the segmentation strategy is simply mapping the list of transformer blocks of student network to that of the teacher network base on the lengths of the two lists. This could be too simple.

For example, assume teacher network has 8 transformer blocks in module 1 and 2 blocks in module 2, while student network consists of 2 blocks, then the second student block would have to emulate the last 3 blocks of module 1 and the 2 blocks of module 2, while module 1 and  module 2 might have been trained separately and possess different knowledge. Empirically, this would result in sub-optical performance.

A brief discuss on the divide strategy could help this paper become more informative.

Limitations:
The paper clearly addressed its limitations including inefficient training process and the performance gap between teacher and student network.

Rating:
9: Very Strong Accept: Technically flawless paper with groundbreaking impact on at least one area of AI/ML and excellent impact on multiple areas of AI/ML, with flawless evaluation, resources, and reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to  distill knowledge from larger teacher models into more compact student trackers. Three techniques are proposed: A stage division strategy that segments the transformer layers of the teacher model. Replacement training technique. Prediction guidance and stage-wise feature mimicking. Experiment verifys the effectiveness of the method.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The proposed techniques are comprehensive and include a bunch of methods to improve the performance and efficiency of the trackers, 
2.	The experiments are extensive which includes 5 VOT benchmarks. 
3.	The speed is fast when applying the 2 layer tracker variants.

Weaknesses:
1.	The most obvious weakness is that the whole method consists of many distilling techniques, including training strategies, feature mimicking, and loss guidance. It is hard to see the inherent consistency between those techniques. This may harm the generalization ability and transferability of the proposed framework, as the author claims the framework is general. 
2.	The overall method is complex. I am worried about its application to other researchers.
3.	When applied to the Mixformer v2, which has only 2 layers, performance can be improved marginally while speed is unchanged. This may indicate the method's shortcomings. Complex techniques only bring a little improvement.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces CompressTracker, a novel general model compression framework that enhances the efficiency of transformer-based object tracking models. It innovatively segments transformer layers into stages, enabling a more effective emulation of complex teacher models by lightweight student models. The framework incorporates a unique replacement training technique, prediction guidance, and feature mimicking to refine the student model's performance. Extensive experiments demonstrate CompressTracker's effectiveness in significantly speeding up tracking models with minimal loss of accuracy, showcasing its potential for real-time applications on resource-constrained devices.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1）	Innovative Approach: The paper presents a novel compression framework, CompressTracker, which innovatively addresses the challenge of deploying transformer-based trackers on resource-limited devices by significantly reducing model size and computation cost without substantial loss of accuracy.

2）Structural Flexibility: A key advantage of the proposed framework is its structural agnosticism, allowing it to be compatible with any transformer architecture. This flexibility enables the adaptation of CompressTracker to various student model configurations, catering to diverse deployment environments and computational constraints.

3）Efficiency and Performance: The paper demonstrates through extensive experiments that CompressTracker achieves a remarkable balance between inference speed and tracking accuracy. It notably accelerates the tracking process while maintaining high performance levels, as evidenced by the nearly 96% retention of original accuracy with a 2.17× speedup.

Weaknesses:
1）The concept of ""prediction guidance and stage-wise feature mimicking"" and the idea of BEVDistill [1] seem somewhat similar.

2）Despite the model's efficiency in inference, the training process for CompressTracker is relatively inefficient.

3）While the paper shows promising results on certain benchmarks, there may be concerns about how well these findings generalize across different types of tracking tasks and real-world scenarios.

4）The paper does not compare with other model compression techniques, such as knowledge distillation, model quantization, and pruning.

5）According to the results in Table 3, I observed that the outcomes of CompressTracker-2 are inferior to those of MixFormerV2-S. What could be the reason for this?

6）It is necessary to apply compression to other tracking models in order to further validate the efficacy of the CompressTracker presented in this paper.

7）The authors lack a sufficiently comprehensive review of the related work. The authors should give more reasonable related work by carefully introducing the recent approaches to tracking with compression, such as [2].

[1] BEVDistill: Cross-Modal BEV Distillation for Multi-View 3D Object Detection, ICLR 2023.

[2] Distilled Siamese Networks for Visual Tracking, TPAMI 2021.

Limitations:
Please refer to weaknesses.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, the authors proposed a general model compression framework for efficient Transformer object tracking, named CompressTracker. The method adopts a novel stage partitioning strategy to divide the Transformer layers of the teacher model into different stages, enabling the student model to more effectively simulate each corresponding teacher stage. The authors also designed a unique replacement training technique, which involves randomly replacing specific stages in the student model with specific stages in the teacher model. Replacement training enhances the student model's ability to replicate the behavior of the teacher model. To further force the student model to simulate the teacher model, we combine predictive guidance and staged feature imitation to provide additional supervision during the compression process of the teacher model. The authors conducted a series of experiments to verify the effectiveness and generality of CompressTracker.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The author has clear ideas and the article is easy to understand. He proposes a general compression framework for single object tracking. This method can efficiently compress large object tracking models into small models. The author has conducted a large number of experiments to prove the effectiveness of this method.

Weaknesses:
The font size of the pictures in the article is too small. The author can adjust the font size appropriately to facilitate reading. The training time line in Figure 1a is blocked, resulting in incomplete display. The font size of the tables is inconsistent, for example, the font size of Tables 5, 6, 7, and 8 is too large. The abstract is redundant and can be appropriately deleted.

Limitations:
For lightweight tracking models, the training time is too long. The author can try to find new ways to reduce the time spent on training.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper introduces CompressTracker, a general model compression framework for efficient transformer-based object tracking. CompressTracker divides the teacher model into stages corresponding to student model layers and randomly replaces student stages with teacher stages during training. It also aligns the teacher and student models using prediction guidance and feature mimicking. The framework gradually increases the probability of using student stages throughout training. CompressTracker achieves significant speed improvements while maintaining high accuracy. For example, CompressTracker-4 accelerates OSTrack by 2.17x while preserving 96% of its accuracy on LaSOT.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- Versatility: Compatible with various transformer architectures for student models.
- Efficiency: Achieves a good balance between inference speed and tracking accuracy.
- Streamlined training: Offers a single-step, end-to-end training process, simplifying the compression pipeline.

Weaknesses:
- Limited theoretical analysis: The paper focuses on empirical results without providing much theoretical justification for the proposed methods.
- Lack of ablation on some components: Some components of the framework are not thoroughly explored. For instance, the impact of different feature mimicking strategies is not extensively analyzed.
- Performance and Efficiency Trade-off: While CompressTracker maintains high accuracy, there's a slight performance drop compared to the original model. Training time for CompressTracker-4 (with only 4 blocks) exceeds that of the original OSTrack. This trade-off between training efficiency, inference speed, and model performance requires further optimization.
- The core idea of reducing the number of Transformer blocks is not new. Similar approaches have been used in other models like TinyViT[1] and MiniViT[2].


[1] Wu K, Zhang J, Peng H, et al. Tinyvit: Fast pretraining distillation for small vision transformers[C]//European conference on computer vision. Cham: Springer Nature Switzerland, 2022: 68-85.
[2] Zhang J, Peng H, Wu K, et al. Minivit: Compressing vision transformers with weight multiplexing[C]//Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2022: 12145-12154.

Limitations:
No

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper investigates the generalization capabilities of world models in RL, particularly with respect to latent representation errors, which arise when observations are encoded into a low-dimensional latent space. The authors provide a bound on latent representation error when using CNN encoder-decoder architectures. The world model is framed as a stochastic differential equation to characterize the impact of latent representation errors on generalization in terms of either zero or non-zero drift. The authors provide theoretical analysis which shows that these errors can result in implicit regularization in the zero drift case, and propose a Jacobian regularization scheme to tackle the unwanted bias term in the non-zero drift case. Finally, when performing model rollouts for learning a policy, the authors study the effect of these errors on the value function. Experiments on Mujoco tasks demonstrate that the proposed Jacobian regularization enhances robustness to noisy states, reduces the detrimental impact of latent representation errors, and improves convergence speed for longer horizon tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- World models are a popular area of research in the RL community, but there is a lack of theoretical understanding. This paper takes one step towards theoretically analyzing the generalization capabilities of world models.
- The analysis of the effect of latent representation error is a novel theoretical contribution, to the best of my knowledge.
- The results in the paper seem mathematically sound and provide useful insights. The empirical results demonstrate that the Jacobian regularization, which naturally arises from the theoretical analysis, is helpful in improving robustness.
- As a very theory-heavy paper, the authors structured the writing such that it makes it easy to follow each individual result (though there is some room for improvement here, see weaknesses).

Weaknesses:
While the paper studies a previously unexplored problem, there are some questions about the significance of these findings and the use of drift and diffusion terms to represent the error. Other areas for improvement include explaining the insights from the theoretical analysis more clearly, describing the experimental settings in more detail, and supporting certain claims with more evidence.

- Studying the effect of latent representation error is certainly useful, however, with recent advances in representation learning approaches, one can learn reasonably good representations such that the reconstruction error is negligible. When it comes to model-based RL, a much bigger issue is the compounding model error, which is a result of error in the latent/state dynamics model predictions. A comment from the authors on this aspect would be helpful.
- The decomposition of latent error into drift and diffusion terms seems a bit contrived. It is not clear how the error can be expressed in this form, and what defines the scenarios of zero versus non-zero drift.
- The interpretation that propagation of latent error leads to the model exploring novel states seems somewhat questionable. My understanding is that the erroneous states improve robustness similar to noise injection, but will most likely not be valid states belonging to the state space of the MDP. Some reasonable evidence is required to support this statement.
- The paper presents several results and including some intuitive or low-level explanation for each of those results would greatly improve readability. Additionally, due to the large amount of mathematical notation used throughout the paper, it would be helpful to include a notation table in the appendix for easy reference.
- The experimental setting is not sufficiently clear, especially in the introduction when the authors refer to Table 1. With regards to the perturbations - are they applied to every state in the trajectory? For masking, is the same mask used for every state, or is the mask also sampled randomly? With regards to injecting encoder error - how to interpret the $\mu_t$ and $\sigma_t$ values?

Limitations:
There is little discussion on the limitations of the analysis. Some points worth discussing could be the impact of various assumptions when deriving the results, the fact that the analysis is mostly focused on a specific setting - learning from pixels using a CNN encoder and an RNN latent dynamics model, and further investigation of the compounding model error problem.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the generalization capability of world models via a stochastic differential equation formulation. They try to understand latent representation errors on generalization, with both zero-drift representation errors and non-zero-drift representation errors. They found that zero drift latent representation errors are implicit regularization and thus bring generalization gain. Jacobian regularization is proposed to enhance training stability and generalization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ A deep understanding of the generalization of world models via stochastic differential equation formulation;
+ A careful study of the different effects of zero drift and non-zero drift on gn

Weaknesses:
+ The unseen images are produced via global/partial Gaussian noises and rotation, which seems more on the robustness side rather than the generalization of unseen images;

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores the generalization capability of world models in reinforcement learning. In particular, they investigate the latent representation error in world models. They show that zero-drift representation error is inherently a regularizer for the learned model functions. On the other hand, they show that the non-zero-drift representation error accumulates errors and Jacobian regularization can be used to alleviate the issue. They demonstrate their proposed approach improves stability, convergence, and performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This work investigates an interesting aspect, the generalization of world models that learn the dynamics of the environment. Very limited work has been done in this facet of RL, thus it will share significant insights with the DRL research community.

2. The paper followed a structured methodology to analyze the world model and its representation errors. They interpret the learned model function as stochastic differential equations (SDEs) and model the variation as Brownian motions. 

3. I liked the way they theoretically analyzed it case-by-case and established connections with prior findings. 

4. The paper articulately presents the findings of zero-drift error as a regularizer and the Jacobian correction term for non-zero-drift representation error.  It systematically proves its hypotheses and shows evidence against the claims. They presented corresponding formulas and interpretations.

Weaknesses:
1. The paper is very thorough in terms of theoretical derivation. However, in my opinion, the experimental section of the paper is somewhat lacking. It utilizes only two tasks from Mujoco to prove the efficacy of the approach. More diverse tasks from other benchmarks and robust perturbations will certainly improve the paper. 

2. The experimental evaluation is limited to reward comparison. However, it would be interesting to see some visualization of how the trajectories unfold in the case of both types of errors and with Jacobian regularization.

Limitations:
While the paper discusses the potential social impact of the work, it doesn’t discuss any limitations. I believe the characterization of the models as SDE and the use of Brownian motion as variation have certain contributions to the identified claims. Other interpretations may alter the findings.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper investigates the generalization capabilities of world models in RL, particularly with respect to latent representation errors, which arise when observations are encoded into a low-dimensional latent space. The authors provide a bound on latent representation error when using CNN encoder-decoder architectures. The world model is framed as a stochastic differential equation to characterize the impact of latent representation errors on generalization in terms of either zero or non-zero drift. The authors provide theoretical analysis which shows that these errors can result in implicit regularization in the zero drift case, and propose a Jacobian regularization scheme to tackle the unwanted bias term in the non-zero drift case. Finally, when performing model rollouts for learning a policy, the authors study the effect of these errors on the value function. Experiments on Mujoco tasks demonstrate that the proposed Jacobian regularization enhances robustness to noisy states, reduces the detrimental impact of latent representation errors, and improves convergence speed for longer horizon tasks.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- World models are a popular area of research in the RL community, but there is a lack of theoretical understanding. This paper takes one step towards theoretically analyzing the generalization capabilities of world models.
- The analysis of the effect of latent representation error is a novel theoretical contribution, to the best of my knowledge.
- The results in the paper seem mathematically sound and provide useful insights. The empirical results demonstrate that the Jacobian regularization, which naturally arises from the theoretical analysis, is helpful in improving robustness.
- As a very theory-heavy paper, the authors structured the writing such that it makes it easy to follow each individual result (though there is some room for improvement here, see weaknesses).

Weaknesses:
While the paper studies a previously unexplored problem, there are some questions about the significance of these findings and the use of drift and diffusion terms to represent the error. Other areas for improvement include explaining the insights from the theoretical analysis more clearly, describing the experimental settings in more detail, and supporting certain claims with more evidence.

- Studying the effect of latent representation error is certainly useful, however, with recent advances in representation learning approaches, one can learn reasonably good representations such that the reconstruction error is negligible. When it comes to model-based RL, a much bigger issue is the compounding model error, which is a result of error in the latent/state dynamics model predictions. A comment from the authors on this aspect would be helpful.
- The decomposition of latent error into drift and diffusion terms seems a bit contrived. It is not clear how the error can be expressed in this form, and what defines the scenarios of zero versus non-zero drift.
- The interpretation that propagation of latent error leads to the model exploring novel states seems somewhat questionable. My understanding is that the erroneous states improve robustness similar to noise injection, but will most likely not be valid states belonging to the state space of the MDP. Some reasonable evidence is required to support this statement.
- The paper presents several results and including some intuitive or low-level explanation for each of those results would greatly improve readability. Additionally, due to the large amount of mathematical notation used throughout the paper, it would be helpful to include a notation table in the appendix for easy reference.
- The experimental setting is not sufficiently clear, especially in the introduction when the authors refer to Table 1. With regards to the perturbations - are they applied to every state in the trajectory? For masking, is the same mask used for every state, or is the mask also sampled randomly? With regards to injecting encoder error - how to interpret the $\mu_t$ and $\sigma_t$ values?

Limitations:
There is little discussion on the limitations of the analysis. Some points worth discussing could be the impact of various assumptions when deriving the results, the fact that the analysis is mostly focused on a specific setting - learning from pixels using a CNN encoder and an RNN latent dynamics model, and further investigation of the compounding model error problem.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the generalization capability of world models via a stochastic differential equation formulation. They try to understand latent representation errors on generalization, with both zero-drift representation errors and non-zero-drift representation errors. They found that zero drift latent representation errors are implicit regularization and thus bring generalization gain. Jacobian regularization is proposed to enhance training stability and generalization.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
+ A deep understanding of the generalization of world models via stochastic differential equation formulation;
+ A careful study of the different effects of zero drift and non-zero drift on gn

Weaknesses:
+ The unseen images are produced via global/partial Gaussian noises and rotation, which seems more on the robustness side rather than the generalization of unseen images;

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper explores the generalization capability of world models in reinforcement learning. In particular, they investigate the latent representation error in world models. They show that zero-drift representation error is inherently a regularizer for the learned model functions. On the other hand, they show that the non-zero-drift representation error accumulates errors and Jacobian regularization can be used to alleviate the issue. They demonstrate their proposed approach improves stability, convergence, and performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This work investigates an interesting aspect, the generalization of world models that learn the dynamics of the environment. Very limited work has been done in this facet of RL, thus it will share significant insights with the DRL research community.

2. The paper followed a structured methodology to analyze the world model and its representation errors. They interpret the learned model function as stochastic differential equations (SDEs) and model the variation as Brownian motions. 

3. I liked the way they theoretically analyzed it case-by-case and established connections with prior findings. 

4. The paper articulately presents the findings of zero-drift error as a regularizer and the Jacobian correction term for non-zero-drift representation error.  It systematically proves its hypotheses and shows evidence against the claims. They presented corresponding formulas and interpretations.

Weaknesses:
1. The paper is very thorough in terms of theoretical derivation. However, in my opinion, the experimental section of the paper is somewhat lacking. It utilizes only two tasks from Mujoco to prove the efficacy of the approach. More diverse tasks from other benchmarks and robust perturbations will certainly improve the paper. 

2. The experimental evaluation is limited to reward comparison. However, it would be interesting to see some visualization of how the trajectories unfold in the case of both types of errors and with Jacobian regularization.

Limitations:
While the paper discusses the potential social impact of the work, it doesn’t discuss any limitations. I believe the characterization of the models as SDE and the use of Brownian motion as variation have certain contributions to the identified claims. Other interpretations may alter the findings.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,No,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper investigates the problem of latent post-treatment bias in causal models where there exists some proxy variables of the latent confounder and post-treatment variables. The authors first derive a general form of latent post-treatment bias which is intractable in most situations (except in special cases such as linear SCM). The authors state that the latent post-treatment bias can be arbitrarily bad for existing proxy-based causal inference methods. They then propose an identifiable VAE-based causal inference algorithm under the assumption that at least one dimension of each sufficient statistic of the latent prior is invertible. The proposed method is evaluated on both synthetic and real-world datasets to demonstrate its causal effect estimation capability with the presence of both latent confounders and post-treatment variables.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
• Causal reasoning in the context of latent confounder and post-treatment variables is an important topic especially with observational data.

• The authors clearly state the necessary assumptions for the identifiability of true latent variables, and the logic of determining the dimensions of $\boldsymbol{C}$ and $\boldsymbol{M}$ is well presented.

• The paper has a well-established theoretical basis.

Weaknesses:
•	For the illustrative example in the introduction, it might be better to explicitly specify what the post-treatment variable is.

•	Other existing works [1-3] on identifying latent confounder/mediators based on the iVAE architecture should also be included in the related work.

•	The role of post-treatment variables $\boldsymbol{M}$ seems to be a bit ambiguous. To be specific, is Theorem 4.1 valid for all types of relationships between $\boldsymbol{M}$ and $Y$?

•	The illustration of (iv) in Assumption 3 is a little confusing, as it assumes one extra degree of freedom on the prior parameters of $\boldsymbol{Z}$ and is critical to the identifiability of $\boldsymbol{Z}$ from $\boldsymbol{X}$. More explanation on this point will be appreciated.

•	The empirical evaluation consists of only one real-world dataset, which somehow limits the applicability of the proposed method.

References:

[1]. Zhou, D., & Wei, X. X. (2020). Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE. Advances in Neural Information Processing Systems, 33, 7234-7247.

[2]. Sorrenson, P., Rother, C., & Köthe, U. (2020). Disentanglement by nonlinear ica with general incompressible-flow networks (gin). arXiv preprint arXiv:2001.04872.

[3]. Jiang, Z., Liu, Y., Klein, M. H., Aloui, A., Ren, Y., Li, K., ... & Carlson, D. (2023). Causal Mediation Analysis with Multi-dimensional and Indirectly Observed Mediators. arXiv preprint arXiv:2306.07918.

Limitations:
The authors do not include a paragraph discussing the limitations and potential societal impact of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors deal with latent post-treatment bias for proxy-based methods which are employed for causal effect estimation.
They show that post-treatment variables can be latent and mixed into the observed covariates along with the latent confounders.
The authors transform the confounder-identifiability problem into a tractable pair-wise conditional independence test problem.
They prove that the latent confounders and latent post-treatment variables can be identified up to bijective transformations. Finally, they provide experimental analysis for their approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper deals with a very interesting problem.	 The proposed method appeared to be theoretically robust. The method is evaluated with proper experimental analysis on synthetic and real-world datasets and compared with multiple benchmarks.

Weaknesses:
Here I provide some weaknesses of the paper:
* Bi-directed edges in Figure 1 are not defined properly.
* Do-operator in equation 3 is not defined in detail.
* Assumptions in Assumption 2 should be described in more detail.
* The proposed method seems to depend on a lot of assumptions. Assumptions 1,2,3 each contain multiple assumptions. The authors should explain how their assumptions hold for the real-world scenarios they considered in their experiment section.

Limitations:
The authors discussed a very few limitations of their paper but more discussion should be done.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of causal inference with observational data, particularly when direct measurement of confounders is infeasible. The authors propose a new method, Confounder-identifiable Variational Autoencoder (CiVAE), to mitigate post-treatment bias using observed proxies for both latent confounders and latent post-treatment variables. The paper provides a theoretical analysis under specific assumptions and validates the proposed approach through experiments on both simulated and real-world datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The paper investigates a critical question concerning the mitigation of post-treatment bias, which is essential in various practical scenarios.
* The ideas presented in the paper are clear and easy to follow, and the theoretical analysis is well-established.

Weaknesses:
* In practical scenarios, interactions among latent factors are often present and can significantly impact the estimation. It would be beneficial if the authors could elaborate on how their method addresses these interactions and whether there are any theoretical guarantees regarding their handling in the proposed approach.

* The theoretical guarantees rely on strong assumptions, and the assumptions are hard to verify in practice. In assumption 1, the paper assumes an injective function of latent confounders and latent post-treatment variables into the observed proxy. This is a strong assumption,  and it will be much harder to meet the assumption in general when the function is nonlinear. The specific setup with strong assumptions limits the practical applicability of the proposed approach. It would be helpful if the authors could provide examples where these assumptions hold and demonstrate how they can be verified.

* The experiment lacks sufficient details on setup and implementation. Could the authors provide more specific information to enhance understanding of the empirical results?

Limitations:
* The proposed method relies on very strong assumptions to ensure identifiability, which can be challenging to verify in practical applications.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors investigated the issue of latent post-treatment bias in causal inference from observational data. They showed that estimator of existing proxy-of-confounder-based methd, i.e., DEV (f(X)), is an arbitrarily biased estimator of the Average Treatment Effect (ATE), when the selected proxy of confounders X accidentally mixes in latent post-treatment variables (Theorem 3.2). To address this issue, they proposed the Confounder-identifiable VAE (CiVAE), which identifies latent confounders up to bijective transformations under a mild assumption regarding the prior of latent factors. They showed that controlling for latent confounders inferred by CiVAE can provide an unbiased estimation of the ATE. Experiments on both simulated and real-world datasets demonstrate that CiVAE exhibits superior robustness to latent post-treatment bias compared to state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Being able to recover latent variables (cofounders, post-treatment variables, or others) from observations is challenging and important. Ignoring latent variables or assuming non-existence of latent variables is unrealistic and can lead to the wrong conclusion and decisions. The authors further motivated the importance of recovering latent cofounders, post-treatment variables and the consequence of not doing so  (Theorem 3.2). The solution provided shows originality and quality.

Weaknesses:
The presentation can be improved.

Limitations:
n.a.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper investigates the problem of latent post-treatment bias in causal models where there exists some proxy variables of the latent confounder and post-treatment variables. The authors first derive a general form of latent post-treatment bias which is intractable in most situations (except in special cases such as linear SCM). The authors state that the latent post-treatment bias can be arbitrarily bad for existing proxy-based causal inference methods. They then propose an identifiable VAE-based causal inference algorithm under the assumption that at least one dimension of each sufficient statistic of the latent prior is invertible. The proposed method is evaluated on both synthetic and real-world datasets to demonstrate its causal effect estimation capability with the presence of both latent confounders and post-treatment variables.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
• Causal reasoning in the context of latent confounder and post-treatment variables is an important topic especially with observational data.

• The authors clearly state the necessary assumptions for the identifiability of true latent variables, and the logic of determining the dimensions of $\boldsymbol{C}$ and $\boldsymbol{M}$ is well presented.

• The paper has a well-established theoretical basis.

Weaknesses:
•	For the illustrative example in the introduction, it might be better to explicitly specify what the post-treatment variable is.

•	Other existing works [1-3] on identifying latent confounder/mediators based on the iVAE architecture should also be included in the related work.

•	The role of post-treatment variables $\boldsymbol{M}$ seems to be a bit ambiguous. To be specific, is Theorem 4.1 valid for all types of relationships between $\boldsymbol{M}$ and $Y$?

•	The illustration of (iv) in Assumption 3 is a little confusing, as it assumes one extra degree of freedom on the prior parameters of $\boldsymbol{Z}$ and is critical to the identifiability of $\boldsymbol{Z}$ from $\boldsymbol{X}$. More explanation on this point will be appreciated.

•	The empirical evaluation consists of only one real-world dataset, which somehow limits the applicability of the proposed method.

References:

[1]. Zhou, D., & Wei, X. X. (2020). Learning identifiable and interpretable latent models of high-dimensional neural activity using pi-VAE. Advances in Neural Information Processing Systems, 33, 7234-7247.

[2]. Sorrenson, P., Rother, C., & Köthe, U. (2020). Disentanglement by nonlinear ica with general incompressible-flow networks (gin). arXiv preprint arXiv:2001.04872.

[3]. Jiang, Z., Liu, Y., Klein, M. H., Aloui, A., Ren, Y., Li, K., ... & Carlson, D. (2023). Causal Mediation Analysis with Multi-dimensional and Indirectly Observed Mediators. arXiv preprint arXiv:2306.07918.

Limitations:
The authors do not include a paragraph discussing the limitations and potential societal impact of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors deal with latent post-treatment bias for proxy-based methods which are employed for causal effect estimation.
They show that post-treatment variables can be latent and mixed into the observed covariates along with the latent confounders.
The authors transform the confounder-identifiability problem into a tractable pair-wise conditional independence test problem.
They prove that the latent confounders and latent post-treatment variables can be identified up to bijective transformations. Finally, they provide experimental analysis for their approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper deals with a very interesting problem.	 The proposed method appeared to be theoretically robust. The method is evaluated with proper experimental analysis on synthetic and real-world datasets and compared with multiple benchmarks.

Weaknesses:
Here I provide some weaknesses of the paper:
* Bi-directed edges in Figure 1 are not defined properly.
* Do-operator in equation 3 is not defined in detail.
* Assumptions in Assumption 2 should be described in more detail.
* The proposed method seems to depend on a lot of assumptions. Assumptions 1,2,3 each contain multiple assumptions. The authors should explain how their assumptions hold for the real-world scenarios they considered in their experiment section.

Limitations:
The authors discussed a very few limitations of their paper but more discussion should be done.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the challenge of causal inference with observational data, particularly when direct measurement of confounders is infeasible. The authors propose a new method, Confounder-identifiable Variational Autoencoder (CiVAE), to mitigate post-treatment bias using observed proxies for both latent confounders and latent post-treatment variables. The paper provides a theoretical analysis under specific assumptions and validates the proposed approach through experiments on both simulated and real-world datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* The paper investigates a critical question concerning the mitigation of post-treatment bias, which is essential in various practical scenarios.
* The ideas presented in the paper are clear and easy to follow, and the theoretical analysis is well-established.

Weaknesses:
* In practical scenarios, interactions among latent factors are often present and can significantly impact the estimation. It would be beneficial if the authors could elaborate on how their method addresses these interactions and whether there are any theoretical guarantees regarding their handling in the proposed approach.

* The theoretical guarantees rely on strong assumptions, and the assumptions are hard to verify in practice. In assumption 1, the paper assumes an injective function of latent confounders and latent post-treatment variables into the observed proxy. This is a strong assumption,  and it will be much harder to meet the assumption in general when the function is nonlinear. The specific setup with strong assumptions limits the practical applicability of the proposed approach. It would be helpful if the authors could provide examples where these assumptions hold and demonstrate how they can be verified.

* The experiment lacks sufficient details on setup and implementation. Could the authors provide more specific information to enhance understanding of the empirical results?

Limitations:
* The proposed method relies on very strong assumptions to ensure identifiability, which can be challenging to verify in practical applications.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors investigated the issue of latent post-treatment bias in causal inference from observational data. They showed that estimator of existing proxy-of-confounder-based methd, i.e., DEV (f(X)), is an arbitrarily biased estimator of the Average Treatment Effect (ATE), when the selected proxy of confounders X accidentally mixes in latent post-treatment variables (Theorem 3.2). To address this issue, they proposed the Confounder-identifiable VAE (CiVAE), which identifies latent confounders up to bijective transformations under a mild assumption regarding the prior of latent factors. They showed that controlling for latent confounders inferred by CiVAE can provide an unbiased estimation of the ATE. Experiments on both simulated and real-world datasets demonstrate that CiVAE exhibits superior robustness to latent post-treatment bias compared to state-of-the-art methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Being able to recover latent variables (cofounders, post-treatment variables, or others) from observations is challenging and important. Ignoring latent variables or assuming non-existence of latent variables is unrealistic and can lead to the wrong conclusion and decisions. The authors further motivated the importance of recovering latent cofounders, post-treatment variables and the consequence of not doing so  (Theorem 3.2). The solution provided shows originality and quality.

Weaknesses:
The presentation can be improved.

Limitations:
n.a.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,No,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper studies how to extract useful visual features from out-of-domain and action-free human videos to enhance robotic visualmotor control. Specifically, the authors argure that naively extracting spatial features via MAE is insufficient for robotics control, in contrast, jointly captureing spatial control and temporal movement will be more effective. To do so, the authors propose STP, a new self-supervised learning method, that simutaneously performs MAE on current frame to extract spatial information and predict furture frames to extract temporal motion clues. The overall motivation, idea and method are straightforward and reasonable. The authors evaluate STP on diverse benchmarks including 21 tasks spanning from simulation to real world tasks using imitation learning.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-motivated, highlighting the importance of pre-training visual features for robotic foundation models.

2. The logic in the paper is clear and easy to follow.


3. The proposed method is straightforward and simple to implement.

Weaknesses:
1. The high costs associated with evaluating real-world tasks using different random seeds make it challenging to report variances. However, assessing the impact of multiple random seeds in simulated tasks could provide more reliable statistical insights. As shown in Table 1, STP's performance improvement over baselines is marginal (STP 63.7 vs. VC-1 61.8, and STP-L/16(Post PT) 78.4 vs. MAE-L/16(Post PT) 76.7). Given the inherent stochastic nature of imitation learning and reinforcement learning, evaluations across multiple episodes and various random seeds are crucial to validate the proposed methods effectively.

2. Some previous methods also consider the temporal movements when extracting the visual features. For instance, the video-language alignment loss in R3M [1] tries to align language with correct visual transitions, which can extract semantic informations about visual movements. Voltron[2] and DecisionNCE [3] also try to extract the semantic features of the temporal movements between two frames. VIP[3] and LIV[4] use RL to extract visual features, which may also capture long-term movements via bootstrapping. Therefore, the authors could strengthen their paper by highlighting these related works, demonstrating awareness of existing methods, situating their contributions and highlighting the differences between STP and these baselines.

[1] R3M: A Universal Visual Representation for Robot Manipulation. CoRL 2023

[2] Language-Driven Representation Learning for Robotics. RSS 2023.

[3] DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning. ICML 2024.

[4] VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training. ICLR 2023.

[5] LIV: Language-Image Representations and Rewards for Robotic Control. ICML 2023

Limitations:
The authors have properly discussed the limiations in the Appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a new spatio-temporal pretraining algorithm for representation learning for robotics. The authors propose using masked autoencoding for reconstructing the current frame (for spatial reasoning) and a future frame (for temporal reasoning). The authors provide extensive experimentation across simulated and real-world settings and provide ablation studies to justify their design choices.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper addresses the important topic of including temporal dynamics in video data for pretraining robot representations.
- The paper does a good job of explaining the method and detailing the various experimental settings.
- The authors provide policy performance using both the pre-trained representations and post-pre-trained representations which helps assess both the quality of representations learned from internet data as well as the advantage of finetuning representations on the task-specific data. Overall, the proposed method has been extensively evaluated over varied settings across a variety of simulated settings.
- The authors provide an insightful ablation study to justify their design choices.

Weaknesses:
- It is unclear where the diverse image data for STP trained with Ego+I in Table 1 is obtained from. Some information about this would be helpful.
- The real-world experiments seem limited with only two real-world tasks where the MAE also performs reasonably well.
- The authors must include comparisons with prior works using MAE for spatiotemporal learning [1].

[1] Feichtenhofer, Christoph, Yanghao Li, and Kaiming He. ""Masked autoencoders as spatiotemporal learners."" Advances in neural information processing systems 35 (2022): 35946-35958.

Limitations:
The limitations have been addressed adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes STP, a visual representation learning method for robotic motor control. Trained on human videos, STP uses masked auto-encoders for spatial-temporal prediction. The spatial decoder predicts the current frame from its representation with 75% of patches masked. The temporal decoder predicts the future frame using the representations of 75%-masked current frame and the 95%-masked future frame. Experiments on various simulation and real-world tasks show the effectiveness of STP compared with baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposed method is simple yet effective, utilizing a masked spatial-temporal prediction objective to learn visual representations for robotics.
2. The paper presents extensive experimental results in both simulation and real-world settings, comparing with proper visual representation baselines.

Weaknesses:
1. Many works have considered temporal information for robot visual representation learning. This paper should mention these and highlight the differences. For example, R3M [1] uses temporal contrastive learning, while VIP [2] and V-PTR [3] use temporal difference.
2. Though STP outperforms the baselines in many benchmarks, the performance gap is not significant (Table 1). The slight performance difference may be due to hyperparameter selection and randomness, as the paper did not provide error bars over multiple seeds.

[1] R3m: A universal visual representation for robot manipulation, 2023
[2] Vip: Towards universal visual reward and representation via value-implicit pre-training, 2022
[3] Robotic Offline RL from Internet Videos via Value-Function Pre-Training, 2023

Limitations:
The authors have discussed the limitations. These cannot be addressed within the scope of this paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, we present a self-supervised pre-trained visual representation in robotic motor control, with spatiotemporal prediction with dual decoders, utilizing large-scale video data. The spatial prediction follows a standard MAE pipeline, and the temporal prediction tries to predict the future based on the current frame. The trained encoder is applied to downstream tasks and real-world robot task for better sample efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper adopts actionless human video data for representation learning, which can be easily obtained. The learned representation can be adapted to downstream robotics tasks. 

2. The experiments contain several real-world tasks, which could be more valuable for applying a pre-trained visual encoder to real-world domains that lack data.

Weaknesses:
1. The major concern is the novelty of the previous methods, considering several related papers that leverage human data and visuals pertaining to downstream tasks have been proposed [1-3].

2. The experiment only contains imitation learning experiments in downstream tasks, while the reinforcement learning framework with sub-optimal data is not considered. 

[1] Learning Manipulation by Predicting Interaction. RSS 2024

[2] Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning. https://arxiv.org/html/2402.14407

[3] Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation. https://arxiv.org/abs/2312.13139

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper studies how to extract useful visual features from out-of-domain and action-free human videos to enhance robotic visualmotor control. Specifically, the authors argure that naively extracting spatial features via MAE is insufficient for robotics control, in contrast, jointly captureing spatial control and temporal movement will be more effective. To do so, the authors propose STP, a new self-supervised learning method, that simutaneously performs MAE on current frame to extract spatial information and predict furture frames to extract temporal motion clues. The overall motivation, idea and method are straightforward and reasonable. The authors evaluate STP on diverse benchmarks including 21 tasks spanning from simulation to real world tasks using imitation learning.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-motivated, highlighting the importance of pre-training visual features for robotic foundation models.

2. The logic in the paper is clear and easy to follow.


3. The proposed method is straightforward and simple to implement.

Weaknesses:
1. The high costs associated with evaluating real-world tasks using different random seeds make it challenging to report variances. However, assessing the impact of multiple random seeds in simulated tasks could provide more reliable statistical insights. As shown in Table 1, STP's performance improvement over baselines is marginal (STP 63.7 vs. VC-1 61.8, and STP-L/16(Post PT) 78.4 vs. MAE-L/16(Post PT) 76.7). Given the inherent stochastic nature of imitation learning and reinforcement learning, evaluations across multiple episodes and various random seeds are crucial to validate the proposed methods effectively.

2. Some previous methods also consider the temporal movements when extracting the visual features. For instance, the video-language alignment loss in R3M [1] tries to align language with correct visual transitions, which can extract semantic informations about visual movements. Voltron[2] and DecisionNCE [3] also try to extract the semantic features of the temporal movements between two frames. VIP[3] and LIV[4] use RL to extract visual features, which may also capture long-term movements via bootstrapping. Therefore, the authors could strengthen their paper by highlighting these related works, demonstrating awareness of existing methods, situating their contributions and highlighting the differences between STP and these baselines.

[1] R3M: A Universal Visual Representation for Robot Manipulation. CoRL 2023

[2] Language-Driven Representation Learning for Robotics. RSS 2023.

[3] DecisionNCE: Embodied Multimodal Representations via Implicit Preference Learning. ICML 2024.

[4] VIP: Towards Universal Visual Reward and Representation via Value-Implicit Pre-Training. ICLR 2023.

[5] LIV: Language-Image Representations and Rewards for Robotic Control. ICML 2023

Limitations:
The authors have properly discussed the limiations in the Appendix.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a new spatio-temporal pretraining algorithm for representation learning for robotics. The authors propose using masked autoencoding for reconstructing the current frame (for spatial reasoning) and a future frame (for temporal reasoning). The authors provide extensive experimentation across simulated and real-world settings and provide ablation studies to justify their design choices.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper addresses the important topic of including temporal dynamics in video data for pretraining robot representations.
- The paper does a good job of explaining the method and detailing the various experimental settings.
- The authors provide policy performance using both the pre-trained representations and post-pre-trained representations which helps assess both the quality of representations learned from internet data as well as the advantage of finetuning representations on the task-specific data. Overall, the proposed method has been extensively evaluated over varied settings across a variety of simulated settings.
- The authors provide an insightful ablation study to justify their design choices.

Weaknesses:
- It is unclear where the diverse image data for STP trained with Ego+I in Table 1 is obtained from. Some information about this would be helpful.
- The real-world experiments seem limited with only two real-world tasks where the MAE also performs reasonably well.
- The authors must include comparisons with prior works using MAE for spatiotemporal learning [1].

[1] Feichtenhofer, Christoph, Yanghao Li, and Kaiming He. ""Masked autoencoders as spatiotemporal learners."" Advances in neural information processing systems 35 (2022): 35946-35958.

Limitations:
The limitations have been addressed adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes STP, a visual representation learning method for robotic motor control. Trained on human videos, STP uses masked auto-encoders for spatial-temporal prediction. The spatial decoder predicts the current frame from its representation with 75% of patches masked. The temporal decoder predicts the future frame using the representations of 75%-masked current frame and the 95%-masked future frame. Experiments on various simulation and real-world tasks show the effectiveness of STP compared with baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposed method is simple yet effective, utilizing a masked spatial-temporal prediction objective to learn visual representations for robotics.
2. The paper presents extensive experimental results in both simulation and real-world settings, comparing with proper visual representation baselines.

Weaknesses:
1. Many works have considered temporal information for robot visual representation learning. This paper should mention these and highlight the differences. For example, R3M [1] uses temporal contrastive learning, while VIP [2] and V-PTR [3] use temporal difference.
2. Though STP outperforms the baselines in many benchmarks, the performance gap is not significant (Table 1). The slight performance difference may be due to hyperparameter selection and randomness, as the paper did not provide error bars over multiple seeds.

[1] R3m: A universal visual representation for robot manipulation, 2023
[2] Vip: Towards universal visual reward and representation via value-implicit pre-training, 2022
[3] Robotic Offline RL from Internet Videos via Value-Function Pre-Training, 2023

Limitations:
The authors have discussed the limitations. These cannot be addressed within the scope of this paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, we present a self-supervised pre-trained visual representation in robotic motor control, with spatiotemporal prediction with dual decoders, utilizing large-scale video data. The spatial prediction follows a standard MAE pipeline, and the temporal prediction tries to predict the future based on the current frame. The trained encoder is applied to downstream tasks and real-world robot task for better sample efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper adopts actionless human video data for representation learning, which can be easily obtained. The learned representation can be adapted to downstream robotics tasks. 

2. The experiments contain several real-world tasks, which could be more valuable for applying a pre-trained visual encoder to real-world domains that lack data.

Weaknesses:
1. The major concern is the novelty of the previous methods, considering several related papers that leverage human data and visuals pertaining to downstream tasks have been proposed [1-3].

2. The experiment only contains imitation learning experiments in downstream tasks, while the reinforcement learning framework with sub-optimal data is not considered. 

[1] Learning Manipulation by Predicting Interaction. RSS 2024

[2] Large-Scale Actionless Video Pre-Training via Discrete Diffusion for Efficient Policy Learning. https://arxiv.org/html/2402.14407

[3] Unleashing Large-Scale Video Generative Pre-training for Visual Robot Manipulation. https://arxiv.org/abs/2312.13139

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,No,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper studies whether fine-tuning can elicit the hidden capabilities of LLMs, especially motivated by the setting of dangerous capabilities evaluations. 

To provide a specific experimental setup, the paper considers password-locked models, which responds with weak answers unless provided with a fixed password in the prompts. By giving strong demonstrations generated from a strong base model to the locked model, they show that SFT typically can recover most of the capabilities hidden in the locked model, achieving unlocking. 

The paper continues to study RL (with reward but not explicit demonstrations) and a toy setting where the password locking is achieved from scratch.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
In my view, the paper studies a well motivated concern in current safety research agenda on dangerous capability eval. The password-locking scenario is a novel instantiation that we can concretely study capability elicitation. I find that simply making this formal and executable is already a meaningful contribution.

The paper is nicely written and well structured. 

The two main lines of the experiments on SFT and RL are solid, and the results are novel.

Weaknesses:
For section 5, a minor concern I have is that the password-locking is achieved by SFT as well. This introduces this  weird ""causal structure"" in the experimental setup: specifically, the capability elicitation (in section 5) and the password-locking are both via SFT. I can imagine that we get the results in section 5, not (only) because SFT is good in capability elicitation, but because it's relatively weak in robustly password-locking the model. This might happen, for example, if password-locking a kind of an unnatural task, and a base LM trained on natural language corpus won't robustly lock itself via SFT. To be fair, I think this is not the most likely explanation of the results in section 5. But I am wondering if the author(s) has any take on the relative power of SFT. (Also, I think section 7 partially addresses this concern anyway, though I look forward to seeing more evidence against this weird hypothesis.)

The paper would generally be stronger if there is a straighforward baseline on capability elicitation via few-shot/many-shot prompting only (no messing with the model weights). Similarly, I'd love to see how RepE / steering vector perform on password-locked models; i.e., can you find a direction in the activation space to unlock the model? (I would guess you can.)

Limitations:
The paper addresses its limitations in section 8.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To determine whether models are safe to deploy, it's important for labs and regulators to be able to ""elicit"" model capabilities, including ones that are somehow ""hidden"" (whether by a malicious deployer, by some unforeseen property of the training data, or by an unaligned model that is able to plan). One way to evaluate for the presence of some capability is to fine-tune the model to perform the capability and check whether the model is able to do it. Does this work? The authors study this question by introducing and training ""password-locked"" models that are trained to only perform particular tasks well if a password is present in the prompt. The authors find that given sufficiently good demonstrations, fine-tuning and RL succeed at eliciting capabilities, but very poor demonstrations are insufficient.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The strongest contribution of this paper is the framing of the problem---motivating the problem of capability elicitation and showing some preliminary experiments in the space. The problem is pretty similar to backdoor detection but the new framing does provide different constraints which I find interesting: (1) the test model (in this case a password-locked model) doesn't necessarily have to be poisoned in a sneaky way like in backdoor attacks, instead the goal is to produce a test model in any way that somehow hides capabilities in a way that realistically imitates ""scheming"" or ""sandbagging""; (2) the attacker is not necessarily given access to the training data or high-quality data.
- Though I think it's unsurprising that fine-tuning successfully unlocks these password-locked models, I think the finding that fine-tuning on lower quality data doesn't work as well is an interesting contribution (a priori I could imagine that fine-tuning on lower quality data would still work to fully unlock the model if the password-locking were implemented internally as a simple conditional). The result in Fig 10 where domain generalization still works when each domain has a different password is also slightly surprising.
- Overall I like the experimental methodology (testing both supervised fine-tuning and RL, modulating the quality of the fine-tuning data, testing generalization across domains---all choices motivated by their problem framing).

Weaknesses:
The technical contributions here are somewhat limited. I'd say password-locking is a fairly simple way of modeling hidden capabilities and could be quite unlike the threat models that the authors are actually interested in (situationally-aware LLMs or AI labs deliberately hiding capabilities from auditors; in the case of situationally-aware models, presumably the model does some kind of complex internal reasoning to determine their next action, whereas these password-locked models seem likely to just learn some kind of simple conditional to output X or Y). Indeed, the authors acknowledge this briefly in the limitations section.  To some extent I think this is a result of the these threats not being understood well enough for us to know how to model them in a more realistic way than these password-locked models. Given the simplicity of password-locking I'd say that the success of the authors' elicitation methods are mostly unsurprising and also may not generalize to their real threat models.

Limitations:
The authors' limitations section is well written, and I can't think of any direct negative societal impacts.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study the problem of testing advanced LLMs for non-obvious capabilities in a toy setting, where a model has been fine-tuned to only perform well when presented with a fixed password, and to perform poorly otherwise. This is meant to simulate the difficulty of finding the correct prompt for eliciting a particular capability among many other suboptimal prompts. The authors then study various methods of fine-tuning to trigger the strong performance even without the right password, such as supervised fine-tuning on high/low-quality demonstrations as well as various forms of RL which do not require demonstrations at all. They find that performance on par with original with-password performance can be easily achieved, suggesting that prompt fragility and variability may not be difficult problems.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The problem is interesting, and the authors present a creative reduction to the toy problem of password-locked models
* A wide array of elicitation methods is considered, from many different settings of SFT as well as various demonstration-free methods using reinforcement learning to explore the more difficult where we do not have access to high quality demonstrations. Editorially, I do not think this is a very important setting, since it seems unlikely that we would ever be testing models for capabilities for which we have no idea what successful behaviors look like. And the dependence on RL approaches, which are in general much flakier than supervised learning, casts a lot of doubt on whether results in this setting would generalize in the real world. But the authors think this is important and study it carefully.
* Lots of analysis on a diverse set of task settings

Weaknesses:
* The authors explain the definition and process of password-locking models quite thoroughly, but their paper would be better served by a clearer and earlier motivation for studying password-locked models.
* In general, the authors lean heavily on references to prior work to explain various decisions in their experimental design, or different analyses of model performance. It would be helpful if these choices were just explained directly, and then separately credited prior work for motivating these decisions.
* There are various alternative approaches to eliciting stronger performance without the correct password, such as prompt optimization (e.g. with GCG or even gradient-free search) or few-shot prompting, which are not explored in this paper
* The nomenclature throughout the paper is a little confusing. The pi^weak_base notation was hard to keep track of, as well as the locked with password/locked without password/unlocked distinctions
* Given the close connection to neural backdoors/trojans, the authors should explain more clearly why or why not backdoor removal methods are not applicable here

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the problem of eliciting hidden capabilities from language models, through fine-tuning. In order to plant 'locked' capabilities and then do controlled test of elicitation, the authors lock the capabilities using passwords they bake into the prompts at fine-tuning. Then, they try to elicit the capability without the password in a zero shot way, and then after fine-tuning with a few good samples, or using RL and existing judgment labels. They find that using a few good FT samples the capabilities are almost completely attainable.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. I liked the idea of using passwords to control for the model behavior, and lock the capabilities.

Weaknesses:
1. I am not sure about the motivation of the paper. Is model locking something that people really do in practice?  Using a set of words in a prompt is pretty weak in practice, there might be stronger ways to lock a model, such as posthoc methods at decoding time. Also the findings and insights are not that surprising. Many safety alignment and jailbreaking papers show that alignment is 'shallow' and can be easily reversed [1,2]

2. Using fine-tuning and RL at decoding time is a pretty strong assumption, as having access to model parameters, training a model and also having access to high quality data is not that realistic.

[1] Patil, Vaidehi et al. “Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks.” ArXiv abs/2309.17410 (2023): n. pag.

[2] Yang, Xianjun, et al. ""Shadow alignment: The ease of subverting safely-aligned language models."" arXiv preprint arXiv:2310.02949 (2023).

Limitations:
The paper discusses limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper studies whether fine-tuning can elicit the hidden capabilities of LLMs, especially motivated by the setting of dangerous capabilities evaluations. 

To provide a specific experimental setup, the paper considers password-locked models, which responds with weak answers unless provided with a fixed password in the prompts. By giving strong demonstrations generated from a strong base model to the locked model, they show that SFT typically can recover most of the capabilities hidden in the locked model, achieving unlocking. 

The paper continues to study RL (with reward but not explicit demonstrations) and a toy setting where the password locking is achieved from scratch.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
In my view, the paper studies a well motivated concern in current safety research agenda on dangerous capability eval. The password-locking scenario is a novel instantiation that we can concretely study capability elicitation. I find that simply making this formal and executable is already a meaningful contribution.

The paper is nicely written and well structured. 

The two main lines of the experiments on SFT and RL are solid, and the results are novel.

Weaknesses:
For section 5, a minor concern I have is that the password-locking is achieved by SFT as well. This introduces this  weird ""causal structure"" in the experimental setup: specifically, the capability elicitation (in section 5) and the password-locking are both via SFT. I can imagine that we get the results in section 5, not (only) because SFT is good in capability elicitation, but because it's relatively weak in robustly password-locking the model. This might happen, for example, if password-locking a kind of an unnatural task, and a base LM trained on natural language corpus won't robustly lock itself via SFT. To be fair, I think this is not the most likely explanation of the results in section 5. But I am wondering if the author(s) has any take on the relative power of SFT. (Also, I think section 7 partially addresses this concern anyway, though I look forward to seeing more evidence against this weird hypothesis.)

The paper would generally be stronger if there is a straighforward baseline on capability elicitation via few-shot/many-shot prompting only (no messing with the model weights). Similarly, I'd love to see how RepE / steering vector perform on password-locked models; i.e., can you find a direction in the activation space to unlock the model? (I would guess you can.)

Limitations:
The paper addresses its limitations in section 8.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To determine whether models are safe to deploy, it's important for labs and regulators to be able to ""elicit"" model capabilities, including ones that are somehow ""hidden"" (whether by a malicious deployer, by some unforeseen property of the training data, or by an unaligned model that is able to plan). One way to evaluate for the presence of some capability is to fine-tune the model to perform the capability and check whether the model is able to do it. Does this work? The authors study this question by introducing and training ""password-locked"" models that are trained to only perform particular tasks well if a password is present in the prompt. The authors find that given sufficiently good demonstrations, fine-tuning and RL succeed at eliciting capabilities, but very poor demonstrations are insufficient.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The strongest contribution of this paper is the framing of the problem---motivating the problem of capability elicitation and showing some preliminary experiments in the space. The problem is pretty similar to backdoor detection but the new framing does provide different constraints which I find interesting: (1) the test model (in this case a password-locked model) doesn't necessarily have to be poisoned in a sneaky way like in backdoor attacks, instead the goal is to produce a test model in any way that somehow hides capabilities in a way that realistically imitates ""scheming"" or ""sandbagging""; (2) the attacker is not necessarily given access to the training data or high-quality data.
- Though I think it's unsurprising that fine-tuning successfully unlocks these password-locked models, I think the finding that fine-tuning on lower quality data doesn't work as well is an interesting contribution (a priori I could imagine that fine-tuning on lower quality data would still work to fully unlock the model if the password-locking were implemented internally as a simple conditional). The result in Fig 10 where domain generalization still works when each domain has a different password is also slightly surprising.
- Overall I like the experimental methodology (testing both supervised fine-tuning and RL, modulating the quality of the fine-tuning data, testing generalization across domains---all choices motivated by their problem framing).

Weaknesses:
The technical contributions here are somewhat limited. I'd say password-locking is a fairly simple way of modeling hidden capabilities and could be quite unlike the threat models that the authors are actually interested in (situationally-aware LLMs or AI labs deliberately hiding capabilities from auditors; in the case of situationally-aware models, presumably the model does some kind of complex internal reasoning to determine their next action, whereas these password-locked models seem likely to just learn some kind of simple conditional to output X or Y). Indeed, the authors acknowledge this briefly in the limitations section.  To some extent I think this is a result of the these threats not being understood well enough for us to know how to model them in a more realistic way than these password-locked models. Given the simplicity of password-locking I'd say that the success of the authors' elicitation methods are mostly unsurprising and also may not generalize to their real threat models.

Limitations:
The authors' limitations section is well written, and I can't think of any direct negative societal impacts.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study the problem of testing advanced LLMs for non-obvious capabilities in a toy setting, where a model has been fine-tuned to only perform well when presented with a fixed password, and to perform poorly otherwise. This is meant to simulate the difficulty of finding the correct prompt for eliciting a particular capability among many other suboptimal prompts. The authors then study various methods of fine-tuning to trigger the strong performance even without the right password, such as supervised fine-tuning on high/low-quality demonstrations as well as various forms of RL which do not require demonstrations at all. They find that performance on par with original with-password performance can be easily achieved, suggesting that prompt fragility and variability may not be difficult problems.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The problem is interesting, and the authors present a creative reduction to the toy problem of password-locked models
* A wide array of elicitation methods is considered, from many different settings of SFT as well as various demonstration-free methods using reinforcement learning to explore the more difficult where we do not have access to high quality demonstrations. Editorially, I do not think this is a very important setting, since it seems unlikely that we would ever be testing models for capabilities for which we have no idea what successful behaviors look like. And the dependence on RL approaches, which are in general much flakier than supervised learning, casts a lot of doubt on whether results in this setting would generalize in the real world. But the authors think this is important and study it carefully.
* Lots of analysis on a diverse set of task settings

Weaknesses:
* The authors explain the definition and process of password-locking models quite thoroughly, but their paper would be better served by a clearer and earlier motivation for studying password-locked models.
* In general, the authors lean heavily on references to prior work to explain various decisions in their experimental design, or different analyses of model performance. It would be helpful if these choices were just explained directly, and then separately credited prior work for motivating these decisions.
* There are various alternative approaches to eliciting stronger performance without the correct password, such as prompt optimization (e.g. with GCG or even gradient-free search) or few-shot prompting, which are not explored in this paper
* The nomenclature throughout the paper is a little confusing. The pi^weak_base notation was hard to keep track of, as well as the locked with password/locked without password/unlocked distinctions
* Given the close connection to neural backdoors/trojans, the authors should explain more clearly why or why not backdoor removal methods are not applicable here

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the problem of eliciting hidden capabilities from language models, through fine-tuning. In order to plant 'locked' capabilities and then do controlled test of elicitation, the authors lock the capabilities using passwords they bake into the prompts at fine-tuning. Then, they try to elicit the capability without the password in a zero shot way, and then after fine-tuning with a few good samples, or using RL and existing judgment labels. They find that using a few good FT samples the capabilities are almost completely attainable.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. I liked the idea of using passwords to control for the model behavior, and lock the capabilities.

Weaknesses:
1. I am not sure about the motivation of the paper. Is model locking something that people really do in practice?  Using a set of words in a prompt is pretty weak in practice, there might be stronger ways to lock a model, such as posthoc methods at decoding time. Also the findings and insights are not that surprising. Many safety alignment and jailbreaking papers show that alignment is 'shallow' and can be easily reversed [1,2]

2. Using fine-tuning and RL at decoding time is a pretty strong assumption, as having access to model parameters, training a model and also having access to high quality data is not that realistic.

[1] Patil, Vaidehi et al. “Can Sensitive Information Be Deleted From LLMs? Objectives for Defending Against Extraction Attacks.” ArXiv abs/2309.17410 (2023): n. pag.

[2] Yang, Xianjun, et al. ""Shadow alignment: The ease of subverting safely-aligned language models."" arXiv preprint arXiv:2310.02949 (2023).

Limitations:
The paper discusses limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposes a framework that integrates large multimodal language models (MLLMs) and diffusion models to enable holistic language planning and vision planning for long-horizon robotic manipulation tasks with complex instructions. The authors jointly train the MLLM and diffusion model for language reasoning and visual imagination through latent image token generation. An explicit consistency loss aligns the reasoned instructions with the imagined subgoal images.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Novel motivation for integrating of multiple modalities for providing better guidance.

2. Principled design of the framework components like the encoding-side alignment and the latent image token generation approach.

Weaknesses:
1. Weak experimental evaluation (see below questions).

Limitations:
Yes, the authors address the limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tackles the problem of long-horizon task planning on pick-and-place tasks in the Ravens domain. Given a dataset of trajectories, it first learns the projection to align the vision and language encoder for a multimodal LLM. Then it finetunes both the multimodal LLM and a diffusion model to generate a step action in language, where the diffusion model is used to generate a conditioning subgoal image, which is proposed as an intermediate step that helps with the step action generation in language.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is overall well-written and the figures are helpful for understanding the method.

Weaknesses:
- It is unclear, at least from the experiments in the paper, that the diffusion model is actually useful, especially when the output is still in language space. For example, it seems that the tasks studied in the paper can be easily tackled by a modern multimodal language model (likely even the open-sourced ones), by simply providing the the initial image and appropriate prompting. However, this is missing as an important baseline in the paper (and this does not require additional training data). Furthermore, to demonstrate the effectiveness of an image subgoal in addition to a language subgoal, the evaluation would have to be done on tasks that have subgoals that are difficult to describe in language but easy to describe in visual space, but all the evaluated tasks are the contrary.
- A related work “Video Language Planning” also seems to be missing from the paper, despite it might involve closed-sourced models. However, the idea seems quite relevant and it’s unclear if the paper provides additional insights for the community.

Limitations:
The limitations are described in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a holistic vision-language planning method for long-horizon robot manipulation, by learning a multi-modal large language model (MLLM). The MLLM generates interleaved language actions and keyframe images based on language goal and the initial image. Each pair of generated language and keyframe image is used as conditioning of a learned motion policy for robot manipulation.

Based on a pretrained MLLM model, the paper first learns a projector to align visual encoding to with language on image captioning tasks tailored to robot manipulation. Then it applies instruction tuning to fine-tune the MLLM, an output projector, and a diffusion model to generate interleaved language and images. Additional, the authors propose another training objective to align the generated language and images. All large models are fine-tuned with LoRA.

On simulated robot manipulatio benchmarks, the proposed method outperforms imitation learning, language planning, and vision planning methods. The paper also systematically evaluates capabilities of the MLLM along different axes, and justifies the benefits introduced by each loss design via ablation studies.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper tackles the important challenge of robot long-horizon planning. The proposed method plans jointly in the language and image space, providing rich information for the low-level policy to condition on.
- The paper exploits the capabilities of MLLM to generate language and images for robot manipulation, used with a separate low-level policy. I think this is good practice as MLLM is not naturally suitable to generate robot motion.
- The experiments are comprehensive and provide useful information on understanding the capability of the trained MLLM.
- The paper is in general well-written and easy to follow.

Weaknesses:
- The explanation of low-level policy is missing from the main paper. This part is very important - the MLLM outputs language and images only, and it's not clear how these modalities are bridged with robot motion.
- The contribution of the alignment loss between generated image and language is not sufficiently justified in the experiment. It will be helpful if the authors can provide the task success rate when the loss is absent.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on robotic manipulation with complex instructions. It proposes PERIA, a framework that integrates MLLM and diffusion models to incorporate both language planning and visual planning for long-horizon language-instructed manipulation tasks. Specifically, PERIA first performs a lightweight multi-modal alignment to consolidate the multi-modal perception capabilities. Then, PERIA performs multi-modal instruction tuning, where it outputs both subgoal language descriptions and visual tokens, both of which are fed to a diffusion model to generate subgoal images. PERIA introduces an additional consistency loss between and generated subgoal image and language descriptions. Experimental results demonstrate that PERIA significantly outperforms competitive baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
•	This work follows a natural and reasonable pipeline to tackle the manipulation tasks with complex language instructions. Combining language planning and visual generation for manipulation is a sound approach.

•	The alignment stage empowers the overall capabilities, as demonstrated in the experimental part.

•	PERIA achieves convincing experimental results compared with previous works. The authors also conduct extensive ablative study to mine more insights.

Weaknesses:
•	End-to-end learning for such a large system requires considerable cost. Such a comprehensive framework may lead to powerful performances but the resources may be a limitation. This paper does not present how much resources PERIA uses or related experiments to address such potential concerns.

•	One of my concerns is that the consistency objective, which forces the MLLM to output subgoal language descriptions, may suffer from accumulative error. This is because when the generated subgoal image is not the desired image but is a natural image that can be reached within one-step action, the MLLM would learn an incorrect subgoal description.

•	More literature references and related baselines should be incorporated.

•	The ablation in visual planning lacks an experiment where PERIA generates subgoal images with either subgoal descriptions or generated visual tokens, which should reveal more insights into what leads to the improvements in visual planning.

Limitations:
Yes, the authors address the limitations at the end of the conclusion.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


",Yes,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposes a framework that integrates large multimodal language models (MLLMs) and diffusion models to enable holistic language planning and vision planning for long-horizon robotic manipulation tasks with complex instructions. The authors jointly train the MLLM and diffusion model for language reasoning and visual imagination through latent image token generation. An explicit consistency loss aligns the reasoned instructions with the imagined subgoal images.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Novel motivation for integrating of multiple modalities for providing better guidance.

2. Principled design of the framework components like the encoding-side alignment and the latent image token generation approach.

Weaknesses:
1. Weak experimental evaluation (see below questions).

Limitations:
Yes, the authors address the limitations of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tackles the problem of long-horizon task planning on pick-and-place tasks in the Ravens domain. Given a dataset of trajectories, it first learns the projection to align the vision and language encoder for a multimodal LLM. Then it finetunes both the multimodal LLM and a diffusion model to generate a step action in language, where the diffusion model is used to generate a conditioning subgoal image, which is proposed as an intermediate step that helps with the step action generation in language.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is overall well-written and the figures are helpful for understanding the method.

Weaknesses:
- It is unclear, at least from the experiments in the paper, that the diffusion model is actually useful, especially when the output is still in language space. For example, it seems that the tasks studied in the paper can be easily tackled by a modern multimodal language model (likely even the open-sourced ones), by simply providing the the initial image and appropriate prompting. However, this is missing as an important baseline in the paper (and this does not require additional training data). Furthermore, to demonstrate the effectiveness of an image subgoal in addition to a language subgoal, the evaluation would have to be done on tasks that have subgoals that are difficult to describe in language but easy to describe in visual space, but all the evaluated tasks are the contrary.
- A related work “Video Language Planning” also seems to be missing from the paper, despite it might involve closed-sourced models. However, the idea seems quite relevant and it’s unclear if the paper provides additional insights for the community.

Limitations:
The limitations are described in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a holistic vision-language planning method for long-horizon robot manipulation, by learning a multi-modal large language model (MLLM). The MLLM generates interleaved language actions and keyframe images based on language goal and the initial image. Each pair of generated language and keyframe image is used as conditioning of a learned motion policy for robot manipulation.

Based on a pretrained MLLM model, the paper first learns a projector to align visual encoding to with language on image captioning tasks tailored to robot manipulation. Then it applies instruction tuning to fine-tune the MLLM, an output projector, and a diffusion model to generate interleaved language and images. Additional, the authors propose another training objective to align the generated language and images. All large models are fine-tuned with LoRA.

On simulated robot manipulatio benchmarks, the proposed method outperforms imitation learning, language planning, and vision planning methods. The paper also systematically evaluates capabilities of the MLLM along different axes, and justifies the benefits introduced by each loss design via ablation studies.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper tackles the important challenge of robot long-horizon planning. The proposed method plans jointly in the language and image space, providing rich information for the low-level policy to condition on.
- The paper exploits the capabilities of MLLM to generate language and images for robot manipulation, used with a separate low-level policy. I think this is good practice as MLLM is not naturally suitable to generate robot motion.
- The experiments are comprehensive and provide useful information on understanding the capability of the trained MLLM.
- The paper is in general well-written and easy to follow.

Weaknesses:
- The explanation of low-level policy is missing from the main paper. This part is very important - the MLLM outputs language and images only, and it's not clear how these modalities are bridged with robot motion.
- The contribution of the alignment loss between generated image and language is not sufficiently justified in the experiment. It will be helpful if the authors can provide the task success rate when the loss is absent.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper focuses on robotic manipulation with complex instructions. It proposes PERIA, a framework that integrates MLLM and diffusion models to incorporate both language planning and visual planning for long-horizon language-instructed manipulation tasks. Specifically, PERIA first performs a lightweight multi-modal alignment to consolidate the multi-modal perception capabilities. Then, PERIA performs multi-modal instruction tuning, where it outputs both subgoal language descriptions and visual tokens, both of which are fed to a diffusion model to generate subgoal images. PERIA introduces an additional consistency loss between and generated subgoal image and language descriptions. Experimental results demonstrate that PERIA significantly outperforms competitive baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
•	This work follows a natural and reasonable pipeline to tackle the manipulation tasks with complex language instructions. Combining language planning and visual generation for manipulation is a sound approach.

•	The alignment stage empowers the overall capabilities, as demonstrated in the experimental part.

•	PERIA achieves convincing experimental results compared with previous works. The authors also conduct extensive ablative study to mine more insights.

Weaknesses:
•	End-to-end learning for such a large system requires considerable cost. Such a comprehensive framework may lead to powerful performances but the resources may be a limitation. This paper does not present how much resources PERIA uses or related experiments to address such potential concerns.

•	One of my concerns is that the consistency objective, which forces the MLLM to output subgoal language descriptions, may suffer from accumulative error. This is because when the generated subgoal image is not the desired image but is a natural image that can be reached within one-step action, the MLLM would learn an incorrect subgoal description.

•	More literature references and related baselines should be incorporated.

•	The ablation in visual planning lacks an experiment where PERIA generates subgoal images with either subgoal descriptions or generated visual tokens, which should reveal more insights into what leads to the improvements in visual planning.

Limitations:
Yes, the authors address the limitations at the end of the conclusion.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


Label: ",Yes,yes,Yes,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper develops a switching RNN (SRNN) framework to model neural activity. It builds up on switching linear dynamical system models that are used in neuroscience to segment and extract underlying dynamics of observed neural activity. The different segments corresponding to unique dynamics often reflect distinct behavioral states. The crucial novelty of this work is that they allow the dynamics to be non-linear, unlike SLDS and rSLDS, making the model more expressive. They fit these models using VI using an inference network. Finally, they apply SRNN to synthetic data, as well as 3 distinct neural datasets and show that it outperforms SLDS and rSLDS on segmenting activity into behavioral modules where each module corresponds to distinct dynamics. They visualize these underlying dynamics, and also evaluate their fitted model on predicting future neural activity.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. As we move towards large-scale neural datasets, it is crucial to scale model complexity in order to fully harness these datasets. This paper makes a step in that direction by allowing for non-linear dynamics, while also providing an appropriate fitting approach.
2. The experiment section is extensive, and I appreciate the application to multiple neural datasets. I particularly found the results on the decision-making dataset to be most impressive. 
3. The literature review is thorough, and the authors do a good job of situating their work in the context of other related studies.

Weaknesses:
1. The authors mention switching nonlinear dynamical systems (Dong et al. 2020), and discuss how their work differs from Dong et al. I think it is important to either provide an experimental comparison to SNLDS or a justification for why these existing models are insufficient to explain neural datasets, as the main novelty/motivation for SRNN and SNLDS is very much related (also noted by the authors in the paper). More on this in the  question section.

2. Behavioral segmentations are somewhat subjective in nature, and while I can see that in the experiments shown here they make sense, in a real world setup we may want to infer the number of such segmentations from the data. Here the authors set the number of discrete states to the # of true behavioral states, however this might not be known in practice. Furthermore, there might be distinct sets of dynamics within one behavioral state due to other reasons not totally explicit from behavior. From the current set of results, it is not clear if SRNN is capable of inferring the # of underlying states. I will elaborate more in the questions section on this as well. 

3. I also think the paper will benefit from some editing by the authors. The references are not formatted properly, commas are missing. The referencing to supplementary figures doesn't seem to be working, it links back to figures in the main text. I also think the authors can trim some of the background, such as the section on VI, in favor of explaining some of the experiments such as the Lorenz attractor setup in more detail.

4. While I appreciate the extensive experiments, I find it hard to reconcile some of the results. It seems like in some of the plots (Fig 3C/D, Fig 5C/D) prediction + reconstruction performance across all models is similar. However, the discrete states being inferred look hugely inaccurate for SLDS and rSLDS. I wonder if the authors have thoughts on why this happens.

Limitations:
The authors have addressed limitations in the last section of the paper, and I do not envision any societal impact of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors develop a new class of probabilistic nonlinear state space models called switching RNNs. In essence, this extends the well-known switching linear dynamical system (SLDS) model to switch between nonlinear dynamics governed by a stochastic RNN.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The results shown in panels A of Figs 3, 4, and 5 are nice and convincing.

Weaknesses:
* Like many other deep learning based approaches, the model is not particularly interpretable. For example, panel F in Figs 3, 4, and 5 shows 2D flow fields for the different hidden states, but the RNN hidden state is 16-dimensional. Here the authors have used PCA to attempt to find a reasonable 2D flow field, but I know from experience that this has the potential to very poorly capture the true dynamics of the system. Intuitively, even small variance dimensions can matter a lot if the flow field changes rapidly along that dimension.

* There are many tunable parameters in this model (e.g. number of continuous and number of discrete states). It is unclear how to choose these on datasets without ground truth, or at least good educated guesses.

* Related to above, I worry a lot about the identifiability of this model. A nonlinear RNN without discrete switching can already model any flow field if given enough units. Thus a model with many continuous states (e.g. $P=128$) but zero discrete states may perform equally well to a model with few continuous states (e.g. $P=16$ or $P=8$) but a handful of discrete states. How would one then go about choosing between these models? Adding discussion or ideally some sort of mathematical analysis regarding the statistical identifiability of the model would be very helpful.

Limitations:
The discussion adequately acknowledges limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose to model time series neural population activity using switching recurrent neural networks. The generative model includes discrete latent states

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed method does appear to outperform related switching linear dynamical systems approaches in certain contexts.

Weaknesses:
High-level:
- The contribution beyond other switching nonlinear dynamical systems models is not clear. Such models include the cited Dong et al., 2020, as well as Karniol-Tambour et al., ICLR 2024. If there is a contribution beyond these works, the authors should compare against those existing related methods.
- The authors do not demonstrate an ability to automatically determine the appropriate number of discrete states. One approach to this might be ""co-smoothing"" (see Yu et al., Gaussian Process Factor Analysis, 2009).

Details:
- The mathematical details and notation are often unclear. For example, equation 2 does not appear to be a valid probability distribution, given the description that f(.) = tanh(.). Shouldn't this instead be a categorical distribution or similar? Relatedly, f is also used in equation 8, but from the context it appears to denote something entirely different.
- The authors should more clearly describe the cross-validation techniques for used for each dataset. The blanket statement in the intro to Section 4 (""On each dataset, we do N-fold cross-validation, where N equals to the number of conditions, sessions, or subjects in the dataset"") obscures how cross-validation was actually applied in each instance.

Limitations:
The authors address several limitations, including their need to manually set the number of discrete states, their need for good parameter initializations, and the heavy computational requirements for fitting their models.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes switching recurrent neural networks (SRNN), which allow the RNN weights to switch across time. The RNN weights switch based on a latent Markovian process of discrete states. The authors apply SRNN to a simulated dataset following the Lorenz attractor and three real-world neural recordings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Clarity: The authors clearly explain the problem, related work, and methodology with well-written equations and easy-to-understand figures. 

- Extensive use of datasets: The paper applies SRNN to numerous real-world neural datasets, illustrating the effectiveness of SRNN in accurately segmenting different datasets in an unsupervised fashion.

Weaknesses:
- Lack of comparison with other methods:
The paper compares SRNN to (r)SLDS models. However, there exist many other models for unsupervised segmentation. For example, ARHMMs and their extensions are simple yet powerful and interpretable models for segmentation [1, 2]. The authors should cite and consider comparisons with multiple model classes.
In addition, the paper notes in line 103 that SRNNs have the most comparable structure to SNLDS, but the authors do not make comparisons. The authors should also cite and compare with [3], which has switching nonlinear dynamics.

[1] Wiltschko, A. B., Johnson, M. J., Iurilli, G., Peterson, R. E., Katon, J. M., Pashkovski, S. L., ... & Datta, S. R. (2015). Mapping sub-second structure in mouse behavior. Neuron, 88(6), 1121-1135.

[2] Lee, H. D., Warrington, A., Glaser, J., & Linderman, S. (2023). Switching autoregressive low-rank tensor models. Advances in Neural Information Processing Systems, 36, 57976-58010.

[3] Karniol-Tambour, O., Zoltowski, D. M., Diamanti, E. M., Pinto, L., Tank, D. W., Brody, C. D., & Pillow, J. W. (2022). Modeling communication and switching nonlinear dynamics in multi-region neural activity. bioRxiv, 2022-09.

- Experiments:
The simulated experiment with the Lorenz attractor shows that SRNN does well when it has access to noiseless observations with known state dimensions. In order to have a more convincing simulated experiment, the authors could consider the following. First project the Lorenz attractor to a higher dimensional space and add additive Gaussian noise. Then fit SRNN (and other compared models) to the dataset to see if it can recover the Lorenz attractor and true latent state dimension (using some metric on held-out data). Another simulated experiment could be done with a dataset that simulates the NASCAR track [1,2].

[1] Linderman, S. W., Miller, A. C., Adams, R. P., Blei, D. M., Paninski, L., & Johnson, M. J. (2016). Recurrent switching linear dynamical systems. arXiv preprint arXiv:1610.08466.

[2] Lee, H. D., Warrington, A., Glaser, J., & Linderman, S. (2023). Switching autoregressive low-rank tensor models. Advances in Neural Information Processing Systems, 36, 57976-58010.

Limitations:
As the authors noted, some limitations of the model are that the model needs good initialization and that the model takes considerably more amount of time to train than simpler models such as SLDSs.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper develops a switching RNN (SRNN) framework to model neural activity. It builds up on switching linear dynamical system models that are used in neuroscience to segment and extract underlying dynamics of observed neural activity. The different segments corresponding to unique dynamics often reflect distinct behavioral states. The crucial novelty of this work is that they allow the dynamics to be non-linear, unlike SLDS and rSLDS, making the model more expressive. They fit these models using VI using an inference network. Finally, they apply SRNN to synthetic data, as well as 3 distinct neural datasets and show that it outperforms SLDS and rSLDS on segmenting activity into behavioral modules where each module corresponds to distinct dynamics. They visualize these underlying dynamics, and also evaluate their fitted model on predicting future neural activity.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. As we move towards large-scale neural datasets, it is crucial to scale model complexity in order to fully harness these datasets. This paper makes a step in that direction by allowing for non-linear dynamics, while also providing an appropriate fitting approach.
2. The experiment section is extensive, and I appreciate the application to multiple neural datasets. I particularly found the results on the decision-making dataset to be most impressive. 
3. The literature review is thorough, and the authors do a good job of situating their work in the context of other related studies.

Weaknesses:
1. The authors mention switching nonlinear dynamical systems (Dong et al. 2020), and discuss how their work differs from Dong et al. I think it is important to either provide an experimental comparison to SNLDS or a justification for why these existing models are insufficient to explain neural datasets, as the main novelty/motivation for SRNN and SNLDS is very much related (also noted by the authors in the paper). More on this in the  question section.

2. Behavioral segmentations are somewhat subjective in nature, and while I can see that in the experiments shown here they make sense, in a real world setup we may want to infer the number of such segmentations from the data. Here the authors set the number of discrete states to the # of true behavioral states, however this might not be known in practice. Furthermore, there might be distinct sets of dynamics within one behavioral state due to other reasons not totally explicit from behavior. From the current set of results, it is not clear if SRNN is capable of inferring the # of underlying states. I will elaborate more in the questions section on this as well. 

3. I also think the paper will benefit from some editing by the authors. The references are not formatted properly, commas are missing. The referencing to supplementary figures doesn't seem to be working, it links back to figures in the main text. I also think the authors can trim some of the background, such as the section on VI, in favor of explaining some of the experiments such as the Lorenz attractor setup in more detail.

4. While I appreciate the extensive experiments, I find it hard to reconcile some of the results. It seems like in some of the plots (Fig 3C/D, Fig 5C/D) prediction + reconstruction performance across all models is similar. However, the discrete states being inferred look hugely inaccurate for SLDS and rSLDS. I wonder if the authors have thoughts on why this happens.

Limitations:
The authors have addressed limitations in the last section of the paper, and I do not envision any societal impact of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors develop a new class of probabilistic nonlinear state space models called switching RNNs. In essence, this extends the well-known switching linear dynamical system (SLDS) model to switch between nonlinear dynamics governed by a stochastic RNN.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The results shown in panels A of Figs 3, 4, and 5 are nice and convincing.

Weaknesses:
* Like many other deep learning based approaches, the model is not particularly interpretable. For example, panel F in Figs 3, 4, and 5 shows 2D flow fields for the different hidden states, but the RNN hidden state is 16-dimensional. Here the authors have used PCA to attempt to find a reasonable 2D flow field, but I know from experience that this has the potential to very poorly capture the true dynamics of the system. Intuitively, even small variance dimensions can matter a lot if the flow field changes rapidly along that dimension.

* There are many tunable parameters in this model (e.g. number of continuous and number of discrete states). It is unclear how to choose these on datasets without ground truth, or at least good educated guesses.

* Related to above, I worry a lot about the identifiability of this model. A nonlinear RNN without discrete switching can already model any flow field if given enough units. Thus a model with many continuous states (e.g. $P=128$) but zero discrete states may perform equally well to a model with few continuous states (e.g. $P=16$ or $P=8$) but a handful of discrete states. How would one then go about choosing between these models? Adding discussion or ideally some sort of mathematical analysis regarding the statistical identifiability of the model would be very helpful.

Limitations:
The discussion adequately acknowledges limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose to model time series neural population activity using switching recurrent neural networks. The generative model includes discrete latent states

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The proposed method does appear to outperform related switching linear dynamical systems approaches in certain contexts.

Weaknesses:
High-level:
- The contribution beyond other switching nonlinear dynamical systems models is not clear. Such models include the cited Dong et al., 2020, as well as Karniol-Tambour et al., ICLR 2024. If there is a contribution beyond these works, the authors should compare against those existing related methods.
- The authors do not demonstrate an ability to automatically determine the appropriate number of discrete states. One approach to this might be ""co-smoothing"" (see Yu et al., Gaussian Process Factor Analysis, 2009).

Details:
- The mathematical details and notation are often unclear. For example, equation 2 does not appear to be a valid probability distribution, given the description that f(.) = tanh(.). Shouldn't this instead be a categorical distribution or similar? Relatedly, f is also used in equation 8, but from the context it appears to denote something entirely different.
- The authors should more clearly describe the cross-validation techniques for used for each dataset. The blanket statement in the intro to Section 4 (""On each dataset, we do N-fold cross-validation, where N equals to the number of conditions, sessions, or subjects in the dataset"") obscures how cross-validation was actually applied in each instance.

Limitations:
The authors address several limitations, including their need to manually set the number of discrete states, their need for good parameter initializations, and the heavy computational requirements for fitting their models.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes switching recurrent neural networks (SRNN), which allow the RNN weights to switch across time. The RNN weights switch based on a latent Markovian process of discrete states. The authors apply SRNN to a simulated dataset following the Lorenz attractor and three real-world neural recordings.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Clarity: The authors clearly explain the problem, related work, and methodology with well-written equations and easy-to-understand figures. 

- Extensive use of datasets: The paper applies SRNN to numerous real-world neural datasets, illustrating the effectiveness of SRNN in accurately segmenting different datasets in an unsupervised fashion.

Weaknesses:
- Lack of comparison with other methods:
The paper compares SRNN to (r)SLDS models. However, there exist many other models for unsupervised segmentation. For example, ARHMMs and their extensions are simple yet powerful and interpretable models for segmentation [1, 2]. The authors should cite and consider comparisons with multiple model classes.
In addition, the paper notes in line 103 that SRNNs have the most comparable structure to SNLDS, but the authors do not make comparisons. The authors should also cite and compare with [3], which has switching nonlinear dynamics.

[1] Wiltschko, A. B., Johnson, M. J., Iurilli, G., Peterson, R. E., Katon, J. M., Pashkovski, S. L., ... & Datta, S. R. (2015). Mapping sub-second structure in mouse behavior. Neuron, 88(6), 1121-1135.

[2] Lee, H. D., Warrington, A., Glaser, J., & Linderman, S. (2023). Switching autoregressive low-rank tensor models. Advances in Neural Information Processing Systems, 36, 57976-58010.

[3] Karniol-Tambour, O., Zoltowski, D. M., Diamanti, E. M., Pinto, L., Tank, D. W., Brody, C. D., & Pillow, J. W. (2022). Modeling communication and switching nonlinear dynamics in multi-region neural activity. bioRxiv, 2022-09.

- Experiments:
The simulated experiment with the Lorenz attractor shows that SRNN does well when it has access to noiseless observations with known state dimensions. In order to have a more convincing simulated experiment, the authors could consider the following. First project the Lorenz attractor to a higher dimensional space and add additive Gaussian noise. Then fit SRNN (and other compared models) to the dataset to see if it can recover the Lorenz attractor and true latent state dimension (using some metric on held-out data). Another simulated experiment could be done with a dataset that simulates the NASCAR track [1,2].

[1] Linderman, S. W., Miller, A. C., Adams, R. P., Blei, D. M., Paninski, L., & Johnson, M. J. (2016). Recurrent switching linear dynamical systems. arXiv preprint arXiv:1610.08466.

[2] Lee, H. D., Warrington, A., Glaser, J., & Linderman, S. (2023). Switching autoregressive low-rank tensor models. Advances in Neural Information Processing Systems, 36, 57976-58010.

Limitations:
As the authors noted, some limitations of the model are that the model needs good initialization and that the model takes considerably more amount of time to train than simpler models such as SLDSs.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper works on node anomaly detection in the novel semi-supervised setting where few labeled normal nodes are given and proposes to generate new anomaly nodes to boost the training data. The anomaly generation algorithm is inspired by the empirical observation that:

(1) Anomaly nodes have lower affinity score than normal nodes
(2) Feature distribution of anomaly nodes are similar to normal nodes if they share similar neighborhood patterns.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
(1) The setting is novel and aligned to the real-world situation where normal nodes are typically known compared with anomaly nodes.

(2) The motivation for the proposed two regularization losses is very intuitive and clear.

(3) The experimental results are very impressive.

Weaknesses:
(1) The proposed two regularization losses are heavily based on the empirical analysis, which might not transfer to other anomalies in other datasets. 

(2) For the second prior, its assumption that anomaly nodes sharing similar local structures would share a similar feature distribution has not been empirically verified.

(3) Experiments miss the comparison with diffusion-based generative anomaly detection baseline.

Limitations:
In addition to the limitations mentioned by the author, there are some other limitations worth addressing:

(1) The currently proposed anomaly generation method is still operated in the embedding space. As admitted by the author anomaly behavior is heavily based on interactional behaviors, therefore, it is also helpful to consider directly characterizing/generating anomaly in the graph space.

(2) The comparison misses one generative-based baseline [1]

[1] Liu, Kay, et al. ""Graph diffusion models for anomaly detection."" (2024).

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel approach called GGAD aimed at improving anomaly detection in graphs under a semi-supervised framework. GGAD generates pseudo anomaly nodes that serve as negative samples for training a one-class classifier. This method is built on two
key priors: asymmetric local affinity and egocentric closeness, which help in generating reliable outlier nodes that mimic real anomalies in terms of both graph structure and feature representation. Extensive experimental results demonstrate the effectiveness of the method across diverse graph anomaly detection datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The method is innovative. The proposed graph anomaly detection method can exploit the feature and structure information of normal nodes more effectively in the studied semi-supervised scenario compared to existing methods.  The proposed two priors provide a meaningful characterization of desired properties of outliers in this semi-supervised setting and can be utilized to explore other beneficial priors further. 

2.The experiments in the paper are comprehensive and thorough.

Weaknesses:
1. The model relies on prior knowledge to generate anomaly points. This prior knowledge can limit the model’s application scenarios. The model performs best only when the real anomalies align with this prior knowledge. For anomaly types that do not conform to the prior knowledge, the model may not effectively detect them.

2.The model does not perform best on the Photo dataset in Table 1, and the article lacks an explanation of the results at the overall data level.

3. This model employs a semi-supervised approach that uses some positive samples for training. However, it does not consider the issue of noise interference within the positive samples, namely, how the model overcomes interference when some positive samples are mislabeled.

4. During the initialization step, only the initial feature of outliers are obtained while the connections between the outliers and normal nodes are not well illustrated in the paper. From Figure 2, one outlier is connected to more than one normal node while the feature of the outlier is generated according to single normal node. The neighborhood of outliers is important since the it involves the computation of node affinity score of outliers.

Limitations:
yes, the authors point out that some anomalies whose characteristics may not be captured by the two priors used

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel generative-based GAD approach, named GGAD, tailored for the semi-supervised scenario. Unlike existing GAD frameworks, the authors highlight the feasibility and importance of a semi-supervised setting where labels for normal nodes are relatively easy to obtain during training, but labeled abnormal nodes are very limited. In this context, the paper proposes generating pseudo-anomaly nodes to serve as substitutes for real anomaly nodes in training, thus aiding in anomaly detection. These pseudo-anomalies are generated through two unique loss-guidance mechanisms. Experimental results demonstrate the effectiveness of GGAD.

However, the description of the semi-supervised setting in this paper lacks clarity and unconvincing. Additionally, there is minimal differentiation between the proposed method and existing works that generate pseudo-anomaly samples for data augmentation. I think this paper's novelty is limited. I still think that doing unsupervised GAD is more necessary, and if the authors can prove that the pseudo-outlier proposed by GGAD can benefit unsupervised GAD as a general module, I can up my score.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.The complete experiment shows the effectiveness of the method and the necessity of each component.

2.Some visual illustrations help the reader understand, although the shapes of the images seem to be compressed.

Weaknesses:
1. I am still confused about the motivation for performing semi-supervised GAD. Why do most methods emphasize unsupervised scenarios? The cost of labeling normal nodes seems too expensive, as the authors themselves state on lines 268 to 269, yet they assert again on line 31 that labels for normal nodes are easy to obtain.This inconsistency hinders a clear understanding of the necessity and practical applications of semi-supervised GAD, which significantly undermines the motivation for this work.

2. While the first loss function proposed by the authors appears intuitively valid, the second loss function aims to generate outliers similar to normal nodes. In my opinion, optimizing these two losses together is unreasonable because they conflict with each other. It seems that they should correspond to different outlier generation processes

3. The paper validates the improvement of unsupervised GAD using labeled normal nodes and claims that GGAD remains superior. I think the authors ignore the fact that unsupervised methods do not obtain this outlier like GGAD and this comparison is not reasonable.

Limitations:
No limitation need to discuss

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper explores the problem of semi-supervised graph anomaly detection (GAD), where some nodes are known to be normal, in contrast to the typical unsupervised setting with no labeled data. The authors show that even a small percentage of labeled normal nodes can improve the performance of existing unsupervised GAD methods when adapted to the semi-supervised scenario. The paper proposes a novel Generative GAD approach (GGAD) to better exploit normal nodes by generating pseudo anomaly nodes, called 'outlier nodes', to provide effective negative samples for training a one-class classifier. GGAD generates these outlier nodes using priors about anomaly nodes, such as asymmetric local affinity and egocentric closeness, to mimic anomalies in structure and features. Experiments on six real-world GAD datasets show that GGAD outperforms state-of-the-art methods in both unsupervised and semi-supervised settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ This paper studies a new problem of semi-supervised GAD that has not been widely studied. 

+ The proposed method is simple and effective from the empirical perspective.

+ The experiments are extensive including effectiveness and efficiency analyses and the method has been tested on real-world large-scale graphs to verify the scalability.

Weaknesses:
- The two priors that are used to generate outlier nodes are heuristic or based on empirical evidence. There is no theoretical analysis provided to better guarantee the effectiveness of the proposed method.

- It will be more interesting and helpful to show the generated outlier nodes can capture the characteristics of anomalous nodes in addition to comparing their representations.

- The experimental settings of anomaly contamination are not very clear: how the contamination is introduced?

- Overall experimental settings. What hardware has been used in the experiments, e.g., memory, and why are the experiments conducted on CPUs?

Limitations:
The authors have adequately addressed the limitations

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies an under-explored graph anomaly detection problem where the detection models have access to a set of labeled normal nodes. To tackle this problem, it introduces a generative approach namely GGAD that generates pseudo anomaly nodes, called outlier nodes, to support the training of a discriminative one-class classifier. The key idea underlying this approach is to generate the outlier nodes in a way that can well simulate real anomaly nodes in both graph structure and feature representation perspectives. To achieve this, GGAD defines and incorporates two priors, including asymmetric local affinity and egocentric closeness, into its optimization objectives, with the former prior focusing on the alignment on the graph structure aspect and the latter on the feature representation aspect. The method is evaluated on six large real-world datasets and shows impressive detection performance compared to existing state-of-the-art methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is generally well-written and easy-to-follow.
- The problem setting is practical since labeled normal samples are easy to obtain in many real-world applications. Compared to the commonly studied unsupervised setting, this semi-supervised setting often results in better detection performance.
- The proposed method GGAD is novel. There have been many generative anomaly detection methods, but as far as I know, they are unable to consider the graph structure and the neighboring nodes’ representations. By introducing the two new priors, GGAD addresses this issue well. Fig.1 and Fig. 3 help demonstrate this effect.
- The method is compared with a range of unsupervised and semi-supervised methods on 6 real-world datasets with diverse genuine anomalies, and gains largely improved detection performance over these competing methods.
- The ablation study is plausible and justifies the contribution of each proposed prior.

Weaknesses:
- The outlier node generation in GGAD may cause non-trivial computational overhead.
- Despite better performance than the competing methods, GGAD gains an AUC of only around 0.6 on some datasets, such as DGraph and Reddit.
- In Fig. 4 (b), GGAD shows a fast AUPRC growth with increasing training size, but the other methods have a flat performance trend. What would be the reason behind?

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper works on node anomaly detection in the novel semi-supervised setting where few labeled normal nodes are given and proposes to generate new anomaly nodes to boost the training data. The anomaly generation algorithm is inspired by the empirical observation that:

(1) Anomaly nodes have lower affinity score than normal nodes
(2) Feature distribution of anomaly nodes are similar to normal nodes if they share similar neighborhood patterns.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
(1) The setting is novel and aligned to the real-world situation where normal nodes are typically known compared with anomaly nodes.

(2) The motivation for the proposed two regularization losses is very intuitive and clear.

(3) The experimental results are very impressive.

Weaknesses:
(1) The proposed two regularization losses are heavily based on the empirical analysis, which might not transfer to other anomalies in other datasets. 

(2) For the second prior, its assumption that anomaly nodes sharing similar local structures would share a similar feature distribution has not been empirically verified.

(3) Experiments miss the comparison with diffusion-based generative anomaly detection baseline.

Limitations:
In addition to the limitations mentioned by the author, there are some other limitations worth addressing:

(1) The currently proposed anomaly generation method is still operated in the embedding space. As admitted by the author anomaly behavior is heavily based on interactional behaviors, therefore, it is also helpful to consider directly characterizing/generating anomaly in the graph space.

(2) The comparison misses one generative-based baseline [1]

[1] Liu, Kay, et al. ""Graph diffusion models for anomaly detection."" (2024).

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a novel approach called GGAD aimed at improving anomaly detection in graphs under a semi-supervised framework. GGAD generates pseudo anomaly nodes that serve as negative samples for training a one-class classifier. This method is built on two
key priors: asymmetric local affinity and egocentric closeness, which help in generating reliable outlier nodes that mimic real anomalies in terms of both graph structure and feature representation. Extensive experimental results demonstrate the effectiveness of the method across diverse graph anomaly detection datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The method is innovative. The proposed graph anomaly detection method can exploit the feature and structure information of normal nodes more effectively in the studied semi-supervised scenario compared to existing methods.  The proposed two priors provide a meaningful characterization of desired properties of outliers in this semi-supervised setting and can be utilized to explore other beneficial priors further. 

2.The experiments in the paper are comprehensive and thorough.

Weaknesses:
1. The model relies on prior knowledge to generate anomaly points. This prior knowledge can limit the model’s application scenarios. The model performs best only when the real anomalies align with this prior knowledge. For anomaly types that do not conform to the prior knowledge, the model may not effectively detect them.

2.The model does not perform best on the Photo dataset in Table 1, and the article lacks an explanation of the results at the overall data level.

3. This model employs a semi-supervised approach that uses some positive samples for training. However, it does not consider the issue of noise interference within the positive samples, namely, how the model overcomes interference when some positive samples are mislabeled.

4. During the initialization step, only the initial feature of outliers are obtained while the connections between the outliers and normal nodes are not well illustrated in the paper. From Figure 2, one outlier is connected to more than one normal node while the feature of the outlier is generated according to single normal node. The neighborhood of outliers is important since the it involves the computation of node affinity score of outliers.

Limitations:
yes, the authors point out that some anomalies whose characteristics may not be captured by the two priors used

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a novel generative-based GAD approach, named GGAD, tailored for the semi-supervised scenario. Unlike existing GAD frameworks, the authors highlight the feasibility and importance of a semi-supervised setting where labels for normal nodes are relatively easy to obtain during training, but labeled abnormal nodes are very limited. In this context, the paper proposes generating pseudo-anomaly nodes to serve as substitutes for real anomaly nodes in training, thus aiding in anomaly detection. These pseudo-anomalies are generated through two unique loss-guidance mechanisms. Experimental results demonstrate the effectiveness of GGAD.

However, the description of the semi-supervised setting in this paper lacks clarity and unconvincing. Additionally, there is minimal differentiation between the proposed method and existing works that generate pseudo-anomaly samples for data augmentation. I think this paper's novelty is limited. I still think that doing unsupervised GAD is more necessary, and if the authors can prove that the pseudo-outlier proposed by GGAD can benefit unsupervised GAD as a general module, I can up my score.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.The complete experiment shows the effectiveness of the method and the necessity of each component.

2.Some visual illustrations help the reader understand, although the shapes of the images seem to be compressed.

Weaknesses:
1. I am still confused about the motivation for performing semi-supervised GAD. Why do most methods emphasize unsupervised scenarios? The cost of labeling normal nodes seems too expensive, as the authors themselves state on lines 268 to 269, yet they assert again on line 31 that labels for normal nodes are easy to obtain.This inconsistency hinders a clear understanding of the necessity and practical applications of semi-supervised GAD, which significantly undermines the motivation for this work.

2. While the first loss function proposed by the authors appears intuitively valid, the second loss function aims to generate outliers similar to normal nodes. In my opinion, optimizing these two losses together is unreasonable because they conflict with each other. It seems that they should correspond to different outlier generation processes

3. The paper validates the improvement of unsupervised GAD using labeled normal nodes and claims that GGAD remains superior. I think the authors ignore the fact that unsupervised methods do not obtain this outlier like GGAD and this comparison is not reasonable.

Limitations:
No limitation need to discuss

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper explores the problem of semi-supervised graph anomaly detection (GAD), where some nodes are known to be normal, in contrast to the typical unsupervised setting with no labeled data. The authors show that even a small percentage of labeled normal nodes can improve the performance of existing unsupervised GAD methods when adapted to the semi-supervised scenario. The paper proposes a novel Generative GAD approach (GGAD) to better exploit normal nodes by generating pseudo anomaly nodes, called 'outlier nodes', to provide effective negative samples for training a one-class classifier. GGAD generates these outlier nodes using priors about anomaly nodes, such as asymmetric local affinity and egocentric closeness, to mimic anomalies in structure and features. Experiments on six real-world GAD datasets show that GGAD outperforms state-of-the-art methods in both unsupervised and semi-supervised settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ This paper studies a new problem of semi-supervised GAD that has not been widely studied. 

+ The proposed method is simple and effective from the empirical perspective.

+ The experiments are extensive including effectiveness and efficiency analyses and the method has been tested on real-world large-scale graphs to verify the scalability.

Weaknesses:
- The two priors that are used to generate outlier nodes are heuristic or based on empirical evidence. There is no theoretical analysis provided to better guarantee the effectiveness of the proposed method.

- It will be more interesting and helpful to show the generated outlier nodes can capture the characteristics of anomalous nodes in addition to comparing their representations.

- The experimental settings of anomaly contamination are not very clear: how the contamination is introduced?

- Overall experimental settings. What hardware has been used in the experiments, e.g., memory, and why are the experiments conducted on CPUs?

Limitations:
The authors have adequately addressed the limitations

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies an under-explored graph anomaly detection problem where the detection models have access to a set of labeled normal nodes. To tackle this problem, it introduces a generative approach namely GGAD that generates pseudo anomaly nodes, called outlier nodes, to support the training of a discriminative one-class classifier. The key idea underlying this approach is to generate the outlier nodes in a way that can well simulate real anomaly nodes in both graph structure and feature representation perspectives. To achieve this, GGAD defines and incorporates two priors, including asymmetric local affinity and egocentric closeness, into its optimization objectives, with the former prior focusing on the alignment on the graph structure aspect and the latter on the feature representation aspect. The method is evaluated on six large real-world datasets and shows impressive detection performance compared to existing state-of-the-art methods.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is generally well-written and easy-to-follow.
- The problem setting is practical since labeled normal samples are easy to obtain in many real-world applications. Compared to the commonly studied unsupervised setting, this semi-supervised setting often results in better detection performance.
- The proposed method GGAD is novel. There have been many generative anomaly detection methods, but as far as I know, they are unable to consider the graph structure and the neighboring nodes’ representations. By introducing the two new priors, GGAD addresses this issue well. Fig.1 and Fig. 3 help demonstrate this effect.
- The method is compared with a range of unsupervised and semi-supervised methods on 6 real-world datasets with diverse genuine anomalies, and gains largely improved detection performance over these competing methods.
- The ablation study is plausible and justifies the contribution of each proposed prior.

Weaknesses:
- The outlier node generation in GGAD may cause non-trivial computational overhead.
- Despite better performance than the competing methods, GGAD gains an AUC of only around 0.6 on some datasets, such as DGraph and Reddit.
- In Fig. 4 (b), GGAD shows a fast AUPRC growth with increasing training size, but the other methods have a flat performance trend. What would be the reason behind?

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper introduces Plan-on-Graph (PoG), a novel self-correcting adaptive planning paradigm for Knowledge Graph-augmented Large Language Models (KG-LLM). PoG aims to address limitations in existing KG-augmented LLM approaches by decomposing questions into sub-objectives and iteratively exploring reasoning paths, updating memory, and reflecting on the need for self-correction. The method incorporates three key mechanisms: Guidance, Memory, and Reflection, designed to ensure adaptive breadth in self-correcting planning for graph reasoning. Experiments on three real-world datasets demonstrate the effectiveness and efficiency of the proposed approach.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Novel approach: The paper introduces reflection and self-correction in KG path exploration for KGQA, addressing limitations of fixed path breadth and irreversible exploration in previous methods.
- Comprehensive context: The paper provides a clear and organized discussion of related work in KG-augmented LLMs, offering a good overview of the research area.
- Experimental comparison: The study includes different types of baselines in the experiments, demonstrating the improvements achieved by the proposed model in a broader context.

Weaknesses:
- Missing important implementation details: While the paper provides visualized knowledge subgraphs to represent path exploration and memory updates, there is no explicit template or example showing how this graph or tree structure is exactly flattened in the prompts of the LLM agent. The use of arrows for paths in both Figure 3 and 5 creates ambiguity about the real implementation. This makes reader unclear and concerns about the real implementation. 

- Concerns about model dependency: The proposed PoG method is a pure prompting approach, which raises questions about its applicability and performance on smaller open-source models. There are concerns about whether the performance improvements achieved by PoG are largely due to the capabilities of the advanced base models used, rather than the method itself. 


- Lack of critical evaluations: The paper is missing several important analyses, such as:
  - The proportion of cases where the LLM needs to trigger reverse direction path visiting
  - The frequency with which this revisiting leads to correct or incorrect answers
  - How the number of breadth in each step affects the effectiveness of the proposed method
  - Whether the length of hops from the query entity to the answer entity affects the number of revisiting and the method's effectiveness
  - How consistent or diverse is the exploration path under the proposed method if experiment with multiple trails?

- Minor thoughts: Additionally, it would be interesting if the authors could discuss or explore whether this method of constructing paths and memories could support effective post-training of open-source models, though this is not a critical weakness.

Limitations:
Yes, the author has discussed the limitations in Appendix G, and leave them as future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to improve KG-augmented LLMs by introducing a self-correcting adaptive planning paradigm. PoG uses three important mechanisms: Guidance, Memory, and Reflection. Guidance leverages LLM to decompose the query into subqueries; Memory stores historical retrieval and reasoning information for reflection; and Reflection uses LLM to decide whether to continue exploration or generate the final output. Experiments show the effectiveness of PoG compared to other SOTA methods and also demonstrate improved efficiency.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The design of the memory is effective, especially the Sub-Objective Status part. The status of sub-objectives contains the current known information related to the sub-objectives and can effectively remind the LLM of the known knowledge so far and the progress of exploration.
2. The experimental results are also good, showing strong improvement over existing methods.
3. PoG is training-free, which makes it suitable to serve as a plug-and-play for evolving LLMs.

Weaknesses:
1. The self-correction is restricted to the retrieval process. However, whether the final answer can be correctedly answer also depend on how the query was decomposed. If the decomposed sub-queries are not suitable for retrieval, the self-correction cannot handle this case.

2. Even though PoG improves efficiency compared with previous methods, it still requires multiple interactions with the LLM (more than 6) and incurs latency, especially for real-time QA (more than 10 seconds on all datasets as reported).

Limitations:
See weakness above

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new self-correcting adaptive planning paradigm for KG-augmented LLM named Plan-on-Graph (PoG). It has three important mechanism: Guidance, Memory and Reflection. Experiments on three knowledge graph question answering datasets show good results.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1, The paper is presented well. The four steps of the algorithm are well explained with Figure 1 and Figure 2, which makes the paper easy to understand. The authors also provide a case study to explain the new method.

2, The experiment results are good compared with other Prompting KG-Augmented LLM methods.

Weaknesses:
The weakness of this paper is from the novelty side. The proposed method PoG is like a trivial improvement on the knowledge graph reasoning task based on [Graph of Thoughs (GoT)](https://ojs.aaai.org/index.php/AAAI/article/view/29720). The path explored in this paper also forms a graph like the graph in GoT paper. And the self-reflection and evaluation are like the backtracing and refining steps in GoT paper. The proposed method just adapts these GoT steps in knowledge graph reasoning task.

Limitations:
Yes  the authors adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a self-correcting adaptive planning paradigm for KG-augmented LLM called PoG. It consists of four components: task decomposition, path exploration, memory updating, and evaluation. Experiments on three datasets demonstrate the effectiveness of PoG, outperforming previous methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The introduction of the self-correcting and adaptive planning mechanism enhances the reasoning capabilities of KG-augmented LLMs.
2.	The experimental results support the effectiveness and efficiency of PoG. The comparison with state-of-the-art baselines demonstrates its superior performance.

Weaknesses:
1.	Although the paper has achieved certain results in the experiments, it lacks sufficient analysis on why PoG can achieve better results compared to the baseline models. Additionally, for other KG-augmented LLMs such as ToG[1], ARI[2], and RoG[3], the paper does not sufficiently highlight the advantages and differences of the proposed method.
2.	There is a concern about the efficiency of the proposed method. In a KG, there may be thousands or even millions of entities. Taking Figure 2 as an example, a singer may have hundreds of songs. When the LLM completes the first step of reasoning (finding the singer's songs) and proceeds to the second step of finding the songs that have won awards, the LLM may need to be called hundreds of times? This could potentially lead to inefficiency and increased computational costs. 
3.	The paper could provide more details on the potential limitations and challenges of PoG, as well as possible strategies to overcome them.

[1] Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph
[2] Temporal knowledge question answering via abstract reasoning induction
[3] Reasoning on graphs: Faithful and interpretable large language model reasoning

Limitations:
The paper could provide more details on the potential limitations and challenges of PoG.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces Plan-on-Graph (PoG), a new paradigm for integrating LLMs with KGs to enhance their reasoning capabilities. The main innovation lies in PoG’s self-correcting adaptive planning mechanism, which addresses the limitations of existing KG-augmented LLMs that rely on predefined exploration spaces and unidirectional navigation. PoG breaks down complex questions into sub-objectives, then iteratively explores and corrects reasoning paths using a combination of Guidance, Memory, and Reflection mechanisms. The Guidance mechanism helps in decomposing the question, Memory stores historical data to support reasoning, and Reflection allows the model to self-correct erroneous paths. The authors validate PoG’s effectiveness through extensive experiments on three KGQA datasets, showing that it outperforms state-of-the-art methods in terms of both accuracy and efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- novel reasoning framework with adaptive searching and reflective thinking. 
- great performance on 3 real-world KGQA datasets validate the effectiveness and efficiency of PoG. PoG achieves the best performance across all three datasets, outperforming both LLM-only and KG-augmented LLM baselines.

Weaknesses:
- The PoG framework’s complexity, with its multiple components and iterative processes, might make it challenging to implement and optimize.

- The necessity for extensive prompt engineering and management of memory and reflection mechanisms could be resource-intensive.

- While PoG performs well on the evaluated datasets, its generalization to other domains or types of KGs is not thoroughly explored.

- Not compared with other RAG methods such as REPLUG[1]. 

- The efficiency and cost can be an important limitation. Efficiency in terms of computational resources and time is a concern, as PoG, while efficient, is still resource-intensive. The reliance on KGs means that the model’s knowledge can become outdated if the KG is not regularly updated.




[1] REPLUG: Retrieval-Augmented Black-Box Language Models

Limitations:
discussed

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper introduces Plan-on-Graph (PoG), a novel self-correcting adaptive planning paradigm for Knowledge Graph-augmented Large Language Models (KG-LLM). PoG aims to address limitations in existing KG-augmented LLM approaches by decomposing questions into sub-objectives and iteratively exploring reasoning paths, updating memory, and reflecting on the need for self-correction. The method incorporates three key mechanisms: Guidance, Memory, and Reflection, designed to ensure adaptive breadth in self-correcting planning for graph reasoning. Experiments on three real-world datasets demonstrate the effectiveness and efficiency of the proposed approach.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Novel approach: The paper introduces reflection and self-correction in KG path exploration for KGQA, addressing limitations of fixed path breadth and irreversible exploration in previous methods.
- Comprehensive context: The paper provides a clear and organized discussion of related work in KG-augmented LLMs, offering a good overview of the research area.
- Experimental comparison: The study includes different types of baselines in the experiments, demonstrating the improvements achieved by the proposed model in a broader context.

Weaknesses:
- Missing important implementation details: While the paper provides visualized knowledge subgraphs to represent path exploration and memory updates, there is no explicit template or example showing how this graph or tree structure is exactly flattened in the prompts of the LLM agent. The use of arrows for paths in both Figure 3 and 5 creates ambiguity about the real implementation. This makes reader unclear and concerns about the real implementation. 

- Concerns about model dependency: The proposed PoG method is a pure prompting approach, which raises questions about its applicability and performance on smaller open-source models. There are concerns about whether the performance improvements achieved by PoG are largely due to the capabilities of the advanced base models used, rather than the method itself. 


- Lack of critical evaluations: The paper is missing several important analyses, such as:
  - The proportion of cases where the LLM needs to trigger reverse direction path visiting
  - The frequency with which this revisiting leads to correct or incorrect answers
  - How the number of breadth in each step affects the effectiveness of the proposed method
  - Whether the length of hops from the query entity to the answer entity affects the number of revisiting and the method's effectiveness
  - How consistent or diverse is the exploration path under the proposed method if experiment with multiple trails?

- Minor thoughts: Additionally, it would be interesting if the authors could discuss or explore whether this method of constructing paths and memories could support effective post-training of open-source models, though this is not a critical weakness.

Limitations:
Yes, the author has discussed the limitations in Appendix G, and leave them as future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper aims to improve KG-augmented LLMs by introducing a self-correcting adaptive planning paradigm. PoG uses three important mechanisms: Guidance, Memory, and Reflection. Guidance leverages LLM to decompose the query into subqueries; Memory stores historical retrieval and reasoning information for reflection; and Reflection uses LLM to decide whether to continue exploration or generate the final output. Experiments show the effectiveness of PoG compared to other SOTA methods and also demonstrate improved efficiency.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The design of the memory is effective, especially the Sub-Objective Status part. The status of sub-objectives contains the current known information related to the sub-objectives and can effectively remind the LLM of the known knowledge so far and the progress of exploration.
2. The experimental results are also good, showing strong improvement over existing methods.
3. PoG is training-free, which makes it suitable to serve as a plug-and-play for evolving LLMs.

Weaknesses:
1. The self-correction is restricted to the retrieval process. However, whether the final answer can be correctedly answer also depend on how the query was decomposed. If the decomposed sub-queries are not suitable for retrieval, the self-correction cannot handle this case.

2. Even though PoG improves efficiency compared with previous methods, it still requires multiple interactions with the LLM (more than 6) and incurs latency, especially for real-time QA (more than 10 seconds on all datasets as reported).

Limitations:
See weakness above

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new self-correcting adaptive planning paradigm for KG-augmented LLM named Plan-on-Graph (PoG). It has three important mechanism: Guidance, Memory and Reflection. Experiments on three knowledge graph question answering datasets show good results.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1, The paper is presented well. The four steps of the algorithm are well explained with Figure 1 and Figure 2, which makes the paper easy to understand. The authors also provide a case study to explain the new method.

2, The experiment results are good compared with other Prompting KG-Augmented LLM methods.

Weaknesses:
The weakness of this paper is from the novelty side. The proposed method PoG is like a trivial improvement on the knowledge graph reasoning task based on [Graph of Thoughs (GoT)](https://ojs.aaai.org/index.php/AAAI/article/view/29720). The path explored in this paper also forms a graph like the graph in GoT paper. And the self-reflection and evaluation are like the backtracing and refining steps in GoT paper. The proposed method just adapts these GoT steps in knowledge graph reasoning task.

Limitations:
Yes  the authors adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a self-correcting adaptive planning paradigm for KG-augmented LLM called PoG. It consists of four components: task decomposition, path exploration, memory updating, and evaluation. Experiments on three datasets demonstrate the effectiveness of PoG, outperforming previous methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.	The introduction of the self-correcting and adaptive planning mechanism enhances the reasoning capabilities of KG-augmented LLMs.
2.	The experimental results support the effectiveness and efficiency of PoG. The comparison with state-of-the-art baselines demonstrates its superior performance.

Weaknesses:
1.	Although the paper has achieved certain results in the experiments, it lacks sufficient analysis on why PoG can achieve better results compared to the baseline models. Additionally, for other KG-augmented LLMs such as ToG[1], ARI[2], and RoG[3], the paper does not sufficiently highlight the advantages and differences of the proposed method.
2.	There is a concern about the efficiency of the proposed method. In a KG, there may be thousands or even millions of entities. Taking Figure 2 as an example, a singer may have hundreds of songs. When the LLM completes the first step of reasoning (finding the singer's songs) and proceeds to the second step of finding the songs that have won awards, the LLM may need to be called hundreds of times? This could potentially lead to inefficiency and increased computational costs. 
3.	The paper could provide more details on the potential limitations and challenges of PoG, as well as possible strategies to overcome them.

[1] Think-on-graph: Deep and responsible reasoning of large language model with knowledge graph
[2] Temporal knowledge question answering via abstract reasoning induction
[3] Reasoning on graphs: Faithful and interpretable large language model reasoning

Limitations:
The paper could provide more details on the potential limitations and challenges of PoG.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces Plan-on-Graph (PoG), a new paradigm for integrating LLMs with KGs to enhance their reasoning capabilities. The main innovation lies in PoG’s self-correcting adaptive planning mechanism, which addresses the limitations of existing KG-augmented LLMs that rely on predefined exploration spaces and unidirectional navigation. PoG breaks down complex questions into sub-objectives, then iteratively explores and corrects reasoning paths using a combination of Guidance, Memory, and Reflection mechanisms. The Guidance mechanism helps in decomposing the question, Memory stores historical data to support reasoning, and Reflection allows the model to self-correct erroneous paths. The authors validate PoG’s effectiveness through extensive experiments on three KGQA datasets, showing that it outperforms state-of-the-art methods in terms of both accuracy and efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- novel reasoning framework with adaptive searching and reflective thinking. 
- great performance on 3 real-world KGQA datasets validate the effectiveness and efficiency of PoG. PoG achieves the best performance across all three datasets, outperforming both LLM-only and KG-augmented LLM baselines.

Weaknesses:
- The PoG framework’s complexity, with its multiple components and iterative processes, might make it challenging to implement and optimize.

- The necessity for extensive prompt engineering and management of memory and reflection mechanisms could be resource-intensive.

- While PoG performs well on the evaluated datasets, its generalization to other domains or types of KGs is not thoroughly explored.

- Not compared with other RAG methods such as REPLUG[1]. 

- The efficiency and cost can be an important limitation. Efficiency in terms of computational resources and time is a concern, as PoG, while efficient, is still resource-intensive. The reliance on KGs means that the model’s knowledge can become outdated if the KG is not regularly updated.




[1] REPLUG: Retrieval-Augmented Black-Box Language Models

Limitations:
discussed

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper targets scaling down Vision Transformers (ViT) to fit environments with dynamically changing resource constraints. The authors propose Scala, a framework enabling a single network to represent multiple smaller ViTs with flexible inference capability by activating various subnets during training. Scala introduces Isolated Activation to disentangle the smallest sub-network and uses Scale Coordination to provide stable and accurate learning objectives. Empirical validations on different tasks show that Scala achieves scalable representation with one-shot training, matching the performance of Separate Training without modifying the original ViT structure. Scala demonstrates an average improvement of 1.6% on ImageNet-1K compared to previous methods, using fewer parameters.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The problem is important in practice.
2. The experimental results seem decent.

Weaknesses:
1. My major concern is that, the same aim of adapting ViTs to dynamically changing resource constraints, can also be achieved by multi-exit networks, e.g., [*1, *2, *3]. However, the paper does not discuss these highly relevant works or compare with them. Hence, I vote for rejection.
2. The method seems to lack novelty. 'smaller ViTs are intrinsically the sub-networks of a larger ViT with different widths' is not a surprising observation. The key techniques (e.g., Isolated Activation and Knowledge Distillation) are not new (naive or have been widely adopted).


[*1] Huang, Gao, et al. ""Multi-Scale Dense Networks for Resource Efficient Image Classification."" International Conference on Learning Representations. 2018.

[*2] Wang, Yulin, et al. ""Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition."" Advances in neural information processing systems 34 (2021): 11960-11973.

[*3] Han, Yizeng, et al. ""Dynamic perceiver for efficient visual recognition."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.

Limitations:
The authors have addressed the limitations and potential negative societal impacts of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents Scala, a novel framework for scalable representation learning developed from US-Net. It identifies the issues of directly applying US-Net to ViTs and proposes solutions including Isolated Activation, Scale Coordination, and Stable Sampling. These innovations enable Scala to output several sub-networks in one-shot learning. Extensive experiments on various network architectures and datasets demonstrate that the sub-networks produced by Scala consistently outperform those generated by separate training, with significantly reduced training time.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Originality: Scala addresses the limitations of US-Net and successfully applies the concept of scaling to ViT backbones. This is a significant step in the adaptation of scaling methods for more complex network architectures.

Quality: The paper supports its claims with extensive experimental results, providing strong evidence for the effectiveness of Scala.

Clarity: The paper is clearly written and well-organized, making it accessible and easy to follow.

Significance: Scala has the potential to influence future research directions in scaling ViTs.

Weaknesses:
Originality: The novelty of Scala is somewhat constrained. For instance, Noise Calibration does not show a distinct difference from standard knowledge distillation. Essentially, Scala integrates US-Net with an alternative activation for the smallest subnet and fixed scaling ratios.

Quality: The authors might consider emphasizing results from a more standard 300-epoch ViT training schedule to align with common practices in the field.

Clarity: No further issues.

Significance: The challenge of scaling ViTs with arbitrary ratios remains unresolved.

Limitations:
The novelty and significance of Scala are somewhat limited, as discussed in the weaknesses section. However, the extensive experimental results provide a robust foundation for the claims made in the paper. Overall, the work is well-executed and makes a valuable contribution to the field, justifying a recommendation for weak acceptance.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces Scala, a novel framework designed to effectively scale down Vision Transformers (ViTs) for use in environments with fluctuating resource constraints. The key insight is that smaller ViTs can function as sub-networks within a larger ViT, differing mainly in width. Scala enables a singular network architecture that can emulate multiple smaller ViTs, thereby offering versatile inference capabilities while maintaining the structural principles of ViTs. The framework uniquely incorporates multiple sub-networks during its training phase, utilizes Isolated Activation to differentiate the smallest sub-network, and implements Scale Coordination to streamline the learning objectives for each sub-network, aiming for simplicity, stability, and accuracy. The empirical results across various tasks confirm that Scala can learn scalable representations efficiently with a single training iteration, maintaining the integrity of the original ViT architecture and achieving performance on par with networks trained separately.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed Scala framework aims to enhance Vision Transformers (ViTs) by enabling them to learn scalable representations suitable for flexible inference. This is achieved through two key innovations: Isolated Activation, which effectively disentangles the representation of the smallest subnet to maintain clarity and specificity, and Scale Coordination, which ensures that each subnet within the larger network receives simplified, consistent, and accurate signals. These mechanisms are designed to optimize the performance and scalability of ViTs, addressing common challenges in adapting these architectures to varied and dynamic operational contexts.

Weaknesses:
1. Recent papers[1,2,3] with ""Scalable"" usually scale ViT to billion size with large scale datasets like DFN, JFT, and Datacomp. Therefore, I suggest authors should reconsider if the experiments can support ""Scalable"".


[1] Zhai, X., Kolesnikov, A., Houlsby, N., & Beyer, L. (2022). Scaling vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 12104-12113).

[2] El-Nouby, A., Klein, M., Zhai, S., Bautista, M. A., Toshev, A., Shankar, V., ... & Joulin, A. (2024). Scalable pre-training of large autoregressive image models. arXiv preprint arXiv:2401.08541.

[3] Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., ... & Houlsby, N. (2023, July). Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning (pp. 7480-7512). PMLR.

Limitations:
This paper addresses the limitations in conclusion

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper advances an approach for training Vision Transfomers (ViTs) such that at inference time they can be dynamically adjusted to fit different budget constraints with reduced drops of performance. To this end, the authors introduce Scala, a framework that allows a single network to encapsulate and train simultaneously multiple sub-networks of different capacities and widths. The methodological backbone of this work are the Universally slimmable networks (US-Net) [37], originally devised for CNNs. The authors identify and analyze a few flaws of US-Nets: difficulty to generalize to ViTs, small interpolation and extrapolation ability to sub-network size unseen during training, impact of sustained activation of the smallest sub-network that coupled with the sandwich rule for selecting sub-networks during training leads to an over-emphasis on it at the expense of the other sub-networks.
The authors propose two simple strategies towards such a method for ViTs: (i) Isolated activation that separates the smallest sub-network from the other sub-networks; (ii) scale coordination consisting of a set of heuristics to ensure that each sub-network gets simple, accurate and stable learning objectives: (a) progressive knowledge transfer from larger networks to smaller ones in gradual decrease of capacity, (b) stable sampling of intermediate width ratios to avoid large variations in capacities in the sandwich, (c) noise calibration, essentially a composite loss of supervised cross-entropy and distillation from the bigger sub-network.
Scala is evaluated on several settings on the ImageNet-1k dataset with ViT-Ti/S/B, hybrid CNN-ViT architectures, lightweight networks, but also for dense prediction on semantic segmentation and self-supervised pre-training with interesting results. The baselines used here were Separate Training,  Autoformer and US-Net.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
### Significance
- the paper deals with a challenging and useful task for deploying ViT models into different operational settings with different computational constraints without retraining or distilling specific architectures each time

- although a computational overhead is expected for such methods, the main components of Scala are relatively simple and make sense 

- Scala achieves good performance with a higher boost in the low parameter regime

### Originality
- the proposed contributions are somehow incremental as they are improving the US-Net prior work, but do have some novelty and they are simple.

### Clarity
- in general this work is well argued and easy to follow. The authors construct well the arguments regarding the challenges when going from CNNs to ViT with US-Net and how to construct their Scala approach.

### Quality
- the paper offers several experiments and studies in the main paper and in the appendix (longer training, fast interpolation, ablation of components) that are well thought and improve the understanding of the method.

- I appreciate the experiments beyond image classification, on semantic segmentation, as well as the self-supervised pretraining and subsequent linear probing on a downstream task.

Weaknesses:
### ""Scalable"" naming
- I think that the framing of the method as _""scalable representation learning""_ is quite confusing as it's not representative for this task, it's not a name used by other related works. Importantly, it can be easily mistaken with most works that use ""scalable"" for depicting the ability/property of a system (method, architecture) to handle a growing amount of data, parameters, and the potential to accommodate this growth. In other words ""scalable"" is rather used for depicting scaling up, whereas this work depicts the property of the proposed approach to accommodate sub-networks of different lower sizes/scales from the original.

- maybe other names us in related works would be more appropriate here: slimmable, elastic, modular, flexibile inference, etc.


### Limited baselines and related work
- some relevant related works dealing with tranformer networks are either just briefly mentioned, e.g., Matformer  [18], or not mentioned at all, e.g., SortedNet [a], Early exit [b]

- One of the main baselines, US-Net is originally designed for CNNs and, as the authors mentioned, moving to ViTs is not straightforward. Matformer is criticized for the limited number of models produced, but can be considered in the several experiments with X=4 sub-networks. Matformer and SortedNet could be included in the experimental evaluation


### Scope of experiments
- While the authors considered several settings for computer vision tasks (image classification, segmentation, light architectures), transformer architectures are also encountered in NLP (as mentioned by the authors in L56). In such cases the original models can have much more parameters and elastic inference for lower computational budgets would be of high interest.

- It would be useful to include an experiment from NLP in the style of those from Matformer or SortedNet.

- The biggest architectures used here is a ViT-B (~86M params). Extending experiments to larger modern architectures would be definitely useful and interesting.

### Clarity
- it's not always clear in the text and cost estimations that Scala needs a pre-trained full network as teacher for the distillation. This add some cost in compute and time in the end. Besides it's not clear whether US-Net also needs and uses a pre-trained teacher in the reported results.

- in the intro, the authors mention that they address the issue of minimal interpolation ability of ViTs. Results from Table 2 show that the interpolation abilities of ViTs with Scala are still very low. However the fast interpolation strategy from $\S$A.2 is actually interesting for practical settings even though not fully solving this issue. It might be worth moving up in the main paper.

- the idea of the transferability experiment ($\S$5.4) with DINOv2 is nice. From the description it is not clear whether DINOv2 was used as teacher for the distillation or also as supervised pre-training on ImageNet-1k? Or the pre-training on ImageNet-1K was done in a supervised manner as in previous experiments?

- the ablation experiment from Table 6 is nice. However the presentation with removing one component at once offers only a partial understanding of the contributions of each module. Different configurations with different modules in on/off mode should give a better global understanding.



**References:**

[a] Valipour et al., SortedNet: A Scalable and Generalized Framework for Training Modular Deep Neural Networks, arXiv 2023

[b] Xin et al., Deebert: Dynamic early exiting for accelerating bert inference, ACL 2020

Limitations:
The authors addressed some of the limitations in the conclusion section.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper targets scaling down Vision Transformers (ViT) to fit environments with dynamically changing resource constraints. The authors propose Scala, a framework enabling a single network to represent multiple smaller ViTs with flexible inference capability by activating various subnets during training. Scala introduces Isolated Activation to disentangle the smallest sub-network and uses Scale Coordination to provide stable and accurate learning objectives. Empirical validations on different tasks show that Scala achieves scalable representation with one-shot training, matching the performance of Separate Training without modifying the original ViT structure. Scala demonstrates an average improvement of 1.6% on ImageNet-1K compared to previous methods, using fewer parameters.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The problem is important in practice.
2. The experimental results seem decent.

Weaknesses:
1. My major concern is that, the same aim of adapting ViTs to dynamically changing resource constraints, can also be achieved by multi-exit networks, e.g., [*1, *2, *3]. However, the paper does not discuss these highly relevant works or compare with them. Hence, I vote for rejection.
2. The method seems to lack novelty. 'smaller ViTs are intrinsically the sub-networks of a larger ViT with different widths' is not a surprising observation. The key techniques (e.g., Isolated Activation and Knowledge Distillation) are not new (naive or have been widely adopted).


[*1] Huang, Gao, et al. ""Multi-Scale Dense Networks for Resource Efficient Image Classification."" International Conference on Learning Representations. 2018.

[*2] Wang, Yulin, et al. ""Not all images are worth 16x16 words: Dynamic transformers for efficient image recognition."" Advances in neural information processing systems 34 (2021): 11960-11973.

[*3] Han, Yizeng, et al. ""Dynamic perceiver for efficient visual recognition."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.

Limitations:
The authors have addressed the limitations and potential negative societal impacts of their work.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents Scala, a novel framework for scalable representation learning developed from US-Net. It identifies the issues of directly applying US-Net to ViTs and proposes solutions including Isolated Activation, Scale Coordination, and Stable Sampling. These innovations enable Scala to output several sub-networks in one-shot learning. Extensive experiments on various network architectures and datasets demonstrate that the sub-networks produced by Scala consistently outperform those generated by separate training, with significantly reduced training time.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Originality: Scala addresses the limitations of US-Net and successfully applies the concept of scaling to ViT backbones. This is a significant step in the adaptation of scaling methods for more complex network architectures.

Quality: The paper supports its claims with extensive experimental results, providing strong evidence for the effectiveness of Scala.

Clarity: The paper is clearly written and well-organized, making it accessible and easy to follow.

Significance: Scala has the potential to influence future research directions in scaling ViTs.

Weaknesses:
Originality: The novelty of Scala is somewhat constrained. For instance, Noise Calibration does not show a distinct difference from standard knowledge distillation. Essentially, Scala integrates US-Net with an alternative activation for the smallest subnet and fixed scaling ratios.

Quality: The authors might consider emphasizing results from a more standard 300-epoch ViT training schedule to align with common practices in the field.

Clarity: No further issues.

Significance: The challenge of scaling ViTs with arbitrary ratios remains unresolved.

Limitations:
The novelty and significance of Scala are somewhat limited, as discussed in the weaknesses section. However, the extensive experimental results provide a robust foundation for the claims made in the paper. Overall, the work is well-executed and makes a valuable contribution to the field, justifying a recommendation for weak acceptance.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces Scala, a novel framework designed to effectively scale down Vision Transformers (ViTs) for use in environments with fluctuating resource constraints. The key insight is that smaller ViTs can function as sub-networks within a larger ViT, differing mainly in width. Scala enables a singular network architecture that can emulate multiple smaller ViTs, thereby offering versatile inference capabilities while maintaining the structural principles of ViTs. The framework uniquely incorporates multiple sub-networks during its training phase, utilizes Isolated Activation to differentiate the smallest sub-network, and implements Scale Coordination to streamline the learning objectives for each sub-network, aiming for simplicity, stability, and accuracy. The empirical results across various tasks confirm that Scala can learn scalable representations efficiently with a single training iteration, maintaining the integrity of the original ViT architecture and achieving performance on par with networks trained separately.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposed Scala framework aims to enhance Vision Transformers (ViTs) by enabling them to learn scalable representations suitable for flexible inference. This is achieved through two key innovations: Isolated Activation, which effectively disentangles the representation of the smallest subnet to maintain clarity and specificity, and Scale Coordination, which ensures that each subnet within the larger network receives simplified, consistent, and accurate signals. These mechanisms are designed to optimize the performance and scalability of ViTs, addressing common challenges in adapting these architectures to varied and dynamic operational contexts.

Weaknesses:
1. Recent papers[1,2,3] with ""Scalable"" usually scale ViT to billion size with large scale datasets like DFN, JFT, and Datacomp. Therefore, I suggest authors should reconsider if the experiments can support ""Scalable"".


[1] Zhai, X., Kolesnikov, A., Houlsby, N., & Beyer, L. (2022). Scaling vision transformers. In Proceedings of the IEEE/CVF conference on computer vision and pattern recognition (pp. 12104-12113).

[2] El-Nouby, A., Klein, M., Zhai, S., Bautista, M. A., Toshev, A., Shankar, V., ... & Joulin, A. (2024). Scalable pre-training of large autoregressive image models. arXiv preprint arXiv:2401.08541.

[3] Dehghani, M., Djolonga, J., Mustafa, B., Padlewski, P., Heek, J., Gilmer, J., ... & Houlsby, N. (2023, July). Scaling vision transformers to 22 billion parameters. In International Conference on Machine Learning (pp. 7480-7512). PMLR.

Limitations:
This paper addresses the limitations in conclusion

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper advances an approach for training Vision Transfomers (ViTs) such that at inference time they can be dynamically adjusted to fit different budget constraints with reduced drops of performance. To this end, the authors introduce Scala, a framework that allows a single network to encapsulate and train simultaneously multiple sub-networks of different capacities and widths. The methodological backbone of this work are the Universally slimmable networks (US-Net) [37], originally devised for CNNs. The authors identify and analyze a few flaws of US-Nets: difficulty to generalize to ViTs, small interpolation and extrapolation ability to sub-network size unseen during training, impact of sustained activation of the smallest sub-network that coupled with the sandwich rule for selecting sub-networks during training leads to an over-emphasis on it at the expense of the other sub-networks.
The authors propose two simple strategies towards such a method for ViTs: (i) Isolated activation that separates the smallest sub-network from the other sub-networks; (ii) scale coordination consisting of a set of heuristics to ensure that each sub-network gets simple, accurate and stable learning objectives: (a) progressive knowledge transfer from larger networks to smaller ones in gradual decrease of capacity, (b) stable sampling of intermediate width ratios to avoid large variations in capacities in the sandwich, (c) noise calibration, essentially a composite loss of supervised cross-entropy and distillation from the bigger sub-network.
Scala is evaluated on several settings on the ImageNet-1k dataset with ViT-Ti/S/B, hybrid CNN-ViT architectures, lightweight networks, but also for dense prediction on semantic segmentation and self-supervised pre-training with interesting results. The baselines used here were Separate Training,  Autoformer and US-Net.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
### Significance
- the paper deals with a challenging and useful task for deploying ViT models into different operational settings with different computational constraints without retraining or distilling specific architectures each time

- although a computational overhead is expected for such methods, the main components of Scala are relatively simple and make sense 

- Scala achieves good performance with a higher boost in the low parameter regime

### Originality
- the proposed contributions are somehow incremental as they are improving the US-Net prior work, but do have some novelty and they are simple.

### Clarity
- in general this work is well argued and easy to follow. The authors construct well the arguments regarding the challenges when going from CNNs to ViT with US-Net and how to construct their Scala approach.

### Quality
- the paper offers several experiments and studies in the main paper and in the appendix (longer training, fast interpolation, ablation of components) that are well thought and improve the understanding of the method.

- I appreciate the experiments beyond image classification, on semantic segmentation, as well as the self-supervised pretraining and subsequent linear probing on a downstream task.

Weaknesses:
### ""Scalable"" naming
- I think that the framing of the method as _""scalable representation learning""_ is quite confusing as it's not representative for this task, it's not a name used by other related works. Importantly, it can be easily mistaken with most works that use ""scalable"" for depicting the ability/property of a system (method, architecture) to handle a growing amount of data, parameters, and the potential to accommodate this growth. In other words ""scalable"" is rather used for depicting scaling up, whereas this work depicts the property of the proposed approach to accommodate sub-networks of different lower sizes/scales from the original.

- maybe other names us in related works would be more appropriate here: slimmable, elastic, modular, flexibile inference, etc.


### Limited baselines and related work
- some relevant related works dealing with tranformer networks are either just briefly mentioned, e.g., Matformer  [18], or not mentioned at all, e.g., SortedNet [a], Early exit [b]

- One of the main baselines, US-Net is originally designed for CNNs and, as the authors mentioned, moving to ViTs is not straightforward. Matformer is criticized for the limited number of models produced, but can be considered in the several experiments with X=4 sub-networks. Matformer and SortedNet could be included in the experimental evaluation


### Scope of experiments
- While the authors considered several settings for computer vision tasks (image classification, segmentation, light architectures), transformer architectures are also encountered in NLP (as mentioned by the authors in L56). In such cases the original models can have much more parameters and elastic inference for lower computational budgets would be of high interest.

- It would be useful to include an experiment from NLP in the style of those from Matformer or SortedNet.

- The biggest architectures used here is a ViT-B (~86M params). Extending experiments to larger modern architectures would be definitely useful and interesting.

### Clarity
- it's not always clear in the text and cost estimations that Scala needs a pre-trained full network as teacher for the distillation. This add some cost in compute and time in the end. Besides it's not clear whether US-Net also needs and uses a pre-trained teacher in the reported results.

- in the intro, the authors mention that they address the issue of minimal interpolation ability of ViTs. Results from Table 2 show that the interpolation abilities of ViTs with Scala are still very low. However the fast interpolation strategy from $\S$A.2 is actually interesting for practical settings even though not fully solving this issue. It might be worth moving up in the main paper.

- the idea of the transferability experiment ($\S$5.4) with DINOv2 is nice. From the description it is not clear whether DINOv2 was used as teacher for the distillation or also as supervised pre-training on ImageNet-1k? Or the pre-training on ImageNet-1K was done in a supervised manner as in previous experiments?

- the ablation experiment from Table 6 is nice. However the presentation with removing one component at once offers only a partial understanding of the contributions of each module. Different configurations with different modules in on/off mode should give a better global understanding.



**References:**

[a] Valipour et al., SortedNet: A Scalable and Generalized Framework for Training Modular Deep Neural Networks, arXiv 2023

[b] Xin et al., Deebert: Dynamic early exiting for accelerating bert inference, ACL 2020

Limitations:
The authors addressed some of the limitations in the conclusion section.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposes an improved approach to multi-label learning using $\mathcal{H}$-consistency bounds by introducing the multi-label logistic loss to effectively handle label correlations. It extends to various multi-label losses, ensuring Bayes-consistency across diverse settings, and includes efficient gradient computation algorithms for minimizing the proposed loss function. This work offers a unified framework with robust consistency guarantees, advancing beyond traditional methods in multi-label learning.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Introducing the multi-label logistic loss, which effectively addresses label correlations often overlooked by traditional binary relevance surrogates under Hamming loss.

- The paper establishes $\mathcal{H}$-consistency bounds for a wide range of multi-label losses, ensuring Bayes-consistency across diverse multi-label learning scenarios. This extends beyond previous research that primarily focused on specific loss functions.

- It offers a unified framework that accommodates various multi-label losses, including novel extensions and adaptations from standard classification. This is supported by efficient gradient computation algorithms specifically designed for minimizing the proposed multi-label logistic loss.

Weaknesses:
- The motivation and background of this paper lack clear logic and hierarchy. It is suggested to first outline the shortcomings of existing methods and then clearly present the research questions addressed in this paper.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper explores surrogate losses and algorithms for multi-label learning, focusing on \( \mathcal{H} \)-consistency bounds. It identifies the limitations of Hamming loss and introduces a new multi-label logistic loss that accounts for label correlations. The study extends this to a broader family of multi-label losses and adapts comp-sum losses from standard classification to multi-label learning. The authors propose a unified framework providing strong consistency guarantees for multi-label losses and describe efficient gradient computation methods for minimizing these losses.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors conduct a detailed analysis of the popular Hamming loss in multi-label learning when using smooth losses. They identify its sub-optimal dependency on the number of labels and its failure to account for label correlations, providing valuable insights into the limitations of existing loss functions. 
1. The authors introduce an improvement by presenting a novel surrogate loss, the multi-label logistic loss, which accounts for label correlations and benefits from label-independent \( \mathcal{H} \)-consistency bounds. This innovation addresses the identified drawbacks of existing loss functions and broadens the analysis to include a more extensive family of multi-label losses, including a new extension based on linear-fractional functions related to the confusion matrix.
1. The authors extend their work by adapting multi-label logistic losses to more comprehensive multi-label comp-sum losses. By demonstrating that this family of surrogate losses benefits from \( \mathcal{H} \)-consistency bounds and Bayes-consistency across any general multi-label loss, they propose a unified surrogate loss framework. This expands upon previous work that only established consistency for specific loss functions, showcasing the applicability of their approach.
1. The authors' writing is clear and well-structured, with each theoretical assumption and conclusion articulated distinctly.

Weaknesses:
1. In section 4, although the excellent properties of the proposed multi-label logistic loss are proven, providing a detailed explanation of each component of this loss would further enhance the reader's understanding of its superiority.
2. If the advantages of this loss could be demonstrated through experimental validation, it would be more intuitive for readers.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study surrogate losses and algorithms for multi-label learning via H-consistency bounds and introduce a novel surrogate loss, multi-label logistic loss in this paper. By broadening the H-consistency bounds analyses to more general multi-label losses and extending to multi-label comp-sum losses, the authors provide a unified surrogate loss framework for H-consistency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy to follow.
2. The authors make comprehensive reviews of related works, including their pros and cons.
3. The authors provide rigorous theoretical analyses of the limitations of existing binary relevance loss, the H-consistency of the proposed multi-label logistic loss, and the extensions to more general multi-label losses. The theoretical contribution is important for multi-label learning.
4. The authors demonstrate the efficient computation of the gradient for the proposed multi-label logistic loss and conduct time complexity analyses.

Weaknesses:
1. I understand that this is a theoretical work, and experiments of empirical evaluations are not its focus. However, adding experiments to compare the proposed loss with commonly used multi-label losses on standard datasets would make the paper more comprehensive and appealing. Besides, it can also verify whether the proposed loss is effective in practice.
2. There is a typo in line 300.($1-\bar{L}_{ham}(\cdot, y)$).

Limitations:
Yes, the authors have adequately addressed the limitations.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper derives H-consistency bounds for binary-relevance style surrogate losses, as well as a new surrogate, for mutli-label learning problems, showing that the proposed multi-label logistic loss whose upper-bound on the Hamming loss is independent of the number of labels.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The $H$-consistency bounds provided in the paper are more informative than existing Bayes-consistency results, as they hold not just in the infinite limit.

The novel multi-label logistic loss allows upper-bounds that do not depend on the number of labels.

Weaknesses:
The paper does not provide any experiments. While this is OK for a theory paper, it does mean that the question of whether the new surrogate works better in practice remains unanswered (which should be reflected in the conclusion section, at least), for two reasons:
a) all the theory provides are upper-bounds, which might not be indicative of actual performance
b) while the theory provides better guarantees for the task loss if the surrogate is reduced to the level $\epsilon$, it might be that reducing the new surrogate is just much more difficult than optimizing binary relevance. In particular, if the computational cost for reducing the multi-label logistic loss to the same level $\epsilon$ as binary relevance is larger by at least $\sqrt{l}$, then, normalized for compute, the advantage of the new surrogate vanishes.

It is claimed that the gradient of the multi-label logistic loss can be computed efficiently, yet the presented formulas still contain sums over the entire $2^l$ entries of the label space. Even if they can be precomputed once, already at moderate label space sizes of l ~ 100 would these quantities be intractable.

It is annoying that most equations are unnumbered. Even if they are not referred to in the paper, your readers and reviewers might want to reference them.

the equation after l. 328 switches between $\mathbf{\mathsf{y}}'$ and $y'$; and $y''$ changes to $y$

l. 114: I'm not sure what the point here is of introducing the threshold $t$, if it is set to $0$ in the same sentence? Couldn't $t$ be simply absorbed into $h$?

l. 178-180; 208: Arguably, completeness does _not_ hold in practice, because there is some form of upper-bound (e.g., weights representable in the given floating-point format)

l. 231. Binary relevance is not just Bayes-consistent w.r.t. the Hamming-loss, but also works for precision-at-$k$

In the equation after line 542, I think $\bar{L}$ should be $\bar{L}_\mathrm{ham}$? 

l. 503: I think $q$ should be $q_i$, and there is a weird subscript on that line.

l. 174 consist -> consisting

Limitations:
I'm not sure if the proposed surrogate actually is tractable for label spaces with more than 50 labels.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposes an improved approach to multi-label learning using $\mathcal{H}$-consistency bounds by introducing the multi-label logistic loss to effectively handle label correlations. It extends to various multi-label losses, ensuring Bayes-consistency across diverse settings, and includes efficient gradient computation algorithms for minimizing the proposed loss function. This work offers a unified framework with robust consistency guarantees, advancing beyond traditional methods in multi-label learning.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Introducing the multi-label logistic loss, which effectively addresses label correlations often overlooked by traditional binary relevance surrogates under Hamming loss.

- The paper establishes $\mathcal{H}$-consistency bounds for a wide range of multi-label losses, ensuring Bayes-consistency across diverse multi-label learning scenarios. This extends beyond previous research that primarily focused on specific loss functions.

- It offers a unified framework that accommodates various multi-label losses, including novel extensions and adaptations from standard classification. This is supported by efficient gradient computation algorithms specifically designed for minimizing the proposed multi-label logistic loss.

Weaknesses:
- The motivation and background of this paper lack clear logic and hierarchy. It is suggested to first outline the shortcomings of existing methods and then clearly present the research questions addressed in this paper.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper explores surrogate losses and algorithms for multi-label learning, focusing on \( \mathcal{H} \)-consistency bounds. It identifies the limitations of Hamming loss and introduces a new multi-label logistic loss that accounts for label correlations. The study extends this to a broader family of multi-label losses and adapts comp-sum losses from standard classification to multi-label learning. The authors propose a unified framework providing strong consistency guarantees for multi-label losses and describe efficient gradient computation methods for minimizing these losses.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors conduct a detailed analysis of the popular Hamming loss in multi-label learning when using smooth losses. They identify its sub-optimal dependency on the number of labels and its failure to account for label correlations, providing valuable insights into the limitations of existing loss functions. 
1. The authors introduce an improvement by presenting a novel surrogate loss, the multi-label logistic loss, which accounts for label correlations and benefits from label-independent \( \mathcal{H} \)-consistency bounds. This innovation addresses the identified drawbacks of existing loss functions and broadens the analysis to include a more extensive family of multi-label losses, including a new extension based on linear-fractional functions related to the confusion matrix.
1. The authors extend their work by adapting multi-label logistic losses to more comprehensive multi-label comp-sum losses. By demonstrating that this family of surrogate losses benefits from \( \mathcal{H} \)-consistency bounds and Bayes-consistency across any general multi-label loss, they propose a unified surrogate loss framework. This expands upon previous work that only established consistency for specific loss functions, showcasing the applicability of their approach.
1. The authors' writing is clear and well-structured, with each theoretical assumption and conclusion articulated distinctly.

Weaknesses:
1. In section 4, although the excellent properties of the proposed multi-label logistic loss are proven, providing a detailed explanation of each component of this loss would further enhance the reader's understanding of its superiority.
2. If the advantages of this loss could be demonstrated through experimental validation, it would be more intuitive for readers.

Limitations:
The authors have adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study surrogate losses and algorithms for multi-label learning via H-consistency bounds and introduce a novel surrogate loss, multi-label logistic loss in this paper. By broadening the H-consistency bounds analyses to more general multi-label losses and extending to multi-label comp-sum losses, the authors provide a unified surrogate loss framework for H-consistency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy to follow.
2. The authors make comprehensive reviews of related works, including their pros and cons.
3. The authors provide rigorous theoretical analyses of the limitations of existing binary relevance loss, the H-consistency of the proposed multi-label logistic loss, and the extensions to more general multi-label losses. The theoretical contribution is important for multi-label learning.
4. The authors demonstrate the efficient computation of the gradient for the proposed multi-label logistic loss and conduct time complexity analyses.

Weaknesses:
1. I understand that this is a theoretical work, and experiments of empirical evaluations are not its focus. However, adding experiments to compare the proposed loss with commonly used multi-label losses on standard datasets would make the paper more comprehensive and appealing. Besides, it can also verify whether the proposed loss is effective in practice.
2. There is a typo in line 300.($1-\bar{L}_{ham}(\cdot, y)$).

Limitations:
Yes, the authors have adequately addressed the limitations.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper derives H-consistency bounds for binary-relevance style surrogate losses, as well as a new surrogate, for mutli-label learning problems, showing that the proposed multi-label logistic loss whose upper-bound on the Hamming loss is independent of the number of labels.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The $H$-consistency bounds provided in the paper are more informative than existing Bayes-consistency results, as they hold not just in the infinite limit.

The novel multi-label logistic loss allows upper-bounds that do not depend on the number of labels.

Weaknesses:
The paper does not provide any experiments. While this is OK for a theory paper, it does mean that the question of whether the new surrogate works better in practice remains unanswered (which should be reflected in the conclusion section, at least), for two reasons:
a) all the theory provides are upper-bounds, which might not be indicative of actual performance
b) while the theory provides better guarantees for the task loss if the surrogate is reduced to the level $\epsilon$, it might be that reducing the new surrogate is just much more difficult than optimizing binary relevance. In particular, if the computational cost for reducing the multi-label logistic loss to the same level $\epsilon$ as binary relevance is larger by at least $\sqrt{l}$, then, normalized for compute, the advantage of the new surrogate vanishes.

It is claimed that the gradient of the multi-label logistic loss can be computed efficiently, yet the presented formulas still contain sums over the entire $2^l$ entries of the label space. Even if they can be precomputed once, already at moderate label space sizes of l ~ 100 would these quantities be intractable.

It is annoying that most equations are unnumbered. Even if they are not referred to in the paper, your readers and reviewers might want to reference them.

the equation after l. 328 switches between $\mathbf{\mathsf{y}}'$ and $y'$; and $y''$ changes to $y$

l. 114: I'm not sure what the point here is of introducing the threshold $t$, if it is set to $0$ in the same sentence? Couldn't $t$ be simply absorbed into $h$?

l. 178-180; 208: Arguably, completeness does _not_ hold in practice, because there is some form of upper-bound (e.g., weights representable in the given floating-point format)

l. 231. Binary relevance is not just Bayes-consistent w.r.t. the Hamming-loss, but also works for precision-at-$k$

In the equation after line 542, I think $\bar{L}$ should be $\bar{L}_\mathrm{ham}$? 

l. 503: I think $q$ should be $q_i$, and there is a weird subscript on that line.

l. 174 consist -> consisting

Limitations:
I'm not sure if the proposed surrogate actually is tractable for label spaces with more than 50 labels.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This work presents a method called MapUnveiler, which aims to improve the construction of vectorized HD maps for autonomous driving. MapUnveiler uses a novel clip-level pipeline to unveil occluded map elements by relating dense image representations with efficient clip tokens and propagating inter-clip information. This approach leverages temporal information across adjacent input frames, addressing the limitations of single-frame and streaming inference methods. The model achieves state-of-the-art performance on the nuScenes and Argoverse2 benchmark datasets, demonstrating promising improvements in challenging scenarios with longer perception ranges and heavy occlusions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The introduction of a clip-level pipeline for vectorized HD map construction effectively addresses occlusion issues and leverages temporal information across multiple frames.
2. The method utilizes clip tokens to propagate map information efficiently, reducing redundant computations and enhancing prediction consistency.
3. Extensive experiments demonstrate that MapUnveiler achieves state-of-the-art performance on nuScenes and Argoverse2 benchmarks, particularly in challenging scenarios.

Weaknesses:
1. The community has noticed a severe data leakage issue with utilizing nuScenes and Argoverse2 datasets for online mapping evaluation {1, 2}, as these datasets are not intentionally built for online mapping. It might also be necessary to validate the proposed method on geo-disjoint training and validation sets.
2. It would be good to see the analysis of added model compacity due to the introduction of the proposed intra-clip unveiler and inter-clip unveiler.
3. It seems the proposed intra-clip unveiler and inter-clip unveiler are adaptable to any single-frame inference online mapping methods. It would be good to validate the effectiveness of the proposed modules on other baseline methods.
4. The authors are encouraged to investigate the consistency of estimated HD maps across frames of the proposed method compared to existing methods with ""inconsistent and suboptimal prediction results"" (mentioned in Line 7).
{1} Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps.
{2} Localization Is All You Evaluate: Data Leakage in Online Mapping Datasets and How to Fix It.

Limitations:
The limitation of dependency on temporally consecutive frames is discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper aims to improve vectorized HD map construction for autonomous driving. Inspired by the global feature association in traditional offline HD mapping, the proposed MapUnveiler processes input frames in a clip-based manner and hopes to resolve occlusions using information from previous frames. Built up MapTRv2, MapUnveiler introduces clip tokens together with the Inter-clip and Intra-clip Unveiler modules to update the map queries with temporal information. Experiments on nuScenes and Argoverse2 datasets demonstrate the superior performance of the proposed method, especially on highly-occluded scenes.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The idea of incorporating and aggregating clip-level information for online vectorized HD mapping is reasonable and is more akin to how humans drive. The proposed method has more thoughtful designs than early works such as StreamMapNet to better handle occlusions and incorporate long-range information. 

2. The proposed MapUnveiler obtains state-of-the-art results in various experimental settings. The improvements over previous methods are especially prominent in the large 100mx50m setting and the highly-occluded scenes collected by the authors. 

3. Extensive ablation studies enumerate the choices of almost all hyper-parameters or model components, which helps better understand and break down each element's contributions.

Weaknesses:
1. The clarity of the method description is poor, making it very hard to thoroughly understand the proposed architecture. Details are discussed below:
    
    - The method explanation is not self-contained:  i) The Inter-clip Unveiler section refers to the TTM and directly skips all details. There is no information at all about how is the compact memory token generated from the denser map queries;  ii) The ""loss"" section refers to MapTRv2 and again skips all details. The authors should not assume the general audience to be aware of the concrete details of TTM and MapTRv2. The core formulation of these components should be elaborated with texts or equations, while full details can go to the appendix.
    - The definitions of the temporal window T and the stride S are unclear. Based on the text descriptions and the common definition of stride, my understanding of ""T=3 and S =2"" is that ""each clip has 3 frames, and every two consecutive frames have a temporal gap of 1."" However, the symbols in L177-178 seem to suggest other meanings of T and S.
    - The description of the inference mechanism is also vague. Is the MapUnveiler executed per frame or per clip? Figure 2 seems to suggest the per-clip inference where the predictions of T frames are obtained together. If this is the case, does it hurt the actual response frequency? 
    
     In short, Section 3 of the paper lacks significant details, and I cannot properly understand MapUnveiler's exact formulation. Given that the authors answer ""No"" to Question 5 of the Checklist, I have to raise concerns about the paper's reproducibility.

2. There is no detail on how the pre-training and fine-tuning are conducted. Do you initialize the MapNet by training MapTRv2? If this is the case, how are the training epochs split for the MapNet pre-training and the end-to-end MapUnveiler fine-tuning?  If the 24/6 epochs for nuScenes/Argo2 are only for the fine-tuning stage, then the comparisons in the main table are unfair, as other methods in the table have not fully converged. 

3. The main comparison results are incomplete. Most previous papers provide the nuScenes results of both short and long training schedules, but the main table only presents short-schedule results. Considering the last question about the pre-training and fine-tuning, the authors should complement the table with long-schedule results to show that MapUnveiler can obtain consistent performance boosts when all the methods are fully converged. This concern is backed up by the fact that MapUnveiler's improvement is much smaller on Argo2 compared to nuScenes -- based on my empirical experience, previous methods like MapTRv2 and its followups converge faster on Argo2, and training for 6 epochs is close to convergence. This probably suggests that the large performance gaps on nuScenes come from unfair training settings. 

4. Your interpretation of StreamMapNet and SQD-MapNet's Argo2 training epochs is wrong. These two methods employ a different frame sampling strategy at training time compared to MapTRv2, but their effective number of training samples is the same as MapTRv2. Therefore, the claim about the ""longer training schedules"" in the main table's caption is misleading.

5. The venues in the main table are not accurate. HIMap[49] and MGMap[24] are accepted by CVPR2024, and the information was already available at the time of NeurIPS submission. Furthermore, a recent HD map construction method, MapTracker[A], also studies temporal modeling and should be very relevant, but it is missing in the discussion and related works.   

    [A] MapTracker: Tracking with Strided Memory Fusion for Consistent Vector HD Mapping, arXiv:2403.15951

Limitations:
The limitations and broader impacts are adequately discussed in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors propose a new approach for constructing vectorized high-definition maps that exploits temporal information across adjacent input frames.  The model, which they call MapUnveiler, operates at the clip-level and consists of an intra-clip unveiler which generates vectorized maps for T frames and an inter-clip unveiler which uses a memory module to aggregate information between clips. The authors present results on two standard benchmarks, vectorized HD map construction benchmarks (nuScenes and Argoverse2) and demonstrate the model’s superior quantitive performance to several previously proposed approaches. They also show several qualitative examples of how MapUnveiler can better handle occlusions in the input images.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The paper is well-written and contextualized well within prior work.
- The methodology is novel and well-motivated.
- The results are strong on the two tested datasets, both quantitatively and qualitatively.
- Many different analyses and ablations were included to justify the design decisions used within MapUnveiler and show its strengths.

Weaknesses:
1. The methods is dense and a bit hard to read. The architecture figures help but are also a bit difficult to parse through. It would be helpful to try to weave more intuition into the text.
2. Claiming ""-9.8%"" is significant but ""-6.0%"" is comparable in the robustness to occlusion section seems a bit arbitrary (and potentially overstating MapUnveiler's performance, as a 6% drop is still considerable). I suggest the authors rephrase this sentence (and address similar claims in the paper).

There are several typos throughout the paper. I have enumerated some here, but encourage the authors to do a detailed proofread:
- 127: With there
- 129: mapnet -> MapNet
- 161: bev -> BEV
- 167 parenthesis 
- 192 backwards parenthesis 
- 294: In addition, if we choose too short

Limitations:
Only one limitation is included. I encourage the authors to think through other potential limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a clip-based vectorized HD map construction paradigm for the processing of long temporal sequence, in which occluded map elements are unveiled explicitly by efficient clip tokens. Through clip token propagation, MapUnveiler achieves effective utilization of long-term temporal map information by associating inter-clip information, in which clip tokens are propagated rather than dense BEV features. Experiments demonstrate that MapUnveiler boosts the performance on public benchmark datasets, also for more challenging setting like long-range perception and heavily occluded driving scenes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy-to-follow. Figures clearly conveys the intended message.
2. “Unveiling the hidden” and clip token propagation are reasonable and effective strategy for static Map element detection, which is practical and alleviates the problem to some extent.
3. The proposed method demonstrates strong performance on benchmark dataset, comprehensive experiments and ablation studies justify the model design.

Weaknesses:
1. As mentioned at line 227, this work is built on pretrained frame-level MapTRV2 and fine-tuned, thus the comparison can be unfair. Results without pretraining are required to verify your effectiveness.
2. At line 53 and BEV Updater in line 151, for occluded features, how to select the tokens that are visible in certain frames? Seems tokens within the temporal window are fully utilized for BEV update by cross attention, how to determine whether these tokens contain unblocked information? More explanations are required.

Limitations:
Yes. The authors mentioned the weakness of their approach on the corrupted input.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This work presents a method called MapUnveiler, which aims to improve the construction of vectorized HD maps for autonomous driving. MapUnveiler uses a novel clip-level pipeline to unveil occluded map elements by relating dense image representations with efficient clip tokens and propagating inter-clip information. This approach leverages temporal information across adjacent input frames, addressing the limitations of single-frame and streaming inference methods. The model achieves state-of-the-art performance on the nuScenes and Argoverse2 benchmark datasets, demonstrating promising improvements in challenging scenarios with longer perception ranges and heavy occlusions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The introduction of a clip-level pipeline for vectorized HD map construction effectively addresses occlusion issues and leverages temporal information across multiple frames.
2. The method utilizes clip tokens to propagate map information efficiently, reducing redundant computations and enhancing prediction consistency.
3. Extensive experiments demonstrate that MapUnveiler achieves state-of-the-art performance on nuScenes and Argoverse2 benchmarks, particularly in challenging scenarios.

Weaknesses:
1. The community has noticed a severe data leakage issue with utilizing nuScenes and Argoverse2 datasets for online mapping evaluation {1, 2}, as these datasets are not intentionally built for online mapping. It might also be necessary to validate the proposed method on geo-disjoint training and validation sets.
2. It would be good to see the analysis of added model compacity due to the introduction of the proposed intra-clip unveiler and inter-clip unveiler.
3. It seems the proposed intra-clip unveiler and inter-clip unveiler are adaptable to any single-frame inference online mapping methods. It would be good to validate the effectiveness of the proposed modules on other baseline methods.
4. The authors are encouraged to investigate the consistency of estimated HD maps across frames of the proposed method compared to existing methods with ""inconsistent and suboptimal prediction results"" (mentioned in Line 7).
{1} Augmenting Lane Perception and Topology Understanding with Standard Definition Navigation Maps.
{2} Localization Is All You Evaluate: Data Leakage in Online Mapping Datasets and How to Fix It.

Limitations:
The limitation of dependency on temporally consecutive frames is discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper aims to improve vectorized HD map construction for autonomous driving. Inspired by the global feature association in traditional offline HD mapping, the proposed MapUnveiler processes input frames in a clip-based manner and hopes to resolve occlusions using information from previous frames. Built up MapTRv2, MapUnveiler introduces clip tokens together with the Inter-clip and Intra-clip Unveiler modules to update the map queries with temporal information. Experiments on nuScenes and Argoverse2 datasets demonstrate the superior performance of the proposed method, especially on highly-occluded scenes.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The idea of incorporating and aggregating clip-level information for online vectorized HD mapping is reasonable and is more akin to how humans drive. The proposed method has more thoughtful designs than early works such as StreamMapNet to better handle occlusions and incorporate long-range information. 

2. The proposed MapUnveiler obtains state-of-the-art results in various experimental settings. The improvements over previous methods are especially prominent in the large 100mx50m setting and the highly-occluded scenes collected by the authors. 

3. Extensive ablation studies enumerate the choices of almost all hyper-parameters or model components, which helps better understand and break down each element's contributions.

Weaknesses:
1. The clarity of the method description is poor, making it very hard to thoroughly understand the proposed architecture. Details are discussed below:
    
    - The method explanation is not self-contained:  i) The Inter-clip Unveiler section refers to the TTM and directly skips all details. There is no information at all about how is the compact memory token generated from the denser map queries;  ii) The ""loss"" section refers to MapTRv2 and again skips all details. The authors should not assume the general audience to be aware of the concrete details of TTM and MapTRv2. The core formulation of these components should be elaborated with texts or equations, while full details can go to the appendix.
    - The definitions of the temporal window T and the stride S are unclear. Based on the text descriptions and the common definition of stride, my understanding of ""T=3 and S =2"" is that ""each clip has 3 frames, and every two consecutive frames have a temporal gap of 1."" However, the symbols in L177-178 seem to suggest other meanings of T and S.
    - The description of the inference mechanism is also vague. Is the MapUnveiler executed per frame or per clip? Figure 2 seems to suggest the per-clip inference where the predictions of T frames are obtained together. If this is the case, does it hurt the actual response frequency? 
    
     In short, Section 3 of the paper lacks significant details, and I cannot properly understand MapUnveiler's exact formulation. Given that the authors answer ""No"" to Question 5 of the Checklist, I have to raise concerns about the paper's reproducibility.

2. There is no detail on how the pre-training and fine-tuning are conducted. Do you initialize the MapNet by training MapTRv2? If this is the case, how are the training epochs split for the MapNet pre-training and the end-to-end MapUnveiler fine-tuning?  If the 24/6 epochs for nuScenes/Argo2 are only for the fine-tuning stage, then the comparisons in the main table are unfair, as other methods in the table have not fully converged. 

3. The main comparison results are incomplete. Most previous papers provide the nuScenes results of both short and long training schedules, but the main table only presents short-schedule results. Considering the last question about the pre-training and fine-tuning, the authors should complement the table with long-schedule results to show that MapUnveiler can obtain consistent performance boosts when all the methods are fully converged. This concern is backed up by the fact that MapUnveiler's improvement is much smaller on Argo2 compared to nuScenes -- based on my empirical experience, previous methods like MapTRv2 and its followups converge faster on Argo2, and training for 6 epochs is close to convergence. This probably suggests that the large performance gaps on nuScenes come from unfair training settings. 

4. Your interpretation of StreamMapNet and SQD-MapNet's Argo2 training epochs is wrong. These two methods employ a different frame sampling strategy at training time compared to MapTRv2, but their effective number of training samples is the same as MapTRv2. Therefore, the claim about the ""longer training schedules"" in the main table's caption is misleading.

5. The venues in the main table are not accurate. HIMap[49] and MGMap[24] are accepted by CVPR2024, and the information was already available at the time of NeurIPS submission. Furthermore, a recent HD map construction method, MapTracker[A], also studies temporal modeling and should be very relevant, but it is missing in the discussion and related works.   

    [A] MapTracker: Tracking with Strided Memory Fusion for Consistent Vector HD Mapping, arXiv:2403.15951

Limitations:
The limitations and broader impacts are adequately discussed in the paper.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors propose a new approach for constructing vectorized high-definition maps that exploits temporal information across adjacent input frames.  The model, which they call MapUnveiler, operates at the clip-level and consists of an intra-clip unveiler which generates vectorized maps for T frames and an inter-clip unveiler which uses a memory module to aggregate information between clips. The authors present results on two standard benchmarks, vectorized HD map construction benchmarks (nuScenes and Argoverse2) and demonstrate the model’s superior quantitive performance to several previously proposed approaches. They also show several qualitative examples of how MapUnveiler can better handle occlusions in the input images.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The paper is well-written and contextualized well within prior work.
- The methodology is novel and well-motivated.
- The results are strong on the two tested datasets, both quantitatively and qualitatively.
- Many different analyses and ablations were included to justify the design decisions used within MapUnveiler and show its strengths.

Weaknesses:
1. The methods is dense and a bit hard to read. The architecture figures help but are also a bit difficult to parse through. It would be helpful to try to weave more intuition into the text.
2. Claiming ""-9.8%"" is significant but ""-6.0%"" is comparable in the robustness to occlusion section seems a bit arbitrary (and potentially overstating MapUnveiler's performance, as a 6% drop is still considerable). I suggest the authors rephrase this sentence (and address similar claims in the paper).

There are several typos throughout the paper. I have enumerated some here, but encourage the authors to do a detailed proofread:
- 127: With there
- 129: mapnet -> MapNet
- 161: bev -> BEV
- 167 parenthesis 
- 192 backwards parenthesis 
- 294: In addition, if we choose too short

Limitations:
Only one limitation is included. I encourage the authors to think through other potential limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a clip-based vectorized HD map construction paradigm for the processing of long temporal sequence, in which occluded map elements are unveiled explicitly by efficient clip tokens. Through clip token propagation, MapUnveiler achieves effective utilization of long-term temporal map information by associating inter-clip information, in which clip tokens are propagated rather than dense BEV features. Experiments demonstrate that MapUnveiler boosts the performance on public benchmark datasets, also for more challenging setting like long-range perception and heavily occluded driving scenes.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper is well-written and easy-to-follow. Figures clearly conveys the intended message.
2. “Unveiling the hidden” and clip token propagation are reasonable and effective strategy for static Map element detection, which is practical and alleviates the problem to some extent.
3. The proposed method demonstrates strong performance on benchmark dataset, comprehensive experiments and ablation studies justify the model design.

Weaknesses:
1. As mentioned at line 227, this work is built on pretrained frame-level MapTRV2 and fine-tuned, thus the comparison can be unfair. Results without pretraining are required to verify your effectiveness.
2. At line 53 and BEV Updater in line 151, for occluded features, how to select the tokens that are visible in certain frames? Seems tokens within the temporal window are fully utilized for BEV update by cross attention, how to determine whether these tokens contain unblocked information? More explanations are required.

Limitations:
Yes. The authors mentioned the weakness of their approach on the corrupted input.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
It is known that usually deep neural networks will learn “easy examples"" that contain fast-learnable features first while learning more complex examples in a second time. The authors argue that mitigating such simplicity bias is the reason method like SAM are outperforming SGD. Based on such analysis, the authors introduce their methods coined as USEFUL that consists in two setups: 1) Identifying the examples with fast-learnable features using a clustering method based on layer output similarity 2) Upsampling by a constant factor the remaining examples with slow-learning features. By doing so, the authors can significantly increase model performances and training time on different classification tasks using different optimizers. They assess their methods across a wide range of dataset and different hyper-parameters and outperform random clustering baseline.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is well motivated and written. The method seems to be sounded and I really appreciate that the authors assess their method using different hyper-parameters such as optimizer, batch size, datasets, upsampling factor, architectures, and data augmentation. It is also great that they ran a baseline with random clustering.

Weaknesses:
It is not clear when and why one should choose the last output activation vector to define the clustering instead of intermediate activation vector. It is also not clear at which epoch one should decide to do the clustering since for a dataset like CIFAR10 the optimal performances are achieved at epoch 8 while for CIFAR100 it is epoch 20. So, finding the correct hyper-parameters for the clustering might be costly and thus impact how fast convergence can really be (if we consider this needed additional ablation on clustering epoch). In addition, the authors mention that they are using an upscaling factor of 2, but I am wondering how robust this is when using long-tail distribution. For example, I am not sure that on something like ImageNet-LT or Inaturalist, we will get the best performances by using a constant factor.  I would also be a bit more cautious about some of the claims made in the papers. For example, the authors claim that their method is generalizing to OOD tasks while providing experiments on only the WaterBird dataset.  So, it would be better to write about promising preliminary results than claiming generalization on OOD.

Limitations:
The authors did not really discuss any limitations (outside the fact that their theoretical result does not extend to CNN) or societal impact. I think that one limitation that could have been highlighted is the smaller scale of the experiment and the focus on classification tasks. Another limitation is the lack of results on OOD or long-tail benchmarks which would seem to be well suited for this type of work.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work aims to modify the training data distribution to improve in-distribution generalization. First, the authors theoretically analyse a 2-layer CNN and compare the feature learning dynamics (fast learnable and slow-learnable features) of Gradient Descent (GD) and Sharpness-Aware Minimization (SAM). It is then shown that SAM mitigates simplicity bias compared to GD. The authors then propose USEFUL (UpSample Early For Uniform Learning), a method that upsamples the examples in the training set that contains slow-learnable features.  USEFUL first clusters the examples with similar outputs early in the training and then upsamples the slow-learnable clusters. The main idea behind USEFUL is to learn features at a uniform speed (similar to SAM) by changing the training data distribution. USEFUL can be trained with SGD, SAM and SAM + Trivial Augment. Results on CIFAR-10, CIFAR-100, STL10, TinyImageNet indicate that USEFUL is across datasets and architectures. Additonal ablation and analysis show that USEFUL learns similar properties to SAM (for e.g less sharp solutions).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Originality: The question posed by the authors “Can we change the training data distribution such that the model trained on it has similar properties to SAM?” is interesting and novel. The proposed method is also well-motivated.
2. Results: The authors perform a comprehensive set of ablations and analysis on the proposed method USEFUL. Section 5.4 that shows that USEFUL’s solution has similar properties to SAM, which answers the question raised in the motivation of the paper. I also particularly like the ablations with upweighting loss and data selection method in Appendix D.6.
3. Overall, the paper is fairly well written. One minor point to address here is that the paper covers multiple concepts like SAM, simplicity bias, flat minima and uniform feature learning. It would be good to explain the relationship between these more clearly.

Weaknesses:
1. The authors explicitly mention that their focus in this paper is only on “in-distribution generalization”. I am a bit confused by this given the motivation of simplicity bias and learning features uniformly. To elaborate more on this point,
    - Springer et al [1] also show that SAM implicitly balances the quality of diverse features (similar to the observations made in Section 3 of this paper. The experimental results in [1] is focused more on datasets with multiple predictive features like CelebA, CIFAR-MNIST. 
    - Past work on simplicity bias and shortcut learning [2, 3, 4, 5] has focused on similar datasets like CelebA, Waterbirds, CIFAR-MNIST, Colored-MNIST to name a few.
    - While the authors have shown encouraging results on Waterbirds dataset in Appendix D5, it would be good to show the complete results on various groups and on other datasets as well.
2. Connection to [1]. Springer et al [1] made a very similar observation as to Section 3 in this paper. It would be great if the authors can clarify the differences with the observations in [1] and this work. Particularly, [1] also shows that SAM mitigates simplicity bias and that  SAM learns higher quality representations of hard-to-learn features. The authors briefly discuss this in Related Works section but a more detailed answer would be helpful.
3. I just wanted to understand the practical usefulness of the proposed method. This method has one additional hyperparameter i.e the separating epoch. The authors have reported the best separating epoch for all the datasets which is epoch 8 for CIFAR-10 and epoch 20 for CIFAR-100 (Appendix C.2). How is this hyperparameter chosen? Is there a separating epoch number that works across various datasets? This is especially relevant given that that the average gain on most of the datasets with USEFUL is less than 1% with additional cost for training. 
    
 [1] Springer, Jacob Mitchell, Vaishnavh Nagarajan, and Aditi Raghunathan. ""Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning."" The Twelfth International Conference on Learning Representations.

[2] Shah, Harshay, et al. ""The pitfalls of simplicity bias in neural networks."" Advances in Neural Information Processing Systems 33 (2020): 9573-9585.
    
 [3] Geirhos, Robert, et al. ""Shortcut learning in deep neural networks."" Nature Machine Intelligence 2.11 (2020): 665-673.
    
 [4] Kirichenko, Polina, Pavel Izmailov, and Andrew Gordon Wilson. ""Last layer re-training is sufficient for robustness to spurious correlations."" arXiv preprint arXiv:2204.02937 (2022).
    
 [5] Teney, Damien, et al. ""Evading the simplicity bias: Training a diverse set of models discovers solutions with superior ood generalization."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.

Limitations:
Yes, the authors have addressed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
- Proves for a 2-layer CNN with fixed second layer weighsts trained on a toy dataset,  SAM learns slow-learnable and fast-learnable features more uniformly in the early epochs compared to SGD
- Based on this analysis, proposes a simple clustering-based upsampling strategy for reducing simplicity bias / excessive reliance on fast-learnable features. The results show that this improves in-distribution generalization of standard small-scale image classification tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Simple easy-to-implement method that uses SAM and upsampling to improve in-distribution generalization
- The method is well justified with theoretical analysis comparing SAM and SGD on a toy data distribution. This analysis indicates that SAM is less sensitive to simplicity bias.

Weaknesses:
- No baselines. There are several papers now that try to reduce simplicity bias in order to improve performance:
    - https://arxiv.org/abs/2105.05612
    - https://arxiv.org/abs/2301.13293
    - https://arxiv.org/abs/2107.09044 (does not focus on simplicity bias explicitly, but similar to method proposed in paper)
    - simpler baselines: there are several papers that propose “example difficulty” metrics (https://arxiv.org/abs/2106.09647). How well do this correlate with the clusters found in your method? If you just train on the k examples with the highest difficulty scores (per class), does this fare worse than the proposed method?
- Limited novelty due to findings in [64] (Sharpness-aware minimization enhances feature quality via balanced learning). This paper also shows that SAM improves feature diversity (on real datasets + backed up with analysis on a toy dataset) and improves performance on transfer-learning tasks.
- Lacking discussion about when this method would fail. I can imagine two scenarios where the method would not work:
    1. Most training examples have one or more slow-learnable features. In this case, the clustering approach would “remove” most of the points in the dataset, and train on very points for multiple epochs. This could result in overfitting and performance that is worse than training. There’s an implicit assumption that there is some sort of one-to-one relation between examples and features. In the case where all examples contain an “easy” (e.g. patch) and a “hard” feature (e.g. CIFAR), would this method improve performance over SGD? 
    2. In noisy datasets, low-quality examples or mislabeled examples would require more time to learn, and this method would cluster them and train on them for longer. That is, it would group examples that are “high-quality” and hard-to-learn with “low-quality” points. In this case, would the proposed method improve performance over SGD? 
- “SB of SGD has been long conjectured to be the reason for the superior generalization performance of overparameterized models, by providing capacity control or implicit regularization” This incorrectly cites https://arxiv.org/abs/2006.07710v2, which shows that too much simplicity bias can lead to robustness and in-distribution generalization issues.
- Unfair evaluation. The experiments compare SAM+TA augmentation and SAM+USEFUL+TA to SGD (no TA). I think there should be two plots, complaring {SGD, SAM, SAM+USEFUL} w/ and w/o TA.
- Experiments on larger datasets. The image classification used here are fairly small-scale. I would like to how well this method scales to ImageNet-scale datasets (TinyImageNet is not a good proxy..)
- Writing is repetitive at times, especially the theory section (3.3)

Limitations:
Please see strengths and weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes an algorithm for changing the distribution of training data to improve the generalization of the model on origin data distribution. The paper is inspired by Sharpness Aware Minimization, which aims at finding a flat minimum meaning that it has a good generalization capability. This paper divides features into two categories: fast-learnable features and slow-learnable features and derives some observations like ""SGD and SAM only learn fast-learnable or easy features early in training"" and ""SAM learns slow-learnable and fast-learnable features at a more uniform speed"". The authors propose the method dubbed as USEFUL to train the model on some slow-learnable features repeatedly. The experiments show the effectiveness of USEFUL on CIFAR10 and CIFAR100 datasets.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written and easy to follow.
- The paper has a theoretical analysis to analyze the learning progress and derive the proposed method.
- The experiments are abundant and comprehensive.

Weaknesses:
There are some questions based on the presentation of this paper, I will not hesitate to improve my score if the following question are solved.
- Difference between this paper and methods for long-tailed data distribution or measuring the difficulty of learning examples. Algorithms for long-tailed data distribution are usually based on resampling training data or reweighing loss value. The proposed USEFUL is similar to the resampling methods except that USEFUL focuses on the features that are hard/slow to learn. Some references for understanding: [Shi, Jiang-Xin, et al. ""How re-sampling helps for long-tail learning?."" Advances in Neural Information Processing Systems 36 (2023).](https://arxiv.org/pdf/2310.18236), [Shrivastava, Abhinav, Abhinav Gupta, and Ross Girshick. ""Training region-based object detectors with online hard example mining."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.](https://arxiv.org/pdf/1604.03540v1) and some references based on it, [A Re-Balancing Strategy for Class-Imbalanced Classification Based on Instance Difficulty](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_A_Re-Balancing_Strategy_for_Class-Imbalanced_Classification_Based_on_Instance_Difficulty_CVPR_2022_paper.pdf), [Active Teacher for Semi-Supervised Object Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Mi_Active_Teacher_for_Semi-Supervised_Object_Detection_CVPR_2022_paper.pdf), I believe a discussion of these references in paper should be helpful.
- The relation between the proposed USEFUL and SAM? It seems like the motivation of USEFUL is changing the data distribution to get a flat minimum like SAM. But the results in Appendix D.2, *i.e.*, 53.8 for SGD 41.8 for SGD+USEFUL 12.4 for SAM in Table 1($\lambda_{max})$, do not show effectiveness compared with SAM. It could show the effectiveness on SGD but it's far from being comparable to SAM. 
Some small questions:
- What's the exact formulation of the Data distribution?
- What's the ""patch"" meaning in Definition 3.1? Is that the same as the patch in ViT or the channel of the image? It's a little confusing.
- The experiments mainly focus on traditional architecture, e.g., n-Layer CNN, ResNet. More experiments on popular models and big datasets, e.g., Transformer ImageNet-1k, would be better.

Limitations:
N.A.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
It is known that usually deep neural networks will learn “easy examples"" that contain fast-learnable features first while learning more complex examples in a second time. The authors argue that mitigating such simplicity bias is the reason method like SAM are outperforming SGD. Based on such analysis, the authors introduce their methods coined as USEFUL that consists in two setups: 1) Identifying the examples with fast-learnable features using a clustering method based on layer output similarity 2) Upsampling by a constant factor the remaining examples with slow-learning features. By doing so, the authors can significantly increase model performances and training time on different classification tasks using different optimizers. They assess their methods across a wide range of dataset and different hyper-parameters and outperform random clustering baseline.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper is well motivated and written. The method seems to be sounded and I really appreciate that the authors assess their method using different hyper-parameters such as optimizer, batch size, datasets, upsampling factor, architectures, and data augmentation. It is also great that they ran a baseline with random clustering.

Weaknesses:
It is not clear when and why one should choose the last output activation vector to define the clustering instead of intermediate activation vector. It is also not clear at which epoch one should decide to do the clustering since for a dataset like CIFAR10 the optimal performances are achieved at epoch 8 while for CIFAR100 it is epoch 20. So, finding the correct hyper-parameters for the clustering might be costly and thus impact how fast convergence can really be (if we consider this needed additional ablation on clustering epoch). In addition, the authors mention that they are using an upscaling factor of 2, but I am wondering how robust this is when using long-tail distribution. For example, I am not sure that on something like ImageNet-LT or Inaturalist, we will get the best performances by using a constant factor.  I would also be a bit more cautious about some of the claims made in the papers. For example, the authors claim that their method is generalizing to OOD tasks while providing experiments on only the WaterBird dataset.  So, it would be better to write about promising preliminary results than claiming generalization on OOD.

Limitations:
The authors did not really discuss any limitations (outside the fact that their theoretical result does not extend to CNN) or societal impact. I think that one limitation that could have been highlighted is the smaller scale of the experiment and the focus on classification tasks. Another limitation is the lack of results on OOD or long-tail benchmarks which would seem to be well suited for this type of work.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work aims to modify the training data distribution to improve in-distribution generalization. First, the authors theoretically analyse a 2-layer CNN and compare the feature learning dynamics (fast learnable and slow-learnable features) of Gradient Descent (GD) and Sharpness-Aware Minimization (SAM). It is then shown that SAM mitigates simplicity bias compared to GD. The authors then propose USEFUL (UpSample Early For Uniform Learning), a method that upsamples the examples in the training set that contains slow-learnable features.  USEFUL first clusters the examples with similar outputs early in the training and then upsamples the slow-learnable clusters. The main idea behind USEFUL is to learn features at a uniform speed (similar to SAM) by changing the training data distribution. USEFUL can be trained with SGD, SAM and SAM + Trivial Augment. Results on CIFAR-10, CIFAR-100, STL10, TinyImageNet indicate that USEFUL is across datasets and architectures. Additonal ablation and analysis show that USEFUL learns similar properties to SAM (for e.g less sharp solutions).

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Originality: The question posed by the authors “Can we change the training data distribution such that the model trained on it has similar properties to SAM?” is interesting and novel. The proposed method is also well-motivated.
2. Results: The authors perform a comprehensive set of ablations and analysis on the proposed method USEFUL. Section 5.4 that shows that USEFUL’s solution has similar properties to SAM, which answers the question raised in the motivation of the paper. I also particularly like the ablations with upweighting loss and data selection method in Appendix D.6.
3. Overall, the paper is fairly well written. One minor point to address here is that the paper covers multiple concepts like SAM, simplicity bias, flat minima and uniform feature learning. It would be good to explain the relationship between these more clearly.

Weaknesses:
1. The authors explicitly mention that their focus in this paper is only on “in-distribution generalization”. I am a bit confused by this given the motivation of simplicity bias and learning features uniformly. To elaborate more on this point,
    - Springer et al [1] also show that SAM implicitly balances the quality of diverse features (similar to the observations made in Section 3 of this paper. The experimental results in [1] is focused more on datasets with multiple predictive features like CelebA, CIFAR-MNIST. 
    - Past work on simplicity bias and shortcut learning [2, 3, 4, 5] has focused on similar datasets like CelebA, Waterbirds, CIFAR-MNIST, Colored-MNIST to name a few.
    - While the authors have shown encouraging results on Waterbirds dataset in Appendix D5, it would be good to show the complete results on various groups and on other datasets as well.
2. Connection to [1]. Springer et al [1] made a very similar observation as to Section 3 in this paper. It would be great if the authors can clarify the differences with the observations in [1] and this work. Particularly, [1] also shows that SAM mitigates simplicity bias and that  SAM learns higher quality representations of hard-to-learn features. The authors briefly discuss this in Related Works section but a more detailed answer would be helpful.
3. I just wanted to understand the practical usefulness of the proposed method. This method has one additional hyperparameter i.e the separating epoch. The authors have reported the best separating epoch for all the datasets which is epoch 8 for CIFAR-10 and epoch 20 for CIFAR-100 (Appendix C.2). How is this hyperparameter chosen? Is there a separating epoch number that works across various datasets? This is especially relevant given that that the average gain on most of the datasets with USEFUL is less than 1% with additional cost for training. 
    
 [1] Springer, Jacob Mitchell, Vaishnavh Nagarajan, and Aditi Raghunathan. ""Sharpness-Aware Minimization Enhances Feature Quality via Balanced Learning."" The Twelfth International Conference on Learning Representations.

[2] Shah, Harshay, et al. ""The pitfalls of simplicity bias in neural networks."" Advances in Neural Information Processing Systems 33 (2020): 9573-9585.
    
 [3] Geirhos, Robert, et al. ""Shortcut learning in deep neural networks."" Nature Machine Intelligence 2.11 (2020): 665-673.
    
 [4] Kirichenko, Polina, Pavel Izmailov, and Andrew Gordon Wilson. ""Last layer re-training is sufficient for robustness to spurious correlations."" arXiv preprint arXiv:2204.02937 (2022).
    
 [5] Teney, Damien, et al. ""Evading the simplicity bias: Training a diverse set of models discovers solutions with superior ood generalization."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.

Limitations:
Yes, the authors have addressed limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
- Proves for a 2-layer CNN with fixed second layer weighsts trained on a toy dataset,  SAM learns slow-learnable and fast-learnable features more uniformly in the early epochs compared to SGD
- Based on this analysis, proposes a simple clustering-based upsampling strategy for reducing simplicity bias / excessive reliance on fast-learnable features. The results show that this improves in-distribution generalization of standard small-scale image classification tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Simple easy-to-implement method that uses SAM and upsampling to improve in-distribution generalization
- The method is well justified with theoretical analysis comparing SAM and SGD on a toy data distribution. This analysis indicates that SAM is less sensitive to simplicity bias.

Weaknesses:
- No baselines. There are several papers now that try to reduce simplicity bias in order to improve performance:
    - https://arxiv.org/abs/2105.05612
    - https://arxiv.org/abs/2301.13293
    - https://arxiv.org/abs/2107.09044 (does not focus on simplicity bias explicitly, but similar to method proposed in paper)
    - simpler baselines: there are several papers that propose “example difficulty” metrics (https://arxiv.org/abs/2106.09647). How well do this correlate with the clusters found in your method? If you just train on the k examples with the highest difficulty scores (per class), does this fare worse than the proposed method?
- Limited novelty due to findings in [64] (Sharpness-aware minimization enhances feature quality via balanced learning). This paper also shows that SAM improves feature diversity (on real datasets + backed up with analysis on a toy dataset) and improves performance on transfer-learning tasks.
- Lacking discussion about when this method would fail. I can imagine two scenarios where the method would not work:
    1. Most training examples have one or more slow-learnable features. In this case, the clustering approach would “remove” most of the points in the dataset, and train on very points for multiple epochs. This could result in overfitting and performance that is worse than training. There’s an implicit assumption that there is some sort of one-to-one relation between examples and features. In the case where all examples contain an “easy” (e.g. patch) and a “hard” feature (e.g. CIFAR), would this method improve performance over SGD? 
    2. In noisy datasets, low-quality examples or mislabeled examples would require more time to learn, and this method would cluster them and train on them for longer. That is, it would group examples that are “high-quality” and hard-to-learn with “low-quality” points. In this case, would the proposed method improve performance over SGD? 
- “SB of SGD has been long conjectured to be the reason for the superior generalization performance of overparameterized models, by providing capacity control or implicit regularization” This incorrectly cites https://arxiv.org/abs/2006.07710v2, which shows that too much simplicity bias can lead to robustness and in-distribution generalization issues.
- Unfair evaluation. The experiments compare SAM+TA augmentation and SAM+USEFUL+TA to SGD (no TA). I think there should be two plots, complaring {SGD, SAM, SAM+USEFUL} w/ and w/o TA.
- Experiments on larger datasets. The image classification used here are fairly small-scale. I would like to how well this method scales to ImageNet-scale datasets (TinyImageNet is not a good proxy..)
- Writing is repetitive at times, especially the theory section (3.3)

Limitations:
Please see strengths and weaknesses.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes an algorithm for changing the distribution of training data to improve the generalization of the model on origin data distribution. The paper is inspired by Sharpness Aware Minimization, which aims at finding a flat minimum meaning that it has a good generalization capability. This paper divides features into two categories: fast-learnable features and slow-learnable features and derives some observations like ""SGD and SAM only learn fast-learnable or easy features early in training"" and ""SAM learns slow-learnable and fast-learnable features at a more uniform speed"". The authors propose the method dubbed as USEFUL to train the model on some slow-learnable features repeatedly. The experiments show the effectiveness of USEFUL on CIFAR10 and CIFAR100 datasets.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written and easy to follow.
- The paper has a theoretical analysis to analyze the learning progress and derive the proposed method.
- The experiments are abundant and comprehensive.

Weaknesses:
There are some questions based on the presentation of this paper, I will not hesitate to improve my score if the following question are solved.
- Difference between this paper and methods for long-tailed data distribution or measuring the difficulty of learning examples. Algorithms for long-tailed data distribution are usually based on resampling training data or reweighing loss value. The proposed USEFUL is similar to the resampling methods except that USEFUL focuses on the features that are hard/slow to learn. Some references for understanding: [Shi, Jiang-Xin, et al. ""How re-sampling helps for long-tail learning?."" Advances in Neural Information Processing Systems 36 (2023).](https://arxiv.org/pdf/2310.18236), [Shrivastava, Abhinav, Abhinav Gupta, and Ross Girshick. ""Training region-based object detectors with online hard example mining."" Proceedings of the IEEE conference on computer vision and pattern recognition. 2016.](https://arxiv.org/pdf/1604.03540v1) and some references based on it, [A Re-Balancing Strategy for Class-Imbalanced Classification Based on Instance Difficulty](https://openaccess.thecvf.com/content/CVPR2022/papers/Yu_A_Re-Balancing_Strategy_for_Class-Imbalanced_Classification_Based_on_Instance_Difficulty_CVPR_2022_paper.pdf), [Active Teacher for Semi-Supervised Object Detection](https://openaccess.thecvf.com/content/CVPR2022/papers/Mi_Active_Teacher_for_Semi-Supervised_Object_Detection_CVPR_2022_paper.pdf), I believe a discussion of these references in paper should be helpful.
- The relation between the proposed USEFUL and SAM? It seems like the motivation of USEFUL is changing the data distribution to get a flat minimum like SAM. But the results in Appendix D.2, *i.e.*, 53.8 for SGD 41.8 for SGD+USEFUL 12.4 for SAM in Table 1($\lambda_{max})$, do not show effectiveness compared with SAM. It could show the effectiveness on SGD but it's far from being comparable to SAM. 
Some small questions:
- What's the exact formulation of the Data distribution?
- What's the ""patch"" meaning in Definition 3.1? Is that the same as the patch in ViT or the channel of the image? It's a little confusing.
- The experiments mainly focus on traditional architecture, e.g., n-Layer CNN, ResNet. More experiments on popular models and big datasets, e.g., Transformer ImageNet-1k, would be better.

Limitations:
N.A.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper reduces the complex policy optimization procedure of alignment to a simple regression objective, using the relation between optimal policy and reward. The paper conduct detailed theoretical analysis in revealing the relation between the proposed algorithm *REBEL* and *NPG/MD*. Comprehensive experiments in both text and image generation exhibit the effectiveness of *REBEL*.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper studies simplified version of policy optimization in RLHF (compared to PPO), which is a research topic of interest.
2. The theoretical analysis of *REBEL* is detailed and insightful.
3. The presentation of this paper is logically clear and has good readability.
4. The experiments in this paper are comprehensive, and the experimental results are well presented.

Weaknesses:
1. The statement ""REBEL ... be extended to handle intransistive preferences ...."" in the abstract is not adequately presented in the main content of the paper. As the major influence brought by intransistive preferences is the degradation of reward score accuracy, which is not addressed by this paper.
2. I would suggest the authors to summarize the limitations of the proposed method in a separate ""Limitations"" section.

Limitations:
none

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes the REBEL algorithm that reduces policy optimization to iteratively solving squared loss regression problems on the difference in rewards between trajectories, based on DPO's analysis. The paper transforms the resulting equation for r(x, y) presented in DPO to a regression loss function, and avoids the intractable calculation of Z(x) by calculating the loss based on a pair of samples from the same input prompt x, i.e., (x, y) and (x, y'). One of the goals for REBEL is to serve as a simple and lightweight RL algorithm that eliminates the need for complex components like value functions and clipping heuristics used in PPO. The authors provide a theoretical analysis showing that Natural Policy Gradient can be seen as a special case of REBEL under some assumptions. The authors conduct two kinds of empirical analysis including language modeling and image generation tasks to demonstrate the performance of REBEL.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Originality:
    - This paper presents a new angle by transforming the analysis of the reward function presented in the DPO paper into a reward regression loss, leading to the proposed REBEL algorithm. 
    - The authors make connections between REBEL and existing RL methods like NPG considering some assumptions, showing that these algorithms can be seen as special cases or approximations of REBEL under certain conditions. 

- Quality:
    - The paper provides a thorough theoretical analysis comparing REBEL with existing RL approaches. 

- Clarity: 
    - The paper is well-written and easy to understand, with a clear logical flow from motivation to theoretical analysis to empirical validation. The authors do an good job of explaining the intuition behind REBEL and highlighting its connections to prior work.

- Significance:
    - The paper tackles the important problem of developing simpler and more efficient RL algorithms that can scale to large-scale generative model fine-tuning.

Weaknesses:
1. Insufficient experimental validation and limited baseline comparisons:
- While the paper presents empirical results on language modeling and image generation tasks, the experimental validation of REBEL could be more comprehensive. The authors should consider including a wider range of benchmarks and datasets to demonstrate the generality and robustness of their approach.
- The comparison with baseline algorithms like PPO and DPO is somewhat limited. The authors should provide more details on the hyperparameter settings and training procedures for the baselines to ensure a fair comparison. Moreover, the poor performance of DPO compared to PPO in the experiments raises questions about the implementation or hyperparameter choices.
- The authors claim that REBEL matches the strongest known theoretical guarantees in terms of convergence and sample complexity. However, the experiments only compare performance at a specific epoch without demonstrating improved sample efficiency. Convergence plots showing the performance of REBEL and baselines over the course of training would provide a clearer picture of the sample efficiency and convergence properties.

2. Lack of support for certain claims and limited exploration of key aspects:
- The paper makes several claims regarding the advantages of REBEL, such as its ability to handle intransitive preferences, incorporate offline datasets, and apply to deterministic MDPs. However, there is a lack of corresponding experimental evidence or theoretical analysis to substantiate these claims.
- The relationship between the regressor's performance and the quality of the dataset used for training is not explored in depth. Insights or experiments that investigate how dataset quality and diversity affect the regressor's ability to capture an improved policy would strengthen the paper.
- The choice of base distribution \mu is mentioned as a determining factor for whether REBEL is hybrid or fully online. However, the paper does not provide experimental results comparing different forms of \mu across various tasks or practical guidelines for choosing \mu in real-world applications.

3. Inconsistencies and potential conflicts with previous statements:
- The authors mention that critic-based variance reduction might be necessary for high-variance trajectory-level rewards in stochastic MDPs, which seems to contradict the criticism of PPO's complexity in the introductory section. The lack of experimental support for REBEL's performance in stochastic MDPs is a significant limitation, and the authors should provide preliminary results or theoretical insights to support their claims.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents REBEL, a minimalist reinforcement learning algorithm that does policy optimization by solving a sequence of regression problems using relative rewards as targets. Theoretical analysis shows that Natural Policy Gradient (NPG) is a variant of REBEL, and thus theoretical guarantees for NPG can be applied to REBEL.  Experimental results  show that REBEL matches or outperforms existing baselines, most notably PPO and RLOO, on multiple tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-organized and technically sound. The general flow of the paper is smooth and proposed methods are explained adequately. The paper has an appropriate number of citations and properly details existing work in the related work section. 
- The method is simple to implement and has little engineering overhead. Given the minimalist implementation, the results are impressive, surpassing even PPO, which typically requires significant engineering.

Weaknesses:
- There are no significant weaknesses in this work, barring some clarifying details. 
- I believe that at least a brief section on related work should be included in the main paper, the in-depth one can be deferred to the appendix. In terms of space, I personally do not think Section 2.2 adds much value to the main paper.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present REBEL, a method for solving contextual bandit problems (such as the alignment of language models) via regressing relative rewards. They first derive their objective by demonstrating that the use of paired responses means that you can get rid of the partition function, which is impossible to estimate. 

They then connect their method to previous methods in RL including detailing, but not . They demonstrate that under strong assumptions REBEL is equivalent to mirror descent, and that under assumptions of coverage by the reference policy, that REBEL produces returns close to an optimal policy. 

Finally the authors run experiments on summarisation, general chat and image alignment, demonstrating their method compares favourably to other methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The idea of using relative rewards to remove the partition function is a nice and simple idea
* The theoretical connections of their method to prior methods grounds their work nicely in existing RL approaches. 
* The empirical results seem to demonstrate their method is competitive or better than other approaches. 
* REBEL compares favourably in terms of runtime and memory usage with other, similarly performing methods. 

Overall the theoretical and empirical examinations of their method seems very thorough.

Weaknesses:
See questions

Limitations:
The authors discuss the limitations throughout their work at relevant stages.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper reduces the complex policy optimization procedure of alignment to a simple regression objective, using the relation between optimal policy and reward. The paper conduct detailed theoretical analysis in revealing the relation between the proposed algorithm *REBEL* and *NPG/MD*. Comprehensive experiments in both text and image generation exhibit the effectiveness of *REBEL*.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. This paper studies simplified version of policy optimization in RLHF (compared to PPO), which is a research topic of interest.
2. The theoretical analysis of *REBEL* is detailed and insightful.
3. The presentation of this paper is logically clear and has good readability.
4. The experiments in this paper are comprehensive, and the experimental results are well presented.

Weaknesses:
1. The statement ""REBEL ... be extended to handle intransistive preferences ...."" in the abstract is not adequately presented in the main content of the paper. As the major influence brought by intransistive preferences is the degradation of reward score accuracy, which is not addressed by this paper.
2. I would suggest the authors to summarize the limitations of the proposed method in a separate ""Limitations"" section.

Limitations:
none

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes the REBEL algorithm that reduces policy optimization to iteratively solving squared loss regression problems on the difference in rewards between trajectories, based on DPO's analysis. The paper transforms the resulting equation for r(x, y) presented in DPO to a regression loss function, and avoids the intractable calculation of Z(x) by calculating the loss based on a pair of samples from the same input prompt x, i.e., (x, y) and (x, y'). One of the goals for REBEL is to serve as a simple and lightweight RL algorithm that eliminates the need for complex components like value functions and clipping heuristics used in PPO. The authors provide a theoretical analysis showing that Natural Policy Gradient can be seen as a special case of REBEL under some assumptions. The authors conduct two kinds of empirical analysis including language modeling and image generation tasks to demonstrate the performance of REBEL.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Originality:
    - This paper presents a new angle by transforming the analysis of the reward function presented in the DPO paper into a reward regression loss, leading to the proposed REBEL algorithm. 
    - The authors make connections between REBEL and existing RL methods like NPG considering some assumptions, showing that these algorithms can be seen as special cases or approximations of REBEL under certain conditions. 

- Quality:
    - The paper provides a thorough theoretical analysis comparing REBEL with existing RL approaches. 

- Clarity: 
    - The paper is well-written and easy to understand, with a clear logical flow from motivation to theoretical analysis to empirical validation. The authors do an good job of explaining the intuition behind REBEL and highlighting its connections to prior work.

- Significance:
    - The paper tackles the important problem of developing simpler and more efficient RL algorithms that can scale to large-scale generative model fine-tuning.

Weaknesses:
1. Insufficient experimental validation and limited baseline comparisons:
- While the paper presents empirical results on language modeling and image generation tasks, the experimental validation of REBEL could be more comprehensive. The authors should consider including a wider range of benchmarks and datasets to demonstrate the generality and robustness of their approach.
- The comparison with baseline algorithms like PPO and DPO is somewhat limited. The authors should provide more details on the hyperparameter settings and training procedures for the baselines to ensure a fair comparison. Moreover, the poor performance of DPO compared to PPO in the experiments raises questions about the implementation or hyperparameter choices.
- The authors claim that REBEL matches the strongest known theoretical guarantees in terms of convergence and sample complexity. However, the experiments only compare performance at a specific epoch without demonstrating improved sample efficiency. Convergence plots showing the performance of REBEL and baselines over the course of training would provide a clearer picture of the sample efficiency and convergence properties.

2. Lack of support for certain claims and limited exploration of key aspects:
- The paper makes several claims regarding the advantages of REBEL, such as its ability to handle intransitive preferences, incorporate offline datasets, and apply to deterministic MDPs. However, there is a lack of corresponding experimental evidence or theoretical analysis to substantiate these claims.
- The relationship between the regressor's performance and the quality of the dataset used for training is not explored in depth. Insights or experiments that investigate how dataset quality and diversity affect the regressor's ability to capture an improved policy would strengthen the paper.
- The choice of base distribution \mu is mentioned as a determining factor for whether REBEL is hybrid or fully online. However, the paper does not provide experimental results comparing different forms of \mu across various tasks or practical guidelines for choosing \mu in real-world applications.

3. Inconsistencies and potential conflicts with previous statements:
- The authors mention that critic-based variance reduction might be necessary for high-variance trajectory-level rewards in stochastic MDPs, which seems to contradict the criticism of PPO's complexity in the introductory section. The lack of experimental support for REBEL's performance in stochastic MDPs is a significant limitation, and the authors should provide preliminary results or theoretical insights to support their claims.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents REBEL, a minimalist reinforcement learning algorithm that does policy optimization by solving a sequence of regression problems using relative rewards as targets. Theoretical analysis shows that Natural Policy Gradient (NPG) is a variant of REBEL, and thus theoretical guarantees for NPG can be applied to REBEL.  Experimental results  show that REBEL matches or outperforms existing baselines, most notably PPO and RLOO, on multiple tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-organized and technically sound. The general flow of the paper is smooth and proposed methods are explained adequately. The paper has an appropriate number of citations and properly details existing work in the related work section. 
- The method is simple to implement and has little engineering overhead. Given the minimalist implementation, the results are impressive, surpassing even PPO, which typically requires significant engineering.

Weaknesses:
- There are no significant weaknesses in this work, barring some clarifying details. 
- I believe that at least a brief section on related work should be included in the main paper, the in-depth one can be deferred to the appendix. In terms of space, I personally do not think Section 2.2 adds much value to the main paper.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present REBEL, a method for solving contextual bandit problems (such as the alignment of language models) via regressing relative rewards. They first derive their objective by demonstrating that the use of paired responses means that you can get rid of the partition function, which is impossible to estimate. 

They then connect their method to previous methods in RL including detailing, but not . They demonstrate that under strong assumptions REBEL is equivalent to mirror descent, and that under assumptions of coverage by the reference policy, that REBEL produces returns close to an optimal policy. 

Finally the authors run experiments on summarisation, general chat and image alignment, demonstrating their method compares favourably to other methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The idea of using relative rewards to remove the partition function is a nice and simple idea
* The theoretical connections of their method to prior methods grounds their work nicely in existing RL approaches. 
* The empirical results seem to demonstrate their method is competitive or better than other approaches. 
* REBEL compares favourably in terms of runtime and memory usage with other, similarly performing methods. 

Overall the theoretical and empirical examinations of their method seems very thorough.

Weaknesses:
See questions

Limitations:
The authors discuss the limitations throughout their work at relevant stages.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,Yes,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper explores the problem of generating 3D audiovisual scenes – that is, generating 3D scenes with spatial audio. The proposed approach, AV Cloud, uses anchor points obtained from Structure-from-Motion (SfM) points. The anchors are then used with an AV Cloud splatting module which decodes the visuals and the audio. Experiments are done on RWAVS and Replay-NVAS with comparisons done with several prior works.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
– 3d audiovisual scene generation is a really interesting problem to solve. WHile there is considerable literature on visual scene generation, generating 3d visual scene is an interesting problem with real-world applications. 

– The model claims to be able to generate the audio and the visuals in parallel. Essentially unlike prior work it decouples the generation of two modalities by not using the generated visuals for generating the audio. 

– On objective metrics, the paper claims to make good improvements

---- 
increased score after rebuttal

Weaknesses:
– The paper is a bit difficult to follow – especially the key part of AudioVisual anchor points. 

– First, a short primer on SfM is desirable, even if it is in Appendix. More importantly though, it is not clear why it makes sense to use SfM points and clustering on top of them to model AV anchor points and generation of spatial points. Why does it make sense to use SfM points or anchors derived from them as the starting point for AV generation ? What relation the anchors have with audio which motivates the fact that these anchors can be used for audio generation ? 

– Second, the details of AV anchor points are fuzzy. The visuals are used for SfM points which are then clustered to get the anchors. Where is the audio into picture here ? Are these anchors visual only ? If so, why are we calling it AV Anchors ? 

– In prior works, for example AV-Nerf, there is an an explicit AV-Mapper which learns the audio visual relations through which the spatial audio generatio happens. Here Visual2Audio splatting transformer is expected to model that ? 

– For the subjective tests, it would be good to actually get proper subjective ratings on the generated spatial audio. The current preference numbers are not very informative. Getting the spatial audio rated with respect to their quality and spatial characteristics would be much more meaningful. 

– Since NAF, INRAS and other works are considered here - I think it would be good to reference NACF ([R1]) below. NACF specifically focuses on using visuals and is ideal for comparison. 

[R1] Neural Acoustic Context Field: Rendering Realistic Room Impulse Response With Neural Fields

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes AV-Cloud, a framework for high-quality spatial audio rendering in 3D scenes without relying on visual cues. AV-Cloud addresses issues in current audio-visual rendering methods, such as audio lag and dependence on visual rendering quality, by introducing Audio-Visual Anchors and the Audio-Visual Cloud Splatting module. These components facilitate the generation of viewpoint-specific spatial audio synchronized with visual content. The method demonstrates superior performance on multiple benchmarks, outperforming existing baselines in audio reconstruction accuracy, perceptual quality, and acoustic effects.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The concept of using Audio-Visual Anchors and Cloud Splatting to decouple audio rendering from visual rendering is interesting.
2. The paper demonstrates comprehensive experimentation and robust evaluation across multiple benchmarks.
3. The paper is well-structured and the presentation of the framework is clear. The figures and supplement examples help the readers better understand.
4. The proposed method addresses critical issues in real-time audio-visual rendering.

Weaknesses:
1. The mathematical formulation of the Audio-Visual Cloud Splatting module could be more detailed. For instance, Equation (2) introduces the softmax function applied to the relative vectors and visual features, but the reason behind this specific formulation and its implications are not sufficiently explained. Clarifying how the weights $a_{ki}$ are computed and how they influence the final output would enhance understanding.
2. The technical derivation of the Spatial Audio Render Head (SARH) lacks depth. Specifically, the process described in Equations (4) and (5), where the mixture mask $m_m$ and the difference mask $m_d$ are used to compute the left and right channel outputs, is not fully elaborated. The significance of these masks and their impact on the final audio quality are not clearly discussed. Additionally, the role and impact of the convolution modules within the residual structure (Figure 3) are not sufficiently explained.
3. While the method shows strong performance on benchmarks and some real-world examples, the provided examples are too idealized and lack challenging elements like interfering sound (e.g., crowd noise). I think the robustness of AV-Cloud in more complex and noisy real-world environments should also be validated.

Limitations:
The authors mention the limitations of their approach's challenges and potential drawbacks. The reliance on camera calibration and the potential issues with noise in real-world audio recordings are noted. Additional imitations can be found in the Weaknesses section

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
A novel approach for rendering high-quality spatial audio in 3D scenes, called AV-Cloud, is proposed. This method synchronizes with the visual stream without relying on or being explicitly conditioned by visual rendering, enabling immersive virtual tourism through real-time dynamic navigation of both audio and visual content. Unlike current audio-visual rendering methods that depend on visual cues and may suffer from visual artifacts causing audio inconsistencies, AV-Cloud overcomes these issues. It uses a set of sparse AV anchor points, forming an Audio-Visual Cloud derived from camera calibration, to represent the audio-visual scene. The Audio-Visual Cloud allows for the generation of spatial audio for any listener location. A novel module, Audio-Visual Cloud Splatting, decodes these AV anchor points into a spatial audio transfer function for the listener’s viewpoint, which is then applied by the Spatial Audio Render Head module to transform monaural input into viewpoint-specific spatial audio. This approach eliminates the need for pre-rendered images and efficiently aligns spatial audio with any visual viewpoint. The results are satisfying.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The AV anchors strategy seems to be interesting and effective for audio-visual scene representation. The Audio-Visual Cloud Splatting is novel for AV tasks but more likely to be a Q-former.
2. The experiment results are good and ablations are clear.

Weaknesses:
As I mentioned in the strengths, the Audio-Visual Cloud Splatting seems to be a Q-former like module.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper explores the problem of generating 3D audiovisual scenes – that is, generating 3D scenes with spatial audio. The proposed approach, AV Cloud, uses anchor points obtained from Structure-from-Motion (SfM) points. The anchors are then used with an AV Cloud splatting module which decodes the visuals and the audio. Experiments are done on RWAVS and Replay-NVAS with comparisons done with several prior works.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
– 3d audiovisual scene generation is a really interesting problem to solve. WHile there is considerable literature on visual scene generation, generating 3d visual scene is an interesting problem with real-world applications. 

– The model claims to be able to generate the audio and the visuals in parallel. Essentially unlike prior work it decouples the generation of two modalities by not using the generated visuals for generating the audio. 

– On objective metrics, the paper claims to make good improvements

---- 
increased score after rebuttal

Weaknesses:
– The paper is a bit difficult to follow – especially the key part of AudioVisual anchor points. 

– First, a short primer on SfM is desirable, even if it is in Appendix. More importantly though, it is not clear why it makes sense to use SfM points and clustering on top of them to model AV anchor points and generation of spatial points. Why does it make sense to use SfM points or anchors derived from them as the starting point for AV generation ? What relation the anchors have with audio which motivates the fact that these anchors can be used for audio generation ? 

– Second, the details of AV anchor points are fuzzy. The visuals are used for SfM points which are then clustered to get the anchors. Where is the audio into picture here ? Are these anchors visual only ? If so, why are we calling it AV Anchors ? 

– In prior works, for example AV-Nerf, there is an an explicit AV-Mapper which learns the audio visual relations through which the spatial audio generatio happens. Here Visual2Audio splatting transformer is expected to model that ? 

– For the subjective tests, it would be good to actually get proper subjective ratings on the generated spatial audio. The current preference numbers are not very informative. Getting the spatial audio rated with respect to their quality and spatial characteristics would be much more meaningful. 

– Since NAF, INRAS and other works are considered here - I think it would be good to reference NACF ([R1]) below. NACF specifically focuses on using visuals and is ideal for comparison. 

[R1] Neural Acoustic Context Field: Rendering Realistic Room Impulse Response With Neural Fields

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes AV-Cloud, a framework for high-quality spatial audio rendering in 3D scenes without relying on visual cues. AV-Cloud addresses issues in current audio-visual rendering methods, such as audio lag and dependence on visual rendering quality, by introducing Audio-Visual Anchors and the Audio-Visual Cloud Splatting module. These components facilitate the generation of viewpoint-specific spatial audio synchronized with visual content. The method demonstrates superior performance on multiple benchmarks, outperforming existing baselines in audio reconstruction accuracy, perceptual quality, and acoustic effects.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. The concept of using Audio-Visual Anchors and Cloud Splatting to decouple audio rendering from visual rendering is interesting.
2. The paper demonstrates comprehensive experimentation and robust evaluation across multiple benchmarks.
3. The paper is well-structured and the presentation of the framework is clear. The figures and supplement examples help the readers better understand.
4. The proposed method addresses critical issues in real-time audio-visual rendering.

Weaknesses:
1. The mathematical formulation of the Audio-Visual Cloud Splatting module could be more detailed. For instance, Equation (2) introduces the softmax function applied to the relative vectors and visual features, but the reason behind this specific formulation and its implications are not sufficiently explained. Clarifying how the weights $a_{ki}$ are computed and how they influence the final output would enhance understanding.
2. The technical derivation of the Spatial Audio Render Head (SARH) lacks depth. Specifically, the process described in Equations (4) and (5), where the mixture mask $m_m$ and the difference mask $m_d$ are used to compute the left and right channel outputs, is not fully elaborated. The significance of these masks and their impact on the final audio quality are not clearly discussed. Additionally, the role and impact of the convolution modules within the residual structure (Figure 3) are not sufficiently explained.
3. While the method shows strong performance on benchmarks and some real-world examples, the provided examples are too idealized and lack challenging elements like interfering sound (e.g., crowd noise). I think the robustness of AV-Cloud in more complex and noisy real-world environments should also be validated.

Limitations:
The authors mention the limitations of their approach's challenges and potential drawbacks. The reliance on camera calibration and the potential issues with noise in real-world audio recordings are noted. Additional imitations can be found in the Weaknesses section

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
A novel approach for rendering high-quality spatial audio in 3D scenes, called AV-Cloud, is proposed. This method synchronizes with the visual stream without relying on or being explicitly conditioned by visual rendering, enabling immersive virtual tourism through real-time dynamic navigation of both audio and visual content. Unlike current audio-visual rendering methods that depend on visual cues and may suffer from visual artifacts causing audio inconsistencies, AV-Cloud overcomes these issues. It uses a set of sparse AV anchor points, forming an Audio-Visual Cloud derived from camera calibration, to represent the audio-visual scene. The Audio-Visual Cloud allows for the generation of spatial audio for any listener location. A novel module, Audio-Visual Cloud Splatting, decodes these AV anchor points into a spatial audio transfer function for the listener’s viewpoint, which is then applied by the Spatial Audio Render Head module to transform monaural input into viewpoint-specific spatial audio. This approach eliminates the need for pre-rendered images and efficiently aligns spatial audio with any visual viewpoint. The results are satisfying.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The AV anchors strategy seems to be interesting and effective for audio-visual scene representation. The Audio-Visual Cloud Splatting is novel for AV tasks but more likely to be a Q-former.
2. The experiment results are good and ablations are clear.

Weaknesses:
As I mentioned in the strengths, the Audio-Visual Cloud Splatting seems to be a Q-former like module.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper analyzes the estimation bias and generalization error of the expected calibration error (ECE). Specifically, in a binary classification setting, the authors provide an upper bound for the total bias with an improved convergence rate, applicable to both uniform mass and uniform width binning strategies. They also determine the optimal number of bins to minimize the total bias. Furthermore, the authors utilize the information-theoretic generalization framework, particularly the Conditional Mutual Information (CMI) framework, to characterize the generalization of ECE.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper achieves a tighter bound for total bias compared to previous works.

2. The optimal number of bins is determined using the upper bound of the total bias.

Weaknesses:
1. As the authors themselves note, a significant limitation is that the analysis in this work is only applicable to binary classification.

2. Some assumptions (e.g., Assumption 2) are not well justified.

3. The writing has significant room for improvement; several arguments are unclear or misleading.

Please find more details in the questions below.

Limitations:
The authors have thoroughly discussed some limitations of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the estimation bias in expected calibration error (ECE) for binary classification models, focusing on uniform mass binning (UMB) and uniform width binning (UWB). The authors present a comprehensive theoretical analysis, establishing upper bounds for the bias and the generalization error. Based on the convergence rates of binning and statistical bias, they identify the optimal number of bins to minimize the total estimation bias.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper provides a comprehensive analysis of the estimation bias in ECE, providing upper bounds and optimal bin size choices for both UWB and UMB.
* The authors further derive upper bounds for the generalization error between ECE and TCE using an information-theoretic approach.
* Numerical experiments on deep learning tasks confirm that the derived bounds are non-vacuous.

Weaknesses:
* The provided results only apply to binary classification, and require Lipschitz continuity which may not be necessarily satisfied in deep learning models. Also, these bounds are analyzing the ECE using test data but not training data, making them less applicable since test data are not always available in practice.
* The convergence rates of the information-theoretic generalization bounds heavily depend on the actual rate of eCMI and fCMI measures, which are not directly clear in analysis. In theorem 6, the authors show that eCMI scales as O(log n) based on metric entropy, but this bound involves the dimensionality d, and is thus hardly applicable to deep learning models.
* For experimental results, only the statistical bias is evaluated but not the total generalization error. It is also hard to see to what extent these bounds are tight in the current results. These bounds are also hard to estimate due to the existence of eCMI or fCMI measures. I would suggest the authors additionally consider some synthetic settings where TCE, eCMI, and fCMI are analytically tractable to show the tightness of the bounds. (maybe Gaussian data points?)

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies  the expected calibration error  using information-theoretical tools. They derive different tight fCMI and eCMI bounds in this setting. Empirical results show that the results are nonvacuous.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1/ The paper is in general well written. Adequate discussions are given in the main body of the paper and the appendices.

2/ The paper provides the first information-theoretic comprehensive analysis of the bias associated with the ECE when using the test and training datasets.

3/ The theoretical results seem sound. I skimmed through most of the proofs (I did not go through all of them in detail)  but the proofs are well-structured and easy to follow. 

3/ Empirical results show that the bound is tight for deep learning models.

Weaknesses:
The only weakness, if any, is perhaps that the paper uses conventional machinery for deriving information-theoretic generalization bounds and that it has not developed novel proof techniques.

Limitations:
The limitations are adequately addressed in the conclusion.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a comprehensive analysis of the estimation bias for expected calibration error (ECE), focusing on two common binning strategies: uniform mass and uniform width binning. The analysis establishes upper bounds on the bias, resulting in an improved convergence rate. Furthermore, these bounds reveal the optimal number of bins needed to minimize the estimation bias. The study also extends the bias analysis to generalization error analysis using an information-theoretic approach, deriving upper bounds that facilitate numerical evaluation for recalibration methods based on training data. Experiments with deep learning models demonstrate that the bounds are nonvacuous, due to the information-theoretic generalization analysis approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
As the author pointed out, the existing literature lacks a theoretical analysis of the estimated ECE and a more principled approach to estimation. This paper addresses and closes this gap.

Weaknesses:
1.	Tightness issue of the upper bound in Corollary 1. It is commendable that the authors included a discussion on the tightness of Equation 12. However, it would be more rigorous to formally establish a minimax lower bound for the estimation bias that applies to all types of estimators. The authors could either use existing results from Tsybakov [33] or construct a worst-case analysis using Le Cam’s method to establish the lower bound. While it is acceptable if the constant does not match the upper bound, it is crucial to demonstrate the rate.

2.	A drawback of information-theoretic (IT) bounds is the implicit dependency on the algorithm. For example, Theorem 7 appears very similar to Theorem 4, as the recalibration-induced dependence is encapsulated in the CMI term. The authors should provide more commentary on this aspect and clarify the connection between Theorems 6 and 5, as well as which bound is more practical for use.

3.	In the caption of Figure 1, It is said that the ECE gap does not change significantly in B. How can we justify that the selection of $B = n^{1/3}$ is better? Figure 1 primarily plots the bound in (14), but as I mentioned earlier, such a bound can be very loose, and more empirical justification should be provided for the selection of optimal B.

Limitations:
The Limitations are well addressed in section 7.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper analyzes the estimation bias and generalization error of the expected calibration error (ECE). Specifically, in a binary classification setting, the authors provide an upper bound for the total bias with an improved convergence rate, applicable to both uniform mass and uniform width binning strategies. They also determine the optimal number of bins to minimize the total bias. Furthermore, the authors utilize the information-theoretic generalization framework, particularly the Conditional Mutual Information (CMI) framework, to characterize the generalization of ECE.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper achieves a tighter bound for total bias compared to previous works.

2. The optimal number of bins is determined using the upper bound of the total bias.

Weaknesses:
1. As the authors themselves note, a significant limitation is that the analysis in this work is only applicable to binary classification.

2. Some assumptions (e.g., Assumption 2) are not well justified.

3. The writing has significant room for improvement; several arguments are unclear or misleading.

Please find more details in the questions below.

Limitations:
The authors have thoroughly discussed some limitations of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper investigates the estimation bias in expected calibration error (ECE) for binary classification models, focusing on uniform mass binning (UMB) and uniform width binning (UWB). The authors present a comprehensive theoretical analysis, establishing upper bounds for the bias and the generalization error. Based on the convergence rates of binning and statistical bias, they identify the optimal number of bins to minimize the total estimation bias.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper provides a comprehensive analysis of the estimation bias in ECE, providing upper bounds and optimal bin size choices for both UWB and UMB.
* The authors further derive upper bounds for the generalization error between ECE and TCE using an information-theoretic approach.
* Numerical experiments on deep learning tasks confirm that the derived bounds are non-vacuous.

Weaknesses:
* The provided results only apply to binary classification, and require Lipschitz continuity which may not be necessarily satisfied in deep learning models. Also, these bounds are analyzing the ECE using test data but not training data, making them less applicable since test data are not always available in practice.
* The convergence rates of the information-theoretic generalization bounds heavily depend on the actual rate of eCMI and fCMI measures, which are not directly clear in analysis. In theorem 6, the authors show that eCMI scales as O(log n) based on metric entropy, but this bound involves the dimensionality d, and is thus hardly applicable to deep learning models.
* For experimental results, only the statistical bias is evaluated but not the total generalization error. It is also hard to see to what extent these bounds are tight in the current results. These bounds are also hard to estimate due to the existence of eCMI or fCMI measures. I would suggest the authors additionally consider some synthetic settings where TCE, eCMI, and fCMI are analytically tractable to show the tightness of the bounds. (maybe Gaussian data points?)

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper studies  the expected calibration error  using information-theoretical tools. They derive different tight fCMI and eCMI bounds in this setting. Empirical results show that the results are nonvacuous.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1/ The paper is in general well written. Adequate discussions are given in the main body of the paper and the appendices.

2/ The paper provides the first information-theoretic comprehensive analysis of the bias associated with the ECE when using the test and training datasets.

3/ The theoretical results seem sound. I skimmed through most of the proofs (I did not go through all of them in detail)  but the proofs are well-structured and easy to follow. 

3/ Empirical results show that the bound is tight for deep learning models.

Weaknesses:
The only weakness, if any, is perhaps that the paper uses conventional machinery for deriving information-theoretic generalization bounds and that it has not developed novel proof techniques.

Limitations:
The limitations are adequately addressed in the conclusion.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a comprehensive analysis of the estimation bias for expected calibration error (ECE), focusing on two common binning strategies: uniform mass and uniform width binning. The analysis establishes upper bounds on the bias, resulting in an improved convergence rate. Furthermore, these bounds reveal the optimal number of bins needed to minimize the estimation bias. The study also extends the bias analysis to generalization error analysis using an information-theoretic approach, deriving upper bounds that facilitate numerical evaluation for recalibration methods based on training data. Experiments with deep learning models demonstrate that the bounds are nonvacuous, due to the information-theoretic generalization analysis approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
As the author pointed out, the existing literature lacks a theoretical analysis of the estimated ECE and a more principled approach to estimation. This paper addresses and closes this gap.

Weaknesses:
1.	Tightness issue of the upper bound in Corollary 1. It is commendable that the authors included a discussion on the tightness of Equation 12. However, it would be more rigorous to formally establish a minimax lower bound for the estimation bias that applies to all types of estimators. The authors could either use existing results from Tsybakov [33] or construct a worst-case analysis using Le Cam’s method to establish the lower bound. While it is acceptable if the constant does not match the upper bound, it is crucial to demonstrate the rate.

2.	A drawback of information-theoretic (IT) bounds is the implicit dependency on the algorithm. For example, Theorem 7 appears very similar to Theorem 4, as the recalibration-induced dependence is encapsulated in the CMI term. The authors should provide more commentary on this aspect and clarify the connection between Theorems 6 and 5, as well as which bound is more practical for use.

3.	In the caption of Figure 1, It is said that the ECE gap does not change significantly in B. How can we justify that the selection of $B = n^{1/3}$ is better? Figure 1 primarily plots the bound in (14), but as I mentioned earlier, such a bound can be very loose, and more empirical justification should be provided for the selection of optimal B.

Limitations:
The Limitations are well addressed in section 7.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,Yes,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The authors propose AutoPalette, which reduces color redundancy in dataset distillation. They use a palette network and color-guided initialization to enhance training efficiency and performance by minimizing redundant color information in synthetic images and datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Color redundancy is a fundamental aspect of natural scene images but is often overlooked in large-scale image analysis. This study focuses on the missing part, and the proposed method is effective.

Weaknesses:
- In the abstract, the authors summarize their framework as the one that minimizes color redundancy at the individual image and overall dataset levels. I think that’s a good summary. However, the description is not utilized when they introduce their framework in the main text. Although they describe it in the last section, it would be better to include the summary in the middle of the main, e.g., when introducing an overview or Figure 1.

- I am confused a little about the definition of the color bit in this manuscript. The authors often describe the 8-bits for the original image (e.g., Figure 2). However, if the color bit is based on the number of color palettes, the original image should have 24 bits. 

- Typo: ""> can encoded in fewer bits” should be ""can be encoded”

Limitations:
- The evaluation was mainly based on a relatively small number of image datasets. I’m not sure to what extent the condensed images change when applying recent large-scale image datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a straightforward yet effective dataset distillation method called AutoPalette. The method minimizes color redundancy at both the individual image level and the entire dataset level. At the image level, it trains the palette network by maximizing color loss and palette balance loss, thereby reducing color redundancy in images. At the dataset level, a color-guided initialization strategy is proposed to minimize color redundancy across the entire dataset. Extensive comparative and ablation experiments convincingly demonstrate the approach's effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed method outperforms other dataset distillation methods in most tasks, providing a new perspective on dataset distillation.
- The experiments and ablation study seem well done. The paper's experiments are comprehensive, and the results of the ablation studies are convincing.

Weaknesses:
-  The paper could benefit from a more detailed explanation of the color loss and palette balance loss. It would be helpful to include an explanation of why the palette balance loss might achieve a more balanced color palette.
-   The paper does not seem to explain why the similarity between the last layer gradients is measured instead of directly measuring the feature level similarity in the Color Guided Initialization Module.

Limitations:
The authors have discussed the limitations of their work in Section 5. However, they could provide more detailed descriptions of how these limitations might impact the results.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper titled introduces AutoPalette, a novel framework for dataset distillation (DD) that focuses on minimizing color redundancy at both the individual image and overall dataset levels. Authors propose a palette network to dynamically allocate colors from a reduced color space to each pixel, ensuring essential features are preserved. Additionally, a color-guided initialization strategy is developed to minimize redundancy among images, selecting representative images based on information gain. Comprehensive experiments on various datasets demonstrate the superior performance of the proposed color-aware DD compared to existing methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Color quantization is an interesting way for dataset distillation, the motivation of this paper is interesting.
2. The methodology is well-defined, with clear explanations of the palette network and the color-guided initialization strategy.
3. The framework is shown to be compatible with other DD methods, indicating its potential for broad application.

Weaknesses:
1. The paper does not discuss the potential impact of the method on the performance of larger dataset beyond the CIFAR-10 and CIFAR-100. These 2 datasets are two small and could not show the effectiveness of the proposed method.

2. There is limited exploration of how the method handles imbalanced datasets or classes with unique color distributions.

Limitations:
See weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces ColorPalette, a framework that minimizes color redundancy at the individual image and overall dataset levels. At the image level, the palette networks generate condensed images in reduced color bit-width while at the dataset level, a color-guided initialization strategy is proposed. The experiments are done using various datasets and IPCs.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. A new direction for exploring DC is proposed. 
2. AutoPalette explores the possibility of performing DC in a reduced color space.
The paper is easy to understand.

Weaknesses:
1. AutoPalette seems like it is built on top of [1] with DC loss.
2. Lack of experiment on large-scale dataset ImageNet-1K.

[1] Learning to Structure an Image with Few Colors, Yunzhong Hou et al.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The authors propose AutoPalette, which reduces color redundancy in dataset distillation. They use a palette network and color-guided initialization to enhance training efficiency and performance by minimizing redundant color information in synthetic images and datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Color redundancy is a fundamental aspect of natural scene images but is often overlooked in large-scale image analysis. This study focuses on the missing part, and the proposed method is effective.

Weaknesses:
- In the abstract, the authors summarize their framework as the one that minimizes color redundancy at the individual image and overall dataset levels. I think that’s a good summary. However, the description is not utilized when they introduce their framework in the main text. Although they describe it in the last section, it would be better to include the summary in the middle of the main, e.g., when introducing an overview or Figure 1.

- I am confused a little about the definition of the color bit in this manuscript. The authors often describe the 8-bits for the original image (e.g., Figure 2). However, if the color bit is based on the number of color palettes, the original image should have 24 bits. 

- Typo: ""> can encoded in fewer bits” should be ""can be encoded”

Limitations:
- The evaluation was mainly based on a relatively small number of image datasets. I’m not sure to what extent the condensed images change when applying recent large-scale image datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a straightforward yet effective dataset distillation method called AutoPalette. The method minimizes color redundancy at both the individual image level and the entire dataset level. At the image level, it trains the palette network by maximizing color loss and palette balance loss, thereby reducing color redundancy in images. At the dataset level, a color-guided initialization strategy is proposed to minimize color redundancy across the entire dataset. Extensive comparative and ablation experiments convincingly demonstrate the approach's effectiveness.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The proposed method outperforms other dataset distillation methods in most tasks, providing a new perspective on dataset distillation.
- The experiments and ablation study seem well done. The paper's experiments are comprehensive, and the results of the ablation studies are convincing.

Weaknesses:
-  The paper could benefit from a more detailed explanation of the color loss and palette balance loss. It would be helpful to include an explanation of why the palette balance loss might achieve a more balanced color palette.
-   The paper does not seem to explain why the similarity between the last layer gradients is measured instead of directly measuring the feature level similarity in the Color Guided Initialization Module.

Limitations:
The authors have discussed the limitations of their work in Section 5. However, they could provide more detailed descriptions of how these limitations might impact the results.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper titled introduces AutoPalette, a novel framework for dataset distillation (DD) that focuses on minimizing color redundancy at both the individual image and overall dataset levels. Authors propose a palette network to dynamically allocate colors from a reduced color space to each pixel, ensuring essential features are preserved. Additionally, a color-guided initialization strategy is developed to minimize redundancy among images, selecting representative images based on information gain. Comprehensive experiments on various datasets demonstrate the superior performance of the proposed color-aware DD compared to existing methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Color quantization is an interesting way for dataset distillation, the motivation of this paper is interesting.
2. The methodology is well-defined, with clear explanations of the palette network and the color-guided initialization strategy.
3. The framework is shown to be compatible with other DD methods, indicating its potential for broad application.

Weaknesses:
1. The paper does not discuss the potential impact of the method on the performance of larger dataset beyond the CIFAR-10 and CIFAR-100. These 2 datasets are two small and could not show the effectiveness of the proposed method.

2. There is limited exploration of how the method handles imbalanced datasets or classes with unique color distributions.

Limitations:
See weakness.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces ColorPalette, a framework that minimizes color redundancy at the individual image and overall dataset levels. At the image level, the palette networks generate condensed images in reduced color bit-width while at the dataset level, a color-guided initialization strategy is proposed. The experiments are done using various datasets and IPCs.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. A new direction for exploring DC is proposed. 
2. AutoPalette explores the possibility of performing DC in a reduced color space.
The paper is easy to understand.

Weaknesses:
1. AutoPalette seems like it is built on top of [1] with DC loss.
2. Lack of experiment on large-scale dataset ImageNet-1K.

[1] Learning to Structure an Image with Few Colors, Yunzhong Hou et al.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces BI-DiffSR, a novel binarized diffusion model for image super-resolution, designed to accelerate the inference speed and reduce computational costs of diffusion models while maintaining high performance. It proposes a UNet architecture optimized for binarization, featuring consistent-pixel downsampling/upsampling and channel-shuffle fusion to address dimension mismatch and fusion difficulty, alongside a timestep-aware redistribution and activation function to adapt to varying activation distributions across different timesteps. The model demonstrates superior results over existing binarization methods, approaching the perceptual quality of full-precision models with significantly reduced memory and computational requirements.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- The paper is well-written and easy to understand.

- This paper designs a novelty 1-bit UNet for accurate binarized diffusion model, including:
   - New downsample module and upsample module for Dimension Consistency.
   - Channel shuffle module to balance the activation value ranges of two input features.
   - The timestep-aware redistribution (TaR) and timestep-aware activation function (TaA)

- Experiments achieve the state-of-the-art in super resolution with diffusion.

Weaknesses:
- The basic BI-Conv block lacks novelty, which is as the same as the binarized module in ReActNet that contains RSign and RPReLU.

- TaR uses different parameters for different time steps, but in the mean while, the normal time embedding is projected into the resblock, it is also a time-aware on feature maps, what is the differences or why TaR works?

- SR3 is not a new diffusion baseline for super resolution, ResShift[1], SinSR[2] should be better, and the metrics of PSNR, SSIM, LPIPS is much old, the CLIPIQA, MUSIQ, MANIQA should be better for evaluating the performance of generative super resolution.

- Self-attention and MLP are common modules in diffusion, such as LDM[3] and ResShift[1], which require a lot of computation. How can the method in this paper be extended to self-attention and MLP?


[1] Yue, Zongsheng, Jianyi Wang, and Chen Change Loy. ""Resshift: Efficient diffusion model for image super-resolution by residual shifting."" Advances in Neural Information Processing Systems 36 (2024).

[2] Wang, Yufei, et al. ""SinSR: diffusion-based image super-resolution in a single step."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.

[3] Rombach, Robin, et al. ""High-resolution image synthesis with latent diffusion models."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.

Limitations:
The authors have addressed the limitations in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work present a novel binarized diffusion model for improving the efficiency of super resolution tasks. Compared with the existing works, this work first pointed out the specific challenges of binarized DMs for SR, including the dimension mismatch and fusion difficulty of representations. Then this work present several techniques: consistent-pixel down/upsampleing, channel-shuffle fusion, and Time-step-aware redistribution function for the aforementioned challenges. Comprehensive results show that the provided binarized DMs for SR not only significantly outperform the binarized models with existing SOTA binarization methods, but also achieve floating-point level performance. And for the efficiency, the statistics of params and flops show the advantage of proposed method, and the paper also present the real inference time on edge, which seems important and encouraged in the binarization community.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. As far as I know, this is the first work to present the specific binarization method for diffusion model of SR. Since the good performance has been achieved by DMs in various SR tasks, it’s important to present novel insight to compress these models, especially considering the severe drop still exists after binarizing by existing SOTA methods.
2. The motivation is intuitive and techniques are novelty, especially considering the features of DMs. The proposed CP-Up/down and channel shuffle are highly specified to the architecture of the diffusion models, which is novel and cannot be achieved by previous methods, including binarization function and binarized structures. And the computation is also small, allowing minor burden with significant performance improvement. And the proposed activation function also focus on the high dynamic of activation range during  time-step, which is one of the most critical problem for the quantization of DMs. 
3. The proposed method achieve SOTA results in accuracy. Comprehensive comparison has been included in this paper, including SOTA binarization methods and various evaluation datasets. The results show that the proposed outperforms than previous binarized DMs for SR with significant improvements.
4. In this paper, diverse analysis, including quantitative, statistical, and visual results are presented in detail. More important, the paper shows the efficiency evaluation based on real inference libraries and edge hardware, which is of great significance for practical application.

Weaknesses:
Though it’s a good paper, some issues should be addressed.
1. The writing and presentation of the paper should be improved, including but not limited to the grammar and description. For example, some basic knowledge about quantization, SR, and DMs seems to be summarized as a preliminaries section; and let the proposed techniques be highlighted in Figure 2.
2. As for the efficiency, I suggest the authors present the computation more detailed, such as present the computation of each part in the whole network before and after the binarization. This will show the efficiency advantage of the proposed method much clearer.
3. The proposed challenge I and II are insightful, but more further discussion (such as visual, quantitative, or theoretical analysis) are presented after proposing. I suggest authors do more discussion about that.
4. Some recent binarization methods for SR [1] are suggested to be compared and some quantized DMs [2] are suggested to be discussed the differences to make the comparison more comprehensive.
[1] Flexible Residual Binarization for Image Super-Resolution. ICML 2024
[2] EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models. ICLR 2024

Limitations:
The authors have addressed the limitations.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose BI-DiffSR to binarize diffusion based image super-resolution (SR) model. They design a UNet architecture for the whole binarized model structure. To maintain dimension consistency, they propose two modules, CP-Down and CP-Up, which can further help transfer full-precision information. To enhance feature fusion, they propose the channel-shuffle-fusion (CS-Fusion). They also propose TaR and TaA to dynamically adjust activation distribution cross different timesteps. The authors provide extensice experiments to demonstrate the effectiveness of their proposed method.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The topic is very important and practical. Diffusion models have shown excellent performance for image super-resolution (SR). It is very practical to quantize the models before deploying them into devices. Binarization is an extreme tool to compress the SR model. Few works have been proposed to investigate such an important problem in image SR. 

The authors give several insights for the specific topic. Namely, there are some key aspects in diffusion based image SR binarization, like dimension mismatch, fusion difficulty, and activation distraction. Those problems hinder the performance of binarized image SR diffusion models. The observation and analyses given in the introduction section are insightful and motivate readers well.

To alliveate the problems in binarized diffuision based SR models, the authors propose consistent-pixel-downsample (CP-Down) and consistent-pixel-upsample (CP-Up) to ensure dimensional consistency. They propose the channel-shuffle-fusion (CS-Fusion) to facilitate the fusion of different features within skip connections and suit binarized modules. They propose the timestep-aware redistribution (TaR) and timestep-aware activation function (TaA) to adjust the binarized module input and output arross different timesteps.

They provide extensive ablation study experiments (including quantitative results in Table 1 and visualization analyses in Figures 6 and 7.) to show the effects of each proposed components. Those experiments are convincing.

The authors provide comparions with SOTA methods. According to the main quantitive and visual comparisons, they show that their proposed BI-DiffSR achieves superior performance over others.

The overall writing and organization are pretty good. I think the work is well-prepared. The supplementary file further provides more details. The paper is easy to follow and they promise to release the code, which makes this work more convincing.

Weaknesses:
When binarizing full-precision model from 32-bit to 1-bit, ideally we can reduce the parameters by 32 times. But, as shown in Table 2, the authors reduce parameters from 55.41M to 4.58M (for scale 2). There is a gap between ideal case and practical one. Please give some analyese about the reasons for this gap. Also, are there any idea to further narrow the gap?

The parameters and Ops are reduced obviously from full-precision to binary one. But the authors did not give results about inference time on real devices or give some analyses. I am curious how fast the binarized model will be.

The writing can further refine in some cases. For example, in the abstract part (Line 9-10), “… to maintain dimension consistent” should be changed to “… to maintain dimension consistency”.

Limitations:
Please refer to weaknesses

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduce a novel binarized diffusion model, BI-DiffSR, for image SR. A UNet architecture optimized for binarization, channel shuffle fusion, and time-step-aware redistribution and activation functions are designed. The experimental results proved the effectiveness of the method.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. This paper is well written, nicely presented, and well organized.

2. Binarized diffusion networks are promising.

3. The performance improvement over other binary SR networks is significant.

Weaknesses:
1. Lack of discussion with some related works[1, 2, 3, 4], in particular [1] which is also for binary SR networks. Please analyze and discuss the differences with [1,2].

2. Ablation experiments are not convincing enough. Comparisons with some other activation function or fusion methods [1, 2, 3, 4] should be included.

3. It is always known that diffusion models are slow. Although binarization will speed up the operation, can it achieve a better trade-off in performance and efficiency than a real-valued efficient SR network. It is suggested to compare with some efficient SR networks [5, 6, 7] in terms of Params, FLOPs, inference time and performance.

> 1. Flexible Residual Binarization for Image Super-Resolution. ICML24.

> 2. Q-DM: An Efficient Low-bit Quantized Diffusion Model. NIPS23. 

> 3. Binarized Low-light Raw Video Enhancement. ICCV23.

> 4. Binarized Spectral Compressive Imaging. NIPS23.

> 5. Efficient long-range attention network for image super-resolution. ECCV22.

> 6. DLGSANet: lightweight dynamic local and global self-attention networks for image super-resolution. ICCV23.

> 7. Feature modulation transformer: Cross-refinement of global representation via high-frequency prior for image super-resolution. ICCV23.

Limitations:
Limitations were discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


",Yes,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces BI-DiffSR, a novel binarized diffusion model for image super-resolution, designed to accelerate the inference speed and reduce computational costs of diffusion models while maintaining high performance. It proposes a UNet architecture optimized for binarization, featuring consistent-pixel downsampling/upsampling and channel-shuffle fusion to address dimension mismatch and fusion difficulty, alongside a timestep-aware redistribution and activation function to adapt to varying activation distributions across different timesteps. The model demonstrates superior results over existing binarization methods, approaching the perceptual quality of full-precision models with significantly reduced memory and computational requirements.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
- The paper is well-written and easy to understand.

- This paper designs a novelty 1-bit UNet for accurate binarized diffusion model, including:
   - New downsample module and upsample module for Dimension Consistency.
   - Channel shuffle module to balance the activation value ranges of two input features.
   - The timestep-aware redistribution (TaR) and timestep-aware activation function (TaA)

- Experiments achieve the state-of-the-art in super resolution with diffusion.

Weaknesses:
- The basic BI-Conv block lacks novelty, which is as the same as the binarized module in ReActNet that contains RSign and RPReLU.

- TaR uses different parameters for different time steps, but in the mean while, the normal time embedding is projected into the resblock, it is also a time-aware on feature maps, what is the differences or why TaR works?

- SR3 is not a new diffusion baseline for super resolution, ResShift[1], SinSR[2] should be better, and the metrics of PSNR, SSIM, LPIPS is much old, the CLIPIQA, MUSIQ, MANIQA should be better for evaluating the performance of generative super resolution.

- Self-attention and MLP are common modules in diffusion, such as LDM[3] and ResShift[1], which require a lot of computation. How can the method in this paper be extended to self-attention and MLP?


[1] Yue, Zongsheng, Jianyi Wang, and Chen Change Loy. ""Resshift: Efficient diffusion model for image super-resolution by residual shifting."" Advances in Neural Information Processing Systems 36 (2024).

[2] Wang, Yufei, et al. ""SinSR: diffusion-based image super-resolution in a single step."" Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition. 2024.

[3] Rombach, Robin, et al. ""High-resolution image synthesis with latent diffusion models."" Proceedings of the IEEE/CVF conference on computer vision and pattern recognition. 2022.

Limitations:
The authors have addressed the limitations in the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work present a novel binarized diffusion model for improving the efficiency of super resolution tasks. Compared with the existing works, this work first pointed out the specific challenges of binarized DMs for SR, including the dimension mismatch and fusion difficulty of representations. Then this work present several techniques: consistent-pixel down/upsampleing, channel-shuffle fusion, and Time-step-aware redistribution function for the aforementioned challenges. Comprehensive results show that the provided binarized DMs for SR not only significantly outperform the binarized models with existing SOTA binarization methods, but also achieve floating-point level performance. And for the efficiency, the statistics of params and flops show the advantage of proposed method, and the paper also present the real inference time on edge, which seems important and encouraged in the binarization community.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. As far as I know, this is the first work to present the specific binarization method for diffusion model of SR. Since the good performance has been achieved by DMs in various SR tasks, it’s important to present novel insight to compress these models, especially considering the severe drop still exists after binarizing by existing SOTA methods.
2. The motivation is intuitive and techniques are novelty, especially considering the features of DMs. The proposed CP-Up/down and channel shuffle are highly specified to the architecture of the diffusion models, which is novel and cannot be achieved by previous methods, including binarization function and binarized structures. And the computation is also small, allowing minor burden with significant performance improvement. And the proposed activation function also focus on the high dynamic of activation range during  time-step, which is one of the most critical problem for the quantization of DMs. 
3. The proposed method achieve SOTA results in accuracy. Comprehensive comparison has been included in this paper, including SOTA binarization methods and various evaluation datasets. The results show that the proposed outperforms than previous binarized DMs for SR with significant improvements.
4. In this paper, diverse analysis, including quantitative, statistical, and visual results are presented in detail. More important, the paper shows the efficiency evaluation based on real inference libraries and edge hardware, which is of great significance for practical application.

Weaknesses:
Though it’s a good paper, some issues should be addressed.
1. The writing and presentation of the paper should be improved, including but not limited to the grammar and description. For example, some basic knowledge about quantization, SR, and DMs seems to be summarized as a preliminaries section; and let the proposed techniques be highlighted in Figure 2.
2. As for the efficiency, I suggest the authors present the computation more detailed, such as present the computation of each part in the whole network before and after the binarization. This will show the efficiency advantage of the proposed method much clearer.
3. The proposed challenge I and II are insightful, but more further discussion (such as visual, quantitative, or theoretical analysis) are presented after proposing. I suggest authors do more discussion about that.
4. Some recent binarization methods for SR [1] are suggested to be compared and some quantized DMs [2] are suggested to be discussed the differences to make the comparison more comprehensive.
[1] Flexible Residual Binarization for Image Super-Resolution. ICML 2024
[2] EfficientDM: Efficient Quantization-Aware Fine-Tuning of Low-Bit Diffusion Models. ICLR 2024

Limitations:
The authors have addressed the limitations.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose BI-DiffSR to binarize diffusion based image super-resolution (SR) model. They design a UNet architecture for the whole binarized model structure. To maintain dimension consistency, they propose two modules, CP-Down and CP-Up, which can further help transfer full-precision information. To enhance feature fusion, they propose the channel-shuffle-fusion (CS-Fusion). They also propose TaR and TaA to dynamically adjust activation distribution cross different timesteps. The authors provide extensice experiments to demonstrate the effectiveness of their proposed method.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The topic is very important and practical. Diffusion models have shown excellent performance for image super-resolution (SR). It is very practical to quantize the models before deploying them into devices. Binarization is an extreme tool to compress the SR model. Few works have been proposed to investigate such an important problem in image SR. 

The authors give several insights for the specific topic. Namely, there are some key aspects in diffusion based image SR binarization, like dimension mismatch, fusion difficulty, and activation distraction. Those problems hinder the performance of binarized image SR diffusion models. The observation and analyses given in the introduction section are insightful and motivate readers well.

To alliveate the problems in binarized diffuision based SR models, the authors propose consistent-pixel-downsample (CP-Down) and consistent-pixel-upsample (CP-Up) to ensure dimensional consistency. They propose the channel-shuffle-fusion (CS-Fusion) to facilitate the fusion of different features within skip connections and suit binarized modules. They propose the timestep-aware redistribution (TaR) and timestep-aware activation function (TaA) to adjust the binarized module input and output arross different timesteps.

They provide extensive ablation study experiments (including quantitative results in Table 1 and visualization analyses in Figures 6 and 7.) to show the effects of each proposed components. Those experiments are convincing.

The authors provide comparions with SOTA methods. According to the main quantitive and visual comparisons, they show that their proposed BI-DiffSR achieves superior performance over others.

The overall writing and organization are pretty good. I think the work is well-prepared. The supplementary file further provides more details. The paper is easy to follow and they promise to release the code, which makes this work more convincing.

Weaknesses:
When binarizing full-precision model from 32-bit to 1-bit, ideally we can reduce the parameters by 32 times. But, as shown in Table 2, the authors reduce parameters from 55.41M to 4.58M (for scale 2). There is a gap between ideal case and practical one. Please give some analyese about the reasons for this gap. Also, are there any idea to further narrow the gap?

The parameters and Ops are reduced obviously from full-precision to binary one. But the authors did not give results about inference time on real devices or give some analyses. I am curious how fast the binarized model will be.

The writing can further refine in some cases. For example, in the abstract part (Line 9-10), “… to maintain dimension consistent” should be changed to “… to maintain dimension consistency”.

Limitations:
Please refer to weaknesses

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduce a novel binarized diffusion model, BI-DiffSR, for image SR. A UNet architecture optimized for binarization, channel shuffle fusion, and time-step-aware redistribution and activation functions are designed. The experimental results proved the effectiveness of the method.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
1. This paper is well written, nicely presented, and well organized.

2. Binarized diffusion networks are promising.

3. The performance improvement over other binary SR networks is significant.

Weaknesses:
1. Lack of discussion with some related works[1, 2, 3, 4], in particular [1] which is also for binary SR networks. Please analyze and discuss the differences with [1,2].

2. Ablation experiments are not convincing enough. Comparisons with some other activation function or fusion methods [1, 2, 3, 4] should be included.

3. It is always known that diffusion models are slow. Although binarization will speed up the operation, can it achieve a better trade-off in performance and efficiency than a real-valued efficient SR network. It is suggested to compare with some efficient SR networks [5, 6, 7] in terms of Params, FLOPs, inference time and performance.

> 1. Flexible Residual Binarization for Image Super-Resolution. ICML24.

> 2. Q-DM: An Efficient Low-bit Quantized Diffusion Model. NIPS23. 

> 3. Binarized Low-light Raw Video Enhancement. ICCV23.

> 4. Binarized Spectral Compressive Imaging. NIPS23.

> 5. Efficient long-range attention network for image super-resolution. ECCV22.

> 6. DLGSANet: lightweight dynamic local and global self-attention networks for image super-resolution. ICCV23.

> 7. Feature modulation transformer: Cross-refinement of global representation via high-frequency prior for image super-resolution. ICCV23.

Limitations:
Limitations were discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


Label: ",Yes,yes,Yes,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
In their paper, the authors introduce RE-CONTROL, a novel approach designed to align Large Language Models (LLMs) through representation editing. They view LLMs as discrete-time stochastic dynamical systems and propose the insertion of control signals into the internal representations. This technique allows for precise manipulation of the model's outputs during test time token by token. The experiments show that this method increases the win rate on HH dataset and does not need significant inference time.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Viewing LLMs as a dynamical system and interpret the steering vector as a kind of controlling signal to align models is innovative.
- Make LLMs adjustable during the generation process, and the evaluation does not have to wait until the entire sentence is generated.
- They empirically show that their method outperform some test-time alignment methods and does not need significant inference time, which makes the method be more practical usable.

Weaknesses:
- Some parts of the paper are confusing, especially certain expressions. For example, they did not clarify some notations like a_t, V_{phi} etc.. The legend in figure 1 seems mismatched. And some figures are not mentioned in the paper.
- I think the performance of this method is highly depend on the value model. However, the paper does not discuss the reliability of the value model, which is crucial since it needs to assess the alignment effectiveness of the entire result based on each newly generated token and do so before the results are generated.
- The theoretical analysis and interpretation of their method is interesting, but lack rigor. e.g. the generated token (y_t) should be determined by logits (o_t), which is a part of state in the dynamic system. However, the paper interprets the generated token as kind of random variable or random noise (w_t).

Limitations:
Limitation are sufficiently discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper suggests editing language model features for alignment tasks. The authors first learn a value function of a language model from a human-preference dataset. They then increment feature representations in model layers to maximize test-time utility. Empirical evidence shows that this feature editing method surpasses both test-time and training-time alignment baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed method, RE-CONTROL, is a useful middle ground between current training-time and test-time alignment methods:

- RE-CONTROL, unlike existing training-time methods, does not alter a language model’s parameters, reducing training costs. Instead, it learns a value function offline. 

- RE-CONTROL, unlike existing test-time methods, employs a learned value function to inject feature increments into features of language models.

The experiments are extensive in that they compared RE-CONTROL with both training-time and test-time alignment methods.

Weaknesses:
While the paper is technically well-executed, I believe it has three main limitations:  (i) the lack of compute--performance tradeoff analysis (ii) the lack of details in comparing RE-CONTROL with training-time alignment baselines. (iii) the limitation in application scope.

First, a compute-performance tradeoff analysis would clarify the behavior of RE-CONTROL. RE-CONTROL is more compute-intensive than other test-time decoding alternatives because it requires gradient ascent steps at decoding time (Section 4.4). These steps add up and can become quite intensive for generating long text. Therefore, comparing RE-CONTROL with test-time alignment alternatives while considering compute time would be informative. For instance, the authors could display the win rate of different test-time decoding methods on the y-axis and their wallclock time on the x-axis.

Second, I think the performance comparison between RE-CONTROL and training-time alignment methods in Section 6.1 seems very preliminary. There, the authors empirically show that the test-time alignment method RE-CONTROL *outperforms* training-time alignment methods like PPO, by concluding that

>We observe that RE-CONTROL achieves a higher GPT-4 win rate and average reward compared to both PPO and DPO. Furthermore, RE-CONTROL also outperforms these methods in terms of diversity and coherence.

I'm puzzled by how to interpret the results here. Should the take-home message here be ""Decoding-time RE-CONTROL is better than training-time PPO in alignment. Period."" or are there qualifications to this statement? I strongly suspect that some qualification is needed. To some extent, RE-CONTROL is a decoding-time approximation of PPO. Both methods use a learned value function to steer the model's behavior. At decoding time, RE-CONTROL does this in a more lossy (due to test-time gradient ascent) and shallower (because not all parameters are updated) way. Thus, with adequate training, I expected PPO to yield better results than RE-CONTROL. Note that this doesn't undermine RE-CONTROL's capability, as it is more lightweight than PPO. 


Thirdly, while RE-CONTROL is technically sound, its application scope seems narrow. To my understanding, RE-CONTROL is most appealing to users who are unwilling to train a language model offline, who are willing to train a value function offline, who aim to save computing power during training, and who don't mind using more compute during decoding. These intersections of users seem limiting. This raises the question: Is it better to simply use a similar compute budget for efficient alignment (e.g., LoRa) of the LM model using standard methods (DPO, PPO, etc.) and avoid ongoing compute costs during decoding?

Limitations:
See the ""Weaknesses"" section above.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces an alternative procedure for LLM alignment that does not fine-tune LLM weights, but instead learns a separate value function that is used to update hidden states. The value function is learned using a variation of temporal difference, then applied at inference time to modify hidden states by gradient ascent, maximizing the predicted state value. Authors evaluate their approach with multiple 7B LLMs on HH-RLHF data, comparing against both RLHF and training-free baselines. The paper also analyzes OOD generalization to HarmfulQA.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Authors propose an interesting approach to that can be used to alter LLM behavior in general
- When experimenting with HH-RLHF dataset, authors evaluate against multiple types of baselines and provide additional analysis that was interesting to read
- The paper is generally well-written and easy to follow
- Authors made the code available, in a (mostly) serviceable state

Weaknesses:
**1a. Motivation for the choice of baselines.**

In your work, you cite, among others, ARGS[26], DeAL [22], Value Augmented Sampling [21] that also learn value functions and use them to steer model outputs (in other ways), but, to the best of my knowledge, you do not compare against them as baselines, instead choosing a relatively older work on controlled decoding. While [21] may be dismissed as concurrent work, the other works appear to be a relevant alternative and it is not clear why they were not chosen as baselines.

If there is a reason why these works will, beyond reasonable doubt, fail at the task that you evaluate on, I would recommend that you explain this in the paper. If there is no such reason, the paper would benefit from comparing against them.

**1b. Motivation for the choice of models**

Your paper focuses on Llama, Vicuna and Falcon models, of the 7B variety. While these are indeed LLMs, the original Llama was released circa 1.5 years ago and since then, LLMs improved **significantly** across tasks.
Picking older LLMs appears counterintuitive, as their generally worse quality makes it harder to measure possible drawdowns introduced by LLM alignment.

If you have a reason for choosing these models, please explain why you focus on older LLMs as compared to, for example, Llama 3 8B (or 70B), Qwen2, Gemma or other models near the top of https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard . If there is no such reason, the paper would benefit from switching to more accurate models.

**2. Inference time exploration**

LLM use cases are often sensitive to inference throughput (tokens per second) and latency (time to first / next token).

To the best of my understanding, RE-Control introduces an iterative optimization step to each forward pass during autoregressive inference. Depending on the configuration, this may result in a significant slowdown, which may limit the practical significance of your approach.

I would argue that the work would benefit from analyzing this difference in speed in different settings (e.g. single-sequence vs batch inference, etc).

**3.  Main experiments are limited to one dataset and relatively small past generation LLMs, ranked by GPT-4**

This is definitely not a fault on authors' side, but the paper makes its main conclusions based on 7B models, using reward functions trained on a single dataset. This could result in accidental false conclusions if it turns out that, for instance, RE-Control harms the quality of stronger models or if it is somehow implicitly overfitting on on GPT4 opinions.

The standard way to minimize this risk is to diversify the experiments: try alternative alignment datasets (e.g. webgpt_comparisons, oasst1, etc), try larger models (llama-3 70B), introduce human rankings in some setups, etc. I understand that not all of these evaluations may be available to the authors, but for a NeurIPS publication, I would expect more variation in the experiments and, if there is a confounder that could not be eliminated (e.g. using GPT4 and no human eval), it should be stated among the limitations section.

Limitations:
The ""Limitations and future work"" appendix can be significantly improved. Currently, it focuses on future work and omits some limitations of the experiments, such as:
- using GPT-4 as the primary metric will make the results irreproducible once OpenAI cycles out GPT4, a closed-source model
- evaluating only on relatively weaker models (pre-previous gen, 7B) may miss some caveats or synergies from more capable LLMs
- using a single training dataset makes it possible that the proposed method is uniquely powerful in this one scenario but not others

The quality of the limitation section did not affect my score.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper ""Aligning Large Language Models with Representation Editing: A Control Perspective"" proposes a method for aligning large language models (LLMs) with human objectives through representation editing. Unlike fine-tuning, which is resource-intensive and unstable, or test-time alignment techniques like prompting that rely on the original model's capabilities, this method introduces external control signals into the hidden states of a pre-trained LLM. The method treats the LLM as a discrete-time stochastic dynamical system and applies control theory to train a value function on the hidden states, optimizing control signals at test time. The experiments show that this method, named RE-CONTROL, outperforms existing test-time alignment techniques and requires fewer resources compared to fine-tuning methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Innovative Approach: The use of control theory to introduce control signals into the hidden states of LLMs is novel and provides a new perspective on alignment.
Resource Efficiency: RE-CONTROL is less resource-intensive than traditional fine-tuning methods, making it more practical for large-scale applications.
Empirical Success: The experiments demonstrate that RE-CONTROL outperforms existing test-time alignment methods, showing strong generalization and alignment capabilities.
Flexibility: The method offers more flexibility than prompting or guided decoding as it perturbs the representation space dynamically during the generation process

Weaknesses:
Complexity: The method involves sophisticated control theory and optimization techniques, which might be challenging to implement and understand for practitioners without a strong background in these areas.
Dependency on Value Function: The success of the method heavily relies on the accuracy and training of the value function, which might introduce additional challenges in terms of training and performance.

Limitations:
Limited Scope: The paper primarily focuses on aligning LLMs for helpfulness and minimizing harmfulness. It might not address other important alignment objectives comprehensively.
Potential Overfitting: The reliance on a specific value function and control signals might lead to overfitting to the training data or specific tasks, limiting the method's generalizability.
Evaluation Metrics: The evaluation metrics, while comprehensive, might not capture all aspects of alignment, especially in diverse and dynamic real-world scenarios.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
In their paper, the authors introduce RE-CONTROL, a novel approach designed to align Large Language Models (LLMs) through representation editing. They view LLMs as discrete-time stochastic dynamical systems and propose the insertion of control signals into the internal representations. This technique allows for precise manipulation of the model's outputs during test time token by token. The experiments show that this method increases the win rate on HH dataset and does not need significant inference time.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
- Viewing LLMs as a dynamical system and interpret the steering vector as a kind of controlling signal to align models is innovative.
- Make LLMs adjustable during the generation process, and the evaluation does not have to wait until the entire sentence is generated.
- They empirically show that their method outperform some test-time alignment methods and does not need significant inference time, which makes the method be more practical usable.

Weaknesses:
- Some parts of the paper are confusing, especially certain expressions. For example, they did not clarify some notations like a_t, V_{phi} etc.. The legend in figure 1 seems mismatched. And some figures are not mentioned in the paper.
- I think the performance of this method is highly depend on the value model. However, the paper does not discuss the reliability of the value model, which is crucial since it needs to assess the alignment effectiveness of the entire result based on each newly generated token and do so before the results are generated.
- The theoretical analysis and interpretation of their method is interesting, but lack rigor. e.g. the generated token (y_t) should be determined by logits (o_t), which is a part of state in the dynamic system. However, the paper interprets the generated token as kind of random variable or random noise (w_t).

Limitations:
Limitation are sufficiently discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper suggests editing language model features for alignment tasks. The authors first learn a value function of a language model from a human-preference dataset. They then increment feature representations in model layers to maximize test-time utility. Empirical evidence shows that this feature editing method surpasses both test-time and training-time alignment baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed method, RE-CONTROL, is a useful middle ground between current training-time and test-time alignment methods:

- RE-CONTROL, unlike existing training-time methods, does not alter a language model’s parameters, reducing training costs. Instead, it learns a value function offline. 

- RE-CONTROL, unlike existing test-time methods, employs a learned value function to inject feature increments into features of language models.

The experiments are extensive in that they compared RE-CONTROL with both training-time and test-time alignment methods.

Weaknesses:
While the paper is technically well-executed, I believe it has three main limitations:  (i) the lack of compute--performance tradeoff analysis (ii) the lack of details in comparing RE-CONTROL with training-time alignment baselines. (iii) the limitation in application scope.

First, a compute-performance tradeoff analysis would clarify the behavior of RE-CONTROL. RE-CONTROL is more compute-intensive than other test-time decoding alternatives because it requires gradient ascent steps at decoding time (Section 4.4). These steps add up and can become quite intensive for generating long text. Therefore, comparing RE-CONTROL with test-time alignment alternatives while considering compute time would be informative. For instance, the authors could display the win rate of different test-time decoding methods on the y-axis and their wallclock time on the x-axis.

Second, I think the performance comparison between RE-CONTROL and training-time alignment methods in Section 6.1 seems very preliminary. There, the authors empirically show that the test-time alignment method RE-CONTROL *outperforms* training-time alignment methods like PPO, by concluding that

>We observe that RE-CONTROL achieves a higher GPT-4 win rate and average reward compared to both PPO and DPO. Furthermore, RE-CONTROL also outperforms these methods in terms of diversity and coherence.

I'm puzzled by how to interpret the results here. Should the take-home message here be ""Decoding-time RE-CONTROL is better than training-time PPO in alignment. Period."" or are there qualifications to this statement? I strongly suspect that some qualification is needed. To some extent, RE-CONTROL is a decoding-time approximation of PPO. Both methods use a learned value function to steer the model's behavior. At decoding time, RE-CONTROL does this in a more lossy (due to test-time gradient ascent) and shallower (because not all parameters are updated) way. Thus, with adequate training, I expected PPO to yield better results than RE-CONTROL. Note that this doesn't undermine RE-CONTROL's capability, as it is more lightweight than PPO. 


Thirdly, while RE-CONTROL is technically sound, its application scope seems narrow. To my understanding, RE-CONTROL is most appealing to users who are unwilling to train a language model offline, who are willing to train a value function offline, who aim to save computing power during training, and who don't mind using more compute during decoding. These intersections of users seem limiting. This raises the question: Is it better to simply use a similar compute budget for efficient alignment (e.g., LoRa) of the LM model using standard methods (DPO, PPO, etc.) and avoid ongoing compute costs during decoding?

Limitations:
See the ""Weaknesses"" section above.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces an alternative procedure for LLM alignment that does not fine-tune LLM weights, but instead learns a separate value function that is used to update hidden states. The value function is learned using a variation of temporal difference, then applied at inference time to modify hidden states by gradient ascent, maximizing the predicted state value. Authors evaluate their approach with multiple 7B LLMs on HH-RLHF data, comparing against both RLHF and training-free baselines. The paper also analyzes OOD generalization to HarmfulQA.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- Authors propose an interesting approach to that can be used to alter LLM behavior in general
- When experimenting with HH-RLHF dataset, authors evaluate against multiple types of baselines and provide additional analysis that was interesting to read
- The paper is generally well-written and easy to follow
- Authors made the code available, in a (mostly) serviceable state

Weaknesses:
**1a. Motivation for the choice of baselines.**

In your work, you cite, among others, ARGS[26], DeAL [22], Value Augmented Sampling [21] that also learn value functions and use them to steer model outputs (in other ways), but, to the best of my knowledge, you do not compare against them as baselines, instead choosing a relatively older work on controlled decoding. While [21] may be dismissed as concurrent work, the other works appear to be a relevant alternative and it is not clear why they were not chosen as baselines.

If there is a reason why these works will, beyond reasonable doubt, fail at the task that you evaluate on, I would recommend that you explain this in the paper. If there is no such reason, the paper would benefit from comparing against them.

**1b. Motivation for the choice of models**

Your paper focuses on Llama, Vicuna and Falcon models, of the 7B variety. While these are indeed LLMs, the original Llama was released circa 1.5 years ago and since then, LLMs improved **significantly** across tasks.
Picking older LLMs appears counterintuitive, as their generally worse quality makes it harder to measure possible drawdowns introduced by LLM alignment.

If you have a reason for choosing these models, please explain why you focus on older LLMs as compared to, for example, Llama 3 8B (or 70B), Qwen2, Gemma or other models near the top of https://huggingface.co/spaces/open-llm-leaderboard/open_llm_leaderboard . If there is no such reason, the paper would benefit from switching to more accurate models.

**2. Inference time exploration**

LLM use cases are often sensitive to inference throughput (tokens per second) and latency (time to first / next token).

To the best of my understanding, RE-Control introduces an iterative optimization step to each forward pass during autoregressive inference. Depending on the configuration, this may result in a significant slowdown, which may limit the practical significance of your approach.

I would argue that the work would benefit from analyzing this difference in speed in different settings (e.g. single-sequence vs batch inference, etc).

**3.  Main experiments are limited to one dataset and relatively small past generation LLMs, ranked by GPT-4**

This is definitely not a fault on authors' side, but the paper makes its main conclusions based on 7B models, using reward functions trained on a single dataset. This could result in accidental false conclusions if it turns out that, for instance, RE-Control harms the quality of stronger models or if it is somehow implicitly overfitting on on GPT4 opinions.

The standard way to minimize this risk is to diversify the experiments: try alternative alignment datasets (e.g. webgpt_comparisons, oasst1, etc), try larger models (llama-3 70B), introduce human rankings in some setups, etc. I understand that not all of these evaluations may be available to the authors, but for a NeurIPS publication, I would expect more variation in the experiments and, if there is a confounder that could not be eliminated (e.g. using GPT4 and no human eval), it should be stated among the limitations section.

Limitations:
The ""Limitations and future work"" appendix can be significantly improved. Currently, it focuses on future work and omits some limitations of the experiments, such as:
- using GPT-4 as the primary metric will make the results irreproducible once OpenAI cycles out GPT4, a closed-source model
- evaluating only on relatively weaker models (pre-previous gen, 7B) may miss some caveats or synergies from more capable LLMs
- using a single training dataset makes it possible that the proposed method is uniquely powerful in this one scenario but not others

The quality of the limitation section did not affect my score.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper ""Aligning Large Language Models with Representation Editing: A Control Perspective"" proposes a method for aligning large language models (LLMs) with human objectives through representation editing. Unlike fine-tuning, which is resource-intensive and unstable, or test-time alignment techniques like prompting that rely on the original model's capabilities, this method introduces external control signals into the hidden states of a pre-trained LLM. The method treats the LLM as a discrete-time stochastic dynamical system and applies control theory to train a value function on the hidden states, optimizing control signals at test time. The experiments show that this method, named RE-CONTROL, outperforms existing test-time alignment techniques and requires fewer resources compared to fine-tuning methods.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
Innovative Approach: The use of control theory to introduce control signals into the hidden states of LLMs is novel and provides a new perspective on alignment.
Resource Efficiency: RE-CONTROL is less resource-intensive than traditional fine-tuning methods, making it more practical for large-scale applications.
Empirical Success: The experiments demonstrate that RE-CONTROL outperforms existing test-time alignment methods, showing strong generalization and alignment capabilities.
Flexibility: The method offers more flexibility than prompting or guided decoding as it perturbs the representation space dynamically during the generation process

Weaknesses:
Complexity: The method involves sophisticated control theory and optimization techniques, which might be challenging to implement and understand for practitioners without a strong background in these areas.
Dependency on Value Function: The success of the method heavily relies on the accuracy and training of the value function, which might introduce additional challenges in terms of training and performance.

Limitations:
Limited Scope: The paper primarily focuses on aligning LLMs for helpfulness and minimizing harmfulness. It might not address other important alignment objectives comprehensively.
Potential Overfitting: The reliance on a specific value function and control signals might lead to overfitting to the training data or specific tasks, limiting the method's generalizability.
Evaluation Metrics: The evaluation metrics, while comprehensive, might not capture all aspects of alignment, especially in diverse and dynamic real-world scenarios.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper concerns the data selection problem: given a collection of $N$ embeddings of dimension $r$ for $r\gg N$, the goal is to pick a subset $S$ of points of size $n$ so that one could run any downstream algorithm on $S$ with a regularization term, so that the empirical risk is small even on the entire finetuning set. Assuming the model is $y=\phi(X) \theta_*+z$ where $\phi: \mathbb{R}^d\rightarrow \mathbb{R}^r$ and $z$ is an i.i.d. noise vector with zero mean and bounded variance, then there exists a subspace that one could project onto and decompose the empirical risk as a bias and a variance term. Further, under the assumption that the second moment matrix has low intrinsic dimension, then one could find a good subspace via gradient sketching: draw a JL matrix $\Gamma\in \mathbb{R}^{r\times m}$ for $m\ll r$, then as long as one has $\Gamma^\top \Sigma^{\phi} \Gamma \preceq c_S \cdot \Gamma^\top \Sigma^{\phi}_S \Gamma$, then the error could be decomposed into a bias, variance and a sketching error term. A sketching gradient, moment-matching algorithm is proposed, involves applying sketching to the gradient, form the Jacobian and solve a quadratic relaxation. Experiments are performed on both synthetic datasets and CIFAR10.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The main theoretical contribution is that for over-parametrized setting where $r\gg n$, one could provably show the existence of a subspace that one could project onto and perform data selection on that subspace. Moreover, if the second moment in addition has low intrinsic dimension, then one could use standard dimensionality reduction techniques (in $\ell_2$ norm) to sketch the high-dimensional gradient. In the sketchy moment-matching algorithm proposed in the paper, the authors first sketch the gradient then use uniform sampling  to construct $S$.

Weaknesses:
The core results of this paper are not technically very novel and surprising, the algorithm could be interpreted as a generalization of the leverage score sampling via JL trick due to Spielman and Srivastava, STOC'08. The analysis largely draws inspirations from the over-parametrization literature, which makes sense as finetuning is essentially training in an over-parametrized setting. Another point that is a bit unsatisfactory is the sketchy moment-matching algorithm utilizes quadratic relaxation to solve the program efficiently with projected gradient descent, but all analysis is based upon *not solving the quadratic programs*. The authors should try to provide some theoretical justifications of sketchy moment-matching, as that's one of the key contributions of this paper.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the problem of data selection for finetuning large pre-trained models. The key contributions are:

1. A theoretical analysis of data selection for finetuning that reveals a variance-bias tradeoff in high dimensions.
2. A provable result showing that gradient sketching can efficiently find a low-dimensional subspace that preserves fast-rate generalization.
3. A practical two-stage algorithm called Sketchy Moment Matching (SkMM) that uses gradient sketching to explore the parameter space and moment matching to exploit the low-dimensional structure.
4. Empirical validation on synthetic and real datasets demonstrating the effectiveness of the approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper provides a rigorous generalization analysis for data selection in both low and high-dimensional settings. The proofs are detailed and appear sound.
2. The proposed SkMM method is simple to implement and scalable to large models/datasets. Experiments on both synthetic and real data demonstrate the effectiveness of the approach.

Weaknesses:
1. Some of the theoretical results rely on assumptions (e.g., low intrinsic dimensionality) that may not always hold in practice. More discussion of the implications when these assumptions are violated would be valuable.
2. The method introduces new hyperparameters (e.g., sketching dimension, moment matching strength) without much guidance on how to set them optimally.

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study the task of data selection. They extend the classical variance reduction to the high dimensional case and provide a variance-bias tradeoff analysis. Based on the theoretical results, they propose sketchy moment matching, which first utilizes gradient sketchy to form a low-dimensional space and then uses moment matching to reduce the variance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposal is a reasonable improvement over the baselines which often only consider bias or variance reduction. The theoretical analysis is also a decent contribution of the paper.

Weaknesses:
The experiment focuses on linear probing, which already limits the scope of the evaluation. Furthermore, even under this limited scope, the setting does not seem to be challenging. For the synthetic setup, the sample count is 2000 while the rank is 2500, so it seems not to be a very high-dimension setup (the rank is not so much larger than the sample count). Also, the cluster count seems to be low for both tasks, 8 for synthetic, while the number of class is 10 for Cifar-10.

Limitations:
Yes, the authors discuss the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the problem of data selection in the over-parametrized fine-tuning regime, i.e. when the number of fine-tuning parameters $r$ is larger than the amount $N$ of available examples. We want to subsample $n\ll N$ examples that form a representative set to train on, and hopefully achieve quality as close as possible to fine-tuning on the whole set.

The idea is to compute the gradients $G\in \mathbb{R}^{N\times r}$ of all examples wrt the fine-tuning params and then select a subsample $S\subseteq [N]$ such that the Gram matrix of the gradients is approximated: $c\cdot \Sigma_S := c \cdot G^\top I_S G \approx G^\top G := \Sigma$. However, this is not possible to achieve since the model is over-parameterized. Fortunately, if the spectral approximation holds on a low-dimensional subspace of the parameter space, this is good enough, so the authors project the gradients on a random low-dimensional space. The proof goes through under the assumption that the singular values of the gradient matrix are well-concentrated on a small enough (<10%) support.

The experimental results include fine-tuning on a synthetic linear task, as well as fine-tuning a vision transformer on CIFAR-10 image classification.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The authors study the data selection for fine-tuning problem from first principles
- The writing is overall good and math looks sound, even though I didn't check details.
- The experimental results look promising since SkMM beats a variety of algorithms including leverage scores.
- The idea of spectral approximation on a subspace of the parameter space is interesting.

Weaknesses:
- Important details on the experimental setup are missing or unclear. Specifically, what is the optimization process after the data is subsampled? For the image classification experiments, what is being fine-tuned, is it all the ViT parameters? For how many epochs? 
- The algorithm requires computing the gradients of all samples, which can be computationally expensive. Besides, if we are computing all gradients, why can't we just train one epoch on all datapoints? Why is data selection useful in this case?
- The literature review could be expanded, including relevant papers such as BADGE [1], Coreset-based sensitivity sampling [2].
- In the experimental results, the authors should also compare with margin sampling (in addition to entropy sampling), as well as uniform sampling for the image classification task.
- Computing the moment-matching subset in Algorithm 3.1 seems overly complicated, see questions

[1]: Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds

[2]: Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper concerns the data selection problem: given a collection of $N$ embeddings of dimension $r$ for $r\gg N$, the goal is to pick a subset $S$ of points of size $n$ so that one could run any downstream algorithm on $S$ with a regularization term, so that the empirical risk is small even on the entire finetuning set. Assuming the model is $y=\phi(X) \theta_*+z$ where $\phi: \mathbb{R}^d\rightarrow \mathbb{R}^r$ and $z$ is an i.i.d. noise vector with zero mean and bounded variance, then there exists a subspace that one could project onto and decompose the empirical risk as a bias and a variance term. Further, under the assumption that the second moment matrix has low intrinsic dimension, then one could find a good subspace via gradient sketching: draw a JL matrix $\Gamma\in \mathbb{R}^{r\times m}$ for $m\ll r$, then as long as one has $\Gamma^\top \Sigma^{\phi} \Gamma \preceq c_S \cdot \Gamma^\top \Sigma^{\phi}_S \Gamma$, then the error could be decomposed into a bias, variance and a sketching error term. A sketching gradient, moment-matching algorithm is proposed, involves applying sketching to the gradient, form the Jacobian and solve a quadratic relaxation. Experiments are performed on both synthetic datasets and CIFAR10.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The main theoretical contribution is that for over-parametrized setting where $r\gg n$, one could provably show the existence of a subspace that one could project onto and perform data selection on that subspace. Moreover, if the second moment in addition has low intrinsic dimension, then one could use standard dimensionality reduction techniques (in $\ell_2$ norm) to sketch the high-dimensional gradient. In the sketchy moment-matching algorithm proposed in the paper, the authors first sketch the gradient then use uniform sampling  to construct $S$.

Weaknesses:
The core results of this paper are not technically very novel and surprising, the algorithm could be interpreted as a generalization of the leverage score sampling via JL trick due to Spielman and Srivastava, STOC'08. The analysis largely draws inspirations from the over-parametrization literature, which makes sense as finetuning is essentially training in an over-parametrized setting. Another point that is a bit unsatisfactory is the sketchy moment-matching algorithm utilizes quadratic relaxation to solve the program efficiently with projected gradient descent, but all analysis is based upon *not solving the quadratic programs*. The authors should try to provide some theoretical justifications of sketchy moment-matching, as that's one of the key contributions of this paper.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the problem of data selection for finetuning large pre-trained models. The key contributions are:

1. A theoretical analysis of data selection for finetuning that reveals a variance-bias tradeoff in high dimensions.
2. A provable result showing that gradient sketching can efficiently find a low-dimensional subspace that preserves fast-rate generalization.
3. A practical two-stage algorithm called Sketchy Moment Matching (SkMM) that uses gradient sketching to explore the parameter space and moment matching to exploit the low-dimensional structure.
4. Empirical validation on synthetic and real datasets demonstrating the effectiveness of the approach.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper provides a rigorous generalization analysis for data selection in both low and high-dimensional settings. The proofs are detailed and appear sound.
2. The proposed SkMM method is simple to implement and scalable to large models/datasets. Experiments on both synthetic and real data demonstrate the effectiveness of the approach.

Weaknesses:
1. Some of the theoretical results rely on assumptions (e.g., low intrinsic dimensionality) that may not always hold in practice. More discussion of the implications when these assumptions are violated would be valuable.
2. The method introduces new hyperparameters (e.g., sketching dimension, moment matching strength) without much guidance on how to set them optimally.

Limitations:
yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study the task of data selection. They extend the classical variance reduction to the high dimensional case and provide a variance-bias tradeoff analysis. Based on the theoretical results, they propose sketchy moment matching, which first utilizes gradient sketchy to form a low-dimensional space and then uses moment matching to reduce the variance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The proposal is a reasonable improvement over the baselines which often only consider bias or variance reduction. The theoretical analysis is also a decent contribution of the paper.

Weaknesses:
The experiment focuses on linear probing, which already limits the scope of the evaluation. Furthermore, even under this limited scope, the setting does not seem to be challenging. For the synthetic setup, the sample count is 2000 while the rank is 2500, so it seems not to be a very high-dimension setup (the rank is not so much larger than the sample count). Also, the cluster count seems to be low for both tasks, 8 for synthetic, while the number of class is 10 for Cifar-10.

Limitations:
Yes, the authors discuss the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the problem of data selection in the over-parametrized fine-tuning regime, i.e. when the number of fine-tuning parameters $r$ is larger than the amount $N$ of available examples. We want to subsample $n\ll N$ examples that form a representative set to train on, and hopefully achieve quality as close as possible to fine-tuning on the whole set.

The idea is to compute the gradients $G\in \mathbb{R}^{N\times r}$ of all examples wrt the fine-tuning params and then select a subsample $S\subseteq [N]$ such that the Gram matrix of the gradients is approximated: $c\cdot \Sigma_S := c \cdot G^\top I_S G \approx G^\top G := \Sigma$. However, this is not possible to achieve since the model is over-parameterized. Fortunately, if the spectral approximation holds on a low-dimensional subspace of the parameter space, this is good enough, so the authors project the gradients on a random low-dimensional space. The proof goes through under the assumption that the singular values of the gradient matrix are well-concentrated on a small enough (<10%) support.

The experimental results include fine-tuning on a synthetic linear task, as well as fine-tuning a vision transformer on CIFAR-10 image classification.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
- The authors study the data selection for fine-tuning problem from first principles
- The writing is overall good and math looks sound, even though I didn't check details.
- The experimental results look promising since SkMM beats a variety of algorithms including leverage scores.
- The idea of spectral approximation on a subspace of the parameter space is interesting.

Weaknesses:
- Important details on the experimental setup are missing or unclear. Specifically, what is the optimization process after the data is subsampled? For the image classification experiments, what is being fine-tuned, is it all the ViT parameters? For how many epochs? 
- The algorithm requires computing the gradients of all samples, which can be computationally expensive. Besides, if we are computing all gradients, why can't we just train one epoch on all datapoints? Why is data selection useful in this case?
- The literature review could be expanded, including relevant papers such as BADGE [1], Coreset-based sensitivity sampling [2].
- In the experimental results, the authors should also compare with margin sampling (in addition to entropy sampling), as well as uniform sampling for the image classification task.
- Computing the moment-matching subset in Algorithm 3.1 seems overly complicated, see questions

[1]: Deep Batch Active Learning by Diverse, Uncertain Gradient Lower Bounds

[2]: Data-Efficient Learning via Clustering-Based Sensitivity Sampling: Foundation Models and Beyond

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper introduces a new causal model in which individual events of the cause variable trigger events of the effect variable with dynamic delays. The authors propose a cause-effect matching approach to learn a fully directed acyclic graph, named the CASCADE algorithm. The algorithm performs a topological search on observational data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper presents a comprehensive theory and algorithm, and conducts extensive experiments, particularly with real data, to validate the effectiveness of the proposed method. The analysis and the algorithm are presented in a logical way.

Weaknesses:
The proposed method is the direct matching between a cause event and an effect event, which precludes modeling a single event causing multiple other events, as well as multiple events jointly causing a single effect event.  This limits the applicability of the algorithm.

Limitations:
The authors discuss in detail the limitations and applicability of their algorithm.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The article employs the Algorithmic Markov Condition alongside Kolmogorov
complexity for causal discovery from event sequences. It focuses on a specific scenario in
which the sequence of events is divided into source and effect variables. The principal
contribution of this study is its innovative application of Pearl's causality model with
combination of AMC method, in contrast to the traditional Granger causality approach,
enabling the identification of both instantaneous and delayed effects.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Originality: The author employs Pearl's model of causality, diverging from
traditional Granger causality, to innovatively incorporate instantaneous effects
into the analysis of sequential events for causal relationship discovery.
2. Quality: The article is with good quality and honest about its strength and
limitation on their work.
3. Clarity: The article presents its algorithm with well-defined logic and
substantiated proofs.
4. Significance: The article offers an innovative approach to integrating
instantaneous effects into the causal discovery of sequential events, proposing a
potential method to enhance causal discovery techniques under such conditions.
However, it imposes strict limitations on the scenarios involving event sequences.

Weaknesses:
1. Significance: As mentioned in the limitation section by the author, strict
assumptions like direct matching between a cause event and an effect event leads
to challenges and possible violations in practical application, and it lacks
flexibility.
2. Section 3.3, which discusses the connection to Hawkes Processes, might be better
placed in an appendix or in a section dedicated to comparing different
methodologies. Its current placement in the theoretical part of the paper is
somewhat abrupt, especially since there is no direct focus on these processes in
your model.
3. The experimentation section lacks depth. It would be beneficial to evaluate and
report on the robustness of your model when its assumptions are challenged
during real-world applications.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In their work, the authors are concerned with recovering causal relations, where cause and corresponding effects occur in varying temporal distances. The authors leverage information theoretic formulations and properties of the algorithmic Markov condition to recover the causal graph via minimum description length principled. To this end, the authors present the 'CASCADE' algorithm, which recovers the topological ordering of the causal structure and proof identifiability results. The algorithm is evaluated on multiple synthetic data setups to examine the algorithm's performance under different varying noise, event type, and collider settings. Lastly, the algorithm is tested on a banking and daily activity data set to demonstrate robust performance on real-world data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well-written and introduces the problem setup and formalisms intuitively. The authors consider the challenging problem of modeling causal event sequences. The information-theoretic treatise and causal modeling of the event-generating process via minimum description length encodings are well described and follow common notation from related work. While I am not an expert on the topic of time series event causality, relevant related work seems to be sufficiently discussed and compared to.

The overall intuition on all proofs is well described. To the best of my knowledge, proofs of theorems 1, 3 and 4 seem to be correct. (Please see minor comments on Thm. 2 below). The presented CASCADE algorithm seems to be sound and its robustness is evaluated via multiple real-world and synthetic experiments, varying the noise and number of event types.

Weaknesses:
While the authors present strong theoretical identifiability results, these guarantees are tied to a restrictive set of assumptions (faithfulness, sufficiency, low noise) and hold only for a specific type of event process (single excitation, no suppressing effects). While the authors state all assumptions explicitly, the paper could be improved by discussing the possible implications and reasonability of real-world applications.


Proof of Theorem 2 (Sec. A.2; second line of l. 496): As all other terms seem to be taken over from the line above, it is unclear to me where the canceled term on the left side of the inequality is coming from. (Since all terms are positive, I believe the transformation to be still correct.) Furthermore, it is not obvious to me how the equation following l.497 and the noise ratio of $n_{i,j}/n_j$ leads to the desired result. The paper could be improved by providing a more detailed explanation of this step.


The experiments seem to demonstrate consistently better results compared to related algorithms. However, from the experimental description in B.1, it seems that the experiment on the especially challenging identification of colliders --due to unclear parent assignment-- only considers a setting with a single collider. The authors might want to demonstrate algorithm performance for settings where multiple colliers exist, to better examine the algorithm's robustness regarding unclear EM assignments.


Minor:
* It would be helpful to mention the definition of H() in Sec. A.1 as the entropy, which is only mentioned afterward in A.2.
* Typos in the Proof of Thm. 2 (sec. A.2 l.490): ""dealys"", ""ofset""; and the Conclusion (l.340) ""discovers"" -> ""discover"".
* In Sec. 4.1 l.201 text and formula disagree on the complexity: ""[...] leading to an overall quadratic complexity $O(p^3)$"".

Limitations:
Limitations with regard to the applicability of the algorithm are discussed. Assumptions required for identifiability of the considered causal models are stated explicitly but might be hard to check in real-world settings. The work might be improved by discussing societal impacts from applying the algorithm under possible assumption violations in real-world settings.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a method for identifying causal relationships in event sequences. The authors presents a causal model that handles both instantaneous and delayed effects, contrasting it with existing methods like Granger causality. This algorithm is evaluated on both synthetic and real-world datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The theoretical foundation based on the AMC and MDL principle is provided.

2. The proposed CASCADE algorithm is evaluated through extensive experiments.

3. The paper is well-organized, with clear explanations of the proposed model, theoretical underpinnings, and algorithmic steps. The use of illustrative examples and detailed proofs enhances understanding.

Weaknesses:
1. The paper acknowledges assumptions such as the direct matching between cause and effect events and the focus on excitatory effects. However, it could provide more discussion on the impact of these assumptions and potential ways to address them.

2. Scalability and computational complexity: The paper demonstrates the algorithm's performance on datasets with a moderate number of variables and events. An evaluation of its scalability to very large datasets, which are common in real-world applications, is less emphasized. The computational complexity of the algorithm, particularly for large datasets with many event types, is a concern. The quadratic complexity in the number of event types may limit its applicability to very large-scale problems.

3. Parameter sensitivity is not provided: How sensitive is the CASCADE algorithm to the choice of parameters for the delay distribution and cause probability?

Limitations:
The authors discussed the limitation in the Conclusion section. The identifiability of instantaneous effects relies on the strengths of the trigger and noise probabilities, which may be challenging to estimate accurately in practice.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper introduces a new causal model in which individual events of the cause variable trigger events of the effect variable with dynamic delays. The authors propose a cause-effect matching approach to learn a fully directed acyclic graph, named the CASCADE algorithm. The algorithm performs a topological search on observational data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper presents a comprehensive theory and algorithm, and conducts extensive experiments, particularly with real data, to validate the effectiveness of the proposed method. The analysis and the algorithm are presented in a logical way.

Weaknesses:
The proposed method is the direct matching between a cause event and an effect event, which precludes modeling a single event causing multiple other events, as well as multiple events jointly causing a single effect event.  This limits the applicability of the algorithm.

Limitations:
The authors discuss in detail the limitations and applicability of their algorithm.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The article employs the Algorithmic Markov Condition alongside Kolmogorov
complexity for causal discovery from event sequences. It focuses on a specific scenario in
which the sequence of events is divided into source and effect variables. The principal
contribution of this study is its innovative application of Pearl's causality model with
combination of AMC method, in contrast to the traditional Granger causality approach,
enabling the identification of both instantaneous and delayed effects.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Originality: The author employs Pearl's model of causality, diverging from
traditional Granger causality, to innovatively incorporate instantaneous effects
into the analysis of sequential events for causal relationship discovery.
2. Quality: The article is with good quality and honest about its strength and
limitation on their work.
3. Clarity: The article presents its algorithm with well-defined logic and
substantiated proofs.
4. Significance: The article offers an innovative approach to integrating
instantaneous effects into the causal discovery of sequential events, proposing a
potential method to enhance causal discovery techniques under such conditions.
However, it imposes strict limitations on the scenarios involving event sequences.

Weaknesses:
1. Significance: As mentioned in the limitation section by the author, strict
assumptions like direct matching between a cause event and an effect event leads
to challenges and possible violations in practical application, and it lacks
flexibility.
2. Section 3.3, which discusses the connection to Hawkes Processes, might be better
placed in an appendix or in a section dedicated to comparing different
methodologies. Its current placement in the theoretical part of the paper is
somewhat abrupt, especially since there is no direct focus on these processes in
your model.
3. The experimentation section lacks depth. It would be beneficial to evaluate and
report on the robustness of your model when its assumptions are challenged
during real-world applications.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In their work, the authors are concerned with recovering causal relations, where cause and corresponding effects occur in varying temporal distances. The authors leverage information theoretic formulations and properties of the algorithmic Markov condition to recover the causal graph via minimum description length principled. To this end, the authors present the 'CASCADE' algorithm, which recovers the topological ordering of the causal structure and proof identifiability results. The algorithm is evaluated on multiple synthetic data setups to examine the algorithm's performance under different varying noise, event type, and collider settings. Lastly, the algorithm is tested on a banking and daily activity data set to demonstrate robust performance on real-world data.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well-written and introduces the problem setup and formalisms intuitively. The authors consider the challenging problem of modeling causal event sequences. The information-theoretic treatise and causal modeling of the event-generating process via minimum description length encodings are well described and follow common notation from related work. While I am not an expert on the topic of time series event causality, relevant related work seems to be sufficiently discussed and compared to.

The overall intuition on all proofs is well described. To the best of my knowledge, proofs of theorems 1, 3 and 4 seem to be correct. (Please see minor comments on Thm. 2 below). The presented CASCADE algorithm seems to be sound and its robustness is evaluated via multiple real-world and synthetic experiments, varying the noise and number of event types.

Weaknesses:
While the authors present strong theoretical identifiability results, these guarantees are tied to a restrictive set of assumptions (faithfulness, sufficiency, low noise) and hold only for a specific type of event process (single excitation, no suppressing effects). While the authors state all assumptions explicitly, the paper could be improved by discussing the possible implications and reasonability of real-world applications.


Proof of Theorem 2 (Sec. A.2; second line of l. 496): As all other terms seem to be taken over from the line above, it is unclear to me where the canceled term on the left side of the inequality is coming from. (Since all terms are positive, I believe the transformation to be still correct.) Furthermore, it is not obvious to me how the equation following l.497 and the noise ratio of $n_{i,j}/n_j$ leads to the desired result. The paper could be improved by providing a more detailed explanation of this step.


The experiments seem to demonstrate consistently better results compared to related algorithms. However, from the experimental description in B.1, it seems that the experiment on the especially challenging identification of colliders --due to unclear parent assignment-- only considers a setting with a single collider. The authors might want to demonstrate algorithm performance for settings where multiple colliers exist, to better examine the algorithm's robustness regarding unclear EM assignments.


Minor:
* It would be helpful to mention the definition of H() in Sec. A.1 as the entropy, which is only mentioned afterward in A.2.
* Typos in the Proof of Thm. 2 (sec. A.2 l.490): ""dealys"", ""ofset""; and the Conclusion (l.340) ""discovers"" -> ""discover"".
* In Sec. 4.1 l.201 text and formula disagree on the complexity: ""[...] leading to an overall quadratic complexity $O(p^3)$"".

Limitations:
Limitations with regard to the applicability of the algorithm are discussed. Assumptions required for identifiability of the considered causal models are stated explicitly but might be hard to check in real-world settings. The work might be improved by discussing societal impacts from applying the algorithm under possible assumption violations in real-world settings.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a method for identifying causal relationships in event sequences. The authors presents a causal model that handles both instantaneous and delayed effects, contrasting it with existing methods like Granger causality. This algorithm is evaluated on both synthetic and real-world datasets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The theoretical foundation based on the AMC and MDL principle is provided.

2. The proposed CASCADE algorithm is evaluated through extensive experiments.

3. The paper is well-organized, with clear explanations of the proposed model, theoretical underpinnings, and algorithmic steps. The use of illustrative examples and detailed proofs enhances understanding.

Weaknesses:
1. The paper acknowledges assumptions such as the direct matching between cause and effect events and the focus on excitatory effects. However, it could provide more discussion on the impact of these assumptions and potential ways to address them.

2. Scalability and computational complexity: The paper demonstrates the algorithm's performance on datasets with a moderate number of variables and events. An evaluation of its scalability to very large datasets, which are common in real-world applications, is less emphasized. The computational complexity of the algorithm, particularly for large datasets with many event types, is a concern. The quadratic complexity in the number of event types may limit its applicability to very large-scale problems.

3. Parameter sensitivity is not provided: How sensitive is the CASCADE algorithm to the choice of parameters for the delay distribution and cause probability?

Limitations:
The authors discussed the limitation in the Conclusion section. The identifiability of instantaneous effects relies on the strengths of the trigger and noise probabilities, which may be challenging to estimate accurately in practice.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposes the Delta Rule method to construct the state updates for Linear Attention. Furthermore, the paper introduces a chunk-wise training approach, allowing the computational cost of training to grow subquadratically with the text length. Experimentally, the paper validates the effectiveness of the model architecture using three synthetic benchmarks: MQAR, MAD, and RegBench. Additionally, the paper uses Common Sense Reasoning and Retrieval tasks in LLM pre-training to verify the model's performance in real-world tasks. The model has been validated at scales ranging from 340M to 1.3B parameters. Furthermore, this paper explores the possibility of combining the Delta Rule with Sliding Window Attention and Global Attention, demonstrating the positive impact of the hybrid architecture on model performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Solid work. The paper provides a good derivation, offering a more general method for state updates in Linear Models.
2. The experiments are comprehensive and effectively demonstrate the validity of the model architecture.

Weaknesses:
1. Have you conducted experiments on long context? For example, measuring extrapolation and scenarios akin to ""looking for a needle in a haystack""? As a linear model, I would like you to further discuss its capability to generalize to long context.
2. The algorithmic speed of Delta Net increases linearly, but it seems to be slower than GLA. Can you analyze the factors contributing to this?
3. Could you further explain the insights of the Delta Net updates? I understand there are algorithmic differences compared to GLA operators, but what unique benefits do they bring? Is there any theoretical analysis?

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel algorithm for the efficient training of DeltaNet Linear Transformers. DeltaNet enhances contextual associative recall using a delta rule-like update but was previously limited by inefficient parallelization in its training algorithm. The work described in this paper presents a hardware-efficient algorithm that leverages the memory-efficient WY representation for computing products of Householder matrices, enabling the scaling of DeltaNet similar to other linear Transformer models. The authors trained a 1.3B parameter model on 100B tokens and found that it outperforms strong linear-time baselines such as Mamba and GLA in terms of perplexity and zero-shot performance on downstream tasks.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper introduces a novel hardware-efficient algorithm for training DeltaNet Linear Transformers, leveraging the WY representation of Householder matrices, which effectively addresses the parallelization limitations of previous algorithms.
- Through large-scale experiments, the authors demonstrate that DeltaNet significantly outperforms existing models like Mamba and GLA in terms of perplexity and zero-shot performance on downstream tasks.
- The new algorithm enables the scaling of DeltaNet to larger datasets and parameter sizes, which is crucial for large language models.

Weaknesses:
The algorithms presented in this paper are satisfactory in terms of efficiency and performance.

Limitations:
Yes

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a hardware-efficient algorithm for training linear transformers with a delta update (DeltaNet; SMS21). This architecture has an attention formulation that prevents the direct application of chunk-wise parallel algorithms for computing its output. To address this issue, the authors introduce a re-parameterization of DeltaNet as a matrix-valued RNN whose recurrence is given by a generalized Householder transformation. This enables the use of WY representation which is memory efficient and eliminates the need to materialize the hidden state matrices. Experiments on synthetic benchmarks and language modeling tasks shows competitive performance compared to strong baselines (Mamba, GLA) and faster speed than the original Deltanet implementation.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well motivated and situated with respect to prior work. It provides sufficient background for linear transformers, demonstrates great scholarship in crediting prior work, and has a clear exposition of the proposed idea. In addition, it presents an informative overview that compares the formulations of recent linear transformers that highlights their differences.  
- Proposes an efficient algorithm for training linear transformers with the delta update which is a competitive variant. The re-parameterization is non-obvious and leverages WY representation for Householder matrices in a novel way. Previously, this architecture could not be easily scaled to larger models and datasets with a recurrent formulation. In addition, it introduces two competitive hybrid methods based on DeltaNet that leverage local and global full attention. 
 - Demonstrates the effectiveness of the proposed approach on two synthetic benchmarks and eleven language modeling and understanding tasks compared to strong baselines such as Mamba and GLA. The results are consistent, have a good coverage, and are important for the researchers working on efficient transformers. 
- The experiments are thorough and have convincing settings, namely all the variants are trained from scratch with the same configurations, there are ablations to justify the design choices, and the experimental reporting is very detailed.

Weaknesses:
- W1. In terms of scale, the model explores two different architectures of increasing size up to 1.3B parameters. Even though this size is considerable, it is still relatively small compared to the LLMs that are widely used such as Llama, Mistral (7B+ size). There is always the question of whether the quality is maintained with further model increase.
- W2. The improved results compared to Mamba and GLA make use of additional architectural components: convolution and local/global attention, without them the results are comparable to the other models.

Limitations:
Yes, they have.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposes the Delta Rule method to construct the state updates for Linear Attention. Furthermore, the paper introduces a chunk-wise training approach, allowing the computational cost of training to grow subquadratically with the text length. Experimentally, the paper validates the effectiveness of the model architecture using three synthetic benchmarks: MQAR, MAD, and RegBench. Additionally, the paper uses Common Sense Reasoning and Retrieval tasks in LLM pre-training to verify the model's performance in real-world tasks. The model has been validated at scales ranging from 340M to 1.3B parameters. Furthermore, this paper explores the possibility of combining the Delta Rule with Sliding Window Attention and Global Attention, demonstrating the positive impact of the hybrid architecture on model performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Solid work. The paper provides a good derivation, offering a more general method for state updates in Linear Models.
2. The experiments are comprehensive and effectively demonstrate the validity of the model architecture.

Weaknesses:
1. Have you conducted experiments on long context? For example, measuring extrapolation and scenarios akin to ""looking for a needle in a haystack""? As a linear model, I would like you to further discuss its capability to generalize to long context.
2. The algorithmic speed of Delta Net increases linearly, but it seems to be slower than GLA. Can you analyze the factors contributing to this?
3. Could you further explain the insights of the Delta Net updates? I understand there are algorithmic differences compared to GLA operators, but what unique benefits do they bring? Is there any theoretical analysis?

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a novel algorithm for the efficient training of DeltaNet Linear Transformers. DeltaNet enhances contextual associative recall using a delta rule-like update but was previously limited by inefficient parallelization in its training algorithm. The work described in this paper presents a hardware-efficient algorithm that leverages the memory-efficient WY representation for computing products of Householder matrices, enabling the scaling of DeltaNet similar to other linear Transformer models. The authors trained a 1.3B parameter model on 100B tokens and found that it outperforms strong linear-time baselines such as Mamba and GLA in terms of perplexity and zero-shot performance on downstream tasks.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper introduces a novel hardware-efficient algorithm for training DeltaNet Linear Transformers, leveraging the WY representation of Householder matrices, which effectively addresses the parallelization limitations of previous algorithms.
- Through large-scale experiments, the authors demonstrate that DeltaNet significantly outperforms existing models like Mamba and GLA in terms of perplexity and zero-shot performance on downstream tasks.
- The new algorithm enables the scaling of DeltaNet to larger datasets and parameter sizes, which is crucial for large language models.

Weaknesses:
The algorithms presented in this paper are satisfactory in terms of efficiency and performance.

Limitations:
Yes

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a hardware-efficient algorithm for training linear transformers with a delta update (DeltaNet; SMS21). This architecture has an attention formulation that prevents the direct application of chunk-wise parallel algorithms for computing its output. To address this issue, the authors introduce a re-parameterization of DeltaNet as a matrix-valued RNN whose recurrence is given by a generalized Householder transformation. This enables the use of WY representation which is memory efficient and eliminates the need to materialize the hidden state matrices. Experiments on synthetic benchmarks and language modeling tasks shows competitive performance compared to strong baselines (Mamba, GLA) and faster speed than the original Deltanet implementation.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The paper is well motivated and situated with respect to prior work. It provides sufficient background for linear transformers, demonstrates great scholarship in crediting prior work, and has a clear exposition of the proposed idea. In addition, it presents an informative overview that compares the formulations of recent linear transformers that highlights their differences.  
- Proposes an efficient algorithm for training linear transformers with the delta update which is a competitive variant. The re-parameterization is non-obvious and leverages WY representation for Householder matrices in a novel way. Previously, this architecture could not be easily scaled to larger models and datasets with a recurrent formulation. In addition, it introduces two competitive hybrid methods based on DeltaNet that leverage local and global full attention. 
 - Demonstrates the effectiveness of the proposed approach on two synthetic benchmarks and eleven language modeling and understanding tasks compared to strong baselines such as Mamba and GLA. The results are consistent, have a good coverage, and are important for the researchers working on efficient transformers. 
- The experiments are thorough and have convincing settings, namely all the variants are trained from scratch with the same configurations, there are ablations to justify the design choices, and the experimental reporting is very detailed.

Weaknesses:
- W1. In terms of scale, the model explores two different architectures of increasing size up to 1.3B parameters. Even though this size is considerable, it is still relatively small compared to the LLMs that are widely used such as Llama, Mistral (7B+ size). There is always the question of whether the quality is maintained with further model increase.
- W2. The improved results compared to Mamba and GLA make use of additional architectural components: convolution and local/global attention, without them the results are comparable to the other models.

Limitations:
Yes, they have.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,Yes,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper studies the problem of learning a diffusion process from samples. It proposes a new scheme based on learning the ""causes mismatch"" of the process, rather than the ""effects mismatch"" as in previous works. The new method is significantly more efficient than the schemes from prior works, and works well in practice.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-written, and the scheme proposed seems to work well in practice on the examples it was tested on. The loss is intuitive, and resembles the score-matching loss from diffusion models, but is the analogous version for arbitrary diffusion processes. Overall, this seems like a paper that people at NeurIPS would be interested in.

Weaknesses:
I am not familiar enough with the literature, but it seems surprising to me that this scheme has never been proposed before. In particular, the loss is exactly the score-matching in the case of diffusion models, and there are works [1], [2] that have proposed a similar loss for arbitrary diffusion processes. 

[1]: https://arxiv.org/abs/2208.09392
[2]: https://arxiv.org/abs/2209.05442

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces JKOnet*, a new method for learning diffusion processes from data. It uses first-order optimality conditions of the JKO scheme instead of complex bilevel optimization. JKOnet* can recover potential, interaction, and internal energy components of diffusion processes. The authors provide theoretical analysis and experiments showing JKOnet* outperforms baselines in accuracy, speed, and ability to handle high-dimensional data. They also derive a closed-form solution for linearly parameterized functionals. JKOnet* offers improved computational efficiency and representational capacity compared to existing approaches for modeling diffusion dynamics from population data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Develops JKOnet*, a method using first-order optimality conditions of the JKO scheme to learn diffusion processes, avoiding bilevel optimization and improving computational efficiency.
- Provides theoretical analysis and proofs for JKOnet*, including a closed-form solution for linearly parameterized functionals, backed by comprehensive experiments across various test functions.
- Demonstrates improved performance in terms of Wasserstein error and computation time compared to existing methods like JKOnet, especially in high-dimensional settings.
- Enables recovery of potential, interaction, and internal energy components of diffusion processes, expanding the model's applicability to more complex systems and improving interpretability.

Weaknesses:
- The experimental evaluation is limited to synthetic datasets. Real-world data applications would strengthen the practical relevance of the method.
- While the paper discusses limitations, it does not thoroughly explore potential failure cases or boundary conditions where JKOnet* might underperform.
- The paper does not provide a comprehensive comparison with other recent approaches in learning diffusion processes beyond JKOnet, which could provide broader context for the method's improvements.

Limitations:
The author discusses limitations in section 5

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers learning diffusion dynamics from observational data of populations over time, identified as learning the energy functional in Equation 3.  Past research has confronted this inverse problem via complex bilevel optimization, limited to potential energies.  This paper proposes an alternative model JKOnet* that can work with potential, internal, and interaction energies, efficiently minimizes a quadratic loss instead of a complex bilevel optimization, has much lower computational complexity, and out-performs baselines in simulations.  A variant for linearly parameterized functionals has a closed form solution.  The paper's new method reconsiders the JKO scheme using first-order optimality conditions, resulting in decompose the problem into first computing optimal transport plans between adjacent populations and then optimizing a loss for fixed plans.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- Inferring diffusion dynamics from observational data is a difficult and significant problem for which this paper appears to provide a solid contribution.  The paper substantially improves upon JKOnet in terms of multiple directions: better performance (Figure 3), simpler optimization objective (Equation 11), better scalability and efficiency (e.g. Table 1, Section 4.2), and improved generality (Table 1, Section 4.3).  These dimensions are analyzed in experiments across a range of different energy functionals, where the gains are shown in log-scale displaying orders of magnitude improvement.  The paper makes a convincing argument for using JKOnet* over JKOnet.
- The methodology appears quite strong, well-motivated, and original, with solid intuition given by the authors throughout the paper.

Weaknesses:
Minor weaknesses:
- While the results are strong, occasionally the language feels too imprecise.  For example, ""runs at lightspeed"" seems inaccurate compared to ""runs very efficiently"".  The authors also mention that they rely upon weeks-old advancements in optimization in the abstract which seems unneeded.
-  The paper is generally very well-written except for the introduction which could use editing.  It introduces a lot of terminology and details from past research.  Similarly, Figure 1 is referenced multiple times including in the introduction but it was hard to understand until after reading Section 3. 
- The construction of the optimal transport plans does not seem to be included in the computational complexity comparisons.  While this is computed once for JKOnet*, it is additional expense over JKOnet.

Limitations:
Limitations are adequately addressed in Section 5

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study diffusion processes from the perspective of Wasserstein gradient flows. Based on the recent fixed-point characterisation for Wasserstein proximal operator methods, they introduce Jordan-Kinderlehrer-Otto (JKO) type methods for learning potential and interaction energies that govern the diffusion process. Such methods are assuming that a sample of the population distribution at each time step is at hand (not necessearily obtained by tracking individual particles) implying important applications across various fields. While theoretical novelties are present (w.r.t. paper [26] that lies in the foundation of this work), the main contribution is the overall methodology for learning diffusion processes.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
Paper is, besides minor issues reported bellow, excellently written - very clear, precise and intuitive with well balances technical details between main text and the appendix. Existing ideas are neatly combined to obtain significant improvements of the JKO-type methods and extensive empirical evaluation is presented. The proofs seem correct and well-written.

Weaknesses:
While I do not find important weaknesses, I feel that next several small issues can be addressed to further improve readability:

1. When addressing content presented in the appendix it would be good to refer to the section, e.g. see Figure 6 in Appendix A.

2. It would be good to say what $\rho_t$ is in Example 2.1

3. While Table 3.1 reports per-epoch complexity for all the methods, it would be important to note that JKOnet$^*$ have additional computational complexity for solving $T$ OT problems of size $N$ in $d$-dimensions. Detailed remark on the initial computational complexity, depending of the algorithm used, should be reported.  

4. In Section 4 it would be helpful to introduce the problems, that is to better explain the task of each experiment and the role of functionals ($V(x)$ ?!)  appearing in Appendix F.  Maybe giving an example on Styblinski-Tang functional appearing in Figures 2, 3 and 4, and then referring to other ones by their names and/or reference equations.

Limitations:
Limitations are addressed adequately.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper studies the problem of learning a diffusion process from samples. It proposes a new scheme based on learning the ""causes mismatch"" of the process, rather than the ""effects mismatch"" as in previous works. The new method is significantly more efficient than the schemes from prior works, and works well in practice.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-written, and the scheme proposed seems to work well in practice on the examples it was tested on. The loss is intuitive, and resembles the score-matching loss from diffusion models, but is the analogous version for arbitrary diffusion processes. Overall, this seems like a paper that people at NeurIPS would be interested in.

Weaknesses:
I am not familiar enough with the literature, but it seems surprising to me that this scheme has never been proposed before. In particular, the loss is exactly the score-matching in the case of diffusion models, and there are works [1], [2] that have proposed a similar loss for arbitrary diffusion processes. 

[1]: https://arxiv.org/abs/2208.09392
[2]: https://arxiv.org/abs/2209.05442

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces JKOnet*, a new method for learning diffusion processes from data. It uses first-order optimality conditions of the JKO scheme instead of complex bilevel optimization. JKOnet* can recover potential, interaction, and internal energy components of diffusion processes. The authors provide theoretical analysis and experiments showing JKOnet* outperforms baselines in accuracy, speed, and ability to handle high-dimensional data. They also derive a closed-form solution for linearly parameterized functionals. JKOnet* offers improved computational efficiency and representational capacity compared to existing approaches for modeling diffusion dynamics from population data.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Develops JKOnet*, a method using first-order optimality conditions of the JKO scheme to learn diffusion processes, avoiding bilevel optimization and improving computational efficiency.
- Provides theoretical analysis and proofs for JKOnet*, including a closed-form solution for linearly parameterized functionals, backed by comprehensive experiments across various test functions.
- Demonstrates improved performance in terms of Wasserstein error and computation time compared to existing methods like JKOnet, especially in high-dimensional settings.
- Enables recovery of potential, interaction, and internal energy components of diffusion processes, expanding the model's applicability to more complex systems and improving interpretability.

Weaknesses:
- The experimental evaluation is limited to synthetic datasets. Real-world data applications would strengthen the practical relevance of the method.
- While the paper discusses limitations, it does not thoroughly explore potential failure cases or boundary conditions where JKOnet* might underperform.
- The paper does not provide a comprehensive comparison with other recent approaches in learning diffusion processes beyond JKOnet, which could provide broader context for the method's improvements.

Limitations:
The author discusses limitations in section 5

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper considers learning diffusion dynamics from observational data of populations over time, identified as learning the energy functional in Equation 3.  Past research has confronted this inverse problem via complex bilevel optimization, limited to potential energies.  This paper proposes an alternative model JKOnet* that can work with potential, internal, and interaction energies, efficiently minimizes a quadratic loss instead of a complex bilevel optimization, has much lower computational complexity, and out-performs baselines in simulations.  A variant for linearly parameterized functionals has a closed form solution.  The paper's new method reconsiders the JKO scheme using first-order optimality conditions, resulting in decompose the problem into first computing optimal transport plans between adjacent populations and then optimizing a loss for fixed plans.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- Inferring diffusion dynamics from observational data is a difficult and significant problem for which this paper appears to provide a solid contribution.  The paper substantially improves upon JKOnet in terms of multiple directions: better performance (Figure 3), simpler optimization objective (Equation 11), better scalability and efficiency (e.g. Table 1, Section 4.2), and improved generality (Table 1, Section 4.3).  These dimensions are analyzed in experiments across a range of different energy functionals, where the gains are shown in log-scale displaying orders of magnitude improvement.  The paper makes a convincing argument for using JKOnet* over JKOnet.
- The methodology appears quite strong, well-motivated, and original, with solid intuition given by the authors throughout the paper.

Weaknesses:
Minor weaknesses:
- While the results are strong, occasionally the language feels too imprecise.  For example, ""runs at lightspeed"" seems inaccurate compared to ""runs very efficiently"".  The authors also mention that they rely upon weeks-old advancements in optimization in the abstract which seems unneeded.
-  The paper is generally very well-written except for the introduction which could use editing.  It introduces a lot of terminology and details from past research.  Similarly, Figure 1 is referenced multiple times including in the introduction but it was hard to understand until after reading Section 3. 
- The construction of the optimal transport plans does not seem to be included in the computational complexity comparisons.  While this is computed once for JKOnet*, it is additional expense over JKOnet.

Limitations:
Limitations are adequately addressed in Section 5

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors study diffusion processes from the perspective of Wasserstein gradient flows. Based on the recent fixed-point characterisation for Wasserstein proximal operator methods, they introduce Jordan-Kinderlehrer-Otto (JKO) type methods for learning potential and interaction energies that govern the diffusion process. Such methods are assuming that a sample of the population distribution at each time step is at hand (not necessearily obtained by tracking individual particles) implying important applications across various fields. While theoretical novelties are present (w.r.t. paper [26] that lies in the foundation of this work), the main contribution is the overall methodology for learning diffusion processes.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
Paper is, besides minor issues reported bellow, excellently written - very clear, precise and intuitive with well balances technical details between main text and the appendix. Existing ideas are neatly combined to obtain significant improvements of the JKO-type methods and extensive empirical evaluation is presented. The proofs seem correct and well-written.

Weaknesses:
While I do not find important weaknesses, I feel that next several small issues can be addressed to further improve readability:

1. When addressing content presented in the appendix it would be good to refer to the section, e.g. see Figure 6 in Appendix A.

2. It would be good to say what $\rho_t$ is in Example 2.1

3. While Table 3.1 reports per-epoch complexity for all the methods, it would be important to note that JKOnet$^*$ have additional computational complexity for solving $T$ OT problems of size $N$ in $d$-dimensions. Detailed remark on the initial computational complexity, depending of the algorithm used, should be reported.  

4. In Section 4 it would be helpful to introduce the problems, that is to better explain the task of each experiment and the role of functionals ($V(x)$ ?!)  appearing in Appendix F.  Maybe giving an example on Styblinski-Tang functional appearing in Figures 2, 3 and 4, and then referring to other ones by their names and/or reference equations.

Limitations:
Limitations are addressed adequately.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,Yes,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The authors derive a novel gradient descent step schedule from a Bayesian point of view, establishing a connection between Bayesian optimization and classical optimization. The theory gives support to some commonly chosen step schedules and is validated on MNIST dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well written, with clearly explained and carefully chosen notations. The figures are very pretty. It's a pleasure to read.
2. The paper has a good motivation. Worse-case theory, in general, can mislead people. Average-case studies are desired. The disparity between Bayesian optimization and classical optimization is quite obvious, and one can imagine there can be many optimization algorithms with mixed characteristics of both genres. The direction the paper explored is promising.
3. The research is very detailed and solid. The authors give sound proofs to their theorems and organise the results in a clear manner. The experiments are very extensive and well displayed.

Weaknesses:
1. The so-called ""average case study"" is not fully justified. The expectation of $J(w_n)$ is not in general equal to the expectation of $J(\theta)$ with $\theta$ fixed and then replaced by $w_n$. This is because $w_n$ is by itself a random variable. More concretely, suppose that $J$ is sampled randomly from $\mathcal{N}(\mu, C)$ with $\mu$ being a constant, say $\mu_0$. Then the expectation of $J(w_0)$ would be $\mu_0$ but the expectation of $J(w_n)$ for $n$ large would be much smaller than $\mu_0$. The method in this paper can only be thought of as average case study in the initial stage of optimization. The authors mention ""forgetful"" but I believe the problem is more serious than it looks. The authors also mention ""risk-affine"", but I don't necessarily agree with it. The claim ""Since RFD is defined as the minimizer of an average instead of an upper bound – making it more risk affine"" feels weak, because I don't think it's well justified yet that RFD is the minimizer of an average.
2. Incomplete story and lack of depth. Overall, there are lots of results but none of them are highlighted enough to be a gem. On the theory side, it's not clear whether there is any nontrivial key technical contribution in the proofs. It's not obvious that the derivation of the step schedule from a Bayesian viewpoint involves more than straightforward calculation. It needs more to stand as a strong theoretical paper. Furthermore, it would be better if there was a clear table presenting a convergence rate comparison of this new method and classical ones. On the empirical side, only MNIST is not enough, although the authors did a lot of experiments on MNIST. So as a new methodology paper, we need stronger empirical evidence. It's understood that the authors are studying a very hard problem, but excuses cannot serve as strengths of the paper.

Limitations:
Yes. Limitations are adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The current paper studies random function descent, draws connection between RFD and SGD, and derives an adaptive step size scheduler. More specifically,  the authors study minimizing a stochastic first Taylor approximation of random functions, which has similar form of gradient descent when the random function is Gaussian process. This connection also hints a step size scheduler for standard GD method. The authors then explore this step size scheduling scheme and study its asymptotic performance, which helps explain some recent step size scheduling tricks such as gradient clipping and warmup. Finally, the authors propose a practical way to evaluate necessary statistics required for the newly found step size scheduler with current ML mini-batch loss. The authors show simulation results for MNIST data to exemplify the effectiveness of the drawn step size scheduler.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This paper is very well-written, theoretically sound, and the findings seem new and pretty insightful, thus I feel it makes good contribution to research on optimizer learning rate scheduling. 

The main topic is about random function descent, and the minimization of stochastic first Taylor approximation of random function results in a gradient-type method (when Gaussian random function is considered) is surprising and impressive. 

The writing is well-organized, with all terms being properly defined and all theorems (Theorem 2.3, 4.2, 5.2, 6.2) well-formulated and capture core ideas. Theorems that are more representative is presented in main text for better digestion with more complete/general versions listed in Appendix. Theorems and Definitions are followed by simple and efficient explanations (i.e., discussion after Definition 2.1, Definition 2.2, around Theorem 2.3, and many others). Plots and tables are provided and are clean and easy to interpret.

The math is clean, sound, and rigorous, with very complete proofs (i.e., D.1.1 and D.1.2). Extensions are well-explored (Section E) and more general cases are discussed (Section E.3 for example). From first derivation of step size (Theorem 4.2), to its asymptotic version A-RFD (Definition 5.2) and its stochastic version S-RFD (Theorem 6.2), all are interesting and important findings.

Practicality of the proposed method has been considered. Though the proposed step size scheduler looks complicated, the authors figure out ways to evaluate necessary statistics required to put the step size scheduler into use (Section 6), and the effectiveness of proposed method applied to current ML tasks is also exemplified by examples (Section 7).

The research topic is valuable. Learning rate scheduling has been an open research area for a long time in optimization field. Currently in machine learning/deep learning research, a great deal of pressure comes from comparing with baseline methods which involves arduous hyperparameter tuning, among which learning rate is often the core. Thus studying learning rate scheduling is of great importance and this paper provides a novel connection between RFD and GD (with also extended comparison to Adam in Section E.1) which is very encouraging. Moreover, classical convergence result for optimization algorithms are mainly with worst case bound, RFD is instead for average case performance, the authors try hard and derive partial result for convergence (Corollary 5.3), and we expect there would be more study of difference between worst case performance and average case performance.

Weaknesses:
Though I appreciate the presentation quality, theoretical soundness, and novelty of the work. The main drawback of current paper boils down to three parts: lacking comparison with prior work, potential concerns with the practicality and effectivenss of the proposed method, and the (relatively) strict assumption of the theory.

1. The current paper doesn't involve literature review section, though it draws connection to prior work dispersedly, no systematic review has been intended. I currently make my evaluation of the novelty of the work based on my own (might be poor) understanding. I feel adding a related work section is desirable and then a more fair evaluation of value of current work can be made.

2. Still about prior work but for baseline method comparison. The simulation results (mainly Figure 3) only compares the proposed method with SGD/Adam with tuned fixed learning rates. More recent work such as D-Adaptation [1] also studies tuning-free learning rate scheduler for SGD/Adam, from not RFD perspective but more classical optimization angle, hasn't be mentioned/compared against. Moreover, the experiment in current paper seems much simpler and less thorough than the setting considered in D-Adaptation. 

3. With respect to practicality, though the authors provide empirical ways to evaluate covariance in mini-batch training, the recipe still looks a bit complex, i.e., one should go evaluate $C$ and $C'$ from the observation first. Unlike current adaptive gradient method such as Adam/AdamW, or even D-Adaptation, which only depends on some statistics involving current/past gradient/function values. Moreover, since RFD is measuring average case performance, it's more risk-affine and tends to predict larger learning rate, which may be harmful for convergence in some cases.

4. Despite that I feel minimizing stochastic Taylor approximation of random function is interesting and worth exploring, the derived GD-type algorithm is for Gaussian random function (Theorem 4.2), though the authors mention this assumption was also used in [2], it might be desirable to more demonstrate to which extent one should expect this assumption to be close to real settings.


[1] Learning-Rate-Free Learning by D-Adaptation (Aaron Defazio and Konstantin Mishchenko).

[2] Yann N Dauphin et al. “Identifying and Attacking the Saddle Point Problem in High Dimensional Non-Convex Optimization”.

Limitations:
Limitations have been discussed in Section 8.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Many machine learning model have parameters that are optimized by some form of gradient descent. Given a parameters $\omega$ in a space $\Omega$ and a loss function $\textbf{J}: \Omega \to \mathbb{R}$, typical gradient descent proceeds by picking a starting point $\omega_0$ and iteratively taking steps in the direction of steepest descent

$$
\omega_{n+1} = \omega_n - h \nabla J(\omega_n) = \omega_n - \eta \frac{\nabla J(\omega_n)}{||\nabla J(\omega_n)||}
$$
where $h$ is the learning rate, or similarly $\eta = $ is the step size. The learning rate/step size is an exogenous pre-determined user hyper parameter.

This paper proposes a method to automatically determine the steps size parameter. (I may have misunderstood and I welcome any correction by the authors) this method, called Random Function Descent (RFD), take a point $\omega$, computes the function value and gradient $J(\omega)$, $\nabla J(\omega)$, which is then used to fit a Gaussian process model. The GP model has a constant prior mean mean and a stationary, isotropic kernel, and by fitting one data point and it's gradient vector, the constant prior mean is updated to a still mostly constant surface however with a single local deformation at $\omega$ resulting in a peak in the uphill direction from $\omega$ and a trough on the direct opposite downhill side, . the RFD method jumps straight to the bottom of the trough, mathematically
$$
\omega_{n+1} = \text{arg min}_{\omega'} \mathbb{E}[J(\omega') | J(\omega), \nabla J(\omega') ]
$$
where the expectation is the posterior mean of the GP having been fit to the one data point. As the GP kernel is isotropic, there is no prior bias in any direction and the direction of the trough is exactly the direction of the gradient, consistent with normal gradient descent.

The paper considers many of the technical and theoretical hurdles and provides solutions in each case. Finally experiments with MNIST are provided. 

I somewhat struggled with the paper and have set my confidence score to low accordingly.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- tuning the baselines in the numerical experiments

Weaknesses:
Unfortunately for me, I struggled to understand much of the paper, I believe this could be partially be due to writing style, I have tried to keep my technical and writing comments separate

I apologize if my understanding is incorrect, and look forward to the authors response to correct any such errors.

- all the parameter updates are using euclidean distance in parameter space. In contrast, Natural gradient descent makes parameter updates that have equal distance in output distribution space. In practice, I believe an approximation is implemented by using inverse squared gradients for each parameter similar to ADAM/RMSprop that use root mean squared gradients. Obviously, 

- the numerical experiments seem a little lacking, RFD doesn't appear to show a significant improvement on Figures 3, 6, 7. MNIST and FashinMNIST are very small and perhaps too easy, any optimizer will ""max out"" any model pretty quickly I assume.


The below points are my personal subjective comments on the writing.
- I am a little reluctant to agree that this paper has much to do with Bayesian optimisation as suggested by the abstract and introduction. RFD fits a GP model to a single data point and  only uses the posterior mean, it is the same as kernel ridge regression.
- I felt the terminology of ""stochastic Taylor Expansion"" was rather unhelpful and somewhat counterproductive. In my mind, zeroth/first/second order Taylor expansion refer to constant/linear/quadratic local polynomial approximations to a function, however the given function approximations are non-linear (lemma 4.12) this description unfortunately rather mis-directed my thoughts.
- L69: as above, ""it naturally incorporates covariance based trust"" assumes a lot of context that has not been introduced in the paper at this point, upon first reading I was rather lost, upon second reading it makes sense but felt out of place.
- (there are many topics and details covered the main paper, would it be possible to focus on a few big ideas?)
- Table 1, Figure 2, what is the scale ""s"", I assume the length scale in the covariance $C()$ function?  This appears not to be introduced in the paper.
- L62, should the final term of the equation be $\frac{L}{2}||\omega - \Theta||^2$?

Limitations:
- the assumption of isometric in parameter space

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
### Summary

The paper ""Random Function Descent"" explores the limitations of classical worst-case optimization theory in explaining the success of optimization in machine learning and selecting appropriate step sizes. It establishes a connection between Bayesian Optimization and classical optimization through a ""stochastic Taylor approximation,"" rediscovering gradient descent. This rediscovery introduces a new step size schedule called Random Function Descent (RFD), which is scale-invariant. The analysis provides a theoretical foundation for common step size heuristics such as gradient clipping and gradual learning rate warmup. The paper also proposes a statistical procedure for estimating the RFD step size schedule and validates this theory with a case study on the MNIST dataset.

In the introduction, the paper emphasizes the importance of cost function minimization in machine learning, typically performed using gradient-based methods that require step sizes chosen by established heuristics. The paper aims to enhance the theoretical understanding of these heuristics and proposes RFD as a new algorithm based on this deeper insight. The authors highlight that classical optimization theory, which relies on \(L\)-smoothness, provides conservative learning rates unsuitable for average cases, necessitating the reliance on step size heuristics in machine learning.

The authors bridge the gap between Bayesian Optimization (BO) and gradient-based methods by introducing a stochastic Taylor approximation based on a forgetful BO posterior. This results in the RFD optimization method, which combines the properties of gradient descent with scale invariance and a complete step size schedule derived from BO. The contributions include proving the scale invariance of RFD, discussing common distributional assumptions in BO, establishing the connection between RFD and gradient descent, and investigating the step size schedule suggested by RFD.

The paper further develops a non-parametric variance estimation method robust to covariance kernel choices and extends RFD to mini-batch losses. The case study on the MNIST dataset demonstrates the practical application and effectiveness of the proposed RFD algorithm compared to traditional methods like Adam and stochastic gradient descent (SGD). The discussion includes limitations and potential extensions of the proposed method, emphasizing the need for new mathematical theory to address the risk-affine nature of RFD and its larger step sizes.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
### Strengths

1. **Innovative Approach**: The paper introduces a novel connection between Bayesian Optimization and gradient descent through the stochastic Taylor approximation, leading to the development of Random Function Descent (RFD). This approach provides a new perspective on step size selection and optimization in machine learning.
   
2. **Theoretical Foundation**: The analysis of RFD step sizes offers a solid theoretical foundation for commonly used heuristics such as gradient clipping and learning rate warmup. This bridges the gap between empirical practices and theoretical understanding.
   
3. **Scale Invariance**: RFD's scale invariance is a significant advantage, making it robust to different scales of input parameters and cost functions. This property is stronger than the affine invariance offered by the Newton method.
   
4. **Practical Validation**: The statistical procedure for estimating the RFD step size schedule and its validation on the MNIST dataset demonstrate the practical applicability and effectiveness of the proposed method. The case study shows that RFD can outperform traditional optimization methods like Adam and SGD.
   
5. **Comprehensive Analysis**: The paper provides a thorough investigation of the step size schedule suggested by RFD, including explicit formulas, asymptotic behavior, and explanations for gradient clipping and learning rate warmup. This comprehensive analysis enhances the understanding of RFD's behavior and potential benefits.

Weaknesses:
### Weaknesses

1. **Complexity and Accessibility**: The theoretical development and mathematical derivations in the paper are complex, which might limit the accessibility and understanding for practitioners who are not well-versed in advanced optimization theory and Bayesian methods.
   
2. **Assumptions and Simplifications**: The paper relies on certain assumptions, such as isotropic Gaussian random functions, which might not hold in all practical scenarios. The need for these assumptions could limit the generalizability of the proposed method.
   
3. **Risk-Affine Nature**: RFD's risk-affine nature, resulting in comparatively larger step sizes, might lead to instability in certain cases. The paper acknowledges this limitation and suggests that further work is needed to address this issue and develop new mathematical theories for convergence guarantees.
   
4. **Empirical Validation Scope**: While the MNIST case study is a valuable demonstration, the empirical validation is limited to a single dataset and a specific neural network architecture. Additional experiments on diverse datasets and models would strengthen the evidence for RFD's effectiveness.
   
5. **Variance Estimation Procedure**: The non-parametric variance estimation method, while robust, involves a bootstrapping procedure that could be computationally intensive. This might pose challenges for large-scale applications and require further optimization for practical use.

Limitations:
n/a

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The authors derive a novel gradient descent step schedule from a Bayesian point of view, establishing a connection between Bayesian optimization and classical optimization. The theory gives support to some commonly chosen step schedules and is validated on MNIST dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well written, with clearly explained and carefully chosen notations. The figures are very pretty. It's a pleasure to read.
2. The paper has a good motivation. Worse-case theory, in general, can mislead people. Average-case studies are desired. The disparity between Bayesian optimization and classical optimization is quite obvious, and one can imagine there can be many optimization algorithms with mixed characteristics of both genres. The direction the paper explored is promising.
3. The research is very detailed and solid. The authors give sound proofs to their theorems and organise the results in a clear manner. The experiments are very extensive and well displayed.

Weaknesses:
1. The so-called ""average case study"" is not fully justified. The expectation of $J(w_n)$ is not in general equal to the expectation of $J(\theta)$ with $\theta$ fixed and then replaced by $w_n$. This is because $w_n$ is by itself a random variable. More concretely, suppose that $J$ is sampled randomly from $\mathcal{N}(\mu, C)$ with $\mu$ being a constant, say $\mu_0$. Then the expectation of $J(w_0)$ would be $\mu_0$ but the expectation of $J(w_n)$ for $n$ large would be much smaller than $\mu_0$. The method in this paper can only be thought of as average case study in the initial stage of optimization. The authors mention ""forgetful"" but I believe the problem is more serious than it looks. The authors also mention ""risk-affine"", but I don't necessarily agree with it. The claim ""Since RFD is defined as the minimizer of an average instead of an upper bound – making it more risk affine"" feels weak, because I don't think it's well justified yet that RFD is the minimizer of an average.
2. Incomplete story and lack of depth. Overall, there are lots of results but none of them are highlighted enough to be a gem. On the theory side, it's not clear whether there is any nontrivial key technical contribution in the proofs. It's not obvious that the derivation of the step schedule from a Bayesian viewpoint involves more than straightforward calculation. It needs more to stand as a strong theoretical paper. Furthermore, it would be better if there was a clear table presenting a convergence rate comparison of this new method and classical ones. On the empirical side, only MNIST is not enough, although the authors did a lot of experiments on MNIST. So as a new methodology paper, we need stronger empirical evidence. It's understood that the authors are studying a very hard problem, but excuses cannot serve as strengths of the paper.

Limitations:
Yes. Limitations are adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The current paper studies random function descent, draws connection between RFD and SGD, and derives an adaptive step size scheduler. More specifically,  the authors study minimizing a stochastic first Taylor approximation of random functions, which has similar form of gradient descent when the random function is Gaussian process. This connection also hints a step size scheduler for standard GD method. The authors then explore this step size scheduling scheme and study its asymptotic performance, which helps explain some recent step size scheduling tricks such as gradient clipping and warmup. Finally, the authors propose a practical way to evaluate necessary statistics required for the newly found step size scheduler with current ML mini-batch loss. The authors show simulation results for MNIST data to exemplify the effectiveness of the drawn step size scheduler.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
This paper is very well-written, theoretically sound, and the findings seem new and pretty insightful, thus I feel it makes good contribution to research on optimizer learning rate scheduling. 

The main topic is about random function descent, and the minimization of stochastic first Taylor approximation of random function results in a gradient-type method (when Gaussian random function is considered) is surprising and impressive. 

The writing is well-organized, with all terms being properly defined and all theorems (Theorem 2.3, 4.2, 5.2, 6.2) well-formulated and capture core ideas. Theorems that are more representative is presented in main text for better digestion with more complete/general versions listed in Appendix. Theorems and Definitions are followed by simple and efficient explanations (i.e., discussion after Definition 2.1, Definition 2.2, around Theorem 2.3, and many others). Plots and tables are provided and are clean and easy to interpret.

The math is clean, sound, and rigorous, with very complete proofs (i.e., D.1.1 and D.1.2). Extensions are well-explored (Section E) and more general cases are discussed (Section E.3 for example). From first derivation of step size (Theorem 4.2), to its asymptotic version A-RFD (Definition 5.2) and its stochastic version S-RFD (Theorem 6.2), all are interesting and important findings.

Practicality of the proposed method has been considered. Though the proposed step size scheduler looks complicated, the authors figure out ways to evaluate necessary statistics required to put the step size scheduler into use (Section 6), and the effectiveness of proposed method applied to current ML tasks is also exemplified by examples (Section 7).

The research topic is valuable. Learning rate scheduling has been an open research area for a long time in optimization field. Currently in machine learning/deep learning research, a great deal of pressure comes from comparing with baseline methods which involves arduous hyperparameter tuning, among which learning rate is often the core. Thus studying learning rate scheduling is of great importance and this paper provides a novel connection between RFD and GD (with also extended comparison to Adam in Section E.1) which is very encouraging. Moreover, classical convergence result for optimization algorithms are mainly with worst case bound, RFD is instead for average case performance, the authors try hard and derive partial result for convergence (Corollary 5.3), and we expect there would be more study of difference between worst case performance and average case performance.

Weaknesses:
Though I appreciate the presentation quality, theoretical soundness, and novelty of the work. The main drawback of current paper boils down to three parts: lacking comparison with prior work, potential concerns with the practicality and effectivenss of the proposed method, and the (relatively) strict assumption of the theory.

1. The current paper doesn't involve literature review section, though it draws connection to prior work dispersedly, no systematic review has been intended. I currently make my evaluation of the novelty of the work based on my own (might be poor) understanding. I feel adding a related work section is desirable and then a more fair evaluation of value of current work can be made.

2. Still about prior work but for baseline method comparison. The simulation results (mainly Figure 3) only compares the proposed method with SGD/Adam with tuned fixed learning rates. More recent work such as D-Adaptation [1] also studies tuning-free learning rate scheduler for SGD/Adam, from not RFD perspective but more classical optimization angle, hasn't be mentioned/compared against. Moreover, the experiment in current paper seems much simpler and less thorough than the setting considered in D-Adaptation. 

3. With respect to practicality, though the authors provide empirical ways to evaluate covariance in mini-batch training, the recipe still looks a bit complex, i.e., one should go evaluate $C$ and $C'$ from the observation first. Unlike current adaptive gradient method such as Adam/AdamW, or even D-Adaptation, which only depends on some statistics involving current/past gradient/function values. Moreover, since RFD is measuring average case performance, it's more risk-affine and tends to predict larger learning rate, which may be harmful for convergence in some cases.

4. Despite that I feel minimizing stochastic Taylor approximation of random function is interesting and worth exploring, the derived GD-type algorithm is for Gaussian random function (Theorem 4.2), though the authors mention this assumption was also used in [2], it might be desirable to more demonstrate to which extent one should expect this assumption to be close to real settings.


[1] Learning-Rate-Free Learning by D-Adaptation (Aaron Defazio and Konstantin Mishchenko).

[2] Yann N Dauphin et al. “Identifying and Attacking the Saddle Point Problem in High Dimensional Non-Convex Optimization”.

Limitations:
Limitations have been discussed in Section 8.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
Many machine learning model have parameters that are optimized by some form of gradient descent. Given a parameters $\omega$ in a space $\Omega$ and a loss function $\textbf{J}: \Omega \to \mathbb{R}$, typical gradient descent proceeds by picking a starting point $\omega_0$ and iteratively taking steps in the direction of steepest descent

$$
\omega_{n+1} = \omega_n - h \nabla J(\omega_n) = \omega_n - \eta \frac{\nabla J(\omega_n)}{||\nabla J(\omega_n)||}
$$
where $h$ is the learning rate, or similarly $\eta = $ is the step size. The learning rate/step size is an exogenous pre-determined user hyper parameter.

This paper proposes a method to automatically determine the steps size parameter. (I may have misunderstood and I welcome any correction by the authors) this method, called Random Function Descent (RFD), take a point $\omega$, computes the function value and gradient $J(\omega)$, $\nabla J(\omega)$, which is then used to fit a Gaussian process model. The GP model has a constant prior mean mean and a stationary, isotropic kernel, and by fitting one data point and it's gradient vector, the constant prior mean is updated to a still mostly constant surface however with a single local deformation at $\omega$ resulting in a peak in the uphill direction from $\omega$ and a trough on the direct opposite downhill side, . the RFD method jumps straight to the bottom of the trough, mathematically
$$
\omega_{n+1} = \text{arg min}_{\omega'} \mathbb{E}[J(\omega') | J(\omega), \nabla J(\omega') ]
$$
where the expectation is the posterior mean of the GP having been fit to the one data point. As the GP kernel is isotropic, there is no prior bias in any direction and the direction of the trough is exactly the direction of the gradient, consistent with normal gradient descent.

The paper considers many of the technical and theoretical hurdles and provides solutions in each case. Finally experiments with MNIST are provided. 

I somewhat struggled with the paper and have set my confidence score to low accordingly.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- tuning the baselines in the numerical experiments

Weaknesses:
Unfortunately for me, I struggled to understand much of the paper, I believe this could be partially be due to writing style, I have tried to keep my technical and writing comments separate

I apologize if my understanding is incorrect, and look forward to the authors response to correct any such errors.

- all the parameter updates are using euclidean distance in parameter space. In contrast, Natural gradient descent makes parameter updates that have equal distance in output distribution space. In practice, I believe an approximation is implemented by using inverse squared gradients for each parameter similar to ADAM/RMSprop that use root mean squared gradients. Obviously, 

- the numerical experiments seem a little lacking, RFD doesn't appear to show a significant improvement on Figures 3, 6, 7. MNIST and FashinMNIST are very small and perhaps too easy, any optimizer will ""max out"" any model pretty quickly I assume.


The below points are my personal subjective comments on the writing.
- I am a little reluctant to agree that this paper has much to do with Bayesian optimisation as suggested by the abstract and introduction. RFD fits a GP model to a single data point and  only uses the posterior mean, it is the same as kernel ridge regression.
- I felt the terminology of ""stochastic Taylor Expansion"" was rather unhelpful and somewhat counterproductive. In my mind, zeroth/first/second order Taylor expansion refer to constant/linear/quadratic local polynomial approximations to a function, however the given function approximations are non-linear (lemma 4.12) this description unfortunately rather mis-directed my thoughts.
- L69: as above, ""it naturally incorporates covariance based trust"" assumes a lot of context that has not been introduced in the paper at this point, upon first reading I was rather lost, upon second reading it makes sense but felt out of place.
- (there are many topics and details covered the main paper, would it be possible to focus on a few big ideas?)
- Table 1, Figure 2, what is the scale ""s"", I assume the length scale in the covariance $C()$ function?  This appears not to be introduced in the paper.
- L62, should the final term of the equation be $\frac{L}{2}||\omega - \Theta||^2$?

Limitations:
- the assumption of isometric in parameter space

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
### Summary

The paper ""Random Function Descent"" explores the limitations of classical worst-case optimization theory in explaining the success of optimization in machine learning and selecting appropriate step sizes. It establishes a connection between Bayesian Optimization and classical optimization through a ""stochastic Taylor approximation,"" rediscovering gradient descent. This rediscovery introduces a new step size schedule called Random Function Descent (RFD), which is scale-invariant. The analysis provides a theoretical foundation for common step size heuristics such as gradient clipping and gradual learning rate warmup. The paper also proposes a statistical procedure for estimating the RFD step size schedule and validates this theory with a case study on the MNIST dataset.

In the introduction, the paper emphasizes the importance of cost function minimization in machine learning, typically performed using gradient-based methods that require step sizes chosen by established heuristics. The paper aims to enhance the theoretical understanding of these heuristics and proposes RFD as a new algorithm based on this deeper insight. The authors highlight that classical optimization theory, which relies on \(L\)-smoothness, provides conservative learning rates unsuitable for average cases, necessitating the reliance on step size heuristics in machine learning.

The authors bridge the gap between Bayesian Optimization (BO) and gradient-based methods by introducing a stochastic Taylor approximation based on a forgetful BO posterior. This results in the RFD optimization method, which combines the properties of gradient descent with scale invariance and a complete step size schedule derived from BO. The contributions include proving the scale invariance of RFD, discussing common distributional assumptions in BO, establishing the connection between RFD and gradient descent, and investigating the step size schedule suggested by RFD.

The paper further develops a non-parametric variance estimation method robust to covariance kernel choices and extends RFD to mini-batch losses. The case study on the MNIST dataset demonstrates the practical application and effectiveness of the proposed RFD algorithm compared to traditional methods like Adam and stochastic gradient descent (SGD). The discussion includes limitations and potential extensions of the proposed method, emphasizing the need for new mathematical theory to address the risk-affine nature of RFD and its larger step sizes.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
### Strengths

1. **Innovative Approach**: The paper introduces a novel connection between Bayesian Optimization and gradient descent through the stochastic Taylor approximation, leading to the development of Random Function Descent (RFD). This approach provides a new perspective on step size selection and optimization in machine learning.
   
2. **Theoretical Foundation**: The analysis of RFD step sizes offers a solid theoretical foundation for commonly used heuristics such as gradient clipping and learning rate warmup. This bridges the gap between empirical practices and theoretical understanding.
   
3. **Scale Invariance**: RFD's scale invariance is a significant advantage, making it robust to different scales of input parameters and cost functions. This property is stronger than the affine invariance offered by the Newton method.
   
4. **Practical Validation**: The statistical procedure for estimating the RFD step size schedule and its validation on the MNIST dataset demonstrate the practical applicability and effectiveness of the proposed method. The case study shows that RFD can outperform traditional optimization methods like Adam and SGD.
   
5. **Comprehensive Analysis**: The paper provides a thorough investigation of the step size schedule suggested by RFD, including explicit formulas, asymptotic behavior, and explanations for gradient clipping and learning rate warmup. This comprehensive analysis enhances the understanding of RFD's behavior and potential benefits.

Weaknesses:
### Weaknesses

1. **Complexity and Accessibility**: The theoretical development and mathematical derivations in the paper are complex, which might limit the accessibility and understanding for practitioners who are not well-versed in advanced optimization theory and Bayesian methods.
   
2. **Assumptions and Simplifications**: The paper relies on certain assumptions, such as isotropic Gaussian random functions, which might not hold in all practical scenarios. The need for these assumptions could limit the generalizability of the proposed method.
   
3. **Risk-Affine Nature**: RFD's risk-affine nature, resulting in comparatively larger step sizes, might lead to instability in certain cases. The paper acknowledges this limitation and suggests that further work is needed to address this issue and develop new mathematical theories for convergence guarantees.
   
4. **Empirical Validation Scope**: While the MNIST case study is a valuable demonstration, the empirical validation is limited to a single dataset and a specific neural network architecture. Additional experiments on diverse datasets and models would strengthen the evidence for RFD's effectiveness.
   
5. **Variance Estimation Procedure**: The non-parametric variance estimation method, while robust, involves a bootstrapping procedure that could be computationally intensive. This might pose challenges for large-scale applications and require further optimization for practical use.

Limitations:
n/a

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper presents a method to estimate 3D human keypoints from a sequence of monocular 2D keypoints observations. It builds upon an existing sequence-to-sequence architecture (MixSTE), with a different output parameterization exploiting a kinematic skeletton prior, and different training losses. Lengths of the skeletton bones are predicted for the whole sequence to ensure consistency across frames (and maybe also left/right symmetry of the skeletton), and five 3D pose hypotheses with associated scores are predicted for each frame, parameterized as a list of 3D relative orientation for each bone with respect to its parent in the kinematic tree.

The authors develop theoretical arguments regarding the benefits of enforcing such structural priors in the predictions, and illustrate with a toy example the interest of having multiple predictions in case of ambiguous multimodal output. They validate their approach on Human3.6M and MPI-INF-3DHP datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The motivation for exploiting bone lengths constraints is well expressed, with a clear and detailed discussion provided in Section 4. The discussion of experimental and ablation results is insightful and shows – in a setting dependent on an oracle – benefits of the proposed approach.

Weaknesses:
The idea of enforcing body priors (constant bone length here) is not novel and has actually been heavily exploited in a whole line of work relying on more advanced parametric models such as SMPL [100]. This line of work would deserve being considered in the paper, as it encompass approaches suitable for 2D-to-3D sequence lifting such as e.g. [101].

The authors present a pose space consisting in 3D coordinates of joints linked by some rigid segments. Based on this definition, a natural pose parameterization would consist in the 3D direction of each segment, yet the authors chose to overparameterize poses by using relative 3D bone orientation instead. I understand that such choice can have practical benefits in term of biomechanical constraints and additional supervision signal when ground truth data is available, but such choice should be properly motivated, discussed and ablated in the paper.


The authors describe two ways of aggregating results L247 but do not state which one they use for MPI-INF-3DHP, and they only report oracle results on Human3.6M and for the ablations.

In my understanding, pose hypotheses are selected independently for each frame and there are no temporal terms in the training objectives or aggregation method. Since the proposed approach deals with temporal sequences, it would be worth evaluating the temporal consistency of the predictions, through qualitative video examples and quantitatively e.g. using joint acceleration metrics. Having multiple hypotheses for each frame brings combinatorial questions worth discussing in my opinion.

References:
- [100] Loper at al., “SMPL: A Skinned Multi-Person Linear Model”, at SIGGRAPH Asia 2015.
- [101] Baradel et al., “PoseBERT: A Generic Transformer Module for Temporal 3D Human Modeling”, in TPAMI 2022.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a MCL-based framework for multi-hypothesis 3D human pose estimation. This framework predicts skeletal parameters so that the predicted 3D poses in a sequence are constrained to one smooth manifold. To prove the superiority of such a framework, the paper presents detailed theoretical analysis on the drawback of unconstrained single-hypothesis HPE and why MPJPE alone is not enough for pose evaluation. The experiments show the proposed framework is capable of keeping the consistency of predicted poses and achieving state-of-the-art MPJPE in the meantime.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
* Simple and reasonable manifold representation. The proposed framework keeps the predicted human pose on the target manifold by representing the human pose with bone lengths and orientations, and the 3D pose is a direct inference from forward kinematics. The manifold is represented by the kinematics itself.
  
* Inspiring theoretical analysis on basic problems in 3D HPE. The paper arrives at some theoretical conclusions (line178-183), along with detailed proofs. They can provide some refreshing ideas on the innate drawbacks of traditional loss functions and MPJPE metrics.
  
* Good performance under both MPJPE and consistency measures, as validated in Table 2 and 3.

Weaknesses:
* Theoretical analysis on the advantage of multi-hypothesis methods over single-hypothesis ones could be added. Specifically, why a **constrained multi-hypothesis** method performs better than an **unconstrained single-hypothesis** method in MPJPE? Though this is already validated by the experiments, I personally believe it would make the paper more solid if the authors could make this analysis.

Minor problem:
* In Fig.4 (C) and (D), it is not quite clear how the estimations (crosses and triangles) correspond with the inputs (black dots). There might be some unexpected shifts, as the projections of the predicitons do not strictly align with the inputs (like in B).

Limitations:
Yes.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a new method to estimate 3D human pose from 2D observations (lifting). To ensure the body symmetry and temporal consistency, the authors disentangle human skeleton to two parts: temporally consistency bone scales and temporally variable bone rotations. The authors use fancy formulas to prove that, minimizing MSE loss could not gurantee manifold consistency. The quantitative and qualitative results on Human3.6m and MPI-INF-3DHP datasets show the superiority of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The evalution results in this paper is quite impressive, especially the newly proposed consistency metric. Figure 1 clearly shows the superiority of the proposed method. 

2. The authors try to prove the theoretical optimal of the proposed method, which is worth encouraging.

Weaknesses:
I am not an expert in manifold theory, therefore my questions only relate to human pose estimation. 

1. How to constrain the rotation space during training? 

2. The pose lifting method is quite similar to Anatomy3D (bone length + rotations). Can I view this paper an multi-hypothesis extension of Anatomy3D? Why? 

3. Previous paper ""POSE-NDF: MODELING HUMAN POSE MANIFOLDS WITH NEURAL DISTANCE FIELDS"" is similar to this paper in concepts. SMPL naturally guarantees bone length symmetry, and the learnable parameters (rotations and shape parameters) are similar to this paper in its functionality. It would be better to cite it. 

4. Suppose that, there is a virtual dataset, all 2D human joints are rendered (projected) from strictly symmetric 3D joints, then, could learning the lifting function on this virtual dataset using MSE loss guarantee the results all lie on manifold? 

5. (An optional question) The ground truth 3D joints of Human3.6M datasets come from the marker tracking on body surface, which naturally could not guarantee skeleton length consistency. Why learning symmetric bones yields better results (both Anatomy3D and the proposed methods)?

Limitations:
The authors addressed limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper propose ManiPose, a manifold-constrained multi-hypothesis model for 3D human pose lifting. The authors provide empirical and experimental evidence to show that joint position regression leads to inconsistent skeleton lengths. And they propose to predict globally consistent pose scale and individual joint rotations per frame (rather than joint positions) to constrain the predictions to the pose manifold. Empirical results demonstrates that the proposed ManiPose framework improves the pose consistency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper provides valuable theoretical analysis to support their arguments and provides intuitive toy examples to illustrate the ambiguity in pose lifting.
* The paper conducts extensive experiments on H36M and MPI-INF-3DHP datasets.

Weaknesses:
* The paper uses a multi-head design to predict multiple hypotheses. This design loses the flexibility of sampling different numbers of hypotheses and limits the maximum number of hypotheses to a small number. This often results in limited hypothesis diversity. In the experimental section, the authors do not provide numerical of visual measurements of hypothesis diversity.
* According to the comparison in Table 4, the manifold constraint proposed in this paper sacrifices MPJPE to improve pose consistency, serving as a trade-off approach between accuracy and consistency. Although the consistency is improved, it lags behind the traditional position regression or manifold regularization in accuracy, and does not bring essential improvement (improve both in accuracy and consistency) compared with these two methods.
* Missing comparison with two recent multi-hypothesis methods. [1] GFPose: Learning 3D Human Pose Prior with Gradient Fields. [2] DiffPose: Toward More Reliable 3D Pose Estimation.

Limitations:
As the authors discussed in the Limitations Section, they used forward kinematics to obtain joint positions, which can lead to error accumulation.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper presents a method to estimate 3D human keypoints from a sequence of monocular 2D keypoints observations. It builds upon an existing sequence-to-sequence architecture (MixSTE), with a different output parameterization exploiting a kinematic skeletton prior, and different training losses. Lengths of the skeletton bones are predicted for the whole sequence to ensure consistency across frames (and maybe also left/right symmetry of the skeletton), and five 3D pose hypotheses with associated scores are predicted for each frame, parameterized as a list of 3D relative orientation for each bone with respect to its parent in the kinematic tree.

The authors develop theoretical arguments regarding the benefits of enforcing such structural priors in the predictions, and illustrate with a toy example the interest of having multiple predictions in case of ambiguous multimodal output. They validate their approach on Human3.6M and MPI-INF-3DHP datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The motivation for exploiting bone lengths constraints is well expressed, with a clear and detailed discussion provided in Section 4. The discussion of experimental and ablation results is insightful and shows – in a setting dependent on an oracle – benefits of the proposed approach.

Weaknesses:
The idea of enforcing body priors (constant bone length here) is not novel and has actually been heavily exploited in a whole line of work relying on more advanced parametric models such as SMPL [100]. This line of work would deserve being considered in the paper, as it encompass approaches suitable for 2D-to-3D sequence lifting such as e.g. [101].

The authors present a pose space consisting in 3D coordinates of joints linked by some rigid segments. Based on this definition, a natural pose parameterization would consist in the 3D direction of each segment, yet the authors chose to overparameterize poses by using relative 3D bone orientation instead. I understand that such choice can have practical benefits in term of biomechanical constraints and additional supervision signal when ground truth data is available, but such choice should be properly motivated, discussed and ablated in the paper.


The authors describe two ways of aggregating results L247 but do not state which one they use for MPI-INF-3DHP, and they only report oracle results on Human3.6M and for the ablations.

In my understanding, pose hypotheses are selected independently for each frame and there are no temporal terms in the training objectives or aggregation method. Since the proposed approach deals with temporal sequences, it would be worth evaluating the temporal consistency of the predictions, through qualitative video examples and quantitatively e.g. using joint acceleration metrics. Having multiple hypotheses for each frame brings combinatorial questions worth discussing in my opinion.

References:
- [100] Loper at al., “SMPL: A Skinned Multi-Person Linear Model”, at SIGGRAPH Asia 2015.
- [101] Baradel et al., “PoseBERT: A Generic Transformer Module for Temporal 3D Human Modeling”, in TPAMI 2022.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a MCL-based framework for multi-hypothesis 3D human pose estimation. This framework predicts skeletal parameters so that the predicted 3D poses in a sequence are constrained to one smooth manifold. To prove the superiority of such a framework, the paper presents detailed theoretical analysis on the drawback of unconstrained single-hypothesis HPE and why MPJPE alone is not enough for pose evaluation. The experiments show the proposed framework is capable of keeping the consistency of predicted poses and achieving state-of-the-art MPJPE in the meantime.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
* Simple and reasonable manifold representation. The proposed framework keeps the predicted human pose on the target manifold by representing the human pose with bone lengths and orientations, and the 3D pose is a direct inference from forward kinematics. The manifold is represented by the kinematics itself.
  
* Inspiring theoretical analysis on basic problems in 3D HPE. The paper arrives at some theoretical conclusions (line178-183), along with detailed proofs. They can provide some refreshing ideas on the innate drawbacks of traditional loss functions and MPJPE metrics.
  
* Good performance under both MPJPE and consistency measures, as validated in Table 2 and 3.

Weaknesses:
* Theoretical analysis on the advantage of multi-hypothesis methods over single-hypothesis ones could be added. Specifically, why a **constrained multi-hypothesis** method performs better than an **unconstrained single-hypothesis** method in MPJPE? Though this is already validated by the experiments, I personally believe it would make the paper more solid if the authors could make this analysis.

Minor problem:
* In Fig.4 (C) and (D), it is not quite clear how the estimations (crosses and triangles) correspond with the inputs (black dots). There might be some unexpected shifts, as the projections of the predicitons do not strictly align with the inputs (like in B).

Limitations:
Yes.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a new method to estimate 3D human pose from 2D observations (lifting). To ensure the body symmetry and temporal consistency, the authors disentangle human skeleton to two parts: temporally consistency bone scales and temporally variable bone rotations. The authors use fancy formulas to prove that, minimizing MSE loss could not gurantee manifold consistency. The quantitative and qualitative results on Human3.6m and MPI-INF-3DHP datasets show the superiority of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The evalution results in this paper is quite impressive, especially the newly proposed consistency metric. Figure 1 clearly shows the superiority of the proposed method. 

2. The authors try to prove the theoretical optimal of the proposed method, which is worth encouraging.

Weaknesses:
I am not an expert in manifold theory, therefore my questions only relate to human pose estimation. 

1. How to constrain the rotation space during training? 

2. The pose lifting method is quite similar to Anatomy3D (bone length + rotations). Can I view this paper an multi-hypothesis extension of Anatomy3D? Why? 

3. Previous paper ""POSE-NDF: MODELING HUMAN POSE MANIFOLDS WITH NEURAL DISTANCE FIELDS"" is similar to this paper in concepts. SMPL naturally guarantees bone length symmetry, and the learnable parameters (rotations and shape parameters) are similar to this paper in its functionality. It would be better to cite it. 

4. Suppose that, there is a virtual dataset, all 2D human joints are rendered (projected) from strictly symmetric 3D joints, then, could learning the lifting function on this virtual dataset using MSE loss guarantee the results all lie on manifold? 

5. (An optional question) The ground truth 3D joints of Human3.6M datasets come from the marker tracking on body surface, which naturally could not guarantee skeleton length consistency. Why learning symmetric bones yields better results (both Anatomy3D and the proposed methods)?

Limitations:
The authors addressed limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper propose ManiPose, a manifold-constrained multi-hypothesis model for 3D human pose lifting. The authors provide empirical and experimental evidence to show that joint position regression leads to inconsistent skeleton lengths. And they propose to predict globally consistent pose scale and individual joint rotations per frame (rather than joint positions) to constrain the predictions to the pose manifold. Empirical results demonstrates that the proposed ManiPose framework improves the pose consistency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* The paper provides valuable theoretical analysis to support their arguments and provides intuitive toy examples to illustrate the ambiguity in pose lifting.
* The paper conducts extensive experiments on H36M and MPI-INF-3DHP datasets.

Weaknesses:
* The paper uses a multi-head design to predict multiple hypotheses. This design loses the flexibility of sampling different numbers of hypotheses and limits the maximum number of hypotheses to a small number. This often results in limited hypothesis diversity. In the experimental section, the authors do not provide numerical of visual measurements of hypothesis diversity.
* According to the comparison in Table 4, the manifold constraint proposed in this paper sacrifices MPJPE to improve pose consistency, serving as a trade-off approach between accuracy and consistency. Although the consistency is improved, it lags behind the traditional position regression or manifold regularization in accuracy, and does not bring essential improvement (improve both in accuracy and consistency) compared with these two methods.
* Missing comparison with two recent multi-hypothesis methods. [1] GFPose: Learning 3D Human Pose Prior with Gradient Fields. [2] DiffPose: Toward More Reliable 3D Pose Estimation.

Limitations:
As the authors discussed in the Limitations Section, they used forward kinematics to obtain joint positions, which can lead to error accumulation.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
In this paper, the authors proposed an essential problems: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? The authors presented a robust camera-insensitivity collaborative perception with a novel dynamic feature-based 3d neural modeling mechanism to address the issue. Moreover, to verify the effectiveness of the model, the authors also provided a new large-scale dataset, OPV2V-N for this field. The experiments result showcase the model’s robustness in proposed dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Strength:
1.	The paper presents an interesting viewpoint that is to recover noisy camera perceptual information from other agents’ views by modeling the collaborative neural rendering field representation, in which the model is divided into two stages: a time-invariant static background and time-varying dynamic foreground.s
2.	The paper develops a new dataset to fill the gap of the lack of a comprehensive collaborative perception dataset that accounts for different camera noise scenarios.
3.	The paper is well-organized and interesting to read.

Weaknesses:
1.	From my perspective, the paper lacks the theory analysis for the proposed method. Moreover, the authors fail to introduce the motivation of each sub-module in the presented model. For example, can the authors showcase the motivation of using Nerf for the static and dynamic fields, are there any dominant advantages of nerf, compared to other 3d reconstruction methods in this method?
2.	It is necessary to give more rigorous mathematic analysis of equations in this paper. Furthermore, the authors are required to introduce the details of each networks, including the training parameters, learning rate, weight values in eq. 12.

Limitations:
The current work focuses on addressing the camera-insensitivity problem in collaborative perception. It is evident that accurate reconstruction can compensate for the negative impact of noisy camera features on collaborative perception.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new problem: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? Therefore, RCDN, a Robust Camera-insensitivity collaborative perception with a novel Dynamic feature-based 3D Neural modeling mechanism is introduced. To validate the new method, the authors also provide a new dataset: OPV2V-N. RCDN serves as baseline here. Ablation Study shows for 5 models (F-cooper, Att-Fuse, Disco-Net, V2VNet, CoBEVT a significant improvement over their baselines, w/o RCDN.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper builds up on three pillars: single perception, collaborative perception and neural rendering. The base idea is novel to the best of my knowledge. The problem formulation is clear and well sounded, easy to follow. The System architecture is strong. The authors also focus on the differentiation between static and dynamic scenarios, especially for the neural fields both based on the BEV volume feature space. This differention is very important, not very often in detail discussed. The ablation study especially table 5.1 shows very accurate an increase of performance for different tasks static (lanes, free space) and dynamic perception. The experimentsl part introduces a new dataset, which is necessray for the investigation.

Weaknesses:
The overall system architecture sounds good. However, there are some open points for me, the impact of section 4.3 and 4.4, i.e. the neural fields part, seems open in terms of clarification. Example: What is difference between sf w , sbw in equation (7)?
The experimental section is a bit too short. I feel its not finished yet. However, there is limited space. The overall approach is not usable for realtime.

Limitations:
The most relevant limitation is the missibg real-time applicability.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents RCDN, a method to aggregate multi-sensor perception signals in dynamic environment.
The key idea, is to improve the aggregated multi-agent feature with the multi-view rendering loss.
At its core, RCDN gathers input streams at varying timesteps of multiple agents. The gathered images are fused into Birds Eye view (BEV) then further decoded into volume. 
The volumetric features are learned into static scene and dynamic scene components with NGP based representation.
Overall procedure is supervised with rendering loss, (cyclic) optical flow consistency.


The method is evaluated on new dataset, OPV2V-N, which is an updated version of OPV2V, with additional masking and optical flow. 
The results show that RCDN helps BEV segmentation with various backbones, compared to the model used without RCDN.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
1: poor

Strengths:
The main benefit of the RCDN, is that it is fairly easy to apply into different existing feature backbones, as it is the post-processing step built on top of BEV features. 
Experimentally, the usage of RCDN significantly improves the segmentations which implies that the features are better aligned throughout the noisy signals. 
This makes the work to be a great off-line data augmentation / preparation pipeline for generating BEV segmentation features. 
The paper additionally proposes OPV2V-N dataset, which may be somewhat valuable addition to the community.

Aside from technical perspective, the paper is easy to follow and well-written.

Weaknesses:
The paper's main weaknesses are two folds. 
1. The paper does not evaluate on tasks other than BEV segmentation. 
While I believe that the pixel-aligned features from NGP would give benefits over various vision tasks, the paper only demonstrates on smaller domain of work which undermines its actual potential. It would have been more interesting to compare how it impacts in different downstreaming tasks, such as detection / tracking.

2. Technical contribution seems to lack novelty. 
The paper is a mix of two known-to-work solutions; BEV feature decoding for segmentation (used with various baselines in the experiments), and NGP (or radiance field based) multi-view pixel / density alignment through rendering loss. Usage of rendering loss to improve segmentation map is well-investigated in different literatures in the NeRF community (e.g, semantic-nerf).

Limitations:
No concerning limitations are found.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposed Bird Eye View (BEV) semantic segmentation pipeline from collaborative perception, robust to motion blur, sensor noise, occlusion and even failure. The proposed a pipeline that adapts neural rendering techniques to overcome the noise/malfunction in camera capture and occlusion. With the proposed method combined with prior methods, performances on OPV2V-N (the proposed BEV semantic segmentation dataset) are improved.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper proposed to apply neural rendering concept for ‘robust’ collaborative-perception BEV segmentation. It is natural way of thinking to overcome noise/malfunction in the caption system but the way the paper adapts neural rendering to BEV segmentation is novel. And, the performance is verified with OPV2V-N dataset.

Weaknesses:
Evaluation is only performed with OPV2V-N dataset which may result in overfitting. More evaluation with different dataset is required. The author may need to compare methodologies on other dataset although the existing dataset do not have noise. The author also may add random noise to the prior dataset and run experiments.

The manuscript was uneasy to read and understand. The paper should re-written. The comments below are without understanding supplemental materials fully.
- The way proposed algorithm is combined with prior method is unclear. The reviewer guessed that the MCP module can be replaced with prior methods, but it is not stated.
- Many abbreviations are not explained sufficiently and terminologies the author defined are ambiguous and may be incorrect. 
- MCP is short for the multi-agents collaborative perception process but the paper did not explain MCP module in details with no reference
- BEV, no full name, no reference.
- “Camera-insensitivity” can be understood terminologies related to camera sensor sensitivity (how much the camera sensor accept photon…).
- Robust Camera-Insensitivity: Robust == Camera-sensitivity? The latter one may be redundant
- Line 6. introduce a new robust camera-insensitivity problem: cam be replaced “introduce BEV segmentation when the camera capture are unreliable (or noisy)?” Should be more concrete without ambiguous words
- Line19 “Ported to” mean?
- There are more unclear sentences.

Limitations:
.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces RCDN, a novel method for robust camera-insensitivity collaborative perception. This method aims to overcome challenges associated with noisy, obscured, or failed camera perspectives by using dynamic feature-based 3D neural modeling. RCDN constructs collaborative neural rendering field representations to recover failed perceptual messages sent by multiple agents. The proposed system consists of two collaborative field phases: a time-invariant static background field and a time-varying dynamic field. To validate RCDN, a new dataset called OPV2V-N was created. The paper demonstrates that RCDN improves the robustness of baseline methods in extreme camera-insensitivity settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
*Innovative Problem Addressing*: The paper tackles a significant real-world problem of camera insensitivity in multi-agent collaborative perception, which is crucial for autonomous systems.

*Novel Methodology*: The introduction of dynamic feature-based 3D neural modeling and the construction of collaborative neural rendering field representations are innovative approaches.

*Comprehensive Dataset*: The creation of the OPV2V-N dataset, which includes various camera failure scenarios, provides a robust platform for testing and validating the proposed method.

*Performance Improvement*: The extensive experiments and quantitative evaluations show significant improvements in robustness and performance over baseline methods.

*Detailed Evaluation*: The paper includes both quantitative and qualitative evaluations, along with ablation studies, which thoroughly demonstrate the effectiveness of RCDN.

Weaknesses:
*Complexity and Computation*: The proposed method involves complex modeling and multiple steps. The author should provide the latency.

Generalizability: The performance of RCDN is primarily validated on the OPV2V-N dataset, which may limit the generalizability of the results to other datasets or real-world scenarios.


*Failure Cases*: It would be nice if the authors provide failure cases, which is important.

Limitations:
Please see weakness and questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
In this paper, the authors proposed an essential problems: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? The authors presented a robust camera-insensitivity collaborative perception with a novel dynamic feature-based 3d neural modeling mechanism to address the issue. Moreover, to verify the effectiveness of the model, the authors also provided a new large-scale dataset, OPV2V-N for this field. The experiments result showcase the model’s robustness in proposed dataset.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Strength:
1.	The paper presents an interesting viewpoint that is to recover noisy camera perceptual information from other agents’ views by modeling the collaborative neural rendering field representation, in which the model is divided into two stages: a time-invariant static background and time-varying dynamic foreground.s
2.	The paper develops a new dataset to fill the gap of the lack of a comprehensive collaborative perception dataset that accounts for different camera noise scenarios.
3.	The paper is well-organized and interesting to read.

Weaknesses:
1.	From my perspective, the paper lacks the theory analysis for the proposed method. Moreover, the authors fail to introduce the motivation of each sub-module in the presented model. For example, can the authors showcase the motivation of using Nerf for the static and dynamic fields, are there any dominant advantages of nerf, compared to other 3d reconstruction methods in this method?
2.	It is necessary to give more rigorous mathematic analysis of equations in this paper. Furthermore, the authors are required to introduce the details of each networks, including the training parameters, learning rate, weight values in eq. 12.

Limitations:
The current work focuses on addressing the camera-insensitivity problem in collaborative perception. It is evident that accurate reconstruction can compensate for the negative impact of noisy camera features on collaborative perception.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new problem: how to overcome the issues caused by the failed camera perspectives, while stabilizing high collaborative performance with low calibration cost? Therefore, RCDN, a Robust Camera-insensitivity collaborative perception with a novel Dynamic feature-based 3D Neural modeling mechanism is introduced. To validate the new method, the authors also provide a new dataset: OPV2V-N. RCDN serves as baseline here. Ablation Study shows for 5 models (F-cooper, Att-Fuse, Disco-Net, V2VNet, CoBEVT a significant improvement over their baselines, w/o RCDN.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper builds up on three pillars: single perception, collaborative perception and neural rendering. The base idea is novel to the best of my knowledge. The problem formulation is clear and well sounded, easy to follow. The System architecture is strong. The authors also focus on the differentiation between static and dynamic scenarios, especially for the neural fields both based on the BEV volume feature space. This differention is very important, not very often in detail discussed. The ablation study especially table 5.1 shows very accurate an increase of performance for different tasks static (lanes, free space) and dynamic perception. The experimentsl part introduces a new dataset, which is necessray for the investigation.

Weaknesses:
The overall system architecture sounds good. However, there are some open points for me, the impact of section 4.3 and 4.4, i.e. the neural fields part, seems open in terms of clarification. Example: What is difference between sf w , sbw in equation (7)?
The experimental section is a bit too short. I feel its not finished yet. However, there is limited space. The overall approach is not usable for realtime.

Limitations:
The most relevant limitation is the missibg real-time applicability.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents RCDN, a method to aggregate multi-sensor perception signals in dynamic environment.
The key idea, is to improve the aggregated multi-agent feature with the multi-view rendering loss.
At its core, RCDN gathers input streams at varying timesteps of multiple agents. The gathered images are fused into Birds Eye view (BEV) then further decoded into volume. 
The volumetric features are learned into static scene and dynamic scene components with NGP based representation.
Overall procedure is supervised with rendering loss, (cyclic) optical flow consistency.


The method is evaluated on new dataset, OPV2V-N, which is an updated version of OPV2V, with additional masking and optical flow. 
The results show that RCDN helps BEV segmentation with various backbones, compared to the model used without RCDN.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
1: poor

Strengths:
The main benefit of the RCDN, is that it is fairly easy to apply into different existing feature backbones, as it is the post-processing step built on top of BEV features. 
Experimentally, the usage of RCDN significantly improves the segmentations which implies that the features are better aligned throughout the noisy signals. 
This makes the work to be a great off-line data augmentation / preparation pipeline for generating BEV segmentation features. 
The paper additionally proposes OPV2V-N dataset, which may be somewhat valuable addition to the community.

Aside from technical perspective, the paper is easy to follow and well-written.

Weaknesses:
The paper's main weaknesses are two folds. 
1. The paper does not evaluate on tasks other than BEV segmentation. 
While I believe that the pixel-aligned features from NGP would give benefits over various vision tasks, the paper only demonstrates on smaller domain of work which undermines its actual potential. It would have been more interesting to compare how it impacts in different downstreaming tasks, such as detection / tracking.

2. Technical contribution seems to lack novelty. 
The paper is a mix of two known-to-work solutions; BEV feature decoding for segmentation (used with various baselines in the experiments), and NGP (or radiance field based) multi-view pixel / density alignment through rendering loss. Usage of rendering loss to improve segmentation map is well-investigated in different literatures in the NeRF community (e.g, semantic-nerf).

Limitations:
No concerning limitations are found.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposed Bird Eye View (BEV) semantic segmentation pipeline from collaborative perception, robust to motion blur, sensor noise, occlusion and even failure. The proposed a pipeline that adapts neural rendering techniques to overcome the noise/malfunction in camera capture and occlusion. With the proposed method combined with prior methods, performances on OPV2V-N (the proposed BEV semantic segmentation dataset) are improved.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper proposed to apply neural rendering concept for ‘robust’ collaborative-perception BEV segmentation. It is natural way of thinking to overcome noise/malfunction in the caption system but the way the paper adapts neural rendering to BEV segmentation is novel. And, the performance is verified with OPV2V-N dataset.

Weaknesses:
Evaluation is only performed with OPV2V-N dataset which may result in overfitting. More evaluation with different dataset is required. The author may need to compare methodologies on other dataset although the existing dataset do not have noise. The author also may add random noise to the prior dataset and run experiments.

The manuscript was uneasy to read and understand. The paper should re-written. The comments below are without understanding supplemental materials fully.
- The way proposed algorithm is combined with prior method is unclear. The reviewer guessed that the MCP module can be replaced with prior methods, but it is not stated.
- Many abbreviations are not explained sufficiently and terminologies the author defined are ambiguous and may be incorrect. 
- MCP is short for the multi-agents collaborative perception process but the paper did not explain MCP module in details with no reference
- BEV, no full name, no reference.
- “Camera-insensitivity” can be understood terminologies related to camera sensor sensitivity (how much the camera sensor accept photon…).
- Robust Camera-Insensitivity: Robust == Camera-sensitivity? The latter one may be redundant
- Line 6. introduce a new robust camera-insensitivity problem: cam be replaced “introduce BEV segmentation when the camera capture are unreliable (or noisy)?” Should be more concrete without ambiguous words
- Line19 “Ported to” mean?
- There are more unclear sentences.

Limitations:
.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces RCDN, a novel method for robust camera-insensitivity collaborative perception. This method aims to overcome challenges associated with noisy, obscured, or failed camera perspectives by using dynamic feature-based 3D neural modeling. RCDN constructs collaborative neural rendering field representations to recover failed perceptual messages sent by multiple agents. The proposed system consists of two collaborative field phases: a time-invariant static background field and a time-varying dynamic field. To validate RCDN, a new dataset called OPV2V-N was created. The paper demonstrates that RCDN improves the robustness of baseline methods in extreme camera-insensitivity settings.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
*Innovative Problem Addressing*: The paper tackles a significant real-world problem of camera insensitivity in multi-agent collaborative perception, which is crucial for autonomous systems.

*Novel Methodology*: The introduction of dynamic feature-based 3D neural modeling and the construction of collaborative neural rendering field representations are innovative approaches.

*Comprehensive Dataset*: The creation of the OPV2V-N dataset, which includes various camera failure scenarios, provides a robust platform for testing and validating the proposed method.

*Performance Improvement*: The extensive experiments and quantitative evaluations show significant improvements in robustness and performance over baseline methods.

*Detailed Evaluation*: The paper includes both quantitative and qualitative evaluations, along with ablation studies, which thoroughly demonstrate the effectiveness of RCDN.

Weaknesses:
*Complexity and Computation*: The proposed method involves complex modeling and multiple steps. The author should provide the latency.

Generalizability: The performance of RCDN is primarily validated on the OPV2V-N dataset, which may limit the generalizability of the results to other datasets or real-world scenarios.


*Failure Cases*: It would be nice if the authors provide failure cases, which is important.

Limitations:
Please see weakness and questions.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper studies multi-view clustering and seeks to investigate the view cooperation issue. The authors consider DMVC as an unsupervised cooperative game and regard each view as a participant. Compared with the existing methods, this consideration is new and interesting. Based on the novel idea, the authors proposed SCE-MVC, a novel shapley-based cooperation enhancing multi-view clustering method. The paper is well-organized. The experiments are convincing.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper proposes a new point also an interesting point for multi-view clustering tasks, i.e., considering the multi-view collaboration as a cooperative game. 

2. The experiments are sufficient and convincing. The authors validate the method from many aspects. The proposed SCE-MVC obtains much better performance on six diverse datasets.

Weaknesses:
1. Figure 2 is confusing. The specific structure of View Cooperation Enhancing Module is not clearly presented.

2. There are many formulas and symbols. It is suggested to add a notation table.

3. Although the authors try to explain model (1), it is still difficult to understand Shapley Value from the model. In addition, many variables are not clearly explained. The authors should present more information about the model and explain all variables used in this model, such as S_i, {i}, s\{i}， etc.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The author introduces a Shapley-based cooperation enhancement framework aimed at fostering collaboration among different views. The SCE-MVC method incorporates cooperative game theory, considering each view as a participant in the model and assessing their contributions using the Shapley Value.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Viewing each view as an individual player within game theory represents a fresh perspective in multi-view clustering. Also, enhancing clustering performance through balancing view contribution is both well-founded and innovative.

Weaknesses:
1. Using the SCE module in an alignment-based framework only provides a marginal improvement to the model. Does this imply that the SCE module is ineffective in the alignment-based framework? 

2. The view contributions of alignment-based method is much balanced than view contributions of joint methods. Does this imply that the alignment-based method is much better than the joint method? It's not reasonable since the clustering performance of alignment-based methods may not necessarily be better than that of joint methods.

3. Is the complexity of computing Shapley values truly O(n!)? When dealing with a larger number of views, can this evaluation framework still be utilized for computation?

4. Are the loss functions L in Eqs (15) and (16) on page 6 the same? If so, there is a problem of inconsistent dependent variables. In addition, $D_ij$ in Eq. (9) is a scalar and should not be bolded.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The study centers on improving task performance via deep multi-view clustering (DMVC) and fostering cooperation among different views. Specifically, the study evaluates view contributions, emphasizing the significance of strengthening cooperation among views.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Considering multi-view tasks from a collaborative standpoint represents a novel approach, with the paper's motivation being notably fresh. Moreover, the paper elucidates potential contribution imbalances in the joint method and addresses them through the SCE method, thereby enhancing cooperation among views.

Weaknesses:
When dealing with datasets comprising more than two views, such as three views, how can one assess whether the contribution of the views has become more evenly distributed after employing SCE? While the paper visually presents the contributions of the views, could a quantitative method be provided for this evaluation?

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This research merges game theory with multi-view clustering by introducing the Shapley-based Cooperation Enhancing (SCE) approach. It features a module to systematically evaluate each view's contribution. The approach promotes view cooperation by adjusting the training convergence rate of view parameters based on their contributions. Extensive experiments on various datasets demonstrate the method's effectiveness when applied to different MVC frameworks.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1) The paper integrates the Shapley value from game theory into DMVC, allowing for precise assessment of each view's contribution.
2) Theoretical analysis is thorough, with clear and intuitive figures.
3) The manuscript is well-organized and clearly written.

Weaknesses:
The article categorizes DMVC into alignment-based and joint methods. What criteria were used for this classification? Furthermore, only one DMJC method is used as a representative for joint methods.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper firstly considered DMVC as an unsupervised cooperative game where each view can be regarded as a participant. Then, the authors introduced the shapley value and propose a novel MVC framework termed Shapley-based Cooperation Enhancing Multi-view Clustering (SCE-MVC), which evaluates view cooperation with game theory. In summary, this paper was well written with obvious superiority.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
-- A MVC framework was designed that utilizeD game theory and Shapley values to evaluate and elevate inter-view cooperation. 
-- The experiments were sufficient, and the analysis of the experimental results was adequate.

Weaknesses:
-- In this paper, why utilize $\phi_i$ to measure the contribution of views instead of the view weight $w_i$? The article's explanation on this is not clear enough, and there is a lack of experiments to demonstrate the relationship between $\phi_i$ and $w_i$.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


",Yes,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper studies multi-view clustering and seeks to investigate the view cooperation issue. The authors consider DMVC as an unsupervised cooperative game and regard each view as a participant. Compared with the existing methods, this consideration is new and interesting. Based on the novel idea, the authors proposed SCE-MVC, a novel shapley-based cooperation enhancing multi-view clustering method. The paper is well-organized. The experiments are convincing.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper proposes a new point also an interesting point for multi-view clustering tasks, i.e., considering the multi-view collaboration as a cooperative game. 

2. The experiments are sufficient and convincing. The authors validate the method from many aspects. The proposed SCE-MVC obtains much better performance on six diverse datasets.

Weaknesses:
1. Figure 2 is confusing. The specific structure of View Cooperation Enhancing Module is not clearly presented.

2. There are many formulas and symbols. It is suggested to add a notation table.

3. Although the authors try to explain model (1), it is still difficult to understand Shapley Value from the model. In addition, many variables are not clearly explained. The authors should present more information about the model and explain all variables used in this model, such as S_i, {i}, s\{i}， etc.

Limitations:
NA

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The author introduces a Shapley-based cooperation enhancement framework aimed at fostering collaboration among different views. The SCE-MVC method incorporates cooperative game theory, considering each view as a participant in the model and assessing their contributions using the Shapley Value.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
Viewing each view as an individual player within game theory represents a fresh perspective in multi-view clustering. Also, enhancing clustering performance through balancing view contribution is both well-founded and innovative.

Weaknesses:
1. Using the SCE module in an alignment-based framework only provides a marginal improvement to the model. Does this imply that the SCE module is ineffective in the alignment-based framework? 

2. The view contributions of alignment-based method is much balanced than view contributions of joint methods. Does this imply that the alignment-based method is much better than the joint method? It's not reasonable since the clustering performance of alignment-based methods may not necessarily be better than that of joint methods.

3. Is the complexity of computing Shapley values truly O(n!)? When dealing with a larger number of views, can this evaluation framework still be utilized for computation?

4. Are the loss functions L in Eqs (15) and (16) on page 6 the same? If so, there is a problem of inconsistent dependent variables. In addition, $D_ij$ in Eq. (9) is a scalar and should not be bolded.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The study centers on improving task performance via deep multi-view clustering (DMVC) and fostering cooperation among different views. Specifically, the study evaluates view contributions, emphasizing the significance of strengthening cooperation among views.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Considering multi-view tasks from a collaborative standpoint represents a novel approach, with the paper's motivation being notably fresh. Moreover, the paper elucidates potential contribution imbalances in the joint method and addresses them through the SCE method, thereby enhancing cooperation among views.

Weaknesses:
When dealing with datasets comprising more than two views, such as three views, how can one assess whether the contribution of the views has become more evenly distributed after employing SCE? While the paper visually presents the contributions of the views, could a quantitative method be provided for this evaluation?

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This research merges game theory with multi-view clustering by introducing the Shapley-based Cooperation Enhancing (SCE) approach. It features a module to systematically evaluate each view's contribution. The approach promotes view cooperation by adjusting the training convergence rate of view parameters based on their contributions. Extensive experiments on various datasets demonstrate the method's effectiveness when applied to different MVC frameworks.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1) The paper integrates the Shapley value from game theory into DMVC, allowing for precise assessment of each view's contribution.
2) Theoretical analysis is thorough, with clear and intuitive figures.
3) The manuscript is well-organized and clearly written.

Weaknesses:
The article categorizes DMVC into alignment-based and joint methods. What criteria were used for this classification? Furthermore, only one DMJC method is used as a representative for joint methods.

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper firstly considered DMVC as an unsupervised cooperative game where each view can be regarded as a participant. Then, the authors introduced the shapley value and propose a novel MVC framework termed Shapley-based Cooperation Enhancing Multi-view Clustering (SCE-MVC), which evaluates view cooperation with game theory. In summary, this paper was well written with obvious superiority.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
-- A MVC framework was designed that utilizeD game theory and Shapley values to evaluate and elevate inter-view cooperation. 
-- The experiments were sufficient, and the analysis of the experimental results was adequate.

Weaknesses:
-- In this paper, why utilize $\phi_i$ to measure the contribution of views instead of the view weight $w_i$? The article's explanation on this is not clear enough, and there is a lack of experiments to demonstrate the relationship between $\phi_i$ and $w_i$.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


Label: ",Yes,yes,Yes,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
1.This paper this contributes a new large-scale dataset named Traffic Object Importance (TOI) to addresses the problem of on-road object importance estimation, which utilizes video sequences captured from the driver’s perspective as the input.
2.The author also proposes a model that integrates multi-fold top-down guidance with the bottom-up feature.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1.This paper describes in great detail the specialized methodology and the structure of the models.

2.The scarcity of large-scale publicly available datasets hinder the development of on-road object importance estimation.

3. This paper considers the effect of traffic rule on object importance and successfully models this abstract concept by proposing an adaptive object-lane interaction mechanism.

Weaknesses:
1.In page 3 the author mentions that the traffic rule is crucial for object importance and focus on the traffic line rules , but the influence of traffic rules is varied, such as signalization. Therefore, in page 4 of table 1, the author is able to provide statistics on the scenario categories of TOI dataset and the traffic rule constraints within the dataset in experiment. 

2.In page 6, the author uses three common intention behaviors in driving to reflect the driver intention (i.e., turning left, going straight, and turning right). Since the video clip length is set at 16 frames, it is important to clarify if each of the three intentions corresponds to individual frames with the 16-frame clip cut during the training and testing phases, or if multiple intentions are present within the 16 frames. The authors should further elaborate and provide the proportion of each intention in the dataset.

3.Insufficient evaluation of indicators in the experimental section. The author may add another evaluation indicator.

4.The section three can include a schematic diagram of the annotation process for the dataset.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper collects a new large-scale dataset and proposes a novel method that integrates multi-fold top-down guidance with the bottom feature to address the problem of on-road object importance estimation. Specifically, the dataset is almost three times larger than the current publicly dataset for on-road object importance. In addition, this paper considers an adaptive mechanism for object-lane interaction, effectively modeling the impact of traffic rules on object importance. Experiments on several benchmarks validate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper makes several key contributions and demonstrates strengths for on-road object importance estimation

(1) This paper introduces a novel, extensive dataset, set to be released to the public, which is nearly three times the size of the current largest public dataset. 

(2) The method is well-motivated and straightforward. It estimates the importance of objects on the road, integrating various top-down guidance factors with bottom-up features, marking the first of its kind.

(3) The proposed method addresses the pivotal role of traffic rules in estimating object importance, an aspect previously overlooked by existing methods. It successfully encapsulates this concept through an innovative, adaptive mechanism for object-lane interaction.

Weaknesses:
This paper has also two weaknesses: 

(1) The paper does not provide a detailed discussion on the computational efficiency of the proposed method, which is crucial for real driving scenarios. Moreover, it is recommended to compare the model parameters and latency with other methods.

(2) Another concern lies in the practicality of the method. This method and the proposed dataset are both for single-camera scenarios, but in real autonomous driving scenarios, surrounding view is a more widely used type and a safer option. Will the proposed method also work well in the surrounding view?

Limitations:
The proposed method only considers the effect of three types of driver intentions on object importance estimation, which is not sufficient for complex driving scenarios. I carefully checked the paper and found no potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel dataset for on-road object importance estimation. More data about which objects are important for self-driving is included and is promised to be released. Moreover, a novel method that integrates driven intention, semantic context, and traffic rule is devised to tackle the related problem. The paper is well-written.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
A new dataset is introduced with rich data and labels. The presented method is novel and shown to be effective for the studied problem. Details about the dataset and the method are comprehensive and technically sound. Results are also promising.

Weaknesses:
Some of the concepts lack sufficient details to explain. See questions below.

Limitations:
The authors have mentioned limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work addresses the issue of estimating the importance of on-road objects using video sequences from a driver’s perspective, a critical task for enhancing driving safety. The authors introduce the Traffic Object Importance (TOI) dataset, which is significantly larger and more diverse than existing datasets, and propose a novel model that integrates multi-fold top-down guidance factors—driver intention, semantic context, and traffic rules—with bottom-up features for more accurate importance estimation. Experimental results demonstrate that the proposed model significantly outperforms state-of-the-art methods in on-road object importance estimation.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The introduction of the Traffic Object Importance (TOI) dataset, which is significantly larger and more diverse than existing datasets, provides a robust foundation for training and evaluating models in on-road object importance estimation, thereby addressing a major limitation in the field.

2.  The proposed model effectively integrates multi-fold top-down guidance factors—driver intention, semantic context, and traffic rules—with bottom-up features, which showed good performance for the TOI task.

Weaknesses:
1. Lack of description of the annotation details. 
How many annotators are involved in the annotation procedure? It would be good if the authors can provide some annotation procedure samples regarding the double-checking annotation mechanism and the triple-discussing annotation mechanism.

2. It seems this annotation will be varied according to different traffic rules. Since KITTI is collected in Germany, the annotators should be familiar to germany traffic rules. However the authors did not mention this information in their submission, thereby the label quality is doubtful.

3. The authors are encouraged to build up the first benchmark based on the proposed dataset by using various existing object detection methods, e.g., Yolo, with the proposed head or simpler head. It is interesting to see how the existing object detectors work on this new task.

4. More statistics of the dataset are encouraged to be given, e.g., the number of important object of different categories, etc.

Limitations:
yes the authors mentioned it in the appendix

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
1.This paper this contributes a new large-scale dataset named Traffic Object Importance (TOI) to addresses the problem of on-road object importance estimation, which utilizes video sequences captured from the driver’s perspective as the input.
2.The author also proposes a model that integrates multi-fold top-down guidance with the bottom-up feature.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1.This paper describes in great detail the specialized methodology and the structure of the models.

2.The scarcity of large-scale publicly available datasets hinder the development of on-road object importance estimation.

3. This paper considers the effect of traffic rule on object importance and successfully models this abstract concept by proposing an adaptive object-lane interaction mechanism.

Weaknesses:
1.In page 3 the author mentions that the traffic rule is crucial for object importance and focus on the traffic line rules , but the influence of traffic rules is varied, such as signalization. Therefore, in page 4 of table 1, the author is able to provide statistics on the scenario categories of TOI dataset and the traffic rule constraints within the dataset in experiment. 

2.In page 6, the author uses three common intention behaviors in driving to reflect the driver intention (i.e., turning left, going straight, and turning right). Since the video clip length is set at 16 frames, it is important to clarify if each of the three intentions corresponds to individual frames with the 16-frame clip cut during the training and testing phases, or if multiple intentions are present within the 16 frames. The authors should further elaborate and provide the proportion of each intention in the dataset.

3.Insufficient evaluation of indicators in the experimental section. The author may add another evaluation indicator.

4.The section three can include a schematic diagram of the annotation process for the dataset.

Limitations:
Yes.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper collects a new large-scale dataset and proposes a novel method that integrates multi-fold top-down guidance with the bottom feature to address the problem of on-road object importance estimation. Specifically, the dataset is almost three times larger than the current publicly dataset for on-road object importance. In addition, this paper considers an adaptive mechanism for object-lane interaction, effectively modeling the impact of traffic rules on object importance. Experiments on several benchmarks validate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper makes several key contributions and demonstrates strengths for on-road object importance estimation

(1) This paper introduces a novel, extensive dataset, set to be released to the public, which is nearly three times the size of the current largest public dataset. 

(2) The method is well-motivated and straightforward. It estimates the importance of objects on the road, integrating various top-down guidance factors with bottom-up features, marking the first of its kind.

(3) The proposed method addresses the pivotal role of traffic rules in estimating object importance, an aspect previously overlooked by existing methods. It successfully encapsulates this concept through an innovative, adaptive mechanism for object-lane interaction.

Weaknesses:
This paper has also two weaknesses: 

(1) The paper does not provide a detailed discussion on the computational efficiency of the proposed method, which is crucial for real driving scenarios. Moreover, it is recommended to compare the model parameters and latency with other methods.

(2) Another concern lies in the practicality of the method. This method and the proposed dataset are both for single-camera scenarios, but in real autonomous driving scenarios, surrounding view is a more widely used type and a safer option. Will the proposed method also work well in the surrounding view?

Limitations:
The proposed method only considers the effect of three types of driver intentions on object importance estimation, which is not sufficient for complex driving scenarios. I carefully checked the paper and found no potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents a novel dataset for on-road object importance estimation. More data about which objects are important for self-driving is included and is promised to be released. Moreover, a novel method that integrates driven intention, semantic context, and traffic rule is devised to tackle the related problem. The paper is well-written.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
A new dataset is introduced with rich data and labels. The presented method is novel and shown to be effective for the studied problem. Details about the dataset and the method are comprehensive and technically sound. Results are also promising.

Weaknesses:
Some of the concepts lack sufficient details to explain. See questions below.

Limitations:
The authors have mentioned limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work addresses the issue of estimating the importance of on-road objects using video sequences from a driver’s perspective, a critical task for enhancing driving safety. The authors introduce the Traffic Object Importance (TOI) dataset, which is significantly larger and more diverse than existing datasets, and propose a novel model that integrates multi-fold top-down guidance factors—driver intention, semantic context, and traffic rules—with bottom-up features for more accurate importance estimation. Experimental results demonstrate that the proposed model significantly outperforms state-of-the-art methods in on-road object importance estimation.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The introduction of the Traffic Object Importance (TOI) dataset, which is significantly larger and more diverse than existing datasets, provides a robust foundation for training and evaluating models in on-road object importance estimation, thereby addressing a major limitation in the field.

2.  The proposed model effectively integrates multi-fold top-down guidance factors—driver intention, semantic context, and traffic rules—with bottom-up features, which showed good performance for the TOI task.

Weaknesses:
1. Lack of description of the annotation details. 
How many annotators are involved in the annotation procedure? It would be good if the authors can provide some annotation procedure samples regarding the double-checking annotation mechanism and the triple-discussing annotation mechanism.

2. It seems this annotation will be varied according to different traffic rules. Since KITTI is collected in Germany, the annotators should be familiar to germany traffic rules. However the authors did not mention this information in their submission, thereby the label quality is doubtful.

3. The authors are encouraged to build up the first benchmark based on the proposed dataset by using various existing object detection methods, e.g., Yolo, with the proposed head or simpler head. It is interesting to see how the existing object detectors work on this new task.

4. More statistics of the dataset are encouraged to be given, e.g., the number of important object of different categories, etc.

Limitations:
yes the authors mentioned it in the appendix

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,Yes,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper investigated the problem of watermarking the Federated Graph Learning (FGL) models. This paper proposed the first backdoor-based FGL watermarking framework, called FedGMark. Specifically, to tackle the issues of ineffectiveness and vulnerability of existing methods, FedGMark designed two modules respectively. One is a Customized Watermark Generator (CWG). CWG aimed to generate the watermarked trigger samples (graphs) using each client's secret key. The other is the Robust Model Loader (RML). RML guaranteed that the watermarked models were certifiably robust against layer perturbation attacks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The first attempt to watermark federated graph learning models.
- The watermarked models are certifiably robust against attacks.
- Experiments on various datasets and models validate the effectiveness of FedGMark.

Weaknesses:
My major concerns are as follows.
1. Unclear threat model: The threat model and the problem formulation of this paper is unclear. What's the capability of the adversary and the defender? And more importantly, who is the adversary to steal the FGL model? This paper proposed to watermark the FGL model from the client side, which means the clients should be trustworthy. Is the central server an adversary in this paper? To my best knowledge, the typical threat model of various attacks in FL (e.g., backdoor attacks or Byzantine attacks) assumes that some of the clients may be malicious. The author should add a section on the threat model or problem formulation and clarify why they make these assumptions. This may be helpful to better understand the problem the authors tried to solve.
2. Privacy concern: I also worry that utilizing FedGMark may raise privacy concerns. In Section 3.4, the watermarked client needs to use a subset of its training graphs as the watermarked graphs. However, in FL, the client's graphs are privacy-sensitive, and using them to verify ownership may lead to privacy leakage. This is contrary to the original purpose (preserve privacy) of FL.
3. Missing experiments on the robustness against backdoor defense: This paper considers three different watermark removal attacks. However, since FedGMark utilizes backdoor-based watermarking methods, it is important to validate whether FedGMark is robust against backdoor defenses.
4. Missing introduction to ownership verification: This paper lacks an important section to introduce the ownership verification procedure of FedGMark.

Limitations:
This paper does not include a discussion of the limitations. However, I think there is a strong assumption that the clients need to be trustworthy in FedGMark. A discussion on this assumption is necessitated.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This manuscript introduces FedGMark, a backdoor-based watermarking method specifically designed to protect Federated Graph Learning (FedGL) models from illegal copying and model theft. They claim that the proposed FedGMark is the first method to safeguard the intellectual property of FedGL models, offering certified robustness against watermark removal attacks, leveraging unique graph structures and client information to create customized and diverse watermarks. Experiments demonstrate its effectiveness and robustness.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper introduces FedGMark to address the overlooked vulnerability of FedGL model ownership and identifies three main challenges in current watermarking techniques: inapplicability to graph data, vulnerability to removal attacks, and lack of formal guarantees. The proposed method, including CWG and RML, is clear and intuitive, and the authors have provided comprehensive experiments to support their approach.

Weaknesses:
1.	I strongly recommend setting a ""Threat Model"" subsection to clarify the potential security threats to FedGL. In my opinion, since the authors consider watermark removal attacks like distillation and finetuning, FedGL operates under a white-box setting.
2.	The paper assumes attackers know the internal information of the target watermarked model, enabling distillation, finetuning, and layer-perturbation attacks. However, I find the white-box setting narrow and trivial. The authors should consider black-box attacks, which are more challenging and meaningful. Many studies on black-box attacks can be found.
3.	In watermarking-related literature, robustness and fidelity are more frequently used terms than watermark accuracy and task accuracy.
4.	In the ""Inapplicable or Ineffective"" item, the authors state, ""For instance, they require input data to have the same size, while graphs can have varying sizes,"" which is not entirely accurate. For example, some Wavelet and DCT-based watermarking methods can be scalable.

Limitations:
Please refer to Weaknesses part

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the problem of protecting model ownership in the emerging domain of Federated Graph Learning (FedGL) by proposing FedGMark, a backdoor-based watermarking technique. The authors argue that existing watermarking approaches are either inapplicable to graph data or exhibit weaknesses in terms of robustness against removal attacks and lack of formal guarantees. FedGMark aims to overcome these limitations by leveraging graph structure and client information to learn customized watermarks, employing a novel graph learning (GL) architecture that enhances robustness, and providing certified robustness guarantees against layer-perturbation attacks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper clearly outlines the limitations of existing watermarking techniques and presents a well-motivated approach to address them. The design of FedGMark, with its CWG and RML modules, is tailored to the specific challenges of watermarking in FedGL.
-  FedGMark demonstrates promising empirical performance in terms of both main task accuracy and watermark accuracy. It outperforms the baseline approach (random graph-based watermarking) significantly, especially under watermark removal attacks.
- The paper provides theoretical guarantees for the robustness of FedGMark against layer-perturbation attacks, a unique and valuable contribution in the watermarking literature.

Weaknesses:
1. The reliance on pre-defined private keys for watermark generation may not be practical in all scenarios, and alternative key management methods should be explored.
2. The assumption of limited attacker knowledge about the watermarked model may not hold in practice. Evaluating FedGMark against more knowledgeable adversaries would provide a more realistic assessment.
3. The focus on FedAvg for model aggregation limits the exploration of other aggregation methods and their impact on watermark robustness.

Limitations:
1. FedGMark's evaluation focuses solely on FedAvg for aggregating client models. The impact of alternative aggregation methods (e.g., those prioritizing clients based on data quality or model performance) on both watermark robustness and overall FedGL model performance remains unexplored.
2. The paper acknowledges the increased computational cost of using more submodels (S) in RML but doesn't fully analyze the scalability of FedGMark. Further investigation is needed to understand how performance scales with different numbers of clients.
3. FedGMark relies heavily on structural modifications of the graph as the watermark.  The effectiveness and robustness of alternative trigger designs, such as feature-based triggers, hybrid triggers, or combinations of different trigger types, have not been explored. 
4. The paper lacks specific details about the hyperparameters used for training the GL models on the client-side. The impact of client training dynamics, particularly the choice of learning rate and the number of local epochs, on the watermarking performance and robustness of FedGMark remains unclear and requires further investigation.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work studies watermarking for federated graph learning (FGL) to protect the ownership of participants. It proposes a customized watermark generator for local clients that can capture the local graph structure and private client information, and a robust model loader consisting of multiple GL submodels and a majority-voting-based ensemble classifier, which can defend against the proposed layer-perturbation attack.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work claims to be the first to study watermarking for FGL models.

2. The method can leverage local graph and client information to generate customized watermarks.

3. The paper introduces a layer-perturbation attack to further demonstrate the certifiably robustness of the proposed backdoor-based watermarking for FGL.

4. The work is well-motivated with preliminary studies.

Weaknesses:
1. The concept of ownership in FGL can be confusing and is not well-defined in this paper. For example, can every client claim ownership of the federated trained model? Since the watermarks from different clients are different, can any single client claim entire ownership? Additionally, for clients who participate in the FL but do not have watermarks, how can they claim ownership?

2. The motivation for using local customized watermarks is not clear. The following problems arise: (1) It is unclear how to conduct ownership verification. Should it use the global watermark or the local watermarks? (2) If using a global watermark, what is the necessity of employing customized watermarks, or what is the adequate way to aggregate the global watermark from customized watermarks? If using local watermarks, how can the customized watermarks be used across clients?

3. The method requires specific GL models (to be split to multiple submodels), which can be hard to adapt to existing FGL methods, especially for advanced FGL methods.

4. The motivation for incorporating submodels for GL is missing. Why is this design necessary?

5. (1) What does “layer indexes” for splitting GL models mean? From section 3.3, it is not clear how the submodels are split and how the split submodels are decoupled from each other regarding cascaded structures. (2) Additionally, structural information can be important for graph learning. How would discarding such structural information impact in this setting?

6. The global model is obtained by simply averaging uploaded clients’ models (not weighted by data size, or applying proxy terms for regularization). Can this method address the potential heterogeneity issue when local watermarks are highly disparate from each other?

7. The proposed method can introduce efficiency issues, as it significantly increases the number of parameters and computation time.

Limitations:
Please see Weaknesses above.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper investigated the problem of watermarking the Federated Graph Learning (FGL) models. This paper proposed the first backdoor-based FGL watermarking framework, called FedGMark. Specifically, to tackle the issues of ineffectiveness and vulnerability of existing methods, FedGMark designed two modules respectively. One is a Customized Watermark Generator (CWG). CWG aimed to generate the watermarked trigger samples (graphs) using each client's secret key. The other is the Robust Model Loader (RML). RML guaranteed that the watermarked models were certifiably robust against layer perturbation attacks.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- The first attempt to watermark federated graph learning models.
- The watermarked models are certifiably robust against attacks.
- Experiments on various datasets and models validate the effectiveness of FedGMark.

Weaknesses:
My major concerns are as follows.
1. Unclear threat model: The threat model and the problem formulation of this paper is unclear. What's the capability of the adversary and the defender? And more importantly, who is the adversary to steal the FGL model? This paper proposed to watermark the FGL model from the client side, which means the clients should be trustworthy. Is the central server an adversary in this paper? To my best knowledge, the typical threat model of various attacks in FL (e.g., backdoor attacks or Byzantine attacks) assumes that some of the clients may be malicious. The author should add a section on the threat model or problem formulation and clarify why they make these assumptions. This may be helpful to better understand the problem the authors tried to solve.
2. Privacy concern: I also worry that utilizing FedGMark may raise privacy concerns. In Section 3.4, the watermarked client needs to use a subset of its training graphs as the watermarked graphs. However, in FL, the client's graphs are privacy-sensitive, and using them to verify ownership may lead to privacy leakage. This is contrary to the original purpose (preserve privacy) of FL.
3. Missing experiments on the robustness against backdoor defense: This paper considers three different watermark removal attacks. However, since FedGMark utilizes backdoor-based watermarking methods, it is important to validate whether FedGMark is robust against backdoor defenses.
4. Missing introduction to ownership verification: This paper lacks an important section to introduce the ownership verification procedure of FedGMark.

Limitations:
This paper does not include a discussion of the limitations. However, I think there is a strong assumption that the clients need to be trustworthy in FedGMark. A discussion on this assumption is necessitated.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This manuscript introduces FedGMark, a backdoor-based watermarking method specifically designed to protect Federated Graph Learning (FedGL) models from illegal copying and model theft. They claim that the proposed FedGMark is the first method to safeguard the intellectual property of FedGL models, offering certified robustness against watermark removal attacks, leveraging unique graph structures and client information to create customized and diverse watermarks. Experiments demonstrate its effectiveness and robustness.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper introduces FedGMark to address the overlooked vulnerability of FedGL model ownership and identifies three main challenges in current watermarking techniques: inapplicability to graph data, vulnerability to removal attacks, and lack of formal guarantees. The proposed method, including CWG and RML, is clear and intuitive, and the authors have provided comprehensive experiments to support their approach.

Weaknesses:
1.	I strongly recommend setting a ""Threat Model"" subsection to clarify the potential security threats to FedGL. In my opinion, since the authors consider watermark removal attacks like distillation and finetuning, FedGL operates under a white-box setting.
2.	The paper assumes attackers know the internal information of the target watermarked model, enabling distillation, finetuning, and layer-perturbation attacks. However, I find the white-box setting narrow and trivial. The authors should consider black-box attacks, which are more challenging and meaningful. Many studies on black-box attacks can be found.
3.	In watermarking-related literature, robustness and fidelity are more frequently used terms than watermark accuracy and task accuracy.
4.	In the ""Inapplicable or Ineffective"" item, the authors state, ""For instance, they require input data to have the same size, while graphs can have varying sizes,"" which is not entirely accurate. For example, some Wavelet and DCT-based watermarking methods can be scalable.

Limitations:
Please refer to Weaknesses part

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper addresses the problem of protecting model ownership in the emerging domain of Federated Graph Learning (FedGL) by proposing FedGMark, a backdoor-based watermarking technique. The authors argue that existing watermarking approaches are either inapplicable to graph data or exhibit weaknesses in terms of robustness against removal attacks and lack of formal guarantees. FedGMark aims to overcome these limitations by leveraging graph structure and client information to learn customized watermarks, employing a novel graph learning (GL) architecture that enhances robustness, and providing certified robustness guarantees against layer-perturbation attacks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper clearly outlines the limitations of existing watermarking techniques and presents a well-motivated approach to address them. The design of FedGMark, with its CWG and RML modules, is tailored to the specific challenges of watermarking in FedGL.
-  FedGMark demonstrates promising empirical performance in terms of both main task accuracy and watermark accuracy. It outperforms the baseline approach (random graph-based watermarking) significantly, especially under watermark removal attacks.
- The paper provides theoretical guarantees for the robustness of FedGMark against layer-perturbation attacks, a unique and valuable contribution in the watermarking literature.

Weaknesses:
1. The reliance on pre-defined private keys for watermark generation may not be practical in all scenarios, and alternative key management methods should be explored.
2. The assumption of limited attacker knowledge about the watermarked model may not hold in practice. Evaluating FedGMark against more knowledgeable adversaries would provide a more realistic assessment.
3. The focus on FedAvg for model aggregation limits the exploration of other aggregation methods and their impact on watermark robustness.

Limitations:
1. FedGMark's evaluation focuses solely on FedAvg for aggregating client models. The impact of alternative aggregation methods (e.g., those prioritizing clients based on data quality or model performance) on both watermark robustness and overall FedGL model performance remains unexplored.
2. The paper acknowledges the increased computational cost of using more submodels (S) in RML but doesn't fully analyze the scalability of FedGMark. Further investigation is needed to understand how performance scales with different numbers of clients.
3. FedGMark relies heavily on structural modifications of the graph as the watermark.  The effectiveness and robustness of alternative trigger designs, such as feature-based triggers, hybrid triggers, or combinations of different trigger types, have not been explored. 
4. The paper lacks specific details about the hyperparameters used for training the GL models on the client-side. The impact of client training dynamics, particularly the choice of learning rate and the number of local epochs, on the watermarking performance and robustness of FedGMark remains unclear and requires further investigation.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work studies watermarking for federated graph learning (FGL) to protect the ownership of participants. It proposes a customized watermark generator for local clients that can capture the local graph structure and private client information, and a robust model loader consisting of multiple GL submodels and a majority-voting-based ensemble classifier, which can defend against the proposed layer-perturbation attack.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work claims to be the first to study watermarking for FGL models.

2. The method can leverage local graph and client information to generate customized watermarks.

3. The paper introduces a layer-perturbation attack to further demonstrate the certifiably robustness of the proposed backdoor-based watermarking for FGL.

4. The work is well-motivated with preliminary studies.

Weaknesses:
1. The concept of ownership in FGL can be confusing and is not well-defined in this paper. For example, can every client claim ownership of the federated trained model? Since the watermarks from different clients are different, can any single client claim entire ownership? Additionally, for clients who participate in the FL but do not have watermarks, how can they claim ownership?

2. The motivation for using local customized watermarks is not clear. The following problems arise: (1) It is unclear how to conduct ownership verification. Should it use the global watermark or the local watermarks? (2) If using a global watermark, what is the necessity of employing customized watermarks, or what is the adequate way to aggregate the global watermark from customized watermarks? If using local watermarks, how can the customized watermarks be used across clients?

3. The method requires specific GL models (to be split to multiple submodels), which can be hard to adapt to existing FGL methods, especially for advanced FGL methods.

4. The motivation for incorporating submodels for GL is missing. Why is this design necessary?

5. (1) What does “layer indexes” for splitting GL models mean? From section 3.3, it is not clear how the submodels are split and how the split submodels are decoupled from each other regarding cascaded structures. (2) Additionally, structural information can be important for graph learning. How would discarding such structural information impact in this setting?

6. The global model is obtained by simply averaging uploaded clients’ models (not weighted by data size, or applying proxy terms for regularization). Can this method address the potential heterogeneity issue when local watermarks are highly disparate from each other?

7. The proposed method can introduce efficiency issues, as it significantly increases the number of parameters and computation time.

Limitations:
Please see Weaknesses above.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper studies the implicit bias of the Adam optimizer for logistic regression on linearly separable data. The authors prove that Adam converges to the linear classifier with the maximum $\ell_\infty$-margin. This result contrasts with the classical results on (stochastic) gradient descent (with or without momentum), which converge to the maximum $\ell_2$-margin solution.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The authors theoretically study a popular yet not well-understood optimization method, Adam, in the context of a well-studied classical problem: logistic regression on linearly separable data. This offers a solid and insightful contribution to understanding Adam. In particular, distinguishing Adam from (S)GD with/without momentum on this classical problem is a very interesting result.
- The technical contributions are also of independent interest, as they prove the results for Adam without relying on the stability constant (which is closer to practice) and use mild assumptions.
- The paper is well-written and easy to follow. The proof sketch provides a clear and comprehensive overview of the proof of the main theorem.

Weaknesses:
There are no major concerns about this paper. Below are minor comments and some areas for improvement:
- The paper does not provide an intuition behind why Adam achieves the maximum $\ell_\infty$-margin solution, in contrast to GD which achieves the maximum $\ell_2$-margin solution. It would be great if the authors could offer insights on how the $\ell_\infty$-margin arises instead of the $\ell_2$-margin, for example, through a warm-up analysis with SignGD ($\beta_1=\beta_2=0$) or RMSProp ($\beta_1=0$). One way to provide an intuition is as follows: Gunasekar et al. (2018) proved that steepest descent converges to the max-margin solution, implying that SignGD (steepest descent w.r.t. $\ell_\infty$-norm) converges to the maximum $\ell_\infty$-margin solution. Since SignGD is known to be a good proxy for Adam, this may offer an insight into why Adam converges to the maximum $\ell_\infty$-margin solution.
- The authors claim that the bounds in Corollary 4.7 are derived under worst-case scenarios and argue that this is why, in practice, we often observe margins converging faster than the bounds in the corollary. However, this statement lacks supporting evidence. The paper should prove that the rate of convergence is tight. Otherwise, the observed faster convergence of margins in experiments might simply indicate that the bound is not tight enough.
- Some sentences, including those in the abstract, use the term ""convergence"" unclearly. For example, in the abstract, ""this convergence occurs within polynomial time"" does not indicate the objective (the normalized $\ell_\infty$-margin in this case) of convergence. This could be confused with other notions of convergence, such as convergence in direction (i.e., $\frac{w_t}{\lVert w_t \rVert} \to \frac{w^*}{\lVert w^* \rVert}$).
- (page 6, line 183) According to the paper, the normalized $\ell_2$-margin converges at a speed of $O(\log \log t / \log t)$ when using GD. However, this should be corrected to $O(1 / \log t)$. According to Soudry et al. (2018), the normalized weight vector converges to the maximum $\ell_2$-margin vector ""in direction"" with a convergence rate of $O(\log \log t / \log t)$, i.e., $\lVert \frac{w_t}{\lVert w_t \rVert} - \frac{w^*}{\lVert w^* \rVert}\rVert = O(\log \log t / \log t)$. However, the normalized $\ell_2$-margin converges at the speed of $O(1/\log t)$, i.e., $|\min \frac{\langle w_t, y_t \cdot x_t \rangle}{\lVert w_t \rVert} - \frac{\langle w^*, y_t \cdot x_t \rangle}{\lVert w^* \rVert} | = O(1/\log t)$.
- (page 1, line 25) Typo: reply on -> rely on

---
[Gunasekar et al. 2018] Characterizing Implicit Bias in Terms of Optimization Geometry, ICML 2018.

[Soudry et al. 2018] The Implicit Bias of Gradient Descent on Separable Data, JMLR 2018.

Limitations:
The paper discusses its limitations and future directions, including the extension of the results to homogeneous neural networks and the analysis of stochastic Adam instead of full-batch Adam. I think both directions are promising avenues for future research.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The main focus of this paper is on the implicit bias of Adam for a single layer linear model which performs binary classification on separable data. In particular, assuming a zero stability constant $\epsilon$, this paper reveals that Adam finds the solution that achieves maximum-$\ell_\infty$-margin and characterizes the convergence rate for different classes of learning rate. This implicit bias is different from the $\ell_2$-norm minimization solution obtained by previous work which does not assume $\epsilon = 0$.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper is clearly written and well-organized. It is easy and clear to follow the argument and motivation of this paper, e.g., the proof sketch makes it easy to follow the way how the theoretical conclusion is developed. In addition, to me, the introduction of the related works are comprehensive and clear. It also clearly summarizes the difference between this paper and related works.
- The settings and results of this paper are new compared to previous works, i.e., previous works showed an $\ell_2$-norm solution implicit bias of Adam on separable data while this paper reveals an $\ell_{\infty}$-norm implicit bias when the stability constant $\epsilon$ is zero.

Weaknesses:
Despite the novelty of the theoretical claims, I still have several concerns, which I will discuss in the following.

1. Removing the stability constant $\epsilon$ makes the approach of this paper fails to characterize the influence of it, which, though being small, still has non-negligible effect, e.g., [1] observed that Adam with an $\epsilon$ that is too small does not even converge in certain circumstances. Treating $\epsilon$ as 0 seems a bit rough to me. 

    In addition, [2] showed that Adam minimizes the interpolation norm of gradients that depends on magnitudes of various hyper parameters including the stability constant $\epsilon$ (although [2] did not specify the types of loss functions and model architectures). [1] claimed that Adam with nonzero $\epsilon$ converges to $\ell_2$-norm solution, which is also verified by extensive experiments. As a comparison, this paper showed that both Adam with $\epsilon=0$ and with a non-negligible $\epsilon$ do not converge to the aforementioned solutions (line 210). In this sense, it seems that the conclusion reached by this paper contradicts with those derived by [1, 2]. Therefore, in my view, it would be better to start with a non-zero $\epsilon$ and let the case with $\epsilon=0$ be a special case to better capture the effect of the $\epsilon$ on the implicit bias.

2. This paper only considers a simple setting: the model is only a one-layer linear model and there is no stochastic sampling noise which is typically necessary in practice. As a comparison, authors of [1] have already studied Adam on separable data for homogeneous models, which can cover the single layer model of the current work as a special case. Thus excluding the stochastic sampling noise in the current work is kind of unsatisfying to me since the model is already a simple one. In addition, I think that the authors of the current work should at least repeat the experiments conducted in [1] (such as those for homogeneous neural networks) to further support their theoretical claims, especially considering that the authors claimed in line 210 that their results are more accurate than those of [1].

**Reference**

[1] Wang et al. The implicit bias for adaptive optimization algorithms on homogeneous neural networks.

[2] Cattaneo et al. On the Implicit Bias of Adam.

Limitations:
I do not find a separate limitation section in the main part. In my view, removing the stability constant is a bit rough. This makes the approach presented in this paper fail to capture how the implicit bias of Adam changes for different values of stability constant.  

The societal impact is not applicable to this work as it focuses on theoretical parts of Adam.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper examines the implicit bias of the Adam optimizer in the context of linear logistic regression, demonstrating that it converges to the maximum $\ell_\infty$-margin solution under certain mild conditions. The authors note that omitting the stability constant in Adam updates results in a different implicit bias than gradient descent, with or without momentum, which converges to the maximum $\ell_2$-margin solution. They also explore various decreasing learning rates, showing that Adam's margin converges at a polynomial rate, which is faster than that of gradient descent. Additionally, they provide numerical experiments that support their findings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Understanding why Adam performs better than GD in several settings is an important problem and this work takes an important step towards this by showing that Adam has a different implicit bias than GD in the linear logistic regression setting.

- Overall, the paper is well-written and easy to follow. The proof sketch in Section 6 is explained well.

Weaknesses:
- The paper does not present results for a fixed learning rate and only considers a set of decreasing learning rates.

    - The discussion in lines 50-52 and after Corollary 4.7, comparing the rates of Adam and GD, should also comment on the convergence rates for GD with adaptive learning rates (e.g., normalized GD) which have been shown to converge faster (see [1] and related work) than GD.

    - (Minor) In Assumption 4.3, ‘non-increasing’ should be ‘decreasing’ or ‘diminishing’.

- The results in prior work on implicit bias of GD are global (hold for any initialization), whereas the results in this paper require an assumption on the initialization (Ass. 4.2). Based on the discussion following this assumption, it might be better to state an assumption on the data and then show that the condition on the initialization holds as a Lemma.

- The paper does not comment on how optimal the obtained rates in Corollary 4.7 are.

**References:**

[1] Wang et al., Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling, 2023.

Limitations:
There is no potential negative impact of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the author studies the implicit bias of Adam optimizer for a single layer neural network on separable data. The author's work suggests that, compared to the implicit bias of gradient descent which is the max $ \ell_2 $ margin solution, Adam solution converges to the maximum $ \ell_\infty $ margin solution. For this work, authors take both exponential and logistic loss and find that the convergence speed is on a polynomial order. 

In order to confirm the results, the authors perform experiments on synthetic datasets for binary classification tasks and confirm Adam’s convergence to the $ \ell_\infty $ margin comparatively.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The work is novel (to the best of my knowledge) and interesting as the study of implicit bias of Adam could have further implications in characterizing the difference in optimization behavior of Adam vs SGD in practical scenarios. The assumptions of the work have been clearly presented and seem reasonable. With regard to the $ \epsilon $, while theoretical results are not provided, the authors include convincing experimental illustrations to convince me of the assumption. I also appreciate the well written proof sketch which helps convey the ideas

Weaknesses:
At the moment, I have some concerns with the paper which are more fit to be discussed as questions.

Limitations:
None

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper studies the implicit bias of the Adam optimizer for logistic regression on linearly separable data. The authors prove that Adam converges to the linear classifier with the maximum $\ell_\infty$-margin. This result contrasts with the classical results on (stochastic) gradient descent (with or without momentum), which converge to the maximum $\ell_2$-margin solution.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The authors theoretically study a popular yet not well-understood optimization method, Adam, in the context of a well-studied classical problem: logistic regression on linearly separable data. This offers a solid and insightful contribution to understanding Adam. In particular, distinguishing Adam from (S)GD with/without momentum on this classical problem is a very interesting result.
- The technical contributions are also of independent interest, as they prove the results for Adam without relying on the stability constant (which is closer to practice) and use mild assumptions.
- The paper is well-written and easy to follow. The proof sketch provides a clear and comprehensive overview of the proof of the main theorem.

Weaknesses:
There are no major concerns about this paper. Below are minor comments and some areas for improvement:
- The paper does not provide an intuition behind why Adam achieves the maximum $\ell_\infty$-margin solution, in contrast to GD which achieves the maximum $\ell_2$-margin solution. It would be great if the authors could offer insights on how the $\ell_\infty$-margin arises instead of the $\ell_2$-margin, for example, through a warm-up analysis with SignGD ($\beta_1=\beta_2=0$) or RMSProp ($\beta_1=0$). One way to provide an intuition is as follows: Gunasekar et al. (2018) proved that steepest descent converges to the max-margin solution, implying that SignGD (steepest descent w.r.t. $\ell_\infty$-norm) converges to the maximum $\ell_\infty$-margin solution. Since SignGD is known to be a good proxy for Adam, this may offer an insight into why Adam converges to the maximum $\ell_\infty$-margin solution.
- The authors claim that the bounds in Corollary 4.7 are derived under worst-case scenarios and argue that this is why, in practice, we often observe margins converging faster than the bounds in the corollary. However, this statement lacks supporting evidence. The paper should prove that the rate of convergence is tight. Otherwise, the observed faster convergence of margins in experiments might simply indicate that the bound is not tight enough.
- Some sentences, including those in the abstract, use the term ""convergence"" unclearly. For example, in the abstract, ""this convergence occurs within polynomial time"" does not indicate the objective (the normalized $\ell_\infty$-margin in this case) of convergence. This could be confused with other notions of convergence, such as convergence in direction (i.e., $\frac{w_t}{\lVert w_t \rVert} \to \frac{w^*}{\lVert w^* \rVert}$).
- (page 6, line 183) According to the paper, the normalized $\ell_2$-margin converges at a speed of $O(\log \log t / \log t)$ when using GD. However, this should be corrected to $O(1 / \log t)$. According to Soudry et al. (2018), the normalized weight vector converges to the maximum $\ell_2$-margin vector ""in direction"" with a convergence rate of $O(\log \log t / \log t)$, i.e., $\lVert \frac{w_t}{\lVert w_t \rVert} - \frac{w^*}{\lVert w^* \rVert}\rVert = O(\log \log t / \log t)$. However, the normalized $\ell_2$-margin converges at the speed of $O(1/\log t)$, i.e., $|\min \frac{\langle w_t, y_t \cdot x_t \rangle}{\lVert w_t \rVert} - \frac{\langle w^*, y_t \cdot x_t \rangle}{\lVert w^* \rVert} | = O(1/\log t)$.
- (page 1, line 25) Typo: reply on -> rely on

---
[Gunasekar et al. 2018] Characterizing Implicit Bias in Terms of Optimization Geometry, ICML 2018.

[Soudry et al. 2018] The Implicit Bias of Gradient Descent on Separable Data, JMLR 2018.

Limitations:
The paper discusses its limitations and future directions, including the extension of the results to homogeneous neural networks and the analysis of stochastic Adam instead of full-batch Adam. I think both directions are promising avenues for future research.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The main focus of this paper is on the implicit bias of Adam for a single layer linear model which performs binary classification on separable data. In particular, assuming a zero stability constant $\epsilon$, this paper reveals that Adam finds the solution that achieves maximum-$\ell_\infty$-margin and characterizes the convergence rate for different classes of learning rate. This implicit bias is different from the $\ell_2$-norm minimization solution obtained by previous work which does not assume $\epsilon = 0$.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- This paper is clearly written and well-organized. It is easy and clear to follow the argument and motivation of this paper, e.g., the proof sketch makes it easy to follow the way how the theoretical conclusion is developed. In addition, to me, the introduction of the related works are comprehensive and clear. It also clearly summarizes the difference between this paper and related works.
- The settings and results of this paper are new compared to previous works, i.e., previous works showed an $\ell_2$-norm solution implicit bias of Adam on separable data while this paper reveals an $\ell_{\infty}$-norm implicit bias when the stability constant $\epsilon$ is zero.

Weaknesses:
Despite the novelty of the theoretical claims, I still have several concerns, which I will discuss in the following.

1. Removing the stability constant $\epsilon$ makes the approach of this paper fails to characterize the influence of it, which, though being small, still has non-negligible effect, e.g., [1] observed that Adam with an $\epsilon$ that is too small does not even converge in certain circumstances. Treating $\epsilon$ as 0 seems a bit rough to me. 

    In addition, [2] showed that Adam minimizes the interpolation norm of gradients that depends on magnitudes of various hyper parameters including the stability constant $\epsilon$ (although [2] did not specify the types of loss functions and model architectures). [1] claimed that Adam with nonzero $\epsilon$ converges to $\ell_2$-norm solution, which is also verified by extensive experiments. As a comparison, this paper showed that both Adam with $\epsilon=0$ and with a non-negligible $\epsilon$ do not converge to the aforementioned solutions (line 210). In this sense, it seems that the conclusion reached by this paper contradicts with those derived by [1, 2]. Therefore, in my view, it would be better to start with a non-zero $\epsilon$ and let the case with $\epsilon=0$ be a special case to better capture the effect of the $\epsilon$ on the implicit bias.

2. This paper only considers a simple setting: the model is only a one-layer linear model and there is no stochastic sampling noise which is typically necessary in practice. As a comparison, authors of [1] have already studied Adam on separable data for homogeneous models, which can cover the single layer model of the current work as a special case. Thus excluding the stochastic sampling noise in the current work is kind of unsatisfying to me since the model is already a simple one. In addition, I think that the authors of the current work should at least repeat the experiments conducted in [1] (such as those for homogeneous neural networks) to further support their theoretical claims, especially considering that the authors claimed in line 210 that their results are more accurate than those of [1].

**Reference**

[1] Wang et al. The implicit bias for adaptive optimization algorithms on homogeneous neural networks.

[2] Cattaneo et al. On the Implicit Bias of Adam.

Limitations:
I do not find a separate limitation section in the main part. In my view, removing the stability constant is a bit rough. This makes the approach presented in this paper fail to capture how the implicit bias of Adam changes for different values of stability constant.  

The societal impact is not applicable to this work as it focuses on theoretical parts of Adam.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper examines the implicit bias of the Adam optimizer in the context of linear logistic regression, demonstrating that it converges to the maximum $\ell_\infty$-margin solution under certain mild conditions. The authors note that omitting the stability constant in Adam updates results in a different implicit bias than gradient descent, with or without momentum, which converges to the maximum $\ell_2$-margin solution. They also explore various decreasing learning rates, showing that Adam's margin converges at a polynomial rate, which is faster than that of gradient descent. Additionally, they provide numerical experiments that support their findings.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Understanding why Adam performs better than GD in several settings is an important problem and this work takes an important step towards this by showing that Adam has a different implicit bias than GD in the linear logistic regression setting.

- Overall, the paper is well-written and easy to follow. The proof sketch in Section 6 is explained well.

Weaknesses:
- The paper does not present results for a fixed learning rate and only considers a set of decreasing learning rates.

    - The discussion in lines 50-52 and after Corollary 4.7, comparing the rates of Adam and GD, should also comment on the convergence rates for GD with adaptive learning rates (e.g., normalized GD) which have been shown to converge faster (see [1] and related work) than GD.

    - (Minor) In Assumption 4.3, ‘non-increasing’ should be ‘decreasing’ or ‘diminishing’.

- The results in prior work on implicit bias of GD are global (hold for any initialization), whereas the results in this paper require an assumption on the initialization (Ass. 4.2). Based on the discussion following this assumption, it might be better to state an assumption on the data and then show that the condition on the initialization holds as a Lemma.

- The paper does not comment on how optimal the obtained rates in Corollary 4.7 are.

**References:**

[1] Wang et al., Achieving Margin Maximization Exponentially Fast via Progressive Norm Rescaling, 2023.

Limitations:
There is no potential negative impact of this work.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this work, the author studies the implicit bias of Adam optimizer for a single layer neural network on separable data. The author's work suggests that, compared to the implicit bias of gradient descent which is the max $ \ell_2 $ margin solution, Adam solution converges to the maximum $ \ell_\infty $ margin solution. For this work, authors take both exponential and logistic loss and find that the convergence speed is on a polynomial order. 

In order to confirm the results, the authors perform experiments on synthetic datasets for binary classification tasks and confirm Adam’s convergence to the $ \ell_\infty $ margin comparatively.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The work is novel (to the best of my knowledge) and interesting as the study of implicit bias of Adam could have further implications in characterizing the difference in optimization behavior of Adam vs SGD in practical scenarios. The assumptions of the work have been clearly presented and seem reasonable. With regard to the $ \epsilon $, while theoretical results are not provided, the authors include convincing experimental illustrations to convince me of the assumption. I also appreciate the well written proof sketch which helps convey the ideas

Weaknesses:
At the moment, I have some concerns with the paper which are more fit to be discussed as questions.

Limitations:
None

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,Yes,1.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces a novel multi-modal model, IsoFormer, designed to integrate DNA, RNA, and protein sequences for predicting RNA transcript isoform expression across different tissues. It utilizes pre-trained modality-specific encoders to generate embeddings that are then combined using a sophisticated aggregation method. The model demonstrates significant improvements in prediction accuracy compared to single-modality approaches.

Contriburion:

1. Developed the first general-purpose multi-modal model integrating DNA, RNA, and protein sequences.

2. Demonstrated successful application of transfer learning from modality-specific encoders.

3. Provided a new robust framework for advancing the prediction of RNA transcript isoform expression.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Innovative Integration of Modalities: The paper presents the first attempt to integrate three biological sequence modalities (DNA, RNA, and proteins) in a unified model, providing a comprehensive approach reflective of natural biological processes.

2. Effective Transfer Learning: IsoFormer effectively leverages pre-trained encoders to enhance its predictive power, benefiting from both intra-modal and inter-modal transfer learning.

3. Robust Evaluation: Experiments demonstrate the model's capability, outperforming existing methods in predicting transcript isoform expression, which is a challenging task due to its multi-modal nature.

Weaknesses:
1. Complexity and Computation: The model's complexity and the computational demands might limit its accessibility and use, particularly in environments with restricted resources.

2. More Comprehensive Evaluation for PLM's representation learning capability would make this paper better.

Limitations:
1. Data Requirements: The effectiveness of the model is contingent on the availability of comprehensive and high-quality multi-modal datasets.

2. Generalizability: While promising, the results are primarily validated on specific types of gene expression data, and its performance across broader biological applications remains to be fully assessed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new framework for the multi-modality pretrain model according to the Central dogma of biology. The method encode DNA, protein and RNA at the same time. The proposed method can transfer knowledge from the encoders pretraining and modalities.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is well-organized and easy to follow
The authors have proved that the multi-modality of single cell data can help model predictions.

Weaknesses:
Lack of experiments. 1. More ablation studies should be conducted about removing different modalities of the model in Table 2 ( e.g. we observe only RNA can achieve a high performance, what about protein+RNA? ).

 2. More dataset details should be included. The split of training/validation/test sets is not clear. If the authors do the experiments on the same dataset, they should split the dataset according to the tissues to validate the transferability of the proposed method.

Limitations:
The biological system is more complex. The proposed method only include the direct map from DNA to RNA. However, in real world, RNA can effect the expression of DNAs. More details should be discuss in the future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper models isoform relative abundance across tissues with a multimodal approach based on 3 pretrained encoders for DNA, RNA, and AA sequences.  DNA encoder uses a sequence centered on the gene’s TSS, RNA encoder uses the known isoform sequence from RNAseq and the protein encoder uses corresponding AA sequence.  They perform multiple ablations on the utility of having all 3 separate encoders and, given separate encoders, how to aggregate them into a single isoform specific embedding/prediction, and look at attention layers of RNA module to find biologically meaningful regions of attention.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Isoform level analysis using 3 separate pretrained encoders for DNA, RNA, and AA sequences is a good strategy.  The authors provide useful ablations on the utility of the multi modal approach and on modern strategies for combining those into a single embedding.  Looking for biolgoically meaningful interpretations of attention layers is useful.

Weaknesses:
I don’t think the authors can claim this is the first attempt to combine DNA, RNA, and AA modalities with techniques from NLP.   See the recent Evo work here https://www.biorxiv.org/content/10.1101/2024.02.27.582234v2 .  While they evaluate their performance against Enformer, that’s a large part of their own model.  So the evaluations have an intramural feel to them.  It’d be interesting to see how their strategy compares to other multi modal models such as Evo, and more RNA centric work like Borzoi, which looks at a more fine grained look of variant effects on the DNA to RNA relationship.  Looking at average isoform abundance across individuals is all well and good, but GTEx also has individual genomes, and genomic variation across individuals will also of course affect splicing patterns and which isoforms come from what individuals.

Limitations:
The authors should be more explicit about the limitation of using reference genome and known isoform sequences and how this kind of sweeps splicing as a function of dna sequence under the rug.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


",No,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces a novel multi-modal model, IsoFormer, designed to integrate DNA, RNA, and protein sequences for predicting RNA transcript isoform expression across different tissues. It utilizes pre-trained modality-specific encoders to generate embeddings that are then combined using a sophisticated aggregation method. The model demonstrates significant improvements in prediction accuracy compared to single-modality approaches.

Contriburion:

1. Developed the first general-purpose multi-modal model integrating DNA, RNA, and protein sequences.

2. Demonstrated successful application of transfer learning from modality-specific encoders.

3. Provided a new robust framework for advancing the prediction of RNA transcript isoform expression.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Innovative Integration of Modalities: The paper presents the first attempt to integrate three biological sequence modalities (DNA, RNA, and proteins) in a unified model, providing a comprehensive approach reflective of natural biological processes.

2. Effective Transfer Learning: IsoFormer effectively leverages pre-trained encoders to enhance its predictive power, benefiting from both intra-modal and inter-modal transfer learning.

3. Robust Evaluation: Experiments demonstrate the model's capability, outperforming existing methods in predicting transcript isoform expression, which is a challenging task due to its multi-modal nature.

Weaknesses:
1. Complexity and Computation: The model's complexity and the computational demands might limit its accessibility and use, particularly in environments with restricted resources.

2. More Comprehensive Evaluation for PLM's representation learning capability would make this paper better.

Limitations:
1. Data Requirements: The effectiveness of the model is contingent on the availability of comprehensive and high-quality multi-modal datasets.

2. Generalizability: While promising, the results are primarily validated on specific types of gene expression data, and its performance across broader biological applications remains to be fully assessed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a new framework for the multi-modality pretrain model according to the Central dogma of biology. The method encode DNA, protein and RNA at the same time. The proposed method can transfer knowledge from the encoders pretraining and modalities.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper is well-organized and easy to follow
The authors have proved that the multi-modality of single cell data can help model predictions.

Weaknesses:
Lack of experiments. 1. More ablation studies should be conducted about removing different modalities of the model in Table 2 ( e.g. we observe only RNA can achieve a high performance, what about protein+RNA? ).

 2. More dataset details should be included. The split of training/validation/test sets is not clear. If the authors do the experiments on the same dataset, they should split the dataset according to the tissues to validate the transferability of the proposed method.

Limitations:
The biological system is more complex. The proposed method only include the direct map from DNA to RNA. However, in real world, RNA can effect the expression of DNAs. More details should be discuss in the future work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper models isoform relative abundance across tissues with a multimodal approach based on 3 pretrained encoders for DNA, RNA, and AA sequences.  DNA encoder uses a sequence centered on the gene’s TSS, RNA encoder uses the known isoform sequence from RNAseq and the protein encoder uses corresponding AA sequence.  They perform multiple ablations on the utility of having all 3 separate encoders and, given separate encoders, how to aggregate them into a single isoform specific embedding/prediction, and look at attention layers of RNA module to find biologically meaningful regions of attention.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Isoform level analysis using 3 separate pretrained encoders for DNA, RNA, and AA sequences is a good strategy.  The authors provide useful ablations on the utility of the multi modal approach and on modern strategies for combining those into a single embedding.  Looking for biolgoically meaningful interpretations of attention layers is useful.

Weaknesses:
I don’t think the authors can claim this is the first attempt to combine DNA, RNA, and AA modalities with techniques from NLP.   See the recent Evo work here https://www.biorxiv.org/content/10.1101/2024.02.27.582234v2 .  While they evaluate their performance against Enformer, that’s a large part of their own model.  So the evaluations have an intramural feel to them.  It’d be interesting to see how their strategy compares to other multi modal models such as Evo, and more RNA centric work like Borzoi, which looks at a more fine grained look of variant effects on the DNA to RNA relationship.  Looking at average isoform abundance across individuals is all well and good, but GTEx also has individual genomes, and genomic variation across individuals will also of course affect splicing patterns and which isoforms come from what individuals.

Limitations:
The authors should be more explicit about the limitation of using reference genome and known isoform sequences and how this kind of sweeps splicing as a function of dna sequence under the rug.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


Label: ",No,no,Yes,0.0,0.6
"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper presents a study on tokenization by investigating the behavior of transformers on simple data generating processes . It shows that, in the absence of any tokenization, transformers trained on $k$th-order Markov processes predict characters according to a unigram model, which is quite problematic given how poor unigram models are at modeling Markovian data. Paradoxically, they observe that, even the simplest unigram model learnt by transformers *with the appropriate tokenization* is able to model the probability of sequences sampled from a $k$th-order Markov process.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written, with empirical observation intermingled with theory, which I quite liked. The theory is also accompanied by a lot if intuition, insight and interpretation, which really helps drive the point home.

Weaknesses:
- In section 3.2, the authors chose to focus on developing guarantees for a newly developed tokenizer, which, to my knowledge, is seldom used. It would've been maybe of greater use to the community to also, or instead, establish these guarantees for the more commonly-used tokenizers, such as BPE.

- I appreciate that this is mostly a theoretical study of tokenizers, and while the observations put forward are valuable, I found myself wondering what practical takeaways this paper presents to improve current tokenizers. That is something I would love the authors to comment on.

Limitations:
yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors show that tokenization is a fundamental property of transformer-based models, in the sense that without it, it is hard (if not impossible) to achieve low cross-entropy loss on next-word prediction. They show that tokenization helps breaking the unigram barrier (i.e., the best loss a unigram model can achieve) and give a theoretical characterization of the information tokenizers provide in terms of statistics on token distribution.

In particular:

Section 2.1 shows how models without tokenization cannot achieve the optimal cross-entropy loss, while when equipped with a tokenizer they break the unigram barrier.

Section 3 studies tokenizers that assign all possible substrings (up to length r) as tokens in the dictionary and shows their theoretical optimality in learning processes ruled by k-Markov chains. A consequence is that unigram models can also do that, in the limit.

Of course, this comes at the expense of the model's efficiency and potential attacks that one can run on an exponential number of tokens (i.e., the surface attack grows very large).

Finally, the authors show that tokenizers can trade off the vocabulary size while maintaining low cross-entropy (i.e., they can behave like an optimal model).

Finally, they extend the theoretical framework to LZW tokenizers.

Experiments are conducted on tokenized vs. non-tokenized models on {k=1}-Markov models and then on some real datasets to show that tokenizers trade-off complexity and efficiency in learning an optimal representation of the characters (and their frequency) in the training distribution.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The article studies an important problem, and I think there is value in the paper.
To the best of my knowledge, comparing BPE to non-tokenized models is new, and the figures give some interesting insights (e.g., Figure 2).
Your paper contains much theoretical work, contributing to its quality.
The results in the limit for unigram and BPE/LZW models are noticeable (Section 3 and Eq. 2).

In general, the results seem solid and are also interesting for linguists and NLP researchers. BPE and other tokenization methods find a trade-off between unigram models, as per Eq. 2, and the complexity of the resulting vocabulary (and model).

Weaknesses:
One of the main weaknesses of this work is how it is presented. 
Maybe it's me, but I found it quite hard to read. See questions.

Another concern is how theoretical results apply to real-world datasets. See questions, but Fig. 5 seems to mitigate the impact of your theoretical results.
In fact, for the vocabulary that grows larger, all the models have a similar value of cross entropy (for around ~50K tokens).

The article seems rushed, as there are many typos (I just listed some).
- Line 150 “the a”
- Line 173, “it make since” --> “sense”
- Line 175, eq. and many others --> Eq. (it’s not wrong per-se, but you capitalize Fig, Example, etc.)
- The Notation paragraph shouldn’t go with related works but should be in the next section.
- Notation in 2 is a bit sloppy (this is a personal suggestion): you can use D() and E() for the decoder/encoder (and enclose them with \mathcal).

Limitations:
Please see previous sections.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper offers theoretical insights into the importance of tokenization in language models. Tokenization is ostensibly the artifact that makes training LMs not an end-to-end procedure. This design choice introduces biases, as it is not optimized for exactly the same criterion as the full model. Yet training without a tokenization step almost always leads to worse language models. This paper attempts to provide reasons based in probability theory for this phenomenon. The authors first explore a toy setting, in which transformer models are tasked with predicting distributions from kth order Markov processes. They offer a theoretical explanation for why the error of models is capped at that of a unigram model and how tokenization alleviates this issue. They then show that tokenization schemes with certain properties can achieve the optimal cross-entropy loss. The work offers some basic experimental results confirming their insights.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
* Tokenization is a core part NLP pipelines yet it still needs to be better understood from a theoretical perspective. The questions that this paper tries to answer are very relevant for both model interpretability and further development
* The theory is presented in an understandable manner and results for specific popular tokenization schemes are provided.

Weaknesses:
* The theory presented in this work is for a specific type of data-generating distribution (kth order Markov) and we can’t directly extrapolate these results to linguistic distributions, which do not necessarily follow such a distribution. There is minimal discussion about the relationship between kth order Markov and linguistic distributions, which leaves the reader questioning how relevant these results actually are.
* Ultimately, the results are limited; they essentially show an expected result (the existence of an optimal unigram language model as the dictionary size grows to infinity). While some intuition can be gained from these results, the theoretical implications are limited.
* There is minimal discussion of the empirical results and what conclusions can be drawn from them. Given how much of the theory is not directly applicable to real language modeling settings, it feels like such a discussion should be very important

Limitations:
Limitations are not discussed in depth. The authors should address their limited experimental setting and the applicability of the results to linguistic distributions (which are not evidently k-order Markovian)

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the learning dynamics of unigram language models on top of tokenised vs non-tokenised data, comparing these models’ expected cross-entropy to the distribution’s entropy. The paper performs this analysis while considering different data generating distributions (mainly focusing on relatively simple markov chains), and different tokenisation methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper tackles an interesting and timely topic: how tokenisation enables language modeling. 

This paper provides an interesting theoretical analysis of the effect of tokenisation on unigram language modeling.

This paper also provides a couple of empirical analyses of how unigram models perform on real data.

The paper is relatively easy to follow, even though some of the mathematical results could be spelled out a bit more clearly to make it easier for a reader to follow them.

Weaknesses:
This paper’s framing, in my opinion, significantly over-claims its results:
* The title “Toward a Theory of Tokenization in LLMs” is very broad for the current contributions. A more appropriate title, in my opinion, would be “Analysing tokenisation’s effect on unigram distributions”, or something analogous to it. There is no “theory of tokenisation” being proposed here, but a theoretical analysis of how tokenisation affects a simple model’s cross-entropy.
* The abstract and introduction also significantly overclaim results, with statements such as “we study the end-to-end cross-entropy loss achieved by transformers with and without tokenization” while focusing on unigram cross-entropies. Transformers may serve as motivation to this work (as they initially learn unigram statistics), but are not in fact analysed here.

I think the paper would also be significantly more straightforward to read if the framing was fixed and it was clear from the start that the paper's analyses would focus on unigram models.

Limitations:
I think some important limitations are not sufficiently discussed in this paper. The most important of which is that the analysis focuses on unigram statistics, and transformers can clearly learn more than that. Expanding the limitations pointed out by Remark 3.3 in a dedicated limitations section could also be useful.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Analyze the provided review and determine whether the paper should be accepted or rejected based on its soundness, presentation, contribution, strengths and weaknesses, and the reviewers' opinions. Provide a label: Yes if the overall assessment leans toward acceptance, otherwise No. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper presents a study on tokenization by investigating the behavior of transformers on simple data generating processes . It shows that, in the absence of any tokenization, transformers trained on $k$th-order Markov processes predict characters according to a unigram model, which is quite problematic given how poor unigram models are at modeling Markovian data. Paradoxically, they observe that, even the simplest unigram model learnt by transformers *with the appropriate tokenization* is able to model the probability of sequences sampled from a $k$th-order Markov process.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well written, with empirical observation intermingled with theory, which I quite liked. The theory is also accompanied by a lot if intuition, insight and interpretation, which really helps drive the point home.

Weaknesses:
- In section 3.2, the authors chose to focus on developing guarantees for a newly developed tokenizer, which, to my knowledge, is seldom used. It would've been maybe of greater use to the community to also, or instead, establish these guarantees for the more commonly-used tokenizers, such as BPE.

- I appreciate that this is mostly a theoretical study of tokenizers, and while the observations put forward are valuable, I found myself wondering what practical takeaways this paper presents to improve current tokenizers. That is something I would love the authors to comment on.

Limitations:
yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors show that tokenization is a fundamental property of transformer-based models, in the sense that without it, it is hard (if not impossible) to achieve low cross-entropy loss on next-word prediction. They show that tokenization helps breaking the unigram barrier (i.e., the best loss a unigram model can achieve) and give a theoretical characterization of the information tokenizers provide in terms of statistics on token distribution.

In particular:

Section 2.1 shows how models without tokenization cannot achieve the optimal cross-entropy loss, while when equipped with a tokenizer they break the unigram barrier.

Section 3 studies tokenizers that assign all possible substrings (up to length r) as tokens in the dictionary and shows their theoretical optimality in learning processes ruled by k-Markov chains. A consequence is that unigram models can also do that, in the limit.

Of course, this comes at the expense of the model's efficiency and potential attacks that one can run on an exponential number of tokens (i.e., the surface attack grows very large).

Finally, the authors show that tokenizers can trade off the vocabulary size while maintaining low cross-entropy (i.e., they can behave like an optimal model).

Finally, they extend the theoretical framework to LZW tokenizers.

Experiments are conducted on tokenized vs. non-tokenized models on {k=1}-Markov models and then on some real datasets to show that tokenizers trade-off complexity and efficiency in learning an optimal representation of the characters (and their frequency) in the training distribution.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The article studies an important problem, and I think there is value in the paper.
To the best of my knowledge, comparing BPE to non-tokenized models is new, and the figures give some interesting insights (e.g., Figure 2).
Your paper contains much theoretical work, contributing to its quality.
The results in the limit for unigram and BPE/LZW models are noticeable (Section 3 and Eq. 2).

In general, the results seem solid and are also interesting for linguists and NLP researchers. BPE and other tokenization methods find a trade-off between unigram models, as per Eq. 2, and the complexity of the resulting vocabulary (and model).

Weaknesses:
One of the main weaknesses of this work is how it is presented. 
Maybe it's me, but I found it quite hard to read. See questions.

Another concern is how theoretical results apply to real-world datasets. See questions, but Fig. 5 seems to mitigate the impact of your theoretical results.
In fact, for the vocabulary that grows larger, all the models have a similar value of cross entropy (for around ~50K tokens).

The article seems rushed, as there are many typos (I just listed some).
- Line 150 “the a”
- Line 173, “it make since” --> “sense”
- Line 175, eq. and many others --> Eq. (it’s not wrong per-se, but you capitalize Fig, Example, etc.)
- The Notation paragraph shouldn’t go with related works but should be in the next section.
- Notation in 2 is a bit sloppy (this is a personal suggestion): you can use D() and E() for the decoder/encoder (and enclose them with \mathcal).

Limitations:
Please see previous sections.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper offers theoretical insights into the importance of tokenization in language models. Tokenization is ostensibly the artifact that makes training LMs not an end-to-end procedure. This design choice introduces biases, as it is not optimized for exactly the same criterion as the full model. Yet training without a tokenization step almost always leads to worse language models. This paper attempts to provide reasons based in probability theory for this phenomenon. The authors first explore a toy setting, in which transformer models are tasked with predicting distributions from kth order Markov processes. They offer a theoretical explanation for why the error of models is capped at that of a unigram model and how tokenization alleviates this issue. They then show that tokenization schemes with certain properties can achieve the optimal cross-entropy loss. The work offers some basic experimental results confirming their insights.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
* Tokenization is a core part NLP pipelines yet it still needs to be better understood from a theoretical perspective. The questions that this paper tries to answer are very relevant for both model interpretability and further development
* The theory is presented in an understandable manner and results for specific popular tokenization schemes are provided.

Weaknesses:
* The theory presented in this work is for a specific type of data-generating distribution (kth order Markov) and we can’t directly extrapolate these results to linguistic distributions, which do not necessarily follow such a distribution. There is minimal discussion about the relationship between kth order Markov and linguistic distributions, which leaves the reader questioning how relevant these results actually are.
* Ultimately, the results are limited; they essentially show an expected result (the existence of an optimal unigram language model as the dictionary size grows to infinity). While some intuition can be gained from these results, the theoretical implications are limited.
* There is minimal discussion of the empirical results and what conclusions can be drawn from them. Given how much of the theory is not directly applicable to real language modeling settings, it feels like such a discussion should be very important

Limitations:
Limitations are not discussed in depth. The authors should address their limited experimental setting and the applicability of the results to linguistic distributions (which are not evidently k-order Markovian)

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the learning dynamics of unigram language models on top of tokenised vs non-tokenised data, comparing these models’ expected cross-entropy to the distribution’s entropy. The paper performs this analysis while considering different data generating distributions (mainly focusing on relatively simple markov chains), and different tokenisation methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This paper tackles an interesting and timely topic: how tokenisation enables language modeling. 

This paper provides an interesting theoretical analysis of the effect of tokenisation on unigram language modeling.

This paper also provides a couple of empirical analyses of how unigram models perform on real data.

The paper is relatively easy to follow, even though some of the mathematical results could be spelled out a bit more clearly to make it easier for a reader to follow them.

Weaknesses:
This paper’s framing, in my opinion, significantly over-claims its results:
* The title “Toward a Theory of Tokenization in LLMs” is very broad for the current contributions. A more appropriate title, in my opinion, would be “Analysing tokenisation’s effect on unigram distributions”, or something analogous to it. There is no “theory of tokenisation” being proposed here, but a theoretical analysis of how tokenisation affects a simple model’s cross-entropy.
* The abstract and introduction also significantly overclaim results, with statements such as “we study the end-to-end cross-entropy loss achieved by transformers with and without tokenization” while focusing on unigram cross-entropies. Transformers may serve as motivation to this work (as they initially learn unigram statistics), but are not in fact analysed here.

I think the paper would also be significantly more straightforward to read if the framing was fixed and it was clear from the start that the paper's analyses would focus on unigram models.

Limitations:
I think some important limitations are not sufficiently discussed in this paper. The most important of which is that the analysis focuses on unigram statistics, and transformers can clearly learn more than that. Expanding the limitations pointed out by Remark 3.3 in a dedicated limitations section could also be useful.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,Yes,1.0,0.6
