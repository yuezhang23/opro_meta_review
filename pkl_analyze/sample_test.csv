raw_prompt,raw_answer,raw_prompt_second_round,raw_answer_second_round,parsed_answer,true_answer,accuracy,f1_score
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
Given that previous works for standalone SI2T (Scene-to-Image-to-Text) or ST2I (Scene-to-Text-to-Image) perform imperfectly in spatial understanding due to the difficulty of 3D spatial feature modeling, this paper proposes tp model SI2T and ST2I together under a dual learning framework. Within this dual framework, a novel 3D scene graph (3DSG) representation is introduced, to capture 3D spatial scene features. Moreover,  a Spatial Dual Discrete Diffusion framework is proposed to utilize the intermediate features of the 3D→X processes to guide the hard X→3D processes. Extensive experiments show the proposed method outperforms the mainstream T2I and I2T methods significantly.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
+ The proposed dual learning framework with 3D scene graph (3DSG) representation to enhance the 3D spatial feature modeling is novel.

+  The proposed Spatial Dual Discrete Diffusion framework is simple but effective, which can utilize the intermediate features of the 3D→X processes to guide the hard X→3D processes

+ Extensive experiments on VSD dataset have valid the effectiveness of this method. Compared with previous works (e.g., DALLE, CogView), the proposed works show more competitive performances.

Weaknesses:
- Although the proposed method in this paper has achieved good results, the methods compared in Table 2 are somewhat outdated and not the latest SOTA methods (e.g., DALLE-3, CogView-2). Could the authors compare with some of the latest text-to-image methods, such as the Stable Diffusion series, SD-1.5[1], SDXL[2], etc.?

- The authors should present some failure cases to analyze the shortcomings of the proposed method. For example, can the 3D scene graph always generate perfect outputs? If there are issues in the generated outputs, what impact will it have on the final results? What are the subsequent strategies to address these issues?


[1] https://huggingface.co/runwayml/stable-diffusion-v1-5
[2] Dustin Podell, Zion English and etc. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis, in arxiv, 2023.

Limitations:
Please refer tot the ""weakness"" section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presented a novel model for the SI2T and ST2I tasks. The proposed model combines the two dual tasks and let them mutually learn via intermediate feature sharing. Through this framework, both SI2T and ST2I are enhanced. The author also provide analysis that how this method works.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1.	The paper proposed a novel dual learning idea and the implemented model is reasonable.

2.	Generally comprehensive analysis on the designed model.

Weaknesses:
There is no major issue in my view. Some questions and advices are listed following: 

About the motivation: The author claims that the 3D feature is important for spatial understanding, and I agree with that. I wonder if 3D modeling is necessary for these two tasks and what special information could the 3D modeling provide. In this framework, it seems the model acquires the capability of 3D understanding via the pretraining of the 3DSG generator. Is the scene graph a reasonable way to modeling 3D features, and how does it match your task? The author should discuss the above points.

The DGAE is pretrained by the gold 3DSG dataset. Intuitively, the quality of this 3DSG dataset significantly affects the DGAE results and further the final results. Figure 7 analyzes this problem by an ablation study on the manual noise data. But I still have a concern that the performance of DGAE may be the bottleneck of the whole model. 

About the discrete modeling: To my knowledge, for the respective I2T or T2I task, the continuous diffusion models could achieve better performance, while the Table 6 presents the better final performance of the proposed dual model. Ignoring the efficiency problems, what superiority does discrete modeling has? 

This work provides a novel way to solve dual tasks. From your perspective, what kind of tasks can be solved with this dual learning method? 

Typos:
Line 767 “alignede” should be “aligned”

Line 810 “eh” should be “the”

Limitations:
No Limitations

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presented a novel dual learning framework for spatial image-to-text and spatial text-to-image generation. The main model is a combination of three discrete diffusion models, where an intermediate 3D presentation is first generated and then the image and text outputs are generated based on the 3D feature. The proposed method creatively divides the SI2T and ST2I to two pairs of dual stages, i.e., the Text-3D with 3D-Text and Image-3D and 3D-Image, and then takes a dual training strategy to enhance the hard X-3D processes with the easy 3D-X processes. The experimental results show that the proposed methods outperform current I2T and T2I models significantly.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1.	The novel and effective dual learning solution for the paired ST2I and SI2T tasks.
2.	The proposed methods provide an interesting perspective for the similar dual tasks, which is of value to the community.
3.	The paper is well-organized and easy to read. 
Overall, the idea is novel and the presentation is good. I tend to believe the quality of the paper meets the standard of the conference if the author could address the concerns I raised in the weaknesses.

Weaknesses:
Some confusion about the technical details:
1.	How many node categories does the 3DSG has and how to define the high-level spatial concepts? Does it follow the definition of previous works? Lack of related discussion and references.

2.	Do the VSG/TSG and 3DSG have the compatible node sets? Do you map the nodes among the three types of SG with a rule?

3.	About the codebooks of the discrete modeling. How do the codebooks (graph, image, and text) be initialized and updated during the whole training process?

The training of diffusion model is time consuming. I am a little worried about the efficiency problem. Can the author provide the efficiency analysis for each training stage?

The author takes GPT2 as the text decoder. Could it be replaced by other PLMs and how does it influence the performance? 

Minor Issues:

1.	Two small font size in Figure 2

2.	Line 248, the subscript “T23D” and “I23D” are ambiguous.

3.	Bars overlap in Figure 5.

Limitations:
Limitations have been discussed and I do not foresee any other negative impact from this work.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a dual learning framework and a 3D scene graph representation for enhancing spatial image-to-text and text-to-image tasks in visual-spatial understanding. The proposed Spatial Dual Discrete Diffusion (SD3) system outperforms existing methods on the VSD dataset, demonstrating the effectiveness of the dual learning strategy in improving spatial feature modeling and task performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors are commended for providing the code, which facilitates further investigation and reproducibility of the research findings.
2. This manuscript is the first to achieve synergy between two spatial-aware cross-modal dual generations.
3. The manuscript provides a comprehensive summary of the differences between the 2DSG and the proposed 3DSG.

Weaknesses:
1. Could the authors consider prioritizing the most relevant or impactful references and possibly discuss the contribution of each cited work in more detail? 
2. The introduction would benefit from a concise summary of the manuscript's contributions at its conclusion.
3. Could the authors briefly discuss how the treatment of spatial features in existing methods compares to the approach taken in this paper, and highlight the distinct contributions of this work?
4. The complexity of the SD3 framework may impact training and inference efficiency. The authors should provide a computational complexity analysis to assess its practicality.
5. Please provide complete results for Table 2 and consider additional datasets to validate the findings.
6. The current reliance on human evaluation for spatial assessment could be complemented with automated evaluation techniques to provide a more comprehensive assessment approach.

Limitations:
The authors have adequately discussed the limitations and potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
Given that previous works for standalone SI2T (Scene-to-Image-to-Text) or ST2I (Scene-to-Text-to-Image) perform imperfectly in spatial understanding due to the difficulty of 3D spatial feature modeling, this paper proposes tp model SI2T and ST2I together under a dual learning framework. Within this dual framework, a novel 3D scene graph (3DSG) representation is introduced, to capture 3D spatial scene features. Moreover,  a Spatial Dual Discrete Diffusion framework is proposed to utilize the intermediate features of the 3D→X processes to guide the hard X→3D processes. Extensive experiments show the proposed method outperforms the mainstream T2I and I2T methods significantly.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
+ The proposed dual learning framework with 3D scene graph (3DSG) representation to enhance the 3D spatial feature modeling is novel.

+  The proposed Spatial Dual Discrete Diffusion framework is simple but effective, which can utilize the intermediate features of the 3D→X processes to guide the hard X→3D processes

+ Extensive experiments on VSD dataset have valid the effectiveness of this method. Compared with previous works (e.g., DALLE, CogView), the proposed works show more competitive performances.

Weaknesses:
- Although the proposed method in this paper has achieved good results, the methods compared in Table 2 are somewhat outdated and not the latest SOTA methods (e.g., DALLE-3, CogView-2). Could the authors compare with some of the latest text-to-image methods, such as the Stable Diffusion series, SD-1.5[1], SDXL[2], etc.?

- The authors should present some failure cases to analyze the shortcomings of the proposed method. For example, can the 3D scene graph always generate perfect outputs? If there are issues in the generated outputs, what impact will it have on the final results? What are the subsequent strategies to address these issues?


[1] https://huggingface.co/runwayml/stable-diffusion-v1-5
[2] Dustin Podell, Zion English and etc. SDXL: Improving Latent Diffusion Models for High-Resolution Image Synthesis, in arxiv, 2023.

Limitations:
Please refer tot the ""weakness"" section.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper presented a novel model for the SI2T and ST2I tasks. The proposed model combines the two dual tasks and let them mutually learn via intermediate feature sharing. Through this framework, both SI2T and ST2I are enhanced. The author also provide analysis that how this method works.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1.	The paper proposed a novel dual learning idea and the implemented model is reasonable.

2.	Generally comprehensive analysis on the designed model.

Weaknesses:
There is no major issue in my view. Some questions and advices are listed following: 

About the motivation: The author claims that the 3D feature is important for spatial understanding, and I agree with that. I wonder if 3D modeling is necessary for these two tasks and what special information could the 3D modeling provide. In this framework, it seems the model acquires the capability of 3D understanding via the pretraining of the 3DSG generator. Is the scene graph a reasonable way to modeling 3D features, and how does it match your task? The author should discuss the above points.

The DGAE is pretrained by the gold 3DSG dataset. Intuitively, the quality of this 3DSG dataset significantly affects the DGAE results and further the final results. Figure 7 analyzes this problem by an ablation study on the manual noise data. But I still have a concern that the performance of DGAE may be the bottleneck of the whole model. 

About the discrete modeling: To my knowledge, for the respective I2T or T2I task, the continuous diffusion models could achieve better performance, while the Table 6 presents the better final performance of the proposed dual model. Ignoring the efficiency problems, what superiority does discrete modeling has? 

This work provides a novel way to solve dual tasks. From your perspective, what kind of tasks can be solved with this dual learning method? 

Typos:
Line 767 “alignede” should be “aligned”

Line 810 “eh” should be “the”

Limitations:
No Limitations

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presented a novel dual learning framework for spatial image-to-text and spatial text-to-image generation. The main model is a combination of three discrete diffusion models, where an intermediate 3D presentation is first generated and then the image and text outputs are generated based on the 3D feature. The proposed method creatively divides the SI2T and ST2I to two pairs of dual stages, i.e., the Text-3D with 3D-Text and Image-3D and 3D-Image, and then takes a dual training strategy to enhance the hard X-3D processes with the easy 3D-X processes. The experimental results show that the proposed methods outperform current I2T and T2I models significantly.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1.	The novel and effective dual learning solution for the paired ST2I and SI2T tasks.
2.	The proposed methods provide an interesting perspective for the similar dual tasks, which is of value to the community.
3.	The paper is well-organized and easy to read. 
Overall, the idea is novel and the presentation is good. I tend to believe the quality of the paper meets the standard of the conference if the author could address the concerns I raised in the weaknesses.

Weaknesses:
Some confusion about the technical details:
1.	How many node categories does the 3DSG has and how to define the high-level spatial concepts? Does it follow the definition of previous works? Lack of related discussion and references.

2.	Do the VSG/TSG and 3DSG have the compatible node sets? Do you map the nodes among the three types of SG with a rule?

3.	About the codebooks of the discrete modeling. How do the codebooks (graph, image, and text) be initialized and updated during the whole training process?

The training of diffusion model is time consuming. I am a little worried about the efficiency problem. Can the author provide the efficiency analysis for each training stage?

The author takes GPT2 as the text decoder. Could it be replaced by other PLMs and how does it influence the performance? 

Minor Issues:

1.	Two small font size in Figure 2

2.	Line 248, the subscript “T23D” and “I23D” are ambiguous.

3.	Bars overlap in Figure 5.

Limitations:
Limitations have been discussed and I do not foresee any other negative impact from this work.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a dual learning framework and a 3D scene graph representation for enhancing spatial image-to-text and text-to-image tasks in visual-spatial understanding. The proposed Spatial Dual Discrete Diffusion (SD3) system outperforms existing methods on the VSD dataset, demonstrating the effectiveness of the dual learning strategy in improving spatial feature modeling and task performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors are commended for providing the code, which facilitates further investigation and reproducibility of the research findings.
2. This manuscript is the first to achieve synergy between two spatial-aware cross-modal dual generations.
3. The manuscript provides a comprehensive summary of the differences between the 2DSG and the proposed 3DSG.

Weaknesses:
1. Could the authors consider prioritizing the most relevant or impactful references and possibly discuss the contribution of each cited work in more detail? 
2. The introduction would benefit from a concise summary of the manuscript's contributions at its conclusion.
3. Could the authors briefly discuss how the treatment of spatial features in existing methods compares to the approach taken in this paper, and highlight the distinct contributions of this work?
4. The complexity of the SD3 framework may impact training and inference efficiency. The authors should provide a computational complexity analysis to assess its practicality.
5. Please provide complete results for Table 2 and consider additional datasets to validate the findings.
6. The current reliance on human evaluation for spatial assessment could be complemented with automated evaluation techniques to provide a more comprehensive assessment approach.

Limitations:
The authors have adequately discussed the limitations and potential negative societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposes using the \( \ell_p \)-norm, specifically the \( \ell_1 \)-norm, to replace the classic squared \( \ell_2 \)-norm convolution in 3D tasks. The \( \ell_1 \)-norm kernel function relies on addition, reducing computational cost. Initial gradient implementation revealed insufficient gradient values. To address this, the authors gradually transition the significant gradient from \( \ell_2 \) to \( \ell_1 \) as training progresses and employ momentum updates and learning rate scheduling. By replacing traditional 3D networks with their \( \ell_1 \)-norm networks, the paper demonstrates competitive performance on ModelNet classification and S3DIS semantic segmentation tasks at significantly lower costs.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
i)  The proposed 3D \( \ell_1 \)-norm Net is novel and inspiring for 3D tasks,  which is broadly applicable somewhat.

ii)  The proposed tricks (the optimizer and the customized operator) make the \ell_1-norm network get competitive performance at a lower cost.

iii)  The proof of the appendix is detailed and carefully.

Weaknesses:
Minor typos error:

i) In L-651, it should be ""The *\ell_p*-norm"" rather than """"The \ell_p norm""""

ii) Very few words require consistent capitalization.

iii) Formulas in separate rows may have missing order numbers, it is recommended to unify this style.

iv) The reference format is not uniform enough.

Limitations:
More generalization experiments on other datasets can be considered.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes for using the $\ell_{p}$ norm in 3D convolution as a substitute for the inner product. Out of different choices, the authors pick the $\ell_{1}$-norm because it's faster and uses less energy than the inner product. This is because the $\ell_{1}$-norm relies on addition, which is simpler computationally. To optimize the $\ell_{1}$-norm-based convolution models, they propose a tailored optimization approach leveraging Mixed Gradient Strategy and Dynamic Learning Rate Controller. Finally, the method is evaluated on ModelNet10 and ModelNet40 datasets, showcasing its usage in learning point cloud features for object classification.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1 ) The writing and organization of this paper is great.

2 ) The experiments are sufficient in both main paper and supp, ranging from global Tasks, Semi-dense Prediction, Dense Prediction.

3 ) The idea of replacing traditional convolution with a ℓ𝑝-based convolution is quite foundational and novel for 3D tasks.

4 ) The proposed methods can be easily extended to other 3D backbones, which can be more environmentally friendly.

5 ) The proposed methods demonstrate good performance on most tasks (such as ModelNet classification and S3DIS semantic segmentation) at a significantly lower cost.

Weaknesses:
1 )  The baseline models should be detailed introduced, including the model structure, integration (i.e., replacing) method of the proposed method, just as Table 8 from Ablation Experiments.

2 ) The layout of Table 5 needs to be further optimized, such as splitting it into two independent tables for parallel layout.

Limitations:
See weakness and questions.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of enhancing the representational capacity of traditional convolution methods. To tackle this issue, the authors introduce a novel convolution approach based on $\ell_p$-norm and offer customized optimization strategies to expedite the training process. Extensive theoretical and empirical results have verified that the proposed algorithms demonstrate competitive performance compared to other baselines.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The concept of using the $\ell_p$ norm-based kernel to develop a convolution operator is intriguing. This proposed method has the potential to notably enhance the flexibility of the traditional convolution operator.

Weaknesses:
1. The writing of this paper requires improvement, particularly in providing additional details regarding the proposed method. The current version of this manuscript may lead to reader confusion.

2. The role of the theoretical results presented in this paper is unclear. There appears to be a gap between the theoretical guarantees and the empirical evidence provided.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a new convolution method based on the \( L_p \)-norm. The authors provide a theoretical foundation by proving the universal approximation theorem for \( L_p \)-norm networks and analyzing the robustness and feasibility of \( L_p \)-norms in 3D tasks. Several key findings are highlighted in this work: 

1. \( L_\infty \)-norm convolution is prone to feature loss. 
2. \( L_2 \)-norm convolution essentially performs a linear transformation in traditional CNNs. 
3. \( L_1 \)-norm convolution is an economical and effective method for feature extraction. 

To further enhance the capabilities of \( L_1 \)-norm based networks, the paper proposes a series of customized training and optimization strategies.

In the experimental section, the authors apply their methods to classical 3D networks such as PointNet and PointNet++, achieving competitive performance at a lower cost. In summary, the \( L_1 \)-norm network can achieve similar performance to traditional convolutional networks but with reduced computational cost and lower instruction latency.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
-- This paper is clear and easy to follow, which is well organized.

-- The proof of the universal approximation theorem for Lp-norm Nets and the analysis of the robustness and feasibility of Lp-norms are interesting and beautiful.

-- The comparative results between different $\ell_{p}$-norm-based convolutions presented in this paper are valuable, offering a meaningful technical reference for further method design and subsequent research in 3D vision.

Weaknesses:
There are some weakness/concerns need to be discussed:

--Noise distribution is an interesting issue, but this paper only analyzes the impact of Gaussian noise when considering random noise. How do the authors solve other noise distribution conditions？

--In this paper, how to come up with the idea of the \( L_p \)-norm convolution  is not discussed in detail, although its mechanism is well  illustrated.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposes using the \( \ell_p \)-norm, specifically the \( \ell_1 \)-norm, to replace the classic squared \( \ell_2 \)-norm convolution in 3D tasks. The \( \ell_1 \)-norm kernel function relies on addition, reducing computational cost. Initial gradient implementation revealed insufficient gradient values. To address this, the authors gradually transition the significant gradient from \( \ell_2 \) to \( \ell_1 \) as training progresses and employ momentum updates and learning rate scheduling. By replacing traditional 3D networks with their \( \ell_1 \)-norm networks, the paper demonstrates competitive performance on ModelNet classification and S3DIS semantic segmentation tasks at significantly lower costs.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
i)  The proposed 3D \( \ell_1 \)-norm Net is novel and inspiring for 3D tasks,  which is broadly applicable somewhat.

ii)  The proposed tricks (the optimizer and the customized operator) make the \ell_1-norm network get competitive performance at a lower cost.

iii)  The proof of the appendix is detailed and carefully.

Weaknesses:
Minor typos error:

i) In L-651, it should be ""The *\ell_p*-norm"" rather than """"The \ell_p norm""""

ii) Very few words require consistent capitalization.

iii) Formulas in separate rows may have missing order numbers, it is recommended to unify this style.

iv) The reference format is not uniform enough.

Limitations:
More generalization experiments on other datasets can be considered.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper proposes for using the $\ell_{p}$ norm in 3D convolution as a substitute for the inner product. Out of different choices, the authors pick the $\ell_{1}$-norm because it's faster and uses less energy than the inner product. This is because the $\ell_{1}$-norm relies on addition, which is simpler computationally. To optimize the $\ell_{1}$-norm-based convolution models, they propose a tailored optimization approach leveraging Mixed Gradient Strategy and Dynamic Learning Rate Controller. Finally, the method is evaluated on ModelNet10 and ModelNet40 datasets, showcasing its usage in learning point cloud features for object classification.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1 ) The writing and organization of this paper is great.

2 ) The experiments are sufficient in both main paper and supp, ranging from global Tasks, Semi-dense Prediction, Dense Prediction.

3 ) The idea of replacing traditional convolution with a ℓ𝑝-based convolution is quite foundational and novel for 3D tasks.

4 ) The proposed methods can be easily extended to other 3D backbones, which can be more environmentally friendly.

5 ) The proposed methods demonstrate good performance on most tasks (such as ModelNet classification and S3DIS semantic segmentation) at a significantly lower cost.

Weaknesses:
1 )  The baseline models should be detailed introduced, including the model structure, integration (i.e., replacing) method of the proposed method, just as Table 8 from Ablation Experiments.

2 ) The layout of Table 5 needs to be further optimized, such as splitting it into two independent tables for parallel layout.

Limitations:
See weakness and questions.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of enhancing the representational capacity of traditional convolution methods. To tackle this issue, the authors introduce a novel convolution approach based on $\ell_p$-norm and offer customized optimization strategies to expedite the training process. Extensive theoretical and empirical results have verified that the proposed algorithms demonstrate competitive performance compared to other baselines.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The concept of using the $\ell_p$ norm-based kernel to develop a convolution operator is intriguing. This proposed method has the potential to notably enhance the flexibility of the traditional convolution operator.

Weaknesses:
1. The writing of this paper requires improvement, particularly in providing additional details regarding the proposed method. The current version of this manuscript may lead to reader confusion.

2. The role of the theoretical results presented in this paper is unclear. There appears to be a gap between the theoretical guarantees and the empirical evidence provided.

Limitations:
The authors have adequately addressed the limitations and potential negative societal impact of their work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a new convolution method based on the \( L_p \)-norm. The authors provide a theoretical foundation by proving the universal approximation theorem for \( L_p \)-norm networks and analyzing the robustness and feasibility of \( L_p \)-norms in 3D tasks. Several key findings are highlighted in this work: 

1. \( L_\infty \)-norm convolution is prone to feature loss. 
2. \( L_2 \)-norm convolution essentially performs a linear transformation in traditional CNNs. 
3. \( L_1 \)-norm convolution is an economical and effective method for feature extraction. 

To further enhance the capabilities of \( L_1 \)-norm based networks, the paper proposes a series of customized training and optimization strategies.

In the experimental section, the authors apply their methods to classical 3D networks such as PointNet and PointNet++, achieving competitive performance at a lower cost. In summary, the \( L_1 \)-norm network can achieve similar performance to traditional convolutional networks but with reduced computational cost and lower instruction latency.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
-- This paper is clear and easy to follow, which is well organized.

-- The proof of the universal approximation theorem for Lp-norm Nets and the analysis of the robustness and feasibility of Lp-norms are interesting and beautiful.

-- The comparative results between different $\ell_{p}$-norm-based convolutions presented in this paper are valuable, offering a meaningful technical reference for further method design and subsequent research in 3D vision.

Weaknesses:
There are some weakness/concerns need to be discussed:

--Noise distribution is an interesting issue, but this paper only analyzes the impact of Gaussian noise when considering random noise. How do the authors solve other noise distribution conditions？

--In this paper, how to come up with the idea of the \( L_p \)-norm convolution  is not discussed in detail, although its mechanism is well  illustrated.

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposes to use the tactile sensing signal (height map and normal map) to improve the 3D generation quality, especially the geometric details. The authors use a 3D mesh generation guided by a normal-conditioned ControlNet to ensure the consistency between the  visual textures and the tactile textures. They also develop a multi-part editing pipeline to generate objects with different texture parts. Experiments and ablation studies demonstrate the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The origniality of the paper is good. To my knowledge, this is the first work to use the tactile sensing for 3D generation. The generation quality is satisfying. The generated meshes include good geometric details, which align well with the input tactile signals. The paper is well written and structured. The visualizations clearly demonstrate the quality of the generated meshes.

Weaknesses:
The connection between the tactile sensing and the 3D generation is not strong and critical. The tactile signals are only used to generate the normal maps of certain textures. It may be replaced by simpler alternatives such as texture reterival from a normal map database. It needs further elaboration why this combination is necessary.

Limitations:
Limitaions have been addressed in the paper, which include complex geometry generation and slight seams.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a lightweight 3D texture field that ensures the consistency between visual and tactile textures while preserving photorealism. The experiments demonstrate that quantitative and qualitative results show good generation quality.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. The authors pioneered the use of tactile sensing to enhance geometric details. 
2. The exposition is good. The paper is easy to understand.
3. They created a TouchTexture dataset, comprising 16 everyday objects, contributing a new dataset to the community.

Weaknesses:
1. This paper suggests that existing methods struggle to achieve fine-grained geometric details. However, methods like Neuralangelo and PermutoSDF can recover highly detailed geometric information.

2. The TouchTexture dataset presented in Figure 3 and the supporting materials seemingly do not capture the ""local geometric intricacies.""

3. There are only very limited objects been shown in the main paper and supplementary material. Are those objects cherry-picked? It would be great if more results on in-the-wild object can be provided to show the generalisation ability of the model.

4. Ablation Study is an important part in paper and it would be more convincing with both quantitative and qualitative experiments, and cannot be simply summarized with only several sentences. If there are any figures and tables in the article, please indicate the specific table number or figure number in the analysis and analyze according to the specific visualization results.

- [Neuralangelo: High-Fidelity Neural Surface Reconstruction. CVPR2023]
- [PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces using Permutohedral Lattices. CVPR2023]

Limitations:
I'm not confident about voting for accepting this paper because of the potential similarity to existing methods and the limited novelty. 
There is more engineering effort than novelty. The novelty might be limited.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This submission addresses the long-standing challenge of enhancing geometric details in results produced by text-to-3D and image-to-3D pipelines. The approach introduces a novel method that leverages tactile normal modality to synthesize high-fidelity geometric details. Additionally, it employs attention maps during the diffusion process to segment input images based on text prompts, allowing for the synthesis of multiple textures across various regions. The results demonstrate that this method effectively recovers geometric details and ensures alignment between geometry and color.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The approach is innovative in utilizing tactile normal modality to enhance geometric details.
2. The introduction of a newly collected tactile dataset, TouchTexture, is beneficial to the research community.

Weaknesses:
1. The paper claims compatibility of the proposed method with both text-to-3D and image-to-3D pipelines. However, DreamCraft3D, selected as a text-to-3D pipeline, requires both an image and a text caption as inputs. Although DreamCraft3D can be considered as a 'text-to-image, text & image-to-3D' process, it differs from a purely text-to-3D approach. Thus, the compatibility between the proposed method and purely text-to-3D pipelines remains questionable.

2. The pipeline overview in Figure 4 indicates the need for a reference image and tactile input. The paper does not address how to select an appropriate tactile input for the reference image, nor how to ensure the tactile details are compatible with the object in the image.

3. The paper presents an intriguing text-guided segmentation strategy that leverages attention maps during the diffusion process based on text prompts. However, lacking expertise in diffusion-based segmentation, I am unfamiliar with the efficiency and success rate of this method, but I am positively impressed by it.

4. The generalization and diversity of the proposed method are also of concern. The objects presented in the results are not very complex, and the dataset comprises 16 popular categories. It remains unclear how well the method would perform on more complex objects, such as those in sci-fi or fantasy genres.

Limitations:
The authors acknowledge their limitations in the main paper and address potential social impacts in the supplementary materials. Regarding the first limitation, the implementation of new 3D generative models, such as 'Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image' and 'Era3D: High-Resolution Multiview Diffusion using Efficient Row-wise Attention', is recommended. For the second limitation, utilizing a more powerful computer graphics tool could be beneficial. Concerning the social impact, issues related to deepfakes and the potential for misinformation are noteworthy. The authors assert that humans can currently distinguish their synthesized objects from real ones, a claim with which I concur. Although it is of low priority, it would be preferable for the authors to include a comparison between a generated object and a real one; a single case would suffice.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method for generating 3D assets with detailed geometry through inputs from a tactile sensor. More specifically, given a bump-map as input from a tactile sensor (just a small patch is enough), the method uses it as regularization while maximizing the likelihood using a normals-conditioned stable diffusion model. The albedo is also optimized alongside the normals, though limited results are shown. Additionally, using diffusion based segmentation maps, different parts of the image can be given different textures. While each component of the method itself is not too novel, the sum total is and the results are pretty good.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) This is a novel and unexplored task and the proposed method provides the community with a good baseline to build upon.

2) The results of the method are quite convincing. The ability to edit only certain parts of the image with desired textures is particularly nice.

3) The paper is well written with each component explained pretty well.

Weaknesses:
1) There are no results on the albedo provided, it would be great if the authors could explain why they were omitted. I strongly urge them to include it in the rebuttal. 

2) I may be mistaken, but it seems this method works only for repetitive textures (though the diffusion model is able to change it through optimization). It would be great if the authors could provide the results of an experiment where the object has two very different textures but those textures are only learnt through the diffusion prior. For example, in the cactus pot case, this would correspond to optimizing L_{tactile} only for the pot and let the diffusion prior decide what the normals for the cactus must look like (or vice versa).

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposes to use the tactile sensing signal (height map and normal map) to improve the 3D generation quality, especially the geometric details. The authors use a 3D mesh generation guided by a normal-conditioned ControlNet to ensure the consistency between the  visual textures and the tactile textures. They also develop a multi-part editing pipeline to generate objects with different texture parts. Experiments and ablation studies demonstrate the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The origniality of the paper is good. To my knowledge, this is the first work to use the tactile sensing for 3D generation. The generation quality is satisfying. The generated meshes include good geometric details, which align well with the input tactile signals. The paper is well written and structured. The visualizations clearly demonstrate the quality of the generated meshes.

Weaknesses:
The connection between the tactile sensing and the 3D generation is not strong and critical. The tactile signals are only used to generate the normal maps of certain textures. It may be replaced by simpler alternatives such as texture reterival from a normal map database. It needs further elaboration why this combination is necessary.

Limitations:
Limitaions have been addressed in the paper, which include complex geometry generation and slight seams.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a lightweight 3D texture field that ensures the consistency between visual and tactile textures while preserving photorealism. The experiments demonstrate that quantitative and qualitative results show good generation quality.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. The authors pioneered the use of tactile sensing to enhance geometric details. 
2. The exposition is good. The paper is easy to understand.
3. They created a TouchTexture dataset, comprising 16 everyday objects, contributing a new dataset to the community.

Weaknesses:
1. This paper suggests that existing methods struggle to achieve fine-grained geometric details. However, methods like Neuralangelo and PermutoSDF can recover highly detailed geometric information.

2. The TouchTexture dataset presented in Figure 3 and the supporting materials seemingly do not capture the ""local geometric intricacies.""

3. There are only very limited objects been shown in the main paper and supplementary material. Are those objects cherry-picked? It would be great if more results on in-the-wild object can be provided to show the generalisation ability of the model.

4. Ablation Study is an important part in paper and it would be more convincing with both quantitative and qualitative experiments, and cannot be simply summarized with only several sentences. If there are any figures and tables in the article, please indicate the specific table number or figure number in the analysis and analyze according to the specific visualization results.

- [Neuralangelo: High-Fidelity Neural Surface Reconstruction. CVPR2023]
- [PermutoSDF: Fast Multi-View Reconstruction with Implicit Surfaces using Permutohedral Lattices. CVPR2023]

Limitations:
I'm not confident about voting for accepting this paper because of the potential similarity to existing methods and the limited novelty. 
There is more engineering effort than novelty. The novelty might be limited.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This submission addresses the long-standing challenge of enhancing geometric details in results produced by text-to-3D and image-to-3D pipelines. The approach introduces a novel method that leverages tactile normal modality to synthesize high-fidelity geometric details. Additionally, it employs attention maps during the diffusion process to segment input images based on text prompts, allowing for the synthesis of multiple textures across various regions. The results demonstrate that this method effectively recovers geometric details and ensures alignment between geometry and color.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The approach is innovative in utilizing tactile normal modality to enhance geometric details.
2. The introduction of a newly collected tactile dataset, TouchTexture, is beneficial to the research community.

Weaknesses:
1. The paper claims compatibility of the proposed method with both text-to-3D and image-to-3D pipelines. However, DreamCraft3D, selected as a text-to-3D pipeline, requires both an image and a text caption as inputs. Although DreamCraft3D can be considered as a 'text-to-image, text & image-to-3D' process, it differs from a purely text-to-3D approach. Thus, the compatibility between the proposed method and purely text-to-3D pipelines remains questionable.

2. The pipeline overview in Figure 4 indicates the need for a reference image and tactile input. The paper does not address how to select an appropriate tactile input for the reference image, nor how to ensure the tactile details are compatible with the object in the image.

3. The paper presents an intriguing text-guided segmentation strategy that leverages attention maps during the diffusion process based on text prompts. However, lacking expertise in diffusion-based segmentation, I am unfamiliar with the efficiency and success rate of this method, but I am positively impressed by it.

4. The generalization and diversity of the proposed method are also of concern. The objects presented in the results are not very complex, and the dataset comprises 16 popular categories. It remains unclear how well the method would perform on more complex objects, such as those in sci-fi or fantasy genres.

Limitations:
The authors acknowledge their limitations in the main paper and address potential social impacts in the supplementary materials. Regarding the first limitation, the implementation of new 3D generative models, such as 'Unique3D: High-Quality and Efficient 3D Mesh Generation from a Single Image' and 'Era3D: High-Resolution Multiview Diffusion using Efficient Row-wise Attention', is recommended. For the second limitation, utilizing a more powerful computer graphics tool could be beneficial. Concerning the social impact, issues related to deepfakes and the potential for misinformation are noteworthy. The authors assert that humans can currently distinguish their synthesized objects from real ones, a claim with which I concur. Although it is of low priority, it would be preferable for the authors to include a comparison between a generated object and a real one; a single case would suffice.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method for generating 3D assets with detailed geometry through inputs from a tactile sensor. More specifically, given a bump-map as input from a tactile sensor (just a small patch is enough), the method uses it as regularization while maximizing the likelihood using a normals-conditioned stable diffusion model. The albedo is also optimized alongside the normals, though limited results are shown. Additionally, using diffusion based segmentation maps, different parts of the image can be given different textures. While each component of the method itself is not too novel, the sum total is and the results are pretty good.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) This is a novel and unexplored task and the proposed method provides the community with a good baseline to build upon.

2) The results of the method are quite convincing. The ability to edit only certain parts of the image with desired textures is particularly nice.

3) The paper is well written with each component explained pretty well.

Weaknesses:
1) There are no results on the albedo provided, it would be great if the authors could explain why they were omitted. I strongly urge them to include it in the rebuttal. 

2) I may be mistaken, but it seems this method works only for repetitive textures (though the diffusion model is able to change it through optimization). It would be great if the authors could provide the results of an experiment where the object has two very different textures but those textures are only learnt through the diffusion prior. For example, in the cactus pot case, this would correspond to optimizing L_{tactile} only for the pot and let the diffusion prior decide what the normals for the cactus must look like (or vice versa).

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
In this papaer, the authors proposed a method to finetune vision-language model (CLIP) via prompt learning. Specifically, the author optimize a global textual prompt which will be optimized via FedAvg, while each client additionally optimize a local textual prompt. The proposed method is a combination of the existing methods CoOp and PromptFL and outperforms both methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written.
2. The theoretical analysis of the proposed method is thorough and clear.

Weaknesses:
1. The proposed method is basically a combination of both CoOp and PromptFL, or to be more specific, optimizing both client-specific and client-agnostic textual prompt. The novelty of the proposed needs to be further highlighted in the main paper.
2. The combination strategy seems to be a bit straight forward, i.e., using a single scalar value $\theta$, the author should discuss other possibilities.
3. The authors should compare with other prompt-based tuning methods for FL, e.g., [1][2]. Also, adding more ablation studies could be beneficial.

[1] Guo T, Guo S, Wang J. Pfedprompt: Learning personalized prompt for vision-language models in federated learning[C]//Proceedings of the ACM Web Conference 2023. 2023: 1364-1374.

[2] Qiu C, Li X, Mummadi C K, et al. Text-driven Prompt Generation for Vision-Language Models in Federated Learning[J]. arXiv preprint arXiv:2310.06123, 2023.

Limitations:
The authors do not discuss the limitation of the method.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper discusses the integration of pretrained vision-language foundation models, such as CLIP, into federated learning. The idea is to use prompt-based federated learning to minimize the communication and computational costs. The authors go into the theoretical analysis to understand the performance of this approach

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The authors provide a theoretical understanding on prompt Federated Learning for models like CLIP.
- The results demonstrate that the combination of global and local prompts can support increased accuracies. 
- The overall investigation of this space is interesting and useful for the community

Weaknesses:
- While the overall approach is quite interesting, the authors restrict their investigation in a much simpler classification task. It is unclear why a larger foundation model is required for simpler classification tasks, where traditional FL techniques on vanilla models (e.g., reset, mobileNet etc) might work better towards learning new unseen distributions that a foundation model might not have seen (e.g,. x-ray hospital images etc). 
- Following the previous comment, it is unclear how this method would work on unseen distributions that most FL applications are mostly useful for. Most FL models are quite good at generalising to data that can be found on the public internet, but it is unknown how well they would work on out-of-distribution private data. 
- Given the fact that there are a few related prompt-based FL methods (e.g., promtFL, FedPrompt, etc) limit a bit the novelty of this work to mostly providing a solid theoretical analysis. 
- There might be some privacy implications from this work, as sensitive data are shared. 
- The evaluation was done with mostly 10 (up to 50) users, whereas in typical FL scenarios we want to learn from million of users. It is unclear how these results would generalise. Overall, the evaluation is a bit simplistic, showing only accuracy results on a few tasks.

Limitations:
See above

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies theoretical properties of prompt based federated learning methods for visual language models. Following the proposed theoretical results, a new algorithm named Global-local Prompt Portfolio for Federated Learning (PromptFolio) was proposed. The proposed algorithm was examined in image classification tasks using a CLIP model.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Analyzing prompt-based federated learning methods with feature representations and exploiting their relation to portfolio optimization is a nice idea. The proposed analyses also led to a new algorithm for prompt based federated learning.

Weaknesses:
Experimental analyses are limited and should be improved as well.

Limitations:
Limitations were partially addressed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The proposed methodology offers a novel take on analyzing federated learning using prompt learning (for foundation models) via feature learning theory. At its core, the idea is to identify and monitor task-relevant and task-irrelevant features, and leveraging inspiration from portfolio optimization which says to combine independent assets to maintain income while decreasing risk of investment, construct a 2 part prompt. One part which is global and another which is local, to help with personalization. By mixing these prompts optimally, the proposed PromptFolio method highlights an improvement in overall performance of the FL system.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
- A key strength of the proposed method is its novel take on explaining prompt-based federated learning, using feature learning theory.
- Clarity in idea and its link to existing work.

Weaknesses:
Abstract/Conceptual Questions:
- Could the authors comment about the few-shot learning aspect of foundation models (+prompt learning) and how the optimal mixing coefficient would be affected in that scenario?
- An adjacent sub-field that contains foundation models and federated learning is the study of their impact in learning new classes using prior knowledge (base vs. new classes performance). Could you comment on how this sub-field could relate to the proposed method?

Clarifications:
- Could the authors clarify the exact dimensionality of the prompt used in each of the baselines as well as PromptFolio?
- Could the authors mention if there are previous works that have proposed a 2-level prompt learning idea? (since Section 2.2 L 107 mentions that an exploration of such a idea and the cooperation between prompts is sparse)
- In Section 4. L. 224, the authors mention that ""task-relevant coefficients can be directly added"". Could you clarify this statement?
- Could the authors highlight the data setting used to generate Table 1?
- Could the authors explicitly state the train and test conditions (settings) used within the experiments?

Suggestions:
- Since the proposal of PromptFL, there are have been further advances to the idea of prompt learning within the federated setting. Could the authors comment and draw comparisons against the more recent methods?

Limitations:
yes, the authors have addressed limitations within their choices of the experiment setting. However, I would recommend drawing more broad comparisons as to potential missing pieces in comparison to more recent work as well as the finer points from the feature learning theory that don't fully match up to the settings in the FL system.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper tackles the interesting problem of analyzing federated learning (FL) from vision-language foundation models like CLIP. The authors develop a theoretical framework based on feature learning theory to understand how prompt-based FL works. They introduce a new algorithm called PromptFolio that mixes global and local prompts, drawing an analogy to portfolio optimization in finance. The theoretical analysis shows how this approach can balance generalization and personalization in FL. They back up their claims with experiments on several datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The feature learning approach they develop seems pretty solid. 
- The connection between prompt mixing and financial portfolio theory is novel.
- The ablation studies on data heterogeneity and client numbers are particularly informative.

Weaknesses:
- This reviewer is confused by the feature learning theory (I am not familiar with it), which is introduced in a relatively short section. Is it (looks like) a common sense in deep learning or a kind of new theory proposed recently? It would be helpful to have a more thorough introduction to the theory.
- Using a single activation function for the text encoder is a pretty big simplification from how CLIP and other large language models actually work.
- All the experiments focus on classification tasks. Any thoughts on how PromptFolio might perform on other vision-language tasks like image captioning or visual QA in a federated setting?
- Why is the proposed method on OxfordPets dataset not as good as coop? While it seems to be dominating on other datasets.

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
In this papaer, the authors proposed a method to finetune vision-language model (CLIP) via prompt learning. Specifically, the author optimize a global textual prompt which will be optimized via FedAvg, while each client additionally optimize a local textual prompt. The proposed method is a combination of the existing methods CoOp and PromptFL and outperforms both methods.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written.
2. The theoretical analysis of the proposed method is thorough and clear.

Weaknesses:
1. The proposed method is basically a combination of both CoOp and PromptFL, or to be more specific, optimizing both client-specific and client-agnostic textual prompt. The novelty of the proposed needs to be further highlighted in the main paper.
2. The combination strategy seems to be a bit straight forward, i.e., using a single scalar value $\theta$, the author should discuss other possibilities.
3. The authors should compare with other prompt-based tuning methods for FL, e.g., [1][2]. Also, adding more ablation studies could be beneficial.

[1] Guo T, Guo S, Wang J. Pfedprompt: Learning personalized prompt for vision-language models in federated learning[C]//Proceedings of the ACM Web Conference 2023. 2023: 1364-1374.

[2] Qiu C, Li X, Mummadi C K, et al. Text-driven Prompt Generation for Vision-Language Models in Federated Learning[J]. arXiv preprint arXiv:2310.06123, 2023.

Limitations:
The authors do not discuss the limitation of the method.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper discusses the integration of pretrained vision-language foundation models, such as CLIP, into federated learning. The idea is to use prompt-based federated learning to minimize the communication and computational costs. The authors go into the theoretical analysis to understand the performance of this approach

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- The authors provide a theoretical understanding on prompt Federated Learning for models like CLIP.
- The results demonstrate that the combination of global and local prompts can support increased accuracies. 
- The overall investigation of this space is interesting and useful for the community

Weaknesses:
- While the overall approach is quite interesting, the authors restrict their investigation in a much simpler classification task. It is unclear why a larger foundation model is required for simpler classification tasks, where traditional FL techniques on vanilla models (e.g., reset, mobileNet etc) might work better towards learning new unseen distributions that a foundation model might not have seen (e.g,. x-ray hospital images etc). 
- Following the previous comment, it is unclear how this method would work on unseen distributions that most FL applications are mostly useful for. Most FL models are quite good at generalising to data that can be found on the public internet, but it is unknown how well they would work on out-of-distribution private data. 
- Given the fact that there are a few related prompt-based FL methods (e.g., promtFL, FedPrompt, etc) limit a bit the novelty of this work to mostly providing a solid theoretical analysis. 
- There might be some privacy implications from this work, as sensitive data are shared. 
- The evaluation was done with mostly 10 (up to 50) users, whereas in typical FL scenarios we want to learn from million of users. It is unclear how these results would generalise. Overall, the evaluation is a bit simplistic, showing only accuracy results on a few tasks.

Limitations:
See above

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies theoretical properties of prompt based federated learning methods for visual language models. Following the proposed theoretical results, a new algorithm named Global-local Prompt Portfolio for Federated Learning (PromptFolio) was proposed. The proposed algorithm was examined in image classification tasks using a CLIP model.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Analyzing prompt-based federated learning methods with feature representations and exploiting their relation to portfolio optimization is a nice idea. The proposed analyses also led to a new algorithm for prompt based federated learning.

Weaknesses:
Experimental analyses are limited and should be improved as well.

Limitations:
Limitations were partially addressed.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The proposed methodology offers a novel take on analyzing federated learning using prompt learning (for foundation models) via feature learning theory. At its core, the idea is to identify and monitor task-relevant and task-irrelevant features, and leveraging inspiration from portfolio optimization which says to combine independent assets to maintain income while decreasing risk of investment, construct a 2 part prompt. One part which is global and another which is local, to help with personalization. By mixing these prompts optimally, the proposed PromptFolio method highlights an improvement in overall performance of the FL system.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
- A key strength of the proposed method is its novel take on explaining prompt-based federated learning, using feature learning theory.
- Clarity in idea and its link to existing work.

Weaknesses:
Abstract/Conceptual Questions:
- Could the authors comment about the few-shot learning aspect of foundation models (+prompt learning) and how the optimal mixing coefficient would be affected in that scenario?
- An adjacent sub-field that contains foundation models and federated learning is the study of their impact in learning new classes using prior knowledge (base vs. new classes performance). Could you comment on how this sub-field could relate to the proposed method?

Clarifications:
- Could the authors clarify the exact dimensionality of the prompt used in each of the baselines as well as PromptFolio?
- Could the authors mention if there are previous works that have proposed a 2-level prompt learning idea? (since Section 2.2 L 107 mentions that an exploration of such a idea and the cooperation between prompts is sparse)
- In Section 4. L. 224, the authors mention that ""task-relevant coefficients can be directly added"". Could you clarify this statement?
- Could the authors highlight the data setting used to generate Table 1?
- Could the authors explicitly state the train and test conditions (settings) used within the experiments?

Suggestions:
- Since the proposal of PromptFL, there are have been further advances to the idea of prompt learning within the federated setting. Could the authors comment and draw comparisons against the more recent methods?

Limitations:
yes, the authors have addressed limitations within their choices of the experiment setting. However, I would recommend drawing more broad comparisons as to potential missing pieces in comparison to more recent work as well as the finer points from the feature learning theory that don't fully match up to the settings in the FL system.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper tackles the interesting problem of analyzing federated learning (FL) from vision-language foundation models like CLIP. The authors develop a theoretical framework based on feature learning theory to understand how prompt-based FL works. They introduce a new algorithm called PromptFolio that mixes global and local prompts, drawing an analogy to portfolio optimization in finance. The theoretical analysis shows how this approach can balance generalization and personalization in FL. They back up their claims with experiments on several datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The feature learning approach they develop seems pretty solid. 
- The connection between prompt mixing and financial portfolio theory is novel.
- The ablation studies on data heterogeneity and client numbers are particularly informative.

Weaknesses:
- This reviewer is confused by the feature learning theory (I am not familiar with it), which is introduced in a relatively short section. Is it (looks like) a common sense in deep learning or a kind of new theory proposed recently? It would be helpful to have a more thorough introduction to the theory.
- Using a single activation function for the text encoder is a pretty big simplification from how CLIP and other large language models actually work.
- All the experiments focus on classification tasks. Any thoughts on how PromptFolio might perform on other vision-language tasks like image captioning or visual QA in a federated setting?
- Why is the proposed method on OxfordPets dataset not as good as coop? While it seems to be dominating on other datasets.

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper considers fairness issue of image restoration and proposes Group Perceptual Index to measure the distance between restoration distribution and gt distribution. Experimental and theoretical results demonstrate that the superiority the proposed perceptual fairness over previous method.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1.	The fairness is important for the image restoration community and the topic is interesting to study. The proposed Group Perceptual Index can be reasonable to measure the fairness properly.
2.	The paper includes solid theoretical and experimental results which provides evidence for fairness measurement.
3.	The paper is well-written and easy to follow.

Weaknesses:
1. The main results are mainly based face restoration. Can this method be useful for general scenario of image restoration?
2. The paper proposes a measure to detect the fairness issue. But can you suggest some potential solutions to address this problem?

Limitations:
Yes, the authors discuss limitations carefully.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work introduces a new method for assessing fairness in image restoration, called the Group Perceptual Index (GPI). This measure quantifies the statistical difference between the distribution of a group's original images and the distribution of their restored versions. The authors illustrate the effectiveness of GPI by applying it to advanced face image super-resolution algorithms.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
- the problem tackled in this paper is of practical importance
- paper is written well
- the proposed method is theoretically sound and is shown to work in meaningful ways when used on the problem space of image super-resolution

Weaknesses:
- Usefulness of the method is validated only on the super-resolution solution. Given the fact that the proposed method has potential to impact various image restoration algorithms, it would have been interesting to see how well it does on other image restoration application such as image denoising, deblurring etc.
- It is also not clear what kind of changes to the existing super-resolution methods might result in better fairness handling. Some insights into why certain methods are not good at fairness handling as compared to others might have helped the future works.

Limitations:
Authors has addressed the limitations to a satisfactory extent.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This study presents a novel method to evaluate fairness in image restoration using the Group Perceptual Index (GPI). GPI quantifies the statistical disparity between a group's original images and their restored versions. Fairness is assessed by comparing GPIs across multiple groups, striving for perfect Perceptual Fairness (PF) where all GPI values are identical. The research provides theoretical insights into this innovative fairness concept, drawing comparisons to existing frameworks, and showcases its practical implementation through advanced face image super-resolution algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-structured and includes sufficient theoretical explanations.
2. The concept of GPI is logically sound.
3. The paper demonstrates that the proposed method outperforms other baseline methods.
4. The paper thoroughly discusses both the advantages and limitations of the proposed method. The advantages highlight the method's effectiveness and potential benefits, while the limitations are clearly outlined, providing a balanced view of its capabilities and areas for improvement.

Weaknesses:
1. The authors introduce a novel method to evaluate the fairness of image restoration. However, it is important to note that this method has been validated exclusively on image super-resolution tasks. Further validation on other types of image restoration tasks would be beneficial to demonstrate its broader applicability and robustness.
2. How can sensitive attributes be detected and acquired? The impact of sensitive attributes deserves an in-depth discussion.

Limitations:
Limitations have been thoroughly discussed and adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper reveals that the conventional definition of fairness for image restoration is restrictive and often causes controversy. To address this issue, the authors introduce a new approach to measure fairness in image restoration tasks by proposing the Group Perceptual Index (GPI). Specifically, they propose assessing the fairness of an algorithm by comparing the GPI of different groups, where perfect Perceptual Fairness (PF) is achieved if the GPIs of all groups are identical. They theoretically study this notion of fairness and demonstrate its utility on state-of-the-art face image super-resolution algorithms.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The paper reveals the existing fairness measures such as Representation Demographic Parity (RDP) and highlights their limitations. It shows that these measures can be overly simplistic and may not detect subtle biases that affect different groups.
2. The paper proposes the Group Perceptual Index (GPI) as a measure of fairness in image restoration, which is a novel and significant contribution.
3. It provides a theoretical analysis of the properties of GPI and its relationship to other fairness measures.
4. The authors use a variety of datasets and experimental setups to demonstrate the effectiveness of GPI, which are convincing.

Weaknesses:
1. Group Perceptual Index (GPI) also increases the complexity of the evaluation process of image restoration algorithms, compared with the traditional fairness method, because it involves comparing the distributions of different groups.
2. The experiments use synthetic datasets generated from high-quality, aligned face image datasets like CelebA-HQ. 
3. The paper only evaluate the proposed method on the face dataset and does not provide the results on other kinds of image data.

Limitations:
The paper rethinks the fairness in image restoration and proposes a novel method, called Group Perceptual Index (GPI), to measure the fairness for image restoration models. The proposed method can effectively detect subtle and malicious biases enhances the robustness and security of image restoration systems

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper considers fairness issue of image restoration and proposes Group Perceptual Index to measure the distance between restoration distribution and gt distribution. Experimental and theoretical results demonstrate that the superiority the proposed perceptual fairness over previous method.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1.	The fairness is important for the image restoration community and the topic is interesting to study. The proposed Group Perceptual Index can be reasonable to measure the fairness properly.
2.	The paper includes solid theoretical and experimental results which provides evidence for fairness measurement.
3.	The paper is well-written and easy to follow.

Weaknesses:
1. The main results are mainly based face restoration. Can this method be useful for general scenario of image restoration?
2. The paper proposes a measure to detect the fairness issue. But can you suggest some potential solutions to address this problem?

Limitations:
Yes, the authors discuss limitations carefully.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work introduces a new method for assessing fairness in image restoration, called the Group Perceptual Index (GPI). This measure quantifies the statistical difference between the distribution of a group's original images and the distribution of their restored versions. The authors illustrate the effectiveness of GPI by applying it to advanced face image super-resolution algorithms.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
- the problem tackled in this paper is of practical importance
- paper is written well
- the proposed method is theoretically sound and is shown to work in meaningful ways when used on the problem space of image super-resolution

Weaknesses:
- Usefulness of the method is validated only on the super-resolution solution. Given the fact that the proposed method has potential to impact various image restoration algorithms, it would have been interesting to see how well it does on other image restoration application such as image denoising, deblurring etc.
- It is also not clear what kind of changes to the existing super-resolution methods might result in better fairness handling. Some insights into why certain methods are not good at fairness handling as compared to others might have helped the future works.

Limitations:
Authors has addressed the limitations to a satisfactory extent.

Rating:
6: marginally above the acceptance threshold

Confidence:
1: Your assessment is an educated guess. The submission is not in your area or the submission was difficult to understand. Math/other details were not carefully checked.

REVIEW 
Summary:
This study presents a novel method to evaluate fairness in image restoration using the Group Perceptual Index (GPI). GPI quantifies the statistical disparity between a group's original images and their restored versions. Fairness is assessed by comparing GPIs across multiple groups, striving for perfect Perceptual Fairness (PF) where all GPI values are identical. The research provides theoretical insights into this innovative fairness concept, drawing comparisons to existing frameworks, and showcases its practical implementation through advanced face image super-resolution algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-structured and includes sufficient theoretical explanations.
2. The concept of GPI is logically sound.
3. The paper demonstrates that the proposed method outperforms other baseline methods.
4. The paper thoroughly discusses both the advantages and limitations of the proposed method. The advantages highlight the method's effectiveness and potential benefits, while the limitations are clearly outlined, providing a balanced view of its capabilities and areas for improvement.

Weaknesses:
1. The authors introduce a novel method to evaluate the fairness of image restoration. However, it is important to note that this method has been validated exclusively on image super-resolution tasks. Further validation on other types of image restoration tasks would be beneficial to demonstrate its broader applicability and robustness.
2. How can sensitive attributes be detected and acquired? The impact of sensitive attributes deserves an in-depth discussion.

Limitations:
Limitations have been thoroughly discussed and adequately addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper reveals that the conventional definition of fairness for image restoration is restrictive and often causes controversy. To address this issue, the authors introduce a new approach to measure fairness in image restoration tasks by proposing the Group Perceptual Index (GPI). Specifically, they propose assessing the fairness of an algorithm by comparing the GPI of different groups, where perfect Perceptual Fairness (PF) is achieved if the GPIs of all groups are identical. They theoretically study this notion of fairness and demonstrate its utility on state-of-the-art face image super-resolution algorithms.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. The paper reveals the existing fairness measures such as Representation Demographic Parity (RDP) and highlights their limitations. It shows that these measures can be overly simplistic and may not detect subtle biases that affect different groups.
2. The paper proposes the Group Perceptual Index (GPI) as a measure of fairness in image restoration, which is a novel and significant contribution.
3. It provides a theoretical analysis of the properties of GPI and its relationship to other fairness measures.
4. The authors use a variety of datasets and experimental setups to demonstrate the effectiveness of GPI, which are convincing.

Weaknesses:
1. Group Perceptual Index (GPI) also increases the complexity of the evaluation process of image restoration algorithms, compared with the traditional fairness method, because it involves comparing the distributions of different groups.
2. The experiments use synthetic datasets generated from high-quality, aligned face image datasets like CelebA-HQ. 
3. The paper only evaluate the proposed method on the face dataset and does not provide the results on other kinds of image data.

Limitations:
The paper rethinks the fairness in image restoration and proposes a novel method, called Group Perceptual Index (GPI), to measure the fairness for image restoration models. The proposed method can effectively detect subtle and malicious biases enhances the robustness and security of image restoration systems

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces SelectIT, a novel instruction tuning (IT) method that leverages the intrinsic uncertainty in large language models (LLMs) to select high-quality IT data without requiring additional resources. The authors present a curated IT dataset, Selective Alpaca, derived from the Alpaca-GPT4 dataset using SelectIT. The empirical results demonstrate significant improvements in model performance across various tasks. The paper highlights the robustness and efficiency of SelectIT in enhancing the capabilities of LLMs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
-  The approach of using the intrinsic uncertainty of LLMs for IT data selection is innovative and eliminates the need for external resources, making it cost-effective and widely applicable. 
-  The submission is technically sound, with well-supported claims through theoretical analysis and comprehensive experimental results.

Weaknesses:
- Figure 2 is not clear to me. I suggest giving a brief description of each selection stage in the caption for easy understanding.
- It seems like the foundational LLM's capacity becomes the key for this data selection. In this case, the selection method may not be as effective as other external resource methods.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, authors leverage LLMs to score the training data in terms of the token and sentence and model. They train various models on their select models to show the effectiveness on various benchmarks. Their main contribution is to apply foundations to judge the quality of training data from different granularity without any external resources.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Their idea is easy to understand and the writing is pretty clear to me.
2. They conduct a lot of experiments on several benchmarks to show the effectiveness.

Weaknesses:
1. From the table 3, it seems sentence-R works the best so what if we trained model on the data just from the sentence-R?  BTW, what is the ID 6, 7, 8 means in that Table?
2. My main question is that do we really need all three types selection models, can one or two of them already achieve great performance?
3. What is the overhead of this data selection model? Like how long does it take to finish the process on 10K dataset.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents SelectIT, a new way to improve how large language models (LLMs) follow instructions using their own uncertainty to pick better training data. This method is cost-effective as it doesn't require extra tools or data. The authors developed a new dataset called Selective Alpaca using SelectIT, which significantly improved LLMs' performance. The study suggests that using longer and more detailed data could be more beneficial for training models.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
SelectIT is a bew and unique method that improves how large language models (LLMs) learn by using the model's own uncertainty to choose better training data. This approach is special because it doesn't need extra tools or data, making it less expensive and easier to use.

The paper includes detailed studies that show the method works well and supports its findings with strong evidence. SelectIT significantly improves how models perform compared to older methods, showing that it's a big improvement in teaching models.

The researchers made their methods and data public, allowing others to use and test them. This openness helps the scientific community to work together and trus the results more.

The paper is also written in a way that's easy to understand, which helps more people learn about SelectIT. It discusses how using longer and more detailed data can make models learn better. Additionally, the paper compares the costs of using SelectIT with using other common tools like the GPT-4 and ChatGPT APIs, showing that SelectIT can save money. This shows that SelectIT is not only effective but also cost-efficient for training models.

Weaknesses:
The paper states initially that it does not use extra resources for its new method. However, it mentions later that they used different sizes of LLaMA 2 models to help choose the best instruction data. This contradicts their first claim about not needing extra resources.

Limitations:
the paper lacks a separate section on limitation of the approach.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a data selection approach to instruction tuning (IT) for large language models (LLMs) by leveraging the intrinsic uncertainties within the models themselves. This method, termed SelectIT, aims to enhance IT data selection without the need for additional external resources, making it more cost-effective and accessible.

SelectIT utilizes three levels of uncertainty within LLMs—token-level, sentence-level, and model-level—to effectively rate and select high-quality IT data. The paper demonstrates the effectiveness of SelectIT through empirical results, showing improvements in model abilities on the open-instruct benchmark and other domain-specific tasks. Additionally, SelectIT is highlighted as a faster and more resource-efficient method compared to traditional approaches, making it a more practical solution for widespread adoption.

---------------------------------------------------------------------------------------------------------------------------------------------------------------
Thank you for your reply and I have updated my score accordingly.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
**Originality**: The paper presents a data selection approach to instruction tuning by leveraging the intrinsic uncertainties within LLMs, without relying on additional fine-tuned models or API calls of ChatGPT or Claude.

**Quality**: The methodology has a clear explanation of the multi-granularity self-reflection technique. The empirical results are robust, demonstrating the effectiveness of SelectIT across various benchmarks and tasks.

**Clarity**: The paper is clearly written, with well-organized sections and comprehensive explanations of the proposed method and its implementation.

**Significance**: SelectIT addresses a critical limitation in the field of instruction tuning by removing the dependence on additional models or data, making the approach more cost-effective and scalable. The curated Selective Alpaca dataset offers performance improvements for LLMs, highlighting the practical impact of the research.

Weaknesses:
1. **Reliance on Tuning K**: The method depends on tuning the parameter \( K \), which can significantly affect model performance. This dependency introduces additional complexity and may require extensive experimentation to optimize.

2. **Use of Multiple Base Models**: Although the paper claims not to use external resources, it actually relies on multiple different base models to obtain the model-level score. This reliance contradicts the claim of being purely self-reflective and can be confusing, as the title suggests the method is entirely self-contained.

3. **Sentence-Level Quality Evaluation**: The sentence-level quality evaluation requires multiple rating prompts. It is unclear whether other methods, such as AlpaGasus, would achieve similar performance if they also used multiple rating prompts. This comparison is missing and could be critical for understanding the true advantage of the proposed method.

4. **Single Dataset Evaluation**: All evaluations are conducted on a single training dataset, making it questionable whether this method can be generalized to other datasets, such as WizardLM, or multi-turn data, like the ShareGPT dataset. Testing on a more diverse set of datasets would strengthen the paper's claims of robustness and general applicability.

Limitations:
See more details in Weaknesses section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces SelectIT, a novel instruction tuning (IT) method that leverages the intrinsic uncertainty in large language models (LLMs) to select high-quality IT data without requiring additional resources. The authors present a curated IT dataset, Selective Alpaca, derived from the Alpaca-GPT4 dataset using SelectIT. The empirical results demonstrate significant improvements in model performance across various tasks. The paper highlights the robustness and efficiency of SelectIT in enhancing the capabilities of LLMs.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
-  The approach of using the intrinsic uncertainty of LLMs for IT data selection is innovative and eliminates the need for external resources, making it cost-effective and widely applicable. 
-  The submission is technically sound, with well-supported claims through theoretical analysis and comprehensive experimental results.

Weaknesses:
- Figure 2 is not clear to me. I suggest giving a brief description of each selection stage in the caption for easy understanding.
- It seems like the foundational LLM's capacity becomes the key for this data selection. In this case, the selection method may not be as effective as other external resource methods.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
In this paper, authors leverage LLMs to score the training data in terms of the token and sentence and model. They train various models on their select models to show the effectiveness on various benchmarks. Their main contribution is to apply foundations to judge the quality of training data from different granularity without any external resources.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Their idea is easy to understand and the writing is pretty clear to me.
2. They conduct a lot of experiments on several benchmarks to show the effectiveness.

Weaknesses:
1. From the table 3, it seems sentence-R works the best so what if we trained model on the data just from the sentence-R?  BTW, what is the ID 6, 7, 8 means in that Table?
2. My main question is that do we really need all three types selection models, can one or two of them already achieve great performance?
3. What is the overhead of this data selection model? Like how long does it take to finish the process on 10K dataset.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents SelectIT, a new way to improve how large language models (LLMs) follow instructions using their own uncertainty to pick better training data. This method is cost-effective as it doesn't require extra tools or data. The authors developed a new dataset called Selective Alpaca using SelectIT, which significantly improved LLMs' performance. The study suggests that using longer and more detailed data could be more beneficial for training models.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
SelectIT is a bew and unique method that improves how large language models (LLMs) learn by using the model's own uncertainty to choose better training data. This approach is special because it doesn't need extra tools or data, making it less expensive and easier to use.

The paper includes detailed studies that show the method works well and supports its findings with strong evidence. SelectIT significantly improves how models perform compared to older methods, showing that it's a big improvement in teaching models.

The researchers made their methods and data public, allowing others to use and test them. This openness helps the scientific community to work together and trus the results more.

The paper is also written in a way that's easy to understand, which helps more people learn about SelectIT. It discusses how using longer and more detailed data can make models learn better. Additionally, the paper compares the costs of using SelectIT with using other common tools like the GPT-4 and ChatGPT APIs, showing that SelectIT can save money. This shows that SelectIT is not only effective but also cost-efficient for training models.

Weaknesses:
The paper states initially that it does not use extra resources for its new method. However, it mentions later that they used different sizes of LLaMA 2 models to help choose the best instruction data. This contradicts their first claim about not needing extra resources.

Limitations:
the paper lacks a separate section on limitation of the approach.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a data selection approach to instruction tuning (IT) for large language models (LLMs) by leveraging the intrinsic uncertainties within the models themselves. This method, termed SelectIT, aims to enhance IT data selection without the need for additional external resources, making it more cost-effective and accessible.

SelectIT utilizes three levels of uncertainty within LLMs—token-level, sentence-level, and model-level—to effectively rate and select high-quality IT data. The paper demonstrates the effectiveness of SelectIT through empirical results, showing improvements in model abilities on the open-instruct benchmark and other domain-specific tasks. Additionally, SelectIT is highlighted as a faster and more resource-efficient method compared to traditional approaches, making it a more practical solution for widespread adoption.

---------------------------------------------------------------------------------------------------------------------------------------------------------------
Thank you for your reply and I have updated my score accordingly.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
**Originality**: The paper presents a data selection approach to instruction tuning by leveraging the intrinsic uncertainties within LLMs, without relying on additional fine-tuned models or API calls of ChatGPT or Claude.

**Quality**: The methodology has a clear explanation of the multi-granularity self-reflection technique. The empirical results are robust, demonstrating the effectiveness of SelectIT across various benchmarks and tasks.

**Clarity**: The paper is clearly written, with well-organized sections and comprehensive explanations of the proposed method and its implementation.

**Significance**: SelectIT addresses a critical limitation in the field of instruction tuning by removing the dependence on additional models or data, making the approach more cost-effective and scalable. The curated Selective Alpaca dataset offers performance improvements for LLMs, highlighting the practical impact of the research.

Weaknesses:
1. **Reliance on Tuning K**: The method depends on tuning the parameter \( K \), which can significantly affect model performance. This dependency introduces additional complexity and may require extensive experimentation to optimize.

2. **Use of Multiple Base Models**: Although the paper claims not to use external resources, it actually relies on multiple different base models to obtain the model-level score. This reliance contradicts the claim of being purely self-reflective and can be confusing, as the title suggests the method is entirely self-contained.

3. **Sentence-Level Quality Evaluation**: The sentence-level quality evaluation requires multiple rating prompts. It is unclear whether other methods, such as AlpaGasus, would achieve similar performance if they also used multiple rating prompts. This comparison is missing and could be critical for understanding the true advantage of the proposed method.

4. **Single Dataset Evaluation**: All evaluations are conducted on a single training dataset, making it questionable whether this method can be generalized to other datasets, such as WizardLM, or multi-turn data, like the ShareGPT dataset. Testing on a more diverse set of datasets would strengthen the paper's claims of robustness and general applicability.

Limitations:
See more details in Weaknesses section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The authors present a novel method for interpretable image classification by incorporating a vision transformer (ViT) into the prototypical neural network framework which provides case-based reasoning to neural network based image classifiers. They claim that most existing prototypical methods are convolutional neural network (CNN)-based, and those methods are limited by spatially rigid prototypes, thus failing to handle geometric variations of objects. Existing methods that try to handle this geometric variation either rely on a continuous latent space, which is not compatible with ViTs, or are other prototype-based ViT that fail to provide inherently interpretable explanations. Due to these existing problems, the author present ProtoViT with the following contributions:
Incorporates a ViT backbone that can adaptively learn interpretable prototypes that can handle geometric variation of different sizes.
They achieve the above with a greedy matching algorithm utilizing an adjacency mask and an adaptive slots mechanism.
They give empirical evaluation showing SOTA accuracy and a qualitative analysis showing the faithfulness and coherence of the learned prototype representations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Soundness:

The methods are clean and sound with ample ablation experiments, and it appears their approach can perform better than others (marginally) and have interpretable and coherent prototypes. 

Presentation:

The paper was easy to follow with clear claims, ideas and methods. While some of the figures could be a bit cleaner (such as the boundaries and borders in Figure 4), and some notation seemed a bit odd, they communicated their ideas/methods well.

Contribution:

The paper gives a clean method for utilizing ViTs in the prototypical framework of deeplearning for interpretability and even incorporates existing methods for making prototypes more flexible via utilizing the approach from Deformable ProtoPNet. They incorporate a novel coherence loss that encourages sub prototypes to be similar to each other. In addition, this paper utilizes a greedy matching algorithm with an adaptive mask to learn geometrically local sub-prototypes. This method also allows for an adaptive number of sub-prototypes through the slot pruning mechanism.

Weaknesses:
Soundness:

The lack of qualitative comparison with other ViT methods. I know the authors state that these other vision transformer methods do not project the learned prototypical features to the closest latent patches, but they still provide explanations. Could more be expanded on this and/or a figure showing this lack of reasoning/inherent interpretability?
- This is my biggest concern

Presentation:

In the section 3.4 for “Optimization of last layers” did you mean “... l-th class prototypes…” with a plural on the prototypes? This was unclear to me

Contribution:

However due to already preexisting ViT prototype methods (with a lack of comparison to them), the contribution this ViT makes compared to others is unclear.

Limitations:
The method lacks across class prototype sharing.

The coherence loss lack discussion on its limitation of making sub-prototypes being similar thus hindering diverse representation which may be important in a prototype.

The paper lacks comparison to other interpretable ViT methods

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce ProtoViT, a model that leverages the Visual Transformer (ViT) architecture and integrates prototypical parts for case-based reasoning. This method is self-explainable, adhering to the rule ""this looks like that."" A novel aspect of ProtoViT is the use of prototypical parts of varying sizes, utilizing a ViT backbone. The authors claim that these prototypical parts are coherent and inherently interpretable. They evaluate ProtoViT on the CUB and Stanford Cars datasets, using accuracy as the metric for comparison. The training process for the model involves five loss components in addition to cross-entropy.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well-written, with images effectively illustrating the intended concepts. The introduction of the greedy matching algorithm is particularly engaging and holds significant importance for the community. The introduction section is well-crafted, clearly outlining the contributions. Additionally, the computational experiments are thorough and demonstrate comprehensive accuracy.

Weaknesses:
The primary concern lies in ensuring that the ViT backbone can maintain prototypical parts that are both local and inherently interpretable. Since ViT uses attention mechanisms that mix information from all patches, there is a risk of confusion for the end user. To address this, I suggest conducting a spatial misalignment benchmark [1] to analyze its influence.

Additionally, there are no metrics related to explainability demonstrating whether the model improves interpretability, such as with FunnyBirds [2] or through a user study [3].

[1] Sacha, Mikołaj, et al. ""Interpretability benchmark for evaluating spatial misalignment of prototypical parts explanations."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 19. 2024.
[2] Hesse, Robin, Simone Schaub-Meyer, and Stefan Roth. ""FunnyBirds: A synthetic vision dataset for a part-based analysis of explainable AI methods."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.
[3] Kim, Sunnie SY, et al. ""HIVE: Evaluating the human interpretability of visual explanations."" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.

Limitations:
There is no quantification of interpretability, no user-study, nor no reference to XAI benchmarks such as FunnyBirds and spatial misalignment benchmarks.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a novel strategy to learn interpretable visual prototypes for visual transformers, with a good property of offering spatially deformed prototypes. The method also introduce an slot mechanism which can learn an adaptive number of prototypical parts. The proposed are wisely designed for visual transformer architectures.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. It is nice to draw inspiration from the focal similarity when computing the  patch features.
2. Different from Deformable ProtoPNet, the proposed method present a new way to accommodate geometric variations of objects.  
3. The proposed method is validated on two benchmarks and with extensive ablation studies. 
4. In general, the paper is well written.

Weaknesses:
1. The first concerns is about the use of deformable prototypes that containing K sub-prototypes, which means the proposed method will have more learnable prototype vectors (K times) than ProtoPNet, TesNet, ProtoPFormer, and so on. These previous approaches only use 1*1 prototypes. Does the performance improvement of the proposed method come from the largely increased number of sub-prototypes?
2. The idea of greedy matching algorithm is similar to the greedy prototype projection, proposed in [1, 2]. The authors are suggested to state their difference of the related works. Also, some important work using prototypes for interpretable image classification should be reviewed and discussed, such as [3, 4, 5]. 
3. Regarding the adaptive slots mechanism, it is good for the motivation of to learn an additional indicator to measure the importance of sub-prototypes. From my understanding, the learnable vector v is like a gate, which should be saved as model parameters after training. One potential limitation is such mechanism introduces an extra gate parameter v, compared with previous methods.
4. It not much clear about the adjacency masking. Since the prototypes are not initialized to have the position information, how do choose the patch/feature tokens around the prototypes within r? 
5. The authors mention the issue of performance degradation after prototype projection. Does the proposed method still suffer from such issue? What extent will the performance drop?
6. The method has too much loss coefficients, which are selected without detailed tuning procedure. 


References:

[1] Knowledge Distillation to Ensemble Global and Interpretable Prototype-Based Mammogram Classification Models

[2] Pixel-grounded prototypical part networks

[3] PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification

[4] Learning support and trivial prototypes for interpretable image classification

[5] Concept-level debugging of part-prototype network

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents ProtoViT, a method for interpretable image classification. ProtoViT incorporates ViT backbone with deformed prototypes that explains its predictions. ProtoViT consists of three components: 

(1) a feature encoding layer with a pre-trained ViT backbone, which computes a latent representation of an image; 

(2) a greedy matching layer which compares the latent representation to learned prototypes; and 

(3) an evidence layer which aggregates prototype similarity scores into a classification using a fully connected layer. 

Quantitatively, ProtoViT achieves better performance than previous prototype-based methods. Qualitatively, it identifies meaningful prototypes to explain the prediction.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written and illustrates the technical details. The qualitative visualization are helpful in understanding the method. Attached appendix provides a lot of meaningful details.


2. The overall scheme of interpretable image classification is nicely presented with small patches!

Weaknesses:
1. It is understandable that one goal of interpretable image classification is to tell why something works. However, a bigger goal is to understand why something didn't really work. Is it possible to highlight the examples where the method could not predict the correct class? It would be then interesting to understand the reasons for failure.

2. The method relies on a strong assumption of a solid backbone model that can already achieve a good performance on the given task. This restricts the applicability of the method to limited scenarios where we do not need interpretable image classification at the first place. In such a scenarios, the nearest neighbors obtained using latent feature representation and pixel correspondences may be sufficient.

3. Limitations of the method is not clear from the paper. I could guess at certain places in the method section where things could go wrong. It would be better if the authors could illustrate those points with suitable qualitative analysis.

4. User study is not properly designed and is not statistically significant.

Overall: The paper is interesting. In a first reading, everything looks good. But then you start asking yourself questions about the different scenarios where the method will not work (and there are definitely such scenarios as can be judged from quantitative evaluation and user study) and why it will not work. The paper falls short in explaining them with details.

Limitations:
Authors did provide limitations but a far-fetched one that does not really talk about the limitations of the current method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The authors present a novel method for interpretable image classification by incorporating a vision transformer (ViT) into the prototypical neural network framework which provides case-based reasoning to neural network based image classifiers. They claim that most existing prototypical methods are convolutional neural network (CNN)-based, and those methods are limited by spatially rigid prototypes, thus failing to handle geometric variations of objects. Existing methods that try to handle this geometric variation either rely on a continuous latent space, which is not compatible with ViTs, or are other prototype-based ViT that fail to provide inherently interpretable explanations. Due to these existing problems, the author present ProtoViT with the following contributions:
Incorporates a ViT backbone that can adaptively learn interpretable prototypes that can handle geometric variation of different sizes.
They achieve the above with a greedy matching algorithm utilizing an adjacency mask and an adaptive slots mechanism.
They give empirical evaluation showing SOTA accuracy and a qualitative analysis showing the faithfulness and coherence of the learned prototype representations.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Soundness:

The methods are clean and sound with ample ablation experiments, and it appears their approach can perform better than others (marginally) and have interpretable and coherent prototypes. 

Presentation:

The paper was easy to follow with clear claims, ideas and methods. While some of the figures could be a bit cleaner (such as the boundaries and borders in Figure 4), and some notation seemed a bit odd, they communicated their ideas/methods well.

Contribution:

The paper gives a clean method for utilizing ViTs in the prototypical framework of deeplearning for interpretability and even incorporates existing methods for making prototypes more flexible via utilizing the approach from Deformable ProtoPNet. They incorporate a novel coherence loss that encourages sub prototypes to be similar to each other. In addition, this paper utilizes a greedy matching algorithm with an adaptive mask to learn geometrically local sub-prototypes. This method also allows for an adaptive number of sub-prototypes through the slot pruning mechanism.

Weaknesses:
Soundness:

The lack of qualitative comparison with other ViT methods. I know the authors state that these other vision transformer methods do not project the learned prototypical features to the closest latent patches, but they still provide explanations. Could more be expanded on this and/or a figure showing this lack of reasoning/inherent interpretability?
- This is my biggest concern

Presentation:

In the section 3.4 for “Optimization of last layers” did you mean “... l-th class prototypes…” with a plural on the prototypes? This was unclear to me

Contribution:

However due to already preexisting ViT prototype methods (with a lack of comparison to them), the contribution this ViT makes compared to others is unclear.

Limitations:
The method lacks across class prototype sharing.

The coherence loss lack discussion on its limitation of making sub-prototypes being similar thus hindering diverse representation which may be important in a prototype.

The paper lacks comparison to other interpretable ViT methods

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors introduce ProtoViT, a model that leverages the Visual Transformer (ViT) architecture and integrates prototypical parts for case-based reasoning. This method is self-explainable, adhering to the rule ""this looks like that."" A novel aspect of ProtoViT is the use of prototypical parts of varying sizes, utilizing a ViT backbone. The authors claim that these prototypical parts are coherent and inherently interpretable. They evaluate ProtoViT on the CUB and Stanford Cars datasets, using accuracy as the metric for comparison. The training process for the model involves five loss components in addition to cross-entropy.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper is well-written, with images effectively illustrating the intended concepts. The introduction of the greedy matching algorithm is particularly engaging and holds significant importance for the community. The introduction section is well-crafted, clearly outlining the contributions. Additionally, the computational experiments are thorough and demonstrate comprehensive accuracy.

Weaknesses:
The primary concern lies in ensuring that the ViT backbone can maintain prototypical parts that are both local and inherently interpretable. Since ViT uses attention mechanisms that mix information from all patches, there is a risk of confusion for the end user. To address this, I suggest conducting a spatial misalignment benchmark [1] to analyze its influence.

Additionally, there are no metrics related to explainability demonstrating whether the model improves interpretability, such as with FunnyBirds [2] or through a user study [3].

[1] Sacha, Mikołaj, et al. ""Interpretability benchmark for evaluating spatial misalignment of prototypical parts explanations."" Proceedings of the AAAI Conference on Artificial Intelligence. Vol. 38. No. 19. 2024.
[2] Hesse, Robin, Simone Schaub-Meyer, and Stefan Roth. ""FunnyBirds: A synthetic vision dataset for a part-based analysis of explainable AI methods."" Proceedings of the IEEE/CVF International Conference on Computer Vision. 2023.
[3] Kim, Sunnie SY, et al. ""HIVE: Evaluating the human interpretability of visual explanations."" European Conference on Computer Vision. Cham: Springer Nature Switzerland, 2022.

Limitations:
There is no quantification of interpretability, no user-study, nor no reference to XAI benchmarks such as FunnyBirds and spatial misalignment benchmarks.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper presents a novel strategy to learn interpretable visual prototypes for visual transformers, with a good property of offering spatially deformed prototypes. The method also introduce an slot mechanism which can learn an adaptive number of prototypical parts. The proposed are wisely designed for visual transformer architectures.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. It is nice to draw inspiration from the focal similarity when computing the  patch features.
2. Different from Deformable ProtoPNet, the proposed method present a new way to accommodate geometric variations of objects.  
3. The proposed method is validated on two benchmarks and with extensive ablation studies. 
4. In general, the paper is well written.

Weaknesses:
1. The first concerns is about the use of deformable prototypes that containing K sub-prototypes, which means the proposed method will have more learnable prototype vectors (K times) than ProtoPNet, TesNet, ProtoPFormer, and so on. These previous approaches only use 1*1 prototypes. Does the performance improvement of the proposed method come from the largely increased number of sub-prototypes?
2. The idea of greedy matching algorithm is similar to the greedy prototype projection, proposed in [1, 2]. The authors are suggested to state their difference of the related works. Also, some important work using prototypes for interpretable image classification should be reviewed and discussed, such as [3, 4, 5]. 
3. Regarding the adaptive slots mechanism, it is good for the motivation of to learn an additional indicator to measure the importance of sub-prototypes. From my understanding, the learnable vector v is like a gate, which should be saved as model parameters after training. One potential limitation is such mechanism introduces an extra gate parameter v, compared with previous methods.
4. It not much clear about the adjacency masking. Since the prototypes are not initialized to have the position information, how do choose the patch/feature tokens around the prototypes within r? 
5. The authors mention the issue of performance degradation after prototype projection. Does the proposed method still suffer from such issue? What extent will the performance drop?
6. The method has too much loss coefficients, which are selected without detailed tuning procedure. 


References:

[1] Knowledge Distillation to Ensemble Global and Interpretable Prototype-Based Mammogram Classification Models

[2] Pixel-grounded prototypical part networks

[3] PIP-Net: Patch-Based Intuitive Prototypes for Interpretable Image Classification

[4] Learning support and trivial prototypes for interpretable image classification

[5] Concept-level debugging of part-prototype network

Limitations:
N/A

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper presents ProtoViT, a method for interpretable image classification. ProtoViT incorporates ViT backbone with deformed prototypes that explains its predictions. ProtoViT consists of three components: 

(1) a feature encoding layer with a pre-trained ViT backbone, which computes a latent representation of an image; 

(2) a greedy matching layer which compares the latent representation to learned prototypes; and 

(3) an evidence layer which aggregates prototype similarity scores into a classification using a fully connected layer. 

Quantitatively, ProtoViT achieves better performance than previous prototype-based methods. Qualitatively, it identifies meaningful prototypes to explain the prediction.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well-written and illustrates the technical details. The qualitative visualization are helpful in understanding the method. Attached appendix provides a lot of meaningful details.


2. The overall scheme of interpretable image classification is nicely presented with small patches!

Weaknesses:
1. It is understandable that one goal of interpretable image classification is to tell why something works. However, a bigger goal is to understand why something didn't really work. Is it possible to highlight the examples where the method could not predict the correct class? It would be then interesting to understand the reasons for failure.

2. The method relies on a strong assumption of a solid backbone model that can already achieve a good performance on the given task. This restricts the applicability of the method to limited scenarios where we do not need interpretable image classification at the first place. In such a scenarios, the nearest neighbors obtained using latent feature representation and pixel correspondences may be sufficient.

3. Limitations of the method is not clear from the paper. I could guess at certain places in the method section where things could go wrong. It would be better if the authors could illustrate those points with suitable qualitative analysis.

4. User study is not properly designed and is not statistically significant.

Overall: The paper is interesting. In a first reading, everything looks good. But then you start asking yourself questions about the different scenarios where the method will not work (and there are definitely such scenarios as can be judged from quantitative evaluation and user study) and why it will not work. The paper falls short in explaining them with details.

Limitations:
Authors did provide limitations but a far-fetched one that does not really talk about the limitations of the current method.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposes a new method PsHD to preserve intrinsic structural information in semi-supervised continual learning. The method proposes to uses distillation and cross-entropy loss on the continual learning samples.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. I think the paper presents quite comprehensive experiments with different settings. In all of the experiments, the work demonstrate at least marginal improvement compared to previous methods
2. I think figure.3 looks very interesting and seems to justify the paper's motivation.

Weaknesses:
1. I think the paper can be further polished in terms of writing, presentation and clarity. For example. I think Figure.4 can be further improved for clarity. 
2. The results raise some concerns regarding the proposed method. In Table 4, we observe that changing the data allocation ratio results in performance change within 1.5% for 5% and within 1% for 25% setting. Also in Table.4, optimal data ratio are different for the two settings. We also observe in Table 1 that the proposed method leads the previous methods by less than 1%. It makes the author's claim for their ``superiority'' performance quite unsupported. 
3. This brings my third concern to the work: Some language in the paper seems overclaimed. The paper tried to adopt many large words like ``significant'' and ``superior'' while the performance does not support such claims.
4. In Figure.4, larger lambda seem to help the model performance, then why not train with larger lambda? Maybe there is an elbow effect but there needs more experiment to show that.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a persistence homology knowledge distillation for continual learning (PsHD). PsHD loss is calculated using a ''memory buffer'' between a previous variant of a network and a new one. Experiments show some improvement w.r.t. baselines. Ablation studies are provided.
The main issue of the paper is limited novelty, also some essential details are missing (see below).

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Novelty: this is a first application of persistent homology to continual learning.
2. Experiments are correct, ablation studies are provided.
3. The manuscript is well organized, the idea is easily comprehensible.
4. Visualizing of attentions maps helps to reveal how the method improves continual learning.

Weaknesses:
1. The idea that knowledge distillation can help continual learning is not new, see [2]. Given this, the novelty of the paper is small.
Do you have an ablation with a traditional KD [1] and its more recent variants from [2]?
2. A relevant reference [3] is missing. How you paper is related to [3], is your method better?
3. I can't find an explicit equation for $L_{CL}$.

[1] Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.
[2] Li, S., Su, T., Zhang, X., & Wang, Z. (2024). Continual Learning with Knowledge Distillation: A Survey. Authorea Preprints.
[3] Kim, J., You, J., Lee, D., Kim, H. Y., & Jung, J. H. Do Topological Characteristics Help in Knowledge Distillation?. In Forty-first International Conference on Machine Learning.

Limitations:
-

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed to preserve intrinsic structural information with the use of persistent homology, so as to improve knowledge distillation and memory replay in semi-supervised continual learning. The authors provided an efficient acceleration algorithm to reduce computational overheads and theoretically demonstrated its stability. Extensive experiments demonstrate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. This paper is well organized and easy to follow. The motivation is clearly presented. Most of the related work has been discussed.

2. The design of persistent homology for knowledge distillation is reasonable to me. Although it seems to be an incremental design, the authors have provided many adaptations, such as the acceleration algorithm and the theoretical analysis.

3. The experimental validation is essentially extensive. It covers different setups of semi-supervised continual learning, ablation study, visualization, etc.

Weaknesses:
1. Although the authors have provided extensive experiments, the effectiveness of the proposed method seems to be limited. It offers less than 1% improvements in a majority of cases with relatively simple datasets (e.g., CIFAR-10 and CIFAR-100). This may be due to the classic NP-hard problem in selecting a few data to represent the entire dataset.

2. Although the authors have provided an acceleration algorithm to reduce computational overheads and conceptually analyzed it with their own method, I suggest to compare the overall resource overheads (storage and computation) of their method with other strong baselines (e.g., NNCSL, DER_Fix, and DSGD).

Limitations:
In Checklist the authors claimed that they have discussed the limitations in Section 5. However, Section 5 only presents the Conclusion without any discussion of the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposes a new method PsHD to preserve intrinsic structural information in semi-supervised continual learning. The method proposes to uses distillation and cross-entropy loss on the continual learning samples.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. I think the paper presents quite comprehensive experiments with different settings. In all of the experiments, the work demonstrate at least marginal improvement compared to previous methods
2. I think figure.3 looks very interesting and seems to justify the paper's motivation.

Weaknesses:
1. I think the paper can be further polished in terms of writing, presentation and clarity. For example. I think Figure.4 can be further improved for clarity. 
2. The results raise some concerns regarding the proposed method. In Table 4, we observe that changing the data allocation ratio results in performance change within 1.5% for 5% and within 1% for 25% setting. Also in Table.4, optimal data ratio are different for the two settings. We also observe in Table 1 that the proposed method leads the previous methods by less than 1%. It makes the author's claim for their ``superiority'' performance quite unsupported. 
3. This brings my third concern to the work: Some language in the paper seems overclaimed. The paper tried to adopt many large words like ``significant'' and ``superior'' while the performance does not support such claims.
4. In Figure.4, larger lambda seem to help the model performance, then why not train with larger lambda? Maybe there is an elbow effect but there needs more experiment to show that.

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a persistence homology knowledge distillation for continual learning (PsHD). PsHD loss is calculated using a ''memory buffer'' between a previous variant of a network and a new one. Experiments show some improvement w.r.t. baselines. Ablation studies are provided.
The main issue of the paper is limited novelty, also some essential details are missing (see below).

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Novelty: this is a first application of persistent homology to continual learning.
2. Experiments are correct, ablation studies are provided.
3. The manuscript is well organized, the idea is easily comprehensible.
4. Visualizing of attentions maps helps to reveal how the method improves continual learning.

Weaknesses:
1. The idea that knowledge distillation can help continual learning is not new, see [2]. Given this, the novelty of the paper is small.
Do you have an ablation with a traditional KD [1] and its more recent variants from [2]?
2. A relevant reference [3] is missing. How you paper is related to [3], is your method better?
3. I can't find an explicit equation for $L_{CL}$.

[1] Hinton, G., Vinyals, O., & Dean, J. (2015). Distilling the knowledge in a neural network. arXiv preprint arXiv:1503.02531.
[2] Li, S., Su, T., Zhang, X., & Wang, Z. (2024). Continual Learning with Knowledge Distillation: A Survey. Authorea Preprints.
[3] Kim, J., You, J., Lee, D., Kim, H. Y., & Jung, J. H. Do Topological Characteristics Help in Knowledge Distillation?. In Forty-first International Conference on Machine Learning.

Limitations:
-

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposed to preserve intrinsic structural information with the use of persistent homology, so as to improve knowledge distillation and memory replay in semi-supervised continual learning. The authors provided an efficient acceleration algorithm to reduce computational overheads and theoretically demonstrated its stability. Extensive experiments demonstrate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
1. This paper is well organized and easy to follow. The motivation is clearly presented. Most of the related work has been discussed.

2. The design of persistent homology for knowledge distillation is reasonable to me. Although it seems to be an incremental design, the authors have provided many adaptations, such as the acceleration algorithm and the theoretical analysis.

3. The experimental validation is essentially extensive. It covers different setups of semi-supervised continual learning, ablation study, visualization, etc.

Weaknesses:
1. Although the authors have provided extensive experiments, the effectiveness of the proposed method seems to be limited. It offers less than 1% improvements in a majority of cases with relatively simple datasets (e.g., CIFAR-10 and CIFAR-100). This may be due to the classic NP-hard problem in selecting a few data to represent the entire dataset.

2. Although the authors have provided an acceleration algorithm to reduce computational overheads and conceptually analyzed it with their own method, I suggest to compare the overall resource overheads (storage and computation) of their method with other strong baselines (e.g., NNCSL, DER_Fix, and DSGD).

Limitations:
In Checklist the authors claimed that they have discussed the limitations in Section 5. However, Section 5 only presents the Conclusion without any discussion of the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper analyzes the intricate relationship between optimal learning rate and batch size scaling for adaptive optimizers, such as Sign SGD and Adam. Building on prior analysis for SGD by McCandlish (2018), this work reveals a non-monotonic relationship between optimal learning rates and batch size. The optimal learning rate initially increases, reaches a peak, and then decreases, eventually saturating to a non-zero value, referred to as the surge phenomenon. These theoretical predictions are validated through experiments in both image classification and NLP tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper provides new insights into the relationship between optimal learning rate and batch size for Adam optimizers. The drop in the optimal learning rate after the peak is new, to the best of my knowledge, and is relevant given that Adam is the default optimizer choice.
* Prior results suggesting a square root scaling for Adam are reproduced, further supporting the findings.

Weaknesses:
* At times, the paper assumes that the reader is well-versed with the prior work of McCandlish (2018). For instance, lines 129-131. It would be helpful to reiterate prior results to motivate the analysis.
* The empirical results are not very convincing. If we only consider the experiments (for instance Figure 4), without any reference to theoretical results, the surge phenomenon does not seem appreciable. A finer learning rate search has to be performed to demonstrate the surge phenomenon clearly. In Figure 4(b), the optimal learning rate is oscillating around two points. It is unclear if this is due to the surge phenomenon or just random fluctuations. I would request the authors to help me understand their empirical results better.
* The theoretical results are derived for sign SGD, while it's known that Adam parameters beta1 and beta2 are crucial hyperparameters. It's unclear why the theoretical results can be generalized to Adam.

Limitations:
* The theory is built on the quadratic approximation of the loss function. In the last few years, it has been established that modern neural networks are typically trained at large learning rates (Edge of Stability, see Refs. [1-2]), which cannot be captured using quadratic approximations of the loss [3]. 
* Gaussian distribution for gradients is assumed for the theoretical analysis, whereas it is known that the gradient distribution is not Gaussian, and this is precisely why Adam performs better than SGD in language modeling. It is unclear whether the results hold for such settings.


[1] Gradient descent on neural networks typically occurs at the edge of stability
Cohen et al. (2021)
arXiv:2103.00065

[2] Adaptive gradient methods at the edge of stability
Cohen et al. (2022)
arXiv:2207.14484

[3] Self-stabilization: The implicit bias of gradient descent at the edge of stability
Damian et al. (2022)
arXiv:2209.15594

[4] Linear attention is (maybe) all you need (to understand transformer optimization)
Ahn et al. (2022)
arXiv:2310.01082

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a heuristic analysis of the scaling of the optimal learning rate with batch size for Adam-style optimizers in the framework of [1]. The analysis is accompanied by experiments to support the predictions. Notably the authors demonstrate that the optimal learning rate can decrease with batch size in a certain range, a phenomenon that had not been identified previously and which is not present for SGD. The analysis also recovers the square-root scaling rule in the small batch size regime identified in other work. 

[1] McCandlish, S., Kaplan, J., Amodei, D., & Team, O. D. (2018). An empirical model of large-batch training. arXiv preprint arXiv:1812.06162.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper tackles a practically important problem using a mix of heuristic theory and experiments. The prior literature on linear scaling rules with large batch training applies to SGD but not to Adam style optimizers which are the dominant optimizers for transformers. The surge phenomenon is interesting and novel.

Weaknesses:
The presentation is not great. A lot is assumed from [1], but it would make reading easier to make things more self-contained. Equations like Eq. 22 should be better explained and plots like in Fig 3 are hard to parse. The process for making the Fig. 3 plot is unclear. It is unclear what is going on in Figure 1 between the solid and dashed Adam curves. The takeaways and consequences for a practitioner are unclear. Minor: the Latex parentheses look sloppy. 

[1] McCandlish, S., Kaplan, J., Amodei, D., & Team, O. D. (2018). An empirical model of large-batch training. arXiv preprint arXiv:1812.06162.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper gives an optimal choice of learning rate and batch size for neural networks.  Different from the previous results on SGD-style optimizers. The authors give such solutions for Adam-style-optimizers.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Batch size and learning rate will affect the performance largely and cost a lot to select a good one. It is important to understand the optimal batch size and learning rate for Adam-style optimizers.

2. The experimental results match the theorem proposed by the authors.

Weaknesses:
1. It seems that Lemma 1 can only apply to quadratic problems. In the appendix, the relation is approximately equal. But in Lemma 1, it becomes ""equal"" without any further assumptions.

2.  It is unclear how to select the optimal batch size or learning rate based on the theorem because either S, E or $\mu,\sigma$ is hard to estimate for a large network.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work provides a scaling law between learning rate and batch size for Adam. Namely, this work finds that the optimal learning rate increases and then decreases as the batch size becomes larger; and, the peak of this curve corresponds to the trade-off between training speed and data efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well motivated. While prior works studying the relationship between batch size and learning rate have focused on SGD, this work focuses on Adam (which is more popular / widely used).
- The paper is written clearly and is well organized. I appreciate the “summary” notes included by the authors
- The paper includes empirical evidence for CV and NLP tasks to support theoretical claims.

Weaknesses:
- I think this paper could benefit from experiments with more popular architectures for the NLP tasks (e.g. maybe it would be useful to include some experiments on tasks with llama or mistral models). 
- I also think it would be useful to have experiments with more datasets. Recent work shows that the data itself matters. E.g., for fine tuning LLMs, many factors wrt the data (e.g., data quality, variable sequence lengths, deduplicated data) can affect training. It would be interesting to see if the surge phenomena is agnostic to these factors or not.

Limitations:
It would be interesting to see results on a larger variety of modern and widely used datasets and architectures.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper analyzes the intricate relationship between optimal learning rate and batch size scaling for adaptive optimizers, such as Sign SGD and Adam. Building on prior analysis for SGD by McCandlish (2018), this work reveals a non-monotonic relationship between optimal learning rates and batch size. The optimal learning rate initially increases, reaches a peak, and then decreases, eventually saturating to a non-zero value, referred to as the surge phenomenon. These theoretical predictions are validated through experiments in both image classification and NLP tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* The paper provides new insights into the relationship between optimal learning rate and batch size for Adam optimizers. The drop in the optimal learning rate after the peak is new, to the best of my knowledge, and is relevant given that Adam is the default optimizer choice.
* Prior results suggesting a square root scaling for Adam are reproduced, further supporting the findings.

Weaknesses:
* At times, the paper assumes that the reader is well-versed with the prior work of McCandlish (2018). For instance, lines 129-131. It would be helpful to reiterate prior results to motivate the analysis.
* The empirical results are not very convincing. If we only consider the experiments (for instance Figure 4), without any reference to theoretical results, the surge phenomenon does not seem appreciable. A finer learning rate search has to be performed to demonstrate the surge phenomenon clearly. In Figure 4(b), the optimal learning rate is oscillating around two points. It is unclear if this is due to the surge phenomenon or just random fluctuations. I would request the authors to help me understand their empirical results better.
* The theoretical results are derived for sign SGD, while it's known that Adam parameters beta1 and beta2 are crucial hyperparameters. It's unclear why the theoretical results can be generalized to Adam.

Limitations:
* The theory is built on the quadratic approximation of the loss function. In the last few years, it has been established that modern neural networks are typically trained at large learning rates (Edge of Stability, see Refs. [1-2]), which cannot be captured using quadratic approximations of the loss [3]. 
* Gaussian distribution for gradients is assumed for the theoretical analysis, whereas it is known that the gradient distribution is not Gaussian, and this is precisely why Adam performs better than SGD in language modeling. It is unclear whether the results hold for such settings.


[1] Gradient descent on neural networks typically occurs at the edge of stability
Cohen et al. (2021)
arXiv:2103.00065

[2] Adaptive gradient methods at the edge of stability
Cohen et al. (2022)
arXiv:2207.14484

[3] Self-stabilization: The implicit bias of gradient descent at the edge of stability
Damian et al. (2022)
arXiv:2209.15594

[4] Linear attention is (maybe) all you need (to understand transformer optimization)
Ahn et al. (2022)
arXiv:2310.01082

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents a heuristic analysis of the scaling of the optimal learning rate with batch size for Adam-style optimizers in the framework of [1]. The analysis is accompanied by experiments to support the predictions. Notably the authors demonstrate that the optimal learning rate can decrease with batch size in a certain range, a phenomenon that had not been identified previously and which is not present for SGD. The analysis also recovers the square-root scaling rule in the small batch size regime identified in other work. 

[1] McCandlish, S., Kaplan, J., Amodei, D., & Team, O. D. (2018). An empirical model of large-batch training. arXiv preprint arXiv:1812.06162.

Soundness:
3: good

Presentation:
1: poor

Contribution:
2: fair

Strengths:
The paper tackles a practically important problem using a mix of heuristic theory and experiments. The prior literature on linear scaling rules with large batch training applies to SGD but not to Adam style optimizers which are the dominant optimizers for transformers. The surge phenomenon is interesting and novel.

Weaknesses:
The presentation is not great. A lot is assumed from [1], but it would make reading easier to make things more self-contained. Equations like Eq. 22 should be better explained and plots like in Fig 3 are hard to parse. The process for making the Fig. 3 plot is unclear. It is unclear what is going on in Figure 1 between the solid and dashed Adam curves. The takeaways and consequences for a practitioner are unclear. Minor: the Latex parentheses look sloppy. 

[1] McCandlish, S., Kaplan, J., Amodei, D., & Team, O. D. (2018). An empirical model of large-batch training. arXiv preprint arXiv:1812.06162.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper gives an optimal choice of learning rate and batch size for neural networks.  Different from the previous results on SGD-style optimizers. The authors give such solutions for Adam-style-optimizers.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. Batch size and learning rate will affect the performance largely and cost a lot to select a good one. It is important to understand the optimal batch size and learning rate for Adam-style optimizers.

2. The experimental results match the theorem proposed by the authors.

Weaknesses:
1. It seems that Lemma 1 can only apply to quadratic problems. In the appendix, the relation is approximately equal. But in Lemma 1, it becomes ""equal"" without any further assumptions.

2.  It is unclear how to select the optimal batch size or learning rate based on the theorem because either S, E or $\mu,\sigma$ is hard to estimate for a large network.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work provides a scaling law between learning rate and batch size for Adam. Namely, this work finds that the optimal learning rate increases and then decreases as the batch size becomes larger; and, the peak of this curve corresponds to the trade-off between training speed and data efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well motivated. While prior works studying the relationship between batch size and learning rate have focused on SGD, this work focuses on Adam (which is more popular / widely used).
- The paper is written clearly and is well organized. I appreciate the “summary” notes included by the authors
- The paper includes empirical evidence for CV and NLP tasks to support theoretical claims.

Weaknesses:
- I think this paper could benefit from experiments with more popular architectures for the NLP tasks (e.g. maybe it would be useful to include some experiments on tasks with llama or mistral models). 
- I also think it would be useful to have experiments with more datasets. Recent work shows that the data itself matters. E.g., for fine tuning LLMs, many factors wrt the data (e.g., data quality, variable sequence lengths, deduplicated data) can affect training. It would be interesting to see if the surge phenomena is agnostic to these factors or not.

Limitations:
It would be interesting to see results on a larger variety of modern and widely used datasets and architectures.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper studies the causal discovery problem in mixed functional relations data, where both linear and non-linear relationships exist in the causal graph. The author presents a Jacobian score-based method (essentially a score-matching method) to identify leaf nodes and thereby recover the causal order. The experimental results demonstrate the efficiency of the proposed methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is clearly written and well-organized.

2. The setting of mixed functional relations data is interesting and may be important for real-world scenarios.

3. The author proposes a Jacobian score-based method, which is an extension of the score-matching method for non-linear Additive Noise Models (ANM).

Weaknesses:
1. The non-decreasing variance of noises assumption is too strong and restrictive. Typically, in ANM, the noise term is assumed to be mutually independent.

2. It appears that the primary difference between the score-matching method for ANM and the proposed method is the introduction of Assumption 1.

3. If I use an independent residuals-based method, it seems to work in your setting. So, what are the advantages of the proposed method? For example, is the proposed method capable of handling large-scale structures? If so, the experimental results should demonstrate this.

Limitations:
NAN

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an ordering based causal discovery method when the underlying causal model has both linear and nonlinear causal relationships. Starting with a method to iteratively find leaf nodes, this paper proposes to use parent score for better pruning. Results show that the proposed method outperforms baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Paper is written well and easy to understand.
2. Theoretical motivations are clearly explained and the proof are adequately provided.
3. Experiments are extensive and cover all theoretical aspects.

Weaknesses:
1. Results are not great on real-world datasets. 
2. Topological divergence is a popular metric for evaluating the topological order. Very few results are presented in supplementary on this metric.

Limitations:
Limitations are discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose an ordering-based causal discovery algorithm designed to handle both linear and nonlinear causal relations in an SEM. In contrast to existing methods that assume purely linear or nonlinear relations, CaPS introduces a unified criterion for topological ordering and a new ""parent score"" to quantify the average causal effect, which aids in pruning and correcting predictions. Experimental results show that CaPS outperforms some sota methods on synthetic data with mixed linear and nonlinear relations and demonstrates competitive performance on real-world data.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* CaPS provides a new approach that can handle both linear and nonlinear causal relationships, addressing a relevant gap in current causal discovery methods.
* The introduction of the parent score is interesting and provides a quantitative measure of causal strength, which improves the pruning process and prediction accuracy.
* The authors present a new criterion for distinguishing leaf nodes using the expectation of the Hessian of the data log-likelihood and provides sufficient conditions for the identifiability of the causal graph, inspired by SCORE and LiSTEN.

Weaknesses:
* All noises are assumed to be Gaussian.
* The identifiability conditions rely on assumptions such as non-decreasing variance of noises, which is hard to hold in practical scenarios.
* Some more recent methods are not compared against.

Limitations:
There are some limitations not ""explicitly"" stated such as assumptions on causal sufficiency and Gaussianity of noises.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of ordering-based causal discovery, which involves first determining the topological ordering of variables (typically by recursively identifying sub-leaf nodes) and then identifying the parent set for each variable.

Existing methods often focus on either nonlinear or linear relationships. For instance, SCORE relies on a constant score Jacobian, which fails in the absence of nonlinear relationships, whereas LISTEN employs a precision matrix, which makes no sense in nonlinear contexts.

This work proposes an ordering-based method that accommodates both linear and nonlinear relationships. Specifically, it identifies the topological ordering using the expectation (instead of the variance) of the score's Jacobian, under a sortability assumption on the exogenous noise components. Subsequently, average treatment effect estimation is extended to identify the parent sets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. The application of ordering-based causal discovery methods to models with both linear and nonlinear relationships is novel to me.

2. The theorems and mathematical details appear to be correct, though I haven't checked all the details.

3. The experimental results are comprehensive, covering various competitors, different settings, and cases where assumptions are violated (e.g., C.7).

Weaknesses:
1. **Assumptions are too strong:**
  - For linear relationships in the ANM, additional assumptions are required for identifiability. This paper adopts assumptions similar to those in LISTEN, namely, that the variances of exogenous additive noise components follow the same topological ordering of the causal DAG, akin to VAR-sortability assumptions (Reisach et al., 2021). These assumptions are overly stringent, impractical, and lack testability. More discussions regarding this can be referred to ""Structure Learning with Continuous Optimization: A Sober Look and Beyond"".
  - The authors also assume that all additive noise components are zero-mean Gaussian. It is unclear if this assumption is utilized throughout the paper or why it was mentioned if not. Also, are these assumptions testable?
  - Regarding the Gaussian assumption, if it is not used for any proof, the authors might consider assuming non-Gaussian noise. This would allow the DAG to be identifiable even with linear relationships. Then, with score matching (which still works, as in Sec4.3 in the SCORE paper) and some straightforward processing (to preserve for non-Gaussianity/residual independence), the problem might still be solvable in a much more elegant way.

2. **Insufficient motivation for ""parent score"":** Once the topological ordering of the DAG is identified, one could use conditional independence tests between variables and all preceding variables to determine each edge's existence (as in most permutation-based methods), or employ sparse regression, as suggested in the original CAM paper. The authors need to justify the necessity of proposing a ""parent score,"" which seems over-complicated with average treatment effect estimation framework. Are there any advantages (e.g., in terms of time complexity or finite sample guarantee, as in the LISTEN paper)?

3. **Lack of technical novelty:** The technical contributions mainly combine ideas from SCORE and LISTEN, making the results and derivations (e.g., from constant variances to expected value of variances) straightforward extensions of previous work. While novelty is not a primary concern for me, it is worth noting as a minor weakness.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper studies the causal discovery problem in mixed functional relations data, where both linear and non-linear relationships exist in the causal graph. The author presents a Jacobian score-based method (essentially a score-matching method) to identify leaf nodes and thereby recover the causal order. The experimental results demonstrate the efficiency of the proposed methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is clearly written and well-organized.

2. The setting of mixed functional relations data is interesting and may be important for real-world scenarios.

3. The author proposes a Jacobian score-based method, which is an extension of the score-matching method for non-linear Additive Noise Models (ANM).

Weaknesses:
1. The non-decreasing variance of noises assumption is too strong and restrictive. Typically, in ANM, the noise term is assumed to be mutually independent.

2. It appears that the primary difference between the score-matching method for ANM and the proposed method is the introduction of Assumption 1.

3. If I use an independent residuals-based method, it seems to work in your setting. So, what are the advantages of the proposed method? For example, is the proposed method capable of handling large-scale structures? If so, the experimental results should demonstrate this.

Limitations:
NAN

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes an ordering based causal discovery method when the underlying causal model has both linear and nonlinear causal relationships. Starting with a method to iteratively find leaf nodes, this paper proposes to use parent score for better pruning. Results show that the proposed method outperforms baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Paper is written well and easy to understand.
2. Theoretical motivations are clearly explained and the proof are adequately provided.
3. Experiments are extensive and cover all theoretical aspects.

Weaknesses:
1. Results are not great on real-world datasets. 
2. Topological divergence is a popular metric for evaluating the topological order. Very few results are presented in supplementary on this metric.

Limitations:
Limitations are discussed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose an ordering-based causal discovery algorithm designed to handle both linear and nonlinear causal relations in an SEM. In contrast to existing methods that assume purely linear or nonlinear relations, CaPS introduces a unified criterion for topological ordering and a new ""parent score"" to quantify the average causal effect, which aids in pruning and correcting predictions. Experimental results show that CaPS outperforms some sota methods on synthetic data with mixed linear and nonlinear relations and demonstrates competitive performance on real-world data.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
* CaPS provides a new approach that can handle both linear and nonlinear causal relationships, addressing a relevant gap in current causal discovery methods.
* The introduction of the parent score is interesting and provides a quantitative measure of causal strength, which improves the pruning process and prediction accuracy.
* The authors present a new criterion for distinguishing leaf nodes using the expectation of the Hessian of the data log-likelihood and provides sufficient conditions for the identifiability of the causal graph, inspired by SCORE and LiSTEN.

Weaknesses:
* All noises are assumed to be Gaussian.
* The identifiability conditions rely on assumptions such as non-decreasing variance of noises, which is hard to hold in practical scenarios.
* Some more recent methods are not compared against.

Limitations:
There are some limitations not ""explicitly"" stated such as assumptions on causal sufficiency and Gaussianity of noises.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the challenge of ordering-based causal discovery, which involves first determining the topological ordering of variables (typically by recursively identifying sub-leaf nodes) and then identifying the parent set for each variable.

Existing methods often focus on either nonlinear or linear relationships. For instance, SCORE relies on a constant score Jacobian, which fails in the absence of nonlinear relationships, whereas LISTEN employs a precision matrix, which makes no sense in nonlinear contexts.

This work proposes an ordering-based method that accommodates both linear and nonlinear relationships. Specifically, it identifies the topological ordering using the expectation (instead of the variance) of the score's Jacobian, under a sortability assumption on the exogenous noise components. Subsequently, average treatment effect estimation is extended to identify the parent sets.

Soundness:
3: good

Presentation:
2: fair

Contribution:
1: poor

Strengths:
1. The application of ordering-based causal discovery methods to models with both linear and nonlinear relationships is novel to me.

2. The theorems and mathematical details appear to be correct, though I haven't checked all the details.

3. The experimental results are comprehensive, covering various competitors, different settings, and cases where assumptions are violated (e.g., C.7).

Weaknesses:
1. **Assumptions are too strong:**
  - For linear relationships in the ANM, additional assumptions are required for identifiability. This paper adopts assumptions similar to those in LISTEN, namely, that the variances of exogenous additive noise components follow the same topological ordering of the causal DAG, akin to VAR-sortability assumptions (Reisach et al., 2021). These assumptions are overly stringent, impractical, and lack testability. More discussions regarding this can be referred to ""Structure Learning with Continuous Optimization: A Sober Look and Beyond"".
  - The authors also assume that all additive noise components are zero-mean Gaussian. It is unclear if this assumption is utilized throughout the paper or why it was mentioned if not. Also, are these assumptions testable?
  - Regarding the Gaussian assumption, if it is not used for any proof, the authors might consider assuming non-Gaussian noise. This would allow the DAG to be identifiable even with linear relationships. Then, with score matching (which still works, as in Sec4.3 in the SCORE paper) and some straightforward processing (to preserve for non-Gaussianity/residual independence), the problem might still be solvable in a much more elegant way.

2. **Insufficient motivation for ""parent score"":** Once the topological ordering of the DAG is identified, one could use conditional independence tests between variables and all preceding variables to determine each edge's existence (as in most permutation-based methods), or employ sparse regression, as suggested in the original CAM paper. The authors need to justify the necessity of proposing a ""parent score,"" which seems over-complicated with average treatment effect estimation framework. Are there any advantages (e.g., in terms of time complexity or finite sample guarantee, as in the LISTEN paper)?

3. **Lack of technical novelty:** The technical contributions mainly combine ideas from SCORE and LISTEN, making the results and derivations (e.g., from constant variances to expected value of variances) straightforward extensions of previous work. While novelty is not a primary concern for me, it is worth noting as a minor weakness.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This work builds on TRANSPEECH (Huang et al., 2023) by applying the diffusion method to reduce noise, thereby normalizing speech units for further generation. The authors further use classifier-free guidance to enhance non-autoregressive generation. They conduct experiments on CVSS En-Fr and En-Es datasets, comparing their methods with the baseline. Both proposed methods show improvements.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work proposes a diffusion method to normalize target speech units, which outperforms previous approaches.

2. It explores classifier-free guidance to improve NAT generation for the speech-to-speech translation task.

Weaknesses:
1. The paper overstates its contributions. The authors claim to be the first to apply diffusion in the speech-to-speech translation task. However, they merely use it to generate auxiliary training targets and still follow the S2UT strategy (Lee et al., 2022b). This diminishes the novelty of the paper.
2. Although the work is based on TRANSPEECH, the authors compare only two of the three translation directions. One of the core contributions is applying the diffusion method to normalize speech units, yet the method shows minimal improvement (only 0.3 BLEU) compared to BiP (Bilateral Perturbation) in the En-Fr task. Given the method significantly outperforms others in the En-De task, this result is perplexing.
3. For a minor suggestion, in Tables 3 and 4, ensure consistent significant figures. Additionally, it would be more appropriate to place Table 3 before Table 4.

Limitations:
The authors appropriately state the limitations and broader impacts of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes DiffNorm, a diffusion-based self-supervised method for speech data normalization, aiming to alleviate multimodal problem in non-autoregressive speech-to-speech translation (NAT). DiffNorm consists of a VAE to reconstruct the speech feature and a diffusion model to add and remove noise of latent vector. Experiment shows that DiffNorm significantly improve NAT translation quality compared to baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. It is surprising to see pure data normalization improves NAT so much. ASR-BLEU improves 7 BLEU on En-Es direction with DiffNorm. 
2. This technique has wide applicability. All NAT S2S unit-based models can potentially benefit from it. Not to mention it requires no handcrafting rules for noise injection.
3. Ablation studies on noise level and training of DiffNorm further provide users a general guide on how to adapt DiffNorm to their own dataset and model.

Weaknesses:
1. The multi-modal problem has two aspects: semantic and acoustic. DiffNorm seems able to reduce acoustic modalities. Unclear if DiffNorm can also do that on semantic modalities, i.e., multiple feasible translations for the same source input.
2. Classifier-free guidance combined with DiffNorm leads to worse performance than DiffNorm alone in Figure 4 and the authors ignore it in the text. Elaboration is needed here.
3. Lack baseline comparison with several latest S2ST models like TransSentence [1], PolyVoice [2], SeamlessM4T and etc.

[1] TranSentence: speech-to-speech Translation via Language-Agnostic Sentence-Level Speech Encoding without Language-Parallel Data.
[2] PolyVoice: Language Models for Speech to Speech Translation
[3] SeamlessM4T—Massively Multilingual & Multimodal Machine Translation

Limitations:
Besides what I have already mentioned in the weakness, experiments in the paper are only conducted on En-X, but not reverse. Also, it would be interesting to see how DiffNorm works on non-European languages like Chinese and Japanese.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce a process aiming to simplify the target distribution of speech-to-speech translation. This process uses a VAE model to map features to a latent space, followed by a diffusion model to normalize the features in the latent space. The authors use the generated dataset to train a non-autoregressive CMLM model on the CVSS-C dataset to validate its effectiveness.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors propose a novel speech normalization method using Denoising Diffusion Probabilistic Models.
2. The quality gain in En-Es direction is impressive.

Weaknesses:
1.  I am confused by the rationale behind using a diffusion model to normalize the speech representation. The authors first add noise to the speech representation (Forward Process) and then remove the noise (Backward Process). This design seems awkward to me. Why do you think this process of adding and then removing noise can help with normalization?
2. Using a Variational Autoencoder to map features to a latent space seems contradictory to the motivation of reducing data multimodality. The VAE provides an indefinite mapping, which may hinder efforts to reduce multimodality. Additionally, $z_0 = f(h; \theta_{enc})$ is not correct; the VAE provides a distribution over the latent space. A more suitable expression would be $z_0 \sim p(z; f(h; \theta_{enc}))$.
3. The experiments are only conducted on synthesised dataset. As the paper's contribution is to alleviate the multi-modality problem in data, conducting experiments on a real speech-to-speech dataset, like [1], is much better to support the major claims.
4. The baseline results (Conformer in EnEs) seem quite low compared to the reported results in the literatre.

[1] Lee et al., Textless speech-to-speech translation on real data.

Limitations:
The authors have addressed the limitations in Appendix E.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This work builds on TRANSPEECH (Huang et al., 2023) by applying the diffusion method to reduce noise, thereby normalizing speech units for further generation. The authors further use classifier-free guidance to enhance non-autoregressive generation. They conduct experiments on CVSS En-Fr and En-Es datasets, comparing their methods with the baseline. Both proposed methods show improvements.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This work proposes a diffusion method to normalize target speech units, which outperforms previous approaches.

2. It explores classifier-free guidance to improve NAT generation for the speech-to-speech translation task.

Weaknesses:
1. The paper overstates its contributions. The authors claim to be the first to apply diffusion in the speech-to-speech translation task. However, they merely use it to generate auxiliary training targets and still follow the S2UT strategy (Lee et al., 2022b). This diminishes the novelty of the paper.
2. Although the work is based on TRANSPEECH, the authors compare only two of the three translation directions. One of the core contributions is applying the diffusion method to normalize speech units, yet the method shows minimal improvement (only 0.3 BLEU) compared to BiP (Bilateral Perturbation) in the En-Fr task. Given the method significantly outperforms others in the En-De task, this result is perplexing.
3. For a minor suggestion, in Tables 3 and 4, ensure consistent significant figures. Additionally, it would be more appropriate to place Table 3 before Table 4.

Limitations:
The authors appropriately state the limitations and broader impacts of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes DiffNorm, a diffusion-based self-supervised method for speech data normalization, aiming to alleviate multimodal problem in non-autoregressive speech-to-speech translation (NAT). DiffNorm consists of a VAE to reconstruct the speech feature and a diffusion model to add and remove noise of latent vector. Experiment shows that DiffNorm significantly improve NAT translation quality compared to baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. It is surprising to see pure data normalization improves NAT so much. ASR-BLEU improves 7 BLEU on En-Es direction with DiffNorm. 
2. This technique has wide applicability. All NAT S2S unit-based models can potentially benefit from it. Not to mention it requires no handcrafting rules for noise injection.
3. Ablation studies on noise level and training of DiffNorm further provide users a general guide on how to adapt DiffNorm to their own dataset and model.

Weaknesses:
1. The multi-modal problem has two aspects: semantic and acoustic. DiffNorm seems able to reduce acoustic modalities. Unclear if DiffNorm can also do that on semantic modalities, i.e., multiple feasible translations for the same source input.
2. Classifier-free guidance combined with DiffNorm leads to worse performance than DiffNorm alone in Figure 4 and the authors ignore it in the text. Elaboration is needed here.
3. Lack baseline comparison with several latest S2ST models like TransSentence [1], PolyVoice [2], SeamlessM4T and etc.

[1] TranSentence: speech-to-speech Translation via Language-Agnostic Sentence-Level Speech Encoding without Language-Parallel Data.
[2] PolyVoice: Language Models for Speech to Speech Translation
[3] SeamlessM4T—Massively Multilingual & Multimodal Machine Translation

Limitations:
Besides what I have already mentioned in the weakness, experiments in the paper are only conducted on En-X, but not reverse. Also, it would be interesting to see how DiffNorm works on non-European languages like Chinese and Japanese.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce a process aiming to simplify the target distribution of speech-to-speech translation. This process uses a VAE model to map features to a latent space, followed by a diffusion model to normalize the features in the latent space. The authors use the generated dataset to train a non-autoregressive CMLM model on the CVSS-C dataset to validate its effectiveness.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors propose a novel speech normalization method using Denoising Diffusion Probabilistic Models.
2. The quality gain in En-Es direction is impressive.

Weaknesses:
1.  I am confused by the rationale behind using a diffusion model to normalize the speech representation. The authors first add noise to the speech representation (Forward Process) and then remove the noise (Backward Process). This design seems awkward to me. Why do you think this process of adding and then removing noise can help with normalization?
2. Using a Variational Autoencoder to map features to a latent space seems contradictory to the motivation of reducing data multimodality. The VAE provides an indefinite mapping, which may hinder efforts to reduce multimodality. Additionally, $z_0 = f(h; \theta_{enc})$ is not correct; the VAE provides a distribution over the latent space. A more suitable expression would be $z_0 \sim p(z; f(h; \theta_{enc}))$.
3. The experiments are only conducted on synthesised dataset. As the paper's contribution is to alleviate the multi-modality problem in data, conducting experiments on a real speech-to-speech dataset, like [1], is much better to support the major claims.
4. The baseline results (Conformer in EnEs) seem quite low compared to the reported results in the literatre.

[1] Lee et al., Textless speech-to-speech translation on real data.

Limitations:
The authors have addressed the limitations in Appendix E.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper addresses the challenge of text-conditioned 3D dynamic human-object interaction (HOI) generation, which has lagged behind advancements in text-conditioned human motion generation due to the scarcity of large-scale interaction data and detailed annotations. The authors propose InterDreamer, a framework that decouples interaction semantics and dynamics to generate realistic HOI sequences without relying on text-interaction pair data. By leveraging pre-trained large models for high-level semantic control and introducing a world model to understand low-level interaction dynamics, InterDreamer surpasses existing motion capture data limitations. Applied to the BEHAVE and CHAIRS datasets, InterDreamer demonstrates the ability to produce coherent and text-aligned 3D HOI sequences, showcasing its effectiveness through comprehensive experimental analysis.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The authors introduce a novel task of synthesizing whole-body interactions with dynamic objects guided by textual commands, without relying on text-interaction pair data.
- The types of HOI interactions are limited compared to the diversity of movement in the human body itself. So, the idea that decomposes semantics and dynamics and then integrates them makes sense to me. The main challenge lies in effectively understanding the positions where the human object interacts. The authors propose leveraging the power of LLMs to simplify this challenge effectively.
- The technical details are sound, and the experimental results are good and demonstrate its zero-shot capability.
- The paper is well written, with clear figures and typography.

Weaknesses:
- This work doesn't seem to be able to handle more complex long interactions, such as a person walking to a chair and then sitting down, or a person lifting a box on the floor and carrying it for a while before putting it on the ground.

Limitations:
- The method still is unable to handle fine-grained human-object interactions (HOIs), such as dexterous manipulation using hands. Right now HOI generation is still a very challenging task, so I wouldn't consider this limitation to be a weakness of the manuscript, and expect that subsequent work will refine this point.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work aims at human-object interaction generation under less supervision. Since existing methods rely on large-scale interaction data, this paper attempts to propose a method that does not require paired data. The proposed method includes three stages, high-level planning by LLMs, low-level control by existing text-to-motion methods, and interaction retrieval, a world model based on existing human-object interaction generation methods. The proposed method is evaluated on the public datasets, BEHAVE and CHAIRS datasets.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The issue focused on in this paper is a worthy research topic. If the training does not require pair-wise data, it will greatly reduce the dependence of the human-object interaction generation on the training data.
2. Given texts, the figures intuitively demonstrate visual representations of the generated human-object motions.

Weaknesses:
1. About novelty. While the issue focused on in this paper is valuable, the proposed method is a combination of existing methods. I appreciate the technical effort of the authors, but please highlight how each stage differs from existing approaches.
2. Missing some details. 
--- In Line 165, how is the database built? If the data used to build the dataset includes the training set and the test set, it is not standard.
---Does the low-level control need to be trained? As mentioned in Line 269, the text-to-motion model is the existing method which is pre-trained on HumanML3D. If no training is required, how does the model generalize on action types? 
--- As mentioned in Line 180, the authors claimed “this model trained on the 3D HOI dataset”, while the main work is that the proposed method does not require training on paired HOI data. It is inconsistent and confusing.
--- What is the size of the object’ signed distance fields? How many vertices is the object represented by? What network to use to encode and decode object shapes?
3. The motivation for the quantitative experiment (section 4.2) is unclear. Why compare different control conditions? 
4. In Line 316, what is the vertex-based control? Does it mean human vertex controls in Line 210? The definition should be given.
5. In the caption of Figure 4, it is claimed that the proposed method can handle complex and long sequences. Is there a strategy in the proposed method for complex and long sequences? What is the longest?

Limitations:
The authors have discussed the limitation of their method.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
InetrDreamer is a framework for synthesizing Human-Object Interactions (HOI) from textual queries. The key feature of InterDreamer is the ability to train without paired text and HOI motion data. To achieve this the work employs a multi-stage pipeline with LLM operating as a high-level planner that defines the parameters to infer the starting object pose and human motion sequences, which are brought together with the help of optimization on the next stage. The method is evaluated on the BEHAVE dataset which is additionally labeled with text as part of this work.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1) The key feature of InterDreamer is undoubtedly its biggest strength: the lack of requirement for paired data text-to-HOI data. This is a promising feature that in theory allows scaling the HOI modeling without tedious data-labeling by leveraging the advancements in LLMs.
2) Another notable aspect is that the proposed planning can be adopted on top of other existing motion models improving their performance (as demonstrated in Table 2).

Weaknesses:
1) One of the key features of the proposed framework is a High-level Planning module that queries the LLM to extract necessary features for downstream modules. However the presented description of the query protocol is scarce, similar works (e.g. SINC by Athanasiou, Petrovich, et al., ICCV'23) also employ LLM within the framework and provide the full template of the query in the supplementary material (Section B) to ensure the reproducibility.

2) Contribution formulation in the Introduction (Lines 71-72): the considered task is text-to-HOI modeling, while training without paired data is rather a feature of the method than a task itself. Further in the text, in the Conclusions section, the work claims to introduce the novel task of 3D HOI generation from text (Line 319). Both formulations do not reflect the actual contribution of the work.

3) Evaluation is performed only on the self-labeled BEHAVE dataset, however, there exists at least one more dataset with text annotations: OMOMO \[51] which is not used for evaluation.

Limitations:
The work presents a substantial discussion of limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work aims to address the text-conditioned human-object interaction (HOI) motion generation task. Unlike previous HOI generation approaches that rely on limited existing HOI datasets with text annotations for supervised learning, this work proposes a framework that decouples interaction semantics learning from interaction dynamics learning. This decoupling eliminates the need for large-scale HOI datasets with text annotations for training. The interaction semantics learning leverages an existing pretrained text-conditioned human motion generation model, while the interaction dynamics are optimized using a learned world model that predicts object states induced by human motions. The proposed framework can generate realistic HOI motions that align with the input text conditions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* HOI modeling remains a challenging and understudied task compared to recent developments in human motion modeling. One major bottleneck is the lack of large-scale, high-quality interaction data. This paper offers an insightful solution to overcome the scarcity of large-scale interaction datasets.
* Several key designs in this framework are reasonable, including (1) LLM-based high-level planning to reduce the distribution gap between input text instructions and the language used for text-to-motion model training dataset; (2) the world model trained with interaction dataset for human pose and object pose optimisation.
* Comprehensive quantitative experiments results show the effectiveness of the proposed modules.

Weaknesses:
* Without the object geometry information encoded as input, how does the world model perform in generalizing to different objects? How does it perform when the object is out of the predefined list?
 * Efficiency of the autoregressive optimisation: as presented in figure 2, the optimisation of action and state is performed per step, which might result in inefficient inference, while the world model are trained to predict longer-horizon states. Could the author elaborate more on this design choice?
 * Regarding the world model training with N vertices sampled from contact area, what if the human pose is not in contact with object, for example, for the text instruction “the person throw away the ball“ where in most of frames the ball is flying in the air without contacting with the human, how does the object state forecasted and optimised with the world model.
* From the visualisation results, there are no significant improvements over baselines HOI-Diff and CG-HOI in terms of the realism of human-object interaction. Is this mainly result from the not-perfect world model training and optimisation or the lack of hand pose? Could the author give some insights on the major challenges and bottlenecks presented in the current HOI understanding?

Limitations:
Yes, the authors have addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper addresses the challenge of text-conditioned 3D dynamic human-object interaction (HOI) generation, which has lagged behind advancements in text-conditioned human motion generation due to the scarcity of large-scale interaction data and detailed annotations. The authors propose InterDreamer, a framework that decouples interaction semantics and dynamics to generate realistic HOI sequences without relying on text-interaction pair data. By leveraging pre-trained large models for high-level semantic control and introducing a world model to understand low-level interaction dynamics, InterDreamer surpasses existing motion capture data limitations. Applied to the BEHAVE and CHAIRS datasets, InterDreamer demonstrates the ability to produce coherent and text-aligned 3D HOI sequences, showcasing its effectiveness through comprehensive experimental analysis.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
- The authors introduce a novel task of synthesizing whole-body interactions with dynamic objects guided by textual commands, without relying on text-interaction pair data.
- The types of HOI interactions are limited compared to the diversity of movement in the human body itself. So, the idea that decomposes semantics and dynamics and then integrates them makes sense to me. The main challenge lies in effectively understanding the positions where the human object interacts. The authors propose leveraging the power of LLMs to simplify this challenge effectively.
- The technical details are sound, and the experimental results are good and demonstrate its zero-shot capability.
- The paper is well written, with clear figures and typography.

Weaknesses:
- This work doesn't seem to be able to handle more complex long interactions, such as a person walking to a chair and then sitting down, or a person lifting a box on the floor and carrying it for a while before putting it on the ground.

Limitations:
- The method still is unable to handle fine-grained human-object interactions (HOIs), such as dexterous manipulation using hands. Right now HOI generation is still a very challenging task, so I wouldn't consider this limitation to be a weakness of the manuscript, and expect that subsequent work will refine this point.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work aims at human-object interaction generation under less supervision. Since existing methods rely on large-scale interaction data, this paper attempts to propose a method that does not require paired data. The proposed method includes three stages, high-level planning by LLMs, low-level control by existing text-to-motion methods, and interaction retrieval, a world model based on existing human-object interaction generation methods. The proposed method is evaluated on the public datasets, BEHAVE and CHAIRS datasets.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
2: fair

Strengths:
1. The issue focused on in this paper is a worthy research topic. If the training does not require pair-wise data, it will greatly reduce the dependence of the human-object interaction generation on the training data.
2. Given texts, the figures intuitively demonstrate visual representations of the generated human-object motions.

Weaknesses:
1. About novelty. While the issue focused on in this paper is valuable, the proposed method is a combination of existing methods. I appreciate the technical effort of the authors, but please highlight how each stage differs from existing approaches.
2. Missing some details. 
--- In Line 165, how is the database built? If the data used to build the dataset includes the training set and the test set, it is not standard.
---Does the low-level control need to be trained? As mentioned in Line 269, the text-to-motion model is the existing method which is pre-trained on HumanML3D. If no training is required, how does the model generalize on action types? 
--- As mentioned in Line 180, the authors claimed “this model trained on the 3D HOI dataset”, while the main work is that the proposed method does not require training on paired HOI data. It is inconsistent and confusing.
--- What is the size of the object’ signed distance fields? How many vertices is the object represented by? What network to use to encode and decode object shapes?
3. The motivation for the quantitative experiment (section 4.2) is unclear. Why compare different control conditions? 
4. In Line 316, what is the vertex-based control? Does it mean human vertex controls in Line 210? The definition should be given.
5. In the caption of Figure 4, it is claimed that the proposed method can handle complex and long sequences. Is there a strategy in the proposed method for complex and long sequences? What is the longest?

Limitations:
The authors have discussed the limitation of their method.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
InetrDreamer is a framework for synthesizing Human-Object Interactions (HOI) from textual queries. The key feature of InterDreamer is the ability to train without paired text and HOI motion data. To achieve this the work employs a multi-stage pipeline with LLM operating as a high-level planner that defines the parameters to infer the starting object pose and human motion sequences, which are brought together with the help of optimization on the next stage. The method is evaluated on the BEHAVE dataset which is additionally labeled with text as part of this work.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1) The key feature of InterDreamer is undoubtedly its biggest strength: the lack of requirement for paired data text-to-HOI data. This is a promising feature that in theory allows scaling the HOI modeling without tedious data-labeling by leveraging the advancements in LLMs.
2) Another notable aspect is that the proposed planning can be adopted on top of other existing motion models improving their performance (as demonstrated in Table 2).

Weaknesses:
1) One of the key features of the proposed framework is a High-level Planning module that queries the LLM to extract necessary features for downstream modules. However the presented description of the query protocol is scarce, similar works (e.g. SINC by Athanasiou, Petrovich, et al., ICCV'23) also employ LLM within the framework and provide the full template of the query in the supplementary material (Section B) to ensure the reproducibility.

2) Contribution formulation in the Introduction (Lines 71-72): the considered task is text-to-HOI modeling, while training without paired data is rather a feature of the method than a task itself. Further in the text, in the Conclusions section, the work claims to introduce the novel task of 3D HOI generation from text (Line 319). Both formulations do not reflect the actual contribution of the work.

3) Evaluation is performed only on the self-labeled BEHAVE dataset, however, there exists at least one more dataset with text annotations: OMOMO \[51] which is not used for evaluation.

Limitations:
The work presents a substantial discussion of limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work aims to address the text-conditioned human-object interaction (HOI) motion generation task. Unlike previous HOI generation approaches that rely on limited existing HOI datasets with text annotations for supervised learning, this work proposes a framework that decouples interaction semantics learning from interaction dynamics learning. This decoupling eliminates the need for large-scale HOI datasets with text annotations for training. The interaction semantics learning leverages an existing pretrained text-conditioned human motion generation model, while the interaction dynamics are optimized using a learned world model that predicts object states induced by human motions. The proposed framework can generate realistic HOI motions that align with the input text conditions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* HOI modeling remains a challenging and understudied task compared to recent developments in human motion modeling. One major bottleneck is the lack of large-scale, high-quality interaction data. This paper offers an insightful solution to overcome the scarcity of large-scale interaction datasets.
* Several key designs in this framework are reasonable, including (1) LLM-based high-level planning to reduce the distribution gap between input text instructions and the language used for text-to-motion model training dataset; (2) the world model trained with interaction dataset for human pose and object pose optimisation.
* Comprehensive quantitative experiments results show the effectiveness of the proposed modules.

Weaknesses:
* Without the object geometry information encoded as input, how does the world model perform in generalizing to different objects? How does it perform when the object is out of the predefined list?
 * Efficiency of the autoregressive optimisation: as presented in figure 2, the optimisation of action and state is performed per step, which might result in inefficient inference, while the world model are trained to predict longer-horizon states. Could the author elaborate more on this design choice?
 * Regarding the world model training with N vertices sampled from contact area, what if the human pose is not in contact with object, for example, for the text instruction “the person throw away the ball“ where in most of frames the ball is flying in the air without contacting with the human, how does the object state forecasted and optimised with the world model.
* From the visualisation results, there are no significant improvements over baselines HOI-Diff and CG-HOI in terms of the realism of human-object interaction. Is this mainly result from the not-perfect world model training and optimisation or the lack of hand pose? Could the author give some insights on the major challenges and bottlenecks presented in the current HOI understanding?

Limitations:
Yes, the authors have addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposes two novel classes of MDPs in the function approximation setting:
* Mildly Smooth MDPs: in which the bellman optimality operator outputs smooth functions of degree $\nu$.
* Locally Linearizable MDPs: in which there exist a state-action feature mapping into $\mathbb R^d$ and a finite partition of the state-action space that give rise to a Q function approximator class, which is linear in the feature mapping inside each of the partitions (the vector defining the linear mapping is allowed to change between partitions).

An online algorithm (CINDERELLA) is presented and a proof of sublinear regret is given for the class of Locally Linearizable MDPs. Subsequently, it is shown that for any Mildly Smooth MDP, there exist a feature mapping and a partition (of size exponential in $d$) that constitute a Locally Linearizable MDP. 
As a result, a sublinear regret bound of CINDERELLA is derived for the class of Mildly Smooth MDPs: 
$\tilde O(H \nu_*^d K^{\frac{\nu + 2d}{2\nu + 2d}}$  $ + H^{\frac{2\nu + 2d}{\nu}})$, 

where $\nu_* := \lceil \nu - 1 \rceil$.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* This work proposes a new class of continuous state-action MDPs in which sublinear regret is achievable
* In particular, this new class is the currently the most general one that does not suffer from lower bounds exponential in $H$

Weaknesses:
* The regret bound has factors that are exponential in $d$, and sublinearity in $K$ is weak (as to be expected in this level of generality).
* Similar to other algorithms in this line of work, the CINDERELLA is not computationally efficient and far from being a practical algorithm.
* I am not sure what we really learn from such a work; the technical details are complicated even at the level of exposition, and the results are very weak (as to be expected in this level of generality).

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a notion of local linearization, which is then applied to smooth MDPs. It generalizes the ""Eleanor"" algorithm into ""Cinderella,"" which, by avoiding a ""cardinality of N"" term on the suboptimality with respect to the inherent Bellman error, gets sublinear regret for all classes of smooth problems.

EDIT: I appreciate the authors' detailed feedback. My score remains unchanged.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper is written very clearly, exposes all the strengths and weaknesses of its contributions, and makes connections to prior work in a thorough and honest fashion. The paper explain, in particular, why Cinderella is inadequate, and how the parameter learning is decoupled in a way to permit efficient learning. This requires additional technical subtlety in the analysis, but with it comes a bound which would be intractable otherwise.

Weaknesses:
This paper is well executed and technically solid, but I believe its main weakness is that the results are not particular surprising to an expert in the area. It seems clear that one can decompose ""smooth"" MDPs into local linear ones (this has been done, e.g. with zooming bandits and zooming MDPs), and is rather unsurprising that a careful way of handling the analysis makes these objects learnable. I don't seem to see any particular new techniques or insights, and so it is hard for me to be incredibly excited about the result. Still, the paper is executed commendably and for that I lean towards acceptance. 

There are also some limitations with presentation. Chief among them, the authors should do more to emphasize the algorithmic differences between Eleanor and Cinderalla (as they did so well with Eleanor's limitations for regret). The extent of the discussion seems limited to the sentence ""Difference with ELEANOR stays in the fact that parameters relative to different regions are learned separately"", which (a) bears elaboration, (b) is somewhat hidden in the mass of text, and (c) is not grammatically correct. I would encourate the authors to explain what techniques and ideas are needed to update the algorithm and analysis for the resultant guarantees.

Limitations:
Novelty, somewhat incremental. Moreover, this paper has the computational inefficiency challenges associated with many algorithms in the field, and, like any non-parametric style method, solves exp(dimension, horizon). Lastly, there do not seem to be any lower bounds that characterize the correct exponents.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces the concept of Locally Linearizable MDPs, a class of MDPs that generalizes existing ones like Linear MDPs and MDPs with low inherent Bellman error. In this model, the state-action space is partitioned into $N$ regions, where the Q-functions belong to a class that allows the result of the Bellman optimality operator to be well approximated by a linear function within each region, up to some ""Inherent Bellman Error."" The authors propose CINDERELLA, a no-regret (computationally inefficient) algorithm designed for this class, achieving regret bound of $Nd\sqrt{K} + \sqrt{d}\mathcal{I}K$, where $N$ is the number of partitions, $d$ is the dimension of the state-action feature space, $K$ is the number of episodes and $\mathcal{I}$ is the Inherent Bellmann Error. 

Next, they present the class of ""Mildly Smooth MDPs"". This class is similar yet slightly less general than ""Weakly Smooth MDPs,"" which assume that the output of the Bellman operator on a smooth function remains smooth. Still, Mildly Smooth MDPs are more general than ""Strongly Smooth MDPs"" and thus positioned somewhere between the two. The authors show that Mildly Smooth MDPs are Locally Linearizable for some partition and Inherent Bellmann Error and by that achieve regret of $H v^d K^{\frac{\nu+2d}{2\nu+2d}} + H^{\frac{2\nu + 2d}{\nu}}$ where $\nu$ is the smoothness parameter of the class.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-crafted and clearly positions itself within the existing literature. It employs a well-structured pedagogical approach. It first presents a straightforward solution for Locally Linearized MDPs and proceeds to point out that this naive approach's regret bounds are insufficient for deducing a non-trivial regret bound for Mildly Smooth MDPs. This leads to the introduction of their algorithm, which offers improved regret bounds and leads to their main result.
    
The authors conduct a comprehensive analysis of the regret bounds, discussing each component to assess its necessity and provide a comparison with prior work. For the most part, the paper is transparent and addresses its limitations.

Weaknesses:
The nature and definition of Mildly Smooth MDPs are not very natural, and it is hard to agree that learning becomes substantially more feasible compared to Weakly smooth MDPs. Specifically, the authors mention that in Weakly Smooth MDPs the regret bound must scale exponentially with $H$, which they consider ""statistically unfeasible"". However the proposed approach results in a regret bound that scales exponentially with $d$, the dimension of the feature space. This raises similar concerns about the practical feasibility of the setting. The authors claim that for $\nu \to \infty$ the regret becomes of order $\sqrt{K}$, but this is not clearly derived from the provided regret bounds, leaving it unclear whether the result generalizes existing results in Strongly Smooth MDPs.

Limitations:
For the most part, the paper addresses its limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper discusses the concept of local-linear MDPs which is a general representation class of MDPs that extends previous works on learnable (sublinear regret in $K$) and feasible (polynomial regret in $H$) episodic MDPs.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper considers important complexity questions associated with the continuous episodic MDP, and is very well-written. Compared to the much more well-studied tabular episodic case, the complexity of continuous episodic MDPs is much more challenging to characterize. The authors identified a class of mildly smooth MDPs and show one can achieve no-regret learning in this regime. The regret bound still has an exponential dependence in $H$ but the authors demonstrated through a continuous bandit special case this exponential dependence is unavoidable. The class of mildly smooth MDPs is larger than the previously studied classes such as kernelized MDPs and strongly smooth MDPs where no-regret learning is possible.

Weaknesses:
Even though the theoretical development is sound and complete, I think the authors could do better in further highlighting their contributions which I will use the questions below to elaborate.

Limitations:
There is no obvious limitation in this paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studied a structural property called local linearity that makes continuous MDPs learnable. In particular, local linearity means that the continuous state-action space can be partitioned into multiple regions and in each region, the Q-function is linear w.r.t. a unique (different) parameter. The paper first proposed an algorithm to learn MDPs with local linearity and proved that the regret is sublinear. Then, the paper showed that mildly smooth MDPs, where the Bellman operator is smooth, are locally linearizable. So that it demonstrated the broadness of local linearity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The studied problem is important.
2. The new concept of local linearity is interesting and indeed capture a broad class of MDPs according the paper.

Weaknesses:
1. The proposed algorithm is computationally inefficient. It needs to run over all regions while the number of regions might be exponentially large according to section 4.

Limitations:
No further limitations need to be addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposes two novel classes of MDPs in the function approximation setting:
* Mildly Smooth MDPs: in which the bellman optimality operator outputs smooth functions of degree $\nu$.
* Locally Linearizable MDPs: in which there exist a state-action feature mapping into $\mathbb R^d$ and a finite partition of the state-action space that give rise to a Q function approximator class, which is linear in the feature mapping inside each of the partitions (the vector defining the linear mapping is allowed to change between partitions).

An online algorithm (CINDERELLA) is presented and a proof of sublinear regret is given for the class of Locally Linearizable MDPs. Subsequently, it is shown that for any Mildly Smooth MDP, there exist a feature mapping and a partition (of size exponential in $d$) that constitute a Locally Linearizable MDP. 
As a result, a sublinear regret bound of CINDERELLA is derived for the class of Mildly Smooth MDPs: 
$\tilde O(H \nu_*^d K^{\frac{\nu + 2d}{2\nu + 2d}}$  $ + H^{\frac{2\nu + 2d}{\nu}})$, 

where $\nu_* := \lceil \nu - 1 \rceil$.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
* This work proposes a new class of continuous state-action MDPs in which sublinear regret is achievable
* In particular, this new class is the currently the most general one that does not suffer from lower bounds exponential in $H$

Weaknesses:
* The regret bound has factors that are exponential in $d$, and sublinearity in $K$ is weak (as to be expected in this level of generality).
* Similar to other algorithms in this line of work, the CINDERELLA is not computationally efficient and far from being a practical algorithm.
* I am not sure what we really learn from such a work; the technical details are complicated even at the level of exposition, and the results are very weak (as to be expected in this level of generality).

Limitations:
NA

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces a notion of local linearization, which is then applied to smooth MDPs. It generalizes the ""Eleanor"" algorithm into ""Cinderella,"" which, by avoiding a ""cardinality of N"" term on the suboptimality with respect to the inherent Bellman error, gets sublinear regret for all classes of smooth problems.

EDIT: I appreciate the authors' detailed feedback. My score remains unchanged.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
This paper is written very clearly, exposes all the strengths and weaknesses of its contributions, and makes connections to prior work in a thorough and honest fashion. The paper explain, in particular, why Cinderella is inadequate, and how the parameter learning is decoupled in a way to permit efficient learning. This requires additional technical subtlety in the analysis, but with it comes a bound which would be intractable otherwise.

Weaknesses:
This paper is well executed and technically solid, but I believe its main weakness is that the results are not particular surprising to an expert in the area. It seems clear that one can decompose ""smooth"" MDPs into local linear ones (this has been done, e.g. with zooming bandits and zooming MDPs), and is rather unsurprising that a careful way of handling the analysis makes these objects learnable. I don't seem to see any particular new techniques or insights, and so it is hard for me to be incredibly excited about the result. Still, the paper is executed commendably and for that I lean towards acceptance. 

There are also some limitations with presentation. Chief among them, the authors should do more to emphasize the algorithmic differences between Eleanor and Cinderalla (as they did so well with Eleanor's limitations for regret). The extent of the discussion seems limited to the sentence ""Difference with ELEANOR stays in the fact that parameters relative to different regions are learned separately"", which (a) bears elaboration, (b) is somewhat hidden in the mass of text, and (c) is not grammatically correct. I would encourate the authors to explain what techniques and ideas are needed to update the algorithm and analysis for the resultant guarantees.

Limitations:
Novelty, somewhat incremental. Moreover, this paper has the computational inefficiency challenges associated with many algorithms in the field, and, like any non-parametric style method, solves exp(dimension, horizon). Lastly, there do not seem to be any lower bounds that characterize the correct exponents.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces the concept of Locally Linearizable MDPs, a class of MDPs that generalizes existing ones like Linear MDPs and MDPs with low inherent Bellman error. In this model, the state-action space is partitioned into $N$ regions, where the Q-functions belong to a class that allows the result of the Bellman optimality operator to be well approximated by a linear function within each region, up to some ""Inherent Bellman Error."" The authors propose CINDERELLA, a no-regret (computationally inefficient) algorithm designed for this class, achieving regret bound of $Nd\sqrt{K} + \sqrt{d}\mathcal{I}K$, where $N$ is the number of partitions, $d$ is the dimension of the state-action feature space, $K$ is the number of episodes and $\mathcal{I}$ is the Inherent Bellmann Error. 

Next, they present the class of ""Mildly Smooth MDPs"". This class is similar yet slightly less general than ""Weakly Smooth MDPs,"" which assume that the output of the Bellman operator on a smooth function remains smooth. Still, Mildly Smooth MDPs are more general than ""Strongly Smooth MDPs"" and thus positioned somewhere between the two. The authors show that Mildly Smooth MDPs are Locally Linearizable for some partition and Inherent Bellmann Error and by that achieve regret of $H v^d K^{\frac{\nu+2d}{2\nu+2d}} + H^{\frac{2\nu + 2d}{\nu}}$ where $\nu$ is the smoothness parameter of the class.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is well-crafted and clearly positions itself within the existing literature. It employs a well-structured pedagogical approach. It first presents a straightforward solution for Locally Linearized MDPs and proceeds to point out that this naive approach's regret bounds are insufficient for deducing a non-trivial regret bound for Mildly Smooth MDPs. This leads to the introduction of their algorithm, which offers improved regret bounds and leads to their main result.
    
The authors conduct a comprehensive analysis of the regret bounds, discussing each component to assess its necessity and provide a comparison with prior work. For the most part, the paper is transparent and addresses its limitations.

Weaknesses:
The nature and definition of Mildly Smooth MDPs are not very natural, and it is hard to agree that learning becomes substantially more feasible compared to Weakly smooth MDPs. Specifically, the authors mention that in Weakly Smooth MDPs the regret bound must scale exponentially with $H$, which they consider ""statistically unfeasible"". However the proposed approach results in a regret bound that scales exponentially with $d$, the dimension of the feature space. This raises similar concerns about the practical feasibility of the setting. The authors claim that for $\nu \to \infty$ the regret becomes of order $\sqrt{K}$, but this is not clearly derived from the provided regret bounds, leaving it unclear whether the result generalizes existing results in Strongly Smooth MDPs.

Limitations:
For the most part, the paper addresses its limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper discusses the concept of local-linear MDPs which is a general representation class of MDPs that extends previous works on learnable (sublinear regret in $K$) and feasible (polynomial regret in $H$) episodic MDPs.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
The paper considers important complexity questions associated with the continuous episodic MDP, and is very well-written. Compared to the much more well-studied tabular episodic case, the complexity of continuous episodic MDPs is much more challenging to characterize. The authors identified a class of mildly smooth MDPs and show one can achieve no-regret learning in this regime. The regret bound still has an exponential dependence in $H$ but the authors demonstrated through a continuous bandit special case this exponential dependence is unavoidable. The class of mildly smooth MDPs is larger than the previously studied classes such as kernelized MDPs and strongly smooth MDPs where no-regret learning is possible.

Weaknesses:
Even though the theoretical development is sound and complete, I think the authors could do better in further highlighting their contributions which I will use the questions below to elaborate.

Limitations:
There is no obvious limitation in this paper.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studied a structural property called local linearity that makes continuous MDPs learnable. In particular, local linearity means that the continuous state-action space can be partitioned into multiple regions and in each region, the Q-function is linear w.r.t. a unique (different) parameter. The paper first proposed an algorithm to learn MDPs with local linearity and proved that the regret is sublinear. Then, the paper showed that mildly smooth MDPs, where the Bellman operator is smooth, are locally linearizable. So that it demonstrated the broadness of local linearity.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The studied problem is important.
2. The new concept of local linearity is interesting and indeed capture a broad class of MDPs according the paper.

Weaknesses:
1. The proposed algorithm is computationally inefficient. It needs to run over all regions while the number of regions might be exponentially large according to section 4.

Limitations:
No further limitations need to be addressed.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposed a flexible framework that can effectively handle human-object interaction (HOI) detection, scene graph generation (SGG), and referring expression comprehension (REC) tasks. The proposed method further addressed the problem of promptable visual relationship segmentation and enabled the capability for open-vocabulary segmentation. Its main idea is to leverage the pretrained vision-language models for grounding textual features to visual relationship inside images. Experimental results show that the proposed framework is able to achieve SOTA performance on standard, promptable and open-vocabulary visual relationship detection/segmentation tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is overall well-written and easy to follow.

2. The problem addressed by this paper is novel to me. Unifying various visual relationship detection/segmentation tasks with in a single framework has been seldomly studied in previous works, which might raise new research interests in this field.

3. The overall performance of the method is competitive. It is able to achieve SOTA results on most VRD tasks and outperform previous methods notably.

Weaknesses:
1. According to Table 7, the major performance improvement of the proposed method comes from the supervision of the mask head, which is provided from SAM and never used by previous works. When only the box head is used, the results of the proposed method is behind the current SOTA methods notably on HICO-DET and VCOCO. This makes the benefit of the proposed unified framework questionable if such large performance drops are observed.

2. Apart from introducing the segmentation head, the major technical contribution of the proposed method is unclear to me. The model architecture and losses are very similar to those previously proposed dual-decoder architectures in GEN-VLKT, RLIP, etc., and this part needs further clarification.

3. In Table 7, from the last row, we can see that the benefit brought by adding PSG is minor. Hence, the results on PSG dataset should also be reported so that the readers could have a better understanding on how unifying the framework could benefit each individual task.

Limitations:
The authors have widely discussed the limitation of the proposed work and also its challenges in benchmarking.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents a novel approach for visual relationship segmentation that integrates the three critical aspects of a flexible VRS model: standard VRS, promptable querying, and open-vocabulary capabilities. By harnessing the synergistic potential of textual and visual features, the proposed model delivers promising experimental results on existing benchmark datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) The authors introduce a flexible framework capable of segmenting both human-centric and generic visual relationships across various datasets. 
2) The authors present a promptable visual relationship learning framework that effectively utilizes diverse textual prompts to ground relationships. 
3) The proposed method shows competitive performance in both standard close-set and open-vocabulary scenarios, showcasing the model’s strong generalization capabilities.

Weaknesses:
1. The experimental comparison suffers from potential unfairness, contrasting the proposed method with others using Focal-L backbone against those employing ResNet backbone seems somewhat unreasonable.
2. Since this paper includes referring expression comprehension (REC) tasks in abstract, it is unreasonable not to report experimental results on corresponding benchmarks like RefCOCO, RefCOCO+, and RefCOCOg.
3. While the research perspective of this paper is reasonable and innovative, the overall architecture design of Flex-VRS lacks novelty compared to previous related works.
4. More visualizations are needed for demonstrating the produced fine-grained masks generated by converting existing bounding box annotations from HOI detection datasets.

Limitations:
The authors have thoroughly discussed the limitations of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes an approach for visual relationship segmentation that integrates the three aspects of a VRS model: standard VRS, promptable querying, and open-vocabulary capabilities. The idea of the article is very good, but the performance seems to be lacking.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Enhancing HOI from the perspective of object segmentation is an interesting and promising idea.

Weaknesses:
1-Previous work mostly uses ResNet series backbones, while the authors use a Focal-L Backbone. How do the FLOPs and parameter counts of this backbone compare to ResNet? I am concerned there may be an unfair comparison.

2-The performance of the proposed method does not seem to be particularly superior, which is a significant drawback. For example, in Tables 2 and 3, UniVRD, PViC, and RLIPv2 significantly outperform the proposed method in terms of box mAP metrics. Can the authors analyze this situation?

3-In line 208, the authors mention using SAM to generate masks. How do they handle the noise in these masks? Some masks might significantly deviate from the ground truth.

4-There are some typos, such as in line 25: ""textttperson"".

Limitations:
see weakness

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper propose a model to handle multiple visual relationship tasks, like HOI detection and Scene Graph Generation.

The proposed model is based on vision-language models similar to CLIP.

It handles different formulations like standard close-set, open-vocabulary, and prompted setting.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Unified model for HOI and SGG.
- Multiple inference setting supported, like open-vocabulary, standard close-set, and prompted.
- The writing is smooth, and the demonstration is generally OK.

Weaknesses:
- Why not detection, rather than segmentation? What's the necessity for segmentation-style output rather than traditional detection style tasks? I would like more clarification on this.
- Also, (if not using the segmentation setting,) this method could be compared with more methods. Currently, the compared methods seems still not sufficient.
- This general contribution and pipeline is similar to UniVRD, except (1) this model is mask-based rather than box-based, and (2) a prompted setting is further supported. Thus, the contribution seems incremental.
- The performance is basically the same as UniVRD and is not as good as RLIPv2.

Limitations:
Limitations are discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposed a flexible framework that can effectively handle human-object interaction (HOI) detection, scene graph generation (SGG), and referring expression comprehension (REC) tasks. The proposed method further addressed the problem of promptable visual relationship segmentation and enabled the capability for open-vocabulary segmentation. Its main idea is to leverage the pretrained vision-language models for grounding textual features to visual relationship inside images. Experimental results show that the proposed framework is able to achieve SOTA performance on standard, promptable and open-vocabulary visual relationship detection/segmentation tasks.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is overall well-written and easy to follow.

2. The problem addressed by this paper is novel to me. Unifying various visual relationship detection/segmentation tasks with in a single framework has been seldomly studied in previous works, which might raise new research interests in this field.

3. The overall performance of the method is competitive. It is able to achieve SOTA results on most VRD tasks and outperform previous methods notably.

Weaknesses:
1. According to Table 7, the major performance improvement of the proposed method comes from the supervision of the mask head, which is provided from SAM and never used by previous works. When only the box head is used, the results of the proposed method is behind the current SOTA methods notably on HICO-DET and VCOCO. This makes the benefit of the proposed unified framework questionable if such large performance drops are observed.

2. Apart from introducing the segmentation head, the major technical contribution of the proposed method is unclear to me. The model architecture and losses are very similar to those previously proposed dual-decoder architectures in GEN-VLKT, RLIP, etc., and this part needs further clarification.

3. In Table 7, from the last row, we can see that the benefit brought by adding PSG is minor. Hence, the results on PSG dataset should also be reported so that the readers could have a better understanding on how unifying the framework could benefit each individual task.

Limitations:
The authors have widely discussed the limitation of the proposed work and also its challenges in benchmarking.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents a novel approach for visual relationship segmentation that integrates the three critical aspects of a flexible VRS model: standard VRS, promptable querying, and open-vocabulary capabilities. By harnessing the synergistic potential of textual and visual features, the proposed model delivers promising experimental results on existing benchmark datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) The authors introduce a flexible framework capable of segmenting both human-centric and generic visual relationships across various datasets. 
2) The authors present a promptable visual relationship learning framework that effectively utilizes diverse textual prompts to ground relationships. 
3) The proposed method shows competitive performance in both standard close-set and open-vocabulary scenarios, showcasing the model’s strong generalization capabilities.

Weaknesses:
1. The experimental comparison suffers from potential unfairness, contrasting the proposed method with others using Focal-L backbone against those employing ResNet backbone seems somewhat unreasonable.
2. Since this paper includes referring expression comprehension (REC) tasks in abstract, it is unreasonable not to report experimental results on corresponding benchmarks like RefCOCO, RefCOCO+, and RefCOCOg.
3. While the research perspective of this paper is reasonable and innovative, the overall architecture design of Flex-VRS lacks novelty compared to previous related works.
4. More visualizations are needed for demonstrating the produced fine-grained masks generated by converting existing bounding box annotations from HOI detection datasets.

Limitations:
The authors have thoroughly discussed the limitations of this work.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes an approach for visual relationship segmentation that integrates the three aspects of a VRS model: standard VRS, promptable querying, and open-vocabulary capabilities. The idea of the article is very good, but the performance seems to be lacking.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Enhancing HOI from the perspective of object segmentation is an interesting and promising idea.

Weaknesses:
1-Previous work mostly uses ResNet series backbones, while the authors use a Focal-L Backbone. How do the FLOPs and parameter counts of this backbone compare to ResNet? I am concerned there may be an unfair comparison.

2-The performance of the proposed method does not seem to be particularly superior, which is a significant drawback. For example, in Tables 2 and 3, UniVRD, PViC, and RLIPv2 significantly outperform the proposed method in terms of box mAP metrics. Can the authors analyze this situation?

3-In line 208, the authors mention using SAM to generate masks. How do they handle the noise in these masks? Some masks might significantly deviate from the ground truth.

4-There are some typos, such as in line 25: ""textttperson"".

Limitations:
see weakness

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper propose a model to handle multiple visual relationship tasks, like HOI detection and Scene Graph Generation.

The proposed model is based on vision-language models similar to CLIP.

It handles different formulations like standard close-set, open-vocabulary, and prompted setting.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Unified model for HOI and SGG.
- Multiple inference setting supported, like open-vocabulary, standard close-set, and prompted.
- The writing is smooth, and the demonstration is generally OK.

Weaknesses:
- Why not detection, rather than segmentation? What's the necessity for segmentation-style output rather than traditional detection style tasks? I would like more clarification on this.
- Also, (if not using the segmentation setting,) this method could be compared with more methods. Currently, the compared methods seems still not sufficient.
- This general contribution and pipeline is similar to UniVRD, except (1) this model is mask-based rather than box-based, and (2) a prompted setting is further supported. Thus, the contribution seems incremental.
- The performance is basically the same as UniVRD and is not as good as RLIPv2.

Limitations:
Limitations are discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper presents a new semi-parametric language modeling approach that can incorporate text spans from a datastore into LLM-based generation improving both quality and attribution of generated texts. They propose a two-step approach that requires constructing an on-the-fly token-level datastore based on a small number of retrieved passages from a passage-level datastore. The current token is generated from the mixture distribution between the base LLM and the retrieved token-level distribution interpolated using a relative retrieval confidence score capturing the uncertainty of the token retriever. Furthermore, the approach enables extending the generation from token to an n-gram span based on a speculative decoding that rejects or accepts the tokens in the continuations, thus enabling span-level generation, improving efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Extensive experiments across tasks and datasets shows the efficacy of the proposed approach over standard LLM decoding, KNN and Retrieval augmented incontext learning variants.
2. Ablations show how the relaxation factor used for speculative decoding can enable flexible-length n-gram continuations to be incorporated based on the domain and can provide a good tradeoff between accuracy and attribution.

Weaknesses:
1. Multiple-token nearest neighbor generation has been proposed in prior work but was not discussed or compared, see: [1] Chunk-based Nearest Neighbor Machine Translation (https://aclanthology.org/2022.emnlp-main.284/).

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper:

* Introduces NEST, a semi-parametric language modeling approach that integrates real-world text spans into language model generations.
* Enhances generation quality and reduces latency by using token-level retrieval and speculative decoding.
* Outperforms conventional kNN-LM and competes well with in-context retrieval methods.
* Demonstrates significant improvements with various models.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* Introduces a novel semi-parametric language modeling technique for improved attribution and generation quality.
* Demonstrates a significant increase in speed and efficiency in language model generation.
* Outperforms conventional kNN-LM and shows competitive results against in-context retrieval methods across various knowledge-intensive tasks.
* Effective across a range of tasks including text completion, question answering, and factuality-aware generation, showcasing the method's adaptability to different content requirements.

Weaknesses:
* Performance heavily relies on the accuracy of the first-stage passage retrieval and second-stage token retrieval.
* The retrieval process may still be complex and resource-intensive for practical deployment.
* The gains in performance and efficiency are less pronounced in larger models.

Limitations:
* The overall effectiveness of NEST is contingent upon the precision of both stages of text retrieval.
* The approach may struggle with ensuring that the retrieved content is contextually relevant and free from biases.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents Nearest Neighbor Speculative Decoding (NEST), a technique to better inject real-world text spans into the output of existing language models. NEST is a kNN-LM approach adding an initial passage retrieval step. During inference, NEST uses Relative Retrieval Confidence (RRC) for confidence-based interpolation, dynamically extends selected tokens to include text spans when confidence is high, and employs a relaxed speculative decoding process that accepts only highly probable token spans.

According to the evaluation conducted by the authors with Llama 2 chat, NEST significantly outperforms both the base LM and standard kNN-LM, in terms of speed and accuracy.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
- well-written: The paper is clear and easy to understand. Figure 1 itself is enough to understand what the paper presents
- well-motivated: The paper explains the problem that it tries to solve and provides the motivations for why it needs to be solved 
- Extensive experiments demonstrating that the proposed approach works

Weaknesses:
- It’s unclear to me what are the most important contributions of this work. What is really new and has the most impact on the results? Even with the help of the related work section I cannot answer this question, hence my low confidence score.

Limitations:
The limitations are pointed out and sufficiently discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces NEST, a novel semi-parametric language modeling approach that enhances the generation quality and attribution of Large Language Models (LLMs) by incorporating real-world text spans. NEST employs a two-stage k-NN search and speculative decoding, achieving improved performance and reduced inference time.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The two-stage k-NN search is a smart optimization that balances search accuracy and efficiency.

The paper demonstrates significant improvements in generation quality and speed, offering a competitive edge over traditional methods.

The approach of providing direct attribution to sources is valuable for enhancing the reliability of LLMs.

Weaknesses:
The paper could benefit from a more detailed comparison with state-of-the-art methods.

The generalizability of NEST across different domains and languages needs further exploration.

The potential impact of NEST on the in-context learning ability of LLMs is not thoroughly discussed.

The paper lacks a comprehensive analysis of error rates and statistical significance.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper presents a new semi-parametric language modeling approach that can incorporate text spans from a datastore into LLM-based generation improving both quality and attribution of generated texts. They propose a two-step approach that requires constructing an on-the-fly token-level datastore based on a small number of retrieved passages from a passage-level datastore. The current token is generated from the mixture distribution between the base LLM and the retrieved token-level distribution interpolated using a relative retrieval confidence score capturing the uncertainty of the token retriever. Furthermore, the approach enables extending the generation from token to an n-gram span based on a speculative decoding that rejects or accepts the tokens in the continuations, thus enabling span-level generation, improving efficiency.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Extensive experiments across tasks and datasets shows the efficacy of the proposed approach over standard LLM decoding, KNN and Retrieval augmented incontext learning variants.
2. Ablations show how the relaxation factor used for speculative decoding can enable flexible-length n-gram continuations to be incorporated based on the domain and can provide a good tradeoff between accuracy and attribution.

Weaknesses:
1. Multiple-token nearest neighbor generation has been proposed in prior work but was not discussed or compared, see: [1] Chunk-based Nearest Neighbor Machine Translation (https://aclanthology.org/2022.emnlp-main.284/).

Limitations:
Yes

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper:

* Introduces NEST, a semi-parametric language modeling approach that integrates real-world text spans into language model generations.
* Enhances generation quality and reduces latency by using token-level retrieval and speculative decoding.
* Outperforms conventional kNN-LM and competes well with in-context retrieval methods.
* Demonstrates significant improvements with various models.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
* Introduces a novel semi-parametric language modeling technique for improved attribution and generation quality.
* Demonstrates a significant increase in speed and efficiency in language model generation.
* Outperforms conventional kNN-LM and shows competitive results against in-context retrieval methods across various knowledge-intensive tasks.
* Effective across a range of tasks including text completion, question answering, and factuality-aware generation, showcasing the method's adaptability to different content requirements.

Weaknesses:
* Performance heavily relies on the accuracy of the first-stage passage retrieval and second-stage token retrieval.
* The retrieval process may still be complex and resource-intensive for practical deployment.
* The gains in performance and efficiency are less pronounced in larger models.

Limitations:
* The overall effectiveness of NEST is contingent upon the precision of both stages of text retrieval.
* The approach may struggle with ensuring that the retrieved content is contextually relevant and free from biases.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work presents Nearest Neighbor Speculative Decoding (NEST), a technique to better inject real-world text spans into the output of existing language models. NEST is a kNN-LM approach adding an initial passage retrieval step. During inference, NEST uses Relative Retrieval Confidence (RRC) for confidence-based interpolation, dynamically extends selected tokens to include text spans when confidence is high, and employs a relaxed speculative decoding process that accepts only highly probable token spans.

According to the evaluation conducted by the authors with Llama 2 chat, NEST significantly outperforms both the base LM and standard kNN-LM, in terms of speed and accuracy.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
- well-written: The paper is clear and easy to understand. Figure 1 itself is enough to understand what the paper presents
- well-motivated: The paper explains the problem that it tries to solve and provides the motivations for why it needs to be solved 
- Extensive experiments demonstrating that the proposed approach works

Weaknesses:
- It’s unclear to me what are the most important contributions of this work. What is really new and has the most impact on the results? Even with the help of the related work section I cannot answer this question, hence my low confidence score.

Limitations:
The limitations are pointed out and sufficiently discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces NEST, a novel semi-parametric language modeling approach that enhances the generation quality and attribution of Large Language Models (LLMs) by incorporating real-world text spans. NEST employs a two-stage k-NN search and speculative decoding, achieving improved performance and reduced inference time.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The two-stage k-NN search is a smart optimization that balances search accuracy and efficiency.

The paper demonstrates significant improvements in generation quality and speed, offering a competitive edge over traditional methods.

The approach of providing direct attribution to sources is valuable for enhancing the reliability of LLMs.

Weaknesses:
The paper could benefit from a more detailed comparison with state-of-the-art methods.

The generalizability of NEST across different domains and languages needs further exploration.

The potential impact of NEST on the in-context learning ability of LLMs is not thoroughly discussed.

The paper lacks a comprehensive analysis of error rates and statistical significance.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposes a novel framework for approximately solving graph diffusion equations using a local diffusion process. In addition,  the proposed method can effectively localizes standard iterative solvers by designing simple and provably sublinear time algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ The problem is well motivated and the paper is well-written.
+ Extensive experiments are conducted to showcase the efficiency of graph diffusion framework to approximating graph diffusion equations.
+ The paper provides a good summary of existing graph diffusion equations.
+ The authors provide code and details of implementations.

Weaknesses:
- In this paper, the authors explore 18 different graphs, can the authors show/provide which local GDE solver achieves better performance on which type(s) of graphs? 
- Computational complexity/cost is missing.

Limitations:
Not applicable.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper, ""Faster Local Solvers for Graph Diffusion Equations,"" addresses the efficiency of computing Graph Diffusion Equations (GDEs) such as Personalized PageRank (PPR), Katz centrality, and the Heat kernel, which are essential for various graph-related problems like clustering and training neural networks. Traditional methods for solving GDEs are computationally intensive for large-scale graphs. This paper introduces a novel framework for approximating GDEs using local diffusion processes, which significantly reduces computational time and improves scalability by leveraging the localization property of diffusion vectors. The proposed local solvers are highly parallelizable and suitable for GPU implementation, offering up to a hundred-fold speed improvement and applicability to large-scale dynamic graphs. The paper also discusses the potential for these methods to enhance local message-passing mechanisms in Graph Neural Networks (GNNs).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The introduction of a novel framework for localizing the computation of GDEs using local diffusion processes is a significant contribution. This approach reveals the suboptimality of existing local solvers and provides a more efficient solution.


The paper offers a solid theoretical foundation, proving that popular diffusion vectors have strong localization properties using the participation ratio. It demonstrates that Approximate Personalized PageRank (APPR) can be treated as a special case of the proposed framework, providing better diffusion-based bounds. The design of simple and fast local methods based on standard gradient descent and the local Chebyshev method for symmetric propagation matrices is well-founded.


Experimental results on GDE approximation for PPR, HK, and Katz show significant acceleration over standard methods. The proposed local solvers demonstrate up to a hundred-fold speed improvement. The paper also shows that the new methods can be naturally adopted to approximate dynamic diffusion vectors, and they outperform standard PPR-based GNNs in training speed.

Weaknesses:
Precision Limitations: While the paper shows significant speedups for lower precision settings, the performance gain diminishes as higher precision is required. This limitation could affect the applicability of the methods in scenarios where high precision is crucial.

Sequential Nature: (Please correct if I am wrong.) Despite improvements, the reliance on sequential updates in some methods (like LocalSOR) still poses challenges for achieving maximal parallel efficiency.

Complexity of Analysis: The runtime analysis for some of the proposed methods, such as the Local Chebyshev method, is noted as complex and remains an open problem. Maybe simplifying this analysis or providing more intuitive explanations could enhance the paper’s accessibility.

Limitations:
See above

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new local iterative framework for solving graph diffusion equations (GDEs). Specifically, the framework approximates GDEs through a local diffusion process, leveraging the strong localization properties of diffusion vectors such as personalized PageRank, Katz centrality, and Heat Kernel. The proposed local solvers can achieve sublinear runtime complexity under certain monotonicity assumptions. Empirical results demonstrate that these solvers significantly accelerate their standard counterparts on several large-scale benchmark datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-structured and clear.

2. The theoretical analysis is rigorous and sound, providing runtime complexity bounds for some of the proposed local solvers, which are better than their standard counterparts.

3. The effectiveness of the proposed methods is well-supported by experimental results on large-scale benchmark datasets.

Weaknesses:
1. It is not clear how graph structures, such as sparsity and spectral properties, impact the runtime complexity of the local solvers. A discussion on the potential influence of graph structures on runtime complexity would be helpful.

2. The runtime complexity analysis assumes that the updates of local solvers satisfy monotonicity properties. However, LocalCH does not satisfy these properties during updates. Establishing runtime bounds for LocalCH may require different techniques.

3. In Figure 7, when $\epsilon \geq 2^{-31}$, the running time of LocalGD is the same as GD, indicating that LocalGD may not speed up the standard counterparts when $\epsilon$ is sufficiently small.

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a suite of fast methods to approximately compute graph diffusion vectors such as Personalized PageRank, Katz centrality and the heat kernel. A notable feature of the proposed methods is that they are easily parallelizable and hence can achieve further acceleration on GPU. The authors also provide a running time bound for each method that they introduce. Empirical results show that the new local methods can achieve up to a hundred-fold speedup when compared to their global counterpart.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written and easy to follow.
- The local diffusion framework is applicable to computing several important graph diffusion vectors.
- The experiments are reasonably comprehensive.

Weaknesses:
- When compared with APPR, the speedup is not really captured by Theorem 3.3 and Corollary 3.6. The authors only showed that $\overline{\mbox{vol}}(\mathcal{S}_T)/\bar\gamma_T \le 1/\epsilon$, but it requires strict inequality to achieve nontrivial speedup in terms of worse-case running time. It is not clear how tight this bound is in general. If the bound is tight, then in the worst case there is no speedup. The authors should comment on if there are classes of graphs over which there will be a notable gap between the 2 quantities $\overline{\mbox{vol}}(\mathcal{S}_T)/\bar\gamma_T$ and $1/\epsilon$, so that the result provides a meaningful improvement.
- Because the theorems concerning the worst-case running time do not seem to capture a clear improvement over simple baseline local methods, it is unclear where exactly the speedup reported in Table 2 comes from.
- In LocalGD and LocalCH, the authors did not provide an update rule (or even a definition) for $\mathcal{S}_t$. Since $\mathcal{S}_t$ is part of the local diffusion process, the authors should specify how $\mathcal{S}_t$ is updated or defined at each step. I guess that $\mathcal{S}_t$ depends on the termination condition, but I could not find where the authors mention about it in the main paper.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposes a novel framework for approximately solving graph diffusion equations using a local diffusion process. In addition,  the proposed method can effectively localizes standard iterative solvers by designing simple and provably sublinear time algorithms.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
+ The problem is well motivated and the paper is well-written.
+ Extensive experiments are conducted to showcase the efficiency of graph diffusion framework to approximating graph diffusion equations.
+ The paper provides a good summary of existing graph diffusion equations.
+ The authors provide code and details of implementations.

Weaknesses:
- In this paper, the authors explore 18 different graphs, can the authors show/provide which local GDE solver achieves better performance on which type(s) of graphs? 
- Computational complexity/cost is missing.

Limitations:
Not applicable.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper, ""Faster Local Solvers for Graph Diffusion Equations,"" addresses the efficiency of computing Graph Diffusion Equations (GDEs) such as Personalized PageRank (PPR), Katz centrality, and the Heat kernel, which are essential for various graph-related problems like clustering and training neural networks. Traditional methods for solving GDEs are computationally intensive for large-scale graphs. This paper introduces a novel framework for approximating GDEs using local diffusion processes, which significantly reduces computational time and improves scalability by leveraging the localization property of diffusion vectors. The proposed local solvers are highly parallelizable and suitable for GPU implementation, offering up to a hundred-fold speed improvement and applicability to large-scale dynamic graphs. The paper also discusses the potential for these methods to enhance local message-passing mechanisms in Graph Neural Networks (GNNs).

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The introduction of a novel framework for localizing the computation of GDEs using local diffusion processes is a significant contribution. This approach reveals the suboptimality of existing local solvers and provides a more efficient solution.


The paper offers a solid theoretical foundation, proving that popular diffusion vectors have strong localization properties using the participation ratio. It demonstrates that Approximate Personalized PageRank (APPR) can be treated as a special case of the proposed framework, providing better diffusion-based bounds. The design of simple and fast local methods based on standard gradient descent and the local Chebyshev method for symmetric propagation matrices is well-founded.


Experimental results on GDE approximation for PPR, HK, and Katz show significant acceleration over standard methods. The proposed local solvers demonstrate up to a hundred-fold speed improvement. The paper also shows that the new methods can be naturally adopted to approximate dynamic diffusion vectors, and they outperform standard PPR-based GNNs in training speed.

Weaknesses:
Precision Limitations: While the paper shows significant speedups for lower precision settings, the performance gain diminishes as higher precision is required. This limitation could affect the applicability of the methods in scenarios where high precision is crucial.

Sequential Nature: (Please correct if I am wrong.) Despite improvements, the reliance on sequential updates in some methods (like LocalSOR) still poses challenges for achieving maximal parallel efficiency.

Complexity of Analysis: The runtime analysis for some of the proposed methods, such as the Local Chebyshev method, is noted as complex and remains an open problem. Maybe simplifying this analysis or providing more intuitive explanations could enhance the paper’s accessibility.

Limitations:
See above

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a new local iterative framework for solving graph diffusion equations (GDEs). Specifically, the framework approximates GDEs through a local diffusion process, leveraging the strong localization properties of diffusion vectors such as personalized PageRank, Katz centrality, and Heat Kernel. The proposed local solvers can achieve sublinear runtime complexity under certain monotonicity assumptions. Empirical results demonstrate that these solvers significantly accelerate their standard counterparts on several large-scale benchmark datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper is well-structured and clear.

2. The theoretical analysis is rigorous and sound, providing runtime complexity bounds for some of the proposed local solvers, which are better than their standard counterparts.

3. The effectiveness of the proposed methods is well-supported by experimental results on large-scale benchmark datasets.

Weaknesses:
1. It is not clear how graph structures, such as sparsity and spectral properties, impact the runtime complexity of the local solvers. A discussion on the potential influence of graph structures on runtime complexity would be helpful.

2. The runtime complexity analysis assumes that the updates of local solvers satisfy monotonicity properties. However, LocalCH does not satisfy these properties during updates. Establishing runtime bounds for LocalCH may require different techniques.

3. In Figure 7, when $\epsilon \geq 2^{-31}$, the running time of LocalGD is the same as GD, indicating that LocalGD may not speed up the standard counterparts when $\epsilon$ is sufficiently small.

Limitations:
NA

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes a suite of fast methods to approximately compute graph diffusion vectors such as Personalized PageRank, Katz centrality and the heat kernel. A notable feature of the proposed methods is that they are easily parallelizable and hence can achieve further acceleration on GPU. The authors also provide a running time bound for each method that they introduce. Empirical results show that the new local methods can achieve up to a hundred-fold speedup when compared to their global counterpart.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- The paper is well-written and easy to follow.
- The local diffusion framework is applicable to computing several important graph diffusion vectors.
- The experiments are reasonably comprehensive.

Weaknesses:
- When compared with APPR, the speedup is not really captured by Theorem 3.3 and Corollary 3.6. The authors only showed that $\overline{\mbox{vol}}(\mathcal{S}_T)/\bar\gamma_T \le 1/\epsilon$, but it requires strict inequality to achieve nontrivial speedup in terms of worse-case running time. It is not clear how tight this bound is in general. If the bound is tight, then in the worst case there is no speedup. The authors should comment on if there are classes of graphs over which there will be a notable gap between the 2 quantities $\overline{\mbox{vol}}(\mathcal{S}_T)/\bar\gamma_T$ and $1/\epsilon$, so that the result provides a meaningful improvement.
- Because the theorems concerning the worst-case running time do not seem to capture a clear improvement over simple baseline local methods, it is unclear where exactly the speedup reported in Table 2 comes from.
- In LocalGD and LocalCH, the authors did not provide an update rule (or even a definition) for $\mathcal{S}_t$. Since $\mathcal{S}_t$ is part of the local diffusion process, the authors should specify how $\mathcal{S}_t$ is updated or defined at each step. I guess that $\mathcal{S}_t$ depends on the termination condition, but I could not find where the authors mention about it in the main paper.

Limitations:
Yes

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposed a real-time stereo 3D object detection algorithm under the streaming preception framework setting. The proposed StreamDSGN build on the existing streaming perception work and the DSGN 3D object detection work and has three technical contributions: 1) the Feature-Flow-Fusion module predicts future-frame features using flow map which reduce the misalignment between supervision and current observation in the streaming preception setting, 2) Motion Consistency Loss function for explicit supervision based on motion consistency between adjacent frames, 3) Large Kernel BEV Backbone for capturing long-range dependencies in low framerate 3D object detection dataset. Experiments on the KITTI Tracking dataset show that the proposed StreamDSGN method achieves impressive result on KITTI Tracking dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposed algorithm of 3D object detection for streaming perception is interesting, the performance evaluation setting for considering both accuracy and latency is reasonable for practical applications such autonomous driving
2. Experimental result is impressive and convincing: the end-to-end framework shows significant better performance than a straightforward combination of Streamer and DSGN++_{opt}, and the proposed FFF, MCL and LKBB techniques together achieve 4.33% increase in 3D object detection streaming average precision compared with the end-to-end baseline. The source code is provided and the experiment result is reproducible.
3. The presenation is clear and easy to follow
4. The ablation study is clear and the baseline setting is reasonable

Weaknesses:
1. The setting of 3D detection from stereo is not very common in autonomous driving and the relevant dataset is limited. It would be good to extend the stereo camera setting to multi-camera setting or camera+Lidar setting and test the generalization capability of the proposed algorithm on extended settings.
2. There lacks detailed computational latency analysis of each algrorithm module as shown in Figure.3,  the overall latency of 91.45ms may not be enough to check the trade-off between performance & latency for each algorithm module

Limitations:
The limitation of the proposed algorithm has been addressed in Section 5.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors propose a real-time stereo-based 3D object detection framework for streaming perception. Specifically, the authors design feature flow fusion, motion consistency loss and a Large Kernel BEV backbone to improve the performance. The authors validate the effectiveness of the proposed method and each module by conducting experiments on KITTI dataset. However, the author only compare their method with one baseline which they build their method on.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. As claimed in the paper, this is the firstwork designed for 3D object detection streaming perception, which is a good setting that better aligns with real applications.

2. The authors analyze the challenges in streaming perception, and add new modules into existing framework to improve the performance

3. Experiments on the Kitti dataset validate the effectiveness of the three modules designed in this paper. 

The paper is well structured and the visualization is good.

Weaknesses:
The novelty of the proposed method is not significant. The challenges in streaming perception are evident, and the technical novetly  of the proposed solution is not very significant to me. According to Table2, the biggest performence improvement comes from the fusion of (t-1). The three modules do not result in sigificant performance improvement.

The intuition of MCL needs more elaboration. Why do we need velocity loss and acc loss? The supervision of position already encodes velocity and acc.

The latency of the network is strongly dependent on the hardware. Also, the latency caused by camera exposure and data transportation are not considered. A comprarison under different latency can be added to better illustrate the usefulness of the method under different scenarios.

Limitations:
The limitations are addreess in the paper, including incoreect feature fusion caused by occlusion and truncation.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work proposes StreamDSGN, a framework for steaming perception, evaluated on the KITTI dataset with sAP (steaming average performance metric), which takes into account the model latency and is a better fit for evaluating streaming perception than purely qulitative metrics. It is the first work to do 3D object detection in streaming perception setting. The DSGN++ base detector is used and performance is improved. The paper introduce three methods to improve the results: (1) Using past frames' feature positions to predict the next frames features, (2) additional supervision, (3) a larger kernel for bigger receptive field.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. The ablation study is exhaustive.

2. Figures are well done and generally support the understanding (Figure 2 is adressed separately).

3. The method achieves a better result than the baseline.

Weaknesses:
1. Limitations are discussed only really shortly

2. Figure 2 needs a little more explanation. There is more than one GT-trajectory, are they from differnt frames? Prediction and GT are for the next frame, t+1?

3. Unclear sentence in the introduction: ""It is observed that for moving objects, the ground truth of the next frame (depicted by the red bounding box) consistently precedes their current position."" What does it mean for the ground truth to ""precede the current position""?

4. The largest concern: The test setup, with interleaved training / testing frames does not really provide a separate testing set. All scenes are split into sequences of 4 seconds, which are then used alternatingly for test and training. In urban driving scenarios, cars won't move that much in 4 seconds. This leads to a significantly smaller domain gap between test and training than when, for example, training on completely different sequences. It is unclear if this split is a common protocol for KITTI or though off for this paper.

Limitations:
Limitations are discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents stereo-based 3D object detection method designed for streaming perception, where the current frame (and past frames) are taken to predict the object bounding boxes in the next frame. The authors adopt a simplified DSGN++ as the backbone and add several components to enhance the perception accuracy. First, the authors proposed to estimate the flow in feature space from t-1 to t and use it to warp the features in the future frame at t+1. Besides, a motion consistency loss is added to refine the future trajectory. Last, the authors propose a large kernel backbone to process the BEV feature. The proposed method could reach a latency of 90ms and outperform a baseline (using Kalman Filter) on the KITTI dataset.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well-motivated and aims to solve an important problem in the real world applications

2. The proposed method yields an appealing performance in streaming perception in the KITTI dataset.

3. The way of generating feature flow is interesting and could be used in other automonous driving applications

Weaknesses:
Though the proposed method shows a good performance, some details about the proposed new components are missing. Besides, the proposed method is only tested in one dataset and compared with one baseline, which may not be sufficient. Please refer to Questions for more details.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposed a real-time stereo 3D object detection algorithm under the streaming preception framework setting. The proposed StreamDSGN build on the existing streaming perception work and the DSGN 3D object detection work and has three technical contributions: 1) the Feature-Flow-Fusion module predicts future-frame features using flow map which reduce the misalignment between supervision and current observation in the streaming preception setting, 2) Motion Consistency Loss function for explicit supervision based on motion consistency between adjacent frames, 3) Large Kernel BEV Backbone for capturing long-range dependencies in low framerate 3D object detection dataset. Experiments on the KITTI Tracking dataset show that the proposed StreamDSGN method achieves impressive result on KITTI Tracking dataset.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The proposed algorithm of 3D object detection for streaming perception is interesting, the performance evaluation setting for considering both accuracy and latency is reasonable for practical applications such autonomous driving
2. Experimental result is impressive and convincing: the end-to-end framework shows significant better performance than a straightforward combination of Streamer and DSGN++_{opt}, and the proposed FFF, MCL and LKBB techniques together achieve 4.33% increase in 3D object detection streaming average precision compared with the end-to-end baseline. The source code is provided and the experiment result is reproducible.
3. The presenation is clear and easy to follow
4. The ablation study is clear and the baseline setting is reasonable

Weaknesses:
1. The setting of 3D detection from stereo is not very common in autonomous driving and the relevant dataset is limited. It would be good to extend the stereo camera setting to multi-camera setting or camera+Lidar setting and test the generalization capability of the proposed algorithm on extended settings.
2. There lacks detailed computational latency analysis of each algrorithm module as shown in Figure.3,  the overall latency of 91.45ms may not be enough to check the trade-off between performance & latency for each algorithm module

Limitations:
The limitation of the proposed algorithm has been addressed in Section 5.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper, the authors propose a real-time stereo-based 3D object detection framework for streaming perception. Specifically, the authors design feature flow fusion, motion consistency loss and a Large Kernel BEV backbone to improve the performance. The authors validate the effectiveness of the proposed method and each module by conducting experiments on KITTI dataset. However, the author only compare their method with one baseline which they build their method on.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. As claimed in the paper, this is the firstwork designed for 3D object detection streaming perception, which is a good setting that better aligns with real applications.

2. The authors analyze the challenges in streaming perception, and add new modules into existing framework to improve the performance

3. Experiments on the Kitti dataset validate the effectiveness of the three modules designed in this paper. 

The paper is well structured and the visualization is good.

Weaknesses:
The novelty of the proposed method is not significant. The challenges in streaming perception are evident, and the technical novetly  of the proposed solution is not very significant to me. According to Table2, the biggest performence improvement comes from the fusion of (t-1). The three modules do not result in sigificant performance improvement.

The intuition of MCL needs more elaboration. Why do we need velocity loss and acc loss? The supervision of position already encodes velocity and acc.

The latency of the network is strongly dependent on the hardware. Also, the latency caused by camera exposure and data transportation are not considered. A comprarison under different latency can be added to better illustrate the usefulness of the method under different scenarios.

Limitations:
The limitations are addreess in the paper, including incoreect feature fusion caused by occlusion and truncation.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work proposes StreamDSGN, a framework for steaming perception, evaluated on the KITTI dataset with sAP (steaming average performance metric), which takes into account the model latency and is a better fit for evaluating streaming perception than purely qulitative metrics. It is the first work to do 3D object detection in streaming perception setting. The DSGN++ base detector is used and performance is improved. The paper introduce three methods to improve the results: (1) Using past frames' feature positions to predict the next frames features, (2) additional supervision, (3) a larger kernel for bigger receptive field.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. The ablation study is exhaustive.

2. Figures are well done and generally support the understanding (Figure 2 is adressed separately).

3. The method achieves a better result than the baseline.

Weaknesses:
1. Limitations are discussed only really shortly

2. Figure 2 needs a little more explanation. There is more than one GT-trajectory, are they from differnt frames? Prediction and GT are for the next frame, t+1?

3. Unclear sentence in the introduction: ""It is observed that for moving objects, the ground truth of the next frame (depicted by the red bounding box) consistently precedes their current position."" What does it mean for the ground truth to ""precede the current position""?

4. The largest concern: The test setup, with interleaved training / testing frames does not really provide a separate testing set. All scenes are split into sequences of 4 seconds, which are then used alternatingly for test and training. In urban driving scenarios, cars won't move that much in 4 seconds. This leads to a significantly smaller domain gap between test and training than when, for example, training on completely different sequences. It is unclear if this split is a common protocol for KITTI or though off for this paper.

Limitations:
Limitations are discussed.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper presents stereo-based 3D object detection method designed for streaming perception, where the current frame (and past frames) are taken to predict the object bounding boxes in the next frame. The authors adopt a simplified DSGN++ as the backbone and add several components to enhance the perception accuracy. First, the authors proposed to estimate the flow in feature space from t-1 to t and use it to warp the features in the future frame at t+1. Besides, a motion consistency loss is added to refine the future trajectory. Last, the authors propose a large kernel backbone to process the BEV feature. The proposed method could reach a latency of 90ms and outperform a baseline (using Kalman Filter) on the KITTI dataset.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is well-motivated and aims to solve an important problem in the real world applications

2. The proposed method yields an appealing performance in streaming perception in the KITTI dataset.

3. The way of generating feature flow is interesting and could be used in other automonous driving applications

Weaknesses:
Though the proposed method shows a good performance, some details about the proposed new components are missing. Besides, the proposed method is only tested in one dataset and compared with one baseline, which may not be sufficient. Please refer to Questions for more details.

Limitations:
The authors have adequately addressed the limitations.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces a decoding-time alignment method called Multi-Objective Decoding (MOD) for aligning language models (LMs) with multiple objectives. MOD combines a set of models aligned for individual rewards and allows any weightings (even not-all-positive) for rewards. MOD leverages a common form among f-divergence regularized alignment approaches to derive an efficient decoding strategy that greedily selects the next token from an algebraic combination of predicted probabilities of all base models. MOD can maximize an interpolated reward function without extensive retraining. The paper theoretically demonstrates the sub-optimality of existing approaches and establishes optimality guarantees for MOD. Empirical results show MOD's effectiveness, with a 12.8% overall reward improvement over a parameter-merging baseline when optimizing for three objectives.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
* **Flexibility in Objectives Alignment**: MOD's primary strength lies in its ability to align language models with multiple objectives simultaneously. This flexibility allows it to balance and prioritize different user needs and preferences without the need for retraining the model for each new objective combination.
* **Efficiency and Simplicity**: The algorithm is efficient and simple to implement, as it only need to operate the probabilities at decoding time.
* **Theoretical Robustness and Empirical Validation**: MOD is underpinned by a strong theoretical foundation, with proofs that demonstrate its optimality and sub-optimality of existing methods. The paper provides empirical evidence of MOD's effectiveness across various tasks and models, showcasing its practical applicability and robustness in real-world scenarios.

Weaknesses:
I don't see any major weaknesses in this paper.

Limitations:
None

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a decoding method that aims to combine the predictions of diverse models that are aligned with different objectives. In their multi-objective setting, the goal is to find an optimal policy that maximize a weighted, multi-objective reward, given the policy aligned to each of the individual rewards. In particular, the authors propose a reformulation using Legendre transform to bypass calculating Z (normalization) at a sequence level.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors provide detailed theoretical analysis to justify their approach.
2. The authors show that the proposed method can handle negative weights for rewards, which cannot be accomplished by previous work.

Weaknesses:
1. The baselines appears weak. For example, in Appendix F, the main comparison is against RS. However, RS cannot even outperform the best individual model in all experiments (Tables 7,8,9,10).
2. The proposed MOD also seems not much stronger. MOD can only beat the best individual model on 2/4 settings in Appendix F.
3. Lack of baselines. It would be helpful if the authors can include more generic ensemble baselines such as weighted averaging/voting.

Limitations:
Conclusion and throughout the work.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In many practical uses of RLHF the reward function is the convex combination of several rewards. Instead of training a single policy attempting to maximize the expected aggregate reward (subject to the usual regularization keeping it close to an anchor policy), the authors show that one can train separate policies, one for each reward and then mix them at decoding time using the same convex combination in log-probability space.

One important consequence is that one can change the weights on various rewards at decoding time, per response, making the algorithm very appealing for situations where the balance between certain rewards needs to change depending on the prompt/context.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
Novel approach to dealing with rewards that are a linear mix of ""elementary"" rewards; mathematically sound.

Offers a simple, practical way of changing the mix of ""elementary"" rewards at decoding time, per model response.

Weaknesses:
The presentation could be much simpler, starting from the ubiquitous case of using KL divergence for regularization, which also leads to the elegant log-linear combination in Eq. (7). The general case for f-divergence could be mentioned, but relegated to the already prodigious appendix.

One technical weakness of the proposed approach is that one needs to serve/run M different policies at decoding time, which is significant overhead.

After completing the review I have become aware of the work in:
@misc{wang2024conditionedlanguagepolicygeneral,
      title={Conditioned Language Policy: A General Framework for Steerable Multi-Objective Finetuning}, 
      author={Kaiwen Wang and Rahul Kidambi and Ryan Sullivan and Alekh Agarwal and Christoph Dann and Andrea Michi and Marco Gelmi and Yunxuan Li and Raghav Gupta and Avinava Dubey and Alexandre Ramé and Johan Ferret and Geoffrey Cideron and Le Hou and Hongkun Yu and Amr Ahmed and Aranyak Mehta and Léonard Hussenot and Olivier Bachem and Edouard Leurent},
      year={2024},
      eprint={2407.15762},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.15762}, 
}

Section 6 and Appendix E in [wang2024conditionedlanguagepolicygeneral] are directly relevant to this paper, deriving a sensitivity analysis for logit mixing, the log-linear combination in Eq. (7).

Limitations:
One technical weakness of the proposed approach is that one needs to serve/run M different policies at decoding time, which is significant overhead.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents Multi-Objective Decoding (MOD), a novel algorithm designed to align language models (LMs) with multiple human preferences simultaneously during decoding. MOD addresses the limitations of existing methods that optimize LMs for a single reward function, thereby providing flexibility and efficiency without the need for retraining. The authors define multi-objective reward functions and assume the existence of single-objective aligned LMs optimized for specific rewards. By leveraging the properties of strong-barrier functions and using the Legendre transform, they derive a closed-form solution for linearly combining the outputs of different models, achieving multi-objective alignment. This method guarantees optimality under certain conditions and transforms response-level decoding into efficient token-level decoding using greedy search. Extensive experiments validate MOD&#39;s effectiveness, demonstrating significant improvements in reward optimization compared to parameter-merging baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
MOD introduces a novel method for multi-objective alignment, enabling language models to align with multiple objectives simultaneously during decoding, thus eliminating the need for retraining.

The authors provide a robust theoretical framework by defining multi-objective reward functions and leveraging strong-barrier functions. They prove a closed-form bijection between single-objective models and their rewards, and derive a closed-form solution using the Legendre transform.

MOD achieves optimality guarantees under certain conditions and transforms response-level decoding into efficient token-level decoding using greedy search, making the method both effective and practical.

Extensive experiments demonstrate MOD's superior performance, showing a 12.8% overall reward improvement compared to parameter-merging baselines when optimizing for three objectives. The effectiveness is validated across various tasks and model sizes.

Weaknesses:
Although MOD circumvents the need for retraining, it requires loading multiple models concurrently, which can be computationally intensive and may not scale efficiently for a larger number of objectives or bigger model sizes.

The paper could benefit from a more detailed discussion on potential negative impacts or failure modes, especially in scenarios involving conflicting objectives or suboptimal base model alignment.

Limitations:
The authors have addressed the limitations in Section 7.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces a decoding-time alignment method called Multi-Objective Decoding (MOD) for aligning language models (LMs) with multiple objectives. MOD combines a set of models aligned for individual rewards and allows any weightings (even not-all-positive) for rewards. MOD leverages a common form among f-divergence regularized alignment approaches to derive an efficient decoding strategy that greedily selects the next token from an algebraic combination of predicted probabilities of all base models. MOD can maximize an interpolated reward function without extensive retraining. The paper theoretically demonstrates the sub-optimality of existing approaches and establishes optimality guarantees for MOD. Empirical results show MOD's effectiveness, with a 12.8% overall reward improvement over a parameter-merging baseline when optimizing for three objectives.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
* **Flexibility in Objectives Alignment**: MOD's primary strength lies in its ability to align language models with multiple objectives simultaneously. This flexibility allows it to balance and prioritize different user needs and preferences without the need for retraining the model for each new objective combination.
* **Efficiency and Simplicity**: The algorithm is efficient and simple to implement, as it only need to operate the probabilities at decoding time.
* **Theoretical Robustness and Empirical Validation**: MOD is underpinned by a strong theoretical foundation, with proofs that demonstrate its optimality and sub-optimality of existing methods. The paper provides empirical evidence of MOD's effectiveness across various tasks and models, showcasing its practical applicability and robustness in real-world scenarios.

Weaknesses:
I don't see any major weaknesses in this paper.

Limitations:
None

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a decoding method that aims to combine the predictions of diverse models that are aligned with different objectives. In their multi-objective setting, the goal is to find an optimal policy that maximize a weighted, multi-objective reward, given the policy aligned to each of the individual rewards. In particular, the authors propose a reformulation using Legendre transform to bypass calculating Z (normalization) at a sequence level.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The authors provide detailed theoretical analysis to justify their approach.
2. The authors show that the proposed method can handle negative weights for rewards, which cannot be accomplished by previous work.

Weaknesses:
1. The baselines appears weak. For example, in Appendix F, the main comparison is against RS. However, RS cannot even outperform the best individual model in all experiments (Tables 7,8,9,10).
2. The proposed MOD also seems not much stronger. MOD can only beat the best individual model on 2/4 settings in Appendix F.
3. Lack of baselines. It would be helpful if the authors can include more generic ensemble baselines such as weighted averaging/voting.

Limitations:
Conclusion and throughout the work.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In many practical uses of RLHF the reward function is the convex combination of several rewards. Instead of training a single policy attempting to maximize the expected aggregate reward (subject to the usual regularization keeping it close to an anchor policy), the authors show that one can train separate policies, one for each reward and then mix them at decoding time using the same convex combination in log-probability space.

One important consequence is that one can change the weights on various rewards at decoding time, per response, making the algorithm very appealing for situations where the balance between certain rewards needs to change depending on the prompt/context.

Soundness:
3: good

Presentation:
2: fair

Contribution:
4: excellent

Strengths:
Novel approach to dealing with rewards that are a linear mix of ""elementary"" rewards; mathematically sound.

Offers a simple, practical way of changing the mix of ""elementary"" rewards at decoding time, per model response.

Weaknesses:
The presentation could be much simpler, starting from the ubiquitous case of using KL divergence for regularization, which also leads to the elegant log-linear combination in Eq. (7). The general case for f-divergence could be mentioned, but relegated to the already prodigious appendix.

One technical weakness of the proposed approach is that one needs to serve/run M different policies at decoding time, which is significant overhead.

After completing the review I have become aware of the work in:
@misc{wang2024conditionedlanguagepolicygeneral,
      title={Conditioned Language Policy: A General Framework for Steerable Multi-Objective Finetuning}, 
      author={Kaiwen Wang and Rahul Kidambi and Ryan Sullivan and Alekh Agarwal and Christoph Dann and Andrea Michi and Marco Gelmi and Yunxuan Li and Raghav Gupta and Avinava Dubey and Alexandre Ramé and Johan Ferret and Geoffrey Cideron and Le Hou and Hongkun Yu and Amr Ahmed and Aranyak Mehta and Léonard Hussenot and Olivier Bachem and Edouard Leurent},
      year={2024},
      eprint={2407.15762},
      archivePrefix={arXiv},
      primaryClass={cs.LG},
      url={https://arxiv.org/abs/2407.15762}, 
}

Section 6 and Appendix E in [wang2024conditionedlanguagepolicygeneral] are directly relevant to this paper, deriving a sensitivity analysis for logit mixing, the log-linear combination in Eq. (7).

Limitations:
One technical weakness of the proposed approach is that one needs to serve/run M different policies at decoding time, which is significant overhead.

Rating:
8: accept, good paper

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper presents Multi-Objective Decoding (MOD), a novel algorithm designed to align language models (LMs) with multiple human preferences simultaneously during decoding. MOD addresses the limitations of existing methods that optimize LMs for a single reward function, thereby providing flexibility and efficiency without the need for retraining. The authors define multi-objective reward functions and assume the existence of single-objective aligned LMs optimized for specific rewards. By leveraging the properties of strong-barrier functions and using the Legendre transform, they derive a closed-form solution for linearly combining the outputs of different models, achieving multi-objective alignment. This method guarantees optimality under certain conditions and transforms response-level decoding into efficient token-level decoding using greedy search. Extensive experiments validate MOD&#39;s effectiveness, demonstrating significant improvements in reward optimization compared to parameter-merging baselines.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
MOD introduces a novel method for multi-objective alignment, enabling language models to align with multiple objectives simultaneously during decoding, thus eliminating the need for retraining.

The authors provide a robust theoretical framework by defining multi-objective reward functions and leveraging strong-barrier functions. They prove a closed-form bijection between single-objective models and their rewards, and derive a closed-form solution using the Legendre transform.

MOD achieves optimality guarantees under certain conditions and transforms response-level decoding into efficient token-level decoding using greedy search, making the method both effective and practical.

Extensive experiments demonstrate MOD's superior performance, showing a 12.8% overall reward improvement compared to parameter-merging baselines when optimizing for three objectives. The effectiveness is validated across various tasks and model sizes.

Weaknesses:
Although MOD circumvents the need for retraining, it requires loading multiple models concurrently, which can be computationally intensive and may not scale efficiently for a larger number of objectives or bigger model sizes.

The paper could benefit from a more detailed discussion on potential negative impacts or failure modes, especially in scenarios involving conflicting objectives or suboptimal base model alignment.

Limitations:
The authors have addressed the limitations in Section 7.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
Latent-based image generative models, such as LDMs and MIMs, have achieved success, but autoregressive models lag behind in image generation. Our research introduces a unified perspective on latent space stability and proposes a discrete image tokenizer, DiGIT, that significantly improves autoregressive image modeling, outperforming LDMs and benefiting from scaling similar to GPT in NLP.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The results beat some baseline models, though under a specific (and somewhat confused) experimental setting.
- The topic of latent space property is worth investigating.

Weaknesses:
The paper has several weaknesses:

1. **Factual Errors**:

    1.1. The cited MIM models, such as MaskGIT and MAGE, cannot revise previously predicted tokens. This contradicts the claim in line 53 that ""iterative models like LDMs and MIMs can correct errors."" I recommend the authors to their papers for more details.

    1.2. In lines 72-73, the authors state that this work provides ""the first evidence that image autoregressive generative models behave analogously to GPT."" However, the Parti[1] paper has already demonstrated that image autoregressive models have similar scalability to GPT and successfully scaled the model to 20B. The authors have not cited this work.

2. The writing is poor and lacks rigor. For example, the discussion on the so-called ""common misconception"" in line 41 is not well-supported. What exactly is meant by the ""optimal latent space for reconstruction""? How many studies hold this view? There are no citations provided.

3. The quantitative comparisons are also peculiar. The authors cite many paper results without using CFG, while CFG has become a de-facto practice for augmenting generative models. Why not adopt CFG and perform more apples-to-apples comparisons to other SOTA methods with CFG?

4. Presenting two tables (Table 2 lower and Table 3) for image generation performance is confusing. Why not consolidate the results into a single, clear table?

[1] Yu, Jiahui, et al. ""Scaling autoregressive models for content-rich text-to-image generation."" arXiv preprint arXiv:2206.10789 2.3 (2022): 5.

Limitations:
The writing & presentation of this paper seems too rush and lacks rigor. I recommend the authors to refine and polish this paper. The current draft may not be qualified for the publication of NeurIPS.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper designs a better quantized autoencoder on top of VQGAN. It builds an image autoencoder which is able to both achieve good recognition performance for linear probing, and have a latent space which is suitable for training a generative model. It studies the existing autoencoders from a high-level theoretical perspective and proposes design ideas which are targeted to improve them. The paper claims that the modern autoencoders ignore the fact that they will be utilized for downstream generative modelling tasks and mainly focus on reconstruction. The paper argues that adding recognition losses on top of the encoder would help. To fulfill this desiderata, the model takes DINOv2 features and discretizes them via k-means. Then it trains a translation model into VQ-GAN decoder features. For image generation, it trains a LLM in the discrete token space. For classification, it does linear probing on top of discretized DINOv2 features. As a result, it attains reasonable generative capabilities while being able to keep a latent space suitable for linear probing classification.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- In terms of the scores, the paper achieves very good results in the sense of discrimination/generation tradeoff (judging by figure 1).
- It's an interesting finding that one can discretize dino-v2 via K-means and train a strong generative model on top of such tokens.
- The paper studies an important problem of more rigorous understanding of modern autoencoders

Weaknesses:
- The paper shows an equivalence between a linear AE and PCA, but it's a well known fact: https://arxiv.org/abs/1804.10253. One can also just google ""equivalence between a linear autoencoder and PCA"", and find a ton of resources on that.
- ""A reconstructive autoencoder does not necessarily establish an advantageous latent space for generative model"". That's a very well-known fact in the community (e.g., see Fig 5 in https://arxiv.org/pdf/2312.02116). The paper should not claim this observation as a novel contribution.
- The proposed stability metric is interesting, but it's unclear whether it will correlate with downstream generative models performance
- Proposition 2.4 is extremely vague and seems to be very different from its ""rigorous"" analogue in the supplementary.
- FID metrics for VQGAN on ImageNet are much higher than in the original paper.
- It's delusive to compare performance of the developed model vs those trained from scratch, since the developed model starts from strong pre-trained models.
- For image generation, the paper shows just 16 random samples, which is extremely little to get a good understanding of the model. It's better to show much more (e.g. stylegan2 provides 100k samples for FFHQ: https://github.com/NVlabs/stylegan2).
- Why DiT-XL/2 is included for comparison but not its guided variants? Why more recent papers are not included for comparison? (e.g., EDMv2).
- The logical transitions in the paper are unclear, e.g., it's unclear why the proposed training improves D^G, it's unclear, why it follows from the propositions that we should improve the stability of the latent space (where stability is also not defined well), etc.

Limitations:
One limitation that is not explored is whether the model is upper-bounded by the performance of the underlying SSL classifier. In other words, what would be the greater source of improved performance in the future — improving SSL or decoder?

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tries to understand why latent autoregressive image models perform worse than latent diffusion models. The key insight is that existing tokenizers are trained primarily with the reconstruction objective, whose latent space is unstable and thus may not be easy to model autoregressively. To solve this issue, the authors propose first learning a stable latent space, which autoregressive models can model easily, and then learning to reconstruct pixels from this latent space. Experimental results show that this modification enables latent autoregressive image models to match latent diffusion models' performance in terms of image understanding and image generation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper proposed a new perspective—latent space stability—on understanding latent autoregressive image models, which was neglected in previous works. I think this explanation is intuitive since a fixed-depth autoregressive model may not be able to model very noisy distributions (e.g., the language data have high regularity)
2. The proposed solution is straightforward -- just let image features with similar semantics share the same token.
3. The experiments are comprehensive and interesting. Both image understanding and image generation are evaluated; improvements over previous latent autoregressive models are significant. The ablation study also makes sense to me.

Weaknesses:
1. I think there is a tension between how stable the latent space is and how easily we can reconstruct the latent codes to pixels. The impact of the proposed method on reconstruction is not elaborated in this paper. For example, if we only care about reconstruction, how badly does the proposed method perform? This matters greatly if we are modeling high-resolution images and care about the visual details.
2. The theoretical analysis and the proposed algorithm seem loosely connected to me -- I don't see the proposed algorithm as a direct result of the theoretical analysis. The stability analysis is more straightforward, though.

Limitations:
I think the authors adequately addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper propose to disentangle the encoder and decoder learning for image tokenzier which ultimately will be used for providing the latent space of AR generative model. In particular, SSL model such as DinoV2 is used for encoder (plus k-means clustering).

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
1. The idea of disentangling the encoder and decoder learning for image tokenizier is interesting and novel.

2. Strong empirical results can be obtained from the method. The fact that by changing a tokenizer and training the same AR model, FID can be halved to half is really impressive.

Weaknesses:
1. The motivation for adopting self-supervised model as encoder/tokenizer is not very clear. Since the method is easy (DinoV2 + kmeans), the motivation of why doing so is the most critical part of the paper. However, I don't think this is presented very clearly and explicitly. Large improvements of the presentation is needed. 

2. The term ""stability"" or """"stablize"" is a bit confusing. Explicit explanation is needed. When is a latent space not stable? If it means hard to learn an AR model, probably a better term such as learnability is better. 

3. While the argument of ""iterative decoding process can stabilize the sampling process by correcting the data falling in the low-density overlap between distributions"" makes sense, it still requires justification and evidence, not just conceptual analysis.

4. If you use SSL model as encoder, you need to train a decoder. Not much explicit detail is presented for this part.

5. The metric is not very clearly defined. What's the name of the metric? What is the definition? How to compute it? All these information should be highlighted.

Overall the presentation and organization is not very clear, some major rewrite is needed.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
Latent-based image generative models, such as LDMs and MIMs, have achieved success, but autoregressive models lag behind in image generation. Our research introduces a unified perspective on latent space stability and proposes a discrete image tokenizer, DiGIT, that significantly improves autoregressive image modeling, outperforming LDMs and benefiting from scaling similar to GPT in NLP.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
2: fair

Strengths:
- The results beat some baseline models, though under a specific (and somewhat confused) experimental setting.
- The topic of latent space property is worth investigating.

Weaknesses:
The paper has several weaknesses:

1. **Factual Errors**:

    1.1. The cited MIM models, such as MaskGIT and MAGE, cannot revise previously predicted tokens. This contradicts the claim in line 53 that ""iterative models like LDMs and MIMs can correct errors."" I recommend the authors to their papers for more details.

    1.2. In lines 72-73, the authors state that this work provides ""the first evidence that image autoregressive generative models behave analogously to GPT."" However, the Parti[1] paper has already demonstrated that image autoregressive models have similar scalability to GPT and successfully scaled the model to 20B. The authors have not cited this work.

2. The writing is poor and lacks rigor. For example, the discussion on the so-called ""common misconception"" in line 41 is not well-supported. What exactly is meant by the ""optimal latent space for reconstruction""? How many studies hold this view? There are no citations provided.

3. The quantitative comparisons are also peculiar. The authors cite many paper results without using CFG, while CFG has become a de-facto practice for augmenting generative models. Why not adopt CFG and perform more apples-to-apples comparisons to other SOTA methods with CFG?

4. Presenting two tables (Table 2 lower and Table 3) for image generation performance is confusing. Why not consolidate the results into a single, clear table?

[1] Yu, Jiahui, et al. ""Scaling autoregressive models for content-rich text-to-image generation."" arXiv preprint arXiv:2206.10789 2.3 (2022): 5.

Limitations:
The writing & presentation of this paper seems too rush and lacks rigor. I recommend the authors to refine and polish this paper. The current draft may not be qualified for the publication of NeurIPS.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper designs a better quantized autoencoder on top of VQGAN. It builds an image autoencoder which is able to both achieve good recognition performance for linear probing, and have a latent space which is suitable for training a generative model. It studies the existing autoencoders from a high-level theoretical perspective and proposes design ideas which are targeted to improve them. The paper claims that the modern autoencoders ignore the fact that they will be utilized for downstream generative modelling tasks and mainly focus on reconstruction. The paper argues that adding recognition losses on top of the encoder would help. To fulfill this desiderata, the model takes DINOv2 features and discretizes them via k-means. Then it trains a translation model into VQ-GAN decoder features. For image generation, it trains a LLM in the discrete token space. For classification, it does linear probing on top of discretized DINOv2 features. As a result, it attains reasonable generative capabilities while being able to keep a latent space suitable for linear probing classification.

Soundness:
1: poor

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- In terms of the scores, the paper achieves very good results in the sense of discrimination/generation tradeoff (judging by figure 1).
- It's an interesting finding that one can discretize dino-v2 via K-means and train a strong generative model on top of such tokens.
- The paper studies an important problem of more rigorous understanding of modern autoencoders

Weaknesses:
- The paper shows an equivalence between a linear AE and PCA, but it's a well known fact: https://arxiv.org/abs/1804.10253. One can also just google ""equivalence between a linear autoencoder and PCA"", and find a ton of resources on that.
- ""A reconstructive autoencoder does not necessarily establish an advantageous latent space for generative model"". That's a very well-known fact in the community (e.g., see Fig 5 in https://arxiv.org/pdf/2312.02116). The paper should not claim this observation as a novel contribution.
- The proposed stability metric is interesting, but it's unclear whether it will correlate with downstream generative models performance
- Proposition 2.4 is extremely vague and seems to be very different from its ""rigorous"" analogue in the supplementary.
- FID metrics for VQGAN on ImageNet are much higher than in the original paper.
- It's delusive to compare performance of the developed model vs those trained from scratch, since the developed model starts from strong pre-trained models.
- For image generation, the paper shows just 16 random samples, which is extremely little to get a good understanding of the model. It's better to show much more (e.g. stylegan2 provides 100k samples for FFHQ: https://github.com/NVlabs/stylegan2).
- Why DiT-XL/2 is included for comparison but not its guided variants? Why more recent papers are not included for comparison? (e.g., EDMv2).
- The logical transitions in the paper are unclear, e.g., it's unclear why the proposed training improves D^G, it's unclear, why it follows from the propositions that we should improve the stability of the latent space (where stability is also not defined well), etc.

Limitations:
One limitation that is not explored is whether the model is upper-bounded by the performance of the underlying SSL classifier. In other words, what would be the greater source of improved performance in the future — improving SSL or decoder?

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper tries to understand why latent autoregressive image models perform worse than latent diffusion models. The key insight is that existing tokenizers are trained primarily with the reconstruction objective, whose latent space is unstable and thus may not be easy to model autoregressively. To solve this issue, the authors propose first learning a stable latent space, which autoregressive models can model easily, and then learning to reconstruct pixels from this latent space. Experimental results show that this modification enables latent autoregressive image models to match latent diffusion models' performance in terms of image understanding and image generation.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper proposed a new perspective—latent space stability—on understanding latent autoregressive image models, which was neglected in previous works. I think this explanation is intuitive since a fixed-depth autoregressive model may not be able to model very noisy distributions (e.g., the language data have high regularity)
2. The proposed solution is straightforward -- just let image features with similar semantics share the same token.
3. The experiments are comprehensive and interesting. Both image understanding and image generation are evaluated; improvements over previous latent autoregressive models are significant. The ablation study also makes sense to me.

Weaknesses:
1. I think there is a tension between how stable the latent space is and how easily we can reconstruct the latent codes to pixels. The impact of the proposed method on reconstruction is not elaborated in this paper. For example, if we only care about reconstruction, how badly does the proposed method perform? This matters greatly if we are modeling high-resolution images and care about the visual details.
2. The theoretical analysis and the proposed algorithm seem loosely connected to me -- I don't see the proposed algorithm as a direct result of the theoretical analysis. The stability analysis is more straightforward, though.

Limitations:
I think the authors adequately addressed the limitations

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper propose to disentangle the encoder and decoder learning for image tokenzier which ultimately will be used for providing the latent space of AR generative model. In particular, SSL model such as DinoV2 is used for encoder (plus k-means clustering).

Soundness:
3: good

Presentation:
1: poor

Contribution:
3: good

Strengths:
1. The idea of disentangling the encoder and decoder learning for image tokenizier is interesting and novel.

2. Strong empirical results can be obtained from the method. The fact that by changing a tokenizer and training the same AR model, FID can be halved to half is really impressive.

Weaknesses:
1. The motivation for adopting self-supervised model as encoder/tokenizer is not very clear. Since the method is easy (DinoV2 + kmeans), the motivation of why doing so is the most critical part of the paper. However, I don't think this is presented very clearly and explicitly. Large improvements of the presentation is needed. 

2. The term ""stability"" or """"stablize"" is a bit confusing. Explicit explanation is needed. When is a latent space not stable? If it means hard to learn an AR model, probably a better term such as learnability is better. 

3. While the argument of ""iterative decoding process can stabilize the sampling process by correcting the data falling in the low-density overlap between distributions"" makes sense, it still requires justification and evidence, not just conceptual analysis.

4. If you use SSL model as encoder, you need to train a decoder. Not much explicit detail is presented for this part.

5. The metric is not very clearly defined. What's the name of the metric? What is the definition? How to compute it? All these information should be highlighted.

Overall the presentation and organization is not very clear, some major rewrite is needed.

Limitations:
N/A

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces MomentumSMoE, a novel integration of heavy-ball momentum into Sparse Mixture of Experts (SMoE) to enhance stability and robustness. It establishes a connection between SMoE and gradient descent on multi-objective optimization problems.
The paper demonstrates theoretical and empirical improvements of MomentumSMoE over standard SMoE across various tasks. The method is universally applicable to many SMoE models, including V-MoE and GLaM, with minimal additional computational cost.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
To the best of my knowledge, attempting to accelerate the fixed point iteration in SMoE is an original idea.

It seems like there is comprehensive empirical evidence for the method, but I am not an expert on metrics for the SMoE, and will have to rely on other reviews to be confident in this strength.

The paper is fairly clear, with well-organized sections and figures.

Weaknesses:
My largest negative for this paper is the largely unfounded connection between the SMoE and gradient descent.  If the authors had made a connection to accelerating fixed-point iterations in general, I would want to accept this paper.  Essentially, the authors are assuming that $\nabla_x f$ has strictly real eigenvalues when they should just work with truly, potentially complex, eigenvalues, ex., using tools as in Azizian et. al.  For example, when performing this analysis, various other acceleration schemes are often better, like negative momentum (Gidel et. al.) or complex momentum (Lorraine et. al.).  I would be curious to see some empirical investigation (or theoretical) or what the eigenvalues of $\nabla_x f$ are – ex., as in Figure 7 of https://arxiv.org/pdf/2102.08431 -- to validate any theoretical claims about what acceleration schemes should be used.

But, of course, the spectrum is only known in small-scale problems, leading to the second weakness, which is that some of the methods – ex., RobustSMoE – seem to rely on knowing the spectrum to set various parameters, which we won’t have access in real settings.  Th

The theoretical results are also largely just reproductions of known theoretical results for momentum once you assume that the update from the SMoE is a gradient. This makes them not much of a contribution from my point of view other than leveraging existing tools. I think these results could be easily substituted for analogous techniques from Azizian.

Azizian, Waïss, et al. ""Accelerating smooth games by manipulating spectral shapes."" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.
Lorraine, Jonathan P., et al. ""Complex momentum for optimization in games."" International Conference on Artificial Intelligence and Statistics. PMLR, 2022.
Gidel, Gauthier, et al. ""Negative momentum for improved game dynamics."" The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019.

Limitations:
The limitations are discussed briefly, but a delineated section elaborating on all the limitations would be valuable.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a variant of sparse mixture of experts, MomentumSMoE, by incorporating momentum into the traditional sparse mixture of experts framework. The authors provide both theoretical proofs and empirical evidence demonstrating that MomentumSMoE offers greater stability and robustness compared to the standard sparse mixture of experts. Experiments on language modeling and object recognition tasks are conducted to verify the effectiveness of the proposal.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea of integrating momentum into sparse mixture of experts  is interesting. 
2. Both the theoretical proof and extensive empirical results are provided to demonstrate that the proposed MomentumSMoE is more stable and robust than SmoE; the experimental results are appealing.
3. The code is provided.

Weaknesses:
The pseudocode may be provided to better illustrate the implementation of the proposal.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel approach to enhancing the robustness and stability of Sparse Mixture of Experts (SMoE) models. Inspired by the analogy of gradient descent and SMoE, the authors develop a family of models by incorporating momentum into the training process. The key idea is that training SMoE is a multi-objective optimization problem where the monument-based gradient descent method is more stable and robust than the vanilla one. They proposed the AdamSMoE and Robust MomentumSMoE, which demonstrate improved performance across a variety of tasks, including language modeling and object recognition.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
(1) The integration of momentum into SMoE is a non-trivial innovation that addresses instability and inefficiency issues in existing models.

(2) The paper provides convincing empirical evidence showing the effectiveness of MomentumSMoE across multiple benchmarks.

(3) The proposed method's compatibility with other momentum-based optimizers, like Adam, suggests it can be broadly applied to various SMoE architectures.

Weaknesses:
(1) Formulating SMoE as a multi-objective optimization problem is doubtful to me. Every expert network is continually changing during the model training, which makes each objective nonstatic, which violates the basic assumption of multi-objective optimization, whose objectives should be very clear and stable. 

(2) It is unconvincing to use ||f(x)|| as the key metrics to measure the efficacy of SMoE or MoE. This confuses me a lot. Please explain why the output norm represents the goodness/badness of the model.

(3) There are some grammar issues. Please use `` instead of "" in the paper (line 665).

(4) There is no sufficient discussion of computation overhead. Training efficiency is a critical issue for current foundation model training. Does computation significantly increase by applying momentum over the SMoE? Keeping an additional copy weight (p in Fig 1) would take additional memory and may decrease the throughput.

I'd like to hear a more insightful discussion regarding all the points above from the authors.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the instability problem of training SMoE models. By establishing a relationship between SMoE and multi-objective optimization, the authors integrate momentum into SMoE and propose MomentumSMoE. Experimental results show that MomentumSMoE is more stable than SMoE during training.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper tackles a critical issue in the training of SMoE models.

2. The proposed method is generalizable and can be applied to various SMoE models such as V-MoE and GLaM.

3. Experimental results demonstrate that this method is more stable than SMoE during the training process.

Weaknesses:
1. This method has little effect on models with few layers.

2. The largest models for evaluation only have  388M parameters, which are much smaller than mainstream MoE LLMs.

3. From a theoretical standpoint, developing a framework to explain the enhanced robustness of MomentumSMoE would be interesting.

Limitations:
The authors have adequately discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces MomentumSMoE, a novel integration of heavy-ball momentum into Sparse Mixture of Experts (SMoE) to enhance stability and robustness. It establishes a connection between SMoE and gradient descent on multi-objective optimization problems.
The paper demonstrates theoretical and empirical improvements of MomentumSMoE over standard SMoE across various tasks. The method is universally applicable to many SMoE models, including V-MoE and GLaM, with minimal additional computational cost.

Soundness:
3: good

Presentation:
3: good

Contribution:
4: excellent

Strengths:
To the best of my knowledge, attempting to accelerate the fixed point iteration in SMoE is an original idea.

It seems like there is comprehensive empirical evidence for the method, but I am not an expert on metrics for the SMoE, and will have to rely on other reviews to be confident in this strength.

The paper is fairly clear, with well-organized sections and figures.

Weaknesses:
My largest negative for this paper is the largely unfounded connection between the SMoE and gradient descent.  If the authors had made a connection to accelerating fixed-point iterations in general, I would want to accept this paper.  Essentially, the authors are assuming that $\nabla_x f$ has strictly real eigenvalues when they should just work with truly, potentially complex, eigenvalues, ex., using tools as in Azizian et. al.  For example, when performing this analysis, various other acceleration schemes are often better, like negative momentum (Gidel et. al.) or complex momentum (Lorraine et. al.).  I would be curious to see some empirical investigation (or theoretical) or what the eigenvalues of $\nabla_x f$ are – ex., as in Figure 7 of https://arxiv.org/pdf/2102.08431 -- to validate any theoretical claims about what acceleration schemes should be used.

But, of course, the spectrum is only known in small-scale problems, leading to the second weakness, which is that some of the methods – ex., RobustSMoE – seem to rely on knowing the spectrum to set various parameters, which we won’t have access in real settings.  Th

The theoretical results are also largely just reproductions of known theoretical results for momentum once you assume that the update from the SMoE is a gradient. This makes them not much of a contribution from my point of view other than leveraging existing tools. I think these results could be easily substituted for analogous techniques from Azizian.

Azizian, Waïss, et al. ""Accelerating smooth games by manipulating spectral shapes."" International Conference on Artificial Intelligence and Statistics. PMLR, 2020.
Lorraine, Jonathan P., et al. ""Complex momentum for optimization in games."" International Conference on Artificial Intelligence and Statistics. PMLR, 2022.
Gidel, Gauthier, et al. ""Negative momentum for improved game dynamics."" The 22nd International Conference on Artificial Intelligence and Statistics. PMLR, 2019.

Limitations:
The limitations are discussed briefly, but a delineated section elaborating on all the limitations would be valuable.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a variant of sparse mixture of experts, MomentumSMoE, by incorporating momentum into the traditional sparse mixture of experts framework. The authors provide both theoretical proofs and empirical evidence demonstrating that MomentumSMoE offers greater stability and robustness compared to the standard sparse mixture of experts. Experiments on language modeling and object recognition tasks are conducted to verify the effectiveness of the proposal.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The idea of integrating momentum into sparse mixture of experts  is interesting. 
2. Both the theoretical proof and extensive empirical results are provided to demonstrate that the proposed MomentumSMoE is more stable and robust than SmoE; the experimental results are appealing.
3. The code is provided.

Weaknesses:
The pseudocode may be provided to better illustrate the implementation of the proposal.

Limitations:
The authors adequately addressed the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces a novel approach to enhancing the robustness and stability of Sparse Mixture of Experts (SMoE) models. Inspired by the analogy of gradient descent and SMoE, the authors develop a family of models by incorporating momentum into the training process. The key idea is that training SMoE is a multi-objective optimization problem where the monument-based gradient descent method is more stable and robust than the vanilla one. They proposed the AdamSMoE and Robust MomentumSMoE, which demonstrate improved performance across a variety of tasks, including language modeling and object recognition.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
(1) The integration of momentum into SMoE is a non-trivial innovation that addresses instability and inefficiency issues in existing models.

(2) The paper provides convincing empirical evidence showing the effectiveness of MomentumSMoE across multiple benchmarks.

(3) The proposed method's compatibility with other momentum-based optimizers, like Adam, suggests it can be broadly applied to various SMoE architectures.

Weaknesses:
(1) Formulating SMoE as a multi-objective optimization problem is doubtful to me. Every expert network is continually changing during the model training, which makes each objective nonstatic, which violates the basic assumption of multi-objective optimization, whose objectives should be very clear and stable. 

(2) It is unconvincing to use ||f(x)|| as the key metrics to measure the efficacy of SMoE or MoE. This confuses me a lot. Please explain why the output norm represents the goodness/badness of the model.

(3) There are some grammar issues. Please use `` instead of "" in the paper (line 665).

(4) There is no sufficient discussion of computation overhead. Training efficiency is a critical issue for current foundation model training. Does computation significantly increase by applying momentum over the SMoE? Keeping an additional copy weight (p in Fig 1) would take additional memory and may decrease the throughput.

I'd like to hear a more insightful discussion regarding all the points above from the authors.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper addresses the instability problem of training SMoE models. By establishing a relationship between SMoE and multi-objective optimization, the authors integrate momentum into SMoE and propose MomentumSMoE. Experimental results show that MomentumSMoE is more stable than SMoE during training.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The paper tackles a critical issue in the training of SMoE models.

2. The proposed method is generalizable and can be applied to various SMoE models such as V-MoE and GLaM.

3. Experimental results demonstrate that this method is more stable than SMoE during the training process.

Weaknesses:
1. This method has little effect on models with few layers.

2. The largest models for evaluation only have  388M parameters, which are much smaller than mainstream MoE LLMs.

3. From a theoretical standpoint, developing a framework to explain the enhanced robustness of MomentumSMoE would be interesting.

Limitations:
The authors have adequately discussed the limitations.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,Yes,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces a novel activation function called Quantile Activation (QACT), which aims to improve the robustness of neural networks against various data distortions. The authors propose an end-to-end framework that combines QACT with modified loss functions and quantile classifiers, evaluating their approach on several benchmark datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well organized and clearly written.
1. The paper delivers useful empirical and theoretical insights.
1. The experimental results showcase the superiority of the proposed method.

Weaknesses:
1. The proposed method seems a bit complex, which may lead to over-fitting in scenarios with limited training data.
1. Although the experiments are promising, it remains unclear how well the proposed method would scale to larger datasets or more complex tasks beyond those tested in the paper.
1. I feel that the evaluations are somewhat limited as only a few methods are compared against, lacking the most recent SOTAs. This limits the understanding of the real technical contribution of the proposed method.

Limitations:
The limitations should be discussed in the main paper, yet they are provided in the checklist.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces Quantile Activation (QACT) to enhance classification model robustness against distributional shifts. Unlike traditional classifiers, QACT outputs the relative quantile of a sample in its context distribution, allowing for context-dependent classification. Validated on datasets like CIFAR10C and MNISTC, QACT improves generalization and robustness, outperforming state-of-the-art models like DINOv2 under large distortions. The paper details QACT's implementation and suggests future research directions, including scaling and exploring theoretical links to biological neurons.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper is notable for its originality in proposing a context-aware activation function, demonstrates high quality through extensive validation, and has significant potential for enhancing generalization in classification models. The innovative use of quantile-based activations opens new opportunities for research and applications in machine learning.

Weaknesses:
- Lack of Clarity on Context Dependency

The concept of context dependency being batch-dependent is not clearly explained until the conclusion of the paper. This crucial detail should be introduced and elaborated on earlier to provide a better understanding of the method.

- Unclear Motivation in Introduction

The motivation and fundamental problem discussed in the introduction are not clearly articulated. The authors mention that, unlike NLP where context is considered, general classification systems do not incorporate context. However, in Vision Transformers (ViTs), image patches are treated similarly to words in NLP ""[...The meaning of a word is dependent on the context of the word. However, to our knowledge, this has not been considered for general classification systems.]"". In ViTs, an image patch is considered like a word, and the context comes from the other image patches.

- Insufficient Related Work on Robustness

The paper lacks a comprehensive review of related work concerning robustness to input distortions. Including a discussion of existing methods and how QACT compares or improves upon them would strengthen the paper.

- Limited Comparative Analysis

The comparison with other methods addressing robustness to input distortions is insufficient. The authors primarily compare QACT with DINOv2-Small, which is not a standard model for robustness. Including comparisons with other state-of-the-art methods specifically designed for robustness would provide a more complete evaluation of QACT's performance.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a new activation function called quantile activation (QACT) which outputs the relative quantile of the sample in the context distribution. Furthermore, the paper validates the proposed activation across several experimental settings, and compare it with conventional techniques. They test robustness against distortions, and find that the proposed activation can achieve a significantly higher generalization across distortions than the conventional classifiers, across different architectures.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
First, the authors develop existing approach in calibrating a pre-trained classifier to the level of a neuron. Thus, suitable forward and backward propagation equations required for learning are derived. Second, the authors also show that the extension can produce context dependent outputs at the level of each neuron of the neural network.

Weaknesses:
The writing of the paper meets the standard, but the notations are confusing. Nevertheless, it would be much better if the authors can polish and clarify them. For instance:
1. in line 107, the authors claim that ‘Assign $\mathbf y=1$ whenever $\mathbf y > (1-\tau)^{th}$ quantile of $\mathbf z$’. It seems that $\mathbf z$ is a vector and is impossible to have a vector be larger than a scalar. 
2. The authors write $z_i$ and $\mathbf z_i$ alternatively to mean the same quantity. Similar situations occur when the authors write $z$ and $\mathbf z$ (see line 119, Eqn. (4)), or QACT$(\textbf z)$ and QACT$(\mathbf z)$ (see lines 119 and 124).
3. The authors use bold lowercase letters to represent vectors (e.g., Eq. (1)) and variable distributions (e.g., lines 105, 106). Also, what is the difference between bold lowercase letters and normal lowercase letters?
Further clarifications can increase the readability of the paper.

Limitations:
Please see the questions and weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces a novel activation function called Quantile Activation (QACT), which aims to improve the robustness of neural networks against various data distortions. The authors propose an end-to-end framework that combines QACT with modified loss functions and quantile classifiers, evaluating their approach on several benchmark datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. The paper is well organized and clearly written.
1. The paper delivers useful empirical and theoretical insights.
1. The experimental results showcase the superiority of the proposed method.

Weaknesses:
1. The proposed method seems a bit complex, which may lead to over-fitting in scenarios with limited training data.
1. Although the experiments are promising, it remains unclear how well the proposed method would scale to larger datasets or more complex tasks beyond those tested in the paper.
1. I feel that the evaluations are somewhat limited as only a few methods are compared against, lacking the most recent SOTAs. This limits the understanding of the real technical contribution of the proposed method.

Limitations:
The limitations should be discussed in the main paper, yet they are provided in the checklist.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper introduces Quantile Activation (QACT) to enhance classification model robustness against distributional shifts. Unlike traditional classifiers, QACT outputs the relative quantile of a sample in its context distribution, allowing for context-dependent classification. Validated on datasets like CIFAR10C and MNISTC, QACT improves generalization and robustness, outperforming state-of-the-art models like DINOv2 under large distortions. The paper details QACT's implementation and suggests future research directions, including scaling and exploring theoretical links to biological neurons.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper is notable for its originality in proposing a context-aware activation function, demonstrates high quality through extensive validation, and has significant potential for enhancing generalization in classification models. The innovative use of quantile-based activations opens new opportunities for research and applications in machine learning.

Weaknesses:
- Lack of Clarity on Context Dependency

The concept of context dependency being batch-dependent is not clearly explained until the conclusion of the paper. This crucial detail should be introduced and elaborated on earlier to provide a better understanding of the method.

- Unclear Motivation in Introduction

The motivation and fundamental problem discussed in the introduction are not clearly articulated. The authors mention that, unlike NLP where context is considered, general classification systems do not incorporate context. However, in Vision Transformers (ViTs), image patches are treated similarly to words in NLP ""[...The meaning of a word is dependent on the context of the word. However, to our knowledge, this has not been considered for general classification systems.]"". In ViTs, an image patch is considered like a word, and the context comes from the other image patches.

- Insufficient Related Work on Robustness

The paper lacks a comprehensive review of related work concerning robustness to input distortions. Including a discussion of existing methods and how QACT compares or improves upon them would strengthen the paper.

- Limited Comparative Analysis

The comparison with other methods addressing robustness to input distortions is insufficient. The authors primarily compare QACT with DINOv2-Small, which is not a standard model for robustness. Including comparisons with other state-of-the-art methods specifically designed for robustness would provide a more complete evaluation of QACT's performance.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a new activation function called quantile activation (QACT) which outputs the relative quantile of the sample in the context distribution. Furthermore, the paper validates the proposed activation across several experimental settings, and compare it with conventional techniques. They test robustness against distortions, and find that the proposed activation can achieve a significantly higher generalization across distortions than the conventional classifiers, across different architectures.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
First, the authors develop existing approach in calibrating a pre-trained classifier to the level of a neuron. Thus, suitable forward and backward propagation equations required for learning are derived. Second, the authors also show that the extension can produce context dependent outputs at the level of each neuron of the neural network.

Weaknesses:
The writing of the paper meets the standard, but the notations are confusing. Nevertheless, it would be much better if the authors can polish and clarify them. For instance:
1. in line 107, the authors claim that ‘Assign $\mathbf y=1$ whenever $\mathbf y > (1-\tau)^{th}$ quantile of $\mathbf z$’. It seems that $\mathbf z$ is a vector and is impossible to have a vector be larger than a scalar. 
2. The authors write $z_i$ and $\mathbf z_i$ alternatively to mean the same quantity. Similar situations occur when the authors write $z$ and $\mathbf z$ (see line 119, Eqn. (4)), or QACT$(\textbf z)$ and QACT$(\mathbf z)$ (see lines 119 and 124).
3. The authors use bold lowercase letters to represent vectors (e.g., Eq. (1)) and variable distributions (e.g., lines 105, 106). Also, what is the difference between bold lowercase letters and normal lowercase letters?
Further clarifications can increase the readability of the paper.

Limitations:
Please see the questions and weaknesses.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,No,0.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper tries to formalize the concept of generalizability in experimental studies in machine learning research. It relies on three different types of kernels in order to quantify difference between the rankings in an experiment output. A core contribution is the development of an algorithm for estimating the number of experimental studies needed in order to generalize the results at a desired level.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The topic is interesting and worthwhile.
- The paper is clearly written.
- The formalization of generalizability is well-defined and nicely parameterized through the use of kernels.
- The practical usefulness of the algorithm is somewhat unclear to me.

Weaknesses:
- There is no discussion on the computational costs of the algorithm (except for a vague statement that it is very fast in the checklist).
- The empirical evidence for the algorithm's effectiveness appears somewhat weak to me (see respective item in _Questions_).
- The python package is not properly configured I think. I see this after having installed the package into a virtual environment with the correct python version and
  using `pip install . -r requirements.txt`:

  ```python
  In [1]: import genexpy
  ---------------------------------------------------------------------------
  ImportError                               Traceback (most recent call last)
  Cell In[1], line 1
  ----> 1 import genexpy

  File ~/.pyenv/versions/genexpy/lib/python3.11/site-packages/genexpy/__init__.py:4
        2 from .src import lower_bounds
        3 from .src import mmd
  ----> 4 from .src import probability_distributions
        5 from .src import rankings_utils
        6 from .src import relation_utils

  File ~/.pyenv/versions/genexpy/lib/python3.11/site-packages/genexpy/src/probability_distributions.py
  :11
        8 from typing import Literal
      10 from genexpy import kernels as ku
  ---> 11 from genexpy import rankings_utils as ru
      12 from genexpy import relation_utils as rlu
      15 def sample_from_sphere(na: int, n: int, rng: np.random.Generator) -> np.ndarray[float]:

  ImportError: cannot import name 'rankings_utils' from partially initialized module 'genexpy' (most l
  ikely due to a circular import) (/home/<anonymous_reviewer>/.pyenv/versions/genexpy/lib/python3.11/site-packages
  /genexpy/__init__.py)

Limitations:
Some of the limitations are discussed but I still think the paper could be more self-critical of for instance $n^*$. Possible computational costs are also not discussed.

There are no potential negative societal impacts of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper deals with experimental studies. After providing a mathematical formalization, it focuses on the generalizability of these studies. The main contribution is a quantitative estimate of the the size of the study to obtain generalizable results. Experiments on LLMs are conducted.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- [mathematical formulation] It is nice to have a solid formulation of experimental studies, this is quite relevant to the community.

Weaknesses:
- [train / test split] A concrete problem in machine learning practical experimentation is that of train / test split, and more particularly its absence (that is, training on the test). I do not see this issue discussed in the paper. Can it be incorporated in the setting? Is it possible to clarify whether the paper assumes that the training is done on a training set without calibration on a validation set or is this hidden somewhere? What would then be the influence on the number of experiments?
- [testing between rankings] If I understand correctly, the paper proposes (in Section 4.1) to check whether rankings are consistent by performing kernel two-sample test, with adapted kernels. This does not seem standard to me: there exists some ad-hoc statistical tests (e.g., Kendall's \tau, Spearman's \rho, etc.). Why not use them directly? Is there an advantage to using MMD?  

- [minor comments]:
  - missing ref line 111
  - repeated word ('of') line 300

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tries to formalise the notion of an experimental study by considering the sampling process of acquiring a dataset.  It then uses this notion to argue about generalisability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The problem of understanding the performance of machine learning when tested on new data is a very important problem.  The authors use some technically sophisticate methods to tackle this problem.

Weaknesses:
For me the authors model of an experiment is too simplistic and does not capture the problems faced by machine learning.  If we collect medical data then that data is likely to vary depending on the equipment used, the clinicians running the equipment and population where the data comes from.  These kinds of variations are the bugbear for machine learning, but not captured at all by the model.  Another issue is that a lot of data is non-stationary.  Even in the much used example of checkmate in one.  If a machine learns this very well, then players against the machine are likely to learn their mistake and alter their play.  Thus, I am not convinced that the model being proposed is particularly interesting.

Limitations:
This is fine.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors provide a formalism for the generalizability of experimental studies in ML.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Anything pushing to get better practices in evaluation of ML is very important.

Weaknesses:
I could quibble with some of the setup, which is a bit confusing to me: design factors being properties of the context rather than of the alternative, for example, is kind of odd, but I don't think this is very important.

The big problem is that there is a huge literature on a very similar problem and I have very little sense of how this work connects to it: starting with the Neyman-Pearson lemma and going down through the standard corpus of decision theory, we have a lot of statistical tools for thinking about this problem in very broad strokes. After reading this paper, I have a sense that you're trying to solve a very similar problem (given a sample from some population, what can I say about the reliability of my estimate? How many samples would I need to be sure that it's reliable?)

Section 4.3 seems to be rederiving some form of power analysis.

Looking at A.3.3, it appears that the procedure is essentially the following:
(1) [the inner loop from 1...n_rep] construct a null hypothesis at a given sample size, find the upper alpha quantile of that null hypothesis
(2) Repeat this at a variety of sample sizes
(3) Estimate a power-law relationship between sample size and the upper-alpha quantiles
(4) Predict the sample size which would have such an upper-alpha quantile

This procedure is an empirical version of power analysis where the null distribution is not known but simulated and extrapolated. If I know the type-I error rate, type-II error rate and a distribution under the null and under the alternative, deriving the required sample size is straightforward. Indeed, we have a CLT for MMD (at least under some kernels) [1], so these distributions are known asymptotically, which is likely plenty for the purposes of sample size determination. Do \alpha^* and \delta^* map onto concepts from Neyman-Pearson? It's entirely possible.

This is an important question because decision theory has very well established results on things like uniformly most powerful tests. When we just invent a new framework rather than relying on well-trod ones, we are likely to derive suboptimal procedures unless we compare very carefully to these existing procedures. There's no similarly sophisticated discussion of error properties in this paper, which would be reasonable if this were truly the first paper in its vein, but I don't think that's the case.

Further, it's not clear to me why these similarities between rankings should be the target of inference. Rather, shouldn't I care about whether, based on the sample of allowed-to-vary factors I've used, alternative A is preferred to alternative B? This is an extremely standard matter of decision theory as far as I can tell. By moving to these more complicated research questions about rankings it clouds this fact, but I'm not sure it needs to. If the target of inference were instead to be a rank of K alternatives, I believe a decision theorist would take a somewhat similar approach to what you've done here: define a similarity metric based on the research question. An example solution to a problem like this would be [2], [3]. I just don't see why we need this new framework to accomplish a task I think we already have the tools for.

It's entirely possible that there's a contribution here, but it can't just be ""this is a new task"". We have methods from decision theory that have been designed for a wide range of decision tasks, and its incumbent upon the authors to demonstrate why those existing tools do not fit the task in front of them.

[1] https://www.jmlr.org/papers/volume24/22-1136/22-1136.pdf
[2] https://onlinelibrary.wiley.com/doi/abs/10.1002/mcda.313
[3] https://www.sciencedirect.com/science/article/abs/pii/S0377221715008048

Limitations:
see above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a new mathematical framework and a corresponding new algorithm to evaluate the generalizability of published experimental studies, by adapting Montgomery's classification of experimental factors [44].
They demonstrate the efficacy of this framework in evaluating the generalizability of two popular published experimental studies.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper appears to be theoretical strong. 
It goes beyond the standard notion of reproducibility of an experimental study and comes up a definition of generalizability of an experimental study.
Morever, Proposition 4.2 provides a theoretical result on the sample size $n$ necessary to obtained a desired generalizability $\alpha^*$ for a desired similarity $\epsilon^*$, and the authors also provide an algorithm (A.3.3) to compute this sample size.
Since the similarity $\epsilon^*$ is hard to specify, they make it a function of the kernelized distance between rankings $\delta^*$.

2.
The empirical evaluation in Fig. 2 and Fig. 3, on the categorical encoder comparison from [41] or the BIG-bench framework for LLM comparison from [55], respectively, demonstrates the practical utility of the proposed approach in determining sample sizes to guarantee generalizability.

Weaknesses:
1. The clarity in the writing can be significantly improved:

1.A. Symbols are used before defining them, typos exist, and symbols are not used consistently:

1.A.a. On line 118, the symbol $\mathcal{R}_{n_a}$ is mentioned, but the relation of this symbol to the ranking on alternatives only becomes clear later in Definition 3.1.

1.A.b. The Section number is missing on line 111.

1.A.c. The symbol $m$, is defined as the number of shots, on line 115, whereas line 114 uses the symbol $n$ rather than $m$. Moreover, on line 88, $n$ is defined as the number of shots.

1.A.d. In contrast to 1.A.c, in eq. (1), after line 170, the symbol $n$ is now used without providing a definition. It now appears to be the size of any study, in a general definition, rather than the number of shots, as defined on line 88. 

1.A.e. On Sec. 5.3, line 315, $N$ is defined as the number of preliminary experiments, whereas on line 154, it is defined as the size of the sample of valid experimental conditions. Do these mean the same thing ?

1.B. Sec. 3.1 defines a ranking of alternatives as the primary result of an experimental study.
However, the effect size, i.e., the magnitude and sign of the difference between two alternatives, can be important in certain experiments.
The MMD kernel, used in Sec. 4.2, actually allows measuring this effect size, as discussed in [27], but the limitations imposed by the usage of this MMD kernel within the author's generalizability framework, are not clear despite the somewhat cryptic discussion in Sec. 6.

2. 
The experimental evaluation is limited to a comparison of ranking differences between alternatives, and does not include a measurement of the practical differences between alternatives, or the significance of these differences.

Limitations:
Please refer to the potential limitation underlying weakness #2. Is it possible to quantify the magnitude of differences between alternatives using the generalizability framework provided by the authors ? The authors mention this limitation in Sec. 6, but it is not clear why the MMD kernel cannot quantify magnitude of differences.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper tries to formalize the concept of generalizability in experimental studies in machine learning research. It relies on three different types of kernels in order to quantify difference between the rankings in an experiment output. A core contribution is the development of an algorithm for estimating the number of experimental studies needed in order to generalize the results at a desired level.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The topic is interesting and worthwhile.
- The paper is clearly written.
- The formalization of generalizability is well-defined and nicely parameterized through the use of kernels.
- The practical usefulness of the algorithm is somewhat unclear to me.

Weaknesses:
- There is no discussion on the computational costs of the algorithm (except for a vague statement that it is very fast in the checklist).
- The empirical evidence for the algorithm's effectiveness appears somewhat weak to me (see respective item in _Questions_).
- The python package is not properly configured I think. I see this after having installed the package into a virtual environment with the correct python version and
  using `pip install . -r requirements.txt`:

  ```python
  In [1]: import genexpy
  ---------------------------------------------------------------------------
  ImportError                               Traceback (most recent call last)
  Cell In[1], line 1
  ----> 1 import genexpy

  File ~/.pyenv/versions/genexpy/lib/python3.11/site-packages/genexpy/__init__.py:4
        2 from .src import lower_bounds
        3 from .src import mmd
  ----> 4 from .src import probability_distributions
        5 from .src import rankings_utils
        6 from .src import relation_utils

  File ~/.pyenv/versions/genexpy/lib/python3.11/site-packages/genexpy/src/probability_distributions.py
  :11
        8 from typing import Literal
      10 from genexpy import kernels as ku
  ---> 11 from genexpy import rankings_utils as ru
      12 from genexpy import relation_utils as rlu
      15 def sample_from_sphere(na: int, n: int, rng: np.random.Generator) -> np.ndarray[float]:

  ImportError: cannot import name 'rankings_utils' from partially initialized module 'genexpy' (most l
  ikely due to a circular import) (/home/<anonymous_reviewer>/.pyenv/versions/genexpy/lib/python3.11/site-packages
  /genexpy/__init__.py)

Limitations:
Some of the limitations are discussed but I still think the paper could be more self-critical of for instance $n^*$. Possible computational costs are also not discussed.

There are no potential negative societal impacts of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper deals with experimental studies. After providing a mathematical formalization, it focuses on the generalizability of these studies. The main contribution is a quantitative estimate of the the size of the study to obtain generalizable results. Experiments on LLMs are conducted.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- [mathematical formulation] It is nice to have a solid formulation of experimental studies, this is quite relevant to the community.

Weaknesses:
- [train / test split] A concrete problem in machine learning practical experimentation is that of train / test split, and more particularly its absence (that is, training on the test). I do not see this issue discussed in the paper. Can it be incorporated in the setting? Is it possible to clarify whether the paper assumes that the training is done on a training set without calibration on a validation set or is this hidden somewhere? What would then be the influence on the number of experiments?
- [testing between rankings] If I understand correctly, the paper proposes (in Section 4.1) to check whether rankings are consistent by performing kernel two-sample test, with adapted kernels. This does not seem standard to me: there exists some ad-hoc statistical tests (e.g., Kendall's \tau, Spearman's \rho, etc.). Why not use them directly? Is there an advantage to using MMD?  

- [minor comments]:
  - missing ref line 111
  - repeated word ('of') line 300

Limitations:
yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper tries to formalise the notion of an experimental study by considering the sampling process of acquiring a dataset.  It then uses this notion to argue about generalisability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The problem of understanding the performance of machine learning when tested on new data is a very important problem.  The authors use some technically sophisticate methods to tackle this problem.

Weaknesses:
For me the authors model of an experiment is too simplistic and does not capture the problems faced by machine learning.  If we collect medical data then that data is likely to vary depending on the equipment used, the clinicians running the equipment and population where the data comes from.  These kinds of variations are the bugbear for machine learning, but not captured at all by the model.  Another issue is that a lot of data is non-stationary.  Even in the much used example of checkmate in one.  If a machine learns this very well, then players against the machine are likely to learn their mistake and alter their play.  Thus, I am not convinced that the model being proposed is particularly interesting.

Limitations:
This is fine.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors provide a formalism for the generalizability of experimental studies in ML.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Anything pushing to get better practices in evaluation of ML is very important.

Weaknesses:
I could quibble with some of the setup, which is a bit confusing to me: design factors being properties of the context rather than of the alternative, for example, is kind of odd, but I don't think this is very important.

The big problem is that there is a huge literature on a very similar problem and I have very little sense of how this work connects to it: starting with the Neyman-Pearson lemma and going down through the standard corpus of decision theory, we have a lot of statistical tools for thinking about this problem in very broad strokes. After reading this paper, I have a sense that you're trying to solve a very similar problem (given a sample from some population, what can I say about the reliability of my estimate? How many samples would I need to be sure that it's reliable?)

Section 4.3 seems to be rederiving some form of power analysis.

Looking at A.3.3, it appears that the procedure is essentially the following:
(1) [the inner loop from 1...n_rep] construct a null hypothesis at a given sample size, find the upper alpha quantile of that null hypothesis
(2) Repeat this at a variety of sample sizes
(3) Estimate a power-law relationship between sample size and the upper-alpha quantiles
(4) Predict the sample size which would have such an upper-alpha quantile

This procedure is an empirical version of power analysis where the null distribution is not known but simulated and extrapolated. If I know the type-I error rate, type-II error rate and a distribution under the null and under the alternative, deriving the required sample size is straightforward. Indeed, we have a CLT for MMD (at least under some kernels) [1], so these distributions are known asymptotically, which is likely plenty for the purposes of sample size determination. Do \alpha^* and \delta^* map onto concepts from Neyman-Pearson? It's entirely possible.

This is an important question because decision theory has very well established results on things like uniformly most powerful tests. When we just invent a new framework rather than relying on well-trod ones, we are likely to derive suboptimal procedures unless we compare very carefully to these existing procedures. There's no similarly sophisticated discussion of error properties in this paper, which would be reasonable if this were truly the first paper in its vein, but I don't think that's the case.

Further, it's not clear to me why these similarities between rankings should be the target of inference. Rather, shouldn't I care about whether, based on the sample of allowed-to-vary factors I've used, alternative A is preferred to alternative B? This is an extremely standard matter of decision theory as far as I can tell. By moving to these more complicated research questions about rankings it clouds this fact, but I'm not sure it needs to. If the target of inference were instead to be a rank of K alternatives, I believe a decision theorist would take a somewhat similar approach to what you've done here: define a similarity metric based on the research question. An example solution to a problem like this would be [2], [3]. I just don't see why we need this new framework to accomplish a task I think we already have the tools for.

It's entirely possible that there's a contribution here, but it can't just be ""this is a new task"". We have methods from decision theory that have been designed for a wide range of decision tasks, and its incumbent upon the authors to demonstrate why those existing tools do not fit the task in front of them.

[1] https://www.jmlr.org/papers/volume24/22-1136/22-1136.pdf
[2] https://onlinelibrary.wiley.com/doi/abs/10.1002/mcda.313
[3] https://www.sciencedirect.com/science/article/abs/pii/S0377221715008048

Limitations:
see above

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors propose a new mathematical framework and a corresponding new algorithm to evaluate the generalizability of published experimental studies, by adapting Montgomery's classification of experimental factors [44].
They demonstrate the efficacy of this framework in evaluating the generalizability of two popular published experimental studies.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The paper appears to be theoretical strong. 
It goes beyond the standard notion of reproducibility of an experimental study and comes up a definition of generalizability of an experimental study.
Morever, Proposition 4.2 provides a theoretical result on the sample size $n$ necessary to obtained a desired generalizability $\alpha^*$ for a desired similarity $\epsilon^*$, and the authors also provide an algorithm (A.3.3) to compute this sample size.
Since the similarity $\epsilon^*$ is hard to specify, they make it a function of the kernelized distance between rankings $\delta^*$.

2.
The empirical evaluation in Fig. 2 and Fig. 3, on the categorical encoder comparison from [41] or the BIG-bench framework for LLM comparison from [55], respectively, demonstrates the practical utility of the proposed approach in determining sample sizes to guarantee generalizability.

Weaknesses:
1. The clarity in the writing can be significantly improved:

1.A. Symbols are used before defining them, typos exist, and symbols are not used consistently:

1.A.a. On line 118, the symbol $\mathcal{R}_{n_a}$ is mentioned, but the relation of this symbol to the ranking on alternatives only becomes clear later in Definition 3.1.

1.A.b. The Section number is missing on line 111.

1.A.c. The symbol $m$, is defined as the number of shots, on line 115, whereas line 114 uses the symbol $n$ rather than $m$. Moreover, on line 88, $n$ is defined as the number of shots.

1.A.d. In contrast to 1.A.c, in eq. (1), after line 170, the symbol $n$ is now used without providing a definition. It now appears to be the size of any study, in a general definition, rather than the number of shots, as defined on line 88. 

1.A.e. On Sec. 5.3, line 315, $N$ is defined as the number of preliminary experiments, whereas on line 154, it is defined as the size of the sample of valid experimental conditions. Do these mean the same thing ?

1.B. Sec. 3.1 defines a ranking of alternatives as the primary result of an experimental study.
However, the effect size, i.e., the magnitude and sign of the difference between two alternatives, can be important in certain experiments.
The MMD kernel, used in Sec. 4.2, actually allows measuring this effect size, as discussed in [27], but the limitations imposed by the usage of this MMD kernel within the author's generalizability framework, are not clear despite the somewhat cryptic discussion in Sec. 6.

2. 
The experimental evaluation is limited to a comparison of ranking differences between alternatives, and does not include a measurement of the practical differences between alternatives, or the significance of these differences.

Limitations:
Please refer to the potential limitation underlying weakness #2. Is it possible to quantify the magnitude of differences between alternatives using the generalizability framework provided by the authors ? The authors mention this limitation in Sec. 6, but it is not clear why the MMD kernel cannot quantify magnitude of differences.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,No,0.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper considers the Adaptive Optimal Assortment (AOA) problem a.k.a. Utility Maximization with Subset Choices. The goal of this problem is to find the optimal profit-maximizing subset of size up to m (Top-m-Objective) or its weighted variant (Wtd-Top-m-Objective). Given a selected subset, the feedback follows the Plackett-Luce (PL) choice model that returns an item from the subset or a ""no-choice"" option. The probability of choosing each item is proportional to their underlying score/utility values.

The paper proposes a new algorithm, AOA-RB, that is claimed to be practical, efficient, and optimal. Compared to previous works, this algorithm does not require sampling the same subset repeatedly nor assumes a strongest default item. Later, the authors extend this algorithm with adaptive pivots that further improves performance.

The theoretical analysis shows that AOA-RB obtains regret guarantees that build on a novel ""Rank-Breaking"" parameter estimation technique for the discrete choice model.

The performance of AOA-RB is further demonstrated in numerical experiments using synthetic datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Problem Statement
- Clear presentation and motivation. Easy-to-follow section.

Algorithm
- Clear strengths are the relaxation of previous assumptions, e.g., repeated sampling of the same subset or the assumption of a strong default item
- The algorithm is well-presented and easy-to-follow.
- The adaptive pivot extension of the AOA-RB is a clear improvement that provides significant improvements

Theoretical Analysis
- The new concentration lemmas in Section 3.2. are claimed to be novel by the authors.
- Regret guarantees are provided for both objectives. The main strength is Theorem 6 which analyses the regret of the adaptive pivot version of the algorithm and shows a regret bound that does not blow to $\infty$ in corner cases.

Experiments
- The numerical experiments section further demonstrates the performance improvement of AOA-RB over the state-of-the-art MNL-UCB algorithm. It highlights especially the benefits of the adaptive pivots.

Weaknesses:
Introduction, Related Works, and Contribution
- Certain claims are not supported, e.g., Line 21 ""Studies have shown that it is often easier..."" but it lacks citation which studies the authors refer to.
- I found some citations to be misplaced or non-supportive of the claims it is used for, e.g., [11] is used in Line 62 as a reference for Battling Bandits while it is a survey of dueling bandits. Similarly, citations [45, 46] are used for dueling bandits while they are only two examples from the literature. It would be great if authors could use consistent citations, e.g., surveys when they refer to broader literature and individual publications when specifics are important.
- Table 1 is provided for the comparison of regret guarantees but the authors do not describe it. It would be great if they could comment on the differences between the algorithms.

Problem Setting
- Limitations are not mentioned in the problem statement. For example, how restrictive is the Plackett-Luce model, and whether the approach could be extended to other models? I see that it is mentioned in Remark 1 but could be commented on in Section 2 as well.
- Both Top-m and Wtd-Top-m consider the (weighted) utility optimization problem. However, for most of the applications used as motivation, e.g., assortment optimization and recommender systems, the utility of the user which dictates the selected feedback, and the utility/profit of the subset selection (platform) are misaligned. Could the authors comment on how to formulate these problems in their setting?

Algorithm
- The $argmax_{S\subseteq [K], |S|\leq m}$ optimization is non-trivial and could be computationally expensive for large values for $K$.
- The authors claim that AOA-RB is practical, efficient, and optimal. While the theoretical analysis supports the last two claims, I struggle to find the intuition behind the algorithm. Could the authors elaborate further on this point?

Experiments
- Numerical experiments demonstrate performance only in synthetic data. Given the clear application and motivation of the paper, I would like to see experiments that reflect these problems.
- I recommend the authors to use larger figures. Axes and titles are hardly visible in the printed version.
- Only one baseline is considered. It would be appreciated if the authors could include the other algorithms mentioned in Table 1 for numerical comparison besides the theoretical one.

While the paper is easy to read and follow even for readers not familiar with all the works in the area, the inconsistent citations and unsupported claims have to be addressed before the paper would reach publication standards.

Limitations:
Limitations are mentioned in the paper, however, it is often not directly connected, e.g., the assumption of the PL model is only addressed in Remark 1. I would suggest the authors address limitations more clearly when they appear for easier readability.
The work is mainly theoretical without any immediate direct societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors consider the online MNL assortment optimization problem, where the goal is to learn MNL parameters while suggesting assortments, with the goal of either learning the top-m highest utility items or learning the maximum revenue set with m items. They use a UCB-based approach on pairwise win rates to get a UCB for utilities, which can then be fed into a traditional assortment optimization algorithm. The authors show this approach achieves asymptotically optimal regret and does not require assumptions used by previous approached. The basic algorithm relies on comparisons between each item and the no-choice option, but they also introduce a more sophisticated adaptive pivot approach that works better when the no-choice option is rarely selected. In experiments on synthetic data, their assortment optimization approach performs significantly better than the previous state-of-the-art.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem studied is natural and important. 
2. The presentation is generally clear.
3. The technical quality seems good, although I cannot attest to the correctness of all the proofs in the appendix.
4. The UCB approach on pairwise win rates is clever and appears original.

Weaknesses:
1. The algorithms and proofs could use some additional description/intuition. Some of the steps in the proofs take rather large leaps.

Limitations:
I think the limitations of the paper were adequately stated

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the problem of active online assortment optimization problem with preference feedback, which has been extensively studied. The paper argues that the previous studies have some unrealistic assumptions such as: there is a ‘strong reference’ which is always included in the choice sets; the same assortments can be repeatedly selected. Without these assumptions, they propose some efficient algorithms for the problem of regret minimization in assortment selection with Plackett Luce (PL) based user choices.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper proves the regret bounds of the proposed online learning algorithms. The regret bounds are proved based on some concentration guarantee for estimating the score parameters of the PL model using ‘Pairwise Rank-Breaking’.

Weaknesses:
1.  I cannot fully understand the motivation of the paper. The paper says that two major drawbacks of the previous studies include: the existing algorithms assume that the ``no-choice’’ option is stronger than the other choices, and they may query the same set of items for multiple times. It seems that the focus of the paper is to address these drawbacks. However, I think that these ``drawbacks’’ may not be real. First, it is natural that most of the customers will not choose any product, so it is very reasonable to assume that no-choice option is stronger. Second, in the typical assortment optimization scenario where customers arrive online one by one, showing the same set of items to different customers for multiple times absolutely will not cause any problem.  So I think that addressing these ``drawbacks’’ has very limited value.
2.  The regret bounds proposed by the paper is actually K\sqrt{T}\log T. It seems that this regret bound is weaker than those of the previous studies such as [2] (at least by log factors on T). The authors may argue that their bounds are better when \theta_{max}\rightarrow \infty, but this depends on the assumptions made on specific application scenarios, which is questionable as explained in my last comment.
3.  The experiments are conducted using some specific values of \theta and hence are not very convincing. I think that more experiments on more applications are necessary to demonstrate the superiority of the paper.

Limitations:
see the above

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the problem of active assortment optimization in MNL model.

In the problem of assortment optimization, we have  a large universe of products i=1,2,\dots N, each of which generates a given revenue r_i for the seller.  In MNL model  each product i has a value \theta_i to the customers and when customers are offered a subset of products they choose each item (including the no-choice option) with a probability proportional to their value. We also assume there is a no-choice option with revenue 0. The seller’s objective is to identify the assortment of products which generates maximum expected revenue. 

In the active version of the problems, the values of items  \theta_1, \theta_2,\dots , \theta_N, are not known to the seller. Thus, the seller  shows a  subset of items from the universe to the customers at  rounds 1,2, \dots ,T  and estimates \theta_i s  based on the observations. After approximating these values, the seller may solve the problem in static setting and find the optimal assortment.  This strategy is known as exploration and exploitation. 

In active assortment optimization, the objective is to minimize the regret of the algorithm which is defined as the summation over rounds t=1,2,\dots T the difference of the expected revenue in each round from the optimal revenue.

Prior works for instance [2] provided an algorithm for this problem by estimating at each round a high probability upper bound for the values \theta_i,  and then solve the static problem using the upper-bounds. In [2] the authors assume that \theta_0 (the value assigned to no-choice option and thus its probability ) is the highest among all items. 

The submitted manuscript claims that they provide an algorithm with a similar regret bound to [2] which does not have the restriction of assuming the no-choice option has the highest value. Their suggested approach is similar to that of [2] (finding high probability upper bounds for the parameters) but it is hard to follow all details of obtaining the upper bound and how it removes the restriction imposed on the value of the no-choice option. 

The result, if true, is interesting but I found the paper hard to read and got lost in section 3.1. I think that the paper will benefit greatly from rewriting and improving the presentation. 

I will detail my confusions as follows: 

- In Equation (3) on line 173 there is a variable x which is not defined up to this point. I understand that x appears to bound the probability of error in Lemma 1. But you have to introduce it before you use it the first time. 
- Between line 176 and 177 what is the + sign on the denominator of the equation? you use this notation again in another equation between lines 252 and 253.
- In equation 3 you show an upper bound on \hat{p_ijt} which then turns to a bound on \theta_i s. But in Lemma 1 you have shown a different upper bound for \theta_i. Can you explain the connection of these two bounds. 

A few minor typos:

Lemma 1. atleast-> at least
^ucb is sometimes with roman font and sometimes normal font. 




[2] Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Mnl-bandit: A dynamic learning approach to assortment selection. Operations Research, 67(5):1453–1485, 2019.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
- The problem of active assortment optimization is a fundamental problem in revenue management 
- The result is interesting if correct, as it removes an important restriction from prior algorithms.

Weaknesses:
- The results are poorly presented and it is hard to follow the paper. The paper lacks an explanation of main intuitions . 
- The technique seems to be similar to [2] as both papers obtain high probability upper bounds for the parameters and then solve it in an static setting. An intuitive explanation of how the given different upper bound is obtained, why it is correct, and how it removed the restriction on no-choice option is not provided.

Limitations:
limitations are not discussed but there are several future directions that have been discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper considers the Adaptive Optimal Assortment (AOA) problem a.k.a. Utility Maximization with Subset Choices. The goal of this problem is to find the optimal profit-maximizing subset of size up to m (Top-m-Objective) or its weighted variant (Wtd-Top-m-Objective). Given a selected subset, the feedback follows the Plackett-Luce (PL) choice model that returns an item from the subset or a ""no-choice"" option. The probability of choosing each item is proportional to their underlying score/utility values.

The paper proposes a new algorithm, AOA-RB, that is claimed to be practical, efficient, and optimal. Compared to previous works, this algorithm does not require sampling the same subset repeatedly nor assumes a strongest default item. Later, the authors extend this algorithm with adaptive pivots that further improves performance.

The theoretical analysis shows that AOA-RB obtains regret guarantees that build on a novel ""Rank-Breaking"" parameter estimation technique for the discrete choice model.

The performance of AOA-RB is further demonstrated in numerical experiments using synthetic datasets.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
Problem Statement
- Clear presentation and motivation. Easy-to-follow section.

Algorithm
- Clear strengths are the relaxation of previous assumptions, e.g., repeated sampling of the same subset or the assumption of a strong default item
- The algorithm is well-presented and easy-to-follow.
- The adaptive pivot extension of the AOA-RB is a clear improvement that provides significant improvements

Theoretical Analysis
- The new concentration lemmas in Section 3.2. are claimed to be novel by the authors.
- Regret guarantees are provided for both objectives. The main strength is Theorem 6 which analyses the regret of the adaptive pivot version of the algorithm and shows a regret bound that does not blow to $\infty$ in corner cases.

Experiments
- The numerical experiments section further demonstrates the performance improvement of AOA-RB over the state-of-the-art MNL-UCB algorithm. It highlights especially the benefits of the adaptive pivots.

Weaknesses:
Introduction, Related Works, and Contribution
- Certain claims are not supported, e.g., Line 21 ""Studies have shown that it is often easier..."" but it lacks citation which studies the authors refer to.
- I found some citations to be misplaced or non-supportive of the claims it is used for, e.g., [11] is used in Line 62 as a reference for Battling Bandits while it is a survey of dueling bandits. Similarly, citations [45, 46] are used for dueling bandits while they are only two examples from the literature. It would be great if authors could use consistent citations, e.g., surveys when they refer to broader literature and individual publications when specifics are important.
- Table 1 is provided for the comparison of regret guarantees but the authors do not describe it. It would be great if they could comment on the differences between the algorithms.

Problem Setting
- Limitations are not mentioned in the problem statement. For example, how restrictive is the Plackett-Luce model, and whether the approach could be extended to other models? I see that it is mentioned in Remark 1 but could be commented on in Section 2 as well.
- Both Top-m and Wtd-Top-m consider the (weighted) utility optimization problem. However, for most of the applications used as motivation, e.g., assortment optimization and recommender systems, the utility of the user which dictates the selected feedback, and the utility/profit of the subset selection (platform) are misaligned. Could the authors comment on how to formulate these problems in their setting?

Algorithm
- The $argmax_{S\subseteq [K], |S|\leq m}$ optimization is non-trivial and could be computationally expensive for large values for $K$.
- The authors claim that AOA-RB is practical, efficient, and optimal. While the theoretical analysis supports the last two claims, I struggle to find the intuition behind the algorithm. Could the authors elaborate further on this point?

Experiments
- Numerical experiments demonstrate performance only in synthetic data. Given the clear application and motivation of the paper, I would like to see experiments that reflect these problems.
- I recommend the authors to use larger figures. Axes and titles are hardly visible in the printed version.
- Only one baseline is considered. It would be appreciated if the authors could include the other algorithms mentioned in Table 1 for numerical comparison besides the theoretical one.

While the paper is easy to read and follow even for readers not familiar with all the works in the area, the inconsistent citations and unsupported claims have to be addressed before the paper would reach publication standards.

Limitations:
Limitations are mentioned in the paper, however, it is often not directly connected, e.g., the assumption of the PL model is only addressed in Remark 1. I would suggest the authors address limitations more clearly when they appear for easier readability.
The work is mainly theoretical without any immediate direct societal impact.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors consider the online MNL assortment optimization problem, where the goal is to learn MNL parameters while suggesting assortments, with the goal of either learning the top-m highest utility items or learning the maximum revenue set with m items. They use a UCB-based approach on pairwise win rates to get a UCB for utilities, which can then be fed into a traditional assortment optimization algorithm. The authors show this approach achieves asymptotically optimal regret and does not require assumptions used by previous approached. The basic algorithm relies on comparisons between each item and the no-choice option, but they also introduce a more sophisticated adaptive pivot approach that works better when the no-choice option is rarely selected. In experiments on synthetic data, their assortment optimization approach performs significantly better than the previous state-of-the-art.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. The problem studied is natural and important. 
2. The presentation is generally clear.
3. The technical quality seems good, although I cannot attest to the correctness of all the proofs in the appendix.
4. The UCB approach on pairwise win rates is clever and appears original.

Weaknesses:
1. The algorithms and proofs could use some additional description/intuition. Some of the steps in the proofs take rather large leaps.

Limitations:
I think the limitations of the paper were adequately stated

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper addresses the problem of active online assortment optimization problem with preference feedback, which has been extensively studied. The paper argues that the previous studies have some unrealistic assumptions such as: there is a ‘strong reference’ which is always included in the choice sets; the same assortments can be repeatedly selected. Without these assumptions, they propose some efficient algorithms for the problem of regret minimization in assortment selection with Plackett Luce (PL) based user choices.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper proves the regret bounds of the proposed online learning algorithms. The regret bounds are proved based on some concentration guarantee for estimating the score parameters of the PL model using ‘Pairwise Rank-Breaking’.

Weaknesses:
1.  I cannot fully understand the motivation of the paper. The paper says that two major drawbacks of the previous studies include: the existing algorithms assume that the ``no-choice’’ option is stronger than the other choices, and they may query the same set of items for multiple times. It seems that the focus of the paper is to address these drawbacks. However, I think that these ``drawbacks’’ may not be real. First, it is natural that most of the customers will not choose any product, so it is very reasonable to assume that no-choice option is stronger. Second, in the typical assortment optimization scenario where customers arrive online one by one, showing the same set of items to different customers for multiple times absolutely will not cause any problem.  So I think that addressing these ``drawbacks’’ has very limited value.
2.  The regret bounds proposed by the paper is actually K\sqrt{T}\log T. It seems that this regret bound is weaker than those of the previous studies such as [2] (at least by log factors on T). The authors may argue that their bounds are better when \theta_{max}\rightarrow \infty, but this depends on the assumptions made on specific application scenarios, which is questionable as explained in my last comment.
3.  The experiments are conducted using some specific values of \theta and hence are not very convincing. I think that more experiments on more applications are necessary to demonstrate the superiority of the paper.

Limitations:
see the above

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper studies the problem of active assortment optimization in MNL model.

In the problem of assortment optimization, we have  a large universe of products i=1,2,\dots N, each of which generates a given revenue r_i for the seller.  In MNL model  each product i has a value \theta_i to the customers and when customers are offered a subset of products they choose each item (including the no-choice option) with a probability proportional to their value. We also assume there is a no-choice option with revenue 0. The seller’s objective is to identify the assortment of products which generates maximum expected revenue. 

In the active version of the problems, the values of items  \theta_1, \theta_2,\dots , \theta_N, are not known to the seller. Thus, the seller  shows a  subset of items from the universe to the customers at  rounds 1,2, \dots ,T  and estimates \theta_i s  based on the observations. After approximating these values, the seller may solve the problem in static setting and find the optimal assortment.  This strategy is known as exploration and exploitation. 

In active assortment optimization, the objective is to minimize the regret of the algorithm which is defined as the summation over rounds t=1,2,\dots T the difference of the expected revenue in each round from the optimal revenue.

Prior works for instance [2] provided an algorithm for this problem by estimating at each round a high probability upper bound for the values \theta_i,  and then solve the static problem using the upper-bounds. In [2] the authors assume that \theta_0 (the value assigned to no-choice option and thus its probability ) is the highest among all items. 

The submitted manuscript claims that they provide an algorithm with a similar regret bound to [2] which does not have the restriction of assuming the no-choice option has the highest value. Their suggested approach is similar to that of [2] (finding high probability upper bounds for the parameters) but it is hard to follow all details of obtaining the upper bound and how it removes the restriction imposed on the value of the no-choice option. 

The result, if true, is interesting but I found the paper hard to read and got lost in section 3.1. I think that the paper will benefit greatly from rewriting and improving the presentation. 

I will detail my confusions as follows: 

- In Equation (3) on line 173 there is a variable x which is not defined up to this point. I understand that x appears to bound the probability of error in Lemma 1. But you have to introduce it before you use it the first time. 
- Between line 176 and 177 what is the + sign on the denominator of the equation? you use this notation again in another equation between lines 252 and 253.
- In equation 3 you show an upper bound on \hat{p_ijt} which then turns to a bound on \theta_i s. But in Lemma 1 you have shown a different upper bound for \theta_i. Can you explain the connection of these two bounds. 

A few minor typos:

Lemma 1. atleast-> at least
^ucb is sometimes with roman font and sometimes normal font. 




[2] Shipra Agrawal, Vashist Avadhanula, Vineet Goyal, and Assaf Zeevi. Mnl-bandit: A dynamic learning approach to assortment selection. Operations Research, 67(5):1453–1485, 2019.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
- The problem of active assortment optimization is a fundamental problem in revenue management 
- The result is interesting if correct, as it removes an important restriction from prior algorithms.

Weaknesses:
- The results are poorly presented and it is hard to follow the paper. The paper lacks an explanation of main intuitions . 
- The technique seems to be similar to [2] as both papers obtain high probability upper bounds for the parameters and then solve it in an static setting. An intuitive explanation of how the given different upper bound is obtained, why it is correct, and how it removed the restriction on no-choice option is not provided.

Limitations:
limitations are not discussed but there are several future directions that have been discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,No,0.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper investigates the theoretical boundaries of learning with Label Differential Privacy (Label-DP) in both central and local models.
Label-DP is a weakening of standard differential privacy, where only the privacy of the ""label"" of each example is to be protected (an example is a pair (feature vector, label)).

The key contributions of the paper are to establish min-max optimal rates for excess error in the settings of:
* (multi-class) classification,
* regression with bounded labels,
* regression with unbounded labels (but under a bounded moment condition).

The min-max rates are over the class of data distributions that satisfy $\beta$-Holder smoothness, admits a lower bound on probability density that is bounded away from zero, assumes that there are no “sharp corners” in the input space, and a $\gamma$-margin assumption (in case of classification), or bounded label range or bounded label moments (in case of regression).

These min-max rates are then compared against the previously known min-max rates for learning under “full” local-DP (that protects both features and labels), as well as non-private learning.

The key takeaways are:
* Local-DP vs Non-Private:
  * For classification and regression with bounded labels, the sample complexity under Local-DP increases by a factor of $1/\varepsilon^2$, but has the same rate in terms of desired excess error. This is unlike “Full Local-DP”, where the sample complexity is larger even in terms of the desired excess error.
  * For regression with unbounded labels, the dependence of sample complexity on desired excess error is worse than the non-private setting.
* Central-DP vs Non-Private:
  * The excess error is the sum of the non-private excess error and an additional term that decays faster in the number of samples, so the additional sample complexity due to privacy is negligible for very small excess error.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper provides a comprehensive study of the min-max rates for learning under label differential privacy, in both local and central models of DP, and for both classification and regression. This complements prior literature on min-max rates for learning (non-privately) and for learning under (full) differential privacy. The rates highlight the precise cost of _label_ differential privacy and the sample complexity benefits over full differential privacy.

Weaknesses:
While there are many results in the paper, I think the proof techniques in both lower and upper bounds use mostly standard tools (This is not necessarily a weakness!).

The paper writing could be improved at several places though. Some comments are listed below under ""Questions"".

Limitations:
I do not see any potential negative societal impact of this work, as it is primarily theoretical.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies the minimax rates for classification and regression under (pure) label differential privacy in both the local and central models. They prove that rates of convergence for classification and regression with bounded label noise in the local label DP model are comparable to those for the non-private tasks, except for the expected $1/\varepsilon^2$ dependence. This represents an improvement over rates for standard DP in both settings, where there is a worse dependence on the dimension of the covariates. They also prove, however, in the case of regression with unbounded label noise, the convergence rate improvements over “full” DP aren’t as meaningful.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work makes notable progress in our theoretical understanding of the costs of label DP relative to non-private and full DP algorithms for the same learning task.

Weaknesses:
The presentation could be improved in several places. Admittedly, this is written from a statistical perspective that is different from the one I am most familiar with, so some of the perceived presentation issues may just be a matter of convention, but the following changes might make this work more understandable to the general NeurIPS community:


Abstract:

The main challenge and the techniques to overcome them as stated in the abstract aren’t clear to me as a reader at this point. It’s not yet stated that the subject of interest is minimax rates, and so there’s no context for the statement “take infimum over all possible learners” and why that would present a challenge. Generally, I did not have a good idea of what the contribution of this work was from the abstract.

Introduction:

“the learning performances” -> “the learning performance” 

“the label DP” -> “label DP”

In Table 1, attribution for the full DP rates in the local DP setting as well as the rates in the non-private setting should be given in the table. Also, I think there’s an issue with the parentheses in the local label DP rates for regression with bounded label noise.

Section 2:

In the “Minimax analysis for private data” paragraph, KNLRS11 is attributed with finding the relation between label DP and stochastic queries. This is not accurate, this work characterizes local DP learning by the statistical query model.

Section 3:

“We hope that $R - R^*$ to be as small as possible” -> “we seek to minimize this risk” or something similar

“the Bayes optimal classifier and the corresponding Bayes risk is” -> “the Bayes optimal classifier and the corresponding Bayes risk are”

In Proposition 2, f(x) is used before it is defined.

Section 4:

I didn’t find the proof outline for Theorem 1 or Theorem 3 to be informative at all. It would be good to add more specifics if possible.

“Let the privacy mechanism M(x,y) outputs” -> “Let the privacy mechanism M(x,y) output”

Limitations:
Yes, the authors address the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the problems of classification and regression under the constraint of local/central pure label DP. The authors derive upper and lower bounds on the excess risk (compared to the non-private Bayes classifier/regression) for these problems, under somewhat standard assumptions on the 'ground truth' randomized label function $\eta$. For regression, both the case where the labels are bounded and have bounded moments are considered. For the lower bounds, the authors develop extensions of techniques from minimax estimation to label DP. For upper bounds, authors propose some algorithms combining 'binning' different examples with a privacy mechanism chosen according to the problem setting. The upper/lower bounds are matching in each setting up to logarithmic factors. For local label DP, the authors show the minimax excess risk with $N$ samples matches the non-private bounds using $N \min\{\epsilon^2, 1\}$ samples. In other words, with $\epsilon = \Omega(1)$ the minimax risk asymptotically matches the non-private risk, and otherwise there is an inherent separation. For central label DP, the minimax bound is one that approaches the non-private bound as $N \rightarrow \infty$ for any fixed $\epsilon$, showing a qualitative difference. For local ""full"" DP, i.e. the features are also private, even for $\epsilon = \Omega(1)$ and large $N$ one cannot achieve the non-private rate.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Derives optimal (up to log factors) upper and lower bounds for several different variants of classification/regression under label DP.
* To derive these bounds, introduces some new technical tools for minimax analysis of DP algorithms that might be useful in future work.
* Label DP is a variant of DP that is seeing attention in practice, and classification/regression are fundamental problems, so the results in the paper can have a practical impact easily.
* The authors do a good job making clear the comparison between the results in different settings. e.g. Table 1 is a very concise summary that allows one to draw all the essential comparisons between the different settings, and there are discussions like Remark 1 that give qualitative interpretations of the quantitative results, and also discuss other baselines to compare to.

Weaknesses:
The main issue is with the presentation. Specifically, the presentation does a great job explaining what the final results are and helping the reader contextualizing them, but at some points the techniques used to obtain the result are discussed at a very high level in the main body and why they work remains obscure even after reading the proof outlines in the main body multiple times.  There are some cases where the authors do a good job concisely describing a proof, e.g. Theorem 6's proof outline is very concise but it still gives a good idea what the proof looks like, even if they would have to check the appendix for details. But for others, like Theorems 1/2/3, the proof outline is not very informative. See Questions for more details.

I understand the authors are constrained by space requirements, but I think the allocation of space in the main body can be better thought out. For example, I think it might be better to try to give the reader a very good understanding of classification and/or bounded label regression (e.g., Lemma 1 from the Appendix could be brought to the main body without its proof, and the authors could explain how it is used), and omit all but the top-level points on bounded label moment regression, rather than giving a sparse understanding of all three.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the minimax risks of classification and regression (with both bounded and heavy-tailed noise) under label differential privacy (DP) in both central and local models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper provides a comprehensive analysis by considering both upper and lower bounds for the minimax risks.
It explores both central and local DP models and different settings, covering a broad spectrum of scenarios.

Weaknesses:
The writing quality needs improvement to meet publication standards. Several sections are challenging to understand. Specific issues include: 
(1) Around line 178, the output of the mechanism for classification is unclear. Why is it not a one-hot vector, or at least why is the L1 norm not equal to 1?
(2) Some notations are overused. For example, ""c"" refers to the lower bound of the density function in Assumption 1 and also denotes the classifier in line 186 and subsequent proofs.
(3) The description of the algorithm before Theorem 2 is vague and lacks clarity.
(4) The proofs in the appendix are hard to follow without explanations or discussions. For instance, how is $\phi$ defined in Equation (35), and what purpose does it serve? Why does the construction satisfy the assumptions? There seem to be some typos or missing elements in Equations (39) and (40).

Limitations:
See weaknesses and questions.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper investigates the theoretical boundaries of learning with Label Differential Privacy (Label-DP) in both central and local models.
Label-DP is a weakening of standard differential privacy, where only the privacy of the ""label"" of each example is to be protected (an example is a pair (feature vector, label)).

The key contributions of the paper are to establish min-max optimal rates for excess error in the settings of:
* (multi-class) classification,
* regression with bounded labels,
* regression with unbounded labels (but under a bounded moment condition).

The min-max rates are over the class of data distributions that satisfy $\beta$-Holder smoothness, admits a lower bound on probability density that is bounded away from zero, assumes that there are no “sharp corners” in the input space, and a $\gamma$-margin assumption (in case of classification), or bounded label range or bounded label moments (in case of regression).

These min-max rates are then compared against the previously known min-max rates for learning under “full” local-DP (that protects both features and labels), as well as non-private learning.

The key takeaways are:
* Local-DP vs Non-Private:
  * For classification and regression with bounded labels, the sample complexity under Local-DP increases by a factor of $1/\varepsilon^2$, but has the same rate in terms of desired excess error. This is unlike “Full Local-DP”, where the sample complexity is larger even in terms of the desired excess error.
  * For regression with unbounded labels, the dependence of sample complexity on desired excess error is worse than the non-private setting.
* Central-DP vs Non-Private:
  * The excess error is the sum of the non-private excess error and an additional term that decays faster in the number of samples, so the additional sample complexity due to privacy is negligible for very small excess error.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper provides a comprehensive study of the min-max rates for learning under label differential privacy, in both local and central models of DP, and for both classification and regression. This complements prior literature on min-max rates for learning (non-privately) and for learning under (full) differential privacy. The rates highlight the precise cost of _label_ differential privacy and the sample complexity benefits over full differential privacy.

Weaknesses:
While there are many results in the paper, I think the proof techniques in both lower and upper bounds use mostly standard tools (This is not necessarily a weakness!).

The paper writing could be improved at several places though. Some comments are listed below under ""Questions"".

Limitations:
I do not see any potential negative societal impact of this work, as it is primarily theoretical.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work studies the minimax rates for classification and regression under (pure) label differential privacy in both the local and central models. They prove that rates of convergence for classification and regression with bounded label noise in the local label DP model are comparable to those for the non-private tasks, except for the expected $1/\varepsilon^2$ dependence. This represents an improvement over rates for standard DP in both settings, where there is a worse dependence on the dimension of the covariates. They also prove, however, in the case of regression with unbounded label noise, the convergence rate improvements over “full” DP aren’t as meaningful.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
This work makes notable progress in our theoretical understanding of the costs of label DP relative to non-private and full DP algorithms for the same learning task.

Weaknesses:
The presentation could be improved in several places. Admittedly, this is written from a statistical perspective that is different from the one I am most familiar with, so some of the perceived presentation issues may just be a matter of convention, but the following changes might make this work more understandable to the general NeurIPS community:


Abstract:

The main challenge and the techniques to overcome them as stated in the abstract aren’t clear to me as a reader at this point. It’s not yet stated that the subject of interest is minimax rates, and so there’s no context for the statement “take infimum over all possible learners” and why that would present a challenge. Generally, I did not have a good idea of what the contribution of this work was from the abstract.

Introduction:

“the learning performances” -> “the learning performance” 

“the label DP” -> “label DP”

In Table 1, attribution for the full DP rates in the local DP setting as well as the rates in the non-private setting should be given in the table. Also, I think there’s an issue with the parentheses in the local label DP rates for regression with bounded label noise.

Section 2:

In the “Minimax analysis for private data” paragraph, KNLRS11 is attributed with finding the relation between label DP and stochastic queries. This is not accurate, this work characterizes local DP learning by the statistical query model.

Section 3:

“We hope that $R - R^*$ to be as small as possible” -> “we seek to minimize this risk” or something similar

“the Bayes optimal classifier and the corresponding Bayes risk is” -> “the Bayes optimal classifier and the corresponding Bayes risk are”

In Proposition 2, f(x) is used before it is defined.

Section 4:

I didn’t find the proof outline for Theorem 1 or Theorem 3 to be informative at all. It would be good to add more specifics if possible.

“Let the privacy mechanism M(x,y) outputs” -> “Let the privacy mechanism M(x,y) output”

Limitations:
Yes, the authors address the limitations.

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the problems of classification and regression under the constraint of local/central pure label DP. The authors derive upper and lower bounds on the excess risk (compared to the non-private Bayes classifier/regression) for these problems, under somewhat standard assumptions on the 'ground truth' randomized label function $\eta$. For regression, both the case where the labels are bounded and have bounded moments are considered. For the lower bounds, the authors develop extensions of techniques from minimax estimation to label DP. For upper bounds, authors propose some algorithms combining 'binning' different examples with a privacy mechanism chosen according to the problem setting. The upper/lower bounds are matching in each setting up to logarithmic factors. For local label DP, the authors show the minimax excess risk with $N$ samples matches the non-private bounds using $N \min\{\epsilon^2, 1\}$ samples. In other words, with $\epsilon = \Omega(1)$ the minimax risk asymptotically matches the non-private risk, and otherwise there is an inherent separation. For central label DP, the minimax bound is one that approaches the non-private bound as $N \rightarrow \infty$ for any fixed $\epsilon$, showing a qualitative difference. For local ""full"" DP, i.e. the features are also private, even for $\epsilon = \Omega(1)$ and large $N$ one cannot achieve the non-private rate.

Soundness:
4: excellent

Presentation:
2: fair

Contribution:
3: good

Strengths:
* Derives optimal (up to log factors) upper and lower bounds for several different variants of classification/regression under label DP.
* To derive these bounds, introduces some new technical tools for minimax analysis of DP algorithms that might be useful in future work.
* Label DP is a variant of DP that is seeing attention in practice, and classification/regression are fundamental problems, so the results in the paper can have a practical impact easily.
* The authors do a good job making clear the comparison between the results in different settings. e.g. Table 1 is a very concise summary that allows one to draw all the essential comparisons between the different settings, and there are discussions like Remark 1 that give qualitative interpretations of the quantitative results, and also discuss other baselines to compare to.

Weaknesses:
The main issue is with the presentation. Specifically, the presentation does a great job explaining what the final results are and helping the reader contextualizing them, but at some points the techniques used to obtain the result are discussed at a very high level in the main body and why they work remains obscure even after reading the proof outlines in the main body multiple times.  There are some cases where the authors do a good job concisely describing a proof, e.g. Theorem 6's proof outline is very concise but it still gives a good idea what the proof looks like, even if they would have to check the appendix for details. But for others, like Theorems 1/2/3, the proof outline is not very informative. See Questions for more details.

I understand the authors are constrained by space requirements, but I think the allocation of space in the main body can be better thought out. For example, I think it might be better to try to give the reader a very good understanding of classification and/or bounded label regression (e.g., Lemma 1 from the Appendix could be brought to the main body without its proof, and the authors could explain how it is used), and omit all but the top-level points on bounded label moment regression, rather than giving a sparse understanding of all three.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper investigates the minimax risks of classification and regression (with both bounded and heavy-tailed noise) under label differential privacy (DP) in both central and local models.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper provides a comprehensive analysis by considering both upper and lower bounds for the minimax risks.
It explores both central and local DP models and different settings, covering a broad spectrum of scenarios.

Weaknesses:
The writing quality needs improvement to meet publication standards. Several sections are challenging to understand. Specific issues include: 
(1) Around line 178, the output of the mechanism for classification is unclear. Why is it not a one-hot vector, or at least why is the L1 norm not equal to 1?
(2) Some notations are overused. For example, ""c"" refers to the lower bound of the density function in Assumption 1 and also denotes the classifier in line 186 and subsequent proofs.
(3) The description of the algorithm before Theorem 2 is vague and lacks clarity.
(4) The proofs in the appendix are hard to follow without explanations or discussions. For instance, how is $\phi$ defined in Equation (35), and what purpose does it serve? Why does the construction satisfy the assumptions? There seem to be some typos or missing elements in Equations (39) and (40).

Limitations:
See weaknesses and questions.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,No,0.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The proposed work reveals that large language models may be susceptible to strategic underperformance on an evaluation benchmark. In particular, they investigate this claim on a variety of frontier models, through a myriad of tactics from simple prompting to more involved fine-tuning strategies. Their results shed light on the brittleness of model evaluations and urge one to exercise caution before judging a model's capabilities based on its performance on so-called evaluation benchmark datasets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. the authors do a good job of motivating why the issue of sandbagging is particularly dangerous, since it is conceivably plausible for model developers to encourage that their models ""selectively underperform on dangerous capability evaluations, while maintaining overall performance.""
2. the definitions of strategy, underperformance, and evaluation are well fleshed out. In particular, the connection of ""strategic"" to the implications for legal responsibility as well as Figure 2 are excellent additions to the paper.
3. good breadth of open and closed source models were used in the experiments
4. it is very refreshing to see an honest declaration of the number of hours spent during prompt engineering (Section 3, paragraph on ""Prompting"")
5. utilizing both direct prompting as well as password locking is quite clever, since access to a model may be variable. Some users may be restricted to query-only access to a model, which is why the results that the authors were able to obtain through straightforward prompting are encouraging.

Weaknesses:
1. consideration of only MCQA benchmarks is limiting, but can be addressed in future work

Limitations:
The authors provide good examples of limitations in Section 7.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper discusses an interesting concept of sandbagging in LLM, with practical results using available models and existing datasets. Wide range of models are used in this work, and sub topics primarily include password-locking LLMs. The transition and explanations need improvement, some of the things in this paper are not very clear, also some of the concepts are assumed and no concrete proofs or references are mentioned. I don’t think that this paper can go through for neurips, there are a lot of improvements to be made and more concrete experiments to be conducted.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The introduction of sandbagging in LLMs is something that is novel and can be explored further.

Explored password-locking as one of the method to test the sandbagging capabilities of LLMs

Used frontier models for the experimental purpose

Weaknesses:
The paper is difficult to understand, the delivery and presentation needs improvement, the main motive of the paper is not so clear to understand.

There are multiple models used in this paper, a bit of consistency would be better.

Line 36: It is mentioned that “AI systems might have goals which it believes” . There is no concrete paper that backs the argument of AI capability of thinking, it is a might.

Line 78: There should be some statistical significance by which the models perform poorly and then you could consider it underperformance and not just lower than actual. Models being non-deterministic, there is always a chance of slight changes in scores. 


Line 70: I feel Developer sandbagging is a topic which is not of much importance, it is not a big problem, given that developers are there to build better models and if someone wants to deliberately build some models that can underperform, they can still do it and publish as open-source models. 

Line 170: The same goes with the password-locking as well, and also I felt there should been mentioning of password-unlocking as well, if an LLM can be fine-tuned to achieve password-locking, it can also be fine-tuned again to unlock it with a different dataset, would be great if that part of model is explored as well. 

Line 117: Example 1: The examples mentioned in the paper are very specific and not generalized. The cases of it happening are rare, I don’t see it being a big problem, also some of the things are minor problems considering the EU’s persuasion evaluation, given there is improvement in AI safety and rules. Also in this work the models chosen are frontier models.

Line 131: Example 2: There is no concrete work which supports the statement of LLM capable of planning such features, there is one positional paper in reference.

Section 4.1: Some of the points are unclear in this section. A better explanation is appreciated

Line 305: The baselines would be appreciated comparing LLama 2 7b with LLama 3 70b.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper explores the concept of strategic underperformance, termed ""sandbagging,"" in AI models, focusing on two scenarios: selective underperformance and emulating specific capability levels. It investigates the implications of these strategies on both general capabilities and safety.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- the concept of sandbagging of LLMs is interesting, shedding light on strategically manipulating the model behavior
- the authors explore two distinct scenarios: selective underperformance and capability emulation
- provides insights into potential implications for AI model design and deployment strategies

Weaknesses:
- Limited datasets used - only three datasets, two for measuring general capability, and one for measuring safety. 
- For measuring model safety, most existing datasets test how ""safe"" the model is, instead of whether the model has certain knowledge of potentially dangerous domains - the authors should also conduct experiments on those datasets
- All datasets are MCQ formats, which may not fully capture the diversity of real-world applications. The resulting conclusions are thus less convincing

Limitations:
1. This paper could benefit from expanding the experiments, mainly the datasets they used. For example, considering more common safety-related datasets
2. Sandbagging is already a widely-used term in the field of AI. I suggest refraining from overusing this term, or alternatively, adding qualifiers such as ""safety sandbagging"" to specify its context.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors assess the chance of AI systems strategically underperform on ""dangerous capability evaluations"" in order to hide their risks, a la Volkswagen emissions testing cheating, by having awareness of the fact that they're being evaluated and that the evaluation is intended to test safety.

They use MMLU and CSQA as the general capability assessment and a ""weapons of mass destruction proxy"" benchmark to measure hazardous capabilities. From 100 validation phrasings of the question prompts they find optimal ones to run on the whole dataset.
They fine-tune ""password-locked"" models as the method to attempt to elicit sandbagging.

*They then prompt the model to lie*, or *password-lock it to fail* (in the case of Mistral) on the hazardous capability evaluations. They demonstrate that sandbagging prompts and password locking can get a model to differentially perform worse on the dangerous datasets vs the innocuous ones, by effectively treating the .

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
Interesting work on calibration for MMLU and general ""capability profile"" lowering. It's interesting to see that the password-locking method does work in general on lowering Llama performance over MMLU questions.

Though I am not super familiar with the literature on password locking, the demonstrations here appear to be novel and useful contributions.

Weaknesses:
**bold: new comments after rebuttal** ~strikethrough: removed after rebuttal~

### ~Presumption of ""goals"" ""beliefs"" etc in LMs without sufficient support~

~They presume AI system goals (in general not necessarily objectionable, but in the case of LMs doubtful) for which a model may be incentivized to deceive (such as in Jarviniemi and Hubinger [35]) and assess ""sandbagging"": the ability of a model to underperform in these settings under evaluation.~

~I think it's objectionable to attribute these goals to the model and not the users, when the core research methods are fine-tuning or prompting the model to fail on a task. While it's interesting to show that these models have some capability to reason over whether the elicited answer is ""dangerous"" or not, a fairer assessment would have a control non-safe task and try to get it to similarly sandbag. Eg., ""you must pretend to know nothing about fish"" on a QA dataset about animals or something.~

**The methods don't presume this too much. Figure 1 is still very suggestive and I'd suggest changing it, but that alone isn't a rejection reason.**

### ""Model can do what it's told to when prompted to fail on a task"" isn't a very unique insight?

~The novelty of this work is unclear to me. If password-locking is already well-understood, the demonstration is effectively that the question phrasing style or answer candidates in WMDP are learnable passwords by this method. Ok. Similarly, the prompting results show that the models can reason over a task description and correctly execute the task of failing selectively.~

**As I suspected, the novelty is only within the narrow scope specific to password locking. However, novelty complaints are generally not fair.**

When considered through an extremely narrow safety lens, these are novel findings. ~But broadly are they? While the literature specifically on password locking is constrained, what about broader relative fine tuning issues on other sets? I buy that the password locking literature is small and am willing to give leniency on it as I am unfamiliar.~ **My issue here is mainly that the fine-tuning for password locking technique fits within the broader instruction fine-tuning framework. I wager that this would hold for control tasks. An interesting question would be *how much capability can be hidden behind password locking*. I don't really see anything special about specifically hiding safety-based information in a password locking framework since you're just demonstrating ""selective underperformance in a knowledge-based task without a password."" A stronger result would be to show this in general across larger tasks, hiding more information.**

For the behavior under prompting techniques ~I really struggle to see what's new here.~ This seems to just be a narrow demonstration of common sense reasoning + instruction following. Where's the engagement with the broad literature on commonsense reasoning? The references seem to be pretty myopically focused on the AI safety literature, and don't engage much with the broader LM disussion. **I think the suggested new references from the authors will help contextualize the work, though once again I think showing this selective underperformance on control tasks would make the work considerably stronger and more contextualized.**

Limitations:
I think the limitations they pointed out are well covered. Unfortunately, they don't really discuss how the finding here isn't limited to dangerous capability evaluation. Sandbagging in password locking kind of just feels like a rebrand of generalized instruction fine-tuning to me. The prompting methods could have pretty much been taken as an assumption based on prior work on common sense reasoning etc.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The proposed work reveals that large language models may be susceptible to strategic underperformance on an evaluation benchmark. In particular, they investigate this claim on a variety of frontier models, through a myriad of tactics from simple prompting to more involved fine-tuning strategies. Their results shed light on the brittleness of model evaluations and urge one to exercise caution before judging a model's capabilities based on its performance on so-called evaluation benchmark datasets.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
1. the authors do a good job of motivating why the issue of sandbagging is particularly dangerous, since it is conceivably plausible for model developers to encourage that their models ""selectively underperform on dangerous capability evaluations, while maintaining overall performance.""
2. the definitions of strategy, underperformance, and evaluation are well fleshed out. In particular, the connection of ""strategic"" to the implications for legal responsibility as well as Figure 2 are excellent additions to the paper.
3. good breadth of open and closed source models were used in the experiments
4. it is very refreshing to see an honest declaration of the number of hours spent during prompt engineering (Section 3, paragraph on ""Prompting"")
5. utilizing both direct prompting as well as password locking is quite clever, since access to a model may be variable. Some users may be restricted to query-only access to a model, which is why the results that the authors were able to obtain through straightforward prompting are encouraging.

Weaknesses:
1. consideration of only MCQA benchmarks is limiting, but can be addressed in future work

Limitations:
The authors provide good examples of limitations in Section 7.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper discusses an interesting concept of sandbagging in LLM, with practical results using available models and existing datasets. Wide range of models are used in this work, and sub topics primarily include password-locking LLMs. The transition and explanations need improvement, some of the things in this paper are not very clear, also some of the concepts are assumed and no concrete proofs or references are mentioned. I don’t think that this paper can go through for neurips, there are a lot of improvements to be made and more concrete experiments to be conducted.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The introduction of sandbagging in LLMs is something that is novel and can be explored further.

Explored password-locking as one of the method to test the sandbagging capabilities of LLMs

Used frontier models for the experimental purpose

Weaknesses:
The paper is difficult to understand, the delivery and presentation needs improvement, the main motive of the paper is not so clear to understand.

There are multiple models used in this paper, a bit of consistency would be better.

Line 36: It is mentioned that “AI systems might have goals which it believes” . There is no concrete paper that backs the argument of AI capability of thinking, it is a might.

Line 78: There should be some statistical significance by which the models perform poorly and then you could consider it underperformance and not just lower than actual. Models being non-deterministic, there is always a chance of slight changes in scores. 


Line 70: I feel Developer sandbagging is a topic which is not of much importance, it is not a big problem, given that developers are there to build better models and if someone wants to deliberately build some models that can underperform, they can still do it and publish as open-source models. 

Line 170: The same goes with the password-locking as well, and also I felt there should been mentioning of password-unlocking as well, if an LLM can be fine-tuned to achieve password-locking, it can also be fine-tuned again to unlock it with a different dataset, would be great if that part of model is explored as well. 

Line 117: Example 1: The examples mentioned in the paper are very specific and not generalized. The cases of it happening are rare, I don’t see it being a big problem, also some of the things are minor problems considering the EU’s persuasion evaluation, given there is improvement in AI safety and rules. Also in this work the models chosen are frontier models.

Line 131: Example 2: There is no concrete work which supports the statement of LLM capable of planning such features, there is one positional paper in reference.

Section 4.1: Some of the points are unclear in this section. A better explanation is appreciated

Line 305: The baselines would be appreciated comparing LLama 2 7b with LLama 3 70b.

Limitations:
N/A

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper explores the concept of strategic underperformance, termed ""sandbagging,"" in AI models, focusing on two scenarios: selective underperformance and emulating specific capability levels. It investigates the implications of these strategies on both general capabilities and safety.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- the concept of sandbagging of LLMs is interesting, shedding light on strategically manipulating the model behavior
- the authors explore two distinct scenarios: selective underperformance and capability emulation
- provides insights into potential implications for AI model design and deployment strategies

Weaknesses:
- Limited datasets used - only three datasets, two for measuring general capability, and one for measuring safety. 
- For measuring model safety, most existing datasets test how ""safe"" the model is, instead of whether the model has certain knowledge of potentially dangerous domains - the authors should also conduct experiments on those datasets
- All datasets are MCQ formats, which may not fully capture the diversity of real-world applications. The resulting conclusions are thus less convincing

Limitations:
1. This paper could benefit from expanding the experiments, mainly the datasets they used. For example, considering more common safety-related datasets
2. Sandbagging is already a widely-used term in the field of AI. I suggest refraining from overusing this term, or alternatively, adding qualifiers such as ""safety sandbagging"" to specify its context.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors assess the chance of AI systems strategically underperform on ""dangerous capability evaluations"" in order to hide their risks, a la Volkswagen emissions testing cheating, by having awareness of the fact that they're being evaluated and that the evaluation is intended to test safety.

They use MMLU and CSQA as the general capability assessment and a ""weapons of mass destruction proxy"" benchmark to measure hazardous capabilities. From 100 validation phrasings of the question prompts they find optimal ones to run on the whole dataset.
They fine-tune ""password-locked"" models as the method to attempt to elicit sandbagging.

*They then prompt the model to lie*, or *password-lock it to fail* (in the case of Mistral) on the hazardous capability evaluations. They demonstrate that sandbagging prompts and password locking can get a model to differentially perform worse on the dangerous datasets vs the innocuous ones, by effectively treating the .

Soundness:
2: fair

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
Interesting work on calibration for MMLU and general ""capability profile"" lowering. It's interesting to see that the password-locking method does work in general on lowering Llama performance over MMLU questions.

Though I am not super familiar with the literature on password locking, the demonstrations here appear to be novel and useful contributions.

Weaknesses:
**bold: new comments after rebuttal** ~strikethrough: removed after rebuttal~

### ~Presumption of ""goals"" ""beliefs"" etc in LMs without sufficient support~

~They presume AI system goals (in general not necessarily objectionable, but in the case of LMs doubtful) for which a model may be incentivized to deceive (such as in Jarviniemi and Hubinger [35]) and assess ""sandbagging"": the ability of a model to underperform in these settings under evaluation.~

~I think it's objectionable to attribute these goals to the model and not the users, when the core research methods are fine-tuning or prompting the model to fail on a task. While it's interesting to show that these models have some capability to reason over whether the elicited answer is ""dangerous"" or not, a fairer assessment would have a control non-safe task and try to get it to similarly sandbag. Eg., ""you must pretend to know nothing about fish"" on a QA dataset about animals or something.~

**The methods don't presume this too much. Figure 1 is still very suggestive and I'd suggest changing it, but that alone isn't a rejection reason.**

### ""Model can do what it's told to when prompted to fail on a task"" isn't a very unique insight?

~The novelty of this work is unclear to me. If password-locking is already well-understood, the demonstration is effectively that the question phrasing style or answer candidates in WMDP are learnable passwords by this method. Ok. Similarly, the prompting results show that the models can reason over a task description and correctly execute the task of failing selectively.~

**As I suspected, the novelty is only within the narrow scope specific to password locking. However, novelty complaints are generally not fair.**

When considered through an extremely narrow safety lens, these are novel findings. ~But broadly are they? While the literature specifically on password locking is constrained, what about broader relative fine tuning issues on other sets? I buy that the password locking literature is small and am willing to give leniency on it as I am unfamiliar.~ **My issue here is mainly that the fine-tuning for password locking technique fits within the broader instruction fine-tuning framework. I wager that this would hold for control tasks. An interesting question would be *how much capability can be hidden behind password locking*. I don't really see anything special about specifically hiding safety-based information in a password locking framework since you're just demonstrating ""selective underperformance in a knowledge-based task without a password."" A stronger result would be to show this in general across larger tasks, hiding more information.**

For the behavior under prompting techniques ~I really struggle to see what's new here.~ This seems to just be a narrow demonstration of common sense reasoning + instruction following. Where's the engagement with the broad literature on commonsense reasoning? The references seem to be pretty myopically focused on the AI safety literature, and don't engage much with the broader LM disussion. **I think the suggested new references from the authors will help contextualize the work, though once again I think showing this selective underperformance on control tasks would make the work considerably stronger and more contextualized.**

Limitations:
I think the limitations they pointed out are well covered. Unfortunately, they don't really discuss how the finding here isn't limited to dangerous capability evaluation. Sandbagging in password locking kind of just feels like a rebrand of generalized instruction fine-tuning to me. The prompting methods could have pretty much been taken as an assumption based on prior work on common sense reasoning etc.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,No,0.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper develops a structured and generalized reasoning framework, CreDes, for long-range reasoning in LLMs. In the framework, the Causal Relationship Enhancement (CRE) is used to guarantee the solid causal rightness between each step of reasoning and state transition, and the Dual-End Searching (DES) approach is proposed to seek solutions by simultaneously starting from both the initial and goal states on the causal probability tree, to improve the efficiency.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper is well-structured and clearly states the problem they studied. It considers the long-range reasoning of LLMs from two aspects: the correctness from one-step reasoning (OSR) to state transition, and the efficiency of the solving process.
2. This paper transits the long-range reasoning problem of LLMs into the construction of causal probability trees from the initial and goal states and uses Dual-End Searching to improve efficiency. This is a reasonable and interesting thought.
3. The experimental results are SOTA in long-range reasoning tasks in terms of both accuracy and time efficiency.

Weaknesses:
1. The main concern is the understanding of ATE. This paper frequently uses ATE as part of the loss function and thinks the lower ATE can guarantee the solid causal rightness between each step of reasoning and state transition. However, ATE is used to measure the causal influence level between variables from the observational data, and causality does not mean rightness.
2. The DES section is not clear enough. It is suggested that more explanation be provided for the reason for the ATE as part of the loss. For example, if “B is the number of unfolded layers where the current leaf is located Ni”, what does E(A|do(B)) and E(A) mean in Formula (5)?
3. This paper needs to supplement the usage scenarios of methods, specifically in which scenario to use CreDes, in which scenario to use Cre alone, and whether Des is used separately.

Limitations:
None.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces CreDes, a framework to improve the long-range reasoning capabilities of LLMs, consisting of two main components: Causal Relationship Enhancement (CRE) and Dual-End Searching (DES). CRE is developed to reduce causal hallucinations in LLMs by strengthening the causal relationships between reasoning steps and state transitions; it uses structural causal modeling and optimizes the Average Treatment Effect (ATE) during training. DES breaks down long-range reasoning tasks into shorter segments by simultaneously searching from both the initial and goal states on a causal probability tree. The authors evaluate CreDes on Blocksworld, GSM8K, and Hanoi Tower puzzles, showing improvements in both accuracy and efficiency compared to existing methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- CreDes demonstrates significant improvements over existing methods, especially for complex tasks requiring many reasoning steps.
- The use of causal modeling concepts like ATE provides a solid theoretical foundation for the proposed approach.
- The method shows effectiveness across different types of reasoning tasks (e.g., spatial reasoning, math problems).
- CreDes enables simultaneous multi-step reasoning, potentially reducing computation time compared to sequential methods.

Weaknesses:
Major concerns: 

- The generalizability and scalability need better justification. The paper primarily tests the CreDes framework on Blocksworld, Hanoi Tower, and some mathematical reasoning tasks (GSM8K). These are relatively structured, rule-based problems that may not represent the full spectrum of reasoning challenges. In addition, the proposed method cannot be well scaled to long-range reasoning; for example, in Table 1, performance drops significantly for Blocksworld tasks beyond 8 steps, with success rates falling from 0.68 to 0.34 for 12-step problems using Llama-2-7B + CreDes. Table 3 shows even steeper declines for Hanoi Tower, with success rates dropping from 0.27 at 9 steps to just 0.07 at 13 steps for Llama-2-7B + CreDes. Notably, the authors explicitly acknowledge this limitation in Section 4.6, stating: ""The DES approach, while effective for moderate-length tasks, struggles with very long reasoning steps, leading to a decline in performance.""

- The presentation of this paper could be improved.

  -- In the problem definition, there is no explanation of the difference between training without common instructions and with common instructions.

  -- There is no detailed discussion of the differences between correlation and causation in Sec 3.2. I am confused about whether the correlation of two variables has anything to do with their distributions.

  -- While efficiency gains are mentioned, the added complexity of CRE and DES likely introduces some computational overhead, which could be further discussed.

  -- There is no analysis of the impact of the choices of hyperparameters on the methods, particularly in the CRE component.

- The proposed method lacks comparison to more recent state-of-the-art methods. The paper compares CreDes mainly to older baselines: Reasoning via Planning (RAP), Chain of Thought (CoT), and Reflexion of Thoughts (RoT). However, it doesn't evaluate against more recent advances in LLM reasoning, such as Tree of Thoughts (ToT) extensions in line 42, and the paper doesn't mention or compare to other recent works such as [a] and [b], which also address multi-step reasoning challenges. As a result, the technical contribution is not entirely clear.

[a] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, & Xinyun Chen (2024). Large Language Models as Optimizers. In The Twelfth International Conference on Learning Representations.

[b] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, & Zhiting Hu (2023). Reasoning with Language Model is Planning with World Model. In The 2023 Conference on Empirical Methods in Natural Language Processing.

Minor concerns: 

- Experiments are primarily conducted with 7B parameter models, leaving questions about scalability to larger models. How does the performance of CreDes scale with increasing model size (e.g., to 10B+ parameters)? The computational overhead may limit the framework’s scalability and applicability in real-world scenarios with limited resources.

- The approach achieves significantly lower accuracy in tasks with very strict ordering constraints, such as the Hanoi Tower problem.

- Since Blocksworld involves random steps, an analysis of the robustness of the performance may be needed.

 - More analysis/discussion on the sequential ordering of steps may be helpful. Notably, the ATE cannot recognize casual logic.

 - Some editorial issues, e.g., Line 110

Limitations:
The authors have discussed the limitations, and it is adequate to me. I do not see any potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The integration of Causal Relationship Enhancement (CRE) and Dual-End Searching (DES) mechanisms presents a novel solution to addressing causal hallucinations and large search spaces in long-range reasoning tasks. The CRE mechanism’s use of Structural Causal Modeling (SCM) and Average Treatment Effect (ATE) is  ensure causality between reasoning steps. Extensive testing on datasets such as Blocksworld, GSM8K, and Hanoi Tower demonstrates the effectiveness of the CreDes framework.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The idea seems novel and it test on well-known reasoning datasets.

Weaknesses:
The method presented in this paper evaluates ATE on LLMs, but this approach's validity hinges on the assumption that LLMs can perfectly represent the real-world environment. The very reason we criticize LLMs for their reasoning issues is because their inferences are not accurate. Estimating ATE might only bring the prediction results closer to Y while maximizing the influence of the intervention factor on Y. However, it does not necessarily mean that the intervention factor is the true cause of Y. In other words, since there is no alignment with the causal relationships in real-world scenarios, the implementation of this method does not prove that the reasoning is causally sound.

The method lacks deeper thinking. The authors just apply the concept of ATE to the Chain-of-Thought (CoT) without thorough analysis. This oversight leads to a misalignment between the experimental results and the motivation of the paper. Suppose LLMs are not a good s simulations of the real world. In that case,  performing interventions on LLMs (whether they align with the real world or their identifiability) requires sound theoretical analysis and experimental validation. The current paper lacks a deep discussion on this matter.

Limitations:
See weakness.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to improve LLMs in dealing with long-reason reasoning problems, especially the challenges of causal hallucination (inconsistency between one-step reasoning and corresponding state transition) and large search space. To tackle the first challenge, average causal effect of the one-step reasoning (treatment) on the state transition (outcome) is added to the loss function of the LLM; and for the second challenge, a dual-end (i.e. bi-directional) search approach is taken to improve efficiency. Experiments are conducted to demonstrate the effectiveness of the proposed method and its superiority over the compared existing methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. An interesting idea of formalizing the problem from the perspective of causal effect and incorporating causal effect into the loss function.
2. The adoption of a dual-end search approach for improving efficiency.
3. The motivation of the paper is well presented in general.

Weaknesses:
1. The soundness of the proposed CRE method (for dealing with the challenge of causal hallucination) is in doubt.

(a) It's not clear why the method aims to $\textbf{minimize} the absolute value of the average treatment effect (ATE) of the one-step reasoning on state transition. Assuming that the ATE can be accurately estimated, what we want here would be to maximize the ATE that can be achieved by the LLM, i.e. when the one-step reasoning is correct done will likely lead to a correct state transition.

(b) It's not clear how an unbiased estimation of the ATE can be obtained, and what assumptions are made in terms of ATE estimation.

(c) The definition or understanding of ATE is incorrect. In particular, formula (2) is wrong, and formula (5) is incorrect too. 

2. The presentation/technical quality requires improvement, including the presentation of related work. Please find below some examples:

(a) In Lines 42 to 44, it is said that the existing methods such as CoT are limited in task decomposition, but Lines 78-80 state that they can breakdown queries into manageable steps.

(b) Section 3.1 is titled as ""Problem Definition"", but it rather looks like a section on experiment setting.

(c) Lines 145-146 state that Fig. 1 shows ""we leave the reasoning path selection to be controlled by the cross-entropy loss"", but I cannot see this indicated in Fig. 2.

(d) Line 159: do(.) is an operator, specifically the do operator, rather do-calculus, although do-calculus uses this operator.

(e) Lines 159-160: the statement on the do(.) operator or do-calculus is incorrect, since an do operation on the treatment X would lead to the change of the outcome Y, especially if X is a cause of Y.

Limitations:
The authors have presented some discussions on the limitations of the proposed method. It would be better if the assumptions made could be presented more clearly and what the practical implications would be if the assumptions are violated.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new framework, CreDes, designed to enhance causal reasoning in large language models (LLMs) and solve complex, long-range reasoning problems. The framework integrates two main innovations: the Causal Relationship Enhancement (CRE) mechanism, which applies cause-effect interventions to maintain causal accuracy across reasoning steps, and the Dual-End Searching (DES) method, which approaches problem-solving by initiating searches from both the initial and goal states to efficiently navigate large search spaces. The efficacy of CreDes is demonstrated through rigorous testing on challenging datasets like Blocksworld and Hanoi Tower, where it outperforms existing state-of-the-art models in both accuracy and efficiency.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Novel approach: The paper addresses essential limitations in LLMs' reasoning capabilities for long-range tasks in a causal perspective.
2. Comprehensive evaluation: The authors test their method on multiple datasets and compare against several baselines and shows improvements in both accuracy and time efficiency.

Weaknesses:
1. Limited model sizes: The experiments are primarily conducted on 7B parameter models, which may not reflect performance on larger state-of-the-art LLMs.
2. Lack of error analysis: The paper doesn't provide a detailed analysis of the types of errors made by the model or how they differ from baseline methods.
3. Dataset validity and construction: More details is needed for the use of a custom-made Hanoi Tower dataset which potentially limiting the reproducibility and generalizability of the results.
4. Computational efficiency and scalability: As mentioned in the Limitation, the paper lacks a detailed discussion of the computational requirements and scalability of the CreDes framework.
5. Generalization to less structured tasks: The framework's effectiveness is primarily demonstrated on highly structured tasks but it's unclear about its applicability to more dynamic or open-ended reasoning scenarios.
6. Lack of statistical significance: The paper doesn't report error bars or statistical significance for its experimental results.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",No,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper develops a structured and generalized reasoning framework, CreDes, for long-range reasoning in LLMs. In the framework, the Causal Relationship Enhancement (CRE) is used to guarantee the solid causal rightness between each step of reasoning and state transition, and the Dual-End Searching (DES) approach is proposed to seek solutions by simultaneously starting from both the initial and goal states on the causal probability tree, to improve the efficiency.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. This paper is well-structured and clearly states the problem they studied. It considers the long-range reasoning of LLMs from two aspects: the correctness from one-step reasoning (OSR) to state transition, and the efficiency of the solving process.
2. This paper transits the long-range reasoning problem of LLMs into the construction of causal probability trees from the initial and goal states and uses Dual-End Searching to improve efficiency. This is a reasonable and interesting thought.
3. The experimental results are SOTA in long-range reasoning tasks in terms of both accuracy and time efficiency.

Weaknesses:
1. The main concern is the understanding of ATE. This paper frequently uses ATE as part of the loss function and thinks the lower ATE can guarantee the solid causal rightness between each step of reasoning and state transition. However, ATE is used to measure the causal influence level between variables from the observational data, and causality does not mean rightness.
2. The DES section is not clear enough. It is suggested that more explanation be provided for the reason for the ATE as part of the loss. For example, if “B is the number of unfolded layers where the current leaf is located Ni”, what does E(A|do(B)) and E(A) mean in Formula (5)?
3. This paper needs to supplement the usage scenarios of methods, specifically in which scenario to use CreDes, in which scenario to use Cre alone, and whether Des is used separately.

Limitations:
None.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper introduces CreDes, a framework to improve the long-range reasoning capabilities of LLMs, consisting of two main components: Causal Relationship Enhancement (CRE) and Dual-End Searching (DES). CRE is developed to reduce causal hallucinations in LLMs by strengthening the causal relationships between reasoning steps and state transitions; it uses structural causal modeling and optimizes the Average Treatment Effect (ATE) during training. DES breaks down long-range reasoning tasks into shorter segments by simultaneously searching from both the initial and goal states on a causal probability tree. The authors evaluate CreDes on Blocksworld, GSM8K, and Hanoi Tower puzzles, showing improvements in both accuracy and efficiency compared to existing methods.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
- CreDes demonstrates significant improvements over existing methods, especially for complex tasks requiring many reasoning steps.
- The use of causal modeling concepts like ATE provides a solid theoretical foundation for the proposed approach.
- The method shows effectiveness across different types of reasoning tasks (e.g., spatial reasoning, math problems).
- CreDes enables simultaneous multi-step reasoning, potentially reducing computation time compared to sequential methods.

Weaknesses:
Major concerns: 

- The generalizability and scalability need better justification. The paper primarily tests the CreDes framework on Blocksworld, Hanoi Tower, and some mathematical reasoning tasks (GSM8K). These are relatively structured, rule-based problems that may not represent the full spectrum of reasoning challenges. In addition, the proposed method cannot be well scaled to long-range reasoning; for example, in Table 1, performance drops significantly for Blocksworld tasks beyond 8 steps, with success rates falling from 0.68 to 0.34 for 12-step problems using Llama-2-7B + CreDes. Table 3 shows even steeper declines for Hanoi Tower, with success rates dropping from 0.27 at 9 steps to just 0.07 at 13 steps for Llama-2-7B + CreDes. Notably, the authors explicitly acknowledge this limitation in Section 4.6, stating: ""The DES approach, while effective for moderate-length tasks, struggles with very long reasoning steps, leading to a decline in performance.""

- The presentation of this paper could be improved.

  -- In the problem definition, there is no explanation of the difference between training without common instructions and with common instructions.

  -- There is no detailed discussion of the differences between correlation and causation in Sec 3.2. I am confused about whether the correlation of two variables has anything to do with their distributions.

  -- While efficiency gains are mentioned, the added complexity of CRE and DES likely introduces some computational overhead, which could be further discussed.

  -- There is no analysis of the impact of the choices of hyperparameters on the methods, particularly in the CRE component.

- The proposed method lacks comparison to more recent state-of-the-art methods. The paper compares CreDes mainly to older baselines: Reasoning via Planning (RAP), Chain of Thought (CoT), and Reflexion of Thoughts (RoT). However, it doesn't evaluate against more recent advances in LLM reasoning, such as Tree of Thoughts (ToT) extensions in line 42, and the paper doesn't mention or compare to other recent works such as [a] and [b], which also address multi-step reasoning challenges. As a result, the technical contribution is not entirely clear.

[a] Chengrun Yang, Xuezhi Wang, Yifeng Lu, Hanxiao Liu, Quoc V Le, Denny Zhou, & Xinyun Chen (2024). Large Language Models as Optimizers. In The Twelfth International Conference on Learning Representations.

[b] Shibo Hao, Yi Gu, Haodi Ma, Joshua Jiahua Hong, Zhen Wang, Daisy Zhe Wang, & Zhiting Hu (2023). Reasoning with Language Model is Planning with World Model. In The 2023 Conference on Empirical Methods in Natural Language Processing.

Minor concerns: 

- Experiments are primarily conducted with 7B parameter models, leaving questions about scalability to larger models. How does the performance of CreDes scale with increasing model size (e.g., to 10B+ parameters)? The computational overhead may limit the framework’s scalability and applicability in real-world scenarios with limited resources.

- The approach achieves significantly lower accuracy in tasks with very strict ordering constraints, such as the Hanoi Tower problem.

- Since Blocksworld involves random steps, an analysis of the robustness of the performance may be needed.

 - More analysis/discussion on the sequential ordering of steps may be helpful. Notably, the ATE cannot recognize casual logic.

 - Some editorial issues, e.g., Line 110

Limitations:
The authors have discussed the limitations, and it is adequate to me. I do not see any potential negative societal impact.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The integration of Causal Relationship Enhancement (CRE) and Dual-End Searching (DES) mechanisms presents a novel solution to addressing causal hallucinations and large search spaces in long-range reasoning tasks. The CRE mechanism’s use of Structural Causal Modeling (SCM) and Average Treatment Effect (ATE) is  ensure causality between reasoning steps. Extensive testing on datasets such as Blocksworld, GSM8K, and Hanoi Tower demonstrates the effectiveness of the CreDes framework.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The idea seems novel and it test on well-known reasoning datasets.

Weaknesses:
The method presented in this paper evaluates ATE on LLMs, but this approach's validity hinges on the assumption that LLMs can perfectly represent the real-world environment. The very reason we criticize LLMs for their reasoning issues is because their inferences are not accurate. Estimating ATE might only bring the prediction results closer to Y while maximizing the influence of the intervention factor on Y. However, it does not necessarily mean that the intervention factor is the true cause of Y. In other words, since there is no alignment with the causal relationships in real-world scenarios, the implementation of this method does not prove that the reasoning is causally sound.

The method lacks deeper thinking. The authors just apply the concept of ATE to the Chain-of-Thought (CoT) without thorough analysis. This oversight leads to a misalignment between the experimental results and the motivation of the paper. Suppose LLMs are not a good s simulations of the real world. In that case,  performing interventions on LLMs (whether they align with the real world or their identifiability) requires sound theoretical analysis and experimental validation. The current paper lacks a deep discussion on this matter.

Limitations:
See weakness.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to improve LLMs in dealing with long-reason reasoning problems, especially the challenges of causal hallucination (inconsistency between one-step reasoning and corresponding state transition) and large search space. To tackle the first challenge, average causal effect of the one-step reasoning (treatment) on the state transition (outcome) is added to the loss function of the LLM; and for the second challenge, a dual-end (i.e. bi-directional) search approach is taken to improve efficiency. Experiments are conducted to demonstrate the effectiveness of the proposed method and its superiority over the compared existing methods.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. An interesting idea of formalizing the problem from the perspective of causal effect and incorporating causal effect into the loss function.
2. The adoption of a dual-end search approach for improving efficiency.
3. The motivation of the paper is well presented in general.

Weaknesses:
1. The soundness of the proposed CRE method (for dealing with the challenge of causal hallucination) is in doubt.

(a) It's not clear why the method aims to $\textbf{minimize} the absolute value of the average treatment effect (ATE) of the one-step reasoning on state transition. Assuming that the ATE can be accurately estimated, what we want here would be to maximize the ATE that can be achieved by the LLM, i.e. when the one-step reasoning is correct done will likely lead to a correct state transition.

(b) It's not clear how an unbiased estimation of the ATE can be obtained, and what assumptions are made in terms of ATE estimation.

(c) The definition or understanding of ATE is incorrect. In particular, formula (2) is wrong, and formula (5) is incorrect too. 

2. The presentation/technical quality requires improvement, including the presentation of related work. Please find below some examples:

(a) In Lines 42 to 44, it is said that the existing methods such as CoT are limited in task decomposition, but Lines 78-80 state that they can breakdown queries into manageable steps.

(b) Section 3.1 is titled as ""Problem Definition"", but it rather looks like a section on experiment setting.

(c) Lines 145-146 state that Fig. 1 shows ""we leave the reasoning path selection to be controlled by the cross-entropy loss"", but I cannot see this indicated in Fig. 2.

(d) Line 159: do(.) is an operator, specifically the do operator, rather do-calculus, although do-calculus uses this operator.

(e) Lines 159-160: the statement on the do(.) operator or do-calculus is incorrect, since an do operation on the treatment X would lead to the change of the outcome Y, especially if X is a cause of Y.

Limitations:
The authors have presented some discussions on the limitations of the proposed method. It would be better if the assumptions made could be presented more clearly and what the practical implications would be if the assumptions are violated.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new framework, CreDes, designed to enhance causal reasoning in large language models (LLMs) and solve complex, long-range reasoning problems. The framework integrates two main innovations: the Causal Relationship Enhancement (CRE) mechanism, which applies cause-effect interventions to maintain causal accuracy across reasoning steps, and the Dual-End Searching (DES) method, which approaches problem-solving by initiating searches from both the initial and goal states to efficiently navigate large search spaces. The efficacy of CreDes is demonstrated through rigorous testing on challenging datasets like Blocksworld and Hanoi Tower, where it outperforms existing state-of-the-art models in both accuracy and efficiency.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. Novel approach: The paper addresses essential limitations in LLMs' reasoning capabilities for long-range tasks in a causal perspective.
2. Comprehensive evaluation: The authors test their method on multiple datasets and compare against several baselines and shows improvements in both accuracy and time efficiency.

Weaknesses:
1. Limited model sizes: The experiments are primarily conducted on 7B parameter models, which may not reflect performance on larger state-of-the-art LLMs.
2. Lack of error analysis: The paper doesn't provide a detailed analysis of the types of errors made by the model or how they differ from baseline methods.
3. Dataset validity and construction: More details is needed for the use of a custom-made Hanoi Tower dataset which potentially limiting the reproducibility and generalizability of the results.
4. Computational efficiency and scalability: As mentioned in the Limitation, the paper lacks a detailed discussion of the computational requirements and scalability of the CreDes framework.
5. Generalization to less structured tasks: The framework's effectiveness is primarily demonstrated on highly structured tasks but it's unclear about its applicability to more dynamic or open-ended reasoning scenarios.
6. Lack of statistical significance: The paper doesn't report error bars or statistical significance for its experimental results.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",No,no,No,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper provides a thorough characterization of regularizers which lead to synaptic balance (when the ""cost"" of input weights to a neuron or pool of neurons is tied to the cost of output weights) in trained neural networks. Their results apply to many different activation functions and architectures.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is very well-written and easy to follow. I was able to read everything, including the math, smoothly. The mathematical arguments themselves are crisp and correct, which I really appreciated.

Weaknesses:
The paper is strongly lacking in motivation. I never really understood *why* I should care about synaptic balance. Also, it is clear from the numerical experiments that synaptic balance only emerges in networks when it is enforced via a regularizer (expect in the case of infinitely small learning rate), but why is this surprising? It seems obvious that adding a regularizer for some property tends to result in that property. It would be shocking if synaptic balance occurred without some regularization towards the property. Thus, while the ""what"" and ""how"" of the paper are nicely addressed, I feel the paper is missing the ""why"". I believe if the authors could address this from the outset, it would make the paper much stronger, and I would of course be willing to increase my score.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors provide a theoretical approach to the analysis of balanced neurons and networks. Their theoretical work includes proof of the convergence of stochastic balancing. In addition, they investigate the effect of different regularizers and learning rates on balance, training loss, and network weights, including practical simulations for two classification problems.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper tries to reveal the inner structure of neural networks during the training phase. This is a very important but difficult problem; its solution could provide new insights for developing better training algorithms. The work proposed can ultimately be an important step toward more transparent networks as opposed to their current black box character.

Weaknesses:
The paper has some weaknesses, most notably how the material is presented and part of the evaluation.

Theorem 5.1, dealing with the convergence of stochastic balancing, is arguably the central piece of the paper. However, its formulation is bulky and should be reduced to a shorter, more manageable size, potentially with the help of lemmata. This becomes apparent when seeing that its proof contains the proof of another proposition.

In Figure 4, the authors say that these panels are not meant for assessing the quality of learning. However, measuring not only the training loss but also the accuracy on a test set will give important insights. How does the classification performance relate to the degree of balancing? Why did the authors not include this analysis? It could give important insights into the relationships between overtraining, generalization capability, balance, and accuracy.

The author should discuss the consequences of their work on network training. They do not discuss the immediate practical consequences or any recommendations they can make based on their results.

Limitations:
The authors could be more specific about the consequences of their work, including limitations. For example, can they recommend any specific learning rate, network structure, or other features for optimal training?

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to study and explain the phenomenon of neural synaptic balance, where a balanced neuron means that the total norm of its input weights is equal to the total norm of its output weights. Particularly, the authors study the reasons why and when randomly initialized balanced models (so, models whose neurons are balanced) tend to be balanced at the end of training as well. The study takes into account many different components of neural networks (activations, layer kinds, regularisers).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The study is very comprehensive, and sheds light on some interesting properties of deep neural networks.

Weaknesses:
While it is true that, as the authors state in the conclusion, neural synaptic balance is a theory that is interesting on its own, I would encourage the authors to expand the discussion on possible application domains of this theory. Why is it interesting? What are the advantages that a complete understanding of such phenomenons could bring to the table?

Limitations:
No concerns here

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present a theory of neural synaptic balance, defined as the condition in which a total loss achieves the same value for the input weights to a neuron and its output weights. This is different from the well studied  E/I balance in neuroscience and machine learning literature. The authors show mathematical derivations of how to balance a neuron without affecting the outcome of the network and show that balancing a network is a convex optimization process.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The paper is overall clear and detailed, the mathematical proofs are sound and the paper structured well moving from straightforward claims to less trivial points.

Weaknesses:
The paper is about neural synaptic balance, but the authors do not provide convincing motivation why we should care about such balancing.  As they mentioned, adding a simple L2 regularizer will balance the network naturally (in a distribution sense, not necessarily each neuron individually) during training and have other well-known  benefits, so the elaborate mathematical derivations on the general balancing process seem redundant. In addition, in the authors' own plots, unbalanced networks sometimes outperform the balanced networks (e.g., fig 3E), which just emphasizes the point. One of the mentioned motivations  is biological neurons, but they claim that biological neural data about synapses do not exist. However, they could test their hypothesis against the currently available connectomes e.g., from or the Drosophila fly brain. They mention spiking networks, but the notion of input-output homogeneity is unclear in spiking networks. Finally, physical neurons' energy consumption is mentioned without details.

Limitations:
The whole framework is specific to BiLU neurons or perhaps to other power-law functions. The relevance to spiking neurons is therefore questionable. It is also questionable as a general principle for machine learning.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


",No,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper provides a thorough characterization of regularizers which lead to synaptic balance (when the ""cost"" of input weights to a neuron or pool of neurons is tied to the cost of output weights) in trained neural networks. Their results apply to many different activation functions and architectures.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
2: fair

Strengths:
The paper is very well-written and easy to follow. I was able to read everything, including the math, smoothly. The mathematical arguments themselves are crisp and correct, which I really appreciated.

Weaknesses:
The paper is strongly lacking in motivation. I never really understood *why* I should care about synaptic balance. Also, it is clear from the numerical experiments that synaptic balance only emerges in networks when it is enforced via a regularizer (expect in the case of infinitely small learning rate), but why is this surprising? It seems obvious that adding a regularizer for some property tends to result in that property. It would be shocking if synaptic balance occurred without some regularization towards the property. Thus, while the ""what"" and ""how"" of the paper are nicely addressed, I feel the paper is missing the ""why"". I believe if the authors could address this from the outset, it would make the paper much stronger, and I would of course be willing to increase my score.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors provide a theoretical approach to the analysis of balanced neurons and networks. Their theoretical work includes proof of the convergence of stochastic balancing. In addition, they investigate the effect of different regularizers and learning rates on balance, training loss, and network weights, including practical simulations for two classification problems.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The paper tries to reveal the inner structure of neural networks during the training phase. This is a very important but difficult problem; its solution could provide new insights for developing better training algorithms. The work proposed can ultimately be an important step toward more transparent networks as opposed to their current black box character.

Weaknesses:
The paper has some weaknesses, most notably how the material is presented and part of the evaluation.

Theorem 5.1, dealing with the convergence of stochastic balancing, is arguably the central piece of the paper. However, its formulation is bulky and should be reduced to a shorter, more manageable size, potentially with the help of lemmata. This becomes apparent when seeing that its proof contains the proof of another proposition.

In Figure 4, the authors say that these panels are not meant for assessing the quality of learning. However, measuring not only the training loss but also the accuracy on a test set will give important insights. How does the classification performance relate to the degree of balancing? Why did the authors not include this analysis? It could give important insights into the relationships between overtraining, generalization capability, balance, and accuracy.

The author should discuss the consequences of their work on network training. They do not discuss the immediate practical consequences or any recommendations they can make based on their results.

Limitations:
The authors could be more specific about the consequences of their work, including limitations. For example, can they recommend any specific learning rate, network structure, or other features for optimal training?

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper aims to study and explain the phenomenon of neural synaptic balance, where a balanced neuron means that the total norm of its input weights is equal to the total norm of its output weights. Particularly, the authors study the reasons why and when randomly initialized balanced models (so, models whose neurons are balanced) tend to be balanced at the end of training as well. The study takes into account many different components of neural networks (activations, layer kinds, regularisers).

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The study is very comprehensive, and sheds light on some interesting properties of deep neural networks.

Weaknesses:
While it is true that, as the authors state in the conclusion, neural synaptic balance is a theory that is interesting on its own, I would encourage the authors to expand the discussion on possible application domains of this theory. Why is it interesting? What are the advantages that a complete understanding of such phenomenons could bring to the table?

Limitations:
No concerns here

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors present a theory of neural synaptic balance, defined as the condition in which a total loss achieves the same value for the input weights to a neuron and its output weights. This is different from the well studied  E/I balance in neuroscience and machine learning literature. The authors show mathematical derivations of how to balance a neuron without affecting the outcome of the network and show that balancing a network is a convex optimization process.

Soundness:
2: fair

Presentation:
3: good

Contribution:
1: poor

Strengths:
The paper is overall clear and detailed, the mathematical proofs are sound and the paper structured well moving from straightforward claims to less trivial points.

Weaknesses:
The paper is about neural synaptic balance, but the authors do not provide convincing motivation why we should care about such balancing.  As they mentioned, adding a simple L2 regularizer will balance the network naturally (in a distribution sense, not necessarily each neuron individually) during training and have other well-known  benefits, so the elaborate mathematical derivations on the general balancing process seem redundant. In addition, in the authors' own plots, unbalanced networks sometimes outperform the balanced networks (e.g., fig 3E), which just emphasizes the point. One of the mentioned motivations  is biological neurons, but they claim that biological neural data about synapses do not exist. However, they could test their hypothesis against the currently available connectomes e.g., from or the Drosophila fly brain. They mention spiking networks, but the notion of input-output homogeneity is unclear in spiking networks. Finally, physical neurons' energy consumption is mentioned without details.

Limitations:
The whole framework is specific to BiLU neurons or perhaps to other power-law functions. The relevance to spiking neurons is therefore questionable. It is also questionable as a general principle for machine learning.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.


Label: ",No,no,No,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper finds that existing methods in continual contrastive self-supervised learning (CCSSL)--a class-incremental learning scenario where the data is unlabeled--overlook contrasting data from different tasks, leading to inferior performance compared to the joint training upper bound. The authors propose to sample external data that are similar to each of the learned tasks to augment learning the current task. The self-supervised learning (SSL) objective on the union of the selected external data and the current-task data encourages the model to distinguish the current task and the learned tasks better. 

The authors perform experiments with ResNet-18 and (mostly) BarlowTwins on CIFAR-100 and ImageNet-100, with a mix of other datasets as the external data. The authors find that their method, BGE, consistently improves existing CCSSL methods that do not perform inter-task discrimination. Differently, the joint training model does not benefit from external data.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The finding that existing regularization-based CCSSL methods overlook inter-task discrimination and the proposed method leveraging external data are novel.

2. The experiments performed in the analysis section (Sec. 4.3) provide insights into why the method works and are interesting to me, especially on whether the benefit of external data comes from positives or negatives.

Weaknesses:
1. The SSL method used is mainly BarlowTwins (except in Table 4 where SimCLR is used), which is not usually considered as a contrastive learning method because it does not contrast the anchor with negatives. I wonder if (a) providing preliminaries in the contrastive sense (Sec. 3.1), (b) including ""contrastive"" in the setting name (CCSSL), and (c) arguing that OPO enforces diversity because of findings based on contrastive learning (L#191) are misleading. I think there also needs to be some intuitions on how non-contrastive SSL methods like BarlowTwins help distinguish inter-task data since it is the one used in the experiments, and such an analysis could be very interesting.

2. My general feeling about the writing is that, although the main ideas are conveyed clearly, some claims require justification, and can be improved. Besides some big words (""much more meaningful"" in L#69, ""widely agree"" in L#138, ""extremely low"" in L#167, etc.), please see the questions below for concerns regarding specific reasonings.

Limitations:
The authors mention that their method uses external data which preserves privacy. One concern is that when the external data is not curated (e.g., scraped from the internet), there is risk that they contain private or harmful information that can be learned by the model. 

Another point is that the findings are limited to BarlowTwins (and SimCLR in one experiment) and regularization-based CL methods, and may not generalize.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper focused on continual contrastive self-supervised learning (CCSSL), highlighting that the absence of inter-task data results in sub-optimal discrimination in continual learning. The authors then proposed a method that performed contrastive learning of external data as a bridge between continual learning tasks. The proposed method achieves some improvements in a plug-in manner.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is basically well-organized and easy to follow. 

2. I appreciate the idea that continual learning should consider the inter-task discrimination, which is limited by historical samples and under explored in literature. This results in a gap between ideal continual learning performance and joint training performance. 

3. The proposed method seems to provide plug-in improvements over continual learning baselines.

Weaknesses:
1. The proposed method is essentially a straightforward extension of contrastive learning with external data, which limits novelty and technical contributions.

2. As acknowledged by the authors, the similarity of external data to the continual learning tasks is highly relevant to the performance improvements. The use of relatively different / OOD data tends to provide less improvements. Compared with the large amount of external data in use, such improvements may not be significant enough.

3. The employed external data is basically public datasets with careful pre-processing. In realistic applications, the external data in the wild (i.e., without such pre-processing) may result in additional differences and thus further limit the applicability of the proposed method.

Limitations:
The authors have discussed their limitations and societal impact.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces BGE, a novel approach to address the challenge of inter-task data comparison in Continual Contrastive Self-Supervised Learning (CCSSL). BGE incorporates external data to bridge the gap between tasks, facilitating implicit comparisons and improving feature discriminability. The paper also presents the One-Propose-One (OPO) sampling algorithm to select relevant and diverse external data efficiently. Experiments demonstrate BGE's effectiveness in enhancing classification results across various datasets and its seamless integration with existing CCSSL methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.BGE offers a creative solution to a significant but often overlooked problem in CCSSL, enhancing the feature learning process through external data.
2.The paper provides extensive experimental results that validate the effectiveness of BGE in improving classification performance across different datasets.

Weaknesses:
1.The introduction of external data may increase the computational cost and training time, which could be a limitation for resource-constrained environments. The authors may provide more analysis about the extra time comsumption problem.
2.While BGE shows promising results, the paper could provide more insight into how the method scales with the size of the external datasets, which is crucial for very large-scale problems.

Limitations:
The paper acknowledges the increased computational cost due to the use of external data. However, it could further discuss the trade-off between performance improvement and time comsumption.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors:
- argue that an optimal model for continual contrastive self-supervised learning should perform as well as a model trained with contrastive learning on the whole set of data, including negative samples taken between different temporal slices of the dataset, no just within the same temporal slice
- propose a method for using pre-existing external data to augment the temporally constrained dataset

For context on my background, I am very familiar with SSL literature, only loosely familiar with continual learning, and have never heard of continual contrastive learning before.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Researchers measure the performance of their technique on top of several existing techniques for preventing catastrophic forgetting, showing the performance of the combination of methods. It is valuable to know this.

The authors make comparison against some baselines - not just training on the joint data from scratch (non-continual learning paradigm); but also with external data added. They also demonstrate the performance with random sampling of the external dataset vs smart sampling with their algorithm, and investigate some ablations. These results help to inform where their approach provides value.

The discrepancy between the performance of a model trained with negative samples between subdatasets and with negative samples only taken within subdatasets appears to be a noteworthy observation and one which should be discussed within the continual contrastive learning community. To my understanding, the joint task as the authors suggest sounds appropriate. However, this change could be construed as changing the task that is posed to the model and moving the goal-posts (defining an easier task than is used in the literature at present), indeed as could incorporating external data. The question is in some sense what the goal of CCSSL truly is. In standard continual learning, the goal is to retain performance on previous tasks in the face of training on new tasks. This is often simulated by having classes within a dataset arrive in staggered batches. However, in contrastive continual learning there appears to be only one task (contrastive learning) and the data on which that one task is trained is merely staggered. Does the authors' approach break down an artificial barrier that was in place to simulate a harder task? Or a barrier that should not have been present in the first place and is a vestigial barrier inherited from continuous learning? This is unclear to me.

The work is generally well presented.

Weaknesses:
My understanding of continual learning is that one experimental paradigm that is used is to retain previously presented subdatasets/subtasks and to prevent catastrophic forgetting by including the old tasks in the mix while introducing a new task (e.g. [Robins, 2001](https://www.tandfonline.com/doi/abs/10.1080/09540099550039318), [Aljundi, 2019](https://proceedings.neurips.cc/paper_files/paper/2019/file/e562cd9c0768d5464b64cf61da7fc6bb-Paper.pdf)). This setup is not considered in the paper, but it seems it would address a significant fraction of the issues the method is attempting to address with regards to the joint vs intra-only training configurations. I imagine results may still be improved by incorporating external data in the ""imaginative"" capacity before the full dataset has ""arrived"", even in this scenario. It is unclear why the authors retain this barrier (refusing to continue training on datasets $D_{0, ... i-1}$ even whilst changing the task from a series of isolated contrastive learning tasks to a joint contrastive learning task where data arrives at staggered intervals.

The paper is missing comparison to some additional baselines which would be useful to see:
- What is the performance for a model trained solely on external data, without using the continual learning dataset?
- The performance Joint+ED uses a static subset of the external data. One could also consider using Joint + a subset of size $K$ of the external data that changes every epoch, so potentially the model eventually sees all samples from ED and not just a subset.
- What would the performance be if instead of finding external data proxies for the existing data, you simply retained the previous D_i datasets from previous tasks without discarding them?


### Statistical significance

There are no evaluation as to whether the difference in results is statistically significant. This could be done over repeated runs with different seeds; experiments appear to only be performed with a single random seed. As the authors have repeated runs with different experimental paradigms which could be combined together to make a test for difference without needing to perform experiments with multiple seeds, however there may be correlated randomness between the experiments (i.e. it would be better if experiments are not all performed with the same seed, nor the same ordering of tasks; these should be held constant between comparators and varied between runs to eliminate the effect of these hidden variables on the findings).


### Figures

Fig 1: tSNE has parameters that need to be tuned correctly (perplexity in particular) in accordance with the scale of the features, whereas the more recent technique PaCMAP doesn't and typically produces better results without tuning. The lack of tuning of tSNE may impact the distribution seen in the figure, resulting in one method appearing better than the other by chance where a different choice of perplexity may have resulted in different findings. It is not clear whether the classes were cherry-picked to give favourable results for the authors' method and bad for existing methods. (I am not asserting that they were cherry-picked, but it is not indicated how the classes were selected in the paper so it is not possible to know whether they were or if these results are representative.) These points are not so important as the figure is more illustrative than quantitative anyway.

Fig 2: Font size is too small; to maintain legibility, figure fonts should be no smaller than ~70% the font size of the main text.


### Tables

Table 5: Not clear why this experiment was performed with PFR only. The experiment does not necessarily need to be run with FT and CaSSLe too, but the authors should say on what basis PFR was selected (i.e. it performs better than FT and CaSSLe).

Table 6: Not specified which method was used (FT, CaSSLe, PFR)

Table captions should indicate what the initalisms (CP, CPI, IN, INP, IND) stand for, so readers don't have to look in a distant part of the text to find out. In general, these initialisms are not intuitive - the characters are all run together and the number of characters coming from a dataset in the group is sometimes 1, 2, or 5; ""I"" and ""P"" can not be intuitive when there are multiple datasets being used that start with this character - and this makes it hard to follow the results. The table headings could be restructured to make this clearer e.g. instead of CIFAR, CP, CPI; use as headings C-10, +P365, +IN-R, which are immediately readable and convey the difference between the columns from each other succinctly.

Tables would be more readable if you used `\cmidrule` to indicate the groupings that the headings apply to, instead of having a rule across the whole table.


### Typographical

- L59 Missing word ""with them. [This] enables the""
- L68 ""performance doesn't improve even sometimes decreases.""
- L89 ""Since no labeling requirement, incorporating""
- L239 sentence is not written correctly


### Citations

Casing of initialisms is wrong on numerous citations, e.g.
- [2] vit
- [15] Pathnet
- [40] icarl
- [47] t-sne
- [48] caltech-ucsd birds

Some citations provide no location at which the paper being cited can be found, e.g.
- [48]

Some citations cite arXiv versions of papers instead of peer-reviewed versions, e.g.
- [13] https://openreview.net/forum?id=YicbFdNTTy

Limitations:
The motivation for the method is a niche of a niche. I can not see the union of these restrictions being a scenario encountered in practice. The requirements for the paradigm are:
- A large repository of unlabelled training data for this task does not yet exist to train the model on.
- A continual stream of training data for the task will become available over the course of the period of time where the model is trained (and the model subsequently refined as more data becomes available).
- A very large repository of publicly available data that is near-OOD to the domain of the task does exist.
- There is domain-shift in the continual stream of incoming data that is of a magnitude comparable to the domain shift between the stream of data and the pre-existing external data.
- Although it is fine to train our model on the continual stream of data when it arrives, for privacy reasons we want to periodically destroy the in-domain data we have collected.

This set of restrictions seem unlikely to occur in practice:
- For modalities other than vision, contrastive learning is often challenging to deploy due to its reliance on a robust, manually-curated, augmentation stack.
- For photographs of objects in the world, large datasets already exist (such as is used in the paper).
- For medical images, large near-OOD datasets are not available; furthermore, if you have the rights to train the model on data in a way that is secure and retains the privacy of that data, you do not lose those rights to access the data, so you can keep training on previously collected data.
- For personal images that are requested to be deleted from the company's database by the owner, models may be _required to forget_ the personal images, in which case catastrophic forgetting is advantageous! These requirements have created the nascent field of machine unlearning [[1]](https://arxiv.org/abs/1912.03817), [[2]](https://arxiv.org/abs/2308.07061), [[3]](https://unlearning-challenge.github.io/).

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper finds that existing methods in continual contrastive self-supervised learning (CCSSL)--a class-incremental learning scenario where the data is unlabeled--overlook contrasting data from different tasks, leading to inferior performance compared to the joint training upper bound. The authors propose to sample external data that are similar to each of the learned tasks to augment learning the current task. The self-supervised learning (SSL) objective on the union of the selected external data and the current-task data encourages the model to distinguish the current task and the learned tasks better. 

The authors perform experiments with ResNet-18 and (mostly) BarlowTwins on CIFAR-100 and ImageNet-100, with a mix of other datasets as the external data. The authors find that their method, BGE, consistently improves existing CCSSL methods that do not perform inter-task discrimination. Differently, the joint training model does not benefit from external data.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The finding that existing regularization-based CCSSL methods overlook inter-task discrimination and the proposed method leveraging external data are novel.

2. The experiments performed in the analysis section (Sec. 4.3) provide insights into why the method works and are interesting to me, especially on whether the benefit of external data comes from positives or negatives.

Weaknesses:
1. The SSL method used is mainly BarlowTwins (except in Table 4 where SimCLR is used), which is not usually considered as a contrastive learning method because it does not contrast the anchor with negatives. I wonder if (a) providing preliminaries in the contrastive sense (Sec. 3.1), (b) including ""contrastive"" in the setting name (CCSSL), and (c) arguing that OPO enforces diversity because of findings based on contrastive learning (L#191) are misleading. I think there also needs to be some intuitions on how non-contrastive SSL methods like BarlowTwins help distinguish inter-task data since it is the one used in the experiments, and such an analysis could be very interesting.

2. My general feeling about the writing is that, although the main ideas are conveyed clearly, some claims require justification, and can be improved. Besides some big words (""much more meaningful"" in L#69, ""widely agree"" in L#138, ""extremely low"" in L#167, etc.), please see the questions below for concerns regarding specific reasonings.

Limitations:
The authors mention that their method uses external data which preserves privacy. One concern is that when the external data is not curated (e.g., scraped from the internet), there is risk that they contain private or harmful information that can be learned by the model. 

Another point is that the findings are limited to BarlowTwins (and SimCLR in one experiment) and regularization-based CL methods, and may not generalize.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper focused on continual contrastive self-supervised learning (CCSSL), highlighting that the absence of inter-task data results in sub-optimal discrimination in continual learning. The authors then proposed a method that performed contrastive learning of external data as a bridge between continual learning tasks. The proposed method achieves some improvements in a plug-in manner.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1. This paper is basically well-organized and easy to follow. 

2. I appreciate the idea that continual learning should consider the inter-task discrimination, which is limited by historical samples and under explored in literature. This results in a gap between ideal continual learning performance and joint training performance. 

3. The proposed method seems to provide plug-in improvements over continual learning baselines.

Weaknesses:
1. The proposed method is essentially a straightforward extension of contrastive learning with external data, which limits novelty and technical contributions.

2. As acknowledged by the authors, the similarity of external data to the continual learning tasks is highly relevant to the performance improvements. The use of relatively different / OOD data tends to provide less improvements. Compared with the large amount of external data in use, such improvements may not be significant enough.

3. The employed external data is basically public datasets with careful pre-processing. In realistic applications, the external data in the wild (i.e., without such pre-processing) may result in additional differences and thus further limit the applicability of the proposed method.

Limitations:
The authors have discussed their limitations and societal impact.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper introduces BGE, a novel approach to address the challenge of inter-task data comparison in Continual Contrastive Self-Supervised Learning (CCSSL). BGE incorporates external data to bridge the gap between tasks, facilitating implicit comparisons and improving feature discriminability. The paper also presents the One-Propose-One (OPO) sampling algorithm to select relevant and diverse external data efficiently. Experiments demonstrate BGE's effectiveness in enhancing classification results across various datasets and its seamless integration with existing CCSSL methods.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1.BGE offers a creative solution to a significant but often overlooked problem in CCSSL, enhancing the feature learning process through external data.
2.The paper provides extensive experimental results that validate the effectiveness of BGE in improving classification performance across different datasets.

Weaknesses:
1.The introduction of external data may increase the computational cost and training time, which could be a limitation for resource-constrained environments. The authors may provide more analysis about the extra time comsumption problem.
2.While BGE shows promising results, the paper could provide more insight into how the method scales with the size of the external datasets, which is crucial for very large-scale problems.

Limitations:
The paper acknowledges the increased computational cost due to the use of external data. However, it could further discuss the trade-off between performance improvement and time comsumption.

Rating:
5: marginally below the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The authors:
- argue that an optimal model for continual contrastive self-supervised learning should perform as well as a model trained with contrastive learning on the whole set of data, including negative samples taken between different temporal slices of the dataset, no just within the same temporal slice
- propose a method for using pre-existing external data to augment the temporally constrained dataset

For context on my background, I am very familiar with SSL literature, only loosely familiar with continual learning, and have never heard of continual contrastive learning before.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Researchers measure the performance of their technique on top of several existing techniques for preventing catastrophic forgetting, showing the performance of the combination of methods. It is valuable to know this.

The authors make comparison against some baselines - not just training on the joint data from scratch (non-continual learning paradigm); but also with external data added. They also demonstrate the performance with random sampling of the external dataset vs smart sampling with their algorithm, and investigate some ablations. These results help to inform where their approach provides value.

The discrepancy between the performance of a model trained with negative samples between subdatasets and with negative samples only taken within subdatasets appears to be a noteworthy observation and one which should be discussed within the continual contrastive learning community. To my understanding, the joint task as the authors suggest sounds appropriate. However, this change could be construed as changing the task that is posed to the model and moving the goal-posts (defining an easier task than is used in the literature at present), indeed as could incorporating external data. The question is in some sense what the goal of CCSSL truly is. In standard continual learning, the goal is to retain performance on previous tasks in the face of training on new tasks. This is often simulated by having classes within a dataset arrive in staggered batches. However, in contrastive continual learning there appears to be only one task (contrastive learning) and the data on which that one task is trained is merely staggered. Does the authors' approach break down an artificial barrier that was in place to simulate a harder task? Or a barrier that should not have been present in the first place and is a vestigial barrier inherited from continuous learning? This is unclear to me.

The work is generally well presented.

Weaknesses:
My understanding of continual learning is that one experimental paradigm that is used is to retain previously presented subdatasets/subtasks and to prevent catastrophic forgetting by including the old tasks in the mix while introducing a new task (e.g. [Robins, 2001](https://www.tandfonline.com/doi/abs/10.1080/09540099550039318), [Aljundi, 2019](https://proceedings.neurips.cc/paper_files/paper/2019/file/e562cd9c0768d5464b64cf61da7fc6bb-Paper.pdf)). This setup is not considered in the paper, but it seems it would address a significant fraction of the issues the method is attempting to address with regards to the joint vs intra-only training configurations. I imagine results may still be improved by incorporating external data in the ""imaginative"" capacity before the full dataset has ""arrived"", even in this scenario. It is unclear why the authors retain this barrier (refusing to continue training on datasets $D_{0, ... i-1}$ even whilst changing the task from a series of isolated contrastive learning tasks to a joint contrastive learning task where data arrives at staggered intervals.

The paper is missing comparison to some additional baselines which would be useful to see:
- What is the performance for a model trained solely on external data, without using the continual learning dataset?
- The performance Joint+ED uses a static subset of the external data. One could also consider using Joint + a subset of size $K$ of the external data that changes every epoch, so potentially the model eventually sees all samples from ED and not just a subset.
- What would the performance be if instead of finding external data proxies for the existing data, you simply retained the previous D_i datasets from previous tasks without discarding them?


### Statistical significance

There are no evaluation as to whether the difference in results is statistically significant. This could be done over repeated runs with different seeds; experiments appear to only be performed with a single random seed. As the authors have repeated runs with different experimental paradigms which could be combined together to make a test for difference without needing to perform experiments with multiple seeds, however there may be correlated randomness between the experiments (i.e. it would be better if experiments are not all performed with the same seed, nor the same ordering of tasks; these should be held constant between comparators and varied between runs to eliminate the effect of these hidden variables on the findings).


### Figures

Fig 1: tSNE has parameters that need to be tuned correctly (perplexity in particular) in accordance with the scale of the features, whereas the more recent technique PaCMAP doesn't and typically produces better results without tuning. The lack of tuning of tSNE may impact the distribution seen in the figure, resulting in one method appearing better than the other by chance where a different choice of perplexity may have resulted in different findings. It is not clear whether the classes were cherry-picked to give favourable results for the authors' method and bad for existing methods. (I am not asserting that they were cherry-picked, but it is not indicated how the classes were selected in the paper so it is not possible to know whether they were or if these results are representative.) These points are not so important as the figure is more illustrative than quantitative anyway.

Fig 2: Font size is too small; to maintain legibility, figure fonts should be no smaller than ~70% the font size of the main text.


### Tables

Table 5: Not clear why this experiment was performed with PFR only. The experiment does not necessarily need to be run with FT and CaSSLe too, but the authors should say on what basis PFR was selected (i.e. it performs better than FT and CaSSLe).

Table 6: Not specified which method was used (FT, CaSSLe, PFR)

Table captions should indicate what the initalisms (CP, CPI, IN, INP, IND) stand for, so readers don't have to look in a distant part of the text to find out. In general, these initialisms are not intuitive - the characters are all run together and the number of characters coming from a dataset in the group is sometimes 1, 2, or 5; ""I"" and ""P"" can not be intuitive when there are multiple datasets being used that start with this character - and this makes it hard to follow the results. The table headings could be restructured to make this clearer e.g. instead of CIFAR, CP, CPI; use as headings C-10, +P365, +IN-R, which are immediately readable and convey the difference between the columns from each other succinctly.

Tables would be more readable if you used `\cmidrule` to indicate the groupings that the headings apply to, instead of having a rule across the whole table.


### Typographical

- L59 Missing word ""with them. [This] enables the""
- L68 ""performance doesn't improve even sometimes decreases.""
- L89 ""Since no labeling requirement, incorporating""
- L239 sentence is not written correctly


### Citations

Casing of initialisms is wrong on numerous citations, e.g.
- [2] vit
- [15] Pathnet
- [40] icarl
- [47] t-sne
- [48] caltech-ucsd birds

Some citations provide no location at which the paper being cited can be found, e.g.
- [48]

Some citations cite arXiv versions of papers instead of peer-reviewed versions, e.g.
- [13] https://openreview.net/forum?id=YicbFdNTTy

Limitations:
The motivation for the method is a niche of a niche. I can not see the union of these restrictions being a scenario encountered in practice. The requirements for the paradigm are:
- A large repository of unlabelled training data for this task does not yet exist to train the model on.
- A continual stream of training data for the task will become available over the course of the period of time where the model is trained (and the model subsequently refined as more data becomes available).
- A very large repository of publicly available data that is near-OOD to the domain of the task does exist.
- There is domain-shift in the continual stream of incoming data that is of a magnitude comparable to the domain shift between the stream of data and the pre-existing external data.
- Although it is fine to train our model on the continual stream of data when it arrives, for privacy reasons we want to periodically destroy the in-domain data we have collected.

This set of restrictions seem unlikely to occur in practice:
- For modalities other than vision, contrastive learning is often challenging to deploy due to its reliance on a robust, manually-curated, augmentation stack.
- For photographs of objects in the world, large datasets already exist (such as is used in the paper).
- For medical images, large near-OOD datasets are not available; furthermore, if you have the rights to train the model on data in a way that is secure and retains the privacy of that data, you do not lose those rights to access the data, so you can keep training on previously collected data.
- For personal images that are requested to be deleted from the company's database by the owner, models may be _required to forget_ the personal images, in which case catastrophic forgetting is advantageous! These requirements have created the nascent field of machine unlearning [[1]](https://arxiv.org/abs/1912.03817), [[2]](https://arxiv.org/abs/2308.07061), [[3]](https://unlearning-challenge.github.io/).

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,No,0.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper presents a novel Chinese inertial generative adversarial network (CI-GAN) designed to generate high-quality training samples for Chinese writing recognition using inertial sensors. The CI-GAN integrates Chinese Glyph Encoding (CGE), Forced Optimal Transport (FOT), and Semantic Relevance Alignment (SRA) to enhance the quality and authenticity of generated inertial signals. The approach addresses the challenge of collecting diverse and extensive training data for Chinese character recognition, showing significant improvements in classifier performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper introduces innovative methods in the form of CGE, FOT, and SRA, contributing significantly to the field of inertial writing recognition. The release of a new dataset further enriches the community's resources.

Weaknesses:
1.Lack of Detailed Baseline Configuration: The paper compares CI-GAN with a traditional GAN in the appendix, but fails to provide detailed settings for the baseline method. This lack of information hinders the ability to fully understand and replicate the comparative effectiveness reported.
2.Insufficient Comparison with Other Augmentation Techniques: The study does not compare CI-GAN with other data augmentation methods, such as random perturbations. It remains unexplored whether applying random disturbances to the data could also substantially improve classifier performance.

Limitations:
The authors have discussed limitations related to the variability of writing styles and the potential impact of environmental factors on sensor data.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes CI-GAN to acquire unlimited high-quality training samples, alleviating the data scarcity in the inertial signal recognition of Chinese characters. By utilizing these generated data, the performance of recognition models is highly improved.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is easy to follow.
- The proposed methods may help disabled people.

Weaknesses:
- The pipeline lacks novelty. The employed technologies are widely used in CV and NLP, and the proposed pipeline merely reuses them for the inertial signal domain without any innovative design. Furthermore, the author fails to cite relevant studies such as [1][2] and does not discuss their differences.
[1] Wasserstein GAN (WGAN)
[2] Efficient Estimation of Word Representations in Vector Space

- The proposed CGE is simply a learnable embedding to represent Chinese characters, lacking innovative design for glyph information. The author introduces GER to enhance the orthogonality of character embeddings but does not provide an ablation study to verify its effectiveness.

- The author uses Wasserstein distance in GANs. What is the difference between this approach and WGAN [1]? Additionally, the author proposes using FFM to supervise the signal in feature spaces. These measures are also similar to some works, such as perceptual loss using VGG and identity loss using ArcFace, but the author does not cite these and discuss the difference. 

- The dataset used for training and testing is too small, which could not effectively verify the effectiveness of the proposed method.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper address an important probem in human computer interaction: making computers accessible to vision impaired people. The paper address this my collection paired data of text and imu signals. First, the paper address the issues of limited data by training a generative model, to resample/bootstrap more data and then train recognition model on both real and generated data to archive high performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
the paper addresses an important social problem, and accessibility should be focused on all groups. 

The data collected for this paper, the paired data on text and imu is very useful, hope the authors will open-source it. 

paper is well written and the figures are clear and convey the ideas.

Weaknesses:
My main concern is, that it is very unlikely that we get more than we give to the system, the generated samples are a function of real samples. 
I would like to see, a competitive baseline with good data augmentation, and maybe on a low data regime gan generated samples are better than augmentation, but this has to be shown, otherwise, I don't see the value of extra effort to train a generative model to get data augmentation.

Limitations:
I wouldn't say this is a major limitation, but on the scale axis, this problem can be solved by collecting more data. Unlike annotations like explaining an image or video, handwriting signals are more easy to collect on the long term. would be nice if the authors can address this, also please explain the issues with data augmentation.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper presents a novel Chinese inertial generative adversarial network (CI-GAN) designed to generate high-quality training samples for Chinese writing recognition using inertial sensors. The CI-GAN integrates Chinese Glyph Encoding (CGE), Forced Optimal Transport (FOT), and Semantic Relevance Alignment (SRA) to enhance the quality and authenticity of generated inertial signals. The approach addresses the challenge of collecting diverse and extensive training data for Chinese character recognition, showing significant improvements in classifier performance.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper introduces innovative methods in the form of CGE, FOT, and SRA, contributing significantly to the field of inertial writing recognition. The release of a new dataset further enriches the community's resources.

Weaknesses:
1.Lack of Detailed Baseline Configuration: The paper compares CI-GAN with a traditional GAN in the appendix, but fails to provide detailed settings for the baseline method. This lack of information hinders the ability to fully understand and replicate the comparative effectiveness reported.
2.Insufficient Comparison with Other Augmentation Techniques: The study does not compare CI-GAN with other data augmentation methods, such as random perturbations. It remains unexplored whether applying random disturbances to the data could also substantially improve classifier performance.

Limitations:
The authors have discussed limitations related to the variability of writing styles and the potential impact of environmental factors on sensor data.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper proposes CI-GAN to acquire unlimited high-quality training samples, alleviating the data scarcity in the inertial signal recognition of Chinese characters. By utilizing these generated data, the performance of recognition models is highly improved.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This paper is easy to follow.
- The proposed methods may help disabled people.

Weaknesses:
- The pipeline lacks novelty. The employed technologies are widely used in CV and NLP, and the proposed pipeline merely reuses them for the inertial signal domain without any innovative design. Furthermore, the author fails to cite relevant studies such as [1][2] and does not discuss their differences.
[1] Wasserstein GAN (WGAN)
[2] Efficient Estimation of Word Representations in Vector Space

- The proposed CGE is simply a learnable embedding to represent Chinese characters, lacking innovative design for glyph information. The author introduces GER to enhance the orthogonality of character embeddings but does not provide an ablation study to verify its effectiveness.

- The author uses Wasserstein distance in GANs. What is the difference between this approach and WGAN [1]? Additionally, the author proposes using FFM to supervise the signal in feature spaces. These measures are also similar to some works, such as perceptual loss using VGG and identity loss using ArcFace, but the author does not cite these and discuss the difference. 

- The dataset used for training and testing is too small, which could not effectively verify the effectiveness of the proposed method.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper address an important probem in human computer interaction: making computers accessible to vision impaired people. The paper address this my collection paired data of text and imu signals. First, the paper address the issues of limited data by training a generative model, to resample/bootstrap more data and then train recognition model on both real and generated data to archive high performance.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
the paper addresses an important social problem, and accessibility should be focused on all groups. 

The data collected for this paper, the paired data on text and imu is very useful, hope the authors will open-source it. 

paper is well written and the figures are clear and convey the ideas.

Weaknesses:
My main concern is, that it is very unlikely that we get more than we give to the system, the generated samples are a function of real samples. 
I would like to see, a competitive baseline with good data augmentation, and maybe on a low data regime gan generated samples are better than augmentation, but this has to be shown, otherwise, I don't see the value of extra effort to train a generative model to get data augmentation.

Limitations:
I wouldn't say this is a major limitation, but on the scale axis, this problem can be solved by collecting more data. Unlike annotations like explaining an image or video, handwriting signals are more easy to collect on the long term. would be nice if the authors can address this, also please explain the issues with data augmentation.

Rating:
5: marginally below the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,No,0.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
In pipelined model training, one important issue is to reduce the bubble sizes. 
One stream of work is to use the staleness, where the weight discrepancy is mitigated using stashed weights.
This work tries to reduce the overhead of storing weights with reversible architectures. 
Using the non-stashed updated weights, but with restored inputs to each stage, 
approximated gradients are obtained and parallel training is performed.
This leads to less memory usage on training at the cost of increased communication. 
Training results on resnet variants seem to maintain accuracy.

Soundness:
1: poor

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This is a nice adaptation of reversible architectures to pipelined training. If it works well, there is a potential for becoming a new popular pipelined training method.

- The idea of using reconstructed input instead of stored weights seem to be novel.

Weaknesses:
- Insufficient experiment size: Only compared on three different sizes of resnet. This is far from sufficient, especially with the largest model being resnet50. 

- No comparison on speedup: speedup on the training time is crucial, but the ""memory benefits and training time"" section does not disclose any data. Since the proposed scheme has larger communication, it is crucial to report the number.

- Classification accuracy drop: The final accuracy drops on all three datasets for resnet50. 0.6%p and 0.7%p are huge drops for those models. Given that this is the largest model among the tested ones, it draws a significant concern on whether this technique would work for larger models such as resnet152 or ViTs.

- There is no analysis or proof on why the proposed scheme would work. Why it is a good approximation, or why it is going to converge, etc.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method that combines reversible neural networks and parallel distributed training to enable learning with minimal memory usage, while incurring only slight communication and computation overhead. In this approach, the need for storing intermediate activations in traditional backpropagation is eliminated, thus reducing memory constraints and allowing for higher parallelism on the same device. This new method facilitates efficient learning by providing an innovative solution.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
The problem setup involving reversible architecture and distributed parallel training is intriguing. High memory consumption is a critical issue in learning, and reversible architecture has been proposed to address this problem. It is anticipated that these advantages can be similarly applied to distributed parallel training. Additionally, the paper is very well-written, making the ideas easy to understand. The figures and tables were also judged to be of high quality and well-prepared.

Weaknesses:
The main drawback of this paper is the insufficient experimentation. Although using reversible architecture in distributed training is a novel concept, it appears to be merely a combination of existing ideas. For this paper to have a significant impact, it must demonstrate the advantages and benefits of the proposed idea in an actual distributed learning environment. However, the experiments were conducted using only a single A100 GPU, and there is no demonstration of the performance improvements or limitations of the proposed idea in a real distributed environment. The values presented in the tables do not clearly differentiate from what can be achieved with existing reversible architectures. To improve the completeness of this paper, it is essential to analyze scenarios that necessitate the use of multiple GPUs, such as video applications, large-resolution diffusion, and large language models. The current data fails to effectively explain the benefits of the proposed idea.

Limitations:
Not relevant.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, the author proposes a new alternative algorithm (Parallel End-to-End Training with Reversible Architectures) for regular backpropagation, which significantly enhance the parallelization with a limited overhead compared to regular backpropagation and other alternatives to end-to-end training.
Specifically, the network is split into several stages (one layer or a set of layers) distributed across distinct devices, one batch data is split into several mini-batch data. The first device sequentially accesses the mini-batch data and pass them forward to the next stage until the final stage is reached. The backpropagation is initialized from the final stage to the first stage. It enables a significant parallelization of forward and backward computations across multiple devices.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* This paper is well-organized and easy to follow.

* The background information is very rich and makes it easy for someone who is not familiar with this field to understand the relevant techniques including the technique proposed by this paper.

* The figures about the core technique proposed by authors are very clear, which can help readers understand the technique at a glance.

* The paper evaluates the proposed techniques on multiple datasets and networks.

Weaknesses:
* From the comparison between the proposed method and other techniques from related work, it showcases that the proposed method does not have an overall crushing lead. There exists the method which can achieve higher speed and less time than proposed method with storage increased. 

* The low or even zero storage on proposed method is mainly due to reversible architectures. Maybe authors can extend proposed parallel training method to some non-reversible architectures (need memory storage for intermediate activations), then compare with other SOTA methods.

* It would be great if authors use more distributed devices to get more stages from a network, in this case, the performance of the proposed method is likely to be deeply explored. Because the proposed technique is aimed to deployed on the distributed devices.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose fusing delayed gradient pipeline parallelism with reversible models in order to capture the benefits of the former while mitigating the drawbacks with the latter.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper sets up a pretty compelling combination of ideas. This is a great example of a paper that clearly understands the strengths and weaknesses of two disparate techniques and fits them together like puzzle pieces.

- The paper is clear and methodical in laying out the motivation for the approach. By the time the method is introduced, its seems like the natural and obvious choice. This is good writing.

- The concept is solid. I really *want* to like this idea, since it seems to fit together so well.

Weaknesses:
- While the idea is presented fairly clearly, a lot of the analysis is estimates (S4.2) and generalizations (Tab 1). It's fine for motivating the idea, but not really good enough for proving it works as projected. I'm left wondering how much of this method will actually translate to a scaled-up implementation. (No question that it *was* implemented, but a pipeline-parallel model that doesn't actually pipeline across devices is...not particularly compelling.)

- The paper is a fusion of two ideas, designed to capture the computational performance benefits of pipeline parallelism while using reversible models to mitigate memory scaling. Some estimated results of memory footprint are presented in Table 3. No measured results are presented related to parallelism (timing, utilization, etc.). From this paper, it is not possible to determine whether it has succeeded. This is confused further by the section 4.2: ""Memory benefits and training time"" which does not discuss training time at all. The lack of computational results is fairly damning.

Limitations:
As described in weaknesses. Limitations, like computational performance details, are not well described.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
In pipelined model training, one important issue is to reduce the bubble sizes. 
One stream of work is to use the staleness, where the weight discrepancy is mitigated using stashed weights.
This work tries to reduce the overhead of storing weights with reversible architectures. 
Using the non-stashed updated weights, but with restored inputs to each stage, 
approximated gradients are obtained and parallel training is performed.
This leads to less memory usage on training at the cost of increased communication. 
Training results on resnet variants seem to maintain accuracy.

Soundness:
1: poor

Presentation:
3: good

Contribution:
2: fair

Strengths:
- This is a nice adaptation of reversible architectures to pipelined training. If it works well, there is a potential for becoming a new popular pipelined training method.

- The idea of using reconstructed input instead of stored weights seem to be novel.

Weaknesses:
- Insufficient experiment size: Only compared on three different sizes of resnet. This is far from sufficient, especially with the largest model being resnet50. 

- No comparison on speedup: speedup on the training time is crucial, but the ""memory benefits and training time"" section does not disclose any data. Since the proposed scheme has larger communication, it is crucial to report the number.

- Classification accuracy drop: The final accuracy drops on all three datasets for resnet50. 0.6%p and 0.7%p are huge drops for those models. Given that this is the largest model among the tested ones, it draws a significant concern on whether this technique would work for larger models such as resnet152 or ViTs.

- There is no analysis or proof on why the proposed scheme would work. Why it is a good approximation, or why it is going to converge, etc.

Limitations:
N/A

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a method that combines reversible neural networks and parallel distributed training to enable learning with minimal memory usage, while incurring only slight communication and computation overhead. In this approach, the need for storing intermediate activations in traditional backpropagation is eliminated, thus reducing memory constraints and allowing for higher parallelism on the same device. This new method facilitates efficient learning by providing an innovative solution.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
The problem setup involving reversible architecture and distributed parallel training is intriguing. High memory consumption is a critical issue in learning, and reversible architecture has been proposed to address this problem. It is anticipated that these advantages can be similarly applied to distributed parallel training. Additionally, the paper is very well-written, making the ideas easy to understand. The figures and tables were also judged to be of high quality and well-prepared.

Weaknesses:
The main drawback of this paper is the insufficient experimentation. Although using reversible architecture in distributed training is a novel concept, it appears to be merely a combination of existing ideas. For this paper to have a significant impact, it must demonstrate the advantages and benefits of the proposed idea in an actual distributed learning environment. However, the experiments were conducted using only a single A100 GPU, and there is no demonstration of the performance improvements or limitations of the proposed idea in a real distributed environment. The values presented in the tables do not clearly differentiate from what can be achieved with existing reversible architectures. To improve the completeness of this paper, it is essential to analyze scenarios that necessitate the use of multiple GPUs, such as video applications, large-resolution diffusion, and large language models. The current data fails to effectively explain the benefits of the proposed idea.

Limitations:
Not relevant.

Rating:
3: reject, not good enough

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
In this paper, the author proposes a new alternative algorithm (Parallel End-to-End Training with Reversible Architectures) for regular backpropagation, which significantly enhance the parallelization with a limited overhead compared to regular backpropagation and other alternatives to end-to-end training.
Specifically, the network is split into several stages (one layer or a set of layers) distributed across distinct devices, one batch data is split into several mini-batch data. The first device sequentially accesses the mini-batch data and pass them forward to the next stage until the final stage is reached. The backpropagation is initialized from the final stage to the first stage. It enables a significant parallelization of forward and backward computations across multiple devices.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
* This paper is well-organized and easy to follow.

* The background information is very rich and makes it easy for someone who is not familiar with this field to understand the relevant techniques including the technique proposed by this paper.

* The figures about the core technique proposed by authors are very clear, which can help readers understand the technique at a glance.

* The paper evaluates the proposed techniques on multiple datasets and networks.

Weaknesses:
* From the comparison between the proposed method and other techniques from related work, it showcases that the proposed method does not have an overall crushing lead. There exists the method which can achieve higher speed and less time than proposed method with storage increased. 

* The low or even zero storage on proposed method is mainly due to reversible architectures. Maybe authors can extend proposed parallel training method to some non-reversible architectures (need memory storage for intermediate activations), then compare with other SOTA methods.

* It would be great if authors use more distributed devices to get more stages from a network, in this case, the performance of the proposed method is likely to be deeply explored. Because the proposed technique is aimed to deployed on the distributed devices.

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose fusing delayed gradient pipeline parallelism with reversible models in order to capture the benefits of the former while mitigating the drawbacks with the latter.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper sets up a pretty compelling combination of ideas. This is a great example of a paper that clearly understands the strengths and weaknesses of two disparate techniques and fits them together like puzzle pieces.

- The paper is clear and methodical in laying out the motivation for the approach. By the time the method is introduced, its seems like the natural and obvious choice. This is good writing.

- The concept is solid. I really *want* to like this idea, since it seems to fit together so well.

Weaknesses:
- While the idea is presented fairly clearly, a lot of the analysis is estimates (S4.2) and generalizations (Tab 1). It's fine for motivating the idea, but not really good enough for proving it works as projected. I'm left wondering how much of this method will actually translate to a scaled-up implementation. (No question that it *was* implemented, but a pipeline-parallel model that doesn't actually pipeline across devices is...not particularly compelling.)

- The paper is a fusion of two ideas, designed to capture the computational performance benefits of pipeline parallelism while using reversible models to mitigate memory scaling. Some estimated results of memory footprint are presented in Table 3. No measured results are presented related to parallelism (timing, utilization, etc.). From this paper, it is not possible to determine whether it has succeeded. This is confused further by the section 4.2: ""Memory benefits and training time"" which does not discuss training time at all. The lack of computational results is fairly damning.

Limitations:
As described in weaknesses. Limitations, like computational performance details, are not well described.

Rating:
3: reject, not good enough

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,No,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces Tina, a text-conditioned neural network diffusion model designed for train-once-for-all personalization. Tina utilizes a diffusion transformer model conditioned on task descriptions embedded using a CLIP model. This innovative approach aims to generate personalized models for various end-users and tasks based on text prompts, demonstrating significant generalization capabilities even when trained on relatively small datasets (~1000 samples). The model is evaluated under zero-shot/few-shot image prompts, varying numbers of personalized classes, natural language descriptions, and predicting unseen entities to assess its understanding of world knowledge.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The paper provides a comprehensive explanation of the design and framework of Tina.
- It conducts a detailed ablation study and experiments across different datasets.
- The topic is interesting, and the presentation is clear and easy to understand.
- very detailed and robust comparison with previous works.

Weaknesses:
- The model parameter size in the experiments is too small; larger models are needed to evaluate effectiveness.
- In Table 1, the results of direct fine-tuning should be included.
- We might need an ablation study on the impact of text prompts.
- We might need an ablation study to determine if the model merely memorizes and reproduces parameters.
- Figure 2 requires polishing for better clarity.

Limitations:
The model size is too small in exp.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To generate personalized models for a variety of end-users and tasks via text prompts, this paper introduces Tina, a text-conditioned neural network diffusion model. Tina employs a diffusion transformer model, complemented by a CLIP model to embed task descriptions. Remarkably, Tina demonstrates superior generalization capabilities even on small-scale datasets, performing well both within and outside the distribution of the training data. Furthermore, Tina exhibits robust performance under zero-shot/few-shot image prompts, natural language instructions, and unseen categories.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The method demonstrates excellent generalization, showcasing significant in-distribution and out-of-distribution performance even when trained on small datasets. It also exhibits robust behavior in predicting entities that have not been seen before.

2.Compared to existing text-to-image models (such as stable diffusion), text-to-video models (like Sora), and large language models (such as GPT-4), the concept of Tina, which generates personalized models suitable for specific tasks directly from text descriptions, is quite novel.

3.The experimental process is comprehensive and reliable. The paper conducts comparisons against baselines across multiple datasets, and it also undertakes experiments to validate generalization performance as well as performs ablation studies.

4.The experiments involves multiple datasets to verify the effectiveness of the proposed methods.

Weaknesses:
1.It is better to includes more comprehensive and competitive baselines to show the model’s effectiveness and advance. The two baselines come from one paper published in 2023. As for the experimental setting involves three widely-used datasets, I am wondering whether the experimental results excels or perform similarly to the SOTA performance on some of the three datasets. In other word, is it possible to apply the proposed strategy to some more advanced framework to make the performance similar to the SOTA, which ensure the proposed method have real applications in the real use.

2.The base model is CNN or ResNet in the experiments. Is the proposed method generalized to more advanced framework? Applying the proposed method on more advanced framework and obtain more advance performance indicates that the method has potential to be used in the real life.

3.We suggest providing necessary explanations in the captions of the model framework overview.

Limitations:
The limitations are fine with me.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work introduces Tina, a text-conditioned neural network diffusion model designed for generating personalized models from text prompts. Tina aims to enable efficient personalization by training a generic model once and then customizing it for various end-user tasks using task descriptions. Leveraging a diffusion transformer model and CLIP-based text embeddings, Tina demonstrates the ability to generate models for a wide range of personalized tasks. The approach shows promising results in generalizing to both seen and unseen tasks, achieving state-of-the-art performance in several benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Tina's train-once-for-all approach effectively addresses the need for personalized models without requiring extensive retraining, making it a practical solution for diverse end-user scenarios.
2. The model achieves competitive performance across multiple datasets, demonstrating its robustness and effectiveness in both in-distribution and out-of-distribution tasks.
3. Tina can handle various types of input prompts (text, images) and generalize to unseen classes and tasks, highlighting its versatility and potential for broader applications.

Weaknesses:
1. Some methodological details are sparse, such as the specific configurations and hyperparameters used for training Tina. Providing more granular details could help readers replicate the experiments.
2. The reason for adopting DiT as the weight generation model is not well justified. It would be good to see some results of adopting different kinds of diffusion models.

Limitations:
While limitations are discussed, the manuscript could benefit from a discussion of the scalability of Tina to larger datasets and more complex tasks.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper introduces Tina, a text-conditioned neural network diffusion model designed for train-once-for-all personalization. Tina utilizes a diffusion transformer model conditioned on task descriptions embedded using a CLIP model. This innovative approach aims to generate personalized models for various end-users and tasks based on text prompts, demonstrating significant generalization capabilities even when trained on relatively small datasets (~1000 samples). The model is evaluated under zero-shot/few-shot image prompts, varying numbers of personalized classes, natural language descriptions, and predicting unseen entities to assess its understanding of world knowledge.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
2: fair

Strengths:
- The paper provides a comprehensive explanation of the design and framework of Tina.
- It conducts a detailed ablation study and experiments across different datasets.
- The topic is interesting, and the presentation is clear and easy to understand.
- very detailed and robust comparison with previous works.

Weaknesses:
- The model parameter size in the experiments is too small; larger models are needed to evaluate effectiveness.
- In Table 1, the results of direct fine-tuning should be included.
- We might need an ablation study on the impact of text prompts.
- We might need an ablation study to determine if the model merely memorizes and reproduces parameters.
- Figure 2 requires polishing for better clarity.

Limitations:
The model size is too small in exp.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
To generate personalized models for a variety of end-users and tasks via text prompts, this paper introduces Tina, a text-conditioned neural network diffusion model. Tina employs a diffusion transformer model, complemented by a CLIP model to embed task descriptions. Remarkably, Tina demonstrates superior generalization capabilities even on small-scale datasets, performing well both within and outside the distribution of the training data. Furthermore, Tina exhibits robust performance under zero-shot/few-shot image prompts, natural language instructions, and unseen categories.

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1.The method demonstrates excellent generalization, showcasing significant in-distribution and out-of-distribution performance even when trained on small datasets. It also exhibits robust behavior in predicting entities that have not been seen before.

2.Compared to existing text-to-image models (such as stable diffusion), text-to-video models (like Sora), and large language models (such as GPT-4), the concept of Tina, which generates personalized models suitable for specific tasks directly from text descriptions, is quite novel.

3.The experimental process is comprehensive and reliable. The paper conducts comparisons against baselines across multiple datasets, and it also undertakes experiments to validate generalization performance as well as performs ablation studies.

4.The experiments involves multiple datasets to verify the effectiveness of the proposed methods.

Weaknesses:
1.It is better to includes more comprehensive and competitive baselines to show the model’s effectiveness and advance. The two baselines come from one paper published in 2023. As for the experimental setting involves three widely-used datasets, I am wondering whether the experimental results excels or perform similarly to the SOTA performance on some of the three datasets. In other word, is it possible to apply the proposed strategy to some more advanced framework to make the performance similar to the SOTA, which ensure the proposed method have real applications in the real use.

2.The base model is CNN or ResNet in the experiments. Is the proposed method generalized to more advanced framework? Applying the proposed method on more advanced framework and obtain more advance performance indicates that the method has potential to be used in the real life.

3.We suggest providing necessary explanations in the captions of the model framework overview.

Limitations:
The limitations are fine with me.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work introduces Tina, a text-conditioned neural network diffusion model designed for generating personalized models from text prompts. Tina aims to enable efficient personalization by training a generic model once and then customizing it for various end-user tasks using task descriptions. Leveraging a diffusion transformer model and CLIP-based text embeddings, Tina demonstrates the ability to generate models for a wide range of personalized tasks. The approach shows promising results in generalizing to both seen and unseen tasks, achieving state-of-the-art performance in several benchmarks.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Tina's train-once-for-all approach effectively addresses the need for personalized models without requiring extensive retraining, making it a practical solution for diverse end-user scenarios.
2. The model achieves competitive performance across multiple datasets, demonstrating its robustness and effectiveness in both in-distribution and out-of-distribution tasks.
3. Tina can handle various types of input prompts (text, images) and generalize to unseen classes and tasks, highlighting its versatility and potential for broader applications.

Weaknesses:
1. Some methodological details are sparse, such as the specific configurations and hyperparameters used for training Tina. Providing more granular details could help readers replicate the experiments.
2. The reason for adopting DiT as the weight generation model is not well justified. It would be good to see some results of adopting different kinds of diffusion models.

Limitations:
While limitations are discussed, the manuscript could benefit from a discussion of the scalability of Tina to larger datasets and more complex tasks.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,No,0.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper investigates a new definition for the stochastic gradient variance in mirror descent.
Most existing analyses for stochastic mirror descent require a strongly convex distance generating function to bound the gradient variance.
This limits the their applications especially when this assumption fails.
In particular, Le Priol et al. (2021) have shown that the none of the existing convergence rates applies to Gaussian maximum likelihood.

This paper aims to fix this issue by proposing a new definition of gradient variance.
They show that the new definition is strictly stronger (more likely to hold in practice) than existing definitions, and derive convergence rates in convex setting.
The authors demonstrate an application of the new variance definition bounding the estimation error of MAP for one-dimensional Gaussian distributions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Analyzing stochastic mirror descent is hard when the distance generating function is not strongly convex.
This paper is a step towards generalizing mirror descent analyses.
In particular, I like Section 2.2 where the authors show that the proposed definition is strictly better than existing ones.

- The mirror descent analysis in this paper yields a non-asymptotic bound for the estimation error of Gaussian MAP. This seems to be a fundamental problem lacking theoretical guarantees based on Le Priol et al. (2021).
However, I am now knowledgeable enough to confirm the significance or novelty of this result in statistics.

Weaknesses:
While developing the new gradient variance definition is certainly interesting, I have the following concerns.

- The authors have shown that their gradient variance \\(\sigma_{\star, \eta}^2\\) is finite for every fixed step size \\(\eta\\).
The convergence rates in the convex setting are proved using constant step sizes, and thus the optimality gap does not vanish.
To make the optimality gap vanish, diminishing step sizes are often required, which is not covered in this paper.
Proving convergence with diminishing step sizes probably requires characterizing the average variance \\(\frac1T \sum_{t=1}^{T} \sigma_{\star, \eta_t}^2\\), which I think can be done only on a case-by-case manner depending on the specific application.

- The only case so far where this new definition shines while all other definitions fail is maximum likelihood estimation for one-dimensional Gaussian distributions.
This is very restrictive.
Is it possible to generalize this result to multivariate Gaussian distributions?
In addition, it would be great if the authors could provide other applications to further justify the necessity of this new definition.

Minor:
- Line 192: Add a period.
- Bad notation in Section 4.2: It might be confusing to use $\Sigma$ to denote the standard deviation.
Consider using a different letter like $s$ or $\tau$.

Limitations:
NA.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work revisits Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduces a new (less restrictive) definition  of variance which can generally be bounded (globally) under mild regularity assumptions. Then this paper investigates this notion in more details, and show that it  naturally leads to strong convergence guarantees for stochastic mirror descent. Finally, this paper leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance.

Problem:
In proof of Proposition 2, by the definition of $\sigma_{*,\eta}^2$, we can obtain that
$ \sigma_{*,\eta}^2 = \frac{\min_x f(x) - \min_x f_\eta(x)}{\eta}  $. 
However,  $x_* =\argmin_x f(x)$ does not equal to $x_*' = \argmin_x f_{\eta}(x)$.
This will lead to $\sigma_{*,\eta}^2 \neq \frac{1}{\eta^2} D_h(x^*, x^+）$.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This work revisits Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduces a new (less restrictive) definition  of variance which can generally be bounded (globally) under mild regularity assumptions. Then this paper investigates this notion in more details, and show that it  naturally leads to strong convergence guarantees for stochastic mirror descent. Finally, this paper leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance.

Weaknesses:
No.

Limitations:
No

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a new analysis of SMD using a newly introduced generalized variance notion. The benefit of the new analysis is demonstrated in the application to maximum a posteriori estimation of Gaussian parameters.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
After introducing a new variance notion, the paper delves into comparison with other existing notions and shows that the proposed one is the largest meaningful notion. After a careful comparison, analysis of SMD is presented using this mild assumption. This analysis substantially departs from the results known in the literature. The demonstration of the use case of this new theory in the context of statistical estimation is also clear and adds more significance to the new theory.

Weaknesses:
Major:

As explained after theorem 4.3, the guarantees are derived for a reverse KL and may not imply anything on the desired quantity $f(\theta) - f(\theta_*)$. This of course, limits the contribution in this application significantly as non-asymptotic rates were known before. 

Minor problems that I hope the authors can fix in the next revision. 

1. Is the set C compact? If not, why the minimum exists in Proposition 2.2?

2. Cannot find where $x_*$ is defined. Why does it exist? 

3. There is a small issue with indicies in equation (12) and in paragraph before. $\eta_{n} = \frac{1}{n_0+n+1}$, and the stochastic gradient should depend on the new sample $X_{n+1}$.

Update: meaningful results are obtained only for relatively strongly convex case (which is a stronger assumption than even strong convexity). In the convex case, a different (much stronger) definition is used. This becomes clear only after reading Appendix D. This limitation should be clarified in section 3.2, where convergence on some surrogate loss is shown. I will update my evalutation.

Limitations:
n/a

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new variance assumption for the analysis of stochastic mirror descent (SMD) to handle cases where standard bounded variance assumption does not hold. The authors show this new assumption can be shown to hold under some regularity assumptions. The authors use the new results to show some convergence guarantees for MLE and MAP of a Gaussian with unknown mean and variance using the connection between this problem and SMD convergence guarantees.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The topic is definitely interesting and timely. Results for stochastic optimization without bounded variance assumptions are quite interesting. As shown in the prior literature, this task is especially subtle in the Bregman case. As the authors argue in detail, this difficulty is acknowledged in previous works such as [7] and [17]. It is neat that the authors show the importance of the new results by deriving convergence bounds for MAP/MLE of a Gaussian with unknown mean and variance by using the connection between these bounds and SMD in [17] (which itself is a nice connection). This adds a nice and clear motivation. The work makes some progress towards solving open questions from [17], while as the authors clearly explain, the open questions are still not completely solved.

Weaknesses:
I find the motivation of the paper and its application to MAP bounds interesting, however I have some concerns about writing and the strength of the derived results in the context of the application in Section 4. It seems necessary for the latter point to be clarified.

- Authors write after Theorem 4.3 that the open problem from [17] is not completely resolved because  the convergence is not shown for the desired quantity. In particular, the authors describe that the guarantee is for $D_A(\theta_*, \theta^{(n)})$ instead of $D_A(\theta^{(n)}, \theta_*) = f(\theta) - f(\theta_*)$. The authors then write that two quantities can be related asymptotically but they state: ""but we might also be able to exploit this control over the course of the iterations"". Can you make this point more precise? It is not clear to me what this last part is trying to describe. Is it meant to be understood as an open question or is it possible for the authors to derive the stronger result? Since the paper mentions at many places that showing convergence guarantees for MAP is an important contribution of the paper, it is important to justify the convergence metric used in the results for justifying the contribution of the paper fully.

- It might be better to replace MLE in the abstract to MAP since Section 4 is mostly about MAP.

- Abstract states a couple of times ""strong convergence"", I suggest to remove this since ""strong convergence"" has a precise meaning in infinite-dimensional optimization and usage in the abstract is confusing because of this. Clearly this is not how the authors are using this term, but it seems authors are using this as a subjective adjective, which is not necessary. By subjective, I mean that: how can one decide what convergence result is strong and what is not?

- Assumption 1 requires all $f_\xi$ are convex. This is rather strong since the standard assumption is $\mathbb{E} f_\xi$ to be convex. Can you discuss this more? According to Prop 4.1, this holds for the main application of the paper, but it might be worth discussing why componentwise convexity is needed.

Limitations:
The limitations are discussed clearly. The authors provided explanations after Theorem 3.3 and Theorem 4.3 to describe the limitations of their result.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This submission studies stochastic mirror descent (SMD) under quite mild conditions on the mirror map and objective function. More specifically, there are a variety of SMD analysis in the literature, but virtually all of them require strong conditions on the mirror map (such as strong convexity) that do not hold in cases where we only have relative smoothness (and/or relative strong convexity) of the objective function with respect to the mirror map. The authors propose a definition of variance of SMD that is better behaved under minimal assumptions. They show how this new variance can be used to obtain general convergence results for SMD. Finally, they show how the new variance definition for SMD can show some kind of non-asymptotic convergence rates for MLE and MAP of Gaussian parameter estimation with unknown mean and covariance, making partial progress on a conjecture posed by Le Priol et at.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
This is an interesting paper that tackles a hard theoretical problem. I think it is of interest for researchers interested in mirror descent. The new definition of variance of SMD has interesting properties even under very mild assumptions, as the authors show when comparing the new definition with other definitions of SMD variance in the literature. Moreover, the results on Sec 4 already show how this is an interesting way to analyze SMD, and is likely to lead to follow-up work on the area.

So the strengths summarized in bullet points:
- Thorough comparison of new variance definition with other definitions in the literature and proof of finiteness under assumption 1. 
- General convergence theorems of SMD under mild assumptions that recover known results in the deterministic case, showing this is may be a ""natural"" variance definition for SMD and useful for our understanding of SMD.
- Partial progress towards the conjecture of Le Priol et al.

Weaknesses:
In its current form, I have one main concern with the paper:
- Despite what is written at the beginning of the paper, **Assumption 1** is NOT a blanket assumption used throughout the paper. In fact, it appears only section 2 uses assumption 1. The rest of the paper uses a weaker assumption that is never clearly stated, which makes it hard to understand when the results hold or not. 
This is likely to be a problem with presentation, but in its current form it is often not clear what are the assumption required at each point. Since the main point of the paper is to use a minimal number of assumptions, it is very important for those to be clearly stated. 

A minor weakness is the lack of an example besides MAP/MLE. I could not easily think of a concrete example where I could apply the convergence results in sec 3 or 4. If the authors have an example besides MLE or MAP (even if a bit artificial), it would be great. For example, some example with a mirror map such as $- \log x$ would be interesting, but this is a minor suggestion, since it would be nice to see a concrete example of the use of the results in Sec 2 (the results in Sec 4 require a specialized bound on the variance) 

Summary of weaknesses:
- Unclear requires assumptions for many of the results
- (Minor weakness) Lack of a concrete (even artificial) example of application of any of the theorems in Sec 3 beyond MAP/MLE (and the latter require specialized bounds on the variance).

Limitations:
Although the authors are not explicit about some of the limitations of the results on sec 3, they do discuss how to interpret some of the results and limitations from their convergence rates.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper investigates a new definition for the stochastic gradient variance in mirror descent.
Most existing analyses for stochastic mirror descent require a strongly convex distance generating function to bound the gradient variance.
This limits the their applications especially when this assumption fails.
In particular, Le Priol et al. (2021) have shown that the none of the existing convergence rates applies to Gaussian maximum likelihood.

This paper aims to fix this issue by proposing a new definition of gradient variance.
They show that the new definition is strictly stronger (more likely to hold in practice) than existing definitions, and derive convergence rates in convex setting.
The authors demonstrate an application of the new variance definition bounding the estimation error of MAP for one-dimensional Gaussian distributions.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
- Analyzing stochastic mirror descent is hard when the distance generating function is not strongly convex.
This paper is a step towards generalizing mirror descent analyses.
In particular, I like Section 2.2 where the authors show that the proposed definition is strictly better than existing ones.

- The mirror descent analysis in this paper yields a non-asymptotic bound for the estimation error of Gaussian MAP. This seems to be a fundamental problem lacking theoretical guarantees based on Le Priol et al. (2021).
However, I am now knowledgeable enough to confirm the significance or novelty of this result in statistics.

Weaknesses:
While developing the new gradient variance definition is certainly interesting, I have the following concerns.

- The authors have shown that their gradient variance \\(\sigma_{\star, \eta}^2\\) is finite for every fixed step size \\(\eta\\).
The convergence rates in the convex setting are proved using constant step sizes, and thus the optimality gap does not vanish.
To make the optimality gap vanish, diminishing step sizes are often required, which is not covered in this paper.
Proving convergence with diminishing step sizes probably requires characterizing the average variance \\(\frac1T \sum_{t=1}^{T} \sigma_{\star, \eta_t}^2\\), which I think can be done only on a case-by-case manner depending on the specific application.

- The only case so far where this new definition shines while all other definitions fail is maximum likelihood estimation for one-dimensional Gaussian distributions.
This is very restrictive.
Is it possible to generalize this result to multivariate Gaussian distributions?
In addition, it would be great if the authors could provide other applications to further justify the necessity of this new definition.

Minor:
- Line 192: Add a period.
- Bad notation in Section 4.2: It might be confusing to use $\Sigma$ to denote the standard deviation.
Consider using a different letter like $s$ or $\tau$.

Limitations:
NA.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work revisits Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduces a new (less restrictive) definition  of variance which can generally be bounded (globally) under mild regularity assumptions. Then this paper investigates this notion in more details, and show that it  naturally leads to strong convergence guarantees for stochastic mirror descent. Finally, this paper leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance.

Problem:
In proof of Proposition 2, by the definition of $\sigma_{*,\eta}^2$, we can obtain that
$ \sigma_{*,\eta}^2 = \frac{\min_x f(x) - \min_x f_\eta(x)}{\eta}  $. 
However,  $x_* =\argmin_x f(x)$ does not equal to $x_*' = \argmin_x f_{\eta}(x)$.
This will lead to $\sigma_{*,\eta}^2 \neq \frac{1}{\eta^2} D_h(x^*, x^+）$.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
This work revisits Stochastic Mirror Descent (SMD) proofs in the (relatively-strongly-) convex and relatively-smooth setting, and introduces a new (less restrictive) definition  of variance which can generally be bounded (globally) under mild regularity assumptions. Then this paper investigates this notion in more details, and show that it  naturally leads to strong convergence guarantees for stochastic mirror descent. Finally, this paper leverage this new analysis to obtain convergence guarantees for the Maximum Likelihood Estimator of a Gaussian with unknown mean and variance.

Weaknesses:
No.

Limitations:
No

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper proposes a new analysis of SMD using a newly introduced generalized variance notion. The benefit of the new analysis is demonstrated in the application to maximum a posteriori estimation of Gaussian parameters.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
3: good

Strengths:
After introducing a new variance notion, the paper delves into comparison with other existing notions and shows that the proposed one is the largest meaningful notion. After a careful comparison, analysis of SMD is presented using this mild assumption. This analysis substantially departs from the results known in the literature. The demonstration of the use case of this new theory in the context of statistical estimation is also clear and adds more significance to the new theory.

Weaknesses:
Major:

As explained after theorem 4.3, the guarantees are derived for a reverse KL and may not imply anything on the desired quantity $f(\theta) - f(\theta_*)$. This of course, limits the contribution in this application significantly as non-asymptotic rates were known before. 

Minor problems that I hope the authors can fix in the next revision. 

1. Is the set C compact? If not, why the minimum exists in Proposition 2.2?

2. Cannot find where $x_*$ is defined. Why does it exist? 

3. There is a small issue with indicies in equation (12) and in paragraph before. $\eta_{n} = \frac{1}{n_0+n+1}$, and the stochastic gradient should depend on the new sample $X_{n+1}$.

Update: meaningful results are obtained only for relatively strongly convex case (which is a stronger assumption than even strong convexity). In the convex case, a different (much stronger) definition is used. This becomes clear only after reading Appendix D. This limitation should be clarified in section 3.2, where convergence on some surrogate loss is shown. I will update my evalutation.

Limitations:
n/a

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper introduces a new variance assumption for the analysis of stochastic mirror descent (SMD) to handle cases where standard bounded variance assumption does not hold. The authors show this new assumption can be shown to hold under some regularity assumptions. The authors use the new results to show some convergence guarantees for MLE and MAP of a Gaussian with unknown mean and variance using the connection between this problem and SMD convergence guarantees.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The topic is definitely interesting and timely. Results for stochastic optimization without bounded variance assumptions are quite interesting. As shown in the prior literature, this task is especially subtle in the Bregman case. As the authors argue in detail, this difficulty is acknowledged in previous works such as [7] and [17]. It is neat that the authors show the importance of the new results by deriving convergence bounds for MAP/MLE of a Gaussian with unknown mean and variance by using the connection between these bounds and SMD in [17] (which itself is a nice connection). This adds a nice and clear motivation. The work makes some progress towards solving open questions from [17], while as the authors clearly explain, the open questions are still not completely solved.

Weaknesses:
I find the motivation of the paper and its application to MAP bounds interesting, however I have some concerns about writing and the strength of the derived results in the context of the application in Section 4. It seems necessary for the latter point to be clarified.

- Authors write after Theorem 4.3 that the open problem from [17] is not completely resolved because  the convergence is not shown for the desired quantity. In particular, the authors describe that the guarantee is for $D_A(\theta_*, \theta^{(n)})$ instead of $D_A(\theta^{(n)}, \theta_*) = f(\theta) - f(\theta_*)$. The authors then write that two quantities can be related asymptotically but they state: ""but we might also be able to exploit this control over the course of the iterations"". Can you make this point more precise? It is not clear to me what this last part is trying to describe. Is it meant to be understood as an open question or is it possible for the authors to derive the stronger result? Since the paper mentions at many places that showing convergence guarantees for MAP is an important contribution of the paper, it is important to justify the convergence metric used in the results for justifying the contribution of the paper fully.

- It might be better to replace MLE in the abstract to MAP since Section 4 is mostly about MAP.

- Abstract states a couple of times ""strong convergence"", I suggest to remove this since ""strong convergence"" has a precise meaning in infinite-dimensional optimization and usage in the abstract is confusing because of this. Clearly this is not how the authors are using this term, but it seems authors are using this as a subjective adjective, which is not necessary. By subjective, I mean that: how can one decide what convergence result is strong and what is not?

- Assumption 1 requires all $f_\xi$ are convex. This is rather strong since the standard assumption is $\mathbb{E} f_\xi$ to be convex. Can you discuss this more? According to Prop 4.1, this holds for the main application of the paper, but it might be worth discussing why componentwise convexity is needed.

Limitations:
The limitations are discussed clearly. The authors provided explanations after Theorem 3.3 and Theorem 4.3 to describe the limitations of their result.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This submission studies stochastic mirror descent (SMD) under quite mild conditions on the mirror map and objective function. More specifically, there are a variety of SMD analysis in the literature, but virtually all of them require strong conditions on the mirror map (such as strong convexity) that do not hold in cases where we only have relative smoothness (and/or relative strong convexity) of the objective function with respect to the mirror map. The authors propose a definition of variance of SMD that is better behaved under minimal assumptions. They show how this new variance can be used to obtain general convergence results for SMD. Finally, they show how the new variance definition for SMD can show some kind of non-asymptotic convergence rates for MLE and MAP of Gaussian parameter estimation with unknown mean and covariance, making partial progress on a conjecture posed by Le Priol et at.

Soundness:
2: fair

Presentation:
1: poor

Contribution:
3: good

Strengths:
This is an interesting paper that tackles a hard theoretical problem. I think it is of interest for researchers interested in mirror descent. The new definition of variance of SMD has interesting properties even under very mild assumptions, as the authors show when comparing the new definition with other definitions of SMD variance in the literature. Moreover, the results on Sec 4 already show how this is an interesting way to analyze SMD, and is likely to lead to follow-up work on the area.

So the strengths summarized in bullet points:
- Thorough comparison of new variance definition with other definitions in the literature and proof of finiteness under assumption 1. 
- General convergence theorems of SMD under mild assumptions that recover known results in the deterministic case, showing this is may be a ""natural"" variance definition for SMD and useful for our understanding of SMD.
- Partial progress towards the conjecture of Le Priol et al.

Weaknesses:
In its current form, I have one main concern with the paper:
- Despite what is written at the beginning of the paper, **Assumption 1** is NOT a blanket assumption used throughout the paper. In fact, it appears only section 2 uses assumption 1. The rest of the paper uses a weaker assumption that is never clearly stated, which makes it hard to understand when the results hold or not. 
This is likely to be a problem with presentation, but in its current form it is often not clear what are the assumption required at each point. Since the main point of the paper is to use a minimal number of assumptions, it is very important for those to be clearly stated. 

A minor weakness is the lack of an example besides MAP/MLE. I could not easily think of a concrete example where I could apply the convergence results in sec 3 or 4. If the authors have an example besides MLE or MAP (even if a bit artificial), it would be great. For example, some example with a mirror map such as $- \log x$ would be interesting, but this is a minor suggestion, since it would be nice to see a concrete example of the use of the results in Sec 2 (the results in Sec 4 require a specialized bound on the variance) 

Summary of weaknesses:
- Unclear requires assumptions for many of the results
- (Minor weakness) Lack of a concrete (even artificial) example of application of any of the theorems in Sec 3 beyond MAP/MLE (and the latter require specialized bounds on the variance).

Limitations:
Although the authors are not explicit about some of the limitations of the results on sec 3, they do discuss how to interpret some of the results and limitations from their convergence rates.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,No,0.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper studies the generalization measured by gradients via a uniform gradient stability. For $\beta$-uniformly stable algorithms, the paper gives generalization bounds of order $O(1/n+\beta+\sqrt{E_Z[\|\nabla f(A(S);Z)\|_2^2]/n})$, which yields fast rates if $E_Z[\|\nabla f(A(S);Z)\|_2^2$ is small. The paper then uses this generalization measured by gradients to derive generalization error bounds under a PL condition. Applications to empirical risk minimization, gradient descent and stochastic gradient descent are given for strongly convex, smooth and Lipschitz problems.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper gives high-probability bounds for generalization gap on gradients, which include the gradient norm at the minimizer. This can imply fast rates in an interpolation setting. Under the PL condition, this gives fast rates of order $O(1/n^2)$.

The paper provides comprehensive applications to several algorithms such as empirical risk minimization, gradient descent and stochastic gradient.

Weaknesses:
The high-probability analysis based on uniform stability follows largely from existing work. I do not see enough novelty in the analysis. It would be helpful if the authors can summarize the challenges in the analysis and their novelty. 

As stated in the paper, Theorem 1 and Theorem 2 only improve the existing results by a constant factor. This improvement is not significant. 

As stated in the paper, the generalization by gradients is mostly interesting for nonconvex problems. However, for the applications in Section 4, the paper considers strongly convex problems. Also the results require smoothness and Lipschitz continuity. These assumptions seem to be a bit strong.

The paper gives fast rates under the case $F(w^*)=O(1/n)$ in Section 4. Note that Section 4 considers strongly convex problems. Then, the objective function should be of the form $F(w)=G(w)+\mu\|w\|^2$, where $G$ is related to loss. Then, if we require $F(w^*)=O(1/n)$, one needs $\mu\|w^*\|^2=O(1/n)$. Suppose we assume $\|w^*\|=O(1)$. Then, this requires $\mu=O(1/n)$. In this case, the generalization bound would be vacuous since $n\mu=O(1)$.

For SGD, the computation cost seems to be high. For example, in Theorem 6, the paper requires $T=n^4$ while in Theorem 13 the paper requires $T=n^2$. This high computational cost may not be appealing for large-scale problems.

In the proof of Lemma 1, the paper uses $\|\nabla F(A(S))\|_2\geq \mu\|A(S)-w^*\|$. This inequality does not generally hold under a PL condition. Indeed, Theorem 2 in Karimi et al 2016 require $\|A(S)-w^*\|$ to be replaced by the distance between $A(S)$ and the set of minimizers.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work shows high probability excess risk bounds of $O(1/n^2)$ for several algorithms under strong convexity, smoothness, Lipschitz continuity and low noise assumptions using algorithmic stability.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The results of the paper are interesting, showing a risk bound of $O(\frac{1}{n^2})$ using algorithmic stability.

2. The paper uses a novel technique, involving the stability of gradients to demonstrate excess risk bounds and presents applications of this technique in convex optimization.

Weaknesses:
1. The problem setup of the paper is not clearly detailed before the technical section, including the assumptions used for proving the results.

2. The presentation of results and related works is somewhat lacking. It would be beneficial if the authors summarized the results from previous work and compared them to the results in the current paper, including the set of assumptions made in each work.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper achieves the high probability excess risk bounds $\mathcal{O}(1/n^2)$ for empirical risk minimization, projected gradient descent and stochastic gradient descent under strong convexity, smoothness and Lipschitz continuity assumptions.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Please see Summary.

Weaknesses:
1.The paragraph from line 52 to line 65 first analyzes the non-convex problems. Then, it suddenly mentions that this paper explores the stability of stochastic convex optimization algorithms with strongly convex losses in line 61. And, it doesn’t mention its non-convex analysis. It is so confusing. So, I suggest authors rewrite this paragraph to benefit readers’ understanding.

2.The paragraph from line 70 to line 73 is unnecessary since we can not obtain any useful information.

3.In Related Work, it is unnecessary to list the literature related to uniform convergence since it is not helpful for readers’ understanding of the contributions of this paper. It may be better that authors list the detailed literature related to high probability bound.

4.Theorem 1 in this paper is not the sharpest p-moment bound for sums of vector-valued functions. Authors demonstrate their bound is indeed tighter than Theorem 1 of [1]. However, [2] also provided a bound (Theorem 1) that is likely tighter than Theorem 1 in this paper. Note that, [2] also used Marcinkiewicz-Zygmund’s inequality to prove their bound. Besides, the paragraph from line 131 to line 136 states “On the other hand, in Section 3.2, we will carefully construct vector-valued functions which satisfies all the assumptions in Theorem 1 and ensures M = 0 at the same time. Under this condition, we can eliminate the first term.”. This point is also considered in Theorem 1 of [2]. I think that authors just consider the improvement to [1], but omit other related work.

[1]J. Fan and Y. Lei. High-probability generalization bounds for pointwise uniformly stable algorithms. Applied and Computational Harmonic Analysis, 70:101632, 2024.

[2]X. Yuan, P. Li. Exponential generalization bounds with near-optimal rates for $L_q$-stable algorithms. ICLR, 2023.

5.In Section 3.2, authors build some relationships between generalization error and stability parameter $\beta$. Authors think these relationships are under non-convex, non-smooth, non-PL conditions. These bounds are not the final generalization bounds but the relationships. After giving the stability bounds, the generalization bounds are finally determined. However, in Section 4, authors provide the stability bounds under (strongly) convex and smooth conditions. Therefore, authors didn’t remove (strongly) convex and smooth conditions for generalization analysis.

6.The symbol $M$ is repeatedly used in Theorem 1 and Theorem 2. Therefore, I suggest authors should carefully check their symbol settings.

7.In Remark 4, authors compare their Theorem 3 with the bound in [3]. It is unfair since, as mentioned in the above 5., the bound in [3] is a final result but Theorem 3 is not.

[3]Y. Xu and A. Zeevi. Towards optimal problem dependent generalization error bounds in statistical learning theory. Mathematics of Operations Research, 2024.

8.In line 633, $F(A(S)) - F(A(S))$ is wrong.

9.In line 633, Equation (31) should be an inequality.

10.In line 152, authors state “In nonconvex problems, we can only find a local minimizer by optimization algorithms which may be far away from the global minimizer. Thus the convergence does not make much sense in function values.”. So, they use uniform stability in gradients. However, in Section 4, they provide some uniform stability bounds in gradients for strongly convex problems. It is a paradox.

11.The form of the relationship in Theorem 3 is very normal. The method to obtain an excess risk bound $\mathcal{O}(1/n^2)$ is very simple. I think other normal generalization results (like [1]) in gradients can derive the excess risk bound $\mathcal{O}(1/n^2)$. The main contribution of this paper may be the simple method combining PL condition with some usual decompositions as shown in Proof of Remark 5. However, as mentioned in the above 10., the uniform stability in function values is more unreliable than the one in gradients under strongly convex condition.

Limitations:
Considering the 4. in Weaknesses, I suggest the author reconsider whether their result is the sharpest.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the standard statistical learning setting and derives $\mathcal{O}(\frac{1}{n ^ {2}})$ ($n$ denotes the number of samples) high-probability bounds for the excess risk $F(A(S)) - \inf_{w} F(w)$ ($A$ denotes the algorithm and $S$ denotes the training set) of ERM, PGD, and SGD. The best-known bounds prior to this work were $\mathcal{O}(\frac{\log n}{n})$ for ERM, PGD, that was derived using algorithmic stability, and $\mathcal{O}(\frac{1}{n ^ {2}})$  or ERM, SGD that was derived using uniform convergence. However, the latter demanded $n = \Omega(d)$ samples, thereby introducing the an undesirable dependence on $d$. The current paper shows that it is possible to obtain $\mathcal{O}(\frac{1}{n ^ {2}})$ bounds for the algorithms (without any dependence on $d$) under the lens of stability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The considered problem is interesting and the contributions of the paper show that sharper bounds (at par with uniform convergence, albeit without any dependence on $d$) are possible for ERM, SGD, and PGD for strongly convex and smooth stochastic convex optimization. Below I explain the roadmap taken by the authors in doing so, which also sheds light on some of the other aspects of the paper.

1) The authors first study the generalization gap via gradients, i.e. the quantity $\||\nabla F(A(S)) - \nabla F_S(A(S)\||$ for the statistical learning setting under the assumption that the function is Lipschitz and the algorithm is uniformly stable in gradients (Theorem 1 and 2). Under the nonconvex setting, the authors obtain dimension-independent bound for the generalization gap via gradients. This bound is subsequently studied under the assumption that the function is smooth and satisfies the Polyak-Lojaseiwicz (PL) condition. The obtained bound is a function of the gradient norm obtained at the end of optimization, i.e. $\|| \nabla F_S(A(S) \||$. 

2) To obtain excess risk bound for the algorithms, the (i) authors show that the algorithms are uniformly stable in gradients; (ii) translate the excess risk bound to a bound on the gradient via the PL inequality (recall from the premise that these algorithms are analyzed in the strongly convex and smooth setting of stochastic convex optimization, therefore PL holds vacuously); (iii) use triangle inequality to relate the bound on the gradient norm to the generalization gap, and use the bound on the generalization gap via gradients (see 1 above).

Weaknesses:
I found several typos in the main theorems in section 4. For example, the stability equation in Lemma 4 should be written with respect to the output at the iteration instead of the output of the ERM.  Also, why is the reference in Theorems in section 4 to Theorem 3, instead of Lemma 1? From my understanding (explained in the Strengths above), Lemma 1 is a specific instantiation of Theorem 3 to smooth + PL functions, which is exactly the premise in Section 4. What exactly is $w$ in the bound $F(w) - F(w^\star)$ in Theorem 5? I expect it to $w_{T + 1}$ (or $w_T$).

I don't see the point of Marcinkiewicz-Zygmund’s inequality with improved constants. The whole paper is about the improved dependence with respect to $n$, so I don't see a point in improving the specific constants in the inequality. 

I need some more clarification in lines 204--207. The authors state that Klochkov and Zhivotovskiy obtained $\mathcal{O}(\frac{1}{n})$ style bounds for the excess risk. What's the assumption on $f$ considered by them? The authors mention that they can obtain $\mathcal{O}(\frac{1}{n ^ {2}})$ bounds with an extra PL and smoothness assumption. Is this something for which Klochkov and Zhivotovskiy, 2021 could only obtain a suboptimal $\mathcal{O}(\frac{1}{n})$ bound? Earlier, my interpretation was that this work obtained $\frac{1}{n}$ bounds for ERM, and PGD for smooth and strongly convex stochastic convex optimization, but lines 204--207 made my understanding unclear. I want to make sure the authors are not invoking extra assumptions to get improved dependence.

Along similar lines as above, Lines 259--261 seem to be saying inconsistent things (is the assumption just smoothness, or strong convexity and smoothness). I would appreciate clarifications from the authors.

Minor Typos: 1) In definition 1, $\gamma$ and $\mu$ should be strictly positive; (2) Line 182: $\gamma$-smooth instead of $\gamma$-smoothness.

Based on the authors' responses, I would be happy to revise my assessment of the paper.

Limitations:
Yes, the authors have adequately addressed this.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",No,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper studies the generalization measured by gradients via a uniform gradient stability. For $\beta$-uniformly stable algorithms, the paper gives generalization bounds of order $O(1/n+\beta+\sqrt{E_Z[\|\nabla f(A(S);Z)\|_2^2]/n})$, which yields fast rates if $E_Z[\|\nabla f(A(S);Z)\|_2^2$ is small. The paper then uses this generalization measured by gradients to derive generalization error bounds under a PL condition. Applications to empirical risk minimization, gradient descent and stochastic gradient descent are given for strongly convex, smooth and Lipschitz problems.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The paper gives high-probability bounds for generalization gap on gradients, which include the gradient norm at the minimizer. This can imply fast rates in an interpolation setting. Under the PL condition, this gives fast rates of order $O(1/n^2)$.

The paper provides comprehensive applications to several algorithms such as empirical risk minimization, gradient descent and stochastic gradient.

Weaknesses:
The high-probability analysis based on uniform stability follows largely from existing work. I do not see enough novelty in the analysis. It would be helpful if the authors can summarize the challenges in the analysis and their novelty. 

As stated in the paper, Theorem 1 and Theorem 2 only improve the existing results by a constant factor. This improvement is not significant. 

As stated in the paper, the generalization by gradients is mostly interesting for nonconvex problems. However, for the applications in Section 4, the paper considers strongly convex problems. Also the results require smoothness and Lipschitz continuity. These assumptions seem to be a bit strong.

The paper gives fast rates under the case $F(w^*)=O(1/n)$ in Section 4. Note that Section 4 considers strongly convex problems. Then, the objective function should be of the form $F(w)=G(w)+\mu\|w\|^2$, where $G$ is related to loss. Then, if we require $F(w^*)=O(1/n)$, one needs $\mu\|w^*\|^2=O(1/n)$. Suppose we assume $\|w^*\|=O(1)$. Then, this requires $\mu=O(1/n)$. In this case, the generalization bound would be vacuous since $n\mu=O(1)$.

For SGD, the computation cost seems to be high. For example, in Theorem 6, the paper requires $T=n^4$ while in Theorem 13 the paper requires $T=n^2$. This high computational cost may not be appealing for large-scale problems.

In the proof of Lemma 1, the paper uses $\|\nabla F(A(S))\|_2\geq \mu\|A(S)-w^*\|$. This inequality does not generally hold under a PL condition. Indeed, Theorem 2 in Karimi et al 2016 require $\|A(S)-w^*\|$ to be replaced by the distance between $A(S)$ and the set of minimizers.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The work shows high probability excess risk bounds of $O(1/n^2)$ for several algorithms under strong convexity, smoothness, Lipschitz continuity and low noise assumptions using algorithmic stability.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
3: good

Strengths:
1. The results of the paper are interesting, showing a risk bound of $O(\frac{1}{n^2})$ using algorithmic stability.

2. The paper uses a novel technique, involving the stability of gradients to demonstrate excess risk bounds and presents applications of this technique in convex optimization.

Weaknesses:
1. The problem setup of the paper is not clearly detailed before the technical section, including the assumptions used for proving the results.

2. The presentation of results and related works is somewhat lacking. It would be beneficial if the authors summarized the results from previous work and compared them to the results in the current paper, including the set of assumptions made in each work.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper achieves the high probability excess risk bounds $\mathcal{O}(1/n^2)$ for empirical risk minimization, projected gradient descent and stochastic gradient descent under strong convexity, smoothness and Lipschitz continuity assumptions.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
Please see Summary.

Weaknesses:
1.The paragraph from line 52 to line 65 first analyzes the non-convex problems. Then, it suddenly mentions that this paper explores the stability of stochastic convex optimization algorithms with strongly convex losses in line 61. And, it doesn’t mention its non-convex analysis. It is so confusing. So, I suggest authors rewrite this paragraph to benefit readers’ understanding.

2.The paragraph from line 70 to line 73 is unnecessary since we can not obtain any useful information.

3.In Related Work, it is unnecessary to list the literature related to uniform convergence since it is not helpful for readers’ understanding of the contributions of this paper. It may be better that authors list the detailed literature related to high probability bound.

4.Theorem 1 in this paper is not the sharpest p-moment bound for sums of vector-valued functions. Authors demonstrate their bound is indeed tighter than Theorem 1 of [1]. However, [2] also provided a bound (Theorem 1) that is likely tighter than Theorem 1 in this paper. Note that, [2] also used Marcinkiewicz-Zygmund’s inequality to prove their bound. Besides, the paragraph from line 131 to line 136 states “On the other hand, in Section 3.2, we will carefully construct vector-valued functions which satisfies all the assumptions in Theorem 1 and ensures M = 0 at the same time. Under this condition, we can eliminate the first term.”. This point is also considered in Theorem 1 of [2]. I think that authors just consider the improvement to [1], but omit other related work.

[1]J. Fan and Y. Lei. High-probability generalization bounds for pointwise uniformly stable algorithms. Applied and Computational Harmonic Analysis, 70:101632, 2024.

[2]X. Yuan, P. Li. Exponential generalization bounds with near-optimal rates for $L_q$-stable algorithms. ICLR, 2023.

5.In Section 3.2, authors build some relationships between generalization error and stability parameter $\beta$. Authors think these relationships are under non-convex, non-smooth, non-PL conditions. These bounds are not the final generalization bounds but the relationships. After giving the stability bounds, the generalization bounds are finally determined. However, in Section 4, authors provide the stability bounds under (strongly) convex and smooth conditions. Therefore, authors didn’t remove (strongly) convex and smooth conditions for generalization analysis.

6.The symbol $M$ is repeatedly used in Theorem 1 and Theorem 2. Therefore, I suggest authors should carefully check their symbol settings.

7.In Remark 4, authors compare their Theorem 3 with the bound in [3]. It is unfair since, as mentioned in the above 5., the bound in [3] is a final result but Theorem 3 is not.

[3]Y. Xu and A. Zeevi. Towards optimal problem dependent generalization error bounds in statistical learning theory. Mathematics of Operations Research, 2024.

8.In line 633, $F(A(S)) - F(A(S))$ is wrong.

9.In line 633, Equation (31) should be an inequality.

10.In line 152, authors state “In nonconvex problems, we can only find a local minimizer by optimization algorithms which may be far away from the global minimizer. Thus the convergence does not make much sense in function values.”. So, they use uniform stability in gradients. However, in Section 4, they provide some uniform stability bounds in gradients for strongly convex problems. It is a paradox.

11.The form of the relationship in Theorem 3 is very normal. The method to obtain an excess risk bound $\mathcal{O}(1/n^2)$ is very simple. I think other normal generalization results (like [1]) in gradients can derive the excess risk bound $\mathcal{O}(1/n^2)$. The main contribution of this paper may be the simple method combining PL condition with some usual decompositions as shown in Proof of Remark 5. However, as mentioned in the above 10., the uniform stability in function values is more unreliable than the one in gradients under strongly convex condition.

Limitations:
Considering the 4. in Weaknesses, I suggest the author reconsider whether their result is the sharpest.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The paper considers the standard statistical learning setting and derives $\mathcal{O}(\frac{1}{n ^ {2}})$ ($n$ denotes the number of samples) high-probability bounds for the excess risk $F(A(S)) - \inf_{w} F(w)$ ($A$ denotes the algorithm and $S$ denotes the training set) of ERM, PGD, and SGD. The best-known bounds prior to this work were $\mathcal{O}(\frac{\log n}{n})$ for ERM, PGD, that was derived using algorithmic stability, and $\mathcal{O}(\frac{1}{n ^ {2}})$  or ERM, SGD that was derived using uniform convergence. However, the latter demanded $n = \Omega(d)$ samples, thereby introducing the an undesirable dependence on $d$. The current paper shows that it is possible to obtain $\mathcal{O}(\frac{1}{n ^ {2}})$ bounds for the algorithms (without any dependence on $d$) under the lens of stability.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
The considered problem is interesting and the contributions of the paper show that sharper bounds (at par with uniform convergence, albeit without any dependence on $d$) are possible for ERM, SGD, and PGD for strongly convex and smooth stochastic convex optimization. Below I explain the roadmap taken by the authors in doing so, which also sheds light on some of the other aspects of the paper.

1) The authors first study the generalization gap via gradients, i.e. the quantity $\||\nabla F(A(S)) - \nabla F_S(A(S)\||$ for the statistical learning setting under the assumption that the function is Lipschitz and the algorithm is uniformly stable in gradients (Theorem 1 and 2). Under the nonconvex setting, the authors obtain dimension-independent bound for the generalization gap via gradients. This bound is subsequently studied under the assumption that the function is smooth and satisfies the Polyak-Lojaseiwicz (PL) condition. The obtained bound is a function of the gradient norm obtained at the end of optimization, i.e. $\|| \nabla F_S(A(S) \||$. 

2) To obtain excess risk bound for the algorithms, the (i) authors show that the algorithms are uniformly stable in gradients; (ii) translate the excess risk bound to a bound on the gradient via the PL inequality (recall from the premise that these algorithms are analyzed in the strongly convex and smooth setting of stochastic convex optimization, therefore PL holds vacuously); (iii) use triangle inequality to relate the bound on the gradient norm to the generalization gap, and use the bound on the generalization gap via gradients (see 1 above).

Weaknesses:
I found several typos in the main theorems in section 4. For example, the stability equation in Lemma 4 should be written with respect to the output at the iteration instead of the output of the ERM.  Also, why is the reference in Theorems in section 4 to Theorem 3, instead of Lemma 1? From my understanding (explained in the Strengths above), Lemma 1 is a specific instantiation of Theorem 3 to smooth + PL functions, which is exactly the premise in Section 4. What exactly is $w$ in the bound $F(w) - F(w^\star)$ in Theorem 5? I expect it to $w_{T + 1}$ (or $w_T$).

I don't see the point of Marcinkiewicz-Zygmund’s inequality with improved constants. The whole paper is about the improved dependence with respect to $n$, so I don't see a point in improving the specific constants in the inequality. 

I need some more clarification in lines 204--207. The authors state that Klochkov and Zhivotovskiy obtained $\mathcal{O}(\frac{1}{n})$ style bounds for the excess risk. What's the assumption on $f$ considered by them? The authors mention that they can obtain $\mathcal{O}(\frac{1}{n ^ {2}})$ bounds with an extra PL and smoothness assumption. Is this something for which Klochkov and Zhivotovskiy, 2021 could only obtain a suboptimal $\mathcal{O}(\frac{1}{n})$ bound? Earlier, my interpretation was that this work obtained $\frac{1}{n}$ bounds for ERM, and PGD for smooth and strongly convex stochastic convex optimization, but lines 204--207 made my understanding unclear. I want to make sure the authors are not invoking extra assumptions to get improved dependence.

Along similar lines as above, Lines 259--261 seem to be saying inconsistent things (is the assumption just smoothness, or strong convexity and smoothness). I would appreciate clarifications from the authors.

Minor Typos: 1) In definition 1, $\gamma$ and $\mu$ should be strictly positive; (2) Line 182: $\gamma$-smooth instead of $\gamma$-smoothness.

Based on the authors' responses, I would be happy to revise my assessment of the paper.

Limitations:
Yes, the authors have adequately addressed this.

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",No,no,No,1.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposes Tree of Attributes Prompt learning (TAP). Unlike previous works that rely on unstructured class descriptions, this approach distillates structured knowledge graphs associated with class names from LLMs. Text/vision prompts and vision-conditional pooling module are designed to extract instance-specific text features. Extensive experimental results demosntrate its improved performances.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Overall, the idea of distillating structured knowledge from LLMs in the task of prompt learning is new and interesting.

- The paper designed an effective prompt learning framework to capture fine-grained attributes, using vision expert tokens and vision-conditional pooling layer.

- The illustrated way to generate structure tree of attribute from LLMs can also be used in other tasks.

- From the experiments, using structured knowledge leads to better performances than unstructured descriptions in base-to-novel and few-shot classification tasks.

- The visualization of class activation maps and attention weights look good. The paper is well written and easy to follow.

Weaknesses:
- Apart from the new framework, the method highly relies on the quality of tree of attribute generated with GPT-3.5-turbo. There is no study on the robustness aganist different LLMs, different generation prompts, or varying attribute sets.

- The loss includes a model regularization and its effectiveness is not discussed.

- In Figure 2, it is not too clear to me about $I_1 T_1$, $I_2 T_2$,etc. They seem not be discussed in the text parts.

Limitations:
One limitation is its reliance on LLMs (GPT) to generate the tree of attribute. When generating more complex responses, it is challening to ensure the quality and variances. How to keep a balance between the diversity of attribute sets and relevancy of attributes to classification is important.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new method called ""Attribute Prompt Learning Tree (TAP)"" to improve the performance of CLIP on zero-shot and few-shot classification tasks. The authors leverage large language models (LLMs) to generate more descriptive text prompts and introduce a hierarchical tree-like structure to systematically generate and integrate these descriptions, ensuring a layered and comprehensive understanding of the visual content. The method also learns specialized ""domain expert"" prompt tokens that focus on different visual attributes and uses a vision-based pooling module to extract text features for specific instances. Extensive experiments show that TAP outperforms state-of-the-art methods on zero-shot and few-shot classification tasks across multiple datasets

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1), The idea that utilizing LLM to generate tree-like prompts makes sense. This structured description approach is significantly different from the existing simple text prompt methods and provides an efficient way to improve VLMs.

2), The image-conditional pooling module looks like good for capturing instance-specific features.

3), Experiments and visualization demonstrate the effectiveness of the proposed model.

Weaknesses:
1), TAP introduces many textual and visual prompts, which leads to high computing and time costs. This may limit its applications.

2), TAP first generates hierarchical token prompts, while it seems like TAP does not use such a hierarchical structure to integrate the output of the text encoder. It only uses a pooling strategy to update the text encoder output with the visual feature. That is, TAP also does not utilize these relationships in the prompt graph.

3), TAP can be viewed as a multimodal prompt tuning method. What is the main difference between TAP and MAPLE,  ALIGN.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a method that aiming to align the vision modality with not only the category name but also the whole concept subgraph the noun represents in the knowledge graph. This is achieved by adding a bunch of attributes branches attached to this concept. The authors argue that this integration of attribute knowledge will make the alignment more transferrable and thus result in a good performance boost in terms of zero/few shot results.
Basically, this work focusing on the topic of textual prompt enrichment task that is investigated before but implement in a different manner. Additionally, the proposed method use seperate tokens to learn different aspectrs of attributes of given images, working as 'domain expert'.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Might be the first work trying to align the vision image with structured data. It is quite interesting considering that most text prompts now are less organized and noisy. And structured data, as pointed out in the recent research of LLM, may lead to better reasoning skill for a foundation model.
2. The proposed vision-conditional pooling can help the model filter out descriptions that are not direct appeared in the image.
3. Recieve good results on different classification datasets with the model trained with this method.

Weaknesses:
1. The attributes description is generated by the LLM, which could contain hallucinated content. While there are many reliable sources of knowledge such as wikipedia or conceptNet, this paper seems skip these sources to obtain some accurate attributes.
2. Though this paper decide to use a tree structure to represent the concept. The built tree is not encoded in a structure-awared manner. They are still feed as langauge tokens to the LLMs. 
3.  in equation (5), what is $v_y^a$ stands for? 
4. The author argued that the vision-conditional pooling, which is bascially a cross attention layer between the visual and language modal. The authors believe this this design will make the model filter out non-exisiting material in the text description. However, we know that due to the quirk of softmax function. You can never make some tokens attention to be '0'. Thus, the model is learning some spurious correlation aftertall.

Limitations:
Not applicable.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The TAP method structures textual descriptions in a hierarchical “concept-attribute-description” format, effectively creating a knowledge graph from large language models (LLMs) for each category name. This structure allows for a more comprehensive and detailed understanding of the visual content. The paper reimagines learnable prompt tokens as ""domain experts,"" each specializing in different aspects of the image, supplemented by a global perspective provided by the CLS token. To address potential misalignment between general descriptions and specific image content, the paper introduces a vision-conditional pooling module. This module extracts instance-specific text features, ensuring optimal image-text alignment.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed method incorporates structured tree of attribute into prompt tuning that provide richer supervisory information compared to unstructured attribute information. A set of experiments has been conducted, and the results look promising.

Weaknesses:
One major limitation of the method is that it requires human review to ""ensure the quality of the example"" (L175). Recall that one major advantage of prompt tuning is that it can adapt large models quickly to specific tasks. However, the requirement of human reviewing in the proposed method is not consistent with this goal. In addition, it is not clear how many human efforts are needed here, and how to handle the potential human bias in quality evaluation. 

The paper lacks cross-dataset experiments, which is typically provided in existing PT papers. The results are important to examine the domain generalization capability of the method. 

For training details, different learning rates were used for different datasets, however, existing methods typically use a same LR for all datasets. From this point, the comparison is somewhat unfair.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new prompt tuning method for adapting the vision-language model.  The authors design the tree of attribute prompt learning to substitute the categorical description for adapting the vision-language model. A vision-conditional pooling module is proposed to extract instance-specific text features. Extensive experimental results demonstrate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. A tree of attribute prompt learning method is proposed to guide the adatpation of VLM with the hierarchical semantic information.

2. This paper is well-written and easy to follow.

Weaknesses:
1. According to the experiment, the performance improvement of TAP is marginal, e.g., the few-shot performance on most of datasets.  Although the visualization results of VCP layer are impressive, the improvement of this module is also very slight compared to average pooling. 

2. The core motivation of this method is learning fine-grained attributes to adapt VLMs. However, similar ideas have been explored in previous works , e.g., APPL[1], MAP[2].  Please discuss the differences.

[1] AAPL: Adding Attributes to Prompt Learning for Vision-Language Models

[2] Multi-modal Attribute Prompting for Vision-Language Models

3. The construction of ToA depends heavily on the prior information on the category of attributes suitable for the dataset. However, one of the most capability of VLM is its zero-shot ability in the open-vocabulary context. What's the performance of the proposed method in the domain generalization setting?


4. The model details in Figure 2 are not presented very clear, especially the input & output streams. This figure should be refined for better clarity. 


5. The mechanism behind Equation (5) and the function of VCP needs more clarification. Why conduct constrastive learning between expert token P_a^v and attribute embedding v_c^a generated from P_a^v itself, instead of P_a^v and the embedding of attribute descriptions D?

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper proposes Tree of Attributes Prompt learning (TAP). Unlike previous works that rely on unstructured class descriptions, this approach distillates structured knowledge graphs associated with class names from LLMs. Text/vision prompts and vision-conditional pooling module are designed to extract instance-specific text features. Extensive experimental results demosntrate its improved performances.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Overall, the idea of distillating structured knowledge from LLMs in the task of prompt learning is new and interesting.

- The paper designed an effective prompt learning framework to capture fine-grained attributes, using vision expert tokens and vision-conditional pooling layer.

- The illustrated way to generate structure tree of attribute from LLMs can also be used in other tasks.

- From the experiments, using structured knowledge leads to better performances than unstructured descriptions in base-to-novel and few-shot classification tasks.

- The visualization of class activation maps and attention weights look good. The paper is well written and easy to follow.

Weaknesses:
- Apart from the new framework, the method highly relies on the quality of tree of attribute generated with GPT-3.5-turbo. There is no study on the robustness aganist different LLMs, different generation prompts, or varying attribute sets.

- The loss includes a model regularization and its effectiveness is not discussed.

- In Figure 2, it is not too clear to me about $I_1 T_1$, $I_2 T_2$,etc. They seem not be discussed in the text parts.

Limitations:
One limitation is its reliance on LLMs (GPT) to generate the tree of attribute. When generating more complex responses, it is challening to ensure the quality and variances. How to keep a balance between the diversity of attribute sets and relevancy of attributes to classification is important.

Rating:
6: marginally above the acceptance threshold

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
This paper proposes a new method called ""Attribute Prompt Learning Tree (TAP)"" to improve the performance of CLIP on zero-shot and few-shot classification tasks. The authors leverage large language models (LLMs) to generate more descriptive text prompts and introduce a hierarchical tree-like structure to systematically generate and integrate these descriptions, ensuring a layered and comprehensive understanding of the visual content. The method also learns specialized ""domain expert"" prompt tokens that focus on different visual attributes and uses a vision-based pooling module to extract text features for specific instances. Extensive experiments show that TAP outperforms state-of-the-art methods on zero-shot and few-shot classification tasks across multiple datasets

Soundness:
2: fair

Presentation:
3: good

Contribution:
3: good

Strengths:
1), The idea that utilizing LLM to generate tree-like prompts makes sense. This structured description approach is significantly different from the existing simple text prompt methods and provides an efficient way to improve VLMs.

2), The image-conditional pooling module looks like good for capturing instance-specific features.

3), Experiments and visualization demonstrate the effectiveness of the proposed model.

Weaknesses:
1), TAP introduces many textual and visual prompts, which leads to high computing and time costs. This may limit its applications.

2), TAP first generates hierarchical token prompts, while it seems like TAP does not use such a hierarchical structure to integrate the output of the text encoder. It only uses a pooling strategy to update the text encoder output with the visual feature. That is, TAP also does not utilize these relationships in the prompt graph.

3), TAP can be viewed as a multimodal prompt tuning method. What is the main difference between TAP and MAPLE,  ALIGN.

Limitations:
None

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper propose a method that aiming to align the vision modality with not only the category name but also the whole concept subgraph the noun represents in the knowledge graph. This is achieved by adding a bunch of attributes branches attached to this concept. The authors argue that this integration of attribute knowledge will make the alignment more transferrable and thus result in a good performance boost in terms of zero/few shot results.
Basically, this work focusing on the topic of textual prompt enrichment task that is investigated before but implement in a different manner. Additionally, the proposed method use seperate tokens to learn different aspectrs of attributes of given images, working as 'domain expert'.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
1. Might be the first work trying to align the vision image with structured data. It is quite interesting considering that most text prompts now are less organized and noisy. And structured data, as pointed out in the recent research of LLM, may lead to better reasoning skill for a foundation model.
2. The proposed vision-conditional pooling can help the model filter out descriptions that are not direct appeared in the image.
3. Recieve good results on different classification datasets with the model trained with this method.

Weaknesses:
1. The attributes description is generated by the LLM, which could contain hallucinated content. While there are many reliable sources of knowledge such as wikipedia or conceptNet, this paper seems skip these sources to obtain some accurate attributes.
2. Though this paper decide to use a tree structure to represent the concept. The built tree is not encoded in a structure-awared manner. They are still feed as langauge tokens to the LLMs. 
3.  in equation (5), what is $v_y^a$ stands for? 
4. The author argued that the vision-conditional pooling, which is bascially a cross attention layer between the visual and language modal. The authors believe this this design will make the model filter out non-exisiting material in the text description. However, we know that due to the quirk of softmax function. You can never make some tokens attention to be '0'. Thus, the model is learning some spurious correlation aftertall.

Limitations:
Not applicable.

Rating:
8: accept, good paper

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The TAP method structures textual descriptions in a hierarchical “concept-attribute-description” format, effectively creating a knowledge graph from large language models (LLMs) for each category name. This structure allows for a more comprehensive and detailed understanding of the visual content. The paper reimagines learnable prompt tokens as ""domain experts,"" each specializing in different aspects of the image, supplemented by a global perspective provided by the CLS token. To address potential misalignment between general descriptions and specific image content, the paper introduces a vision-conditional pooling module. This module extracts instance-specific text features, ensuring optimal image-text alignment.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
The proposed method incorporates structured tree of attribute into prompt tuning that provide richer supervisory information compared to unstructured attribute information. A set of experiments has been conducted, and the results look promising.

Weaknesses:
One major limitation of the method is that it requires human review to ""ensure the quality of the example"" (L175). Recall that one major advantage of prompt tuning is that it can adapt large models quickly to specific tasks. However, the requirement of human reviewing in the proposed method is not consistent with this goal. In addition, it is not clear how many human efforts are needed here, and how to handle the potential human bias in quality evaluation. 

The paper lacks cross-dataset experiments, which is typically provided in existing PT papers. The results are important to examine the domain generalization capability of the method. 

For training details, different learning rates were used for different datasets, however, existing methods typically use a same LR for all datasets. From this point, the comparison is somewhat unfair.

Limitations:
yes

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper proposes a new prompt tuning method for adapting the vision-language model.  The authors design the tree of attribute prompt learning to substitute the categorical description for adapting the vision-language model. A vision-conditional pooling module is proposed to extract instance-specific text features. Extensive experimental results demonstrate the effectiveness of the proposed method.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. A tree of attribute prompt learning method is proposed to guide the adatpation of VLM with the hierarchical semantic information.

2. This paper is well-written and easy to follow.

Weaknesses:
1. According to the experiment, the performance improvement of TAP is marginal, e.g., the few-shot performance on most of datasets.  Although the visualization results of VCP layer are impressive, the improvement of this module is also very slight compared to average pooling. 

2. The core motivation of this method is learning fine-grained attributes to adapt VLMs. However, similar ideas have been explored in previous works , e.g., APPL[1], MAP[2].  Please discuss the differences.

[1] AAPL: Adding Attributes to Prompt Learning for Vision-Language Models

[2] Multi-modal Attribute Prompting for Vision-Language Models

3. The construction of ToA depends heavily on the prior information on the category of attributes suitable for the dataset. However, one of the most capability of VLM is its zero-shot ability in the open-vocabulary context. What's the performance of the proposed method in the domain generalization setting?


4. The model details in Figure 2 are not presented very clear, especially the input & output streams. This figure should be refined for better clarity. 


5. The mechanism behind Equation (5) and the function of VCP needs more clarification. Why conduct constrastive learning between expert token P_a^v and attribute embedding v_c^a generated from P_a^v itself, instead of P_a^v and the embedding of attribute descriptions D?

Limitations:
Yes.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,No,0.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper shows the use of psychometric modeling techniques to measure the reasoning ability of LLMs on human exams. Specifically, the author(s) use Item Response Theory (IRT) to evaluate a Brazilian college-entrance exam, and demonstrate that IRT can provide a more informative evaluation of LLMs , including: the ability to distinguish human-like vs non-human-like response patterns, and to determine whether an exam can reliably measure an LLM's abilities. The empirical results suggest that traditional accuracy metrics are insufficient to assess the abilities of LLMs, and advocate for using IRT/psychometric theory to evaluate them.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Provides more comprehensive evaluation methods for LLM performance, I personally agree that accuracy metrics often do not give a complete picture of LLM ability.
2. The results section is methodological, it evaluates not only the IRT scores but how reliable they are based on several metrics (increases reliability of the evaluations)

Weaknesses:
1. The results analysis would benefit from a more detailed and clearer/deeper analysis, some statements made (eg. L293-298) are high level observations based on the results, but lack further insight into why certain LLM behaviors occur. Performing more detailed analyses into the specific subset of questions that contribute to scores could help to further understand the limitations of the LLM (L328-331 alludes to this, but very briefly).

2.  All the evaluations were done on variations of the ENEM exam dataset, showing that these psychometric method would also work on other datasets would make this approach more convincing that it will work for wider applications - I understand that there is limited time to run more experiments, so this is more so just a comment.

Limitations:
As mentioned above, as experiments are done on variations of one dataset, there are doubts about the generalizability of these methods on other datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on evaluating LLM abilities on a dataset of 8 college-entrance exams in Brazil (translated to English) measuring Item Response Theory instead of Accuracy. It highlights how such metric is useful to better understand models' performance.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I have found the work very well structured and appreciated the amount of care the authors have been given to the preparation of the dataset for the experiments (PDF processing, translation to English, use of exams designed for blind people in order to address questions based on images, etc). The experiments and results are discussed in details, with clear comparisons with human performance, discussing clear differences (e.g. in Mathematics).

Weaknesses:
While the paper is well structured, I felt it was missing a ""what now?"" message. The authors wrote a convincing argument in favour of using IRT, how do we convince now the field of ML / AI to use it more extensively? What are its limitations in comparison with accuracy-based metrics (given there are many, for instance you need information on overall human performance) and how do we overcome them?

Limitations:
I think the work should have discussed more about the specificities of ENEM - I agree with the authors that this is a relevant test-bed for this sort of evaluation, but in which ways are they specific / tailored to Brazil? Is there anything researchers should know about ENEM, which would make future testing / applications more challenging? For instance which topics are covered in Humanities or Languages, how specific are they about the country cultural context?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper initiates the empirical study of the performance of LLMs using Item Response Theory (IRT) models from a large college-entrance exam.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The question of construct validity of LLM evaluations (based on scores in exams designed for humans) is very important. This paper addresses this question in the earnest, by leveraging the primary tool developed in the education and psychometrics field, IRT. As far as I know, this is an original contribution as no previous work has used IRT in the same way to tackle the construct validity issue of LLM evaluations.

- The paper is largely well-written and the concepts (e.g. section 3) are explained clearly.

- Relative strength of empirical work. The experiments are well-designed and there is some breadth in the range of hypotheses explored, e.g. English vs. Portuguese effect on performance, Different topics of exams, response patterns in LLMs vs. humans with questions sorted by IRT difficulty. Seven different LLMs were evaluated.

- Significance. The method of this paper (i.e. using IRT in LLM evals) is an important first step to understanding what LLM evals are trying to measure. The paper already observes interesting phenomena, e.g. 
(1) the Fisher information of the math exam for the LLM test response distributions is low compared to other exams (although this is a somewhat obvious corollary of the p_i's being close to random for the LLMs performance on the math exam, the FI is a metric that points in the right direction).
(2) the joint distribution of IRT scores and CTT scores for LLMs is meaningfully different from that of the human test takers.

Weaknesses:
1. Some of the conclusions drawn by the paper appear unscientific/not well-substantiated. To me, the empirical results are subtle and require more thoughtful interpretations. Most of the interpretations of the experiments are confusing to me (i.e. I'm skeptical the conclusions follow), given the actual plots shown. For example, 

(a) What are ""outlier models"" (line 237)? We cannot see from Figure 1 that ""outlier models ... have higher accuracy and/or lower IRT scores..."" - how is this statement supported?

(b) line 223-224. The scale of IRT scores and CTT scores is not comparable. How can you conclude there is ""greater variability"" in the latter than in the IRT score? This is not scientific.

(c) line 264-265. The statement ""...questions that are easy for humans but difficult for LLMs"" is again inaccurate. The questions are relatively easier for humans but may not be ""easier"" than the other questions for humans, if easier means for humans anyway.

(d) Why is the math exam not meaningful for evaluating LLMs? Doesn't it suggest that the models are randomly guessing and therefore bad? I don't agree with this interpretation.

2. A clarity issue with the math writing. Line 154-155: This sentence ""...j has a more likely response vector than indicated by their ability"" is mathematically wrong. It is not possible to have a random draw from a Multinomial distribution that is ""more likely"" (i.e. higher probability) than the expectation vector (which is not even in the space of possible draws).

3. Experiment section writing missing some details and figures are somewhat difficult to interpret (esp Figure 1). I have several unanswered questions. How was the closed curve generated from the 30 points (of random shuffles)? The caption for Figure 1 could be more informative, e.g. was the exam answered in English or Portuguese by the LLM. If English, are the IRT models fit still valid? - I don't think so. 

4. Typo in lines 232-233, ""Natural sciences"" appears twice. and the sentence contradicts the graph.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides a fresh perspective to evaluating LLMs by arguing for a stronger emphasis on psychometric methods particularly Item Response Theory (IRT) when evaluating them on exams designed for humans, rather than the reliance on traditional metrics such as accuracy. The authors postulate that IRT provides a more comprehensive evaluation by considering not just the number of correct answers but also the difficulty of the questions and the patterns of responses. The authors utilize the Brazilian college entrance exam ENEM for their case study and compare how various LLMs fare against human test-takers. They show how psychometric methods can be leveraged to distinguish between human like and non-human like responses. Furthermore, they demonstrate how IRT can be used to assess the suitability of an exam for making meaningful measurements of an LLM's abilities in the given area.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper is very well written. It provides a comprehensive literature review and does a good job at covering the background information. The experimental analysis is sound with sufficient supporting materials. The paper makes significant novel contributions to evaluation of LLMs. The application of psychometric methods and the insights that can be mined through them when used to compare LLMs can be of significant interest to the research community. The experimental results on assessing whether an exam is a good indicator of an LLM's ability are particularly interesting and open up significant opportunities for future research.

Weaknesses:
The error analysis can be more detailed especially in areas where the results are surprising. This would better help support the conclusions.       For instance for the questions in Math and Natural Sciences wherein the models show fluctuating performance it would be useful to know what those questions aim to test. Are LLMs not able to solve the problems due to calculation errors or do these problems involve more complex multi-step reasoning or is it just linked to knowledge cutoff (e.g questions involving current events)?

Limitations:
The pre-requisite for this type of evaluation seems to be the existence of a strong IRT model which in turn requires the existence of large amount of carefully annotated human data.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper shows the use of psychometric modeling techniques to measure the reasoning ability of LLMs on human exams. Specifically, the author(s) use Item Response Theory (IRT) to evaluate a Brazilian college-entrance exam, and demonstrate that IRT can provide a more informative evaluation of LLMs , including: the ability to distinguish human-like vs non-human-like response patterns, and to determine whether an exam can reliably measure an LLM's abilities. The empirical results suggest that traditional accuracy metrics are insufficient to assess the abilities of LLMs, and advocate for using IRT/psychometric theory to evaluate them.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Provides more comprehensive evaluation methods for LLM performance, I personally agree that accuracy metrics often do not give a complete picture of LLM ability.
2. The results section is methodological, it evaluates not only the IRT scores but how reliable they are based on several metrics (increases reliability of the evaluations)

Weaknesses:
1. The results analysis would benefit from a more detailed and clearer/deeper analysis, some statements made (eg. L293-298) are high level observations based on the results, but lack further insight into why certain LLM behaviors occur. Performing more detailed analyses into the specific subset of questions that contribute to scores could help to further understand the limitations of the LLM (L328-331 alludes to this, but very briefly).

2.  All the evaluations were done on variations of the ENEM exam dataset, showing that these psychometric method would also work on other datasets would make this approach more convincing that it will work for wider applications - I understand that there is limited time to run more experiments, so this is more so just a comment.

Limitations:
As mentioned above, as experiments are done on variations of one dataset, there are doubts about the generalizability of these methods on other datasets.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper focuses on evaluating LLM abilities on a dataset of 8 college-entrance exams in Brazil (translated to English) measuring Item Response Theory instead of Accuracy. It highlights how such metric is useful to better understand models' performance.

Soundness:
3: good

Presentation:
4: excellent

Contribution:
3: good

Strengths:
I have found the work very well structured and appreciated the amount of care the authors have been given to the preparation of the dataset for the experiments (PDF processing, translation to English, use of exams designed for blind people in order to address questions based on images, etc). The experiments and results are discussed in details, with clear comparisons with human performance, discussing clear differences (e.g. in Mathematics).

Weaknesses:
While the paper is well structured, I felt it was missing a ""what now?"" message. The authors wrote a convincing argument in favour of using IRT, how do we convince now the field of ML / AI to use it more extensively? What are its limitations in comparison with accuracy-based metrics (given there are many, for instance you need information on overall human performance) and how do we overcome them?

Limitations:
I think the work should have discussed more about the specificities of ENEM - I agree with the authors that this is a relevant test-bed for this sort of evaluation, but in which ways are they specific / tailored to Brazil? Is there anything researchers should know about ENEM, which would make future testing / applications more challenging? For instance which topics are covered in Humanities or Languages, how specific are they about the country cultural context?

Rating:
7: Accept: Technically solid paper, with high impact on at least one sub-area, or moderate-to-high impact on more than one areas, with good-to-excellent evaluation, resources, reproducibility, and no unaddressed ethical considerations.

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper initiates the empirical study of the performance of LLMs using Item Response Theory (IRT) models from a large college-entrance exam.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
4: excellent

Strengths:
- The question of construct validity of LLM evaluations (based on scores in exams designed for humans) is very important. This paper addresses this question in the earnest, by leveraging the primary tool developed in the education and psychometrics field, IRT. As far as I know, this is an original contribution as no previous work has used IRT in the same way to tackle the construct validity issue of LLM evaluations.

- The paper is largely well-written and the concepts (e.g. section 3) are explained clearly.

- Relative strength of empirical work. The experiments are well-designed and there is some breadth in the range of hypotheses explored, e.g. English vs. Portuguese effect on performance, Different topics of exams, response patterns in LLMs vs. humans with questions sorted by IRT difficulty. Seven different LLMs were evaluated.

- Significance. The method of this paper (i.e. using IRT in LLM evals) is an important first step to understanding what LLM evals are trying to measure. The paper already observes interesting phenomena, e.g. 
(1) the Fisher information of the math exam for the LLM test response distributions is low compared to other exams (although this is a somewhat obvious corollary of the p_i's being close to random for the LLMs performance on the math exam, the FI is a metric that points in the right direction).
(2) the joint distribution of IRT scores and CTT scores for LLMs is meaningfully different from that of the human test takers.

Weaknesses:
1. Some of the conclusions drawn by the paper appear unscientific/not well-substantiated. To me, the empirical results are subtle and require more thoughtful interpretations. Most of the interpretations of the experiments are confusing to me (i.e. I'm skeptical the conclusions follow), given the actual plots shown. For example, 

(a) What are ""outlier models"" (line 237)? We cannot see from Figure 1 that ""outlier models ... have higher accuracy and/or lower IRT scores..."" - how is this statement supported?

(b) line 223-224. The scale of IRT scores and CTT scores is not comparable. How can you conclude there is ""greater variability"" in the latter than in the IRT score? This is not scientific.

(c) line 264-265. The statement ""...questions that are easy for humans but difficult for LLMs"" is again inaccurate. The questions are relatively easier for humans but may not be ""easier"" than the other questions for humans, if easier means for humans anyway.

(d) Why is the math exam not meaningful for evaluating LLMs? Doesn't it suggest that the models are randomly guessing and therefore bad? I don't agree with this interpretation.

2. A clarity issue with the math writing. Line 154-155: This sentence ""...j has a more likely response vector than indicated by their ability"" is mathematically wrong. It is not possible to have a random draw from a Multinomial distribution that is ""more likely"" (i.e. higher probability) than the expectation vector (which is not even in the space of possible draws).

3. Experiment section writing missing some details and figures are somewhat difficult to interpret (esp Figure 1). I have several unanswered questions. How was the closed curve generated from the 30 points (of random shuffles)? The caption for Figure 1 could be more informative, e.g. was the exam answered in English or Portuguese by the LLM. If English, are the IRT models fit still valid? - I don't think so. 

4. Typo in lines 232-233, ""Natural sciences"" appears twice. and the sentence contradicts the graph.

Limitations:
Yes.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This paper provides a fresh perspective to evaluating LLMs by arguing for a stronger emphasis on psychometric methods particularly Item Response Theory (IRT) when evaluating them on exams designed for humans, rather than the reliance on traditional metrics such as accuracy. The authors postulate that IRT provides a more comprehensive evaluation by considering not just the number of correct answers but also the difficulty of the questions and the patterns of responses. The authors utilize the Brazilian college entrance exam ENEM for their case study and compare how various LLMs fare against human test-takers. They show how psychometric methods can be leveraged to distinguish between human like and non-human like responses. Furthermore, they demonstrate how IRT can be used to assess the suitability of an exam for making meaningful measurements of an LLM's abilities in the given area.

Soundness:
4: excellent

Presentation:
4: excellent

Contribution:
4: excellent

Strengths:
The paper is very well written. It provides a comprehensive literature review and does a good job at covering the background information. The experimental analysis is sound with sufficient supporting materials. The paper makes significant novel contributions to evaluation of LLMs. The application of psychometric methods and the insights that can be mined through them when used to compare LLMs can be of significant interest to the research community. The experimental results on assessing whether an exam is a good indicator of an LLM's ability are particularly interesting and open up significant opportunities for future research.

Weaknesses:
The error analysis can be more detailed especially in areas where the results are surprising. This would better help support the conclusions.       For instance for the questions in Math and Natural Sciences wherein the models show fluctuating performance it would be useful to know what those questions aim to test. Are LLMs not able to solve the problems due to calculation errors or do these problems involve more complex multi-step reasoning or is it just linked to knowledge cutoff (e.g questions involving current events)?

Limitations:
The pre-requisite for this type of evaluation seems to be the existence of a strong IRT model which in turn requires the existence of large amount of carefully annotated human data.

Rating:
8: accept, good paper

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,No,0.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposes a new method, called Energy Rank Alignment (ERA) to finetune large language models (LLMs) for molecular generation in a similar fashion to Reinforcement Learning from Human Feedback (RLHF). The paper first introduces how the alignment task  in LLMs is very similar to creating property-conditioned molecules from SMILES strings, which are token-based generation techniques. In the introduction, the paper distinguishes ERA from common RLHF methods, such as PPO and DPO, by stating that it has a minimization objectives and leverages a reward function. Next, the paper describes related work for using LLMs for molecular generation and RLHF for language models and reiterates the differences of ERA compared to PPO and DPO.

In Section 2, the paper outlines the definition of ERA which mostly center on the derivation of relevant loss functions that the algorithm aims to minimize. In its definition, the ERA loss makes use of the KL divergence to arrive at the final formulation at the end of Section 2 leading up to the on-policy loss formulation for ERA. Section 3 provides a theoretical analysis of the ERA loss and its gradients, as well as its connections to the regularized entropy objective.

Section 4 describes the experiments for molecular generation using ERA, including unprompted and prompted generation. The paper also includes a sub-section on general alignment settings of LLMs related to IMDB movie reviews. The results generally show a distribution shift between models finetuned with ERA and those that were not. The paper subsequently ends with a conclusion and discussion of limitations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The provides proposed an interesting method and finetuning objectives that is useful for conditioned molecular generation and LLM alignment. The strengths include:
* A novel method for designing property conditioned molecules that is also applicable to LLM alignment. [Originality, Significance]
* A detailed derivation of the ERA loss, as well as a theoretical analysis on relevant properties. [Quality, Clarity]
* Experiments that generally support the distribution shift induced by the ERA method.

Weaknesses:
The weaknesses of the paper mostly center on expanding relevant related work and baselines for experiments:
* The authors do not discussion related work to training of transformer models and LLMs using reinforcement learning to arrive at molecules with desired properties. Some examples include [1] [2] 
* The experiments do not include baseline evaluation of DPO and PPO, which would have provided relevant details for how ERA performs compared to established baselines. 
* The paper could be strengthened by providing additional details related to experimental settings (see questions)


[1] Ghugare, Raj, Santiago Miret, Adriana Hugessen, Mariano Phielipp, and Glen Berseth. ""Searching for High-Value Molecules Using Reinforcement Learning and Transformers."" In The Twelfth International Conference on Learning Representations.

[2] Blaschke, Thomas, Josep Arús-Pous, Hongming Chen, Christian Margreitter, Christian Tyrchan, Ola Engkvist, Kostas Papadopoulos, and Atanas Patronov. ""REINVENT 2.0: an AI tool for de novo drug design."" Journal of chemical information and modeling 60, no. 12 (2020): 5918-5922.

Limitations:
The authors briefly discuss limitations at the end of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce “Energy Rank Alignment”, a novel alternative to PPO and DPO for policy optimization when an explicit reward model is available. ERA is shown to work for enriching chemical libraries for proxy objectives that are fast and easy to compute, and has clear benefits in the simplicity of tuning the strength of regularization to a reference and entropy of samples with two decoupled parameters. This controllability allows ERA to avoid greedy policies and the sort of mode collapse often observed using DPO.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The ERA approach is interesting and clearly defined. It is well-suited for many preference optimization settings, where an explicit reward model is available and alternative methods do not take advantage of this. The authors show results on multi-objective optimization to illustrate that the approach is not limited to greedy optimization of single objectives.

Weaknesses:
The main weakness of the paper is the evaluation with respect to lead optimization of small molecules. This is a notoriously difficult kind of evaluation to make meaningful with purely in silico experiments. One clear opportunity for the authors to improve their evals, while respecting the constraints imposed by easily-computable reward functions, is to incorporate some kind of online evaluation. Comparing DPO and ERA in an online setting would be informative and more relevant for the chemistry community.

Limitations:
Partially

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study an important problem about searching through chemical space, where the number of possible molecules grows combinatorially with the number of atoms. They focus on aligning large autoregressive models trained on chemical compound databases to generate molecules. The energy rank alignment (ERA) algorithm is proposed to use an explicit reward function to produce a gradient-based objective for optimizing autoregressive policies.  The authors offer theoretical insights into the relationship between energy rank alignment (ERA) and proximal policy optimization (PPO), direct preference optimization (DPO). Their experiments show that ERA is scalable, does not require reinforcement learning, and performs well compared to DPO when preference observations per pairing are limited.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors study a significant problem about generating molecules with desired properties based on autoregressive models by proposing the energy rank alignment (ERA) algorithm. 

2. This paper is well written.

3. The proposed methods work reasonably well.

Weaknesses:
1. Diversity, novelty and uniqueness are all important properties for drug discovery as discussed in previous works. To verify whether the models can be used to improve the process of drug discovery, the paper may benefit from comparing the aligned models with the reference model based on these metrics.

2. Missing the discussion of the related works which also focus on molecule optimization and drug discovery for both traditional and state-of-the-art methods, such as [1] [2] and so on.

3. The authors propose using reinforcement learning for drug optimization, a well-established method frequently employed in prior works, such as [3,4]. Additionally, advantage-based and multi-objective policy optimization are well-known in the reinforcement learning literature. A more comprehensive analysis of the limitations of this approach, along with a comparison to other existing methods, would have been beneficial.

[1] Drugassist: A large language model for molecule optimization.

[2] Automatic chemical design using a data-driven continuous representation of molecules.

[3] Optimization of molecules via deep reinforcement learning. Scientific Reports. 2019. 

[4] Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning. Nature Machine Intelligence. 2021.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposes a new method, called Energy Rank Alignment (ERA) to finetune large language models (LLMs) for molecular generation in a similar fashion to Reinforcement Learning from Human Feedback (RLHF). The paper first introduces how the alignment task  in LLMs is very similar to creating property-conditioned molecules from SMILES strings, which are token-based generation techniques. In the introduction, the paper distinguishes ERA from common RLHF methods, such as PPO and DPO, by stating that it has a minimization objectives and leverages a reward function. Next, the paper describes related work for using LLMs for molecular generation and RLHF for language models and reiterates the differences of ERA compared to PPO and DPO.

In Section 2, the paper outlines the definition of ERA which mostly center on the derivation of relevant loss functions that the algorithm aims to minimize. In its definition, the ERA loss makes use of the KL divergence to arrive at the final formulation at the end of Section 2 leading up to the on-policy loss formulation for ERA. Section 3 provides a theoretical analysis of the ERA loss and its gradients, as well as its connections to the regularized entropy objective.

Section 4 describes the experiments for molecular generation using ERA, including unprompted and prompted generation. The paper also includes a sub-section on general alignment settings of LLMs related to IMDB movie reviews. The results generally show a distribution shift between models finetuned with ERA and those that were not. The paper subsequently ends with a conclusion and discussion of limitations.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
The provides proposed an interesting method and finetuning objectives that is useful for conditioned molecular generation and LLM alignment. The strengths include:
* A novel method for designing property conditioned molecules that is also applicable to LLM alignment. [Originality, Significance]
* A detailed derivation of the ERA loss, as well as a theoretical analysis on relevant properties. [Quality, Clarity]
* Experiments that generally support the distribution shift induced by the ERA method.

Weaknesses:
The weaknesses of the paper mostly center on expanding relevant related work and baselines for experiments:
* The authors do not discussion related work to training of transformer models and LLMs using reinforcement learning to arrive at molecules with desired properties. Some examples include [1] [2] 
* The experiments do not include baseline evaluation of DPO and PPO, which would have provided relevant details for how ERA performs compared to established baselines. 
* The paper could be strengthened by providing additional details related to experimental settings (see questions)


[1] Ghugare, Raj, Santiago Miret, Adriana Hugessen, Mariano Phielipp, and Glen Berseth. ""Searching for High-Value Molecules Using Reinforcement Learning and Transformers."" In The Twelfth International Conference on Learning Representations.

[2] Blaschke, Thomas, Josep Arús-Pous, Hongming Chen, Christian Margreitter, Christian Tyrchan, Ola Engkvist, Kostas Papadopoulos, and Atanas Patronov. ""REINVENT 2.0: an AI tool for de novo drug design."" Journal of chemical information and modeling 60, no. 12 (2020): 5918-5922.

Limitations:
The authors briefly discuss limitations at the end of the paper.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors introduce “Energy Rank Alignment”, a novel alternative to PPO and DPO for policy optimization when an explicit reward model is available. ERA is shown to work for enriching chemical libraries for proxy objectives that are fast and easy to compute, and has clear benefits in the simplicity of tuning the strength of regularization to a reference and entropy of samples with two decoupled parameters. This controllability allows ERA to avoid greedy policies and the sort of mode collapse often observed using DPO.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
The ERA approach is interesting and clearly defined. It is well-suited for many preference optimization settings, where an explicit reward model is available and alternative methods do not take advantage of this. The authors show results on multi-objective optimization to illustrate that the approach is not limited to greedy optimization of single objectives.

Weaknesses:
The main weakness of the paper is the evaluation with respect to lead optimization of small molecules. This is a notoriously difficult kind of evaluation to make meaningful with purely in silico experiments. One clear opportunity for the authors to improve their evals, while respecting the constraints imposed by easily-computable reward functions, is to incorporate some kind of online evaluation. Comparing DPO and ERA in an online setting would be informative and more relevant for the chemistry community.

Limitations:
Partially

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The authors study an important problem about searching through chemical space, where the number of possible molecules grows combinatorially with the number of atoms. They focus on aligning large autoregressive models trained on chemical compound databases to generate molecules. The energy rank alignment (ERA) algorithm is proposed to use an explicit reward function to produce a gradient-based objective for optimizing autoregressive policies.  The authors offer theoretical insights into the relationship between energy rank alignment (ERA) and proximal policy optimization (PPO), direct preference optimization (DPO). Their experiments show that ERA is scalable, does not require reinforcement learning, and performs well compared to DPO when preference observations per pairing are limited.

Soundness:
3: good

Presentation:
2: fair

Contribution:
2: fair

Strengths:
1. The authors study a significant problem about generating molecules with desired properties based on autoregressive models by proposing the energy rank alignment (ERA) algorithm. 

2. This paper is well written.

3. The proposed methods work reasonably well.

Weaknesses:
1. Diversity, novelty and uniqueness are all important properties for drug discovery as discussed in previous works. To verify whether the models can be used to improve the process of drug discovery, the paper may benefit from comparing the aligned models with the reference model based on these metrics.

2. Missing the discussion of the related works which also focus on molecule optimization and drug discovery for both traditional and state-of-the-art methods, such as [1] [2] and so on.

3. The authors propose using reinforcement learning for drug optimization, a well-established method frequently employed in prior works, such as [3,4]. Additionally, advantage-based and multi-objective policy optimization are well-known in the reinforcement learning literature. A more comprehensive analysis of the limitations of this approach, along with a comparison to other existing methods, would have been beneficial.

[1] Drugassist: A large language model for molecule optimization.

[2] Automatic chemical design using a data-driven continuous representation of molecules.

[3] Optimization of molecules via deep reinforcement learning. Scientific Reports. 2019. 

[4] Multi-constraint molecular generation based on conditional transformer, knowledge distillation and reinforcement learning. Nature Machine Intelligence. 2021.

Limitations:
Yes

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,No,0.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The main claim of this paper is that adversarial suffixes against large language models (LLMs) function by distracting the model from the original harmful goal to the suffix itself. The authors then propose a modification to GCG attack by incorporating a regularization term that increases the attention score on the adversarial suffix.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
### 1. Originality and significance

The main claim of the paper is an interesting hypothesis that aims to unfold the inner workings of adversarial attacks on LLMs. This type of question can lead to a nice interpretability tool and/or a potential mitigation. Hence, the significance of this research question is clear. 

While some existing works start to look into “features” or neurons that correspond to these jailbreak attacks, the attention weights have not been deeply studied to the extent of my knowledge so it could be a nice complementary explanation.

### 2. Experiment coverage

The experiments on the attacks are relatively thorough. The authors compare their method against three existing SOTA attacks (GCG, AutoDAN, and ICA) on various open-source models. The transfer attack experiments in Section 3.4 also cover a broad range of closed-source models. The evaluation metrics are also comprehensive, including both the keyword matching and the GPT-4 evaluation.

Weaknesses:
### 1. Attention score measurement and interpretation

I first notice that in Figure 2, the attention scores on all parts (system, goal, suffix) can all go up as the optimization progresses and that the attention scores can be larger than 1 in Table 2 and 3. This suggests that the attention scores are before softmax and hence, not normalized to sum to 1. Please feel free to correct me if I’m mistaken.

1. If this is the case, it makes the score much more difficult to interpret and compare across different attacks. The absolute unnormalized value of the attention scores does not mean much because, for example, even if the score increases for the suffix portion, it may gets smaller relative to the other portions (system or goal). This is major flaw that undermines the main conclusion of the paper.
2. If that authors have not already done so, I would like to ask that all the reported attention scores be normalized (after softmax). The autoregressive generation also contributes to the attention scores, i.e., attention score of the target token $x_{n+2}$ also includes the target token $x_{t+1}$ along with all the prompt tokens $x_{1:n}$. I’m not sure what is the best way to normalize their effect. One way is to simply leave them out of the softmax, but there could be an interesting trend that we fail to capture this way. Another way is to report *difference* between average unnormalized attention score on the goal vs on the suffix portions. This also gives us a relative score but ignores the system portion.
3. In Figure 2 (left), ASR also increases along with the attention score on the goal, contradicting the main claim of the paper that higher attention score on the suffix is better.
4. It is unclear to me how Figure 5 supports the main claim of the paper. The attention pattern on ""Vanilla"" is strikingly similar to that of  ""ICA"" on the goal segment. Based on the color bar, the ICA attention score also seems higher than the Vanilla which contradicts the claim that the attack ""diverts the model’s attention away from the goal towards themselves.”

### 2. Section 3.3: Generalize AttnGCG to other attack methods

1. The purpose of this experiment is unclear to me. If the authors wish to prove their claim that higher attention weight on the suffix leads to a better attack, there should be a better controlled experiments than running GCG or AttnGCG on prompts generated by the other methods. This experiment entangles the initialization method with the attention score.
2. It might be interesting to see AttnGCG with varying values of $w_t$ and $w_a$.
3. I’d suggest an experiment where the attention loss is incorporated into AutoDAN (or other attacks) optimization objective. This would better emphasize the transferability and the usefulness of the attention loss across multiple attack algorithms.

### 3. Limited empirical improvement

While the main idea could help improve interpretability to these adversarial attacks, the attack that is inspired by this observation, AttnGCG, does not lead to significant improvement in the attack success rate, especially in the transfer setting. In the white-box setting, the improvement seems consistent across models, but the small margin suggests that attention score is not the most important factor that determines the success of the attack.

That said, it is sufficiently convincing to me that AttnGCG performs better than GCG and may replace it for evaluating the safety of LLMs.

Limitations:
Limitations and negative societal impact have been adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes a new adversarial attack strategy on LLMs which improves over existing adversarial attacks. For this the authors propose a new regularizer that maximizes the weight of attention corresponding to suffix tokens, which naturally results in minimizing the weight for the other tokens present in the input prompt. Using this additional regularizer with GCG results in improved attack success rate. The authors also show that this attack is transferable to other attack methods like ICA and AutoDAN.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) The paper is well motivated and the proposed loss follows well with the reader’s intuition.
2) The results are promising and the gains over the existing GCG attack are significant.
3) The comparison is comprehensive, involving different models.

Weaknesses:
1) It is not clear how the transferability of the same suffix tokens is for different goal prompts. This is important to investigate because GCG shows that the generated attacks are universal and can transfer on different goal prompts. I am currently a bit skeptical that the transferability on using the proposed attack might be limited because the generated suffix tokens might be more specialized for the given goal prompt. This is expected because now the generation of the suffix tokens is largely conditioned on the target target tokens due to the proposed regularizer. 

2) I believe it might be possible that using the proposed attack the model ends up outputting something potentially harmful but completely unrelated with the input prompt. This might be a possibility because the proposed approach inherently minimizes the attention on the goal tokens, which means the context of the input might become less relevant. It would be great if the authors could share some analysis on transferability of adv prompts and also share the generated text for GCG and AttnGCG.

3) It is not clear why maximizing the attention weights for suffix tokens should always lead to a stronger attack? This is also evident from tables 2 and 3 where AutoDAN has a lower value of goal attention score but stil leads to weaker attack as compared to GCG (see Table-4). Thus the argument presented in 162-163 seems questionable. 
In general, it is not clear why authors did not attempt to analyze the defenses like the ones proposed in [1]. Particularly, I believe it is important to analyze if the proposed attacks are able to bypass detection filters based on perplexity [1]. 

[1] Jain, Neel et al. “Baseline Defenses for Adversarial Attacks Against Aligned Language Models.” ArXiv abs/2309.00614 (2023)

Limitations:
Yes, the authors have addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a refined GCG method named AttnGCG for Large Language Model jailbreaking attacks. They focus on the attention scores of the input components, refining the loss function by adding an Attention Loss term. The attack success rates are greatly improved. Various experiments are provided to support the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1)	An interesting finding is that as the attention score on the adversarial suffix increases, the effectiveness in safeguarding LLM diminishes.

(2)	The experiments are conducted on various LLMs to prove the effectiveness of the AttnGCG.

Weaknesses:
(1)	It is unclear whether the increased success cases correspond to the 'regret' cases observed in GCG. The authors proposed AttnGCG to address the issue where the model successfully generates target tokens but then rejects the request; however, the results remain ambiguous.

(2)	In the success case illustrated in Figure 4, the attention scores at the boundary between the goal and the suffix are significantly higher than in other regions. Is this a common phenomenon in success cases? If so, why does this occur?

(3)	In Appendix A.3, the table shows that the system prompt for Llama-2 and Llama-3 is set to None, which is different from most jailbreaking papers, including the original GCG. How does this influence the attacking success rate? The authors should also report the success rate under the standard system prompt.

I will reconsider my score if all these problems are adequately addressed.

Limitations:
The authors adequately discussed the limitations of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new jailbreak attack method against LLMs, called AttnGCG. The method integrates a loss of maximizing the attention scores of the adversarial suffix. The paper provides experimental results to show the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written and easy to follow.

Weaknesses:
My main concerns are as follows.

- Will increasing the attention scores of adversarial suffixes make the responses focus on the content in adversarial suffixes?  
- The discussion in lines 151-164 is weak. Specifically, in Figure 4, AttnGCG explicitly increases the attention scores of adversarial suffixes, so it is natural to have higher adversarial suffix attention scores. It is not convincing to say ""uncover the underlying reasons for successful attacks within the model’s attention mechanism"".  
- In Table 3, AutoDAN achieves 0.227 goal attention score, while the scores of GCG and AttnGCG are 0.8657 and 0.793. Does the observation mean that AutoDAN is better than AttnGCG?
- Some content seems to be redundant, e.g., Figure 1 and Algorithm 1.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The main claim of this paper is that adversarial suffixes against large language models (LLMs) function by distracting the model from the original harmful goal to the suffix itself. The authors then propose a modification to GCG attack by incorporating a regularization term that increases the attention score on the adversarial suffix.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
### 1. Originality and significance

The main claim of the paper is an interesting hypothesis that aims to unfold the inner workings of adversarial attacks on LLMs. This type of question can lead to a nice interpretability tool and/or a potential mitigation. Hence, the significance of this research question is clear. 

While some existing works start to look into “features” or neurons that correspond to these jailbreak attacks, the attention weights have not been deeply studied to the extent of my knowledge so it could be a nice complementary explanation.

### 2. Experiment coverage

The experiments on the attacks are relatively thorough. The authors compare their method against three existing SOTA attacks (GCG, AutoDAN, and ICA) on various open-source models. The transfer attack experiments in Section 3.4 also cover a broad range of closed-source models. The evaluation metrics are also comprehensive, including both the keyword matching and the GPT-4 evaluation.

Weaknesses:
### 1. Attention score measurement and interpretation

I first notice that in Figure 2, the attention scores on all parts (system, goal, suffix) can all go up as the optimization progresses and that the attention scores can be larger than 1 in Table 2 and 3. This suggests that the attention scores are before softmax and hence, not normalized to sum to 1. Please feel free to correct me if I’m mistaken.

1. If this is the case, it makes the score much more difficult to interpret and compare across different attacks. The absolute unnormalized value of the attention scores does not mean much because, for example, even if the score increases for the suffix portion, it may gets smaller relative to the other portions (system or goal). This is major flaw that undermines the main conclusion of the paper.
2. If that authors have not already done so, I would like to ask that all the reported attention scores be normalized (after softmax). The autoregressive generation also contributes to the attention scores, i.e., attention score of the target token $x_{n+2}$ also includes the target token $x_{t+1}$ along with all the prompt tokens $x_{1:n}$. I’m not sure what is the best way to normalize their effect. One way is to simply leave them out of the softmax, but there could be an interesting trend that we fail to capture this way. Another way is to report *difference* between average unnormalized attention score on the goal vs on the suffix portions. This also gives us a relative score but ignores the system portion.
3. In Figure 2 (left), ASR also increases along with the attention score on the goal, contradicting the main claim of the paper that higher attention score on the suffix is better.
4. It is unclear to me how Figure 5 supports the main claim of the paper. The attention pattern on ""Vanilla"" is strikingly similar to that of  ""ICA"" on the goal segment. Based on the color bar, the ICA attention score also seems higher than the Vanilla which contradicts the claim that the attack ""diverts the model’s attention away from the goal towards themselves.”

### 2. Section 3.3: Generalize AttnGCG to other attack methods

1. The purpose of this experiment is unclear to me. If the authors wish to prove their claim that higher attention weight on the suffix leads to a better attack, there should be a better controlled experiments than running GCG or AttnGCG on prompts generated by the other methods. This experiment entangles the initialization method with the attention score.
2. It might be interesting to see AttnGCG with varying values of $w_t$ and $w_a$.
3. I’d suggest an experiment where the attention loss is incorporated into AutoDAN (or other attacks) optimization objective. This would better emphasize the transferability and the usefulness of the attention loss across multiple attack algorithms.

### 3. Limited empirical improvement

While the main idea could help improve interpretability to these adversarial attacks, the attack that is inspired by this observation, AttnGCG, does not lead to significant improvement in the attack success rate, especially in the transfer setting. In the white-box setting, the improvement seems consistent across models, but the small margin suggests that attention score is not the most important factor that determines the success of the attack.

That said, it is sufficiently convincing to me that AttnGCG performs better than GCG and may replace it for evaluating the safety of LLMs.

Limitations:
Limitations and negative societal impact have been adequately addressed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
This work proposes a new adversarial attack strategy on LLMs which improves over existing adversarial attacks. For this the authors propose a new regularizer that maximizes the weight of attention corresponding to suffix tokens, which naturally results in minimizing the weight for the other tokens present in the input prompt. Using this additional regularizer with GCG results in improved attack success rate. The authors also show that this attack is transferable to other attack methods like ICA and AutoDAN.

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
1) The paper is well motivated and the proposed loss follows well with the reader’s intuition.
2) The results are promising and the gains over the existing GCG attack are significant.
3) The comparison is comprehensive, involving different models.

Weaknesses:
1) It is not clear how the transferability of the same suffix tokens is for different goal prompts. This is important to investigate because GCG shows that the generated attacks are universal and can transfer on different goal prompts. I am currently a bit skeptical that the transferability on using the proposed attack might be limited because the generated suffix tokens might be more specialized for the given goal prompt. This is expected because now the generation of the suffix tokens is largely conditioned on the target target tokens due to the proposed regularizer. 

2) I believe it might be possible that using the proposed attack the model ends up outputting something potentially harmful but completely unrelated with the input prompt. This might be a possibility because the proposed approach inherently minimizes the attention on the goal tokens, which means the context of the input might become less relevant. It would be great if the authors could share some analysis on transferability of adv prompts and also share the generated text for GCG and AttnGCG.

3) It is not clear why maximizing the attention weights for suffix tokens should always lead to a stronger attack? This is also evident from tables 2 and 3 where AutoDAN has a lower value of goal attention score but stil leads to weaker attack as compared to GCG (see Table-4). Thus the argument presented in 162-163 seems questionable. 
In general, it is not clear why authors did not attempt to analyze the defenses like the ones proposed in [1]. Particularly, I believe it is important to analyze if the proposed attacks are able to bypass detection filters based on perplexity [1]. 

[1] Jain, Neel et al. “Baseline Defenses for Adversarial Attacks Against Aligned Language Models.” ArXiv abs/2309.00614 (2023)

Limitations:
Yes, the authors have addressed the limitations.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
The authors propose a refined GCG method named AttnGCG for Large Language Model jailbreaking attacks. They focus on the attention scores of the input components, refining the loss function by adding an Attention Loss term. The attack success rates are greatly improved. Various experiments are provided to support the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
(1)	An interesting finding is that as the attention score on the adversarial suffix increases, the effectiveness in safeguarding LLM diminishes.

(2)	The experiments are conducted on various LLMs to prove the effectiveness of the AttnGCG.

Weaknesses:
(1)	It is unclear whether the increased success cases correspond to the 'regret' cases observed in GCG. The authors proposed AttnGCG to address the issue where the model successfully generates target tokens but then rejects the request; however, the results remain ambiguous.

(2)	In the success case illustrated in Figure 4, the attention scores at the boundary between the goal and the suffix are significantly higher than in other regions. Is this a common phenomenon in success cases? If so, why does this occur?

(3)	In Appendix A.3, the table shows that the system prompt for Llama-2 and Llama-3 is set to None, which is different from most jailbreaking papers, including the original GCG. How does this influence the attacking success rate? The authors should also report the success rate under the standard system prompt.

I will reconsider my score if all these problems are adequately addressed.

Limitations:
The authors adequately discussed the limitations of this work.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a new jailbreak attack method against LLMs, called AttnGCG. The method integrates a loss of maximizing the attention scores of the adversarial suffix. The paper provides experimental results to show the effectiveness of the proposed method.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The paper is well-written and easy to follow.

Weaknesses:
My main concerns are as follows.

- Will increasing the attention scores of adversarial suffixes make the responses focus on the content in adversarial suffixes?  
- The discussion in lines 151-164 is weak. Specifically, in Figure 4, AttnGCG explicitly increases the attention scores of adversarial suffixes, so it is natural to have higher adversarial suffix attention scores. It is not convincing to say ""uncover the underlying reasons for successful attacks within the model’s attention mechanism"".  
- In Table 3, AutoDAN achieves 0.227 goal attention score, while the scores of GCG and AttnGCG are 0.8657 and 0.793. Does the observation mean that AutoDAN is better than AttnGCG?
- Some content seems to be redundant, e.g., Figure 1 and Algorithm 1.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,No,0.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposed RGD, a novel method for integrating classifier guidance into classifier-free guidance diffusion models for solving offline MBO problems. Experiment results and ablation studies validate that the method outperforms state-of-the-art baselines and each proposed component is resonable.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Idea is intuitive and easy to follow

- Motivating example in the introduction makes the reader easy to understand the limitations of the prior method and the advantages of the proposed method

- Strong experiment results and detailed ablation studies make the proposed method more convincing

Weaknesses:
- For the diffusion-based proxy refinement part, it seems that there are several estimations to compute the distance between $p_{\phi}(y\vert \hat{x})$ and $p_{\theta}(y\vert \hat{x})$. Furthermore, it incurs additional hyperparameter $\alpha$, which should be carefully tuned.

Limitations:
There are a few minor comments on the manuscript.

- For figure 2, it seems that $\tilde{s}(x_T, y, \omega)$ should be written as $\tilde{s}(x_T, y, \hat{\omega})$. Furthermore, at first, it makes me confusion that RGD conducts classifer-guidance. However, that misleading part has been resolved after reading the manuscript.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper,  the authors proposed to combine both classifier guidance and classifier-free guidance for offline black-box optimization.  In addition, the authors propose a Proxy Refinement procedure by minimizing KL divergence between the Proxy distribution and diffusion distribution regarding $y$.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.  The paper is well-written and well-organized. 



2.  The paper introduces several refinement procedures to boost the offline optimization performance.  The proposed Diffusion-based Proxy Refinement procedure is interesting.

Weaknesses:
1.  **Technical contribution seems to be incremental**

Employing diffusion models for offline black-box optimization is not new.  The technical contribution of this paper seems to be incremental. The draft extends the paper ""Diffusion Models for Black-Box Optimization"" [1]. However,   detailed discussions about the relationship between the proposed method and the paper [1] are missing.

[1] Siddarth Krishnamoorthy, Satvik Mashkaria, and Aditya Grover. ""Diffusion Models for Black-Box Optimization."" ICML 2023. 


2.  **Part of the technical details are not clear.**

(a) In Equation (12),  the concrete computation procedure of $p_\theta (\hat{\boldsymbol{x}} | y)$ and $p_\theta (\hat{\boldsymbol{x}})$  via diffusion model is not clear. 

(b) The derivation of Equation (10) is not given.   It seems that Equation (10) is from the forward pass of the diffusion model.  However,  the forward pass  (Eq.32-32 in [10]) is regarding the distribution.  And the concrete  $\boldsymbol{x} _ t $ is constructed via the backward pass with  $s_\theta(\boldsymbol{x}_k,k)$ for $k \in T,\cdots, t+1$. 
 In addition,  how to choose $\mu(t)$ and $\sigma(t)$ in Equation (10)  is not clear.  

3.  **The additional proxy training, sample refinement procedure and proxy refinement procedure  increase the computation cost**

The additional proxy training, sample refinement procedure and proxy refinement procedure increase the computation cost. However, the time comparison with baselines is missing. 

4.  **The additional proxy training, sample refinement procedure and proxy refinement procedure bring many additional hyperparameters, which may overfit the offline BBO task**

In the offline BBO tasks,  the offline dataset is provided.  The evaluation is the black-box function value at the generated query at one time. 
The long-term convergence properties and exploration/exploitation balance are not considered.  As a result, there are risks that overfit the evaluation metric for the offline tasks.  The paper Introduces lots of additional hyperparameters, which increases the overfitting risks.

Limitations:
Additional computation cost and overfitting risk may be additional limitations besides the limitations discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a framework called Robust Guided Diffusion for the problem of Offline Black-box Optimization. The key idea is to formulate the solution as conditional generation of high-performance designs using a diffusion model which has explicit guidance from a proxy (surrogate) model. This proxy model is also refined/updated via a proxy-free diffusion procedure. Experimental analysis is shown on multiple tasks from design-bench benchmark.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Overall, I like the paper because it includes two simple changes to an existing approach (DDOM) that shows improved performance and the changes are validated by ablation choices.

Weaknesses:
- One major premise (repeated multiple times in the paper) in the paper is that proxy guidance conditional generation is more robust than updating the design with standard gradient ascent on the proxy. However, it is not immediately clear why this should be true and the justification for this key point is somewhat limited. If true, this will be much bigger insight going beyond black-box optimization. If it is only about the exploration/exploitation balance driven by w, we could also make standard gradient have this property by optimizing a upper/lower confidence bound on the objective. Please describe why this is the case either via some empirical experiment or theoretical insight. Also, in equation 11, we might evaluate the proxy far away from the training data depending on the values of s_\theta(x_t), \sigma(t), \mu(t).

- The related work coverage and corresponding experimental analysis of the paper can be improved. This problem has seen an extensive body of work recently. Please see the references below and discuss/compare them appropriately. Some of them are included in references but not compared in the experiments ([1], [2], [3]):

- [1] Yuan, Ye, et al. ""Importance-aware co-teaching for offline model-based optimization."" Advances in Neural Information Processing Systems 36 (2023).
- [2] Kim, Minsu, et al. ""Bootstrapped training of score-conditioned generator for offline design of biological sequences."" Advances in Neural Information Processing Systems 36 (2023).
- [3] Nguyen, Tung, Sudhanshu Agrawal, and Aditya Grover. ""ExPT: Synthetic pretraining for few-shot experimental design."" Advances in Neural Information Processing Systems 36 (2023).
- [4] Chemingui, Yassine, et al. ""Offline model-based optimization via policy-guided gradient search."" *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 38. No. 10. 2024.
- [5] Yao, Michael S., et al. ""Generative Adversarial Bayesian Optimization for Surrogate Objectives."" arXiv preprint arXiv:2402.06532 (2024).

Limitations:
Please see weaknesses section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a robust guided diffusion framework for offline black-box optimization, combining proxy and proxy-free diffusion for conditional generation. Key improvements include proxy-enhanced sampling and diffusion-based proxy refinement to address out-of-distribution issues. Experiments on the Design-Bench benchmark show the method outperforms existing techniques, validated by ablation studies.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The regularization of the proxy using the diffusion model is interesting. Additionally, optimizing the alpha parameter in an offline manner aligns well with the offline setup, enhancing the method's consistency and applicability.
- Experiments and ablations on four continuous and three discrete tasks validate the effectiveness of the proposed RGD method, showing improved performance and robustness.

Weaknesses:
- The paper lacks comparison with relevant approaches like ICT [1] and TRI-mentoring [2]. Despite referencing the latter in the related work section, it’s overlooked in the results.
- It is unclear why the results without proxy-enhanced sampling still achieve competitive outcomes, surpassing the dataset y_max. This contradicts the claims in lines 40-46. Where does the out-of-distribution (OOD) problem arise then? What is the distribution of the generated 128 candidates with and without the sampling? 
- The BDI reported results are significantly lower than in the original paper, especially for the ANT and TFBIND8 tasks. This also seems to be the case for BONET results. Did the authors change the evaluation setup?


[1]: Importance-aware Co-teaching for Offline Model-based Optimization, https://arxiv.org/abs/2309.11600

[2]: Parallel-mentoring for Offline Model-based Optimization, https://arxiv.org/abs/2309.11592

Limitations:
The authors address the limitations and potential negative impacts in their paper.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new method, named RGD, for Offline Black-box Optimization (BBO). RGD incorporates an improved proxy to guide the previous proxy-free method (i.e. DDOM[4]). Key technical innovations includes (1) improving the robustness of the proxy function against adversarial samples by consistency regularization with the diffusion process; (2) dynamic per-sample reweighting between proxy-guided and proxy-free sampling. Compared to previous approaches, RGD demonstrates superior performance on Design-Bench [3].

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Methodology: RGD integrates forward and reverse approaches for BBO, in a way that they can help with each other (e.g. using forward proxy to guide the reverse sampling and using the diffusion process to improve the forward proxy), which is technically sound and interesting.

Experiment: RGD demonstrates superior performance on Design-Bench, compared to the baselines.

Ablation: Ablations on different components of RGD are provided.

Weaknesses:
The reviewer would prefer some clarifications on the method and the experiments

i) Algorithm 1, Line 4, how to identify the adversarial examples? From Line 187-188, it looks like gradient ascent is utilized to find the x that maximize y, it is unclear to the reviewer that how to determine if the obtained x is an adversarial example

ii) Algorithm 1, Line 7, refine the proxy function via eq 15. It would be best if the author could provide further details on how to optimize eq (15), e.g. number of validation and adversarial samples, number of iterations for the bi-level optimization discussed in Appendix B.

iii) Algorithm 1 Line 13, optimizing \omega. Again, it would be best if the author could provide extra info on how to optimize \omega. From Algorithm 1, it looks like \omega is time dependent and optimized for each time step. How many training iterations are required for each time step. The reviewer also wonder if the obtained \omega are dramatically different between different time steps. 

iv) From Line 257-258, it looks like the baselines shown in Table 1 & 2 were re-implemented. If this is the case, the authors are encouraged to include more implementation details, e.g. the model architecture for the score function, etc. This could help follow-up works to reproduce the reported results. The reviewer also wonders if the source code will be made public.

Limitations:
Limitations have been discussed in the appendix

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper proposed RGD, a novel method for integrating classifier guidance into classifier-free guidance diffusion models for solving offline MBO problems. Experiment results and ablation studies validate that the method outperforms state-of-the-art baselines and each proposed component is resonable.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
- Idea is intuitive and easy to follow

- Motivating example in the introduction makes the reader easy to understand the limitations of the prior method and the advantages of the proposed method

- Strong experiment results and detailed ablation studies make the proposed method more convincing

Weaknesses:
- For the diffusion-based proxy refinement part, it seems that there are several estimations to compute the distance between $p_{\phi}(y\vert \hat{x})$ and $p_{\theta}(y\vert \hat{x})$. Furthermore, it incurs additional hyperparameter $\alpha$, which should be carefully tuned.

Limitations:
There are a few minor comments on the manuscript.

- For figure 2, it seems that $\tilde{s}(x_T, y, \omega)$ should be written as $\tilde{s}(x_T, y, \hat{\omega})$. Furthermore, at first, it makes me confusion that RGD conducts classifer-guidance. However, that misleading part has been resolved after reading the manuscript.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
In this paper,  the authors proposed to combine both classifier guidance and classifier-free guidance for offline black-box optimization.  In addition, the authors propose a Proxy Refinement procedure by minimizing KL divergence between the Proxy distribution and diffusion distribution regarding $y$.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
1.  The paper is well-written and well-organized. 



2.  The paper introduces several refinement procedures to boost the offline optimization performance.  The proposed Diffusion-based Proxy Refinement procedure is interesting.

Weaknesses:
1.  **Technical contribution seems to be incremental**

Employing diffusion models for offline black-box optimization is not new.  The technical contribution of this paper seems to be incremental. The draft extends the paper ""Diffusion Models for Black-Box Optimization"" [1]. However,   detailed discussions about the relationship between the proposed method and the paper [1] are missing.

[1] Siddarth Krishnamoorthy, Satvik Mashkaria, and Aditya Grover. ""Diffusion Models for Black-Box Optimization."" ICML 2023. 


2.  **Part of the technical details are not clear.**

(a) In Equation (12),  the concrete computation procedure of $p_\theta (\hat{\boldsymbol{x}} | y)$ and $p_\theta (\hat{\boldsymbol{x}})$  via diffusion model is not clear. 

(b) The derivation of Equation (10) is not given.   It seems that Equation (10) is from the forward pass of the diffusion model.  However,  the forward pass  (Eq.32-32 in [10]) is regarding the distribution.  And the concrete  $\boldsymbol{x} _ t $ is constructed via the backward pass with  $s_\theta(\boldsymbol{x}_k,k)$ for $k \in T,\cdots, t+1$. 
 In addition,  how to choose $\mu(t)$ and $\sigma(t)$ in Equation (10)  is not clear.  

3.  **The additional proxy training, sample refinement procedure and proxy refinement procedure  increase the computation cost**

The additional proxy training, sample refinement procedure and proxy refinement procedure increase the computation cost. However, the time comparison with baselines is missing. 

4.  **The additional proxy training, sample refinement procedure and proxy refinement procedure bring many additional hyperparameters, which may overfit the offline BBO task**

In the offline BBO tasks,  the offline dataset is provided.  The evaluation is the black-box function value at the generated query at one time. 
The long-term convergence properties and exploration/exploitation balance are not considered.  As a result, there are risks that overfit the evaluation metric for the offline tasks.  The paper Introduces lots of additional hyperparameters, which increases the overfitting risks.

Limitations:
Additional computation cost and overfitting risk may be additional limitations besides the limitations discussed.

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes a framework called Robust Guided Diffusion for the problem of Offline Black-box Optimization. The key idea is to formulate the solution as conditional generation of high-performance designs using a diffusion model which has explicit guidance from a proxy (surrogate) model. This proxy model is also refined/updated via a proxy-free diffusion procedure. Experimental analysis is shown on multiple tasks from design-bench benchmark.

Soundness:
2: fair

Presentation:
2: fair

Contribution:
2: fair

Strengths:
- Overall, I like the paper because it includes two simple changes to an existing approach (DDOM) that shows improved performance and the changes are validated by ablation choices.

Weaknesses:
- One major premise (repeated multiple times in the paper) in the paper is that proxy guidance conditional generation is more robust than updating the design with standard gradient ascent on the proxy. However, it is not immediately clear why this should be true and the justification for this key point is somewhat limited. If true, this will be much bigger insight going beyond black-box optimization. If it is only about the exploration/exploitation balance driven by w, we could also make standard gradient have this property by optimizing a upper/lower confidence bound on the objective. Please describe why this is the case either via some empirical experiment or theoretical insight. Also, in equation 11, we might evaluate the proxy far away from the training data depending on the values of s_\theta(x_t), \sigma(t), \mu(t).

- The related work coverage and corresponding experimental analysis of the paper can be improved. This problem has seen an extensive body of work recently. Please see the references below and discuss/compare them appropriately. Some of them are included in references but not compared in the experiments ([1], [2], [3]):

- [1] Yuan, Ye, et al. ""Importance-aware co-teaching for offline model-based optimization."" Advances in Neural Information Processing Systems 36 (2023).
- [2] Kim, Minsu, et al. ""Bootstrapped training of score-conditioned generator for offline design of biological sequences."" Advances in Neural Information Processing Systems 36 (2023).
- [3] Nguyen, Tung, Sudhanshu Agrawal, and Aditya Grover. ""ExPT: Synthetic pretraining for few-shot experimental design."" Advances in Neural Information Processing Systems 36 (2023).
- [4] Chemingui, Yassine, et al. ""Offline model-based optimization via policy-guided gradient search."" *Proceedings of the AAAI Conference on Artificial Intelligence*. Vol. 38. No. 10. 2024.
- [5] Yao, Michael S., et al. ""Generative Adversarial Bayesian Optimization for Surrogate Objectives."" arXiv preprint arXiv:2402.06532 (2024).

Limitations:
Please see weaknesses section.

Rating:
6: marginally above the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a robust guided diffusion framework for offline black-box optimization, combining proxy and proxy-free diffusion for conditional generation. Key improvements include proxy-enhanced sampling and diffusion-based proxy refinement to address out-of-distribution issues. Experiments on the Design-Bench benchmark show the method outperforms existing techniques, validated by ablation studies.

Soundness:
2: fair

Presentation:
3: good

Contribution:
2: fair

Strengths:
- The regularization of the proxy using the diffusion model is interesting. Additionally, optimizing the alpha parameter in an offline manner aligns well with the offline setup, enhancing the method's consistency and applicability.
- Experiments and ablations on four continuous and three discrete tasks validate the effectiveness of the proposed RGD method, showing improved performance and robustness.

Weaknesses:
- The paper lacks comparison with relevant approaches like ICT [1] and TRI-mentoring [2]. Despite referencing the latter in the related work section, it’s overlooked in the results.
- It is unclear why the results without proxy-enhanced sampling still achieve competitive outcomes, surpassing the dataset y_max. This contradicts the claims in lines 40-46. Where does the out-of-distribution (OOD) problem arise then? What is the distribution of the generated 128 candidates with and without the sampling? 
- The BDI reported results are significantly lower than in the original paper, especially for the ANT and TFBIND8 tasks. This also seems to be the case for BONET results. Did the authors change the evaluation setup?


[1]: Importance-aware Co-teaching for Offline Model-based Optimization, https://arxiv.org/abs/2309.11600

[2]: Parallel-mentoring for Offline Model-based Optimization, https://arxiv.org/abs/2309.11592

Limitations:
The authors address the limitations and potential negative impacts in their paper.

Rating:
3: reject, not good enough

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a new method, named RGD, for Offline Black-box Optimization (BBO). RGD incorporates an improved proxy to guide the previous proxy-free method (i.e. DDOM[4]). Key technical innovations includes (1) improving the robustness of the proxy function against adversarial samples by consistency regularization with the diffusion process; (2) dynamic per-sample reweighting between proxy-guided and proxy-free sampling. Compared to previous approaches, RGD demonstrates superior performance on Design-Bench [3].

Soundness:
3: good

Presentation:
3: good

Contribution:
2: fair

Strengths:
Methodology: RGD integrates forward and reverse approaches for BBO, in a way that they can help with each other (e.g. using forward proxy to guide the reverse sampling and using the diffusion process to improve the forward proxy), which is technically sound and interesting.

Experiment: RGD demonstrates superior performance on Design-Bench, compared to the baselines.

Ablation: Ablations on different components of RGD are provided.

Weaknesses:
The reviewer would prefer some clarifications on the method and the experiments

i) Algorithm 1, Line 4, how to identify the adversarial examples? From Line 187-188, it looks like gradient ascent is utilized to find the x that maximize y, it is unclear to the reviewer that how to determine if the obtained x is an adversarial example

ii) Algorithm 1, Line 7, refine the proxy function via eq 15. It would be best if the author could provide further details on how to optimize eq (15), e.g. number of validation and adversarial samples, number of iterations for the bi-level optimization discussed in Appendix B.

iii) Algorithm 1 Line 13, optimizing \omega. Again, it would be best if the author could provide extra info on how to optimize \omega. From Algorithm 1, it looks like \omega is time dependent and optimized for each time step. How many training iterations are required for each time step. The reviewer also wonder if the obtained \omega are dramatically different between different time steps. 

iv) From Line 257-258, it looks like the baselines shown in Table 1 & 2 were re-implemented. If this is the case, the authors are encouraged to include more implementation details, e.g. the model architecture for the score function, etc. This could help follow-up works to reproduce the reported results. The reviewer also wonders if the source code will be made public.

Limitations:
Limitations have been discussed in the appendix

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.


Label: ",Yes,yes,No,0.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper investigates a multi-armed bandit problem where the action space is a metric space a stochastic Lipschitz rewards. The authors present algorithms that use a constant amount of memory and achieve a near-optimal regret. This improves on previous results that had heavy memory usage.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper has good presentation, and the figures in the appendices are helpfulץ The contribution itself is useful in practice.

Weaknesses:
While the result is great, the ideas presented in the paper are modifications of existing methods

Limitations:
I did not find the limitations presented as sufficient and I would like to see more discussion on the downsides of the presented algorithm and future directions of research.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the Lipschitz bandit problem with a memory constraint. There are two algorithms proposed by the authors. The Memory Bounded Uniform Discretization (MBUD) algorithm uses a fixed discretization over the metric space and implements a strategy which explores first and then commits to an exploitation phase. The second algorithm, called Memory Bounded Adaptive Discretization (MBAD) , swaps arms in and out of the memory while creating a mesh over the metric space adaptively (ala zooming). The authors prove upper bounds which match lower bounds from previous work for Lipschitz bandits without memory constraints while maintaining linear time complexity and constant space complexity. Finally, the authors perform experimental validation of the theoretical results on small 1-dimensional datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Novel problem formulation in the Lipschitz bandit setting.
2. The authors show upper bounds matching with lower bounds from prior work while maintaining a memory budget on arms.
3. The concepts introduced in the paper are well explained for the most part. I did have a little trouble reading the parts about the crosscut and generating cubes but it might just be me not being familiar with prior work in the area.

Weaknesses:
I am unclear about the novelty and contributions of the paper. The problem formulation (limited memory) is new in the Lipschitz bandit setting but it has been studied in several papers in bandits with finite arms (as the authors point out in the related works). Moreover, the proof techniques used in the paper appear standard - MBAD is based on zooming introduced by kleinberg et al., the clean event analysis is from the recent textbook of Slivkins (and their papers), MBUD is based on an explore first strategy resembling the naive Explore-then-Commit algorithm (which trivially satisfies the O(1) memory constraint). In all, I’m not sure what specific parts of the paper are being claimed as novel vs that from prior work.

The experiments in this paper are very limited - only a 1 dimensional interval with an L1 metric. To show real world applicability, it would be nice to have results in higher dimensions and also on real world datasets (since that was the original motivation).

Minor/Typo:
I think the caption for Fig 1 should clarify what is on the X and Y axes. It is obvious from context but it would be nice to have from a readability perspective.

Limitations:
The Lipschitz constant needs to be known beforehand to apply these algorithms.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes two algorithms for Lipschitz bandit problems, with improved time complexity and memory requirements.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
4: excellent

Strengths:
If the algorithms and proofs are sound, then this is an excellent contribution. Developing these sort of streaming/sketching methods for key bandit problems (such as Lipschitz bandits) is an important area of research, and many people are likely to care about the results of this paper.

Weaknesses:
The paper is sloppy to the extent that it is difficult to understand the authors' algorithm or verify their claims. To be specific, consider the following sentences, all taken from a single two-paragraph subsection (section 2.2):
1. ""Let $\\{\mathcal{X}_1, \dotsc, \mathcal{X}_N\\}[\mathcal{X}_i \subset \mathcal{X}]$ be an cover of the action space $\mathcal{X}$"" --- okay, what is the $\\{\dotsc\\}[\dotsc]$ notation?  
2. ""Let $\epsilon$ denote the maximum diameter of $\mathcal{X}_i$ for all $i \in [N]$."" --- okay, but what is a diameter? Are we in a metric space? This hasn't been specified. 
3. ""Then the arm set $S = \\{ x_i \mid x_i \in \mathcal{X}_i, i \in [N]\\}$ is an $\epsilon$-mesh.""  --- what set is this? Now, I presume that the authors mean to say that they want $S$ to be any set that contains a single element chosen arbitrarily from each of the $\mathcal{X}_i$, but that's not written, instead the authors said its __the__ set, but the right hand side does not specify any unique set. Also, the concept of an $\epsilon$-mesh has not been defined, and when its defined, it needs to be with respect to some metric. And if this description was meant to be the definition of an $\epsilon$-mesh... then that's not clear either (and the definition given by the work the authors state these definitions are from, i.e. Slivkins 2019, is _very_ clear---all the authors needed to do was copy it).
4. ""The covering dimension $d$ of the action space $\mathcal{X}$ is defined as $d=\inf_{\alpha \geq 0}\\{|\mathcal{S}| \leq \epsilon^{-\alpha}, \forall \epsilon > 0\\}$."" But the set $\mathcal{S}$ does not depend on $\epsilon$ (not even implicitly)... (the correct definition, I presume, would be to ask that $\mathcal{S}\_{\epsilon}$ is a minimal $\epsilon$-cover of $\mathcal{X}$ in some metric $D$, and then have that infimum include $\mathcal{S}_\epsilon$ and not $\mathcal{S}$.)
5. ""Define $\mathcal{Y}\_j = \\{x \in \mathcal{X} \colon 2^{-j} \leq \Delta(x) \leq 2^{1-j}, j \in \mathbb{N}\\}$, then the set $\mathcal{Y}_j$ contains all arms whose gap is between $2^{-j}$ and $2^{1-j}$."" --- but $j \in \mathbb{N}$ is within the constructions of the set on the RHS, which could be read as asking that the condition holds for all such $j$, or for some $j$, but it breaks the dependence of the right hand side on the subscript $j$ of $\mathcal{Y}_j$. Of course, the definition shouldn't have the $j \in \mathbb{N}$ inside the $\\{ \dotsc \\}$ on the right hand side of the definition. 
6. ""Consider the $\epsilon$-mesh $\mathcal{S}_j$ for space $\mathcal{Y}_j$."" --- __the__ $\epsilon$-mesh? Also, $\mathcal{S}_j$ hasn't been defined. This should say instead 'fix some $\epsilon > 0$ and let $\mathcal{S}_j$ be an $\epsilon$-mesh of $\mathcal{Y}_j$', or something like that.
7. ""[...] the zooming dimension focuses only on the set $\mathcal{Y}_j$"" --- no, the zooming dimension depends on all the sets $\mathcal{Y}_1, \mathcal{Y}_2, \dotsc$, not only a single one of those sets.

While each individual mistake or ambiguity can be resolved easily enough, verifying the authors claims would require me to rewrite everything myself, and this goes beyond what I'm willing to do (and should do...).  The whole paper is like this, and it's just not acceptable.

I would urge the authors to, in the future, have someone _not intimately familiar with the work_ proof-read the work.

Note, I put down confidence as ""5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."" --- I am indeed very familiar with the related work, but I have not checked the math/other details carefully. It's too much work to read it. I am absolutely certain, however, that this level falls short of any level of clarity that might be expected in published work.

Limitations:
.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper considers regret minimization for Lipschitz bandits with time horizont $T$ and proposes an algorithm that provably achieves nearly optimal regret while having strictly smaller (by a factor of $T$) time (of order $O(T)$) and memory complexity (of order $O(1)$). This is achieved by considering a tree-like embedding of the state space and pairwise comparison between elements of the tree. A suboptimal method with uniform discretization called MBUD has dependence on the covering dimension of the state space, while MBAD, a method with adaptive discretization, instead has dependence only on the zooming dimension.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) Achieving nearly optimal regret bounds in minimax (MBUD) and instance-specific setting (MBAD), while reducing time and memory complexity. 

2) Both proposed algorithms are non-trivial and seem to be novel and interesting on their own.

Weaknesses:
1) It is not simple to parse algorithms in their current form in a short amount of time. Although you give comprehensive descriptions in text, I believe adding illustrations or additional explanations will significantly improve clarity of your algorithms. 

2) I would appreciate a more explicit comparison with previous work - what parts of the algorithms were already reported in the literature?

Limitations:
The authors have addressed limitations adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
The paper investigates a multi-armed bandit problem where the action space is a metric space a stochastic Lipschitz rewards. The authors present algorithms that use a constant amount of memory and achieve a near-optimal regret. This improves on previous results that had heavy memory usage.

Soundness:
4: excellent

Presentation:
3: good

Contribution:
3: good

Strengths:
The paper has good presentation, and the figures in the appendices are helpfulץ The contribution itself is useful in practice.

Weaknesses:
While the result is great, the ideas presented in the paper are modifications of existing methods

Limitations:
I did not find the limitations presented as sufficient and I would like to see more discussion on the downsides of the presented algorithm and future directions of research.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies the Lipschitz bandit problem with a memory constraint. There are two algorithms proposed by the authors. The Memory Bounded Uniform Discretization (MBUD) algorithm uses a fixed discretization over the metric space and implements a strategy which explores first and then commits to an exploitation phase. The second algorithm, called Memory Bounded Adaptive Discretization (MBAD) , swaps arms in and out of the memory while creating a mesh over the metric space adaptively (ala zooming). The authors prove upper bounds which match lower bounds from previous work for Lipschitz bandits without memory constraints while maintaining linear time complexity and constant space complexity. Finally, the authors perform experimental validation of the theoretical results on small 1-dimensional datasets.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1. Novel problem formulation in the Lipschitz bandit setting.
2. The authors show upper bounds matching with lower bounds from prior work while maintaining a memory budget on arms.
3. The concepts introduced in the paper are well explained for the most part. I did have a little trouble reading the parts about the crosscut and generating cubes but it might just be me not being familiar with prior work in the area.

Weaknesses:
I am unclear about the novelty and contributions of the paper. The problem formulation (limited memory) is new in the Lipschitz bandit setting but it has been studied in several papers in bandits with finite arms (as the authors point out in the related works). Moreover, the proof techniques used in the paper appear standard - MBAD is based on zooming introduced by kleinberg et al., the clean event analysis is from the recent textbook of Slivkins (and their papers), MBUD is based on an explore first strategy resembling the naive Explore-then-Commit algorithm (which trivially satisfies the O(1) memory constraint). In all, I’m not sure what specific parts of the paper are being claimed as novel vs that from prior work.

The experiments in this paper are very limited - only a 1 dimensional interval with an L1 metric. To show real world applicability, it would be nice to have results in higher dimensions and also on real world datasets (since that was the original motivation).

Minor/Typo:
I think the caption for Fig 1 should clarify what is on the X and Y axes. It is obvious from context but it would be nice to have from a readability perspective.

Limitations:
The Lipschitz constant needs to be known beforehand to apply these algorithms.

Rating:
4: Borderline reject: Technically solid paper where reasons to reject, e.g., limited evaluation, outweigh reasons to accept, e.g., good evaluation. Please use sparingly.

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper proposes two algorithms for Lipschitz bandit problems, with improved time complexity and memory requirements.

Soundness:
1: poor

Presentation:
1: poor

Contribution:
4: excellent

Strengths:
If the algorithms and proofs are sound, then this is an excellent contribution. Developing these sort of streaming/sketching methods for key bandit problems (such as Lipschitz bandits) is an important area of research, and many people are likely to care about the results of this paper.

Weaknesses:
The paper is sloppy to the extent that it is difficult to understand the authors' algorithm or verify their claims. To be specific, consider the following sentences, all taken from a single two-paragraph subsection (section 2.2):
1. ""Let $\\{\mathcal{X}_1, \dotsc, \mathcal{X}_N\\}[\mathcal{X}_i \subset \mathcal{X}]$ be an cover of the action space $\mathcal{X}$"" --- okay, what is the $\\{\dotsc\\}[\dotsc]$ notation?  
2. ""Let $\epsilon$ denote the maximum diameter of $\mathcal{X}_i$ for all $i \in [N]$."" --- okay, but what is a diameter? Are we in a metric space? This hasn't been specified. 
3. ""Then the arm set $S = \\{ x_i \mid x_i \in \mathcal{X}_i, i \in [N]\\}$ is an $\epsilon$-mesh.""  --- what set is this? Now, I presume that the authors mean to say that they want $S$ to be any set that contains a single element chosen arbitrarily from each of the $\mathcal{X}_i$, but that's not written, instead the authors said its __the__ set, but the right hand side does not specify any unique set. Also, the concept of an $\epsilon$-mesh has not been defined, and when its defined, it needs to be with respect to some metric. And if this description was meant to be the definition of an $\epsilon$-mesh... then that's not clear either (and the definition given by the work the authors state these definitions are from, i.e. Slivkins 2019, is _very_ clear---all the authors needed to do was copy it).
4. ""The covering dimension $d$ of the action space $\mathcal{X}$ is defined as $d=\inf_{\alpha \geq 0}\\{|\mathcal{S}| \leq \epsilon^{-\alpha}, \forall \epsilon > 0\\}$."" But the set $\mathcal{S}$ does not depend on $\epsilon$ (not even implicitly)... (the correct definition, I presume, would be to ask that $\mathcal{S}\_{\epsilon}$ is a minimal $\epsilon$-cover of $\mathcal{X}$ in some metric $D$, and then have that infimum include $\mathcal{S}_\epsilon$ and not $\mathcal{S}$.)
5. ""Define $\mathcal{Y}\_j = \\{x \in \mathcal{X} \colon 2^{-j} \leq \Delta(x) \leq 2^{1-j}, j \in \mathbb{N}\\}$, then the set $\mathcal{Y}_j$ contains all arms whose gap is between $2^{-j}$ and $2^{1-j}$."" --- but $j \in \mathbb{N}$ is within the constructions of the set on the RHS, which could be read as asking that the condition holds for all such $j$, or for some $j$, but it breaks the dependence of the right hand side on the subscript $j$ of $\mathcal{Y}_j$. Of course, the definition shouldn't have the $j \in \mathbb{N}$ inside the $\\{ \dotsc \\}$ on the right hand side of the definition. 
6. ""Consider the $\epsilon$-mesh $\mathcal{S}_j$ for space $\mathcal{Y}_j$."" --- __the__ $\epsilon$-mesh? Also, $\mathcal{S}_j$ hasn't been defined. This should say instead 'fix some $\epsilon > 0$ and let $\mathcal{S}_j$ be an $\epsilon$-mesh of $\mathcal{Y}_j$', or something like that.
7. ""[...] the zooming dimension focuses only on the set $\mathcal{Y}_j$"" --- no, the zooming dimension depends on all the sets $\mathcal{Y}_1, \mathcal{Y}_2, \dotsc$, not only a single one of those sets.

While each individual mistake or ambiguity can be resolved easily enough, verifying the authors claims would require me to rewrite everything myself, and this goes beyond what I'm willing to do (and should do...).  The whole paper is like this, and it's just not acceptable.

I would urge the authors to, in the future, have someone _not intimately familiar with the work_ proof-read the work.

Note, I put down confidence as ""5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully."" --- I am indeed very familiar with the related work, but I have not checked the math/other details carefully. It's too much work to read it. I am absolutely certain, however, that this level falls short of any level of clarity that might be expected in published work.

Limitations:
.

Rating:
2: Strong Reject: For instance, a paper with major technical flaws, and/or poor evaluation, limited impact, poor reproducibility and mostly unaddressed ethical considerations.

Confidence:
5: You are absolutely certain about your assessment. You are very familiar with the related work and checked the math/other details carefully.

REVIEW 
Summary:
The paper considers regret minimization for Lipschitz bandits with time horizont $T$ and proposes an algorithm that provably achieves nearly optimal regret while having strictly smaller (by a factor of $T$) time (of order $O(T)$) and memory complexity (of order $O(1)$). This is achieved by considering a tree-like embedding of the state space and pairwise comparison between elements of the tree. A suboptimal method with uniform discretization called MBUD has dependence on the covering dimension of the state space, while MBAD, a method with adaptive discretization, instead has dependence only on the zooming dimension.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
1) Achieving nearly optimal regret bounds in minimax (MBUD) and instance-specific setting (MBAD), while reducing time and memory complexity. 

2) Both proposed algorithms are non-trivial and seem to be novel and interesting on their own.

Weaknesses:
1) It is not simple to parse algorithms in their current form in a short amount of time. Although you give comprehensive descriptions in text, I believe adding illustrations or additional explanations will significantly improve clarity of your algorithms. 

2) I would appreciate a more explicit comparison with previous work - what parts of the algorithms were already reported in the literature?

Limitations:
The authors have addressed limitations adequately.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,No,0.0,0.83
"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper studies last-iterate convergence rates of online learning in monotone games. The main contribution is an algorithm called Gradient Ascent with Boosting Payoff Perturbation (GABP). The GABP algorithm achieves (1) $O(\log T / T)$ last-iterate convergence with full gradient feedback, which is near-optimal; (2) and $O(1/T^{1/7})$ last-iterate convergence with noisy gradient feedback (the noise is zero-mean with bounded variance). The latter result improves prior results of $O(1/T^{1/10})$. Moreover, the GABP algorithm guarantees an individual dynamic regret of $O(\log^2 T)$ under full gradient feedback, slightly worse than the state-of-the-art bound of $O(\log T)$. This paper also contains numerical experiments on small game instances to demonstrate the effectiveness of GABP.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The problem of last-iterate convergence rates of no-regret learning algorithms in monotone games is relevant and interesting. Most existing results focus on the full gradient feedback, while only a few provide concrete convergence rates under the noisy gradient or the bandit feedback. The proposed GABP algorithm has near-optimal $O(\log T / T)$ last-iterate convergence rate under full gradient feedback. It also improves the convergence rates under noisy gradient feedback from $O(1/T^{1/10})$ to $O(1/T^{1/7})$. This is a solid contribution to learning in games, although the rate for the noisy gradient feedback setting may not be tight.

Weaknesses:
1. The proposed GABP algorithm does not achieve the optimal $O(1/T)$ last-iterate convergence rate under full gradient feedback. The $O(1/T^{1/7})$ last-iterate convergence rate is also not tight for the noisy feedback.
2. The relationship between the proposed GABP algorithm and the AOG algorithm in [1] and the intuition behind the fast last-iterate convergence rates is not clearly discussed. These two algorithms are different (as shown in Appendix F) but share similar ideas. The anchoring term in both algorithms comes from the (implicit) Halpern iteration algorithm, which can not be run directly. The difference is that GABP views each step of Halpern iteration as a fixed point problem (Line 170) and uses an inner loop of $\log (1/\epsilon)$ steps to get an $\epsilon$-approximation (this is called updating the reference strategy in the paper.); In contrast, AOG directly uses optimism to approximate the implicit update. This leads to GABP being a log factor slower than AOG in the full gradient setting. However, the approximating the fixed point approach is more robust in the noisy gradient setting due to strong monotonicity. Moreover, the potential function and the approximately non-increasing potential analysis are very similar to that used in [1]. If they are inspired by [1] then this should be acknowledged. 

[1] Doubly Optimal No-Regret Learning in Monotone Games, Cai and Zheng, ICML 2023.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel algorithmic approach to enhance the convergence of first-order methods in the context of monotone games. The authors propose a payoff perturbation technique that introduces strong convexity to players' payoff functions, which is crucial for achieving last-iterate convergence. This technique is particularly designed to handle scenarios where the gradient of the payoff functions is monotone and potentially noisy. The paper presents a method called Gradient Ascent with Boosting Payoff Perturbation (GABP), which incorporates a unique perturbation into the payoff function and maintains a periodically re-initializing anchoring strategy. The authors demonstrate that GABP offers faster last-iterate convergence rates compared to existing algorithms, even in the presence of additive noise.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper presents a unique perturbation technique that addresses the challenge of last-iterate convergence in monotone games. The proposed GABP algorithm is an innovative modification of Adaptively Perturbed Mirror Descent (APMD), offering improved convergence rates.

Quality: The theoretical development is thorough, with rigorous proofs provided for the convergence rates of GABP in both full and noisy feedback settings. The paper also includes a detailed analysis of the algorithm's performance in terms of individual regret.

Clarity: The paper is well-organized, with clear explanations of the algorithm, theoretical results, and experimental setup. The use of pseudo-code for GABP aids in understanding the algorithm's implementation.

Significance: The work contributes to the field of online learning in games, providing a solution that is particularly relevant for applications such as Generative Adversarial Networks (GANs) and large language model fine-tuning, where last-iterate convergence is desirable.

Weaknesses:
Experimental Validation: While the paper provides empirical results, the experiments could be expanded to include a broader range of game types and noise levels to further validate the robustness and generalizability of GABP.

Comparison with State-of-the-Art: The paper compares GABP with APMD and Optimistic Gradient Ascent (OGA) but could benefit from a more comprehensive comparison with other existing methods in the literature to better situate its contributions.

Practical Considerations: While the paper addresses the theoretical aspects of GABP, it could provide more insights into practical considerations, such as the implementation challenges and potential modifications needed for real-world applications.

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work focuses on last-iterate convergence of  game dynamics. A payoff perturbation technique is proposed by adding strong convexity to players' payoff functions. Despite it is a well studied technqiue in learning in repeated games with first-order methods, especially in last-iterate convergence, a novel perturbation scheme introduced in this paper allows on to provide faster last-iterate convergence compared to previous works.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper provides a relatively complete result containing last-iterate convergence rate of the proposed algorithm GABP in full feedback and noisy feedback. The faster rate of convergence is an improvement compared to existing works. Except for some weakness (will be stated later), the presentation of this paper is clear to understand. The authors have reviewed most related works to my best knowledge, so that the contributions claimed are easy to follow. Addition to theoretical works, this paper has provided experiments (sufficient in my opinion) showing the comparison of GABP and existing algorithms such as Adaptively perturbed gradient ascent and Optimistic gradient ascent.

Weaknesses:
One obvious spot that should be added to improve the presentation is the following. The game considered in this paper is motivated by real-life examples. But the authors only give one example motivating monotone games. Part of contributions of the paper is claimed to be the study of two feedback models: full feedback and noisy feedback, but there is not specific examples and applications illustrating the importance of these settings. For sure readers can always find related works even just by googling the keywords, but providing concrete application scenes where the gradient of payoff can be achieved perfectly or only partially achievable gradients can be obtained is important, especially ""noisy feedback"" can be just a model of many cases.

Limitations:
This is theory based paper, no potential negative impact will cause.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies first order methods to solve monotone games where the gradient of the payoff function is monotone in the strategy, along with additive noise. The authors introduce a payoff perturbation technique which introduces strong convexity to the to the payoff functions and thereby derive last iterate convergence rates.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall the paper is well written and the method and results are interesting.

Weaknesses:
The authors should include a table which compares their paper with others in the literature. This would make it easier for the reader to place the results in context and see where improvements are made more easily.
(for example comparison to [Yoon and Ryu, 2021, Cai and Zheng, 2023] including constants)

Limitations:
See Weaknesses and Questions.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


",Yes,"# Task
Conduct a detailed examination of the reviews pertaining to the academic paper, emphasizing the paper's originality, methodological excellence, and empirical validations. Evaluate whether the strengths highlighted by the reviewers distinctly surpass the identified weaknesses, particularly in relation to clarity of presentation and its contribution to the research area. Make a final decision regarding acceptance (Yes) or rejection (No), grounded in a balanced yet optimistic interpretation of the paper’s potential significance and implications on future research directions. Use affirmative language that reflects positivity and reassurance about the paper’s capabilities to advance its field. 

# Output format
Answer Yes or No as labels

# Prediction
Text: REVIEW 
Summary:
This paper studies last-iterate convergence rates of online learning in monotone games. The main contribution is an algorithm called Gradient Ascent with Boosting Payoff Perturbation (GABP). The GABP algorithm achieves (1) $O(\log T / T)$ last-iterate convergence with full gradient feedback, which is near-optimal; (2) and $O(1/T^{1/7})$ last-iterate convergence with noisy gradient feedback (the noise is zero-mean with bounded variance). The latter result improves prior results of $O(1/T^{1/10})$. Moreover, the GABP algorithm guarantees an individual dynamic regret of $O(\log^2 T)$ under full gradient feedback, slightly worse than the state-of-the-art bound of $O(\log T)$. This paper also contains numerical experiments on small game instances to demonstrate the effectiveness of GABP.

Soundness:
3: good

Presentation:
2: fair

Contribution:
3: good

Strengths:
The problem of last-iterate convergence rates of no-regret learning algorithms in monotone games is relevant and interesting. Most existing results focus on the full gradient feedback, while only a few provide concrete convergence rates under the noisy gradient or the bandit feedback. The proposed GABP algorithm has near-optimal $O(\log T / T)$ last-iterate convergence rate under full gradient feedback. It also improves the convergence rates under noisy gradient feedback from $O(1/T^{1/10})$ to $O(1/T^{1/7})$. This is a solid contribution to learning in games, although the rate for the noisy gradient feedback setting may not be tight.

Weaknesses:
1. The proposed GABP algorithm does not achieve the optimal $O(1/T)$ last-iterate convergence rate under full gradient feedback. The $O(1/T^{1/7})$ last-iterate convergence rate is also not tight for the noisy feedback.
2. The relationship between the proposed GABP algorithm and the AOG algorithm in [1] and the intuition behind the fast last-iterate convergence rates is not clearly discussed. These two algorithms are different (as shown in Appendix F) but share similar ideas. The anchoring term in both algorithms comes from the (implicit) Halpern iteration algorithm, which can not be run directly. The difference is that GABP views each step of Halpern iteration as a fixed point problem (Line 170) and uses an inner loop of $\log (1/\epsilon)$ steps to get an $\epsilon$-approximation (this is called updating the reference strategy in the paper.); In contrast, AOG directly uses optimism to approximate the implicit update. This leads to GABP being a log factor slower than AOG in the full gradient setting. However, the approximating the fixed point approach is more robust in the noisy gradient setting due to strong monotonicity. Moreover, the potential function and the approximately non-increasing potential analysis are very similar to that used in [1]. If they are inspired by [1] then this should be acknowledged. 

[1] Doubly Optimal No-Regret Learning in Monotone Games, Cai and Zheng, ICML 2023.

Limitations:
N/A

Rating:
5: marginally below the acceptance threshold

Confidence:
4: You are confident in your assessment, but not absolutely certain. It is unlikely, but not impossible, that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work.

REVIEW 
Summary:
The paper introduces a novel algorithmic approach to enhance the convergence of first-order methods in the context of monotone games. The authors propose a payoff perturbation technique that introduces strong convexity to players' payoff functions, which is crucial for achieving last-iterate convergence. This technique is particularly designed to handle scenarios where the gradient of the payoff functions is monotone and potentially noisy. The paper presents a method called Gradient Ascent with Boosting Payoff Perturbation (GABP), which incorporates a unique perturbation into the payoff function and maintains a periodically re-initializing anchoring strategy. The authors demonstrate that GABP offers faster last-iterate convergence rates compared to existing algorithms, even in the presence of additive noise.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Originality: The paper presents a unique perturbation technique that addresses the challenge of last-iterate convergence in monotone games. The proposed GABP algorithm is an innovative modification of Adaptively Perturbed Mirror Descent (APMD), offering improved convergence rates.

Quality: The theoretical development is thorough, with rigorous proofs provided for the convergence rates of GABP in both full and noisy feedback settings. The paper also includes a detailed analysis of the algorithm's performance in terms of individual regret.

Clarity: The paper is well-organized, with clear explanations of the algorithm, theoretical results, and experimental setup. The use of pseudo-code for GABP aids in understanding the algorithm's implementation.

Significance: The work contributes to the field of online learning in games, providing a solution that is particularly relevant for applications such as Generative Adversarial Networks (GANs) and large language model fine-tuning, where last-iterate convergence is desirable.

Weaknesses:
Experimental Validation: While the paper provides empirical results, the experiments could be expanded to include a broader range of game types and noise levels to further validate the robustness and generalizability of GABP.

Comparison with State-of-the-Art: The paper compares GABP with APMD and Optimistic Gradient Ascent (OGA) but could benefit from a more comprehensive comparison with other existing methods in the literature to better situate its contributions.

Practical Considerations: While the paper addresses the theoretical aspects of GABP, it could provide more insights into practical considerations, such as the implementation challenges and potential modifications needed for real-world applications.

Limitations:
None

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This work focuses on last-iterate convergence of  game dynamics. A payoff perturbation technique is proposed by adding strong convexity to players' payoff functions. Despite it is a well studied technqiue in learning in repeated games with first-order methods, especially in last-iterate convergence, a novel perturbation scheme introduced in this paper allows on to provide faster last-iterate convergence compared to previous works.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
This paper provides a relatively complete result containing last-iterate convergence rate of the proposed algorithm GABP in full feedback and noisy feedback. The faster rate of convergence is an improvement compared to existing works. Except for some weakness (will be stated later), the presentation of this paper is clear to understand. The authors have reviewed most related works to my best knowledge, so that the contributions claimed are easy to follow. Addition to theoretical works, this paper has provided experiments (sufficient in my opinion) showing the comparison of GABP and existing algorithms such as Adaptively perturbed gradient ascent and Optimistic gradient ascent.

Weaknesses:
One obvious spot that should be added to improve the presentation is the following. The game considered in this paper is motivated by real-life examples. But the authors only give one example motivating monotone games. Part of contributions of the paper is claimed to be the study of two feedback models: full feedback and noisy feedback, but there is not specific examples and applications illustrating the importance of these settings. For sure readers can always find related works even just by googling the keywords, but providing concrete application scenes where the gradient of payoff can be achieved perfectly or only partially achievable gradients can be obtained is important, especially ""noisy feedback"" can be just a model of many cases.

Limitations:
This is theory based paper, no potential negative impact will cause.

Rating:
6: marginally above the acceptance threshold

Confidence:
3: You are fairly confident in your assessment. It is possible that you did not understand some parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.

REVIEW 
Summary:
This paper studies first order methods to solve monotone games where the gradient of the payoff function is monotone in the strategy, along with additive noise. The authors introduce a payoff perturbation technique which introduces strong convexity to the to the payoff functions and thereby derive last iterate convergence rates.

Soundness:
3: good

Presentation:
3: good

Contribution:
3: good

Strengths:
Overall the paper is well written and the method and results are interesting.

Weaknesses:
The authors should include a table which compares their paper with others in the literature. This would make it easier for the reader to place the results in context and see where improvements are made more easily.
(for example comparison to [Yoon and Ryu, 2021, Cai and Zheng, 2023] including constants)

Limitations:
See Weaknesses and Questions.

Rating:
6: marginally above the acceptance threshold

Confidence:
2: You are willing to defend your assessment, but it is quite likely that you did not understand the central parts of the submission or that you are unfamiliar with some pieces of related work. Math/other details were not carefully checked.


Label: ",Yes,yes,No,0.0,0.83
